{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=100_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[1]', diff_loss_weight=100.0, gradient_accumulation_steps=1, kg_eval_freq=50, kg_eval_mode='metric', kr_eval_freq=50, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=100, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=100_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[1]_result.json', stream_id=1, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4120, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["the Cobham\u2013Edmonds thesis", "15 February 1546", "special efforts", "17", "southwestern France", "CBS Sports", "different viewpoints and political parties", "Thomas Commerford Martin", "24 August \u2013 3 October 1572", "long, slender tentacles", "45 minutes", "Town Moor", "BBC HD", "Ealy", "August 15, 1971", "a squared integer", "declared Japan a \"nonfriendly\" country", "a cubic interpolation formula", "huge mouths armed with groups of large, stiffened cilia that act as teeth", "1852", "an intuitive understanding", "the Small Catechism", "learning of the execution of Johann Esch and Heinrich Voes", "Super Bowl XLVII", "Ozone depletion and global warming", "widespread education", "chloroplasts", "Warraghiggey", "The Scotland Act 1998", "The Bachelor", "delivery of these messages by store and forward switching", "9000 BP", "criminal investigations", "2002", "sculptures, friezes and tombs", "Sonderungsverbot", "The Simpsons", "826", "English", "energize electrons", "Catholicism", "Robert R. Gilruth", "He prayed, consulted friends, and gave his response the next day", "young men who had not fought", "Manakin Town", "tidal delta", "A Charlie Brown Christmas", "formal", "Establishing \"natural borders\"", "(sworn brother or blood brother)", "Tyneside's shipbuilding heritage, and inventions which changed the world", "structural collapse, cost overruns, and/or litigation", "severely reduced rainfall and increased temperatures", "sponges", "Cam Newton", "science fiction", "Sonia Shankman Orthogenic School", "an aided or an unaided school", "steam turbine plant", "metamorphic processes", "faith", "article 49", "the meeting of the Church's General Assembly", "missing self"], "metric_results": {"EM": 0.765625, "QA-F1": 0.781423611111111}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-526", "mrqa_squad-validation-2974", "mrqa_squad-validation-1763", "mrqa_squad-validation-4621", "mrqa_squad-validation-2394", "mrqa_squad-validation-8719", "mrqa_squad-validation-8896", "mrqa_squad-validation-5773", "mrqa_squad-validation-5812", "mrqa_squad-validation-2113", "mrqa_squad-validation-5676", "mrqa_squad-validation-5226", "mrqa_squad-validation-337", "mrqa_squad-validation-1662", "mrqa_squad-validation-6947"], "SR": 0.765625, "CSR": 0.765625, "EFR": 0.9333333333333333, "Overall": 0.8494791666666667}, {"timecode": 1, "before_eval_results": {"predictions": ["The Adventures of Ozzie and Harriet", "The Open Championship golf and The Wimbledon tennis tournaments", "32.9%", "365.2425 days of the year", "health care professional", "1970s", "Sunni Arabs from Iraq and Syria", "the computational problem of determining whether two finite graphs are isomorphic", "Daniel Burke", "the highest terrace", "major national and international patient information projects and health system interoperability goals", "three", "net force", "12 January", "1976\u201377", "E. W. Scripps Company", "zoning and building code requirements", "river Deabolis", "1968", "King George III", "Baden-W\u00fcrttemberg", "lines or a punishment essay", "Book of Discipline", "complicated definitions", "coordinating lead author", "TFEU article 294", "G. H. Hardy", "30-second", "Royal Ujazd\u00f3w Castle", "Church and the Methodist-Christian theological tradition", "main hall", "Teaching Council", "One could wish that Luther had died before ever [On the Jews and Their Lies] was written", "Russell T Davies", "Cape Town", "Gospi\u0107, Austrian Empire", "Classic FM's Hall of Fame", "optimisation", "2014", "late 1970s", "30% less", "1983", "Happy Days", "1,230 kilometres", "23 November 1963", "Apollo 20", "six divisions", "scoil phr\u00edobh\u00e1ideach", "business", "teachers in publicly funded schools", "Stanford University Professor of Comparative Literature Richard Rorty", "1991", "organisms", "41", "carbon", "the fertile highlands", "harder", "50% to 60%", "Norman Greenbaum", "appellate courts are also called appeals courts, courts of appeals, superior courts, or supreme courts", "The Prisoners ( Temporary Discharge for Ill Health ) Act", "Carol Ann Susi", "Daenerys Targaryen", "Raabta"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8237356635237827}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.2105263157894737, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9752", "mrqa_squad-validation-1791", "mrqa_squad-validation-6388", "mrqa_squad-validation-6059", "mrqa_squad-validation-8616", "mrqa_squad-validation-2611", "mrqa_squad-validation-6282", "mrqa_squad-validation-3352", "mrqa_squad-validation-1906", "mrqa_squad-validation-8035", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7792", "mrqa_hotpotqa-validation-1006"], "SR": 0.78125, "CSR": 0.7734375, "EFR": 0.8571428571428571, "Overall": 0.8152901785714286}, {"timecode": 2, "before_eval_results": {"predictions": ["235", "P", "Smith and Jones", "1767", "53,000", "Fu\u00dfach", "leptin, pituitary growth hormone, and prolactin", "reverse direction", "7 West 66th Street", "patent archives", "Any member", "4-week period", "six", "His wife Katharina", "Colorado Desert", "John Pell, Lord of Pelham Manor", "United States", "2014", "Alberto Calder\u00f3n", "Roger NFL", "1950s", "1980s", "the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "second use of the law", "free", "1973", "September 1969", "Mansfeld", "Warsaw Stock Exchange", "390 billion", "a suite of network protocols", "eighteenth century", "journal Nature", "2009", "Franz Pieper", "geochemical evolution of rock units", "three times", "rhetoric", "Genoese traders", "the flail of God", "Saudi Arabia and Iran", "149,025", "13 May 1899", "Lunar Module Pilot", "citizenship", "immediately north of Canaveral at Merritt Island", "accountants", "return home", "June 4, 2014", "kinetic friction force", "\u2153 to Tesla", "signal amplification", "Lituya Bay in Alaska", "120 m ( 390 ft ) at its widest part", "the eighth season will have only six episodes", "100 members", "photoelectric", "Welch, West Virginia", "26 January was chosen as the Republic day because it was on this day in 1930", "twelve Wimpy Kid books have been released, plus one do - it - yourself book and two movie diaries", "Hal David and Burt Bacharach", "five points", "The Monitor and Merrimac", "the Atlantic Ocean, the Gulf of Mexico and the Straits of Florida"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7573502452408702}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.375, 1.0, 1.0, 0.6153846153846154, 0.4444444444444445, 1.0, 0.4, 1.0, 0.4090909090909091, 1.0, 0.0, 0.5, 0.5, 0.16666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1759", "mrqa_squad-validation-4731", "mrqa_squad-validation-5972", "mrqa_squad-validation-2689", "mrqa_squad-validation-80", "mrqa_squad-validation-9173", "mrqa_squad-validation-5788", "mrqa_squad-validation-4415", "mrqa_squad-validation-4673", "mrqa_squad-validation-1454", "mrqa_squad-validation-3840", "mrqa_squad-validation-1841", "mrqa_squad-validation-1220", "mrqa_naturalquestions-validation-3722", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-6125", "mrqa_naturalquestions-validation-2016", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-3996"], "SR": 0.671875, "CSR": 0.7395833333333333, "EFR": 0.8095238095238095, "Overall": 0.7745535714285714}, {"timecode": 3, "before_eval_results": {"predictions": ["immunosuppressive", "William of Volpiano and John of Ravenna", "April 1523", "Excellent job opportunities", "rebellion is much more destructive", "the principle of inclusions and components", "they were accepted and allowed to worship freely", "12 December 2007", "six", "redistributive taxation", "rubisco", "Abercrombie was recalled and replaced by Jeffery Amherst", "Egypt", "algae", "245,306", "the Data Distribution Centre and the National Greenhouse Gas Inventories Programme", "Stromules", "Yam route systems", "Stairs", "transplastomic", "around 300,000", "three", "Von Miller", "Africa", "clinical services that pharmacists can provide for their patients", "Raghuram Rajan", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "Mark Ronson", "Calvin cycle", "their Annual Conference", "Philo of Byzantium", "the mayor (the President of Warsaw)", "cloud storage", "Doritos", "Warsaw University of Technology building", "the Great Yuan", "Lenin", "the Solim\u00f5es Basin", "Charles Darwin", "23 November", "oppidum Ubiorum", "John Elway", "Downtown Riverside", "Capital Cities Communications", "lamprey and hagfish", "physicians and other healthcare professionals", "Golden Gate Bridge", "Michael Schumacher holds the record for the most Grand Prix victories, having won 91 times", "10.5 %", "The Man", "President Gerald Ford", "Bud '' Bergstein", "Janie Crawford", "it extends from the optic disc to the optic chiasma and continues as the optic tract to the lateral geniculate nucleus, pretectal nuclei, and superior colliculus", "Jerry Ekandjo", "961", "Forney Hull ( James Frain ), the surly librarian who looks after his alcoholic sister Mary Elizabeth ( Margaret Hoard )", "In October 1973, the price was raised to $42.22", "the land itself, while blessed, did not cause mortals to live forever", "The earliest credible evidence of either coffee drinking or knowledge of the coffee tree appears in the middle of the 15th century, in Yemen's Sufi monasteries", "6 March 1983", "Viola Larsen", "horror fiction", "26,000"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7710565476190476}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 0.8, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.4, 0.2, 0.13333333333333333, 0.16666666666666669, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4108", "mrqa_squad-validation-8830", "mrqa_squad-validation-4759", "mrqa_squad-validation-298", "mrqa_squad-validation-6614", "mrqa_squad-validation-670", "mrqa_squad-validation-962", "mrqa_squad-validation-9298", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4433", "mrqa_hotpotqa-validation-454"], "SR": 0.703125, "CSR": 0.73046875, "EFR": 0.9473684210526315, "Overall": 0.8389185855263157}, {"timecode": 4, "before_eval_results": {"predictions": ["infrequent rain", "the king of France", "approximately 80 avulsions", "15", "Presque Isle", "wireless", "Coldplay", "the Yuan dynasty", "same-gender marriages", "red algae red", "after their second year", "1960s", "narcotic drugs", "Napoleon", "Immunology", "geophysical surveys", "topographic gradients", "130 million cubic foot", "50 fund", "was particularly forceful, stating that British colonists would not be safe as long as the French were present.", "sponges, both ctenophores and cnidarians", "motivated students", "Michael Mullett", "15", "James Gamble & Reuben Townroe", "struggle, famine, and bitterness among the populace", "Blaine Amendments", "the Jews", "six", "Big Ten Conference", "Thames River", "NDS, a Cisco Systems company", "shipping toxic waste", "anarchists", "carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "immunoglobulins and T cell receptors", "previously separated specialties", "a thylakoid", "University College London", "to protect their tribal lands from commercial interests", "religious beliefs", "a guilty plea", "the kettle and the Cricket", "Manilal", "Vlad the Impaler", "\"Take us the foxes, the little... Alexandra - first cousins - as a means of getting Horace's money", "Betamax", "Vincent van Gogh", "Earth's orbital period is 365 & this fraction of a day", "Tiger Woods won the 1994 U.S. Amateur in dramatic, history-making fashion.", "It was Arizona's territorial capital from 1867 to 1877", "Marshal Dillon", "a visual reminder of the chores they still", "The Best Hotels on Bali", "Teddy's made this presidential Hollywood hotel a happening", "Light Amplification by Stimulated Emission by radiation", "Incomprehensible", "Saturn", "Hundreds of species of peat mosses are found in bogs throughout Canada", "why", "Daya", "meat", "American", "Mexican military"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6715890012765013}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 0.7499999999999999, 0.3333333333333333, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.2666666666666667, 1.0, 0.8, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.4, 0.22222222222222224, 0.25, 0.0, 0.0, 0.15384615384615385, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9357", "mrqa_squad-validation-10204", "mrqa_squad-validation-8840", "mrqa_squad-validation-10186", "mrqa_squad-validation-4424", "mrqa_squad-validation-1960", "mrqa_squad-validation-8131", "mrqa_squad-validation-2526", "mrqa_squad-validation-2804", "mrqa_squad-validation-5214", "mrqa_squad-validation-6721", "mrqa_searchqa-validation-12428", "mrqa_searchqa-validation-14338", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-9448", "mrqa_searchqa-validation-12311", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-5639", "mrqa_searchqa-validation-10360", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12931", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-6541", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-10506", "mrqa_searchqa-validation-16377", "mrqa_searchqa-validation-5669", "mrqa_searchqa-validation-11224", "mrqa_naturalquestions-validation-124"], "SR": 0.546875, "CSR": 0.69375, "EFR": 0.7241379310344828, "Overall": 0.7089439655172414}, {"timecode": 5, "before_eval_results": {"predictions": ["bacteriophage T4", "6.7", "second-largest", "time and space", "the Meuse", "a Western Union superintendent", "Super Bowl XLIV", "1891", "New Orleans", "fell from his horse while hunting", "the member state cannot enforce conflicting laws", "J. F. D. Shrewsbury", "a mouth that can usually be closed by muscles; a pharynx (\"throat\"); a wider area in the center that acts as a stomach; and a system of internal canals", "inversely", "Europe", "he was illiterate in Czech", "colonies", "$37.6 billion", "Kalenjin", "1269", "the 17th century", "Time Warner Cable", "toward the Atlantic", "economic", "CrossCountry", "ITV", "SAP Center", "Variable lymphocytes receptors (VLRs)", "Edict of Fontainebleau", "Levi's Stadium", "ten million", "the Lippe", "Video On Demand content", "time and storage", "semester", "the courts of member states and the Court of Justice of the European Union", "Thomas Edison", "1971", "quantum mechanics", "Lawrence", "League of the Three Emperors", "the field of science", "143,007", "Bill Clinton", "Waltham Abbey", "Secretariat", "coaxial", "Mary Harron", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.,", "Thomas Christopher Ince", "American Chopper", "drawn the name out of a hat", "German", "Fort Valley, Georgia", "American", "Easy", "Belvoir", "Congo River", "Abigail", "Murwillumbah, New South Wales, Australia", "Joel Chandler Harris", "corruption", "How long", "Dover Beach"], "metric_results": {"EM": 0.75, "QA-F1": 0.8128176510989011}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3846153846153846, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8000000000000002, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1775", "mrqa_squad-validation-6218", "mrqa_squad-validation-1187", "mrqa_squad-validation-8544", "mrqa_squad-validation-6676", "mrqa_squad-validation-1672", "mrqa_squad-validation-7214", "mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-4573", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-323", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2387", "mrqa_hotpotqa-validation-2315", "mrqa_triviaqa-validation-1616", "mrqa_searchqa-validation-14229"], "SR": 0.75, "CSR": 0.703125, "EFR": 0.875, "Overall": 0.7890625}, {"timecode": 6, "before_eval_results": {"predictions": ["1540s", "the Court of Justice of the European Union", "its circle logo", "three", "negative", "fear of their lives", "80%", "1521", "Gibraltar and the \u00c5land islands", "distorting the grana and thylakoids", "exceeds any given number", "Hulagu Khan", "poet", "quality rental units", "Grover Cleveland", "overthrow a government", "entertainment", "A vote clerk", "high growth rates", "vicious and destructive", "Sony", "Stagecoach", "Silk Road", "San Diego", "a German Nazi colonial administration", "four public charter schools on the South Side of Chicago", "the means to invest in new sources of creating wealth", "Spanish", "Structural geologists", "president and CEO", "indulgences for the living", "BSkyB", "terrorist organisation", "Cam Newton", "The U2 360\u00b0 Tour", "The 5 foot 9 inch tall twins", "James Victor Chesnutt", "Benjamin Burwell Johnston, Jr.", "a large green dinosaur", "Taylor Swift", "Eric Edward Whitacre", "the Joint Chiefs of Staff", "Linux Format", "Jasenovac concentration camp", "Rabat", "between 11 or 13 and 18", "Heather Langenkamp (born July 17, 1964)", "Henry Gwyn Jeffreys Moseley", "paracyclist", "Vilnius Airport (IATA: VNO, ICAO: EYVI)", "It is based in Bury St Edmunds, Suffolk, England", "Charmed", "Lily Hampton", "English former international footballer", "the Philadelphia Eagles", "Rickie Lee Skaggs", "48,982", "Ashanti", "79", "Algeria", "a novel", "The Biafran War", "Polar Bear", "Atlantic City's First Boardwalk"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7655615371148459}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.2, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.4, 0.3333333333333333, 0.5, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.8, 0.5, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_squad-validation-3939", "mrqa_squad-validation-5774", "mrqa_squad-validation-6788", "mrqa_squad-validation-6029", "mrqa_squad-validation-7983", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-5324", "mrqa_hotpotqa-validation-5649", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-1291", "mrqa_hotpotqa-validation-976", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-5300", "mrqa_newsqa-validation-3377", "mrqa_searchqa-validation-5279", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-13072"], "SR": 0.640625, "CSR": 0.6941964285714286, "EFR": 0.8260869565217391, "Overall": 0.7601416925465838}, {"timecode": 7, "before_eval_results": {"predictions": ["IgG", "Amazoneregenwoud", "co-NP", "BBC Radio Newcastle", "England, Wales, Scotland, Denmark, Sweden, Switzerland, the Dutch Republic", "the working fluid", "a suite of network protocols created by Digital Equipment Corporation", "American Baptist Education Society", "Dutch", "output", "those who already hold wealth", "center of mass", "attention-seeking and disruptive students", "more than $45,000", "Defensive ends", "MLB", "the papacy", "through homologous recombination", "canalized section", "in protest against the occupation of Prussia by Napoleon", "improved markedly", "northern (German) shore of the lake", "computer programs", "General Conference", "1996", "dreams", "The Judiciary", "deterministic", "Bart Starr", "allotrope", "Karluk Kara-Khanid", "Perth, Western Australia", "Ian Rush", "Gerry Adams", "New Orleans Saints", "2016", "four operas", "Harris Museum, Harris Institute or Art School, Harris Technical School and the Harris Orphanage", "Alfred Edward Housman", "the country's second largest city by population", "Sevens", "fennec fox", "Bart Conner", "fantasy", "Martin \"Marty\" McCorm (born 20 July 1983)", "Black Mountain College", "Atat\u00fcrk Museum Mansion", "Bothtec", "Cody Miller", "140 to 219 passengers", "the \"Father of Liberalism\"", "Christophe Lourdelet", "Pablo Escobar", "African", "Mexico City", "Sleeping Beauty", "PeopleMover", "8 December 1985", "Oui Oui", "Ali Bongo", "General Mills", "Ray Harroun", "Juice Newton", "David Tennant"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7585069444444444}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3019", "mrqa_squad-validation-1771", "mrqa_squad-validation-9287", "mrqa_squad-validation-1819", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-2127", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-398", "mrqa_hotpotqa-validation-919", "mrqa_hotpotqa-validation-4405", "mrqa_hotpotqa-validation-3885", "mrqa_triviaqa-validation-1573", "mrqa_newsqa-validation-3925"], "SR": 0.71875, "CSR": 0.697265625, "EFR": 0.9444444444444444, "Overall": 0.8208550347222222}, {"timecode": 8, "before_eval_results": {"predictions": ["Russian", "cellular respiration", "railroad", "Non-revolutionary", "higher efficiency", "Lunar Excursion Module", "Zwickau prophets", "six years", "700", "fire", "arms", "two", "minor", "Fringe or splinter movements", "17", "lower temperatures", "architect or engineer", "1917", "Columbus Avenue and West 66th Street", "TeacherspayTeachers.com", "stratigraphic", "commensal flora", "a + bi", "General Conference in Dallas, Texas", "Central Asian Muslims", "from home viewers who made tape recordings of the show", "1330 Avenue of the Americas", "Alberta and British Columbia", "\"Pimp My Ride\"", "James \"Sonny\" Crockett", "Section.80", "25 million", "8,515", "13 October 1958", "tailless", "Environmental Protection Agency", "between 1932 and 1934", "English former international footballer", "Los Angeles", "England", "Armin Meiwes", "Jean- Marc Vall\u00e9e", "Miss Universe 2010", "Dusty Dvoracek", "boxer", "Boston University", "Fulham", "A55", "Ranulf de Gernon, 4th Earl of Chester", "\u00c6thelstan", "Special economic zone", "44", "I", "Harriet Tubman", "Manchester United", "Dragon TV", "Greek-American", "A diastema ( plural diastemata )", "Shirley Horn", "not tolerate a nuclear Iran", "Sasquatch", "Papua New Guinea", "Edgar Degas", "Manchester"], "metric_results": {"EM": 0.625, "QA-F1": 0.7098417207792208}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.2, 0.0, 0.28571428571428575, 0.0, 0.5, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6789", "mrqa_squad-validation-3391", "mrqa_squad-validation-9859", "mrqa_squad-validation-7643", "mrqa_squad-validation-5972", "mrqa_hotpotqa-validation-4363", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-5108", "mrqa_hotpotqa-validation-1633", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-305", "mrqa_triviaqa-validation-3170", "mrqa_newsqa-validation-1268", "mrqa_searchqa-validation-10087", "mrqa_triviaqa-validation-1423"], "SR": 0.625, "CSR": 0.6892361111111112, "EFR": 0.6666666666666666, "Overall": 0.6779513888888888}, {"timecode": 9, "before_eval_results": {"predictions": ["$32 billion", "centrifugal governor", "Orange County", "The chloroplast peripheral reticulum", "1962", "European Court of Justice held that a Commissioner giving her dentist a job, for which he was clearly unqualified, did in fact not break any law", "Rugby", "Germany", "politically and socially unstable", "Theatre Museum", "90\u00b0", "iTunes", "its unpaired electrons", "French", "Behind the Sofa", "he sent missionaries, backed by a fund to financially reward converts to Catholicism", "pyrenoid and thylakoids", "Woodward Park", "refusal to submit to arrest", "25 May 1521", "essentially holy people", "diplomacy or military force", "increase in the land available for cultivation", "the value of the spin", "pivotal event", "an American YouTube personality, spokesmodel, television personality, and LGBT rights activist", "John Alexander", "David Michael Bautista Jr.", "Thursday", "actor, singer and a DJ", "Prince Amedeo", "Lambic", "Mazatl\u00e1n", "Assistant Director Neil J. Welch", "March 30, 2025", "England", "Kentucky, Virginia, and Tennessee", "Autopia", "Yasir Hussain", "USC Marshall School of Business", "Stephen James Ireland", "Marko Tapani", "Estadio de L\u00f3pez Cort\u00e1zar", "Kohlberg K Travis Roberts", "Fort Albany", "I'm Shipping Up to Boston", "2500 ft", "Central Park", "Robert John Day", "Afroasiatic", "James Tinling", "Italy", "the 79th Masters Tournament", "Kristoffer Rygg", "University of Kentucky College of Pharmacy", "William Shakespeare", "Bob Dylan", "Erika Mitchell Leonard", "Santiago", "couscousCouscous", "more than 22 million", "morphine sulfate oral solution 20 mg/ml", "The Firm", "freshwater"], "metric_results": {"EM": 0.625, "QA-F1": 0.6878370374486361}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.45161290322580644, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727272, 1.0, 1.0, 0.35294117647058826, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4147", "mrqa_squad-validation-2943", "mrqa_squad-validation-3130", "mrqa_squad-validation-8651", "mrqa_squad-validation-4572", "mrqa_squad-validation-6797", "mrqa_squad-validation-9735", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-3553", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-2743", "mrqa_naturalquestions-validation-10208", "mrqa_triviaqa-validation-2522", "mrqa_newsqa-validation-1668", "mrqa_searchqa-validation-3622"], "SR": 0.625, "CSR": 0.6828125, "EFR": 0.8333333333333334, "Overall": 0.7580729166666667}, {"timecode": 10, "before_eval_results": {"predictions": ["November 1979", "Timucuan", "suburban", "vertebrates", "Fears of being labelled a pedophile or hebephile", "it consumes ATP and oxygen, releases CO2, and produces no sugar", "Panthers", "Sanders", "even greater inequality and potential economic instability", "Gamal Abdul Nasser", "Immunodeficiencies", "counterflow", "John B. Goodenough", "his arrest was not covered in any newspapers in the days, weeks and months after it happened", "arrows, swords, and leather shields", "Cybermen in series 2, the Macra and the Master in series 3, the Sontarans and Davros in series 4", "to attend school at the Higher Real Gymnasium", "Standard Model", "\u00d6gedei", "Rhine-Ruhr region", "a course of study", "Prevenient grace", "Missouri Tigers", "Gladstone Region", "Chris Pine", "Yoo Seung-ho", "World War II", "NCAA Division I", "The The Onion", "Mickey's PhilharMagic", "A Bug's Life", "1978", "May 2008", "Italy", "La Familia Michoacana", "Uzumaki", "Tom Jones", "Steven A. Austin", "Barbara Niven", "13\u20133", "Eliot Spitzer", "5,042", "European", "the first integrated circuit", "Tianhe Stadium", "1952", "the fourth Thursday", "Giuseppe Verdi", "Germany", "New Jersey", "Bath, Maine", "Ector County", "Jim Davis", "Buck Owens", "World Health Organization", "Emmanuel Ofosu Yeboah", "California", "Heather Stebbins", "Islam", "Sir Giles Gilbert Scott", "the rig's cementing, or the casing that holds the well in place, must have failed", "Mediterranean", "Onomastic Sobriquets In The Food And Beverage Industry", "microchips"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6861531391402715}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.923076923076923, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7269", "mrqa_squad-validation-797", "mrqa_squad-validation-7502", "mrqa_squad-validation-7729", "mrqa_squad-validation-1166", "mrqa_squad-validation-6166", "mrqa_squad-validation-1877", "mrqa_hotpotqa-validation-2725", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-2977", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-4986", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-7415", "mrqa_triviaqa-validation-7398", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3339", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-10351"], "SR": 0.609375, "CSR": 0.6761363636363636, "EFR": 0.76, "Overall": 0.7180681818181818}, {"timecode": 11, "before_eval_results": {"predictions": ["UHF", "deflate", "Battle of Olustee", "Spanish", "100\u2013150", "Philo of Byzantium", "cooler", "marine waters worldwide", "$60,000 in cash and stock and a royalty of $2.50 per AC horsepower produced by each motor", "his mother's genetics and influence", "shock", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "new element", "the building is ready to occupy", "boom-and-bust cycles", "Edinburgh", "Richard Allen and Absalom Jones", "earn as much as a healthy young man", "Jamukha", "1969", "about $700 billion more than we export", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations", "body bags", "near Warsaw, Kentucky", "Arthur E. Morgan III", "April 2010", "Paul McCartney, Yoko Ono Lennon, Olivia Harrison and Ringo Starr", "does not involve MDC head Morgan Tsvangirai", "an accidental death", "200", "two phone calls for delivery: One for pizza, the other for the drug ketamine", "opposition party members", "Missouri", "President Obama's race in 2008", "executive director of the Americas Division of Human Rights Watch", "Dominican Republic", "90", "KARK", "a space for aspiring entrepreneurs to brainstorm with like-minded people", "her home", "Employee Free Choice Act", "Bush administration", "more than 200", "This is not a project for commercial gain. It is done with the parents' full consent", "best-of-three series", "Kaka", "Japanese ex-wife", "Dan Parris, 25, and Rob Lehr, 26", "Fayetteville apartment of 2nd Lt. Holley Wimunc", "two", "nearly $2 billion", "Jacob", "Molotov cocktails, rocks and glass", "as many as 250,000 unprotected civilians", "Winehouse", "the Ark of the Covenant ( the Aron Habrit in Hebrew )", "Jean F Kernel ( 1497 -- 1558 )", "Thomas Hardy\u2019s novel", "Richmond", "1994", "The Conjuring", "the Anzacs", "Georgian Bay", "Nowhere Boy"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6347958465145965}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.4, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.15384615384615385, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.38095238095238093, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.6, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1257", "mrqa_squad-validation-2493", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-3944", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-2463", "mrqa_newsqa-validation-2804", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3068", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-5769", "mrqa_triviaqa-validation-5434", "mrqa_searchqa-validation-2548", "mrqa_searchqa-validation-8335"], "SR": 0.546875, "CSR": 0.6653645833333333, "EFR": 0.8275862068965517, "Overall": 0.7464753951149425}, {"timecode": 12, "before_eval_results": {"predictions": ["threatened \"Old Briton\" with severe consequences if he continued to trade with the British", "wealth", "every good work designed to attract God's favor", "Napoleon", "mass production", "James O. McKinsey", "private actors", "Bell Northern Research", "a body of treaties and legislation, such as Regulations and Directives, which have direct effect or indirect effect on the laws of European Union member states.", "1227", "lower lake", "three", "Elders", "587,000 square kilometres", "Private Bill Committees", "Bruno Mars", "the Catechism", "Stagg Field", "Ian Botham", "E. T. A. Hoffmann", "Vincent Motorcycle Company", "Groucho", "Salvador Allende", "Marie Antoinette", "Redmond", "Erik Thorvaldson", "Marsyas", "Pal Joey", "Mary Jane Grant", "green", "Indonesia", "supreme religious leader of the Israelites", "Antonio", "European Economic Community (EEC)", "Christine Keeler", "Jesus", "Jack Nicholson Easy Rider", "four", "Netherlands", "\"Sugar Baby Love\"", "Coretta Scott King", "Sean", "Bill and Taffy Danoff", "the zygote", "Travis", "Blue Peter", "Robert Kennedy", "Q", "an umbrella", "Hobbies", "barber", "Harry Hopman", "Murrah Federal Office Building", "Evita", "an old, unsavoury, and oily black clay pipe", "fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "bohrium", "Eleanor of Aquitaine", "Mickey Gilley", "get four successful women together on a movie set and you'd think it's all claws, all the time.", "a delegation of American Muslim and Christian leaders", "Royal Wives", "the Greek Village", "Juan Martin Del Potro"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5762090773809523}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.07142857142857142, 1.0, 0.8571428571428571, 1.0, 0.125, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2262", "mrqa_squad-validation-4257", "mrqa_squad-validation-9418", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-2240", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-3027", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-215", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-6974", "mrqa_triviaqa-validation-712", "mrqa_naturalquestions-validation-4905", "mrqa_hotpotqa-validation-3819", "mrqa_newsqa-validation-3987", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-5929"], "SR": 0.515625, "CSR": 0.6538461538461539, "EFR": 0.7419354838709677, "Overall": 0.6978908188585609}, {"timecode": 13, "before_eval_results": {"predictions": ["Polignac's conjecture", "Chilaun", "Pittsburgh Steelers", "Sky Digital", "Allston Science Complex", "divergent boundaries", "9th century", "many", "1775\u20131795", "Dorothy and Michael Hintze", "William Ellery Channing and Ralph Waldo Emerson", "Fu\u00dfach", "Wesleyan Holiness Consortium", "Maxwell", "in whole by charging their students tuition fees", "Dublin, Cork, Youghal and Waterford", "Tangled", "dernell", "moles", "leucippus", "k Kwajalein Atoll", "Anne Boleyn", "Calvin Coolidge", "Steve McQueen", "Portugal", "jazz tenor saxophonist", "(1/6)", "komando Pasukan Khusus", "Carlisle", "a liquid form", "Chillicothe and Zanesville", "Lucas McCain", "Antarctica", "mercury gilding", "aniridia", "t.S. Eliot", "the River Forth", "woe", "NOW Magazine", "Burmese", "Italy", "Canada", "typhoid fever", "Tina Turner", "Action Man", "Walt Kowalski-Gran Torino", "2010", "einasto's law", "Venezuela", "rothel Cecile Rosalie Allen", "temperature inversion", "40", "phrenology", "San Francisco", "Fall 1998", "Marcus Atilius Regulus", "Chris Weidman", "Drillers Stadium", "one", "Virgin America", "John Grisham", "apr 27, 2016", "Iran's parliament speaker", "In Group D, Bundesliga Hertha Berlin beat Sporting Lisbon of Portugal 1-0 through Gojko Kacar's second half strike.It meant Dutch side Heerenveen were eliminated despite a 5-0 home victory"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5880208333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, false, true, false, true, false, false, true, true, false, true, false, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6983", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-1142", "mrqa_triviaqa-validation-2222", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-2992", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-4391", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-2290", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-1733", "mrqa_naturalquestions-validation-5675", "mrqa_hotpotqa-validation-1390", "mrqa_searchqa-validation-2972", "mrqa_searchqa-validation-15784", "mrqa_newsqa-validation-2281"], "SR": 0.546875, "CSR": 0.6462053571428572, "EFR": 0.7586206896551724, "Overall": 0.7024130233990148}, {"timecode": 14, "before_eval_results": {"predictions": ["in an adult plant's apical meristems", "Tugh Temur", "Persia", "Parliament Square, High Street and George IV Bridge in Edinburgh", "Revolutionary", "Beijing", "three years", "27 July 2008", "chemically", "Aristotle", "St. George's Church", "Missy", "Strathclyde Regional Council debating chamber in Glasgow, and to the University of Aberdeen", "public official", "the most cost efficient bidder", "acorn trees", "Russia Time Zone", "thighbone", "Olympia", "Ukrainian Soviet Socialist Republic", "phyph", "alberta", "a hopeful miner, one of the many that rushed to the Yukon when gold was discovered there in", "amber", "high school football", "The executioner's Song", "a right-angle", "Astana", "anamosa", "wetzsteon", "Ephesus", "drinking", "jedoublen/jeopardy", "rope", "glare", "Cologne", "Leadership Academy for Girls", "an ingot", "Kosovo", "James Jeffords", "Prague", "tennis", "silk", "black cowboys", "a citizen", "Key deer", "kyoto", "burt Reynolds", "Thant", "NASCAR NH", "windjammer", "Monsieur Verdoux", "stanley & Miller", "Augusta", "counter clockwise", "2013", "Nick Hornby", "parachutes", "December 24, 1973", "David Weissman", "bikinis", "the Dalai Lama", "memories of his mother", "Israel"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4770833333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7000000000000001, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2105", "mrqa_squad-validation-7818", "mrqa_squad-validation-9402", "mrqa_squad-validation-6801", "mrqa_searchqa-validation-2291", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-15477", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-16197", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-4727", "mrqa_searchqa-validation-6146", "mrqa_searchqa-validation-9588", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-4439", "mrqa_searchqa-validation-6335", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1187", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-3873", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-16219", "mrqa_searchqa-validation-12545", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-4426", "mrqa_searchqa-validation-15235", "mrqa_searchqa-validation-1976", "mrqa_searchqa-validation-5100", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-6518", "mrqa_searchqa-validation-2445", "mrqa_searchqa-validation-4459", "mrqa_searchqa-validation-10412", "mrqa_naturalquestions-validation-325", "mrqa_triviaqa-validation-6129", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3084"], "SR": 0.40625, "CSR": 0.6302083333333333, "EFR": 0.8421052631578947, "Overall": 0.736156798245614}, {"timecode": 15, "before_eval_results": {"predictions": ["younger", "gambling", "28,000", "Muhammad ibn Zakar\u012bya R\u0101zi", "river Deabolis", "April 20", "Rhenus", "1996", "wine", "German-Swiss", "Melbourne", "enter the priesthood", "Seattle Seahawks", "IBM", "Arthur Wynne", "Paula Abdul", "Strongsville, Ohio", "Flemish", "Mastercard", "Roger Bonham Smith", "Nashville", "the olfactory nerve", "Ivan the IV", "Nancy Astor", "Liver", "a Las Vegas heist", "Toronto Maple", "Barbra Streisand", "Method", "Utah", "rum", "a Lyrical Writer of the Middle-Class Man", "Johann Strauss II", "kangaroo pal", "pro bono", "Italy", "The Fun Factory", "a brown", "Manfred von Richthofen", "Nacho Libre", "copper", "black magic or of dealings with the devil", "Cucumbers", "Jeffrey S. Wigand", "poetry", "(No Pickle, Lettuce, onions)", "meager", "1942", "squadrons", "the Bunsen burner", "a geisha", "Bigfoot", "altruism", "Frederic Remington", "Juan Francisco Ochoa", "ThonMaker", "a tin star", "dark", "The Legend of Sleepy Hollow", "Doc Hollywood", "Afghanistan", "two", "Belgium", "Rio de Janeiro"], "metric_results": {"EM": 0.5, "QA-F1": 0.5963541666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, false, true, true, false, false, false, true, true, false, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, true, false, true, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9248", "mrqa_squad-validation-9270", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-2440", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-16789", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-16099", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-10427", "mrqa_searchqa-validation-13453", "mrqa_searchqa-validation-508", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-7784", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-1728", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-12150", "mrqa_searchqa-validation-15167", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-15471", "mrqa_searchqa-validation-12729", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-14698", "mrqa_searchqa-validation-3653", "mrqa_naturalquestions-validation-309", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-3675", "mrqa_newsqa-validation-2036"], "SR": 0.5, "CSR": 0.6220703125, "EFR": 0.625, "Overall": 0.62353515625}, {"timecode": 16, "before_eval_results": {"predictions": ["Keraite", "respiration", "1997", "late 1920s", "\u00a31.3bn", "27 July 2008", "unequal", "October 1973", "dragonnades", "Isiah Bowman", "assembly center", "Ominde Commission", "the Weser River", "Eva Peron", "Ho Chi Minh", "circum", "the Igloo", "Detroit Rock City", "Toronto Blue Jays", "President Lincoln", "R.L. Stine", "hate crimes based on gender and... the Hate Crimes Statistics Act", "Julien XIII", "Pl Srkzy de Nagy-Bcsa", "Rubicon River", "Justin Timberlake", "17.5 million", "Jo March", "Play-Doh", "Aphrodite", "The Bible", "The Prince and the Pauper", "Crystal Pepsi", "Hillary Rodham Clinton", "Massasoit", "Pelops", "Balaam", "The Wharton School of the University of Pennsylvania", "The Caine Mutiny", "the Hawks", "(founded 1932)", "(John) Coltrane", "N(uclear) and D(isarmament)", "oxygen", "Oedipus", "Jan Hus", "USA Network's Nashville Star", "Mavericks", "Onegin", "J.L. Hudson's Department Store", "cotton-spinning machine", "(H0H 0H0)", "Dennis Haysbert", "negligence", "the courts", "attached to another chromosome", "Goosnargh", "Australia", "The Jefferson Memorial", "between 11 or 13 and 18", "Michoacan Family", "prisoners", "his entire personal fortune", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan."], "metric_results": {"EM": 0.4375, "QA-F1": 0.5335908882783883}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 0.5, 0.0, 0.33333333333333337, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.23076923076923078]}}, "before_error_ids": ["mrqa_squad-validation-1796", "mrqa_searchqa-validation-2453", "mrqa_searchqa-validation-834", "mrqa_searchqa-validation-9233", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-16726", "mrqa_searchqa-validation-8945", "mrqa_searchqa-validation-6610", "mrqa_searchqa-validation-8220", "mrqa_searchqa-validation-15303", "mrqa_searchqa-validation-9172", "mrqa_searchqa-validation-1355", "mrqa_searchqa-validation-6202", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-11707", "mrqa_searchqa-validation-10168", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-9150", "mrqa_searchqa-validation-13648", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-15453", "mrqa_searchqa-validation-8757", "mrqa_searchqa-validation-15626", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-1371", "mrqa_searchqa-validation-4373", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-13667", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-6265", "mrqa_naturalquestions-validation-794", "mrqa_triviaqa-validation-4973", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-1759"], "SR": 0.4375, "CSR": 0.6112132352941176, "EFR": 0.7222222222222222, "Overall": 0.6667177287581699}, {"timecode": 17, "before_eval_results": {"predictions": ["September 5, 1985", "mannerist architecture", "stratigraphers", "trade unions", "23.9%", "earn as much as a healthy young man", "Centrum", "Benjamin Vail", "Kenneth Swezey", "Party of National Unity", "22", "Dauphin", "Phillip Marlowe", "piracy", "Roger Clemens", "Jesse Jackson", "Puerto Rico", "Mausolus", "Anacondas", "Switzerland", "Deutsche Lufthansa", "The Old Man and the Sea", "French", "Joe Louis", "the Nemean lion", "the three Musketeers", "the Bayeux Tapestry", "Front Porch", "China", "Shia", "notes placed at the bottom of a page", "Roger Hawking", "Marcus Tullius Cicero", "Memphis", "Mountain Dew", "Blanche DuBois", "quilt", "FRAM", "House of Representatives", "Labatt Brewing Company", "Michael Moore", "Oman", "Chevy", "Ingenue", "Pennsylvania", "El burlador de Sevilla", "Ian Fleming", "the Headless Horseman", "London", "Yellowstone National Park", "Ronald Reagan", "Fiddler on the Roof", "Ethiopian", "six", "1992", "a salt", "Bromley-By- Bow", "The Kree", "Cartoon Network", "Caylee Anthony", "know what is important in life", "the \"face of the peace initiative has been attacked\"", "nuclear", "The drama of the action in-and-around the golf course"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6822916666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1295", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-15593", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-5228", "mrqa_searchqa-validation-2195", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-69", "mrqa_searchqa-validation-2709", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-14588", "mrqa_searchqa-validation-12176", "mrqa_searchqa-validation-3176", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-9270", "mrqa_naturalquestions-validation-3267", "mrqa_triviaqa-validation-5158", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-3692", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-4110"], "SR": 0.640625, "CSR": 0.6128472222222222, "EFR": 0.8260869565217391, "Overall": 0.7194670893719807}, {"timecode": 18, "before_eval_results": {"predictions": ["Super Bowl XXXIII", "1993", "June 1979", "friend", "tentacles", "Robert R. Gilruth", "complexity", "same-gender marriages", "2006", "mid-18th century", "blood", "A Raisin in the Sun", "the Sistine Chapel", "Belarus", "a letter T", "a trowel", "Big Bang", "Sex Pistols", "endodontist", "Saturn", "White Cliffs of Dover", "Genoa", "Fanchette", "Jersey Boys", "the door", "Indiana", "Seattle", "Belle", "Chinese-American noodle dish", "10", "the Civil", "Copeina arnoldi", "Paul McCartney", "omega-3", "Raphael", "Bachman Turner Overdrive", "ParaNorman", "yukongoesfishing", "Tokyo", "Bogota", "Wynonna Ellen", "Narnia", "Finnegans Wake", "Wordsworth", "Norway", "bears", "a major earthquake", "Jesus", "elephants", "Mazur", "Denmark", "covert", "Our Country", "May 2010", "in the United States", "Sugarloaf Mountain", "Thailand", "gender queer", "Minister for Social Protection", "Berga", "Mount Vernon Estate & Gardens", "McFerrin, Robin Williams, and Bill Irwin", "ase", "North America"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5516447368421051}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.4, 0.5, 1.0, 0.10526315789473684]}}, "before_error_ids": ["mrqa_searchqa-validation-4484", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-1295", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-15811", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-13718", "mrqa_searchqa-validation-5862", "mrqa_searchqa-validation-4853", "mrqa_searchqa-validation-7964", "mrqa_searchqa-validation-2801", "mrqa_searchqa-validation-12757", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-3043", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-13226", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-5208", "mrqa_searchqa-validation-3547", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-9991", "mrqa_searchqa-validation-3298", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-15717", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-10266", "mrqa_searchqa-validation-9572", "mrqa_naturalquestions-validation-554", "mrqa_triviaqa-validation-2612", "mrqa_hotpotqa-validation-2217", "mrqa_newsqa-validation-2421", "mrqa_newsqa-validation-3343", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-2870"], "SR": 0.421875, "CSR": 0.602796052631579, "EFR": 0.8108108108108109, "Overall": 0.7068034317211949}, {"timecode": 19, "before_eval_results": {"predictions": ["to avoid trivialization", "transplastomic", "Earth", "53,000", "one", "poet", "two", "20,000", "kip", "skeletal muscle and the brain", "2014", "peptide bonds", "Montreal", "Sunday", "sperm and ova", "volcanic activity", "Montgomery", "Rock Island, Illinois", "April 9, 2012", "Squamish, British Columbia, Canada", "Proposition 103", "Arousal regulation", "Charlene Holt", "Buffalo Bill", "1991", "electron shells", "Cornett family", "acid rain", "April 15, 2018", "inefficient", "he cheated on Miley", "2001", "flawed democracy", "735 feet", "1871", "Rick Rude", "Ohio", "a form of business network", "a cylinder of glass or plastic", "James Hutton", "Wakanda and the Savage Land", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Necator americanus", "February 29", "Furious 7", "issues of the American Civil War", "oxygen", "Cecil Lockhart", "Maraade", "British and French Canadian fur traders", "semi-autonomous organisational units", "Lou Rawls", "Hermia", "Jupiter", "east", "15", "John Robert Cocker", "Israel", "a simple puzzle video game", "a palace", "the olfactory nerve", "Eucalyptus", "a lion", "oxygen"], "metric_results": {"EM": 0.5625, "QA-F1": 0.637109375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.625, 1.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-7591", "mrqa_naturalquestions-validation-2890", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-5804", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-5964", "mrqa_hotpotqa-validation-4926", "mrqa_newsqa-validation-3747", "mrqa_triviaqa-validation-2227"], "SR": 0.5625, "CSR": 0.60078125, "EFR": 0.8214285714285714, "Overall": 0.7111049107142857}, {"timecode": 20, "before_eval_results": {"predictions": ["petroleum", "the Cloth of St Gereon", "Thomas Sowell", "more than 70", "death of a heretic", "choosing their own ministers", "1886", "\"Blue Harvest\" and \"420\"", "Jacob Zuma", "gang rape", "illegal crossings", "10", "Wednesday", "201-262-2800", "different women coping with breast cancer", "well over 1,000 pounds", "security", "Mutassim", "from Texas and Oklahoma to points east", "Polo", "his mother, Katherine Jackson, his three children and undisclosed charities", "in the cellar", "computer problems", "Silvan Shalom", "Climatecare", "Steve Wozniak", "12-hour-plus shifts", "prisoners", "childbirth", "consumer confidence", "5:20 p.m.", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian", "India", "1964", "Davidson", "Pakistan", "Friday", "1979", "the United States", "behind the counter", "chief executive officer", "there's no chance", "set a hearing next week to determine under what conditions they will be settled in the United States", "non-European Union player in Frank Rijkaard's squad.Mexican forward Giovani dos Santos is set to take up the vacant slot alongside Cameroon international Samuel Eto'o and Ivory Coast", "Michael Schumacher", "Hurricane Gustav", "gun", "Henrik Stenson", "asylum", "40", "Derek Mears", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military", "two years", "1966", "winter", "Whitsunday", "Aberdeen", "Dumb and Dumber", "Nokia Sugar Bowl", "Earl Warren", "focal length", "passing of the year", "season five", "Revenge of the Wars ( 2005 )"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6628081553862804}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.4, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.18181818181818182, 0.5, 0.6666666666666666, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.8, 0.125, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.7499999999999999, 0.3333333333333333, 0.22222222222222224]}}, "before_error_ids": ["mrqa_newsqa-validation-565", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-911", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3051", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-1549", "mrqa_naturalquestions-validation-7266", "mrqa_triviaqa-validation-3457", "mrqa_hotpotqa-validation-1094", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-3422"], "SR": 0.515625, "CSR": 0.5967261904761905, "EFR": 0.7741935483870968, "Overall": 0.6854598694316436}, {"timecode": 21, "before_eval_results": {"predictions": ["Cologne", "occupational stress", "Combined Statistical Area", "chief electrician", "Newton", "static friction", "the assassination of US President John F. Kennedy", "responsibility", "Winter Park at Union Station in Denver, Colorado.", "Camorra", "Kenneth Cole", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", ", a North Korean Foreign Ministry spokesman blasted Clinton for what he called a \"spate of vulgar remarks unbecoming for her position everywhere she went since she was sworn in,\"", "\"Maude\"", "ClimateCare, one of Europe's most experienced providers of carbon offset,", "Wednesday", "Cash for Clunkers", "Bobby Jindal", "9:20 p.m. ET Wednesday", "Kim Clijsters", "Mashhad", "Amanda Knox's aunt", "jazz", "$17,000", "Barney Stinson", "Luiz Inacio Lula da Silva", "his father's parenting skills.", "two", "Bill", "J.G. Ballard", "a nurse who tried to treat Jackson's insomnia with natural remedies", "Sarah", "\"They left without me,' which is what I thought I would do.\"", "1981", "17 Again", "Nigeria", "$81,88010", "Republicans", "EU naval force", "Chris Robinson", "Bongo", "a floating National Historic Landmark", "Hyundai Steel", "a pregnancy could pose to her health.", "London Heathrow's Terminal 5.", "canceled the swimming privileges", "February 12", "more than 30", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "India", "Steve Williams", "military strike", "The White House Executive chef", "Russell Huxtable", "Willy Russell", "Budapest", "\"The Expendables 2\" (2012)", "Northumbrian", "\"get thee to a nunnery\"", "a helicopter", "Argentinian", "Mercedes-Benz Superdome", "Eduard Leopold"], "metric_results": {"EM": 0.5, "QA-F1": 0.6291710587206911}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.15384615384615383, 0.5, 0.4, 0.5, 1.0, 0.11764705882352941, 0.5454545454545454, 0.0, 0.0, 0.9, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.1, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.6666666666666666, 0.25, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-2717", "mrqa_squad-validation-7746", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-2036", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-3926", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-1462", "mrqa_newsqa-validation-2221", "mrqa_naturalquestions-validation-613", "mrqa_triviaqa-validation-110", "mrqa_hotpotqa-validation-4514", "mrqa_searchqa-validation-7642", "mrqa_searchqa-validation-8602", "mrqa_hotpotqa-validation-1056"], "SR": 0.5, "CSR": 0.5923295454545454, "EFR": 0.78125, "Overall": 0.6867897727272727}, {"timecode": 22, "before_eval_results": {"predictions": ["X-rays", "WMO Executive Council and UNEP Governing Council", "everyday Germans", "New York and Virginia, especially. The English welcomed the French refugees, providing money from both government and private agencies to aid their relocation.", "two", "glowed even when turned off.", "five female pastors", "a very dark and very cold place.", "sovereignty over them.", "April 6, 1994", "Prague", "backbreaking labor, virtually zero outside recognition, and occasional accusations of being shills for the timber industry rewards.", "a federal judge in Mississippi", "the department has been severely affected by the earthquake, with thousands of officers injured, killed or unaccounted for.", "22 million", "severe flooding", "a music video on his land.", "at the country's third-largest oil refinery,", "\"Slumdog Millionaire\" (No. 4)", "\"The Real Housewives of Atlanta\"", "18", "88", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide", "a president who understands the world today, the future we seek and the change we need.", "the commissions as a legitimate forum for prosecution, while bringing them in line with the rule of law.", "Kase Ng", "Larry King", "Steven Chu", "racially motivated.", "a man who was raised at Camp Lejeune", "homeless veterans and their entire family.", "The torch for the 2010 Vancouver Olympics was lit in a ceremony at the ancient Greek site of Olympia on Thursday, less than four months ahead of the games' opening ceremony.", "Zimbabwe's dire economic situation.", "No. 1 slot", "nine", "ash and rubble", "Friday", "Some truly mind-blowing structures are being planned for the Middle East.", "Rima Fakih", "the two-hour finale.", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Ben Roethlisberger", "one", "the underprivileged.", "Alwin Landry's supply vessel Damon Bankston", "researchers", "involvement during World War II in killings at a Nazi German death camp in Poland.", "opium", "warning -- the FDA's strongest -- to alert patients of possible tendon ruptures and tendonitis.", "84-year-old", "Robert Park", "Rima Fakih", "the Isthmus of Corinth", "Nalini Negi", "( 2017 - 12 - 10 )", "Runcorn", "a bone of the pectoral arch.", "geographic horizon", "UFC 50: The War of '04", "June 11, 1973", "San Diego County Fair", "Toy Story", "Emiliano Zapata", "# Quiz # Question"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5775090468302058}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.07692307692307693, 1.0, 1.0, 0.0, 0.15384615384615383, 1.0, 0.5, 0.19999999999999998, 0.967741935483871, 1.0, 0.6086956521739131, 1.0, 1.0, 0.5714285714285715, 0.5, 0.4444444444444445, 1.0, 0.0, 1.0, 0.923076923076923, 0.9565217391304348, 0.0851063829787234, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1875, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.23529411764705882, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.7499999999999999, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1407", "mrqa_squad-validation-3127", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-2760", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2114", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1418", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-53", "mrqa_triviaqa-validation-3875", "mrqa_triviaqa-validation-7532", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-482", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-4464"], "SR": 0.421875, "CSR": 0.5849184782608696, "EFR": 0.7027027027027027, "Overall": 0.6438105904817861}, {"timecode": 23, "before_eval_results": {"predictions": ["phycoerytherin", "lost in the 5th Avenue laboratory fire of March 1895.", "economic inequality", "Davros", "Church and the Methodist-Christian theological tradition in order to profess their ultimate faith in Christ.", "Museum of the Moving Image in London", "Tulsa, Oklahoma", "56", "Yemen", "2005", "Karen Floyd", "Four Americans", "those missing", "Haiti", "Susan Boyle", "Saturday", "Spain", "Jared Polis", "Janet and La Toya, and brother Randy Jackson", "Hyundai", "30", "Michael Krane,", "lightning strikes", "Evans", "Italian government", "flooding was so fast that the thing flipped over", "threatening messages", "if she would try to travel to Japan for summer vacation.", "Citizens are picking members of the lower house of parliament, which will be tasked with drafting a new constitution after three decades of Mubarak's rule.", "fake his own death", "\"in the interest of justice.\"", "martial arts, which he taught upon his return to India before becoming a male model.", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "then-Sen. Obama", "Congress", "curfew", "Anne Frank, whose account of hiding from Jewish persecution in Nazi-occupied Amsterdam", "once on New Year's Day and once in June, to mark the queen's \"official\" birthday.", "The alleged surviving attacker from last month's Mumbai terror attacks is seeking help from Pakistani officials, India", "Zuma", "haute, bandeau-style little numbers", "five", "Iraq Prime Minister Nouri al-Maliki", "September 11, 2001", "about 50", "a group of teenagers.", "body bags on the roadway near the bus,", "al Fayed", "Desmond Tutu", "$17,000", "Pixar's \"Toy Story\"", "$81,88010", "to provide school districts with federal funds, in the form of competitive grants, to establish innovative educational programs for students with limited English speaking ability", "a transformiation, change of mind, repentance, and atonement", "Jason Lee", "phase of sleep", "noun", "Kent", "beer and soft drinks", "five aerial victories", "Cherokee River", "Snowball", "a former NASA astronaut and a retired captain in the United States Navy,", "Florida"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5936339375493788}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, false, false, false, false, true, true, false, false, true, false, true, false, false, true, true, false, true, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.9333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.56, 0.08333333333333333, 0.5, 0.0, 0.25, 0.15384615384615383, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.058823529411764705, 1.0, 0.0, 1.0, 0.8000000000000002, 1.0, 0.6666666666666666, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8595", "mrqa_squad-validation-10100", "mrqa_squad-validation-7674", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-2969", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3671", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-1721", "mrqa_hotpotqa-validation-162", "mrqa_searchqa-validation-8458", "mrqa_searchqa-validation-10787"], "SR": 0.484375, "CSR": 0.5807291666666667, "EFR": 0.5757575757575758, "Overall": 0.5782433712121213}, {"timecode": 24, "before_eval_results": {"predictions": ["black-and-yellow", "Frederick II the Great", "Muslims in the semu class", "manually suppress the fire", "compound", "Nigeria", "Vonn", "one of the shocks of the year", "him to step down as majority leader.", "The EU naval force", "gang rape of a 15-year-old girl", "ClimateCare, one of Europe's most experienced providers of carbon offset,", "The Louvre", "his club", "will be at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.", "1979", "Heshmat Tehran Attarzadeh", "jazz", "antihistamine and an epinephrine auto-injector", "Bangladesh", "Michael Arrington, founder and former editor of Tech Crunch, and Vivek Wadhwa,", "one out of every 17 children under 3 years old in America", "President Sheikh Sharif Sheikh Ahmed", "Brazil, travels four hours to reach a government-run health facility that provides her with free drug treatment.", "Britain's Got Talent", "military personnel", "behind the counter.", "11 healthy eggs", "one Iraqi soldier", "Michael Partain,", "her fianc\u00e9", "racial intolerance.", "all animal products.", "Vicente Carrillo Leyva, a leader of the Carrillo Fuentes drug cartel,", "Symbionese Liberation Army", "$8.8 million", "work together to stabilize Somalia and cooperate in security and military operations.", "The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza, saying the ad would compromise the public broadcaster's appearance of unbiasedity.", "it -- you know -- black is beautiful", "$104,327,006", "Picasso's muse and mistress, Marie-Therese Walter.", "NATO to do more to stop the Afghan opium trade", "a new GI Bill that expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of unemployment benefits and more than $2 billion in disaster assistance", "off the coast of Dubai", "fallen comrades lost in the heat of battle.", "Washington", "27 Awa", "Mark Obama Ndesandjo", "The premier of \"Dance\" rated highly for Oxygen, with more than 1 million viewers tuning in.", "famous faces", "Steamboat Bill, Jr.", "fatally shooting a limo driver on February 14, 2002.", "nucleus", "the division of Italy into independent states, the restoration of the Bourbon kings of Spain, and the enlargement of the Netherlands to include what in 1830 became modern Belgium", "Sebastian Lund ( Rob Kerkovich )", "President Obama", "Tom Watson", "Sandi Toksvig", "Hispania Racing F1 Team", "Viscount Cranborne", "Walt Disney World Resort in Lake Buena Vista, Florida", "Iceland", "wedlock", "platinum"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6022987776992277}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5714285714285715, 0.18181818181818182, 1.0, 0.4, 0.06896551724137931, 1.0, 0.4, 1.0, 1.0, 1.0, 0.2857142857142857, 0.23529411764705882, 0.33333333333333337, 0.06666666666666667, 1.0, 0.0, 0.8, 0.5, 0.5, 1.0, 0.0, 1.0, 0.8, 0.5, 1.0, 1.0, 0.15384615384615385, 0.23076923076923078, 0.6666666666666666, 1.0, 1.0, 0.3, 0.1142857142857143, 1.0, 0.0, 1.0, 0.4, 0.5, 0.125, 0.0, 1.0, 0.6666666666666666, 0.0, 0.07999999999999999, 0.6666666666666666, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-3304", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-1744", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-4927", "mrqa_hotpotqa-validation-1346", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-8678"], "SR": 0.4375, "CSR": 0.575, "EFR": 0.6666666666666666, "Overall": 0.6208333333333333}, {"timecode": 25, "before_eval_results": {"predictions": ["unity of God", "Treaty of Logstown", "Jordan Norwood", "RNA silencing", "large scale", "Anthony Hopkins", "New Zealand", "Tamar", "rhododendron", "35", "specialist", "beetle", "Arthropoda", "Wayne Allwine", "St Pauls", "holography", "Pelias", "Barry White", "Northumbria", "Harvard", "cricketer", "Seymour Hersh,", "a long pole", "copper and zinc", "Tigris", "Cordelia", "pamphlets, posters, ballads", "dermatitis", "33", "a Rh\u00f4ne Grape Varietal Grown at Tablas Creek Vineyard", "Joseph Smith,", "Huntington Beach, California", "palladium", "moon", "13", "a peplos.", "The Virgin Spring", "Canada", "Clement Attlee", "Stockholm", "Peter Parker", "Goldie Myerson", "Lesa Ukman", "bullfight", "Sparks", "Ginger Rogers", "Plymouth Rock", "Comedy Playhouse,", "citric", "Charles Darwin", "John Denver", "Mr. Boddy", "Marie Van Brittan Brown", "in the late 1930s in southern California,", "In 1995, California was the first state to enact a statewide smoking ban ; throughout the early to mid-2000s", "Bourbon", "Taylor Swift.", "Adam Rex.", "had his personal.40-caliber pistol, and not consistent with long-range rifle fire.\"", "a class to help women \" learn how to dance and feel sexy,\"", "Amy Bishop", "calathus", "the Louvre", "Seaver College"], "metric_results": {"EM": 0.46875, "QA-F1": 0.540312043128655}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 0.125, 0.6666666666666666, 1.0, 0.0, 0.2105263157894737, 0.7777777777777778, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6547", "mrqa_squad-validation-8618", "mrqa_triviaqa-validation-1928", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-2038", "mrqa_triviaqa-validation-147", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6296", "mrqa_triviaqa-validation-7070", "mrqa_triviaqa-validation-3096", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-2160", "mrqa_triviaqa-validation-3082", "mrqa_triviaqa-validation-2301", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-1059", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-7521", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-8908", "mrqa_hotpotqa-validation-649", "mrqa_hotpotqa-validation-1047", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-2288", "mrqa_searchqa-validation-7980", "mrqa_searchqa-validation-2376"], "SR": 0.46875, "CSR": 0.5709134615384616, "EFR": 0.5882352941176471, "Overall": 0.5795743778280543}, {"timecode": 26, "before_eval_results": {"predictions": ["\"The Space Museum\"", "eight", "affordable housing", "Mao Zedong", "Verona", "Pontiac Silverdome", "elephants", "a large party of guests.", "Frank McCourt", "Captain Nemo", "j Judy Cassab", "margo Leadbetter", "Schengen Area", "\u201cA\u201d", "city of Sheffield, England", "Famous Players Film Company", "the Beatles", "england Durrell", "j Jezebel", "Cork", "jason", "Arabian", "Halifax", "Noises Off", "jerry j jerry", "Frank Wilson", "Carlos the Jackal", "Edwina Currie", "st Moritz Winter Olympics in 1928, Gillis Grafstr\u00f6m", "Robert Maxwell", "1768", "For gallantry", "Tuesday", "the Caucasus", "Cahaba", "The Good Life", "Tahrir Square", "uranium", "Count de la F\u00e8re", "27", "Jack Ruby", "tintoretto", "Eric Coates", "Dubai", "Lester", "Thailand", "Sydney", "dove", "Tunisia", "Prince Philip", "Apsley House", "Tokyo", "Edgar Lungu", "49 cents", "heart rate that exceeds the normal resting rate", "672", "Linda McCartney's Life in Photography", "The Frost Place Advanced Seminar", "#notakidanymore", "Juan Martin Del Potro.", "27", "Edgar Allan Poe", "Richard Cory", "Buddhism"], "metric_results": {"EM": 0.5, "QA-F1": 0.5453125}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-2797", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-7031", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-7240", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-6100", "mrqa_triviaqa-validation-2529", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4476", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-4287", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-7193", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3707", "mrqa_triviaqa-validation-7370", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-3354", "mrqa_naturalquestions-validation-10131", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-733", "mrqa_newsqa-validation-279", "mrqa_searchqa-validation-12829"], "SR": 0.5, "CSR": 0.568287037037037, "EFR": 0.8125, "Overall": 0.6903935185185185}, {"timecode": 27, "before_eval_results": {"predictions": ["two", "80", "more than 70", "forced Tesla out leaving him penniless. He even lost control of the patents he had generated since he had assigned them to the company in lieu of stock.", "Benazir Bhutto", "Iran's nuclear program.", "at least 27 Awa", "(l-r) Paul McCartney, Yoko Ono Lennon, Olivia Harrison and Ringo Starr", "FBI Special Agent Daniel Cain", "an acid attack by a spurned suitor.", "Wally", "2008", "after Wood went missing off Catalina Island, near the California coast, following an argument the couple had.", "Rima Fakih", "Afghanistan", "The Everglades, known as the River of Grass,", "made 109 as Sri Lanka, seeking a win to level the series at 1-1, closed on 366 for eight wickets on the opening day.", "1950s", "64", "Iran's parliament speaker", "27", "young self-styled anarchists", "$163 million (180 million Swiss francs)", "unwanted baggage from the 80s and has grown beyond a resort town into something more substantial.", "around 3.5 percent of global greenhouse emissions.", "ensenada, Mexico", "Orbiting Carbon Observatory", "Switzerland", "Robert Redford", "Janet and La Toya", "more than 22 million people in sub-Saharan Africa", "hours", "returning combat veterans", "improve health and beauty.", "U.S. Chamber of Commerce", "it pulls the scab and it cracks, and it starts to bleed.\"", "al-Shabaab", "posting a $1,725 bail", "sustain future exploration of the moon and beyond.", "his business dealings for possible securities violations", "opryland", "Number Ones", "normal maritime traffic", "reports he was diagnosed with skin cancer.", "al Qaeda", "juart gaffney", "\"gotten the balance right\" on Myanmar, the military junta-ruled Asian nation formerly known as Burma, by starting a dialogue while maintaining sanctions,", "oceans", "bit off his ear and a finger", "a nurse who tried to treat Jackson's insomnia with natural remedies", "off the coast of Dubai", "Bill Haas", "Oona Castilla Chaplin", "1932", "1923 and 1925", "gilda", "jarrywell", "table tennis", "Tamil", "DreamWorks Animation", "Indianola", "Empire", "king Kong", "a rising sun"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5684288061815636}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, true, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.375, 0.4, 0.8, 0.28571428571428575, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.11764705882352941, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6, 0.0, 1.0, 0.8, 0.7272727272727273, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1302", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2976", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4072", "mrqa_triviaqa-validation-4193", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-1816", "mrqa_searchqa-validation-9810", "mrqa_searchqa-validation-14496", "mrqa_searchqa-validation-15354"], "SR": 0.453125, "CSR": 0.5641741071428572, "EFR": 0.6571428571428571, "Overall": 0.6106584821428571}, {"timecode": 28, "before_eval_results": {"predictions": ["Bermuda 419 turf", "25-foot (7.6 m)", "symbols", "Hyundai Steel", "Monday night", "Bailey, Colorado", "hopes the journalists and the flight crew will be freed,", "40", "Illuminati", "in a public housing project, not too far from the stadium of her favorite team -- the New York Yankees.", "toxic smoke from burn pits", "one of South Africa's most famous musicians,", "two Israeli soldiers, Ehud \"Udi\" Goldwasser and Eldad Regev.", "space shuttle Discovery", "Gavin de Becker", "a nuclear weapon", "in Japan", "Arizona", "between South America and Africa.", "Tetris", "outside influences", "humanitarian issues", "flipped and landed on its right side", "suppress the memories and to live as normal a life as possible;", "Tuesday", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "the area was sealed off, so they did not know casualty figures.", "knocking the World Cup off the front pages for the first time in days.", "Cash for Clunkers program", "Oregon Fire Lines", "Oprah: A Biography", "80 percent", "London", "to try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "brewer", "$50", "Australian officials", "Hollywood headquarters of Capitol Records", "Dr. Jennifer Arnold and husband Bill Klein,", "gun", "38", "Argentina", "mayor of Seoul from 2002 to 2004,", "Somalia's piracy problem was fueled by environmental and political events", "17 Again", "Kabul", "22", "Steven Gerrard", "12.3 million", "the area was sealed off, so they did not know casualty figures.", "Rima Fakih", "Old Trafford", "help bring creative projects to life", "season two", "Mary Elizabeth Patterson", "mozart's finales", "Fifth", "Nepal", "Merck Sharp & Dohme", "Fort Albany", "Knoxville, Tennessee", "Jawaharlal Nehru", "Transpiration", "hypomanic"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5821837884337884}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, false, false, false, false, true, true, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false, false, false, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 0.9333333333333333, 1.0, 0.0, 0.4, 0.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.5714285714285715, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.9090909090909091, 0.4444444444444445, 0.6666666666666666, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5100", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-2572", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-3536", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-4044", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-1683", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-1265", "mrqa_naturalquestions-validation-10292", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-79", "mrqa_hotpotqa-validation-4763", "mrqa_searchqa-validation-10091", "mrqa_searchqa-validation-5587", "mrqa_searchqa-validation-4465"], "SR": 0.4375, "CSR": 0.5598060344827587, "EFR": 0.7222222222222222, "Overall": 0.6410141283524904}, {"timecode": 29, "before_eval_results": {"predictions": ["Mike Carey", "100% oxygen", "Betty Meggers", "ancient cult activity", "US - grown fruit ( grown by its cooperative members primarily in Polk County, Florida )", "sex organs", "Russian army", "diffuse nebulae", "August 6 and 9, 1945", "Doug Diemoz", "Colony of Virginia", "Monk's Caf\u00e9", "central plains", "al - Mamlakah al - \u02bbArab\u012byah", "Southport, North Carolina", "Iran", "maintenance utility", "July 4, 1776", "pick yourself up and dust yourself off and keep going", "John Garfield as Al Schmid", "enabled European empire expansion into the Americas and trade routes to become established across the Atlantic and Pacific oceans", "1979", "Lorazepam", "2013 non-fiction book of the same name by David Finkel", "salami", "Ethel `` Edy '' Proctor", "who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "Husrev Pasha", "Jodie Sweetin", "ulnar nerve", "McFerrin, Robin Williams, and Bill Irwin", "Watson and Crick", "Gorakhpur", "Patris et Filii et Spiritus Sancti", "the Rashidun Caliphs", "Lake Powell", "ornament", "September 6, 2019", "population", "substitute good", "Veronica", "74", "1987", "cunnilingus", "1999", "New York City", "Mamata Banerjee", "the United States economy first went into an economic recession.", "closing of the atrioventricular valves and semilunar valves", "Hermann Ebbinghaus", "Marvin Gaye", "used their knowledge of Native American languages as a basis to transmit coded messages", "Donny Osmond", "Rome and Carthage", "George Herbert Walker Bush", "Gesellschaft mit beschr\u00e4nkter Haftung", "7.63\u00d725mm Mauser", "seven", "Muslim", "the two remaining crew members from the helicopter,", "Saturday's Hungarian Grand Prix.", "Rickey Henderson", "Lake Baikal", "Harry & Kumar Go to White Castle"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5912926154305465}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5555555555555556, 0.0, 1.0, 0.3076923076923077, 0.9090909090909091, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.7142857142857143, 0.5714285714285715, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.14285714285714288, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.4, 0.9333333333333333, 1.0, 0.0, 0.20689655172413793, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.8, 0.5714285714285715, 1.0, 0.0, 0.2857142857142857]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-10469", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-10459", "mrqa_naturalquestions-validation-4496", "mrqa_triviaqa-validation-5010", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-1733", "mrqa_searchqa-validation-9259", "mrqa_searchqa-validation-5753"], "SR": 0.40625, "CSR": 0.5546875, "EFR": 0.6578947368421053, "Overall": 0.6062911184210527}, {"timecode": 30, "before_eval_results": {"predictions": ["address information", "Pleurobrachia", "1953", "AT&T", "pioneers.", "Chingachgook,", "shoes.", "novem", "Rashid Akmaev,", "acetylene", "'Archer' Jokes", "fiber.", "gray fox", "Whats", "Winston Rodney", "sand", "Nanjing,", "Montana", "a knight", "Louis XIV", "GILBERT & SullIVAN.", "Fox Network", "the Belgae", "Ned Lamont", "the Boston Marathon", "fibreboard", "tin", "a wooden performance", "Frida Kahlo", "his father had boycotted Thomas Jefferson's", "\"Y\" 2 \"K\": An Eskimo...", "Fat Man", "Hair", "William Randolph Hearst", "pumice", "ale", "primate", "dog", "a song performed by English pop punk band Busted.", "a man tasked with hammering a steel drill into rock to make holes for explosives to blast the rock in constructing a railroad tunnel.", "\"The New Colossus\"", "yelped", "Wagner", "a British writer, charity patron, public speaker, film producer and... 1 Early life; 2 Marriage to Prince Andrew; 3 Personal life after divorce", "getting married tomorrow", "a former Major League Baseball (MLB) pitcher.", "bronchodilators", "One", "fluorescent lights.", "Red Lake Indian Reservation", "a Chenard", "Earl Long", "Neil Patrick Harris", "Wyatt and Dylan Walters", "1999", "vitamin D", "five", "Alberto juantorena", "R&B", "Awake", "Doctor of Philosophy", "Pakistan", "Seoul.", "an African-American woman"], "metric_results": {"EM": 0.453125, "QA-F1": 0.48020833333333335}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, false], "QA-F1": [0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.5, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4798", "mrqa_searchqa-validation-10169", "mrqa_searchqa-validation-13591", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-135", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-4041", "mrqa_searchqa-validation-14740", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-1693", "mrqa_searchqa-validation-1994", "mrqa_searchqa-validation-3900", "mrqa_searchqa-validation-13153", "mrqa_searchqa-validation-12259", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-3715", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-3579", "mrqa_searchqa-validation-15750", "mrqa_searchqa-validation-15306", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16940", "mrqa_searchqa-validation-15632", "mrqa_searchqa-validation-3528", "mrqa_naturalquestions-validation-5485", "mrqa_triviaqa-validation-7493", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5297", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-723"], "SR": 0.453125, "CSR": 0.5514112903225806, "EFR": 0.7714285714285715, "Overall": 0.661419930875576}, {"timecode": 31, "before_eval_results": {"predictions": ["non-Mongol physicians", "Prospect Park", "the macula", "the volume", "a crossword clue", "Joseph Conrad", "Diners' Club Card", "Christian Dior", "The Pittsburgh Cycle", "a Montague", "Notre Dame", "Table Mountain", "Tate", "Lt. Bligh", "Cecil Rhodes", "Edinburgh", "Swaziland", "Kevin Spacey", "Manhattan", "Pennsylvania Railroad", "Mike Huckabee", "Queen", "a megrim", "a Board of Governors of the Federal Reserve System", "kozo", "Rex", "William Butler Yeats", "Rachel Carsons", "Vietnam", "Baseball", "William Morris Agency", "Franklin D. Roosevelt", "Kate Middleton", "Georgine Ferrera", "an R", "a white dairy cattle", "New Jersey", "Lake Ontario", "Matt LeBlanc", "The 23-year-old singer", "John Ford", "fortune", "canibalism", "artillery", "aluminum", "General McClellan", "Ned Kelly", "an assemblage", "a gravitational field so strong that nothing, not even light, can escape", "Isis", "a case for arrows", "Heroes", "on the two tablets", "organ transplant of a kidney into a patient with end - stage renal disease", "seven units", "Geheimrat Dr. Max", "Duke Ellington", "Stevie Wonder", "Ludwig van Beethoven", "March 13, 2013", "February 20, 1978", "two years", "Arsene Wenger", "as time goes on, it kind of becomes more and more of a phenomenon.\""], "metric_results": {"EM": 0.375, "QA-F1": 0.4458333333333333}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, false, false, true, false, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-16496", "mrqa_searchqa-validation-12751", "mrqa_searchqa-validation-16751", "mrqa_searchqa-validation-8269", "mrqa_searchqa-validation-11182", "mrqa_searchqa-validation-12766", "mrqa_searchqa-validation-3537", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-10501", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-13455", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-9411", "mrqa_searchqa-validation-10370", "mrqa_searchqa-validation-15436", "mrqa_searchqa-validation-5737", "mrqa_searchqa-validation-2228", "mrqa_searchqa-validation-1175", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-9682", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-1379", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-9799", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-10868", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-10042", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-10526", "mrqa_triviaqa-validation-2878", "mrqa_triviaqa-validation-114", "mrqa_newsqa-validation-2123"], "SR": 0.375, "CSR": 0.5458984375, "EFR": 0.775, "Overall": 0.66044921875}, {"timecode": 32, "before_eval_results": {"predictions": ["increased in weight", "Fresno Street and Thorne Ave", "the Black Death", "Sir Elton", "Henry Sidgwick", "Emperor Norton", "CIA", "piano", "Rickey Henderson", "Jawaharlal Nehru", "the daucus carota subsp. sativus", "John Grunsfeld", "Llados and Llanmad", "1976", "Galileo Descartes", "the neutron", "The Lamentations of Zeno", "Rudy Giuliani", "the Free Speech Clause", "Virginia", "Sif", "Hadrosaurus", "The Omega Man", "a walk-in pantry", "a barrel", "the 1984 Summer Olympics", "Hugo Chvez", "the dollar", "Hinduism", "tin", "Maid Tells of Seeing Jackson", "The Rime of the Ancient Mariner", "pine tar", "the Lincoln Tunnel", "Michael Collins", "Brittany Davenport", "Los Angeles", "the Anemoi", "Richard III", "Labour", "The pen", "Croatia", "Max Landis", "Strindberg", "Hawaii", "Stephen Crane", "Prussia", "Sophocles", "Mark Cuban", "FBI", "a bust", "Central Park", "Alice", "Part 2", "Coconut Cove", "a piano", "trumpet", "Mel Gibson", "2.1 million", "Edward James Olmos", "ZZ Top", "Omar Bongo,", "South Africa", "Ignazio La Russa"], "metric_results": {"EM": 0.5, "QA-F1": 0.5668402777777778}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, true, true, true, true, false, true, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3434", "mrqa_searchqa-validation-4870", "mrqa_searchqa-validation-2479", "mrqa_searchqa-validation-3242", "mrqa_searchqa-validation-10316", "mrqa_searchqa-validation-513", "mrqa_searchqa-validation-8138", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-16331", "mrqa_searchqa-validation-12683", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-6555", "mrqa_searchqa-validation-13862", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-3792", "mrqa_searchqa-validation-11191", "mrqa_searchqa-validation-10213", "mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-6404", "mrqa_searchqa-validation-1487", "mrqa_searchqa-validation-1923", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-5756", "mrqa_searchqa-validation-1405", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1310", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-4767"], "SR": 0.5, "CSR": 0.5445075757575757, "EFR": 0.6875, "Overall": 0.6160037878787878}, {"timecode": 33, "before_eval_results": {"predictions": ["the BBC", "pathogens, an allograft", "a large concrete block is next to his shoulder, with shattered pieces of it around him. Blood trickles down the road.", "hours", "28", "back at work", "Oxbow,", "201-262-2800", "opium", "he regret describing her as \"wacko.\"", "the annual White House Correspondents' Association dinner Saturday, taking jabs at his administration, his Republican rivals and even himself.", "Hussein's Revolutionary Command Council", "drugs", "the Dalai Lama", "Myanmar", "The station", "Hundreds of women protest child trafficking and shout anti-French slogans", "forgery and flying without a valid license,", "a Little Rock military recruiting center", "Cash for Clunkers", "eco-horror scenarios", "North Korea intends to launch a long-range missile in the near future,", "terrorism", "hardship for terminally ill patients and their caregivers", "different women coping with breast cancer in five vignettes.", "a long-range missile", "Police", "sodium dichromate, an inorganic compound containing a highly toxic form of chromium known as hexavalent chromium.", "Roger Federer", "Miami Beach, Florida", "over 1000 square meters", "CNN", "no chance", "a children's hospital in St. Louis, Missouri.", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "two years ago", "two", "a very beautifully painted ruff of Italian lacework", "Symbionese Liberation Army", "he acted in self defense in punching businessman Marcus McGhee.", "two tickets to Italy on Expedia.", "Colombia", "in-cabin lighting system", "resources", "1981", "Los Angeles", "16", "Pope Benedict XVI", "Sri Lanka, seeking a win to level the series at 1-1, closed on 366 for eight wickets on the opening day.", "Appathurai", "$40 and a loaf of bread", "Kgalema Motlanthe,", "the Ming dynasty", "George II", "2014 -- 15", "November 5, 2013", "Javier Bardem", "Scotland", "Erika Girardi", "Terry the Tomboy", "Araminta Ross", "Mrs. Potts", "Spokescandy", "the Star-Spangled Banner"], "metric_results": {"EM": 0.625, "QA-F1": 0.720233718487395}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.09999999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7058823529411765, 0.4, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5333333333333333, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-533", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-438", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-1379", "mrqa_triviaqa-validation-6451", "mrqa_hotpotqa-validation-145", "mrqa_searchqa-validation-6616", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-3588"], "SR": 0.625, "CSR": 0.546875, "EFR": 0.75, "Overall": 0.6484375}, {"timecode": 34, "before_eval_results": {"predictions": ["3", "the Koori", "anti- strike", "Washington State's decommissioned Hanford nuclear site,", "Yemen", "bankruptcy", "nearly $2 billion in stimulus funds", "a businessman, team owner, radio-show host and author.", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Spaniard Carlos Moya", "Bahrain", "children of street cleaners and firefighters.", "Rivers", "$3 billion", "hardship for terminally ill patients and their caregivers", "Honduras", "Brazil", "environmental", "strife in Somalia", "Roy", "WBO welterweight title from Miguel Cotto", "relatives of the five suspects,", "Meredith Kercher.", "former U.S. soldier Steven Green exhibited clear symptoms of acute stress disorder in Iraq and that a military psychiatric nurse-practitioner failed to diagnose the troubled infantryman and pull him out of combat.", "Aniston, Demi Moore and Alicia Keys", "work together to stabilize Somalia and cooperate in security and military operations.", "Friday", "cancerous tumors.", "20", "Matthew Fisher", "$1.5 million", "Tim Clark, Matt Kuchar and Bubba Watson", "40", "a model of sustainability", "glamour and hedonism", "J. Crew", "Department of Homeland Security Secretary Janet Napolitano", "543", "The patient, who prefers to be anonymous,", "Robert Gates", "Israel", "rural Tennessee", "in critical condition", "Seoul,", "Nicole", "assess a school test score of 98 with a \"What about those other two points?\"", "next week", "Adam Lambert", "inspectors", "early detection and helping other women cope with the disease.", "Her husband and attorney, James Whitehouse,", "hopes the journalists and the flight crew will be freed,", "gentry", "Lionel Hardcastle", "Stephen Lang", "Dick Van Dyke", "Noreg", "Beer", "Revengers Tragedy", "1754", "Black Elk", "The Hogan Family", "the hippopotamus", "St Paul"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6123079790779145}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, true, false, false, true, false, true, true, false, false, false, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.9333333333333333, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.18181818181818182, 1.0, 0.15384615384615385, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.06896551724137931, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-2445", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2915", "mrqa_newsqa-validation-923", "mrqa_naturalquestions-validation-5809", "mrqa_naturalquestions-validation-2761", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-2276", "mrqa_searchqa-validation-7879"], "SR": 0.515625, "CSR": 0.5459821428571429, "EFR": 0.6451612903225806, "Overall": 0.5955717165898617}, {"timecode": 35, "before_eval_results": {"predictions": ["walked to the Surveyor, photographed it, and removed some parts", "Border Reiver", "July 4, 1826", "rum", "Nantucket", "an Islamic leadership position", "leaves", "Malibu", "Sisyphus", "measure of sound", "Australia", "Ayla", "Rudolf Hess", "Cubism", "Gettysburg", "Paul Simon", "horseshoe", "Prospero", "crayon", "the Aegean Sea", "the Battle of the Little Bighorn", "the Shakers", "a bellwether", "permeneutics", "chips", "Boxer", "The Spiderwick Chronicles", "Mabel Harding", "Las Vegas", "choosing actors", "the Rose Bowl", "Norman Rockwell", "long locks", "light tunais", "Napa Valley", "Eurail France-Germany-Spain Pass", "Washington, D.C.", "Atlanta", "klezmer", "Japan", "Grease", "12 men", "Nancy Pelosi", "blogging", "Jupiter", "Sadat", "National Ice Cream Day", "Mary Shelley", "50 million cells per litre (quart)", "Volitan Lionfish", "Charlie Sheen", "(the three sons all have the middle)", "Bonnie Aarons", "5 September 1666", "pop ballad", "Seth", "Lou Gehrig", "meaning and origin", "1949", "Aamir Khan", "My Gorgeous Life", "Argentina has always claimed sovereignty over the islands and invaded them in 1982, prompting a war in which more than 600 Argentinean and 255 British military personnel died.", "High Court Judge Justice Davis", "Cipro, Levaquin, Avelox, Noroxin and Floxin."], "metric_results": {"EM": 0.484375, "QA-F1": 0.5869460978835979}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, false, true, false, true, true, true, false, false, false, true, false, true, true], "QA-F1": [0.5, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.07407407407407407, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4001", "mrqa_searchqa-validation-193", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-2343", "mrqa_searchqa-validation-4034", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-15843", "mrqa_searchqa-validation-1389", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-1935", "mrqa_searchqa-validation-12541", "mrqa_searchqa-validation-306", "mrqa_searchqa-validation-14770", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-5061", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-821", "mrqa_searchqa-validation-16962", "mrqa_searchqa-validation-2511", "mrqa_searchqa-validation-9342", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-7619", "mrqa_searchqa-validation-7041", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-12049", "mrqa_searchqa-validation-12788", "mrqa_naturalquestions-validation-590", "mrqa_triviaqa-validation-7591", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-827", "mrqa_newsqa-validation-3884"], "SR": 0.484375, "CSR": 0.5442708333333333, "EFR": 0.7272727272727273, "Overall": 0.6357717803030303}, {"timecode": 36, "before_eval_results": {"predictions": ["lower-pressure boiler feed water", "Luzon", "a scallop", "nothing gained", "numismatic", "Supernanny", "Atlantic", "Cincinnati", "mosque", "(Buss)", "a single-seat fighter", "dry ice", "Taft", "Entourage", "eels", "Philadelphia", "Museum of Modern Art", "the Unicorn", "John C. Frmont", "Russia", "Barbara STREISAND", "Hermann Hesse", "the Taj Mittal", "English Monarchs", "Carmen", "Margaret Mitchell", "Quasimodo", "music", "Pandarus", "a gloomy landscape", "Burt Reynolds", "Sphinx", "Satchmo", "Saudi Arabia", "American new wave", "Arby's", "coffee", "a chivalry", "Burns", "Hulk", "Winnipeg", "Memphis Belle", "Burkina Faso", "the Central Pacific", "Attorney General", "Icelandic", "an American bison", "NBC", "Piaf", "Ivan IV", "a prologue", "birch", "investor couple", "Jack Gleeson", "Phil Hurtt", "animals", "Massachusetts", "the City of Starachowice", "Fredric March", "2009", "Democratic", "meteorologist", "$104,327,006", "17 Again"], "metric_results": {"EM": 0.625, "QA-F1": 0.6834077380952381}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-15899", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-10220", "mrqa_searchqa-validation-16500", "mrqa_searchqa-validation-12153", "mrqa_searchqa-validation-11632", "mrqa_searchqa-validation-8556", "mrqa_searchqa-validation-15286", "mrqa_searchqa-validation-2262", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-8958", "mrqa_searchqa-validation-9274", "mrqa_searchqa-validation-8503", "mrqa_searchqa-validation-15272", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-8702", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-5571", "mrqa_naturalquestions-validation-2026", "mrqa_triviaqa-validation-3956", "mrqa_newsqa-validation-3951"], "SR": 0.625, "CSR": 0.5464527027027026, "EFR": 0.6666666666666666, "Overall": 0.6065596846846846}, {"timecode": 37, "before_eval_results": {"predictions": ["Liechtenstein", "impressionist", "Kentucky Fried Chicken", "oats", "Deval Patrick", "Ivan the Terrible", "Sally Field", "Eighty-six years ago", "a solid alliance", "pi", "tin", "Lake Maurapas", "a tuxedo", "W", "Marriott International", "a constitutional monarchy", "Canada", "The Secret", "gold", "collagen", "the Soviet Union", "a compound", "the cranes", "a claw", "James Reynolds", "the Gulf of Mexico", "Austin", "axiomatic", "Eva Peron", "Cain", "Edward Asner", "X-Men: The Last Stand", "the Louvre", "(chinook)", "Prison Break", "Mars", "Maine", "a sheep's milk cheese", "Meg March", "Sonnets to Orpheus", "deuce", "Hans", "Peter Bogdanovich", "a #3 hit song", "Pilate", "boat propulsion", "the Quaternary Period", "nolo contendere", "Jr. Walker", "the Czech Republic", "tuna", "NIRA", "John Ernest Crawford", "beta decay", "France", "Priam", "Mariette", "Charles Quinton Murphy", "\"The Disaster Artist\"", "Australian", "sins of the members of the church", "22 million", "\"State of Play\"", "Nelson"], "metric_results": {"EM": 0.5, "QA-F1": 0.584375}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, true, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-9798", "mrqa_searchqa-validation-15864", "mrqa_searchqa-validation-5213", "mrqa_searchqa-validation-12123", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-10268", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-11028", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-12000", "mrqa_searchqa-validation-10441", "mrqa_searchqa-validation-15664", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-1987", "mrqa_searchqa-validation-3594", "mrqa_searchqa-validation-4650", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-477", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-10648", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-14545", "mrqa_searchqa-validation-12168", "mrqa_searchqa-validation-8068", "mrqa_triviaqa-validation-6008", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-900", "mrqa_newsqa-validation-1527", "mrqa_hotpotqa-validation-5774"], "SR": 0.5, "CSR": 0.5452302631578947, "EFR": 0.9375, "Overall": 0.7413651315789473}, {"timecode": 38, "before_eval_results": {"predictions": ["tuition fees", "Holden Caulfield", "Wild Bill Hickok", "Leptospirosis", "a recession", "a mermaid", "Jay Silverheels", "Singapore", "the M1 Abrams", "a drumstick", "a boat", "forgetting Sarah Marshall", "Witness", "Jack the Ripper", "3800", "Rene Auberjonois", "taxonomy", "Spain", "the spinal cord", "\"captain goes down with his ship\"", "William Shakespeare", "comedy", "Mary Poppins", "Casowasco", "Fresh Prince of Bel-Air", "Land of Nod", "watermelon", "\"Don't throw the baby out with the bathwater\"", "a second marriage", "Livin' On A Prayer", "Sherlock Holmes", "licorice", "Marie Antoinette", "Ford", "Marie Curie", "Roger Brooke Taney", "a slope", "Nunavut", "Katamari Damacy", "Bill Murray", "Margaret Thatcher", "The Queen of Spades", "manganese", "forests", "Olympia", "Waylon Jennings", "Lawrence", "Brazil", "British Columbia", "Out of Africa", "a pork scraps and trimmings combined with cornmeal and wheat flour, often buckwheat flour, and spices.", "Oona Castilla Chaplin", "October 6, 2017", "John Cooper Clarke", "the different levels of importance of human psychological and physical needs.", "one", "Norfolk Island", "Wright brothers", "sexual activity", "Sam tick,", "the L'Aquila earthquake,", "voluntary misdemeanor", "\"deep sorrow\" at the death of two women killed in a stampede at one of his events in Angola on Saturday,", "Pygmalion"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5452108739837398}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, false, false, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.9268292682926829, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-16680", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-14938", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-5541", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-16786", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-4413", "mrqa_searchqa-validation-6803", "mrqa_searchqa-validation-4288", "mrqa_searchqa-validation-683", "mrqa_searchqa-validation-11976", "mrqa_searchqa-validation-3394", "mrqa_searchqa-validation-8689", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-9146", "mrqa_searchqa-validation-13348", "mrqa_searchqa-validation-14951", "mrqa_searchqa-validation-11444", "mrqa_searchqa-validation-2282", "mrqa_searchqa-validation-402", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-4013", "mrqa_newsqa-validation-630", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-600"], "SR": 0.484375, "CSR": 0.5436698717948718, "EFR": 0.6363636363636364, "Overall": 0.590016754079254}, {"timecode": 39, "before_eval_results": {"predictions": ["Brazil", "\"Boogie Woogie Bugle Boy\"", "the European Economic Community", "Jack Nicholson", "Glory", "Sweeney Todd", "The Bridge on the River Kwai", "the 1,000-year Roman Empire", "Independence", "Jefferson", "Ford Madox Ford", "The Orinoco", "a ready-to-use cotton swab", "Alaska", "Dixie's Land", "RAND Corporation", "Warren Harding", "engrave", "Shue", "Francis Crick", "Jay and Silent Bob", "Heath", "Abkhazia", "Twelfth Night", "Hawaii", "a key", "Tito", "conformation", "Ratatouille", "circadian rhythms", "Calvin Coolidge", "Mark Cuban", "Rudy Giuliani", "eyes", "Tony Dungy", "the Danube", "Andrew Johnson", "26.2", "Prince", "a flowering plant", "chess", "GIGO", "Johannes Brahms", "Charleston Southern", "Italian", "The Grapes of Wrath", "a bicentennial exposition", "Byzantines", "Mayo", "Led Zeppelin", "a Tesla coil", "Denmark", "Anna Murphy", "March 15, 1945", "Charles Darwin", "Old Trafford", "Miles Morales", "Honey Irani", "global peace", "Kalahari Desert", "Graham", "Bob Dole", "Ben Kingsley", "managing his time"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5420386904761905}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, false, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-459", "mrqa_searchqa-validation-3741", "mrqa_searchqa-validation-6543", "mrqa_searchqa-validation-6991", "mrqa_searchqa-validation-8782", "mrqa_searchqa-validation-6190", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-7830", "mrqa_searchqa-validation-14076", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-15394", "mrqa_searchqa-validation-2211", "mrqa_searchqa-validation-11808", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-9351", "mrqa_searchqa-validation-1130", "mrqa_searchqa-validation-5025", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-5754", "mrqa_searchqa-validation-15687", "mrqa_searchqa-validation-2876", "mrqa_searchqa-validation-7544", "mrqa_searchqa-validation-10941", "mrqa_searchqa-validation-11314", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-6266", "mrqa_triviaqa-validation-6323", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-4134", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-4073"], "SR": 0.46875, "CSR": 0.541796875, "EFR": 0.7647058823529411, "Overall": 0.6532513786764705}, {"timecode": 40, "before_eval_results": {"predictions": ["63", "Baden-W\u00fcrttemberg", "James Weldon Johnson", "horror", "Oakdale", "Missouri", "the FAI Junior Cup", "Flaw", "alt-right", "The Drudge Report", "15,000 people", "Yellow fever", "an all-female a cappella singing group", "1934", "13\u20133", "\"The Andy Williams Christmas Album\"", "Tsavo East National Park", "New York Islanders", "Algirdas", "nearly 80 years", "Jean Acker", "the Championship", "The Gettysburg Address", "most awarded female act of all-time.", "Premier League club Manchester United", "The Rite of Spring", "1", "26,000", "Kristin Scott Thomas", "Mayor Ed Lee", "1958", "1993", "American burlesque", "Afro-Russian", "Loretta Lynn", "England", "a Boeing B-17 Flying Fortress", "1 December 1948", "11", "the 2007 Summer Universiade", "2012", "1994", "Kansas City", "1995", "Pinellas County", "beer", "London", "a prototype of the B-17 Flying Fortress bomber", "Mindy Kaling", "1988", "Leonard Cohen", "Erika Mitchell Leonard", "Mase Dinehart", "Tevye", "Sir Tom Finney", "Cameroon", "obtaining and proper handling of human blood", "toxic smoke from burn pits", "two", "Iggy Pop invented punk rock.", "a lawyer", "a man", "Quizlet", "a pathological ex-lover"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6619383169934641}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.0, 0.35294117647058826, 0.4, 0.0, 1.0, 0.0, 0.22222222222222224, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-1786", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-2880", "mrqa_hotpotqa-validation-4472", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-91", "mrqa_hotpotqa-validation-2151", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-3523", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-3552", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1030", "mrqa_searchqa-validation-4643", "mrqa_searchqa-validation-16547", "mrqa_searchqa-validation-13997", "mrqa_naturalquestions-validation-6326"], "SR": 0.5625, "CSR": 0.5423018292682926, "EFR": 0.9285714285714286, "Overall": 0.7354366289198606}, {"timecode": 41, "before_eval_results": {"predictions": ["a punt", "10", "a red minivan ran a red light and struck two vehicles at an intersection,", "Les Bleus", "2005", "more than 4,000", "Specter", "an angry mob.", "normal maritime traffic", "Sri Lanka", "her fetus were found beneath in a fire pit January 11 in Marine Cpl. Cesar Laurean's backyard.", "an average of 25 percent", "fatally shooting a limo driver", "Al Nisr Al Saudi", "as many as 50,000", "piano", "$250,000", "a \"prostitute\"", "the mammoth's skull", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military.", "Los Ticos", "acute stress disorder", "Russia", "Twitter", "through a facility in Salt Lake City, Utah", "Manmohan Singh's Congress party", "Haiti", "Tuesday afternoon", "militants", "23 years.", "a head injury.", "Tim Cahill", "an open window that fits neatly around him", "Leo Frank", "(l-r) Paul McCartney, Yoko Ono Lennon, Olivia Harrison and Ringo Starr", "it has witnessed only normal maritime traffic around Haiti, and it has not intercepted any Haitians attempting illegal crossings into U.S. waters.", "President Robert Mugabe", "don't have to visit laundromats", "three", "United Kingdom Dance Championships.", "on-loan David Beckham claimed his first goal in Italian football.", "\"He is more American than German.\"", "\"Twilight\"", "forgery and flying without a valid license", "11", "third beluga whale belonging to the world's largest aquarium has died", "Fayetteville, North Carolina,", "Indonesia military transport plane crashed into a residential area in East Java early Wednesday, killing 98 people, military officials said.", "Taliban", "Secretary of State Hillary Clinton", "Rihanna", "angular rotation", "from the right side of the heart to the lungs", "54 Mbit / s", "House of Lords", "the Liberator", "cereal", "Oakdale", "Melbourne", "David S. Goyer", "stocks", "Monty Python and the Holy Grail", "Sweden", "Department of Transportation"], "metric_results": {"EM": 0.484375, "QA-F1": 0.639202535839764}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, true, false, false, true, true, false, true, true, true, false, false, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.8, 1.0, 0.0, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 0.0, 0.8, 0.5, 0.0, 0.7272727272727273, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.625, 0.08695652173913045, 1.0, 0.625, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.2857142857142857, 1.0, 0.0, 0.8571428571428571, 0.11428571428571428, 1.0, 0.5714285714285715, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-48", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-3461", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-1659", "mrqa_naturalquestions-validation-5552", "mrqa_triviaqa-validation-376", "mrqa_hotpotqa-validation-1275"], "SR": 0.484375, "CSR": 0.5409226190476191, "EFR": 0.7878787878787878, "Overall": 0.6644007034632035}, {"timecode": 42, "before_eval_results": {"predictions": ["Accountants", "Chinese", "Zimbabwe", "Italian Serie A title", "Darrel Mohler", "her dancing against a stripper's pole.", "Michoacan Family,", "WTA Tour titles", "Morgan Tsvangirai.", "42", "takes on the swords of the Taliban.", "If huge hunks of ice -- such as parts of Greenland and the western shelf of Antarctica -- melt, then the rise is expected to be more dramatic.\"", "80 percent", "1979", "\"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "Elena Kagan", "CNN's Campbell Brown", "an auxiliary lock", "1-1", "AbdulMutallab", "Myanmar", "Collier County Sheriff Kevin Rambosk", "Marcus Schrenker,", "Philippine National Police.", "poems", "Channel 4 said the program was made with the parents' full consent.", "(the Democratic VP candidate)", "The Red Cross, UNHCR and UNICEF", "Russia", "debris", "Steven Gerrard was cleared by a court in Liverpool of affray.", "capital murder and three counts of attempted murder", "Basel", "17", "Daytime Emmy Lifetime Achievement Award", "state senators", "31 meters (102 feet) long and 15 meters (49 feet) wide", "its nude beaches.", "how preachy and awkward cancer movies can get.", "Florida girl who disappeared in February, plans to file for divorce from the girl's stepmother,", "shark River Park in Monmouth County", "three out of four", "Islamabad", "partying", "Capitol Hill,", "\"theoretically\" Iran could develop a nuclear weapon", "1940's", "March 22,", "i report form", "at a depth of about 1,300 meters in the Mediterranean Sea.", "Antichrist", "a major fall in stock prices", "Thomas Jefferson", "Jeff East", "Orion", "brown", "Selfie", "2002", "England", "Los Alamos National Laboratory", "the Rat", "rain", "Crawford", "Pyrenees"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6860437025188864}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, false, false, true, false, true, false, true, true, false, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.5, 0.11764705882352941, 0.0, 0.5714285714285715, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8235294117647058, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.4, 0.25, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.12500000000000003, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-495", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-1683", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-1772", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2", "mrqa_naturalquestions-validation-1799", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-920"], "SR": 0.578125, "CSR": 0.5417877906976745, "EFR": 0.7037037037037037, "Overall": 0.6227457472006891}, {"timecode": 43, "before_eval_results": {"predictions": ["north,", "legitimacy of that race.", "At least 88", "North Korea intends to launch a long-range missile in the near future,", "Kurt Cobain", "American Civil Liberties Union", "33-year-old", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "hardship for terminally ill patients and their caregivers,", "Araceli Valencia,", "Zac Efron", "finance", "nearly $2 billion", "The National Infrastructure Program, as he called it,", "1941", "The station", "an MBA in finance but appeared to have been unemployed for several months and had worked for major accounting firms, such as Price Waterhouse.", "a man's lifeless, naked body", "Robert Mugabe", "the wife of Gov. Mark Sanford,", "Afghanistan's Helmand province,", "Saturday.", "$1.5 million", "a violent government crackdown seeped out.", "Iran could be secretly working on a nuclear weapon", "the fact that the teens were charged as adults.", "death squad killings carried out during his rule in the 1990s.", "Elena Kagan", "Hyundai Steel's Dangjin plant,", "100 percent", "Saturday", "Afghanistan,", "Brad Blauser, center,", "seven", "200", "Pakistan", "Seminole Tribe", "a Muslim with Lebanese heritage,", "South Africa", "Barack Obama", "helicopters and unmanned aerial vehicles from the White House to patrol the border region with Mexico.", "Secretary of State Hillary Clinton,", "maintain an \"aesthetic environment\" and ensure public safety,", "165-room", "second", "Jund Ansar Allah", "1,500", "\"Most of those who managed to survive the incident hid in a boiler room and storage closets", "$50", "$60 billion on America's infrastructure.", "ALS6,", "Malayalam", "Mad - Eye Moody and Hedwig", "1960 Summer Olympics in Rome", "Villa Park", "peasants, small and medium-size farmers, landless people, women farmers, indigenous people, migrants and agricultural workers", "pool", "1822", "The Dressmaker", "Trilochanapala", "crote", "the buffalo", "\"The Wizard of Oz\"", "the frontal lobe"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5863672854010025}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, false, true, true, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.08333333333333333, 1.0, 0.9523809523809523, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5263157894736842, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.7499999999999999, 0.33333333333333337, 1.0, 0.125, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-1975", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-8741", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-4307", "mrqa_hotpotqa-validation-2278", "mrqa_searchqa-validation-11223", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-2281"], "SR": 0.484375, "CSR": 0.5404829545454546, "EFR": 0.8181818181818182, "Overall": 0.6793323863636365}, {"timecode": 44, "before_eval_results": {"predictions": ["Bermuda 419 turf", "Los Angeles", "Christopher Livingstone \" Chris\" Eubank Jr.", "Duval County", "Benj Pasek and Justin Paul,", "Andes", "1952", "Angola", "19th", "January 28, 2016", "Araminta Ross", "Roger Thomas Staubach", "1944", "Johns Creek", "Franconia, New Hampshire,", "Operation Watchtower", "Dan Crow", "War & Peace", "Amberley Village", "What Are Little Boys Made Of?", "Berea College", "the United Football League", "Luca Guadagnino", "Liesl", "Germany and other parts of Central Europe,", "New York Islanders", "Todd Phillips", "26,788", "the Troubles", "1967", "Marktown", "jus sanguinis", "Radcliffe College", "Charles Guiteau", "Ford Motor Company", "If the citizen's heart was heavier than a feather", "India", "German", "armed", "25 million", "\"The Snowman\"", "Ella Jane Fitzgerald", "Chris Claremont", "Rain Man", "Interscope Records", "Robert Grosvenor", "4,000", "\"the most influential private citizen in the America of his day\"", "I'm Shipping Up to Boston", "American", "British singer and \"Britain's Got Talent\" winner Jai McDowall.", "China", "Australia's capital is Canberra, and its largest urban area is Sydney", "the first to develop lethal injection as a method of execution", "Nicola Adams", "\"bay of geese,\"", "Russia", "dependable Camry", "Steven Green", "in a hotel,", "Chaucer", "rattlesnakes", "the One Ring", "healthy"], "metric_results": {"EM": 0.59375, "QA-F1": 0.645809659090909}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-357", "mrqa_hotpotqa-validation-1058", "mrqa_hotpotqa-validation-1815", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-797", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3658", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-5", "mrqa_hotpotqa-validation-4828", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-1433", "mrqa_triviaqa-validation-3532", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2515", "mrqa_searchqa-validation-12418", "mrqa_searchqa-validation-13986", "mrqa_searchqa-validation-4414"], "SR": 0.59375, "CSR": 0.5416666666666667, "EFR": 0.7307692307692307, "Overall": 0.6362179487179487}, {"timecode": 45, "before_eval_results": {"predictions": ["Kelvin Benjamin", "murder in the beating death of a company boss who fired them.", "Indian Ocean waters", "30", "crocodile eggs", "Colorado prosecutor", "Jared Polis", "the annual White House Correspondents' Association dinner Saturday,", "in Haiti", "in July for A Country Christmas, and the festivities run from mid-November until the holidays end.", "trail the illegal traffic.", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "Herman Cain", "\"17 Again,\"", "Kim Jong Il seems to be \"testing the new administration.\"", "Wigan Athletic", "Mitt Romney", "two years ago.", "businessman", "Picasso's muse and mistress, Marie-Therese Walter.", "low-calorie meals", "Heshmatollah Attarzadeh", "the ireport form", "the government", "Nine out of 10 children with HIV in the world live in the region,", "Phoenix, Arizona, police", "Sen. Joe Lieberman, I-Connecticut,", "a crocodile", "a bronze medal in the women's figure skating final,", "killed at least 63 people and wounded more than 200.", "Congress", "Susan Boyle", "\"Common Access Cards,\"", "Phillip A. Myers.", "Obama's", "Gyanendra, 60,", "homicide by undetermined means,", "Casey Anthony, 22,", "officers at a Texas  airport", "10 municipal police officers", "UNICEF", "surrogate", "228", "Kerstin and two of her brothers, ages 18 and 5,", "since 2004.", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "Joan Rivers", "supermodel and philanthropist", "Jacob Zuma", "Oaxaca, Mexico", "Arsene Wenger", "secession", "Kat ( Jessie Wallace ), Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan ), and grandmother Mo ( Laila Morse )", "Latin liberalia studia", "Enid Blyton", "Johnny Mathis", "Eddie Murphy Raw (both 1987)", "Champion Jockey", "Luca Guadagnino", "Ms. Jackson", "unknown", "how timing shapes and supports brain function", "a bar jigger", "a Bristol Box Kite"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5726853485996288}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.6666666666666666, 0.14285714285714288, 0.0, 0.47619047619047616, 1.0, 1.0, 0.10526315789473685, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5882352941176471, 0.5, 0.0, 0.5, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.6666666666666666, 0.7499999999999999, 1.0, 0.5, 1.0, 0.25, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-4013", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-134", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-2877", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-994", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-319", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-1360", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-4", "mrqa_hotpotqa-validation-5640", "mrqa_searchqa-validation-239", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-2431", "mrqa_triviaqa-validation-7461"], "SR": 0.421875, "CSR": 0.5390625, "EFR": 0.8108108108108109, "Overall": 0.6749366554054055}, {"timecode": 46, "before_eval_results": {"predictions": ["acular", "bipartisan", "Nirvana", "phone calls or by text messaging,", "Orange County District Attorney's Office.", "12.3 million", "Mexico", "United", "Vivek Wadhwa,", "Brett Cummins,", "Indian army troopers, including one officer, and 17 militants,", "Saturday", "Nicole", "legitimacy of that race.", "the diversity the collaborations provide,", "Dennis Davern,", "Africa", "American", "bartering -- trading goods and services without exchanging money", "Wednesday.", "improve health and beauty.", "Chinese", "Newcastle retained fourth place with a 3-1 victory", "Nothing But Love", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry", "June 6, 1944,", "Iran,", "twice", "October 19", "\"It was a wrong thing to say, something that we both acknowledge,\"", "Seoul,", "promotes fuel economy and safety while boosted the economy.", "ALS6", "eight", "Siri", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "246", "Grayback Forestry in Medford, Oregon,", "children of street cleaners and firefighters.", "North Korea intends to launch a long-range missile in the near future,", "the area was sealed off, so they did not know casualty figures.", "but they're not going anyplace we wouldn't expect them to,\"", "The American Civil Liberties Union", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.\"", "38", "Her husband and attorney, James Whitehouse,", "whites", "Palestinian", "the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "cancer,", "two", "Arnold Schoenberg", "Brooklyn, New York", "Jean Fernel ( 1497 -- 1558 )", "Discworld", "Japan", "fox hunting", "New York", "travel", "16,116", "\"Cry-Baby\"", "sukkar", "bumblebee", "Rowan Blanchard"], "metric_results": {"EM": 0.578125, "QA-F1": 0.629666990995116}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285714, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.625, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 0.04761904761904762, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.15384615384615388, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-89", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1764", "mrqa_naturalquestions-validation-5769", "mrqa_hotpotqa-validation-2280", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-11573"], "SR": 0.578125, "CSR": 0.5398936170212766, "EFR": 0.7777777777777778, "Overall": 0.6588356973995272}, {"timecode": 47, "before_eval_results": {"predictions": ["Corendon Dutch Airlines", "A Rush of Blood to the Head", "5", "Chicago", "The Ones Who Walk Away from Omelas", "child actor", "Republican", "drawing the name out of a hat", "Chris DeStefano", "Indian Super League", "two or three", "Badfinger", "Lady Frederick Windsor", "Point", "1853", "1983", "Citizens for a Sound Economy", "2027 Fairmount Avenue", "1946 and 1947", "5,112", "1992", "artists' lofts and art galleries, but is now better known for its variety of shops ranging from stylish upscale boutiques to national and international chain store outlets.", "14,673", "6'5\" and 190 pounds", "Mickey Gilley", "Swiss federal popular initiative \"against mass immigration\"", "German shepherd", "Mexican", "December 24, 1973", "1933", "Tremont, Maine", "Ulver and the Troms\u00f8 Chamber Orchestra", "1730", "London Luton Airport", "the Salzburg Festival", "Mississippi", "Afghanistan", "1991\u201392", "Imelda Marcos", "Randall Boggs", "Messiah Part II", "Battle of Bunker Hill", "lion", "Royal", "World War II", "Knoxville, Tennessee", "Three's Company", "P.O.S,", "Labour", "Linda McCartney's Life in Photography", "Erich Maria Remarque", "September 14, 2008", "79", "Ted '' Levine", "Romania", "Zephyr, Billy Cobham, Alphonse Mouzon, the James Gang, Deep Purple, and Moxy.", "Mt Kenya", "Aung San Suu Kyi", "Afghan National Security Forces at the site.", "Her husband and attorney, James Whitehouse,", "Cairo", "Secretariat", "cloakroom", "Lehman Bros International (Europe)"], "metric_results": {"EM": 0.546875, "QA-F1": 0.680251555197676}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, false, true, false], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.13793103448275862, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.5, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-802", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-621", "mrqa_hotpotqa-validation-5691", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-4435", "mrqa_hotpotqa-validation-4960", "mrqa_hotpotqa-validation-1017", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-1069", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-1931", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-5531", "mrqa_naturalquestions-validation-4043", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-5309", "mrqa_newsqa-validation-1795", "mrqa_searchqa-validation-6735", "mrqa_triviaqa-validation-2701"], "SR": 0.546875, "CSR": 0.5400390625, "EFR": 0.8620689655172413, "Overall": 0.7010540140086207}, {"timecode": 48, "before_eval_results": {"predictions": ["spruce", "St Petersburg", "ryegrass", "offensive", "Vulcan", "mating", "Fawn Hall", "forearm", "Wanda", "Barnum", "Peter John", "cathode", "a torque screw", "gold", "Marlon Brando", "Middle German", "\"Impressionists\"", "Kentucky", "a reddish color", "Brussels", "Macbeth", "General Lee", "piracy", "Fyodor Dostoevsky", "Martin Luther", "Clue", "Poe", "Norway", "Douglas", "seven", "Mike Connors", "Jungle Jim", "Jim Inhofe", "sancire", "Corpus Christi", "South Africa", "an ostrich", "a \"rigid\" constitution", "night", "mug", "Desperate Housewives", "Galileo Galilei", "Canada", "Andy Sachs", "a pin", "the Grail", "West Virginia", "Thomas Jefferson", "movie house", "Renold", "kritikos", "Khrushchev", "1904", "a young girl", "Bobby Tambling", "ambidextral", "chariot", "Humberside Airport", "more than 265 million", "100 million", "freezing gasoline prices for the rest of the year and lowering natural gas prices by 10 percent.", "a head injury.", "Pope Benedict XVI refused Wednesday to soften the Vatican's ban on condom use", "Charles II"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5070878623188406}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.8, 1.0, 0.5, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.08695652173913045, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12777", "mrqa_searchqa-validation-599", "mrqa_searchqa-validation-8786", "mrqa_searchqa-validation-507", "mrqa_searchqa-validation-15329", "mrqa_searchqa-validation-6241", "mrqa_searchqa-validation-12540", "mrqa_searchqa-validation-6959", "mrqa_searchqa-validation-3406", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-4061", "mrqa_searchqa-validation-5735", "mrqa_searchqa-validation-15736", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-4314", "mrqa_searchqa-validation-3026", "mrqa_searchqa-validation-5649", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-10077", "mrqa_searchqa-validation-7557", "mrqa_searchqa-validation-12071", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-2710", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-10801", "mrqa_searchqa-validation-9942", "mrqa_searchqa-validation-16389", "mrqa_searchqa-validation-1530", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-14589", "mrqa_searchqa-validation-15062", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-2811", "mrqa_hotpotqa-validation-2171", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-1663"], "SR": 0.421875, "CSR": 0.5376275510204082, "EFR": 0.8648648648648649, "Overall": 0.7012462079426365}, {"timecode": 49, "before_eval_results": {"predictions": ["National Security Agency", "Heisman Trophy", "Brandi Chastain", "the Colorado", "Pamela Anderson", "a \"carnaval\"", "Treasure Island", "Pocahontas", "improvisation", "(Whizzer) White", "ukulele", "a spray", "The Verb", "(Frederick) Toho", "Joseph Campbell", "Margaret Mitchell", "Charles Busch", "a draft horse", "Ernest Lawrence", "rodeo", "a fresco", "Nevil Shute", "(Ulysses) Grant", "Jesse Jackson", "Tudor", "Department of Homeland Security", "the Black Sea", "leotard", "Bulworth", "a small intestine", "the mouthpiece", "Cuba", "the Fellowship of the Ring", "Have You Neverbeen Mellow", "repellents", "Manhattan", "130th anniversary", "Leontyne Price", "a composting mixture", "Lauren Hutton", "Christopher Columbus", "Phil Mickelson", "Carrie Bradshaw", "a dang'rous thing", "Bern", "a thawb robe", "Philadelphia", "peanut butter", "Edgar Allan Poe", "soft", "Lex Luthor", "food and clothing", "Schwarzenegger", "Master Christopher Jones", "Hebrew", "Cowdenbeath", "St Moritz", "October", "Drifting", "Ellesmere Port, United Kingdom", "The incident Sunday evening", "three out of four", "poems", "\"Nebo Zovyot\""], "metric_results": {"EM": 0.515625, "QA-F1": 0.5616319444444444}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, false, false, false, false, true, true, false, false, true, false, false, true, true, false, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4444444444444445, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-8249", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-10212", "mrqa_searchqa-validation-10510", "mrqa_searchqa-validation-4813", "mrqa_searchqa-validation-1695", "mrqa_searchqa-validation-1364", "mrqa_searchqa-validation-13989", "mrqa_searchqa-validation-8175", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-5787", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-1897", "mrqa_searchqa-validation-2904", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-7715", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-1028", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-3602", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-1301"], "SR": 0.515625, "CSR": 0.5371874999999999, "EFR": 0.8064516129032258, "Overall": 0.6718195564516128}, {"timecode": 50, "UKR": 0.8046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1046", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-2118", "mrqa_hotpotqa-validation-2280", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2387", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2865", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-323", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-3527", "mrqa_hotpotqa-validation-357", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4370", "mrqa_hotpotqa-validation-4378", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4638", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5103", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5324", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-586", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-91", "mrqa_hotpotqa-validation-97", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1714", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-3641", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7108", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7527", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7930", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9447", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1134", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1631", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-228", "mrqa_newsqa-validation-2283", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-3270", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3483", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-363", "mrqa_newsqa-validation-3646", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3747", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-3874", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3951", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-4083", "mrqa_newsqa-validation-4090", "mrqa_newsqa-validation-4135", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-511", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-712", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-10042", "mrqa_searchqa-validation-10087", "mrqa_searchqa-validation-10175", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-10501", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-10941", "mrqa_searchqa-validation-11328", "mrqa_searchqa-validation-11492", "mrqa_searchqa-validation-11686", "mrqa_searchqa-validation-1175", "mrqa_searchqa-validation-11948", "mrqa_searchqa-validation-1197", "mrqa_searchqa-validation-12123", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-12269", "mrqa_searchqa-validation-12405", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-12748", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-13072", "mrqa_searchqa-validation-13226", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-13458", "mrqa_searchqa-validation-13875", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-13989", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-14624", "mrqa_searchqa-validation-14703", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-15062", "mrqa_searchqa-validation-15112", "mrqa_searchqa-validation-15176", "mrqa_searchqa-validation-15186", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-1530", "mrqa_searchqa-validation-15354", "mrqa_searchqa-validation-15436", "mrqa_searchqa-validation-15556", "mrqa_searchqa-validation-16418", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-16666", "mrqa_searchqa-validation-16725", "mrqa_searchqa-validation-16842", "mrqa_searchqa-validation-1695", "mrqa_searchqa-validation-205", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-219", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2279", "mrqa_searchqa-validation-2376", "mrqa_searchqa-validation-239", "mrqa_searchqa-validation-2453", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2689", "mrqa_searchqa-validation-3011", "mrqa_searchqa-validation-306", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-3242", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-3394", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3496", "mrqa_searchqa-validation-386", "mrqa_searchqa-validation-3952", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-4604", "mrqa_searchqa-validation-4643", "mrqa_searchqa-validation-4650", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-5194", "mrqa_searchqa-validation-5318", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-5862", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-5984", "mrqa_searchqa-validation-6162", "mrqa_searchqa-validation-6219", "mrqa_searchqa-validation-6241", "mrqa_searchqa-validation-629", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-6764", "mrqa_searchqa-validation-6959", "mrqa_searchqa-validation-6991", "mrqa_searchqa-validation-7049", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-7377", "mrqa_searchqa-validation-7379", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-7557", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7879", "mrqa_searchqa-validation-8503", "mrqa_searchqa-validation-8505", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-8597", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8786", "mrqa_searchqa-validation-9107", "mrqa_searchqa-validation-9296", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-945", "mrqa_searchqa-validation-9496", "mrqa_searchqa-validation-9810", "mrqa_searchqa-validation-9903", "mrqa_squad-validation-1002", "mrqa_squad-validation-10020", "mrqa_squad-validation-10100", "mrqa_squad-validation-10186", "mrqa_squad-validation-10254", "mrqa_squad-validation-10306", "mrqa_squad-validation-1146", "mrqa_squad-validation-1204", "mrqa_squad-validation-1506", "mrqa_squad-validation-1758", "mrqa_squad-validation-1906", "mrqa_squad-validation-1943", "mrqa_squad-validation-1960", "mrqa_squad-validation-2059", "mrqa_squad-validation-2225", "mrqa_squad-validation-2351", "mrqa_squad-validation-2466", "mrqa_squad-validation-2487", "mrqa_squad-validation-2530", "mrqa_squad-validation-2880", "mrqa_squad-validation-298", "mrqa_squad-validation-3265", "mrqa_squad-validation-3279", "mrqa_squad-validation-3703", "mrqa_squad-validation-3840", "mrqa_squad-validation-4047", "mrqa_squad-validation-4290", "mrqa_squad-validation-4315", "mrqa_squad-validation-4330", "mrqa_squad-validation-4353", "mrqa_squad-validation-4415", "mrqa_squad-validation-4455", "mrqa_squad-validation-4468", "mrqa_squad-validation-4517", "mrqa_squad-validation-4524", "mrqa_squad-validation-4673", "mrqa_squad-validation-4759", "mrqa_squad-validation-4812", "mrqa_squad-validation-4876", "mrqa_squad-validation-4998", "mrqa_squad-validation-5010", "mrqa_squad-validation-5170", "mrqa_squad-validation-549", "mrqa_squad-validation-5568", "mrqa_squad-validation-5581", "mrqa_squad-validation-5643", "mrqa_squad-validation-5812", "mrqa_squad-validation-5917", "mrqa_squad-validation-6106", "mrqa_squad-validation-6176", "mrqa_squad-validation-6218", "mrqa_squad-validation-6282", "mrqa_squad-validation-6547", "mrqa_squad-validation-6645", "mrqa_squad-validation-6694", "mrqa_squad-validation-670", "mrqa_squad-validation-6741", "mrqa_squad-validation-6797", "mrqa_squad-validation-6801", "mrqa_squad-validation-6842", "mrqa_squad-validation-6927", "mrqa_squad-validation-6941", "mrqa_squad-validation-7035", "mrqa_squad-validation-7069", "mrqa_squad-validation-7159", "mrqa_squad-validation-7674", "mrqa_squad-validation-7674", "mrqa_squad-validation-7757", "mrqa_squad-validation-7790", "mrqa_squad-validation-7818", "mrqa_squad-validation-7855", "mrqa_squad-validation-7937", "mrqa_squad-validation-8047", "mrqa_squad-validation-8503", "mrqa_squad-validation-8651", "mrqa_squad-validation-8733", "mrqa_squad-validation-8745", "mrqa_squad-validation-8833", "mrqa_squad-validation-8836", "mrqa_squad-validation-8896", "mrqa_squad-validation-9080", "mrqa_squad-validation-910", "mrqa_squad-validation-9170", "mrqa_squad-validation-9270", "mrqa_squad-validation-9298", "mrqa_squad-validation-9311", "mrqa_squad-validation-9398", "mrqa_squad-validation-940", "mrqa_squad-validation-9411", "mrqa_squad-validation-9543", "mrqa_squad-validation-9726", "mrqa_squad-validation-9752", "mrqa_squad-validation-9815", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1474", "mrqa_triviaqa-validation-1546", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-1729", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-1928", "mrqa_triviaqa-validation-1959", "mrqa_triviaqa-validation-1989", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-3020", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4", "mrqa_triviaqa-validation-42", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-492", "mrqa_triviaqa-validation-5445", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-6451", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6824", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-803", "mrqa_triviaqa-validation-993"], "OKR": 0.7578125, "KG": 0.5109375, "before_eval_results": {"predictions": ["Fatih Ozmen", "Volvo 850", "Skyscraper", "Chevy Stingray", "Norway", "Tom Jones", "VIMN Russia", "Homer Hickam, Jr.", "2015", "Hawaii County", "Robert Downey, Jr.", "Continental AG", "band director", "Germanic", "Anaheim", "Reinhard Heydrich", "Big Ben", "Standard Oil", "\"The Longest Yard\"", "Chiwetel Umeadi Ejiofor", "president of Guggenheim Partners", "19th", "Lady Antebellum", "WikiLeaks", "vice president", "Tottenham Hotspur", "October 2016", "Vixen", "Forbidden Quest", "Rymill Park", "Balloon Street, Manchester", "May 1, 2011", "Santa Fe", "political commentator", "Adelaide Lightning", "Landing Barge", "Lancia-Abarth #037", "Lonely", "254th", "Diamond White", "Ferrara", "created the American Land-Grant universities and colleges", "Indooroopilly Shoppingtown", "2006", "Matt Flynn", "Indian", "hamburgers", "Liverpool", "spaghetti", "Luigi Segre", "United States House of Representatives", "February 9, 2018", "the studies and developments department of the French firm R2E Micral", "Nacio Herb Brown", "Geoff Hurst", "Precambrian", "Iona", "his death cast a shadow over festivities ahead of South Africa's highly- anticipated appearance in the rugby World Cup final with England this weekend.", "accusing her of having a relationship with another person.", "a progressive neurological disease", "Paul Newman", "Puccini", "Steve Martin", "milk and honey"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5051339285714286}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.4, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.2, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.13333333333333333, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5838", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-4691", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4170", "mrqa_hotpotqa-validation-5283", "mrqa_hotpotqa-validation-1509", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-221", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-5125", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-4995", "mrqa_hotpotqa-validation-712", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-2260", "mrqa_hotpotqa-validation-2137", "mrqa_hotpotqa-validation-3352", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-5600", "mrqa_triviaqa-validation-4774", "mrqa_triviaqa-validation-535", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-2317", "mrqa_searchqa-validation-3825", "mrqa_searchqa-validation-13015"], "SR": 0.4375, "CSR": 0.5352328431372548, "EFR": 0.8055555555555556, "Overall": 0.682845179738562}, {"timecode": 51, "before_eval_results": {"predictions": ["1979", "an Indian", "Chiltern Shakespeare Company", "1961", "Stacey Kent", "1970s", "Arthur Freed", "Kalokuokamaile", "Gothic Revival", "Buffalo", "Sam Waterston", "George Timothy Clooney", "January 4, 1976", "237", "11,163", "an album", "its air-cushioned sole", "The White Knights of the Ku Klux Klan", "WikiLeaks", "Nine-card Brag", "Montana State University", "Tool", "Wikimedia Foundation", "Flashback: The Quest for Identity", "ARY Films", "2001", "dementia", "two Grammy awards", "Port of Boston", "Denmark", "Las Vegas", "1961", "Rochdale", "Israeli Declaration of Independence", "1971", "Blue Origin", "Target Corporation", "small forward", "2012", "United States", "Sargent Shriver", "35", "Mark Neary Donohue Jr.", "Peach", "Switzerland", "Richard Price", "Archie Andrews", "George Mikan", "June 11, 1986", "2018\u201319 UEFA Europa League", "Magdalen College", "Lake Powell", "Malvolio", "the Royal Air Force ( RAF )", "Separate Tables", "the Celtic Sea", "a string of obscure words", "near the Somali coast", "Daytime Emmy Lifetime Achievement Award", "last week", "(Joseph Holt)", "hunter sauce", "The Tolkien's", "carbon"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7373461174242424}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.375, 0.5, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-1055", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-2803", "mrqa_naturalquestions-validation-950", "mrqa_triviaqa-validation-3042", "mrqa_newsqa-validation-645", "mrqa_searchqa-validation-6747", "mrqa_searchqa-validation-1741"], "SR": 0.6875, "CSR": 0.5381610576923077, "EFR": 0.7, "Overall": 0.6623197115384615}, {"timecode": 52, "before_eval_results": {"predictions": ["My ntonia", "King Henry VIII", "lead", "the Rose Bowl", "VC-25", "amber", "Denmark", "pups", "Katrina & the Waves", "Nazareth", "freestyle", "DOG'S", "Ustilaginales", "the Stargate", "Lou Reed", "General Stonewall Jackson", "Fennoscandia", "Emma Peel", "canvas", "potted plants", "The X-Files", "Frankie Muniz", "the undersea world", "Huron", "Louis Braille", "kinetic", "Santera", "Starsky and Hutch", "a lighthouse", "quicksand", "The Return of the Native", "AOL", "Pop-Tarts", "Minnesota", "Gatun Lake", "the Cornucopia", "The Criterion Collection", "Ankara", "condensation", "eight", "Uberti Winchester", "Chinese", "The Larry Sanders Show", "The Virgin Spring", "Como agua para chocolate", "Niger", "TGI Fridays", "John Tyler", "Daniel Craig", "humility", "programming", "the Mount Mannen in Norway and at the Isle of Sheppey in England", "A footling breech", "if the concentration of a compound exceeds its solubility", "Lawrence of Arabia", "Bristol", "Gregory v. Helvering", "Pan Am Railways", "Berthold Heinrich K\u00e4mpfert", "1961", "Atlantic Ocean", "consistency, whose naturalness is part of their excitement.", "fake his own death", "the Stockton & Darlington Railway"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5769044757326007}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, false, false, true, false, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false], "QA-F1": [0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.625, 1.0, 0.923076923076923, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.13333333333333333, 0.5, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-7579", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-7806", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-8158", "mrqa_searchqa-validation-5567", "mrqa_searchqa-validation-13812", "mrqa_searchqa-validation-7184", "mrqa_searchqa-validation-540", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-2922", "mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-6879", "mrqa_searchqa-validation-10982", "mrqa_searchqa-validation-9100", "mrqa_searchqa-validation-4716", "mrqa_searchqa-validation-15189", "mrqa_searchqa-validation-10899", "mrqa_searchqa-validation-16650", "mrqa_searchqa-validation-15002", "mrqa_searchqa-validation-4954", "mrqa_searchqa-validation-3189", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-2965", "mrqa_triviaqa-validation-3526", "mrqa_triviaqa-validation-5698", "mrqa_hotpotqa-validation-4336", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-2686", "mrqa_triviaqa-validation-5426"], "SR": 0.484375, "CSR": 0.5371462264150944, "EFR": 0.7575757575757576, "Overall": 0.6736318967981705}, {"timecode": 53, "before_eval_results": {"predictions": ["Michael Rosen and illustrated by Helen Oxenbury", "Neil Young", "The kidnapper tells Shawn to tell `` Abigail '' that he loved her", "to manage the characteristics of the beer's head", "they find cool, dark, and moist areas, such as tree holes or rock crevices, in which to sleep", "the teaching of the ritual skills and moral behavior occurs informally and nonverbally", "birch", "on the microscope's stage by slide clips, slide clamps or a cross-table", "Gary Player", "The all - time record for lowest number of goals scored to be bestowed the award, however, is 18 goals ; this was achieved during the 1997 -- 98 and 1998 -- 99 seasons,", "season two", "in the sequence of pieces of DNA called genes", "to `` help bring creative projects to life ''", "warm and is considered to be the most comfortable climatic conditions of the year", "David Motl", "The Portuguese", "Wisconsin", "September 1972", "2017", "Gustav Bauer", "detritus", "the motion of the continents is linked to seafloor spreading by the theory of plate tectonics", "126", "Brooke Wexler", "John Barry", "1961", "111", "Brazil, Turkey and Uzbekistan", "the dromedary", "13", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "complex", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "the Coriolis force", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "James Rodr\u00edguez", "Donald Sutherland", "James Madison", "The location of the Super Bowl is chosen by the NFL well in advance, usually three to five years before the game", "Jethalal Gada", "74", "the type of hazard ahead", "in various submucosal membrane sites", "noble gas", "Office of Inspector General", "four distinct levels", "Janie Crawford, an African - American woman in her early forties,", "Justin Timberlake", "1966", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "September 2017", "Hercule Poirot", "Paul Gauguin", "USA Today serving as its megaphone.", "creeks", "Martin Scorsese", "Ian Fleming", "well over 1,000 pounds", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "off the coast of Dubai", "Nunavut", "Chayka", "a robe", "the death of a pregnant soldier"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5485412431136115}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, false, true, false, true, false, false, false, true, false, false, true, false, true, true, false, true, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, false, true], "QA-F1": [0.4444444444444445, 1.0, 0.13333333333333333, 0.7692307692307692, 0.8, 0.6666666666666666, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4615384615384615, 0.5, 0.0, 1.0, 0.0, 1.0, 0.10526315789473684, 0.0, 0.6666666666666666, 0.0, 0.3571428571428571, 1.0, 0.2, 0.5, 0.3636363636363636, 1.0, 0.5, 0.3076923076923077, 0.4444444444444445, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9722", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-9812", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-2498", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-9985", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-4225", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-8329", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-3243", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-5943", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-388", "mrqa_naturalquestions-validation-8483", "mrqa_triviaqa-validation-4748", "mrqa_newsqa-validation-973", "mrqa_searchqa-validation-5130", "mrqa_searchqa-validation-5562", "mrqa_searchqa-validation-8459"], "SR": 0.390625, "CSR": 0.5344328703703703, "EFR": 0.8461538461538461, "Overall": 0.6908048433048434}, {"timecode": 54, "before_eval_results": {"predictions": ["Oona Castilla Chaplin", "Mike Czerwien", "heavenly virtues", "Tex - Mex cuisine is characterized by its heavy use of shredded cheese, meat ( particularly beef and pork ), beans, peppers and spices, in addition to flour tortillas", "George Harrison", "Kanawha Rivers", "1803", "heads of federal executive departments who form the Cabinet of the United States", "3000 BC", "password recovery tool for Microsoft Windows", "Charlotte Thornton", "Western Australia", "Buffalo Bill", "May 3, 2005", "Ellen is restored to life and is married to Bobby", "California, Utah and Arizona", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "1773", "John J. Flanagan", "1988", "elected or appointed by means of a commission ( letters patent ) to keep the peace", "at slightly different times when viewed from different points on Earth", "Jeff East", "Charlene Holt", "December 1, 1969", "over seven years", "Sets heart in mediastinum and limits its motion", "Alex Skuby", "Matt Monro", "12.65 m ( 41.5 ft )", "The management team", "1999", "supervillains", "the courts", "Malvolio", "Beyonc\u00e9", "Arkansas", "the birth centenary of Pandit Jawaharlal Nehru", "island owner `` U.N. Owen '' ( i.e., `` Unknown '' )", "Atlanta", "22", "Helena", "Joseph Sherrard Kearns", "Cyndi Grecco", "Michael Phelps", "Taron Egerton", "S\u00e9rgio Mendes", "Secretary of Homeland Security", "cylinder of glass or plastic that runs along the fiber's length", "embryo", "741 weeks", "Zimbabwe", "London", "Sarah Palin's", "Tampa", "Battle of Prome", "kitty Hawk", "John Lennon and George Harrison", "the ship of violating Chinese and international laws during its patrols,", "beautiful", "Tater Tots", "Yemen", "QED", "Coffeyville, Kansas"], "metric_results": {"EM": 0.5, "QA-F1": 0.5879659250063662}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.058823529411764705, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.4, 0.5, 1.0, 1.0, 1.0, 0.0, 0.060606060606060615, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.25, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8571428571428572, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-8982", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-440", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-7692", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-10598", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-3099", "mrqa_newsqa-validation-3310", "mrqa_newsqa-validation-2827", "mrqa_searchqa-validation-16172", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-8575"], "SR": 0.5, "CSR": 0.5338068181818182, "EFR": 0.78125, "Overall": 0.6776988636363637}, {"timecode": 55, "before_eval_results": {"predictions": ["Barbara Stanwyck", "Matt Monro", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "Aristotle", "reproductive system", "Peter Andrew Beardsley MBE", "USS Chesapeake", "In 1967, Celtic became the first British team to win the competition", "U.S. states of Oregon and Washington", "Northeast Monsoon", "2013", "Hold On", "living and organic material", "annual income of US $11,770", "Neil Young", "the closing of the atrioventricular valves and semilunar valves", "minimum viable product that addresses and solves a problem or need that exists", "London", "Marty J. Walsh", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "Ernest Rutherford", "any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "the sacroiliac joint", "HTTP / 1.1", "Brooklyn, New York", "1 mile ( 1.6 km )", "pop ballad", "1985", "during meiosis", "2007", "Arnold Schoenberg", "notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "Orographic lift", "Hebrew Bible", "Scarlett Johansson", "InterContinental Hotels Group", "Benzodiazepines", "Steve Valentine", "John J. Flanagan", "innermost layer of human skin", "2007", "Her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries ; the producers like to imagine that she was directing an episode", "10,605", "Marcus Rosner", "Sebastian Vettel", "San Antonio", "Sun Harvester", "Eukarya", "depending on the gender of the reigning monarch", "pathology", "Kevin Lima", "Celebrity Big Brother", "Sir Roger Casement", "James Garner", "Boston, Massachusetts", "Robert Jenrick", "Robert Matthew Hurley", "introduce legislation Thursday to improve the military's suicide-prevention programs.", "five", "Deputy ChiefBrend Johnson", "palabra", "Madonna", "Eiffel Tower", "Teddy Riley"], "metric_results": {"EM": 0.5625, "QA-F1": 0.671085565518974}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, false, true, false, true, true, true, false, false, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.9333333333333333, 0.0, 1.0, 1.0, 0.5, 1.0, 0.1111111111111111, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 0.0, 1.0, 0.7878787878787877, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25000000000000006, 0.0, 0.1290322580645161, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-8465", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-4388", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9047", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-1449", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-46", "mrqa_triviaqa-validation-6540", "mrqa_newsqa-validation-104", "mrqa_searchqa-validation-1762", "mrqa_searchqa-validation-14136"], "SR": 0.5625, "CSR": 0.5343191964285714, "EFR": 0.75, "Overall": 0.6715513392857142}, {"timecode": 56, "before_eval_results": {"predictions": ["Gerald Ford", "10 May 1940", "Sumitra", "six 50 minute ( one - hour with advertisements ) episodes", "1877", "1999", "Old Trafford", "Tami Lynn", "sign", "United States", "Max", "April 13, 2018", "Jenna Boyd", "Spencer Treat Clark", "Sedimentary rock", "Theodore Roosevelt", "Nepal", "Dutch navy captain Jurriaen Aernoutsz", "4 September 1936", "oxidant, usually atmospheric oxygen, that produces oxidized, often gaseous products, in a mixture termed as smoke", "1940", "Authority", "April 1st", "Brobee", "The claims process starts at noon Eastern Time and ends 24 hours later", "Francisco Pizarro", "habitat", "Ben Faulks", "Lady Gaga", "mental disorder characterized by at least two weeks of low mood that is present across most situations", "1989", "Liam Cunningham", "Eddie Deezen", "Walter Pauk", "After Margaret Thatcher became Prime Minister in May 1979", "septum", "Buddhism", "foreign exchange market", "`` Singing the Blues '' by Guy Mitchell in 1957", "Sir Ernest Rutherford", "Nigel Lythgoe, Mia Michaels, and Adam Shankman", "December 2, 2013", "gastrocnemius", "Art Carney", "the term `` train of thoughts '' was introduced and elaborated as early as in 1651 by Thomas Hobbes in his Leviathan", "March 26, 1973", "1986", "on location", "President Lyndon Johnson", "prenatal development of the human heart", "a Nativity scene", "1840", "2007", "Branson", "first baseman", "South Plainfield, New Jersey", "River Shiel", "brewer", "Polo", "Terra Firma, which controls EMI, owner of the recording studios.", "ego", "Nova Scotia", "Sir Isaac Newton", "Love Letter"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7003909513366751}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.4210526315789474, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.4444444444444445, 0.0, 0.6666666666666666, 1.0, 0.4166666666666667, 1.0, 1.0, 1.0, 1.0, 0.4000000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-305", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-215", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-1008", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-7486", "mrqa_hotpotqa-validation-3278", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-2096", "mrqa_searchqa-validation-10569"], "SR": 0.609375, "CSR": 0.5356359649122807, "EFR": 0.76, "Overall": 0.6738146929824561}, {"timecode": 57, "before_eval_results": {"predictions": ["France", "Massachusetts", "the one - mile - wide ( 1.6 km ) strait connecting San Francisco Bay and the Pacific Ocean", "a numeric scale used to specify the acidity or basicity of an aqueous solution", "Casino promotions such as complimentary matchplay vouchers or 2 : 1 blackjack payouts allow the player to acquire an advantage without deviating from basic strategy", "the Infamy Speech of US President Franklin D. Roosevelt", "the coffee shop Monk's", "Joe Young", "the original game release", "Ozzie Smith", "Mark Jackson", "2017", "Representative and Delegates serve for two - year terms, while the Resident Commissioner serves for four years", "January 2018", "all land - living organisms, both alive and dead, as well as carbon stored in soils", "September 30", "avian origin", "Gerald Ford", "September 8, 2017", "1998", "political ideology", "Spektor", "an object", "the nucleus", "1946", "October 12, 2017", "Ren\u00e9 Verdon", "changes the relationship of the overall pitch range compared to the range of the instruments or voices that perform the music", "the contestant", "Hebrew \u05de\u05b7\u05dc\u05b0\u05db\u05b8\u05bc\u05dd\u200e Malkam `` great king ''", "P.V. Sindhu", "Carpenter", "Asuka", "126", "Scorpions", "Brazil", "UNESCO / ILO Recommendation concerning the Status of Teachers", "upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "spherical boundary of zero thickness in which photons that move on tangents to that sphere would be trapped in a circular orbit about the black hole", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "Rich Mullins", "prenatal development", "skeletal muscle and the brain", "American country music duo Brooks & Dunn", "Ireland", "Felicity Huffman", "1908", "Sir Henry Cole", "Long Island in the summer of 1922", "Eukarya", "commemorating fealty and filial piety", "Luigi Pirandello", "Russ Conway", "the liver", "Fomento Econ\u00f3mico Mexicano, S.A.B. de C.V.", "Irish Chekhov", "Gust Avrakotos", "\"Jenny Sanford,", "Lance Cpl. Maria Lauterbach", "step up", "1920-30", "Joe Louis", "Richard Cory", "ancient Mayan settlement"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5849303569908112}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 0.4, 0.375, 0.14285714285714288, 0.07692307692307693, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 0.2222222222222222, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.06451612903225806, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.9142857142857143, 0.0, 0.9600000000000001, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.3, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4497", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-2299", "mrqa_hotpotqa-validation-572", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-5291", "mrqa_newsqa-validation-2524", "mrqa_searchqa-validation-6103", "mrqa_searchqa-validation-5902"], "SR": 0.484375, "CSR": 0.5347521551724138, "EFR": 0.5151515151515151, "Overall": 0.6246682340647858}, {"timecode": 58, "before_eval_results": {"predictions": ["William Wyler", "a mid-size four - wheel drive luxury Mercedes -Benz GL - Class", "1986", "Idaho", "October 14, 2017", "Exodus and Deuteronomy", "The acid plays a key role in digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "1979", "the 14th most common surname in Wales and 21st most common in England", "iron", "the Reverse - Flash", "Los Angeles, California", "the thirteen American colonies regarded themselves as a new nation, the United States of America, and were no longer part of the British Empire", "Jaffa Cakes are biscuit - sized cakes introduced by McVitie and Price in the UK in 1927", "the name of a work gang", "Nebuchadnezzar", "Eddie Murphy", "17 - year - old", "between 1923 and 1925", "the brain and spinal cord", "Seattle, Washington", "the story, a part of gothic and ghost story genres, first appeared in serial format in Collier's Weekly magazine ( 27 January -- 16 April 1898 )", "Yosemite National Park", "Gugu Mbatha - Raw", "LED illuminated display", "turkey", "1917", "January 2004", "Anna Faris", "smen", "Mount Sinai", "Macon Blair", "the genome", "by each state's DMV, which is required to drive", "The Convention's first act, on 10 August 1792, was to establish the French First Republic and officially strip the king of all political powers", "four", "a divergent tectonic plate boundary", "Steve Russell", "peace between two entities ( especially between man and God or between two countries ), or to the well - being, welfare or safety of an individual or a group of individuals", "New York University", "into the intermembrane space", "Northeast Monsoon or Retreating Monsoon", "13 February", "291 episodes", "the early 1960s", "Yahya Khan", "Thespis", "France", "Wednesday, 5 September 1666", "March 1995", "Zoe Zebra", "dysmenorrhea", "1960", "Justin Trudeau", "2006", "Walldorf", "superhero roles as the Marvel Comics characters Steve Rogers / Captain America in the Marvel Cinematic Universe and Johnny Storm / Human Torch in \"Fantastic Four\" and.", "pipelines and hostage-taking", "zulasmodics, which are drugs that relax the smooth muscle in the gut and relieve cramping, and found that they were all more effective than a placebo, according to the report in the British Medical Journal.", "FBI", "ferry", "Leland Stanford", "Oaxaca de Jurez", "Nepal"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6313853546850102}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.5, 0.9818181818181818, 0.5, 0.4444444444444444, 0.0, 0.0, 1.0, 0.18181818181818182, 0.125, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.3076923076923077, 0.08695652173913045, 0.0, 0.0, 0.2857142857142857, 0.5, 0.5, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 0.5945945945945945, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3571428571428571, 0.0, 0.046511627906976744, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-7342", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-3760", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-8796", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-3485", "mrqa_triviaqa-validation-3434", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-98", "mrqa_searchqa-validation-10231"], "SR": 0.484375, "CSR": 0.5338983050847458, "EFR": 0.6060606060606061, "Overall": 0.6426792822290703}, {"timecode": 59, "before_eval_results": {"predictions": ["regulatory", "Tevin Campbell", "Ireland", "Vicente Fox", "Daryl Sabara", "February 6, 2005", "Justin Timberlake", "Biotic -- Biotic resources are obtained from the biosphere ( living and organic material ), such as forests and animals, and the materials that can be obtained from them", "IIII", "an expression of at least a moderate amount of manual dexterity", "quarterback", "July 2012", "Audrey II", "Tim Russert", "Jodie Foster", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "by January 2018", "George Strait", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F ) at Vostok Station", "Herman Hollerith", "94 by 50 feet", "transmission, which contains a number of different sets of gears that can be changed to allow a wide range of vehicle speeds, and also in the differential", "Gibraltar", "a chimera ( a mixture of several animals ), who would probably be classified as a carnivore overall", "October 1, 2014", "The Miracles", "provide information about advance health care directives to adult patients upon their admission to the healthcare facility", "Marie Fredriksson", "Long Island", "the earliest known official or large - scale celebration of Pi Day was organized by Larry Shaw at the San Francisco Exploratorium,", "Germany", "Rococo - era France", "Michael Crawford", "Devastator", "the final episode of the series", "2010", "Ram Nath Kovind", "Abid Ali Neemuchwala", "August 3, 1945", "1950s", "Napoleon", "A 30 - something man ( XXXX ), is a London underworld criminal who has established himself as one of the biggest cocaine suppliers in the city, with effective legitimate cover", "the early - to - mid fourth century", "to turn our will and our lives over to the care of God as we understood Him", "De pictura", "January 2, 1971", "J. Presper Eckert and John William Mauchly's ENIAC", "diastema ( plural diastemata )", "July 21, 1861", "the Mayflower", "Efren Manalang Reyes", "Joker Wild", "Chicago", "Dijon", "Lucas Stephen Grabeel", "15,024", "model", "the test results by the medical examiner's office, Garavaglia said.", "15-year-old", "Sunday", "Vietnam", "the cello", "Richard", "Son of Sam"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6192920927945316}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07999999999999999, 0.0, 0.13333333333333333, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8421052631578948, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.6153846153846153, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.07999999999999999, 1.0, 0.27586206896551724, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-9157", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-3032", "mrqa_naturalquestions-validation-1584", "mrqa_naturalquestions-validation-234", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-7165", "mrqa_triviaqa-validation-34", "mrqa_hotpotqa-validation-3979", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-3928", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-8465"], "SR": 0.515625, "CSR": 0.53359375, "EFR": 0.8064516129032258, "Overall": 0.6826965725806451}, {"timecode": 60, "before_eval_results": {"predictions": ["a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "1998", "used their knowledge of Native American languages as a basis to transmit coded messages", "Gilbert building", "Tom Brady", "James Rodr\u00edguez", "a computer maintenance utility included in Microsoft Windows designed to free up disk space on a computer's hard drive", "1837", "writ of certiorari", "silk floss tree", "Ferm\u00edn Francisco de Lasu\u00e9n", "Fats Waller", "coffee, macadamia nuts, pineapple, livestock, sugarcane and honey", "79", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "49 cents", "Jason Lee", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Lorenzo Lamas", "Mahatma Gandhi", "the people of the United States", "eighth", "Erica Rivera", "John Young", "Russia", "2019", "Charles Perrault", "1990", "James `` Jamie '' Dornan", "left coronary artery", "Jane Fonda", "Brazil", "Nicklaus", "1957", "Clare Torry", "ummat al - Islamiyah", "Brazil, Turkey and Uzbekistan", "Parashara ( c. 400 -- c. 500 AD )", "Domhnall Gleeson", "Brazil and Paraguay", "agriculture", "St. John's, Newfoundland and Labrador", "Greek \u0392\u03bf\u03ce\u03c4\u03b7\u03c2, Bo\u014dt\u0113s, meaning `` herdsman '' or `` plowman ''", "plant anatomy", "the bloodstream or surrounding tissue following surgery, disease, or trauma", "1923", "1871 A.D.", "`` Mirror Image ''", "a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "1966", "Eridu is the oldest Sumerian site settled during this period, around 5300 BC, and the city of Ur also first dates to the end of this period", "\"The closest approach to the original sound\"", "Peter Sellers", "New Netherland Institute", "Atlantic", "mistress of the Robes", "Australian Electoral Division", "Conway", "Kurt Cobain", "\"Empire of the Sun,\"", "Stephen Dedalus", "The Killing Fields", "the Endeavour", "News of the World tabloid."], "metric_results": {"EM": 0.515625, "QA-F1": 0.6380732548701299}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, false, true, true, false], "QA-F1": [0.625, 0.0, 0.06666666666666667, 1.0, 0.0, 1.0, 0.9142857142857143, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.33333333333333337, 1.0, 0.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0909090909090909, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.2857142857142857, 1.0, 0.0, 1.0, 0.7272727272727272, 0.6363636363636364, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.05714285714285715]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-9371", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-9085", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-9058", "mrqa_triviaqa-validation-4907", "mrqa_triviaqa-validation-2476", "mrqa_hotpotqa-validation-2900", "mrqa_hotpotqa-validation-3716", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-1963", "mrqa_searchqa-validation-1451", "mrqa_newsqa-validation-1282"], "SR": 0.515625, "CSR": 0.5332991803278688, "EFR": 0.6451612903225806, "Overall": 0.6503795941300898}, {"timecode": 61, "before_eval_results": {"predictions": ["May 26, 2017", "to form a higher alkane", "shared", "Jason Marsden", "New Mexico", "In 1889", "Poems : Series 1", "William the Conqueror", "March 2, 2016", "July 20, 2017", "five", "September 1972", "James Rodr\u00edguez", "the world's sixth - largest country by total area", "The Vamps, McGregor Maynard, Bronnie, Ella Eyre, Sheppard and Louisa Johnson", "Mickey Rourke", "John Donne", "1980s", "David Gahan", "Emma Watson", "the Continental Congress", "2018", "the first bull running is on 7 July, followed by one on each of the following mornings of the festival, beginning every day at 8 am", "2009", "4.25 inches ( 108 mm )", "Judi Dench", "Madhouse", "159", "Chris Rea", "between $10,000 and $30,000", "Kelly Reno", "Ozzie Smith", "8 December 1985", "18th century", "Thomas Jefferson", "Ian Holm", "cat in the hat", "Brad Dourif", "counter clockwise direction", "Joanne Wheatley", "vice president", "the rise of literacy, technological advances in printing, and improved economics of distribution", "Donna Mills", "1994 season", "Matt Flinders", "parthenogenic", "the major contributor and the associated free software philosophy", "the efferent nerves that directly innervate muscles", "1773", "The Union's forces", "American country music duo Brooks & Dunn", "\"Maljanne\"", "South America", "The Pilgrim's Progress", "Bourbon County", "Venancio Flores", "Hungary", "Sen. Barack Obama", "Sri Lanka's Tamil rebels", "Osama bin Laden's sons", "(Jack) London", "Arthur C. Clarke", "the Koran", "whooping cough"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6679673451548451}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, true, true, false, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.24000000000000002, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.07692307692307691, 0.5, 0.7272727272727273, 1.0, 0.0, 1.0, 1.0, 0.32, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10285", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-2618", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7513", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-8452", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-4444", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-3340", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-1069", "mrqa_hotpotqa-validation-1887", "mrqa_newsqa-validation-648", "mrqa_triviaqa-validation-4519"], "SR": 0.5625, "CSR": 0.5337701612903225, "EFR": 0.8214285714285714, "Overall": 0.6857272465437788}, {"timecode": 62, "before_eval_results": {"predictions": ["1952", "beneath the liver", "Rudy Clark", "Abbot Suger", "Yuzuru Hanyu", "Tim Russert", "Byzantine Greek culture and Eastern Christianity became founding influences in the Arab / Muslim world and among the Eastern and Southern Slavic peoples", "toys or doorbell installations", "microfilament", "in positions Arg15 - Ile16 and produces \u03c0 - Chymotrypsin", "the northernmost point on the Earth", "Beyonce Drake", "Eduardo", "M\u00e1ximo Gomez and Antonio Maceo", "1971", "Leo Arnaud ( / \u02c8le\u026a. o\u028a \u0251\u02d0r \u02c8no\u028a / ; July 24, 1904 -- April 26, 1991 )", "Emmanuelle Chriqui", "Carlos Alan Autry Jr. ( also known for a period of time as Carlos Brown ; born July 31, 1952 )", "16 March 2018", "Hollywood, Los Angeles, California", "Merry Clayton", "a bronze statue designed by Thomas Crawford ( 1814 -- 1857 ) that, since 1863, has crowned the dome of the U.S. Capitol building in Washington, D.C.", "judges", "1936", "George Harrison", "Djokovic", "Abraham Gottlob Werner", "1922", "2017", "scythe", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "Leonard Bernstein", "Toronto", "listing the telephone numbers of individuals and families who have requested that telemarketers not contact them", "September 2017", "October 2012", "Kaley Christine Cuoco ( / \u02c8ke\u026ali \u02c8kwo\u028ako\u028a / KAY - lee KWOH - koh", "2013", "Dido", "Isekai wa Sum\u0101tofon to Tomo ni", "the fourth season", "Phillip Paley", "1981", "Jakkur, Bangalore, India", "New Orleans going north through Chicago and to New York", "the port of Nueva Espa\u00f1a to the Spanish coast", "10.5 %", "U.S. Interior Highlands region", "White House Executive chef", "the International Border ( IB )", "Bart Millard", "an informal term for mother 1, wife", "Thabo Mbeki", "Midnight Cowboy", "Austrian", "heavy metal drummer", "Suffolk County, New York, United States", "Muslims", "five minutes before commandos descended from ropes that dangled from helicopters, Zoabi said during a press conference in Nazareth, Israel.", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "Chastity", "the United Nations", "Oshkosh", "65 mi long"], "metric_results": {"EM": 0.5, "QA-F1": 0.6087346681096681}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, false, false, true, false, false, true, false], "QA-F1": [0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.25, 0.0, 0.5, 0.0, 0.5714285714285715, 1.0, 0.2857142857142857, 1.0, 0.38095238095238093, 1.0, 0.7272727272727273, 1.0, 0.16666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.4166666666666667, 1.0, 0.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4954", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-2648", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-2381", "mrqa_naturalquestions-validation-1840", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-8734", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-7211", "mrqa_triviaqa-validation-7273", "mrqa_hotpotqa-validation-529", "mrqa_hotpotqa-validation-5848", "mrqa_newsqa-validation-1307", "mrqa_newsqa-validation-1292", "mrqa_searchqa-validation-14197", "mrqa_searchqa-validation-9194", "mrqa_hotpotqa-validation-1201"], "SR": 0.5, "CSR": 0.533234126984127, "EFR": 0.9375, "Overall": 0.7088343253968254}, {"timecode": 63, "before_eval_results": {"predictions": ["Robyn", "1998", "the closing of the atrioventricular valves and semilunar valves", "the Coppolas and, technically, the Farrow / Previn / Allens", "SI joint", "Identification of alternative plans / policies", "Mexico", "development of electronic computers in the 1950s", "Employers", "Numbers 22 : 22", "Bhupendranath Dutt", "George III's German - born wife, Charlotte of Mecklenburg - Strelitz", "April 13, 2018", "at each place there are a bread roll ( generally on a bread plate, sometimes in the napkin )", "Andrew McCarthy as Blane McDonough", "Jakkur, Bangalore, India", "Five years later", "2001", "the European economy had collapsed", "brothers Henry, Jojo and Ringo Garza", "Ben Findon", "Incudomalleolar joint", "Terry Reid", "an active supporter of the League of Nations", "Kennedy Space Center ( KSC ) in Florida", "supported modern programming practices and enabled business applications to be developed with Flash", "Forbes Burnham", "on Saturday", "Isekai wa Sum\u0101tofon to Tomo ni", "the tsar's Moscow residence", "the court from its members for a three - year term", "Alicia Vikander", "over 300,000", "April 21, 2015", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "115", "eight", "Lori Rom", "a Czech word, robota", "Arthur `` The President '' Flanders", "Cameron Fraser", "Austin and Pflugerville", "three times", "Exodus 20 : 7", "four", "25 -- 30 \u00b0 C / km ( 28 -- 34 \u00b0 F / mi )", "Utah, Arizona, Wyoming, and Oroville, California", "Jack Barry", "Hugo Weaving", "from the heraldic crest carved in the lintel on St. Ignatius'family home in Azpeitia, Spain", "Lana Del Rey", "The Matterhorn", "calypsos", "Jiles Perry (JP) Richardson Jr.", "The Pentagon", "Croatan, Nantahala, and Uwharrie", "Johnnie Ray", "Robert Mugabe", "Capitol Hill,", "provided Syria and Iraq 500 cubic meters of water a second,", "impressionist", "Pussycat Dolls Present", "tuberculosis", "May 4"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5148872471482766}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true], "QA-F1": [0.0, 0.0, 0.9333333333333333, 0.2857142857142857, 0.5, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6153846153846153, 0.3333333333333333, 0.35294117647058826, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.33333333333333337, 0.0, 0.1818181818181818, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.2222222222222222, 0.5714285714285715, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.9523809523809523, 0.5, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5787", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-8460", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-4860", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-462", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-225", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-2806", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-3801", "mrqa_triviaqa-validation-6825", "mrqa_triviaqa-validation-2582", "mrqa_hotpotqa-validation-4240", "mrqa_searchqa-validation-9798", "mrqa_searchqa-validation-8333"], "SR": 0.359375, "CSR": 0.530517578125, "EFR": 0.6829268292682927, "Overall": 0.6573763814786586}, {"timecode": 64, "before_eval_results": {"predictions": ["Agra", "2018\u201319 UEFA Europa League group stage", "FIFA Women's World Cup", "Dan Brandon Bilzerian", "Len Wiseman", "Viglen Ltd", "1896", "Randall Boggs", "Detroit, Michigan", "Roots: The Saga of an American Family", "St. Louis Cardinals", "Foxborough", "local South Australian and Australian produced content", "Hindi", "Ronald Wilson Reagan", "Los Angeles", "Ben Johnston", "Nia Temple Sanchez", "Vanessa Hudgens", "top division", "Amber Laura Heard", "Peter Seamus O'Toole", "March 8, 1942", "co-founder and lead guitarist of the alternative rock band R.E.M.", "January 30, 1930", "A Doctor of Philosophy", "Government of Ireland", "James Weldon Johnson", "Wilmington, North Carolina, United States", "1979", "Taylor Swift", "first and second segment", "Kew Gardens", "7 January 1936", "Towards the Sun", "\"The Braes o' Bowhether\"", "Westminster system", "Ionolyce", "For Love alone", "October 4, 1970", "King of the Polish-Lithuanian Commonwealth", "Sam Waterston", "Transporter 3", "2000", "Gauteng province", "Vietnam War", "Bill Walton", "the Darling River", "Brian Keith Bosworth", "over 140 million", "American", "Teri Garr", "the employer", "1962", "Wyoming", "Wee Jimmy Krankie", "endangerophobia", "ordered the immediate release", "The clothing must be black, red or white,", "trying to prevent attempted defections as the country goes through a tumultuous transition, the report said.", "American alternative rock band", "Ellicott City", "sprints", "Southport, North Carolina"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5962867861305361}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.8, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.8, 0.5, 0.6153846153846153, 1.0, 0.5, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.36363636363636365, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-257", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-5143", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-3311", "mrqa_hotpotqa-validation-5596", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-3299", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-4781", "mrqa_hotpotqa-validation-5297", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3342", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2569", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2442", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-5565", "mrqa_naturalquestions-validation-8444", "mrqa_triviaqa-validation-7411", "mrqa_triviaqa-validation-1171", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2777", "mrqa_searchqa-validation-12166", "mrqa_searchqa-validation-9071", "mrqa_searchqa-validation-16474"], "SR": 0.46875, "CSR": 0.5295673076923078, "EFR": 0.7647058823529411, "Overall": 0.6735421380090498}, {"timecode": 65, "before_eval_results": {"predictions": ["Stephen Lee", "Arrested Development", "Albert", "September 30, 2017", "322,520", "New York Giants", "the Swiss tourism boom", "Eliot Cutler", "the 1946 Winecoff Hotel fire", "Odense Boldklub", "Stephen of Blois", "Scott Eastwood", "gweilo", "Tufts College", "Prince Amedeo, 5th Duke of Aosta", "1936", "The Wu-Tang Clan", "Hey Dad", "a midtempo hip hop ballad with a pop refrain, sung by Rihanna,", "melodic hard rock", "G\u00e9rard Depardieu", "rural", "Las Vegas", "Appleby-in-Westmorland", "from 1345 to 1377", "Indiana University", "James Bond", "Syracuse", "Kings Point, New York", "Robbie Gould", "It's Always Sunny in Philadelphia", "Baldwin", "Port Clinton", "November 20, 1942", "Wayne Conley", "on the Australian coast", "Faith", "a terrible date", "the Cleveland Celtics", "Supernatural", "CHO", "eight", "the regime of Emperor Napoleon III", "Sippin' on Some Syrup", "James Harrison", "the third Viscount", "James FitzJames, 1st Duke of Berwick,", "Lester Ben \"Benny\" Binion", "two Grammy awards", "S7", "2017", "Aibak's successor and son - in - law Iltutmish", "14 November 2001", "Thomas Jefferson", "Luxembourg", "Margaret Thatcher", "The Muffin Man", "President George Bush", "250,000", "former", "a sponge", "blown", "ghosts", "Jamie Lee Curtis"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6341393849206349}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false], "QA-F1": [0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.4, 1.0, 0.6666666666666666, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3860", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-4733", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-5245", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-3246", "mrqa_hotpotqa-validation-5487", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-1542", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-2978", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-10202", "mrqa_triviaqa-validation-2994", "mrqa_newsqa-validation-2677", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-30", "mrqa_searchqa-validation-7440", "mrqa_searchqa-validation-3848", "mrqa_naturalquestions-validation-1925"], "SR": 0.515625, "CSR": 0.5293560606060606, "EFR": 0.8387096774193549, "Overall": 0.688300647605083}, {"timecode": 66, "before_eval_results": {"predictions": ["Captain Mark Phillips", "Sheffield Wednesday", "Paraguay", "steam engine", "Jesus", "Terry Hall", "December", "Anthony Joshua", "George IV", "Zsa Zsa Gabor", "ambidextral", "Louis Daguerre", "Stephen Hawking", "geochronology", "an international award given each year to a living architect who, in the opinion of select Pritzker Prize jury, has made profound achievements in the world of architecture.", "Guy the gorilla", "a cat", "Port Moresby", "orange", "kursk nuclear submarine", "pyrotechnics", "badminton", "Morgan Choir", "goose", "Olympics", "Typhon", "Syria", "Wyoming", "Professor Brian Cox", "Stephen King", "Albert Finney", "Scotland", "24", "Martin Van Buren", "Ellice Islands", "Meta", "Oil Capital of Europe", "about a mile north of the village of Dunvegan", "a double-reed instrument", "Spice Girls", "\"Mr Loophole\"", "Istanbul", "drinking song", "Texas", "Pablo Picasso", "Yalta Conference", "Rajasthan", "African violet", "bali", "Glee", "Cardigan", "notorious Welsh pirate Edward Kenway, grandfather and father of Assassin's Creed III protagonist and antagonist Ratonhnhak\u00e9 : ton and Haytham Kenway respectively", "Djokovic", "1912", "fennec fox", "1950", "Prince Nikolai Sergeyevich Trubetzkoy", "Vernon Forrest,", "Linda Hogan,", "development of two courses on the Black Sea coast in Bulgaria.", "N.C. Wyeth", "parasites", "Steve Wynn", "substitute good"], "metric_results": {"EM": 0.625, "QA-F1": 0.6814583333333333}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.07999999999999999, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1508", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-4191", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-3729", "mrqa_triviaqa-validation-2064", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-6457", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-4970", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-3610", "mrqa_naturalquestions-validation-3922", "mrqa_hotpotqa-validation-4090", "mrqa_hotpotqa-validation-5590", "mrqa_newsqa-validation-2391", "mrqa_searchqa-validation-9098"], "SR": 0.625, "CSR": 0.5307835820895522, "EFR": 0.75, "Overall": 0.6708442164179104}, {"timecode": 67, "before_eval_results": {"predictions": ["$249", "Cambodia", "Ireland", "tranquil beaches", "flooding", "Werder Bremen", "Secretary of State", "Obama", "nearly three out of four", "Fernando Caceres", "six Africans", "homicide.", "the America's Cup", "Preah Vihear temple", "Uzbekistan", "voluntary manslaughter", "mosteller's brother-in-law", "celebrity-inspired names", "Miami Beach, Florida", "\"Percy Jackson & The Olympians,\"", "contraband cell phones", "three", "brulett Keeling", "the Southern Baptist Convention", "Graeme Smith", "former U.S. secretary of state.", "tried to fake his own death by crashing his private plane into a Florida swamp.", "54-year-old", "Manchester, England shows", "helicopters and boats, as well as vessels from other agencies", "terrorize", "two tickets to Italy", "Oxbow", "\"a potential hazard may occur due to re-entry of satellite debris into the earth's atmosphere.\"", "21-year-old", "Jacob Zuma", "toffelmakaren", "former Procol Harum bandmate Gary Brooker", "a civil disturbance call", "Pew Research Center", "people would call her names on the street,", "Kenyan and Somali governments", "30,000", "1983", "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"", "North Korea", "Steve Jobs", "Garth Brooks", "40-year-old", "Facebook and Google,", "1983", "Carolyn Sue Jones", "Nitty Gritty Dirt Band", "Koine Greek", "Phil Mickelson", "1941", "Yardbirds", "1969", "\"$10,000 Kelly,\"", "Estadio de L\u00f3pez Cort\u00e1zar", "julie taymor", "Perkins", "(Jack) Cardiff", "batterers work to change their attitudes and personal behavior so they would learn to be nonviolent in any relationship"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5790789072039072}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.22222222222222224, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.07692307692307693, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-308", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-2036", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2616", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-833", "mrqa_triviaqa-validation-822", "mrqa_searchqa-validation-5451", "mrqa_searchqa-validation-2492", "mrqa_searchqa-validation-4134", "mrqa_naturalquestions-validation-9387"], "SR": 0.515625, "CSR": 0.5305606617647058, "EFR": 0.967741935483871, "Overall": 0.7143480194497153}, {"timecode": 68, "before_eval_results": {"predictions": ["BADBUL", "2050", "Molotov cocktails, rocks and glass.", "after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "German Chancellor Angela Merkel", "The son of Gabon's former president", "to put a lid on the marking of Ashura this year.", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "off Somalia's coast.", "Chrysler", "AS Roma beat Lecce 3-2", "President Barack Obama,", "the Southern Baptist Convention", "in body bags on the roadway near the bus, which was on its side across both lanes and onto the shoulder of the highway.", "Tuesday", "an American who entered the country illegally from China on Christmas Eve.", "after nine years.", "at least 300", "Thursday", "volatile and dangerous.", "Israeli", "drama that pulls in the crowds", "2008", "root out terrorists within its borders.", "25 years", "a key find by paleontologists at Los Angeles' George C. Page Museum.", "Ciudad Juarez, across the border from El Paso, Texas.", "105-year", "Michael Schumacher", "Jobs", "Jenny Sanford,", "northwestern Montana", "genocide", "stealing the personal credit information of thousands of unsuspecting American and European consumers,", "out in the woods", "John Demjanjuk,", "Venus Williams", "two weeks", "How I Met Your Mother", "British", "Six", "requested the pardon.", "a bank", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "Barnes & Noble", "14", "Michael Arrington", "well over 1,000 pounds", "stand down", "his past and his future", "Mombasa, Kenya", "a loanword of the Visigothic word guma `` man", "Taron Egerton", "Treaty of Chaumont", "Hard Times", "purpurea", "Nellie Melba", "The King of Hollywood", "1947", "the backside", "Sweden", "garcinia cambogia", "Picasso", "improved the speed of encryption of communications at both ends in front line operations during World War II"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5979725861707245}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.8, 0.13333333333333333, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.5185185185185185, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.1818181818181818, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.21276595744680848, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-241", "mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-2374", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-2426", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-4367", "mrqa_triviaqa-validation-4494", "mrqa_searchqa-validation-9014", "mrqa_searchqa-validation-7337"], "SR": 0.515625, "CSR": 0.5303442028985508, "EFR": 0.8387096774193549, "Overall": 0.6884982760635812}, {"timecode": 69, "before_eval_results": {"predictions": ["line the cavities and surfaces of blood vessels and organs throughout the body", "Tim Russert", "on the microscope's stage", "P.V. Sindhu", "Nick Kroll", "April 1917", "Sir Donald Bradman", "two - stroke engines and chain drive", "revenge", "Kevin Sumlin", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive", "Detective Eddie Thawne", "Hathi Jr", "LED illuminated display", "Spektor", "The Star Spangled Banner", "Bill Russell", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "by October 1986", "a protocol ( http )", "solids", "1997", "Carol Worthington", "September 6, 2019", "1973", "1902", "SURFACE HEREA of ROOTS", "back down to the ground", "Battle of Antietam", "at an intersection with U.S. Route 340 ( US 340 ) near Front Royal", "Clarence Anglin", "Andrew Garfield", "Under normal conditions", "the 1980s", "Pasek & Paul", "a Czech word, robota", "prospective studies that examine epidemiology and the long - term effects of nutrition, hormones, environment, and nurses'work - life on health and disease development", "2013", "Billie `` The Blue Bear ''", "eusebeia", "Daniel Suarez", "White House Executive chef", "place of trade", "performers must receive the highest number of votes, and also greater than 50 % of the votes", "the bank", "One Night in the Tropics", "Waylon Jennings", "libretto", "the Rolling Stones", "Sun Tzu", "Pre-evaluation, strategic planning, operative planning, implementation", "Mallee", "inflation", "Charlie Hall Chase 89 (Celtic shot)", "John M. Dowd", "December 17, 1974", "the Northrop P-61 Black widow", "26", "The patient, who prefers to be anonymous,", "as soon as 2050,", "West Point", "Paul Bunyan", "thyroid", "1965"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7023418698350763}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true], "QA-F1": [0.9523809523809523, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.8181818181818182, 0.3333333333333333, 1.0, 0.2857142857142857, 1.0, 0.0, 0.5714285714285715, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.75, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.17391304347826084, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.962962962962963, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9056", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-710", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8374", "mrqa_triviaqa-validation-5361", "mrqa_triviaqa-validation-6937", "mrqa_newsqa-validation-1092"], "SR": 0.5625, "CSR": 0.5308035714285715, "EFR": 0.8214285714285714, "Overall": 0.6851339285714285}, {"timecode": 70, "before_eval_results": {"predictions": ["the President", "Walter Pauk", "Madison's", "Brevet Colonel Robert E. Lee", "instructions", "January 2, 1971", "minced meat", "St. Louis Cardinals", "Bonhomme Carnaval", "1792", "Longliners", "Sebastian Vettel", "Niles", "China", "2017", "Upstate New York", "Carol Ann Susi", "a stem", "Nala", "ZZ Top", "P.V. Sindhu", "Anglican", "the closing of the atrioventricular valves and semilunar valves, respectively", "investment bank Friedman Billings Ramsey", "the NFL", "N 17.44667 \u00b0", "1 January 1904", "a password recovery tool for Microsoft Windows", "from 35 to 40 hours per week", "by week 4 of development", "contemporary Earth", "somatic cell nuclear transfer", "The UN General Assembly", "benzodiazepines", "two", "David Ben - Gurion", "as far west as Las Vegas, as far north as Indianapolis and Denver, and as far east as Richmond, Virginia", "the 7th century at Rendlesham in East Anglia", "St. Theodosius Russian Orthodox Cathedral", "Ray Charles", "a jazz funeral without a body", "2004", "May 31, 2012", "Michael Edwards ( briefly as the older Connor ) and then by teenage actor Edward Furlong throughout the remainder of the film", "Malware", "an ex ( plural is exes ) is someone with whom a person was once associated, in a relationship, marriage, or once talked to", "Beorn", "South Dakota", "Ronald Reagan", "100,000", "1967", "Rajasthan", "Sodor", "eye", "44,300", "2008", "Anglo-Frisian", "Long Island", "\"Z Zimbabwe cannot be British, it cannot be American. Yes, it is African,\"", "11", "bones", "White Queen", "Thailand", "500-room"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6530951462784061}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, false, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.2608695652173913, 0.35294117647058826, 0.058823529411764705, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5185185185185185, 0.6, 0.0, 1.0, 0.0, 0.0, 1.0, 0.21052631578947367, 1.0, 0.08333333333333333, 1.0, 0.0, 1.0, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9600000000000001, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-9852", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-2168", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-2146", "mrqa_hotpotqa-validation-2098", "mrqa_newsqa-validation-3943", "mrqa_searchqa-validation-6656", "mrqa_searchqa-validation-7551"], "SR": 0.578125, "CSR": 0.5314700704225352, "EFR": 0.7407407407407407, "Overall": 0.6691296622326552}, {"timecode": 71, "before_eval_results": {"predictions": ["Mae West", "Diana Vickers", "Tina Turner", "Woodrow Wilson", "Striding Edge", "photographer", "clown", "the Titanic", "Campania", "Hadrian", "Madagascar", "French school of landscape painters", "Michel Denance", "Manet", "Gary Sparrow", "Columbia", "Edinburgh City F.C.", "Lacock Abbey", "Clive Cussler", "Canada", "Hansel and Gretel' cottage", "Honda", "Glasgow", "Hep Stars", "Sonja Henie", "six", "Lord Snooty", "Skye terrier", "Rudolf Hess", "Institute of Chartered Institute of Research and Education", "Stieg Larsson", "songs and albums shared from the leading music services", "1957", "a giant menhir", "steel", "Rotherham United", "Joseph Priestley", "a greyhound, gazelle hound or tazi", "tennis", "Periodic Table", "CameroonCameroon", "region of SW Asia between the lower and middle reaches of the Tigris and Euphrates rivers", "Martin Clunes", "Spanish", "indiget", "Patience", "Ernest Evans", "Quentin Tarantino", "smartphones and similar devices to establish radio communication with each other by touching them together or bringing them into close proximity, usually no more than a few centimetres.", "Salvador Dal\u00ed", "par five 13th", "San Francisco", "90s", "Brooke Wexler", "The Tigers compiled an 11\u20131 regular season record and then defeated the No. 5 Georgia Bulldogs in the SEC Championship Game,", "aging issues", "monthly", "Jet Republic", "businesses hiring veterans as well as job training for all service members leaving the military.", "40 lash after he was convicted of drinking alcohol in Sudan where he plays for first division side Al-Merreikh of Omdurman.", "the American Civil War", "Fed Chief Alan Greenspan", "a gun blast tubes", "UFC Fight Pass"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5977683011610442}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.23529411764705882, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.07142857142857142, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.08695652173913042, 0.8, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-4888", "mrqa_triviaqa-validation-5241", "mrqa_triviaqa-validation-5576", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-1797", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-5665", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-4791", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-4282", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-2277", "mrqa_triviaqa-validation-5816", "mrqa_hotpotqa-validation-1094", "mrqa_hotpotqa-validation-1233", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-1758", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-11196", "mrqa_searchqa-validation-15899"], "SR": 0.546875, "CSR": 0.5316840277777778, "EFR": 0.6551724137931034, "Overall": 0.6520587883141763}, {"timecode": 72, "before_eval_results": {"predictions": ["Bobby Darin", "the chilterns", "Altamont Speedway", "Hanna-Barbera", "26.22", "ankle joint", "an overprotective clownfish", "Samson", "Connecticut", "Daedalus", "a critically burned English accented Hungarian man, his Canadian nurse, a Canadian-Italian thief, and an Indian sapper in the British Army as they live out the end of World War II in an Italian villa.", "Pandemonium", "a pointed stick", "Miles Morales", "Fourteen", "aircraft", "Queen Elizabeth II", "Tonto", "hippocampus", "Frank Miller", "tennis", "Orwell", "Atlantic Ocean", "The treaty of Waitangi", "Chatsworth House", "giorgio Armani", "Budapest", "eyes", "wolfman Speights", "baker Street Irregulars", "Aug. 24, 1572", "a fatty hump on their shoulders, drooping ears and a large dewlap", "Augustus", "Venezuela", "Southwest Airlines", "sunset bOULEVARD", "Johnny Colla", "Derwent", "unhulled seeds", "Laos", "Allardyce", "Petain", "Oliver Barrett IV", "Miami", "Bill Haley & His comets", "a corkscrew shaped pasta, A.K.A. pasta spirals", "1768", "Joan Rivers", "Athens", "William Refrigerator Perry", "Ghana", "Near East", "magnetic stripe `` anomalies '' on the ocean floor", "1999", "Easy", "seven", "Karl Johan Schuster", "U.S. Holocaust Memorial Museum", "Robert Barnett,", "Diego Milito's", "Dick Grayson", "Dumbo the Flying elephant", "Casey at the Bat", "reticulated Python"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5413489736070382}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.06451612903225806, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-1061", "mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4380", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-5979", "mrqa_triviaqa-validation-2629", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-4492", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2423", "mrqa_triviaqa-validation-7682", "mrqa_triviaqa-validation-2011", "mrqa_triviaqa-validation-5164", "mrqa_triviaqa-validation-5974", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-2048", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-7344", "mrqa_hotpotqa-validation-3333", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-2017", "mrqa_newsqa-validation-2030", "mrqa_searchqa-validation-2901", "mrqa_searchqa-validation-7110", "mrqa_searchqa-validation-4706", "mrqa_searchqa-validation-4802"], "SR": 0.453125, "CSR": 0.5306078767123288, "EFR": 0.7714285714285715, "Overall": 0.67509478962818}, {"timecode": 73, "before_eval_results": {"predictions": ["Scottish national team", "Speedway World Championship", "Bears", "\"Time\"", "Babylon", "1501", "The Shins", "11,791", "Eliot Cutler", "Manchester", "Hellenism", "The Ansonia Hotel", "Washington", "Helen Mirren", "horse breeder and owner", "Schutzstaffel", "Edward Albert Heimberger", "The Bye Bye Man", "Chevron Corporation", "ragby", "Indianapolis", "pastels and oil painting", "Premier League", "Sleepy Hollow", "Jane Mayer", "Obafemi Martins", "Knowlton School", "143,007", "Philadelphia", "7", "American former model, actress and television host.", "1967", "mathematician, physicist, and spectroscopist", "king Duncan", "St Andrews Agreement", "Royal College of Music", "4145 ft above mean sea level", "Japan Airlines Flight 123", "near North Chicago, in Lake County, Illinois", "HBO miniseries \"Empire Falls\"", "2013", "The American relay of Michael Phelps, Ryan Lochte, Peter Vanderkaay, and Keller", "a main east-west road connecting the inner northern suburbs", "schoolteacher and publisher", "People v. Turner", "William Harold \"Bill\" Ponsford", "Faysal Qureshi", "one", "Mortal Kombat", "Mike Holmgren", "Gauteng", "Herman Hollerith", "from 6 -- 14 July", "parashah", "paramitas", "1881", "writing", "called it the largest and perhaps most sophisticated ring of its kind in U.S. history.", "Jobs", "Christianity and Judaism", "blintz", "Texas Chainsaw Massacre III", "joltin' Joe", "Caster Semenya"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6713652883574759}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, false, true, true, false, false, true], "QA-F1": [0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0625, 0.6, 0.0, 1.0, 0.3076923076923077, 0.0, 0.8571428571428571, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.1090909090909091, 0.0, 1.0, 1.0, 0.8571428571428571, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-3024", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-5164", "mrqa_hotpotqa-validation-680", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-5036", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5010", "mrqa_hotpotqa-validation-1002", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-3546", "mrqa_triviaqa-validation-1757", "mrqa_triviaqa-validation-3539", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-2621", "mrqa_searchqa-validation-7139", "mrqa_searchqa-validation-10465"], "SR": 0.53125, "CSR": 0.5306165540540541, "EFR": 0.7333333333333333, "Overall": 0.6674774774774775}, {"timecode": 74, "before_eval_results": {"predictions": ["Jesuits", "ribonucleic acid", "ketchup", "igloo", "compound eyes of flies", "construction boots", "The New York Times", "Burma", "Latvia", "spleen", "auf wiedersehen", "rely", "Ramesses II", "wine coolers", "esophagus", "Dallas Cowboys", "The King Jesus Gospel", "The Twist", "Marie Tussaud", "Biscay", "Atlas", "March", "Magellan", "Kevin Spacey", "children of prostitutes", "an oblate spheroid", "The Aviator", "Gioachino Rossini", "Pico de Orizaba", "tail", "Nashville", "Hanging Gardens of Babylon", "The Last Starfighter", "Billy Crystal", "skin cancer", "Gerard", "kbec", "pontificio", "The Drew Carey Show", "Onagraceae", "Moonlighting", "Corpus Christi", "mentor", "Ruth Bader Ginsburg", "Edward R. Murrow", "(Hambantota)", "in vitro fertilization", "Diogenes of Sinope", "pastries", "chocolate milk drink", "the Electric Company", "the following day", "Roger Dean Stadium", "March 31, 2013", "Helen Reddy", "Celsius", "Jeremy Irons", "January", "Jennifer Grey", "Don Johnson", "after giving birth to baby daughter Jada, who was watching her mum from the stands again on Saturday.", "4,000", "Princess Diana", "Melbourne"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6278645833333334}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 0.8, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5833333333333334, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-4940", "mrqa_searchqa-validation-6725", "mrqa_searchqa-validation-10572", "mrqa_searchqa-validation-6347", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-4454", "mrqa_searchqa-validation-1613", "mrqa_searchqa-validation-6459", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-3466", "mrqa_searchqa-validation-13743", "mrqa_searchqa-validation-4143", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-9947", "mrqa_searchqa-validation-661", "mrqa_searchqa-validation-5131", "mrqa_searchqa-validation-85", "mrqa_searchqa-validation-14509", "mrqa_searchqa-validation-5114", "mrqa_searchqa-validation-16566", "mrqa_searchqa-validation-9376", "mrqa_searchqa-validation-9143", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-976", "mrqa_hotpotqa-validation-4363", "mrqa_newsqa-validation-801"], "SR": 0.53125, "CSR": 0.530625, "EFR": 0.9, "Overall": 0.7008125}, {"timecode": 75, "before_eval_results": {"predictions": ["eleven", "Randy VanWarmer", "October 2012", "Sylvester Stallone", "1765", "The Miracles", "1900", "provides a site for genetic transcription that is segregated from the location of translation in the cytoplasm", "25 -- 30 \u00b0 C / km ( 28 -- 34 \u00b0 F / mi )", "AD 1600", "1976", "The Satavahanas", "the Central Board of Artisans", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "16 August 1975", "MFSK", "28 July 1914 to 11 November 1918", "Lager", "908 mbar", "North Atlantic Ocean", "during the winter of the 2017 -- 18 network television season on CBS", "October 2000", "The Lutheran Church of Sweden", "commemorating fealty and filial piety", "on the fictional Iron River Ranch, Colorado", "Allsup", "American singer - songwriter - actress Debbie Gibson", "Lula", "31 January 1934", "Austin, Texas", "southeastern United States", "gastrocnemius", "Daniel A. Dailey", "Jesus'birth", "President Yahya Khan", "Ramanaa", "function like an endocrine organ", "Kyla Coleman", "Bill Patriots", "September 1972", "Tim Passmore", "Garbi\u00f1e Muguruza", "Spanish / Basque origin", "Lilian Bellamy", "about 13,000 astronomical units ( 0.21 ly )", "Shirley Mae Jones", "on April 3, 1973", "Saint Etienne", "`` 5 lakhs of rupees ''", "Chuck Noland", "indigenous to many forested parts of the world", "arithmetic", "arrows", "Red squirrels", "Michael Swango", "Maria von Trapp", "Skatoony", "President Felipe Calderon", "program for low-calorie meals that he could prepare.", "0-0", "Hapsburg", "Mexico", "coyote", "Majid Movahedi,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6950935088947859}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, true, false, false, false, true, true, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6842105263157894, 0.9523809523809523, 0.0, 1.0, 0.0, 0.0, 0.9428571428571428, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.05714285714285714, 0.0, 1.0, 1.0, 1.0, 0.11764705882352942, 0.6666666666666666, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.888888888888889, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-6066", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-2402", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-9367", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-8747", "mrqa_naturalquestions-validation-6207", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-178", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-1646"], "SR": 0.546875, "CSR": 0.5308388157894737, "EFR": 0.8620689655172413, "Overall": 0.693269056261343}, {"timecode": 76, "before_eval_results": {"predictions": ["$699.4 million", "Total Drama World Tour", "Christopher Lloyd", "senators", "rape", "the fictional town of West Egg on prosperous Long Island in the summer of 1922", "layered systems of sovereignty", "Authority", "Jughead Jones", "American rock band Los Lonely Boys", "ecological regions", "a cake", "Kiss", "18 September to 31 October", "Julie Adams", "After a visit by Adolf Hitler", "Borseshoe Bartender", "January 2004", "The Vamps, McGregor Maynard, Bronnie, Ella Eyre, Sheppard and Louisa Johnson", "Oscar", "Tennessee Titan", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins", "southern Turkey", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "Ashoka", "Spanish / Basque origin", "a contemporary drama in a rural setting", "1916", "Billie Jean King", "a sweet alcoholic drink made with rum, fruit juice, and syrup or Grenadine", "2014 -- 15", "October 28, 2007", "Kara Lawson", "Transvaginal ultrasonography", "It is a homodimer of 37 - kDa subunits and is classified as a glycosyl Transferase", "Matt Monro", "a Nativity scene", "IV", "first No. 1 seed to lose to a No. 16 seed since the field expanded to 64 teams in 1985", "Saphira", "September 2017", "an Irish feminine name", "Ace", "Spike", "regulatory site", "After releasing Xander from the obligation to be Sweet's `` bride '', tells the group how much fun they have been ( `` What You Feel -- Reprise '' ) and disappears", "Agamemnon", "InterContinental Hotels Group", "the Isthmus of Corinth", "Jason Flemyng", "the Isthmus of Corinth", "Norman Mailer", "a Bristol Box Kite", "EMI", "Part I", "17 October 2006", "Dr. Alberto Taquini", "$1.5 million", "San Diego", "CNN.com", "jazz funeral", "echidna", "Forrest Gump", "31 meters (102 feet) long and 15 meters (49 feet) wide"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6023091385344674}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.8181818181818181, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.5, 0.7999999999999999, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7368421052631579, 1.0, 1.0, 0.5, 0.0, 0.09090909090909093, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.07407407407407408, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-8037", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-5640", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-4008", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-8762", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-9672", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-921", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2067", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-6055", "mrqa_newsqa-validation-3170"], "SR": 0.53125, "CSR": 0.5308441558441559, "EFR": 0.8, "Overall": 0.6808563311688312}, {"timecode": 77, "before_eval_results": {"predictions": ["Pegasus", "As You Like It", "Apollo 11", "Live and Let Die", "Giuliano Bugiardini", "palladium", "pulsar", "Seth", "Honda", "The 'Erroneous", "Adolf Hitler", "Merchant of Venice", "the 2010 FIFA World Cup", "Elizabeth I", "June", "Italy", "1960s", "Mel Brooks", "Belgians", "chlorophyll", "Paul Dukas", "the US", "Uranus", "rum", "apples", "Aberlemno", "Roddy Doyle", "discus thrower", "Separate Tables", "voice", "Beatrix Potter", "Magpie", "Bill Haley & His comets", "sports which are no longer present on the current program, like polo and tug of war", "Kansas City", "Fidel Castro", "Space Oddity", "Scotland", "Red Admiral", "Illinois", "green", "Splash", "South Africa", "menorah", "Good Will Hunting", "gollum", "otter", "John McCarthy", "John Mortimer", "Cheerios", "line code", "native to Asia", "Liam Cunningham", "Brobee", "Fuenlabrada", "Los Angeles Xtreme, San Francisco Demons and Memphis Maniax", "E22", "security breach", "at checkposts and military camps in the Mohmand agency, part of the lawless Federally Administered Tribal Areas", "Mashhad, Iran.", "St Bernard", "France", "Barnard College", "the equator"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6520833333333333}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.5, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1337", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-6348", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-693", "mrqa_triviaqa-validation-3464", "mrqa_triviaqa-validation-6105", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-3008", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-2258", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-7621", "mrqa_triviaqa-validation-4753", "mrqa_triviaqa-validation-1628", "mrqa_triviaqa-validation-2314", "mrqa_naturalquestions-validation-5687", "mrqa_hotpotqa-validation-844", "mrqa_hotpotqa-validation-2404", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-1775", "mrqa_searchqa-validation-4403", "mrqa_searchqa-validation-4278", "mrqa_newsqa-validation-3978"], "SR": 0.578125, "CSR": 0.5314503205128205, "EFR": 0.7407407407407407, "Overall": 0.6691257122507123}, {"timecode": 78, "before_eval_results": {"predictions": ["France", "Granada", "Verdi", "month", "Al Pacino", "Mohanda Karamchand", "by increasing the number of arcs", "William Golding", "a nerve cell cluster  or a group of nerve cell bodies located in the autonomic nervous system", "vitamin B3", "Director General of the Security Service", "Hell Upside Down", "Funchal", "gabriel", "pasta joke", "Northern Ireland", "passport", "Marcel Duchamp", "The Quatermass Experiment", "Mumbai", "daedalus", "1875", "raven", "hound", "Sue", "Estimate", "$x^2", "Narendra Modi", "Richard Wagner", "arpad \u2018Arki\u2019 Busson", "Argentina", "shorthand", "Kitzb\u00fchel", "Tunisia", "Crystal Gayle", "prairies", "Romania", "brindisi", "Muriel", "Emeril Lagasse", "the 11th Century Church", "Endora", "springtime for Hitler", "the Holy Land", "Eva Herzigov\u00e1", "David Hockney", "Ireland", "gambit", "Carrie", "Colombia", "rolling hillsides", "the anterolateral system", "magnetic stripe `` anomalies '' on the ocean floor", "the ruling city of the Northern Kingdom of Israel, Samaria", "Tudor music and English folk-song", "Martin O'Malley", "1992", "sculptures", "when people gathered outside as the conference in the building ended.", "Kenyan forces who have entered Somalia,", "The Old Man and the Sea", "Edward of Carnarvon", "the Cranberries", "there were no radar outages and said it had not lost contact with any planes during the computer glitches."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6016084558823529}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.35294117647058826, 0.6666666666666666, 0.25, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-4300", "mrqa_triviaqa-validation-994", "mrqa_triviaqa-validation-5551", "mrqa_triviaqa-validation-5714", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-5434", "mrqa_triviaqa-validation-5155", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-6729", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-1150", "mrqa_triviaqa-validation-6121", "mrqa_triviaqa-validation-4483", "mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-6986", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-6999", "mrqa_triviaqa-validation-428", "mrqa_triviaqa-validation-2699", "mrqa_naturalquestions-validation-7511", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2233", "mrqa_searchqa-validation-7161", "mrqa_newsqa-validation-904"], "SR": 0.546875, "CSR": 0.5316455696202531, "EFR": 0.8275862068965517, "Overall": 0.686533855303361}, {"timecode": 79, "before_eval_results": {"predictions": ["Astor", "Addis Ababa", "peacock", "French", "HMS amethyst", "Libya", "tomato", "Kyoto", "Fancy Dress Shop", "Bull Moose Party", "key", "Jake La Motta", "resistance of an unknown resistor", "hobanie McDaniel", "South Africa", "indigestion", "discretion", "swimmers", "Apprentice", "George Washington", "Corinth Canal", "human rights lawyer", "Iceland", "ascot", "pearls", "Bruce Jenner", "gangsters", "doe", "Duncan", "UKIP", "Argentina", "South Sudan", "cars, jewelry, stamps, art, wines, pens, antiques, cigars, even sneakers", "Darby and Joan", "Toplis", "Julian WikiLeaks", "IT Crowd", "\u00ef\u00bf\u00bd", "local police officer Rip Nix", "bobby barley", "Richard Curtis", "terms of endearment", "China", "lagertha", "1790", "greenock", "pinterestflower", "driving Miss Daisy", "orchid", "Hilary Swank", "Aberdeen", "latitude 90 \u00b0 North", "18th century", "UTC \u2212 09 : 00", "just 18 minutes", "England", "Sri Lanka Freedom Party", "\"GoldenEye\"", "Afghanistan's Helmand province,", "Rodong Sinmun", "theology", "cyd charisse", "sanctions", "February"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7191666666666666}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-2433", "mrqa_triviaqa-validation-2739", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5839", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-3356", "mrqa_triviaqa-validation-2354", "mrqa_triviaqa-validation-7585", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-1829", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-2200", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-5706", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7034", "mrqa_naturalquestions-validation-3505", "mrqa_hotpotqa-validation-4993", "mrqa_newsqa-validation-1857", "mrqa_searchqa-validation-2116"], "SR": 0.640625, "CSR": 0.5330078125, "EFR": 0.8695652173913043, "Overall": 0.6952021059782609}, {"timecode": 80, "before_eval_results": {"predictions": ["Wisconsin", "Charles Habib Malik", "senators", "2", "dress shop", "Robert Gillespie Adamson IV", "Colon Street", "off the rez", "Los Angeles", "1969", "Tim Passmore", "2003", "5 : 7 -- 8", "the Hudson Bay", "the only acid excreted as a gas by the lungs", "Miami Heat", "September 30", "four", "manifestation of God's presence as perceived by humans according to the Abrahamic religions", "Emmanuelle Chriqui", "British Indian Association", "tolled ( quota ) highways", "set to 0.05 ( 5 % )", "Australian reality television talent show which premiered on 18 February 2007 on the Seven Network", "A thin film is a layer of material ranging from fractions of a nanometer ( monolayer ) to several micrometers in thickness", "Tulsa, Oklahoma", "Kristy Swanson", "Corey Taylor", "Bonanza Creek Ranch", "Tbilisi", "non-coding sequences", "North Atlantic Ocean", "Native American nation from the Great Plains whose historic territory, known as Comancheria, consisted of present - day eastern New Mexico, southeastern Colorado, southwestern Kansas, western Oklahoma, and most of northwest Texas", "United Nations", "as of October 1, 2015, when the green class A was retired", "2026", "318", "Director of National Intelligence", "Michael Crawford", "Patris et Filii et Spiritus Sancti", "Kida", "September 28, 2017", "Staci Keanan", "Brooklyn, New York", "1837", "Lagaan", "1996", "American rock band Los Lonely Boys", "appearances", "the foreign exchange market (FX )", "Coppolas and, technically, the Farrow / Previn / Allens", "Sunday Post", "Karl Pilkington", "peking", "1860", "back to December", "Ringo Starr", "crawly hills chihuahua", "boyhood experience in a World War II internment camp", "off east  Africa", "modify", "perky ringwald", "faerie", "skull and crossbones"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6763896889400921}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, false, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 0.0, 0.9032258064516129, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.5, 0.0, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-8711", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-9332", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-5915", "mrqa_triviaqa-validation-7286", "mrqa_hotpotqa-validation-5254", "mrqa_newsqa-validation-4104", "mrqa_newsqa-validation-1024", "mrqa_searchqa-validation-10600", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-691"], "SR": 0.59375, "CSR": 0.5337577160493827, "EFR": 0.8846153846153846, "Overall": 0.6983621201329534}, {"timecode": 81, "before_eval_results": {"predictions": ["Sun Tzu", "Foot Container", "Berenice III", "tests above ground", "capitals", "pepperoni minis", "Sarah Jessica Parker", "Long Island Sound", "Hawaii", "fauvism", "Auguste Deter", "Las Vegas", "spinning jenny", "gestation", "ravens", "J.R. Tolkien", "James Franco", "Blue Ridge Mountain range", "georgia", "a mixture of iron oxide and aluminum oxide", "buddha's toe", "Apple", "Thomas R. Gray", "barbels", "A Chorus Line", "ponte Vecchio", "Robbie Turner", "a feeling of sorrow, guilt or... regret", "Olivia Newton-John", "Virginia", "College of William", "small", "Louisiana", "Vassar College", "Japan", "setlery", "The Police", "Air France", "assandro scarlatti", "slew his three sons", "trudge", "Violent Femmes", "Albert Camus", "Ikea", "Rhode Island", "falsetto", "Indian Ocean", "a syringe", "Charlotte Corday", "nanosecond", "bat", "Mason Alan Dinehart", "plays a key role in chain elongation in fatty acid biosynthesis and polyketide biosynthesis", "on location", "2010", "cymbals", "Madagascar", "Thomas William Hiddleston", "Estadio Victoria", "Allerdale", "Mugabe's opponents", "70,000 or so", "Israel", "Owsley Stanley"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5995163690476191}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, false, true, false, false, false, false, true, false, true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.7499999999999999, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.2857142857142857, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6955", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-13181", "mrqa_searchqa-validation-6021", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-4715", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-12632", "mrqa_searchqa-validation-14519", "mrqa_searchqa-validation-16119", "mrqa_searchqa-validation-1201", "mrqa_searchqa-validation-4338", "mrqa_searchqa-validation-1527", "mrqa_searchqa-validation-12500", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-9661", "mrqa_searchqa-validation-6884", "mrqa_searchqa-validation-14625", "mrqa_searchqa-validation-9895", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-13846", "mrqa_searchqa-validation-5489", "mrqa_searchqa-validation-2254", "mrqa_searchqa-validation-9609", "mrqa_searchqa-validation-16936", "mrqa_searchqa-validation-13352", "mrqa_searchqa-validation-506", "mrqa_searchqa-validation-1975", "mrqa_naturalquestions-validation-2110", "mrqa_hotpotqa-validation-980", "mrqa_hotpotqa-validation-868", "mrqa_newsqa-validation-1720"], "SR": 0.484375, "CSR": 0.5331554878048781, "EFR": 0.696969696969697, "Overall": 0.660712536954915}, {"timecode": 82, "before_eval_results": {"predictions": ["Nautilus", "Hopi", "Vatican City", "Pope John Paul II", "Yangtze", "Gnarls Barkley", "the Parthenon", "my Therapist", "Marilyn Monroe", "souvlaki", "Richard III", "the bald eagle", "the National Gallery of Art", "4,840 square yards", "Baha de Darwin, Spanish for \"Darwin Bay.\"", "Frans Hals", "the Black Sox Scandal", "a lynx", "Grenadine", "Constantine", "Aleutian", "alchemy", "art nouveau", "autobahn", "Anglo-Saxon", "California quail", "curtsy", "lacrosse", "Toronto", "assent", "King David", "Riboflavin", "plumage", "Indiana Jones", "Michigan", "Blue", "freelance", "Independence", "Goodyear", "The hobbit", "Red Sox", "William Claude Dukenfield", "Yale University", "Graceland", "the Caspian Sea", "point d'Angleterre", "Lewis Payne", "Prince Edward Island", "Westminster Abbey", "Superbad", "The Granite State", "1885", "$2.187 billion", "Isle Vierge", "France", "acai berry", "Benedictine Order", "Pansexuality", "textual analysis", "getaway driver", "Drew Kesse,", "surrogate", "eight-week", "1999"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6485321969696969}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, false, true, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.07272727272727272, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-9065", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-15986", "mrqa_searchqa-validation-4928", "mrqa_searchqa-validation-13920", "mrqa_searchqa-validation-7058", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-13324", "mrqa_searchqa-validation-14340", "mrqa_searchqa-validation-6941", "mrqa_searchqa-validation-13625", "mrqa_searchqa-validation-11580", "mrqa_searchqa-validation-8315", "mrqa_searchqa-validation-6810", "mrqa_searchqa-validation-14126", "mrqa_searchqa-validation-16140", "mrqa_searchqa-validation-9427", "mrqa_searchqa-validation-5031", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-5308", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-3637", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-475", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-1388", "mrqa_hotpotqa-validation-943"], "SR": 0.5625, "CSR": 0.5335090361445782, "EFR": 0.8571428571428571, "Overall": 0.692817878657487}, {"timecode": 83, "before_eval_results": {"predictions": ["George Washington", "The Office", "Jesus", "penguins", "vrai", "Napoleon Bonaparte", "A.J. Foyt", "vulture", "the Aigle", "Ebony", "Trinity College", "Algeria", "Joseph Haydn", "Dick Cheney", "the black market", "a number", "Saturday Night Fever", "Australia", "pizza al taglio", "a turtle", "the Empire State Building", "white blood cells", "a picayune", "dogwood", "kbec", "Larry McMurtry", "Kellogg's", "Helen of Troy", "the Sweatshirt", "Fd", "Napoleon", "gold", "Spmi", "Ben & Jerry", "Rigoletto", "Tim Tebow", "schizophrenia", "Catherine of Aragon", "Ba", "Pancho Gonzales", "the Aleutians", "Latter-day Saints", "Jane Grey", "Tommy Tutone", "the crescent moon", "Iraq", "a daisy", "Aristotle", "Stuffed Poblano Chiles", "William Safire", "Leonardo da Vinci", "to visit the shrine of Saint Thomas Becket at Canterbury Cathedral", "Charlton Heston", "Andrea Brooks", "the fallopian tube", "Some Like It Hot", "Jeannie C. Riley", "Casablanca", "Theodore Roosevelt Mason", "Parlophone", "Wednesday.", "Daryeel Bulasho Guud", "1995", "four"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6145833333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-4076", "mrqa_searchqa-validation-12923", "mrqa_searchqa-validation-2764", "mrqa_searchqa-validation-7924", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-5131", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-14614", "mrqa_searchqa-validation-546", "mrqa_searchqa-validation-7466", "mrqa_searchqa-validation-8128", "mrqa_searchqa-validation-12014", "mrqa_searchqa-validation-7264", "mrqa_searchqa-validation-5412", "mrqa_searchqa-validation-699", "mrqa_searchqa-validation-12489", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-9461", "mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-1989", "mrqa_naturalquestions-validation-9348", "mrqa_naturalquestions-validation-7457", "mrqa_triviaqa-validation-112", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-3182", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-537"], "SR": 0.515625, "CSR": 0.5332961309523809, "EFR": 0.9354838709677419, "Overall": 0.7084435003840246}, {"timecode": 84, "before_eval_results": {"predictions": ["Russia", "Catherine of Aragon", "Judas Iscariot", "Windsor, Ontario", "Stephen Douglas", "Comrade", "The Great Gatsby", "foxes", "Sexuality", "Salaries", "a king", "John McEnroe", "a bicycle", "Johnson", "La Fea", "push", "Alexander Solzhenitsyn", "tomfoolery", "Mexico", "Easter", "John Denver", "Hurricane Katrina", "Paris", "leeches", "Daughters of the American Revolution", "Manila", "St Mark", "Eragon", "The Beatles", "Louisiana", "Mexico", "a pirate", "engrave", "Daisy Miller", "the Fr. (Franois)", "X", "a ship", "Kamehameha", "raccoons", "Virginia", "Jerry Maguire", "the north magnetic pole", "the Terminal", "Armstrong", "the stadium", "Zimbabwe", "30.0lb", "Patty Duke", "Pronouns", "Hoffmann", "a calico cat", "Frankie Muniz", "during season two", "A complex sentence", "40", "Neptune", "Nowhere Boy", "August 1973", "boar-crested helm", "Richa Sharma", "Carrefour", "financial gain", "concentration camps,", "\"Tiger Woods will \"apologize for his behavior\" Friday when he makes a statement at PGA headquarters in Ponte Vedra Beach, Florida,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6103422619047618}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.09523809523809523]}}, "before_error_ids": ["mrqa_searchqa-validation-5876", "mrqa_searchqa-validation-8842", "mrqa_searchqa-validation-6222", "mrqa_searchqa-validation-6736", "mrqa_searchqa-validation-6488", "mrqa_searchqa-validation-9592", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-6178", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-10285", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10841", "mrqa_searchqa-validation-271", "mrqa_searchqa-validation-8196", "mrqa_searchqa-validation-16155", "mrqa_searchqa-validation-3347", "mrqa_searchqa-validation-11098", "mrqa_searchqa-validation-1469", "mrqa_searchqa-validation-9898", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-9404", "mrqa_hotpotqa-validation-1226", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-3759"], "SR": 0.546875, "CSR": 0.5334558823529412, "EFR": 0.8620689655172413, "Overall": 0.6937924695740365}, {"timecode": 85, "before_eval_results": {"predictions": ["Boston", "Pooh", "Italian", "Eggs Benedict", "the Taj Mahal", "Ayn Rand", "Brahma", "Jon Stewart", "The Sweet", "Tiger Woods", "the Amazon", "(Harry) Houdini", "Falconer", "Ella Fitzgerald", "Ezra Cornell", "The Beatles", "The Hague", "Geena Davis", "pharmacy", "Amos", "the NFL", "James H. Doolittle", "air", "Shakespeare", "Floyd Mayweather Jr", "ABBA", "the League of Nations", "Marlee Matlin", "money changers", "The X-Files", "Animals", "Mensa", "Edward Hopper", "oratorios", "steak", "a voodoo sorcerer", "a Booster seat", "the Church of Jesus Christ of Latter-day Saints", "Veneto", "a cranberries", "the Warsaw Pact", "Sparta", "the Sunday New York Times", "anode", "boldly go", "The National Teachers Hall of Fame", "1876", "the Cherokee", "hair", "the Texas Rangers", "fluoxetine", "H CO ( equivalently OC (OH ) )", "Middle Eastern alchemy", "Brooklyn, New York", "Eton College", "Leeds", "Bexar", "Dwight D. Eisenhower", "Battleship", "\"Shake It Off\"", "ketamine.", "has to move out of her rental house because it is facing foreclosure", "Why he's more American than a German, I don't know,\"", "Charles Sherrington"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6166666666666667}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4481", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-14183", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-7391", "mrqa_searchqa-validation-5581", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-6109", "mrqa_searchqa-validation-16428", "mrqa_searchqa-validation-10775", "mrqa_searchqa-validation-12611", "mrqa_searchqa-validation-2458", "mrqa_searchqa-validation-13169", "mrqa_searchqa-validation-1894", "mrqa_searchqa-validation-16201", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-7363", "mrqa_searchqa-validation-12882", "mrqa_searchqa-validation-10056", "mrqa_searchqa-validation-14355", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-2999", "mrqa_hotpotqa-validation-5190", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-151"], "SR": 0.5625, "CSR": 0.5337936046511628, "EFR": 0.6785714285714286, "Overall": 0.6571605066445183}, {"timecode": 86, "before_eval_results": {"predictions": ["Happy Feet", "a short distance (6)", "the Himalayan mountains", "Joseph", "Chicago", "Aphrodite", "Cannery Row", "Palatine", "California", "Mississippi", "Alpha", "Quebec City", "nacreous", "The Texas Chainsaw Massacre III", "the Senate Chamber", "a Medal of Honor", "Manet", "Plutarch", "Mediolanum", "Corin", "Shropshire", "kidney", "Afghanistan", "satin", "Lady Godiva", "Job", "Vasco da Gama", "Millard", "a denim", "Finnegans Wake", "Es Selamu", "the black market", "professor of higher education", "seismic waves", "Maastricht", "Delilah", "neurons", "croissant", "\"Magnificent Inn\" Grand Hotel", "lungs", "pink", "metacarpal", "a pool", "Warsaw", "a trowel", "Mercury", "Taiwan", "Gettysburg", "the United States", "trout", "\"I'd Like to Get You on a) Slow Boat to China\"", "1959", "season two", "$75,000", "British South Africa Company", "15", "Stonemason's Yard", "Agent Carter", "Orson Welles", "Manhattan", "56", "Secretary of State", "Bright Automotive,", "James Hogg"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5839420995670995}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, true, false, false, false, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16474", "mrqa_searchqa-validation-1883", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-16459", "mrqa_searchqa-validation-14805", "mrqa_searchqa-validation-8253", "mrqa_searchqa-validation-7139", "mrqa_searchqa-validation-6251", "mrqa_searchqa-validation-2960", "mrqa_searchqa-validation-4065", "mrqa_searchqa-validation-861", "mrqa_searchqa-validation-16219", "mrqa_searchqa-validation-12181", "mrqa_searchqa-validation-13659", "mrqa_searchqa-validation-9319", "mrqa_searchqa-validation-2509", "mrqa_searchqa-validation-11869", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-5239", "mrqa_searchqa-validation-6883", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-2311", "mrqa_searchqa-validation-9618", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-1095", "mrqa_triviaqa-validation-5762", "mrqa_hotpotqa-validation-24"], "SR": 0.515625, "CSR": 0.5335847701149425, "EFR": 0.8387096774193549, "Overall": 0.6891463895068595}, {"timecode": 87, "before_eval_results": {"predictions": ["Tim Russert", "Alfonsists and the Carlists", "Michael Crawford", "Alka Yagnik", "Pat McCormick", "Louis Mountbatten", "David Ben - Gurion", "April 6, 1917", "assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "close to 5,770 guaranies", "Geoffrey Zakarian", "4.5 pounds or 2.04 kg", "Mary Elizabeth ( Margaret Hoard )", "Scott Schwartz", "Milan, Italy", "homicidal thoughts of a troubled youth", "Florida", "Husrev Pasha", "Patrick Warburton", "Alan Shearer", "Francis Ford Coppola", "$66.5 million", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "slavery", "Donny Osmond", "political pamphlet", "Taiwan", "Anakin Skywalker", "Alexander Salkind", "one", "Mary Elizabeth Ellis", "Jesse Triplett", "Kevin Garnett", "a star", "Brazil", "Selena Gomez", "Washington", "6th century AD", "Triple threat", "1998", "Nicolas Anelka", "foreign investors", "Napoleon", "marks locations in Google Maps", "Rudy Clark", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "2004", "User State Migration Tool ( USMT )", "Robber baron", "December 20, 1951", "Watson and Crick", "Aconcagua", "Bake Off", "1924", "Eugene Levy", "Aldosterone", "Fionnula Flanagan", "last summer.", "get involved in service and volunteerism in their communities.", "fourth time lucky in Atlanta in 1996.", "banker", "eyes", "the Cubs", "thief"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5077024046512997}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.782608695652174, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7741935483870968, 1.0, 0.0, 0.25, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.05263157894736842, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9825", "mrqa_naturalquestions-validation-6424", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-7509", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-1656", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-5638", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-2008", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-8763", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-3468", "mrqa_triviaqa-validation-1386", "mrqa_triviaqa-validation-3335", "mrqa_hotpotqa-validation-3321", "mrqa_hotpotqa-validation-397", "mrqa_hotpotqa-validation-995", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-152", "mrqa_searchqa-validation-14144", "mrqa_searchqa-validation-9976", "mrqa_searchqa-validation-14176", "mrqa_triviaqa-validation-4676"], "SR": 0.40625, "CSR": 0.5321377840909092, "EFR": 0.868421052631579, "Overall": 0.6947992673444976}, {"timecode": 88, "before_eval_results": {"predictions": ["Cologne, Germany", "Philip Markoff", "a bag", "Federer", "Veracruz, Mexico", "Diego Milito", "Because you hold your breath, one bare breast at a time is tightly compressed between two flat panels and X-rayed.", "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,", "Salt Lake City, Utah,", "normal maritime traffic", "Drottningtorget", "to make space for two ocean wind farms -- taking up 2 percent of the state's waters -- without angering fishing industries, killing whales or harming ecosystems.", "Rocky Ford brand cantaloupes", "These oceans are growing crowded, and governments are increasingly trying to plan their use.", "Two suspects are in custody.", "\"We want to reset our relationship and so we will do it together.'\"", "club managers", "Long Island", "90", "FBI", "Reggae legend Lucky Dube,", "the Kurdish militant group in Turkey", "At least 14", "\"It hurts my heart to see him in pain, but it enlightenedens at the same time to know my son is strong enough to make it through on a daily basis,\"", "Kerstin", "Justice Department motion filed last week in support of the Defense of Marriage Act -- which effectively bars the federal government from recognizing same-sex unions.", "Europe", "\"I believe it's discriminatory. I think it interferes with state's rights, and we will work with Congress to overturn it,\"", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Greeley, Colorado", "Festival Foods", "When the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating.", "drugs", "Daniel Radcliffe", "1.2 million", "\"It was a wrong thing to say, something that we both acknowledge,\"", "12.3 million", "Krishna Rajaram,", "North Korea", "Patrick McGoohan,", "saying Chaudhary's death was warning to management.", "Hamas", "state senators", "2,000 euros ($2,963)", "Anil Kapoor", "this administration recognizes the importance of Turkey and wants to engage with it from the start.", "the Yemeni port city of Aden", "federal officers", "dogs", "central business district of Bangkok", "journalists and the flight crew will be freed,", "writ of certiorari", "pigs", "James Corden", "Norway", "Hamlet", "Hague Conventions", "Chris Hemsworth", "Viscount Cranborne", "England", "beef", "Sleyman I", "a caribou", "Ramadan"], "metric_results": {"EM": 0.5, "QA-F1": 0.6190223512544106}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8, 1.0, 0.07407407407407407, 1.0, 0.10526315789473682, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.9655172413793104, 1.0, 0.24000000000000002, 0.0, 0.0, 0.0, 1.0, 0.0, 0.9268292682926829, 1.0, 1.0, 1.0, 0.3157894736842105, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.0, 0.8, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-834", "mrqa_newsqa-validation-1361", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-1534", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-1427", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-1212", "mrqa_hotpotqa-validation-3169", "mrqa_searchqa-validation-11658", "mrqa_searchqa-validation-3763", "mrqa_searchqa-validation-6846"], "SR": 0.5, "CSR": 0.5317766853932584, "EFR": 0.53125, "Overall": 0.6272928370786517}, {"timecode": 89, "before_eval_results": {"predictions": ["133d Air Refueling Squadron", "Sun Woong", "president", "Comedy Film Nerds", "9\u201310 March 1945,", "2011", "John D Rockefeller's Standard Oil Company", "the early 1970s", "Asiana Town building", "American R&B", "Rockland County", "Manitowoc County, Wisconsin", "34.9 kilometres", "1967", "alcoholic drinks for consumption on the premises.", "Fiat Group", "Chrysler", "Australian", "gorillas", "\"Traumnovelle\" (\"Dream Story\")", "The Royal Navy", "National Archives", "Brad Wilk", "the Beatles", "Baden-W\u00fcrttemberg", "2001 NBA All-Star Game", "rated R", "95 AD", "1614", "French", "\"Grimjack\" (from First Comics) and \"Firestorm\", \"The Spectre\", and \"Martian Manhunter\"", "Mondays", "Michael Jordan", "I, (Annoyed Grunt)-bot", "HSBC Building", "1987", "Kalokuokamaile", "17 October 2006", "melodic hard rock", "Home Rule Party", "Anne Fletcher", "1822", "Mulberry", "Suspiria", "\"Wired\"", "Kansas\u2013Nebraska Act", "Scandinavian design", "Buck Owens", "Big Machine Records", "postal delivery", "Flaw", "June 5, 2017", "1972", "the initiator must go through an intensive week - long initiation process in which the teaching of the ritual skills and moral behavior occurs informally and nonverbally", "Daniel Boone", "forearm", "Tigris", "African National Congress Deputy President Kgalema Motlanthe,", "CNN's \"Piers Morgan Tonight\"", "misdemeanor assault charges", "Florida", "The Partridge Family", "Mickey Spillane", "for flooding from Hurricane Irene that pummeled the East Coast last August and for damages from Tropical Storm Lee in Schoharie, Tioga, Broome, Greene, and Orange counties."], "metric_results": {"EM": 0.65625, "QA-F1": 0.7121527777777779}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3795", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-5233", "mrqa_hotpotqa-validation-5188", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-569", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-3504", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-252", "mrqa_hotpotqa-validation-2672", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-3832", "mrqa_triviaqa-validation-7701", "mrqa_triviaqa-validation-702", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-3369"], "SR": 0.65625, "CSR": 0.5331597222222222, "EFR": 0.8636363636363636, "Overall": 0.6940467171717172}, {"timecode": 90, "before_eval_results": {"predictions": ["Sharon Sheeley", "Ardeth Bay", "2009", "actress and singer", "Pakistan", "1754", "\"Confessions of a Teenage Drama Queen\"", "Bundesliga", "d\u00edsabl\u00f3t", "David Villa", "Adrian Peter McLaren", "2013", "an early colonist of South Australia, remembered as a schoolmaster at J. L. Young's Adelaide Educational Institution and at Saint Peter's College.", "The Birds", "Leon Marcus Uris", "Knoxville, Tennessee", "cancer", "Kim Yoon-seok and Ha Jung-woo", "Diamond White", "25 November 2015", "Craig William Macneill", "January 14, 2010", "2,664", "Tamil", "Objectivism", "Chicago", "Gatwick Airport", "Riot Act", "The Gold Coast", "January 30, 1930", "Soma", "October 29, 1985", "35,124", "Estadio de L\u00f3pez Cort\u00e1zar", "Sir Seretse Goitsebeng Maphiri Khama, GCB, KBE", "Scandinavian design", "Donald Trump", "Thomas Perez", "Flex-fuel", "Bulgarian", "1949", "Trappist beer", "Waiting for Guffman", "Presbyterian", "138,535", "Ry\u016bky\u016b", "1972", "Stern-Plaza", "Life Is a Minestrone", "Columbia Records", "The Spiderwick Chronicles", "Jewel Akens", "gravitation", "ensures consistency within a document and across multiple documents and enforces best practice in usage and in language composition, visual composition, orthography and typography", "South Africa", "Ann Widdecombe", "Jennifer Eccles", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "sailing", "Robert Barnett,", "Barbbie Miss Astronaut", "Diane Cilento", "CO2", "Shout"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6616106455440695}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.787878787878788, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.30434782608695654, 1.0, 1.0, 1.0, 0.2564102564102564, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3064", "mrqa_hotpotqa-validation-1592", "mrqa_hotpotqa-validation-277", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-1539", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-4501", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-5509", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-598", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1457", "mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-4050", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-2030", "mrqa_searchqa-validation-4335", "mrqa_searchqa-validation-10988", "mrqa_searchqa-validation-11743"], "SR": 0.578125, "CSR": 0.5336538461538461, "EFR": 0.8148148148148148, "Overall": 0.6843812321937321}, {"timecode": 91, "before_eval_results": {"predictions": ["Wiener-Dog", "Apple", "the Jaguar S-Type", "the terminal", "Friday", "Sabino Canyon", "Orlando Bloom", "Babe Ruth", "Monica Lee", "Arkansas", "Vince Lombardi", "Virgo", "Contemporary", "the Quiz", "bcolicus", "Tito Puente", "hydrogen", "Ben Jonson", "Hodgkin's lymphoma", "Princess Margaret", "Salt Lake City", "San Francisco", "the Fountainhead", "Mary Baker Eddy", "Bank One", "the College of William & Mary", "The Wright Brothers", "Badminton", "John Deere", "depth and height", "Pontiac", "Reptilia", "Georgia", "Key lime pie", "Lettuce", "the 101 Nights", "bumblebee", "Savannah", "Rickey Henderson", "parquet", "Walker", "F Troop", "Russia", "Lincoln", "Eva Pern", "Port Royal", "a key", "Ghost", "Francisco Pizarro", "Iraq", "Jean-Paul Marat", "Cetshwayo", "Bay of Montevideo", "the bank, rather than the purchaser, is responsible for paying the amount", "a spirit-lifting jingle", "Bobby Brown,", "Bath", "1.5 million households", "Macomb County", "Kristoffer Rygg", "four university students and a safety officer -- told the Coast Guard they were forced off their sailboat after it took on water and capsized.", "Monday", "eight", "minister and biographer"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6631810897435897}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, false, true, false, false, false, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07692307692307693, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-4871", "mrqa_searchqa-validation-5873", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-6087", "mrqa_searchqa-validation-7846", "mrqa_searchqa-validation-5927", "mrqa_searchqa-validation-13146", "mrqa_searchqa-validation-13929", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-595", "mrqa_searchqa-validation-14099", "mrqa_searchqa-validation-10498", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-6272", "mrqa_searchqa-validation-10442", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-13477", "mrqa_searchqa-validation-1574", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-6896", "mrqa_naturalquestions-validation-3303", "mrqa_triviaqa-validation-7696", "mrqa_newsqa-validation-4011", "mrqa_hotpotqa-validation-4539"], "SR": 0.5625, "CSR": 0.5339673913043479, "EFR": 0.8928571428571429, "Overall": 0.7000524068322982}, {"timecode": 92, "before_eval_results": {"predictions": ["Luzon", "Brancusi", "Quantico Virginia", "the East River", "William Shakespeare", "William Shakespeare", "abscesses, ulcers and mucous membranes", "Juneau", "Sputnik I", "Richmond", "Revolvy", "Java", "Haydn", "Blanche DuBois", "flag", "room-temperature vulcanization", "Linton", "Muhammad", "Actress", "Pirates of the Black Pearl", "Frederick Forsyth", "Chesterfield", "a chipmunk", "Marie-Joseph-Rose de Tascher de la Pagerie", "salt", "a disorderly Conduct", "Gioachino Rossini", "Oman", "Lapland", "Tom Canty", "Andrzej Wajda", "Didion", "Chesapeake", "Baltimore", "Bay of Bengal", "buttonedoublen", "Hillary Clinton's", "Terrific", "geology", "six sides", "Olympia", "Ship of Fools", "scare zone", "tendang", "Color Splash", "Margaret Mitchell", "Frances", "Toorop", "cremation", "French & Indian War", "a hypomanic episode", "the Canadian Rockies continental divide east to central Saskatchewan", "lighter fluid", "a scuffle with the Beast Folk", "Judi Dench", "Germany", "Caernarfon", "Dar es Salaam", "Love Streams", "My Beautiful Dark Twisted Fantasy", "between June 20 and July 20", "Michael Krane,", "Virgin America", "AIDS and HIV"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5105902777777778}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1244", "mrqa_searchqa-validation-16790", "mrqa_searchqa-validation-5559", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-12499", "mrqa_searchqa-validation-4155", "mrqa_searchqa-validation-10070", "mrqa_searchqa-validation-173", "mrqa_searchqa-validation-13847", "mrqa_searchqa-validation-2299", "mrqa_searchqa-validation-14332", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-11414", "mrqa_searchqa-validation-13171", "mrqa_searchqa-validation-3320", "mrqa_searchqa-validation-3466", "mrqa_searchqa-validation-20", "mrqa_searchqa-validation-4824", "mrqa_searchqa-validation-15084", "mrqa_searchqa-validation-11893", "mrqa_searchqa-validation-15704", "mrqa_searchqa-validation-15802", "mrqa_searchqa-validation-13701", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-315", "mrqa_searchqa-validation-14800", "mrqa_searchqa-validation-15780", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-6507", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-4465", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-894", "mrqa_triviaqa-validation-6867", "mrqa_hotpotqa-validation-3557"], "SR": 0.421875, "CSR": 0.5327620967741935, "EFR": 0.8378378378378378, "Overall": 0.6888074869224063}, {"timecode": 93, "before_eval_results": {"predictions": ["Mesopotamia", "Gettysburg College", "Tim McGraw", "provides the public with financial information about a nonprofit organization", "Lulu", "Tony Orlando", "Mel Gibson", "2017", "drivers who were 2016 Pole Award winners, former Clash race winner, former Daytona 500 pole winners who competed full - time in 2016, and drivers who qualified for the 2016 Chase", "1967", "Pacific Grove", "while in a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape '' ) form", "Audrey II", "January 2017", "NIRA", "1922", "Julieorah Kavner", "Justin Timberlake", "American production duo The Chainsmoker", "13 May 1787", "Prince James, Duke of York and of Albany ( later King James II & VII )", "his brother", "Seattle, Washington, site of the Century 21 Exposition, the 1962 World's Fair", "honey bees", "Article 1, Section 2, Clause 3", "McFerrin", "Napoleon", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "September 27, 2017", "Fusajiro Yamauchi", "March 31 to April 8, 2018", "Tbilisi", "Tiffany Adams Coyne", "ice giants", "hyperinflation", "1939", "Richard Masur", "Al Foster", "Tagalog or English", "Sauron", "Rick Nowels", "statistical", "159", "The Third Five - year Plan", "Chemistry professor E.H.S. Bailey and his colleagues were returning by train to Lawrence after a conference", "makes Maria a dress to wear to the neighborhood dance", "activates a relay which will handle the higher current load", "limited period of time", "commemorating fealty and filial piety", "in the stems and roots of certain vascular plants", "when the cell is undergoing the metaphase of cell division ( where all chromosomes are aligned in the center of the cell in their condensed form )", "euro", "Robin Hood Men in Tights", "Snowshoe", "February 13, 1946", "Crystal Dynamics", "Congo River", "Jason Chaffetz", "Da Vinci Code", "humiliate herself by standing next to a story,\"", "Khrushchev", "Julie Andrews", "Ichabod Crane", "Leo Frank,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6313961989877939}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, true, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.9655172413793104, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.4, 1.0, 0.75, 0.0, 0.0, 1.0, 0.8571428571428572, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.13333333333333333, 0.0, 0.30769230769230765, 1.0, 0.2, 0.0909090909090909, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-9264", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-2989", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8159", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-3305", "mrqa_newsqa-validation-3374", "mrqa_searchqa-validation-105"], "SR": 0.515625, "CSR": 0.5325797872340425, "EFR": 0.7741935483870968, "Overall": 0.6760421671242278}, {"timecode": 94, "before_eval_results": {"predictions": ["direct scattering and inverse scattering", "ThonMaker", "Battle of Chester", "youngest TV director ever", "19 February 1927", "on the shore, associated with \"the Waters of Death\"", "playback singer, director, writer and producer", "L\u00edneas A\u00e9reas", "English", "National Basketball Development League", "Neville Chamberlain,", "Boulder High School in Boulder, Colorado", "Revengers Tragedy", "Japan", "rural", "8", "Larry Alphonso Johnson Jr.", "Gabriel Iglesias", "August 28, 1774", "CMYKOG process", "Las Vegas Boulevard", "intelligent design: The Bridge Between Science and Theology", "Anthony Herrera", "Jack Elam", "Adelaide Botanic Garden, Hutt Street, and Victoria Park.", "Uzumaki", "Kansas City", "nearly 80 years", "Chevy Corvette", "Field of Dreams", "eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music.", "The Wachowskis", "Pour le M\u00e9rite", "mastered recordings for many well known musicians,", "Drowning Pool", "typically found within a casino, ranging from card to slot machines.", "Food and Agriculture Organization", "dance partner", "Bharat Ratna", "Cesar Millan", "Eurasia", "Beauty and the Beast", "Bardney", "Holinshed's Chronicles", "August 9, 2017", "Bangalore University", "25 October 1921", "Australia", "Bonkyll Castle", "February 5, 2015", "Zeffirelli", "giant planet", "alveolar process", "Duisburg", "Hugh Quarshie", "king Frederick William III of Prussia", "Tokyo", "Utah Valley Regional Medical Center", "Madonna", "Fareed Zakaria", "Easter Island", "Eli Whitney", "Today", "25 percent"], "metric_results": {"EM": 0.625, "QA-F1": 0.7757936507936508}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false], "QA-F1": [0.8333333333333333, 0.0, 1.0, 1.0, 0.0, 0.6, 0.2857142857142857, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.4, 1.0, 0.47619047619047616, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-706", "mrqa_hotpotqa-validation-839", "mrqa_hotpotqa-validation-429", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-367", "mrqa_hotpotqa-validation-5255", "mrqa_hotpotqa-validation-436", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2344", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-2880", "mrqa_hotpotqa-validation-5838", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-1206", "mrqa_hotpotqa-validation-1606", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-5456", "mrqa_hotpotqa-validation-5406", "mrqa_naturalquestions-validation-5155", "mrqa_triviaqa-validation-514", "mrqa_searchqa-validation-2056", "mrqa_newsqa-validation-2361"], "SR": 0.625, "CSR": 0.5335526315789474, "EFR": 0.875, "Overall": 0.6963980263157895}, {"timecode": 95, "before_eval_results": {"predictions": ["more than 250 million copies worldwide", "Ben Ainslie", "1978", "Spoorloos", "Scott Mosier", "from 1945 to 1951", "Roy Spencer", "1964", "Shawnee Mission Parkway", "VH1", "March", "Russian", "Harrison Ford", "July 25 to August 4", "Claude Mak\u00e9l\u00e9l\u00e9 Sinda", "singer, songwriter, actress, and radio and television presenter", "Northern Lights", "non-alcoholic", "Mach number", "Jordan Ridgeway", "Maine", "Encore Las Vegas", "\"Baa, Baa, Black sheep\"", "It's Always Sunny in Philadelphia", "John Francis Kelly", "Madeleine L' Engle", "1972", "Sargent Shriver", "paracyclist", "Mandarin", "Kevin Spacey", "Deputy Vice-Chancellor (Academic)", "Song Il-gon", "Teenitans Go!", "Mickey Mouse Cup", "three", "right-hand", "Sheen Michaels Entertainment", "Sela Ann Ward", "whale lice", "Houston Rockets", "DI Humphrey Goodman", "Daphnis et Chlo\u00e9", "Nebraska Cornhuskers women's basketball", "Metro-Goldwyn-Mayer", "P.O.S", "My Backyard", "Kim So-hyun", "boxer", "Aloe Vera of America", "creeks", "1992", "the Second Continental Congress", "Roger Dean Stadium", "cirrocumulus", "Compiegne", "La traviata", "in just over two weeks.", "Da Vinci Code", "Dog patch Labs", "iceberg", "a battery", "Queen Victoria", "Venus Williams"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6762400793650793}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-74", "mrqa_hotpotqa-validation-5147", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5551", "mrqa_hotpotqa-validation-4280", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-5809", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4196", "mrqa_hotpotqa-validation-5569", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-5665", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-2029", "mrqa_hotpotqa-validation-2388", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-191", "mrqa_searchqa-validation-14503"], "SR": 0.59375, "CSR": 0.5341796875, "EFR": 0.8076923076923077, "Overall": 0.6830618990384615}, {"timecode": 96, "before_eval_results": {"predictions": ["an obsessed and tormented king", "1927", "16,116", "the 2012 Summer Olympics", "at the end of the 18th century", "1942", "Johnny Cash and Waylon Jennings", "Estadio de L\u00f3pez Cort\u00e1zar", "Syracuse", "Kim Jong-hyun", "Mercer", "Gillian Leigh Anderson", "the alternative rock band R.E.M.", "Ice Princess", "The Summer Olympic Games", "Oldham County", "1896", "Oracle Corporation", "143,007", "SARS", "5.3 million", "chocolate-colored Labrador Retriever", "Norman Graham Hill", "1952", "Neneh Mariann Karlsson", "Eminem", "Love Streams", "In a Better World", "Shropshire Union Canal", "Easy", "The Killer", "2015", "Dutch", "Lowestoft, Suffolk", "Trey Parker", "\"Pimp My Ride\"", "Big 12 Conference", "Hillsborough County", "Dancing with the Stars", "an Indian", "John Francis Kelly", "early Romantic period", "approximately $700 million", "the Sun", "Bhushan Patel", "1692", "interstate commerce", "Wu-Tang Clan", "Kids", "Mortal Kombat X", "Kew Gardens", "third season", "May 2016", "Kristy Swanson", "Colonel", "Conan Doyle", "scalene", "put a lid on the marking of Ashura", "Pakistan", "homicide", "dough bread", "leather", "cornea", "Eleanor Roosevelt"], "metric_results": {"EM": 0.65625, "QA-F1": 0.8093389249639249}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.888888888888889, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2376", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-1285", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-153", "mrqa_hotpotqa-validation-5879", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-1829", "mrqa_hotpotqa-validation-3344", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-2286", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-2482", "mrqa_hotpotqa-validation-4514", "mrqa_triviaqa-validation-2789", "mrqa_newsqa-validation-1218", "mrqa_searchqa-validation-13280"], "SR": 0.65625, "CSR": 0.5354381443298969, "EFR": 0.9090909090909091, "Overall": 0.7035933106841611}, {"timecode": 97, "before_eval_results": {"predictions": ["The Theory of Everything", "the main Caucasus range", "David Bowie", "Steve Davis", "Granada", "Treaty of Brest-Litovsk", "Karl Marx", "The Paramounts", "The titles", "feisty", "1957", "1912", "transsexual", "Michael V. Gazzo", "Edinburgh", "Scotland Yard detective", "Inverness-shire", "full of woe", "innie Mae", "Rudyard Kipling", "1921", "Hamish Macbeth", "Desdemona", "avocado", "Frans Hals", "Syriza", "Ford", "soy", "Cole Porter", "1826", "William WymarkJacobs", "the Parthenon", "The My Big Fat Gypsy Wedding star", "Thomas Aquinas", "Dubonnet Rouge Aperitif", "elephant", "Bobby Fischer", "underfund", "Westminster Abbey", "Canada", "Seal", "Edward VII", "Tombstone", "s\u00e3o Miguel", "Mr. Men and 33 Little Miss", "Worcester Cathedral", "Mercury", "December 7, 1941", "the ear", "Nadia Comaneci", "Buzz Aldrin", "Ant & Dec", "John Ernest Crawford", "March 1930", "Gillian Anderson", "VAQ-135", "95 AD", "more than 170", "Entourage", "Karen Floyd", "awe-inspiring", "the Caspian Sea", "Francisco Pizarro's", "from 1922 to 1991"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5225694444444444}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444444, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7127", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5066", "mrqa_triviaqa-validation-4459", "mrqa_triviaqa-validation-3538", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-7266", "mrqa_triviaqa-validation-7293", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-6630", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-5204", "mrqa_triviaqa-validation-6617", "mrqa_triviaqa-validation-2651", "mrqa_triviaqa-validation-1602", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-329", "mrqa_triviaqa-validation-1968", "mrqa_triviaqa-validation-4876", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-1547", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-124", "mrqa_triviaqa-validation-174", "mrqa_hotpotqa-validation-2058", "mrqa_newsqa-validation-1171", "mrqa_newsqa-validation-3100", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-15233", "mrqa_searchqa-validation-16068", "mrqa_naturalquestions-validation-7080"], "SR": 0.4375, "CSR": 0.534438775510204, "EFR": 0.6944444444444444, "Overall": 0.6604641439909298}, {"timecode": 98, "before_eval_results": {"predictions": ["he'll send a text message and e-mail to his supporters to let them know who his sidekick will be.", "Afghanistan", "deutschneudorf", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "his health and about a comeback.", "poems telling of the pain and suffering of children just like her; girls banned from school, their books burned, as the hard-core Islamic militants spread their reign of terror across parts of Pakistan.", "then-presidential candidate Barack Obama", "women", "581 points", "The Everglades, known as the River of Grass,", "Herman Cain", "Brett Cummins,", "3-0", "what caused the collapse of the building which contained the city's historical archives, bringing down parts of the two nearby structures.", "Former Mobile County Circuit Judge Herman Thomas", "celebrities and ministers, ranging from Yolanda Adams to Bishop T.D. Jakes to Kirk Franklin.", "Iraqi economy.", "Phillip A. Myers.", "In 1959, Bobby Darin, left, was Larry's first major guest on his WKAT radio program.", "publish what they are doing using 140 characters or less.", "Argentina", "Sheik Mohammed Ali al-Moayad", "Iraqi Prime Minister Nouri al-Maliki", "France", "\"It was incredible. We've had so much rain, and yet today it was beautiful. The rain held off wherever Muhammad Ali went,\" Frankie Neylon, the town's mayor said.", "\" Few people are so lucky to have that from the moment you meet that one person,", "WBO welterweight title from Miguel Cotto on a 12th round technical knockout in Las Vegas.'", "Austin, Texas,", "15-month", "to pay him a monthly allowance,", "Manmohan Singh's", "the federal government apparently has no voice in this crisis, when in some cases, they are the only answer, that pointed to a lot of the criticism, didn't it?", "for death squad killings carried out during his rule in the 1990s.", "100 meter", "sniff out cell phones.", "Fayetteville, North Carolina,", "American Bill Haas", "Consumer Reports", "28", "step up.\"", "42 years old", "since 1983", "health ailment or beauty concern.", "almost 100", "Espinoza Barron's", "Jason Voorhees", "\"By working together, we will set wise and effective policies.\"", "fastest circumnavigation of the globe in a powerboat", "$106,482,500", "18th", "Haeftling", "the left of the dinner plate", "Asuka", "Bart Millard", "Nissan", "stone arch bridges", "Jane Austen", "the Marx Brothers film", "Indian", "early 20th-century Europe", "hold business' IT systems hostage", "Shakespeare in Love", "w. Somerset Maugham", "Leicestershire"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5587431582164796}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, false, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, true], "QA-F1": [0.14285714285714285, 0.0, 0.0, 0.8918918918918919, 0.0, 0.0625, 0.3333333333333333, 0.0, 0.6666666666666666, 0.2857142857142857, 1.0, 1.0, 1.0, 0.38095238095238093, 0.8333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.35294117647058826, 1.0, 1.0, 0.1818181818181818, 1.0, 0.0, 0.2105263157894737, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.9090909090909091, 1.0, 0.4, 0.0, 0.13333333333333333, 0.2857142857142857, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.33333333333333337, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-2182", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-3073", "mrqa_newsqa-validation-3258", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-1336", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-2610", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1391", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3323", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-3208", "mrqa_newsqa-validation-2745", "mrqa_newsqa-validation-2541", "mrqa_naturalquestions-validation-2024", "mrqa_triviaqa-validation-3928", "mrqa_triviaqa-validation-5307", "mrqa_hotpotqa-validation-3326", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-14191"], "SR": 0.40625, "CSR": 0.5331439393939394, "EFR": 0.6842105263157895, "Overall": 0.6581583931419457}, {"timecode": 99, "UKR": 0.763671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1002", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1069", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-136", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-153", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1707", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-1834", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-2094", "mrqa_hotpotqa-validation-2132", "mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-2463", "mrqa_hotpotqa-validation-2489", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2640", "mrqa_hotpotqa-validation-2652", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-277", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2844", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-2862", "mrqa_hotpotqa-validation-3024", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3278", "mrqa_hotpotqa-validation-3289", "mrqa_hotpotqa-validation-3301", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-3658", "mrqa_hotpotqa-validation-367", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-377", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3996", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-4169", "mrqa_hotpotqa-validation-4378", "mrqa_hotpotqa-validation-4435", "mrqa_hotpotqa-validation-4514", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5201", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5283", "mrqa_hotpotqa-validation-5310", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5630", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-634", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-900", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-969", "mrqa_naturalquestions-validation-10077", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-10693", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1329", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1756", "mrqa_naturalquestions-validation-1840", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-228", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-2349", "mrqa_naturalquestions-validation-2459", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-4501", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5582", "mrqa_naturalquestions-validation-5600", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-6408", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7591", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-8329", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8741", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-9208", "mrqa_naturalquestions-validation-9219", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-9595", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-9987", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1475", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1663", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-1744", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-189", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2379", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-2993", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-3077", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-3118", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3258", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3464", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-3536", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3925", "mrqa_newsqa-validation-3941", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-965", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10077", "mrqa_searchqa-validation-10146", "mrqa_searchqa-validation-10231", "mrqa_searchqa-validation-10351", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-10763", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-11028", "mrqa_searchqa-validation-11077", "mrqa_searchqa-validation-11089", "mrqa_searchqa-validation-11111", "mrqa_searchqa-validation-11151", "mrqa_searchqa-validation-11196", "mrqa_searchqa-validation-11599", "mrqa_searchqa-validation-11976", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-12092", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-12942", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13182", "mrqa_searchqa-validation-13352", "mrqa_searchqa-validation-13625", "mrqa_searchqa-validation-13654", "mrqa_searchqa-validation-13659", "mrqa_searchqa-validation-1371", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-14001", "mrqa_searchqa-validation-14197", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14519", "mrqa_searchqa-validation-14614", "mrqa_searchqa-validation-14625", "mrqa_searchqa-validation-14705", "mrqa_searchqa-validation-14740", "mrqa_searchqa-validation-14770", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-14805", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-15157", "mrqa_searchqa-validation-15235", "mrqa_searchqa-validation-15394", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-15746", "mrqa_searchqa-validation-15843", "mrqa_searchqa-validation-15883", "mrqa_searchqa-validation-16119", "mrqa_searchqa-validation-16140", "mrqa_searchqa-validation-16335", "mrqa_searchqa-validation-16515", "mrqa_searchqa-validation-1655", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16751", "mrqa_searchqa-validation-16786", "mrqa_searchqa-validation-16962", "mrqa_searchqa-validation-1741", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-1897", "mrqa_searchqa-validation-2116", "mrqa_searchqa-validation-2215", "mrqa_searchqa-validation-2228", "mrqa_searchqa-validation-2392", "mrqa_searchqa-validation-2436", "mrqa_searchqa-validation-2801", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-3026", "mrqa_searchqa-validation-3087", "mrqa_searchqa-validation-334", "mrqa_searchqa-validation-3347", "mrqa_searchqa-validation-3469", "mrqa_searchqa-validation-3496", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-3825", "mrqa_searchqa-validation-386", "mrqa_searchqa-validation-4023", "mrqa_searchqa-validation-4481", "mrqa_searchqa-validation-4512", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4808", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-543", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-5625", "mrqa_searchqa-validation-5649", "mrqa_searchqa-validation-5669", "mrqa_searchqa-validation-5733", "mrqa_searchqa-validation-5906", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-629", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-6616", "mrqa_searchqa-validation-6736", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-69", "mrqa_searchqa-validation-6941", "mrqa_searchqa-validation-7139", "mrqa_searchqa-validation-7166", "mrqa_searchqa-validation-7440", "mrqa_searchqa-validation-746", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-7753", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-8239", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-8383", "mrqa_searchqa-validation-8459", "mrqa_searchqa-validation-8575", "mrqa_searchqa-validation-861", "mrqa_searchqa-validation-8702", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-8933", "mrqa_searchqa-validation-9119", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9461", "mrqa_searchqa-validation-9682", "mrqa_searchqa-validation-9752", "mrqa_searchqa-validation-9942", "mrqa_squad-validation-10026", "mrqa_squad-validation-10227", "mrqa_squad-validation-112", "mrqa_squad-validation-1204", "mrqa_squad-validation-1454", "mrqa_squad-validation-1758", "mrqa_squad-validation-1759", "mrqa_squad-validation-2225", "mrqa_squad-validation-2365", "mrqa_squad-validation-2466", "mrqa_squad-validation-2784", "mrqa_squad-validation-3080", "mrqa_squad-validation-3110", "mrqa_squad-validation-3130", "mrqa_squad-validation-3581", "mrqa_squad-validation-3632", "mrqa_squad-validation-4259", "mrqa_squad-validation-457", "mrqa_squad-validation-4621", "mrqa_squad-validation-4770", "mrqa_squad-validation-5010", "mrqa_squad-validation-5651", "mrqa_squad-validation-5784", "mrqa_squad-validation-5913", "mrqa_squad-validation-6166", "mrqa_squad-validation-6694", "mrqa_squad-validation-6789", "mrqa_squad-validation-6947", "mrqa_squad-validation-7214", "mrqa_squad-validation-7269", "mrqa_squad-validation-7521", "mrqa_squad-validation-7547", "mrqa_squad-validation-7596", "mrqa_squad-validation-7848", "mrqa_squad-validation-8052", "mrqa_squad-validation-8151", "mrqa_squad-validation-8733", "mrqa_squad-validation-8830", "mrqa_squad-validation-9233", "mrqa_squad-validation-930", "mrqa_squad-validation-9311", "mrqa_squad-validation-962", "mrqa_squad-validation-9816", "mrqa_squad-validation-9859", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1216", "mrqa_triviaqa-validation-124", "mrqa_triviaqa-validation-1450", "mrqa_triviaqa-validation-1547", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-1923", "mrqa_triviaqa-validation-1968", "mrqa_triviaqa-validation-2038", "mrqa_triviaqa-validation-2200", "mrqa_triviaqa-validation-2208", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-2505", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3226", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3471", "mrqa_triviaqa-validation-3707", "mrqa_triviaqa-validation-3796", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-4365", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-4483", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-4660", "mrqa_triviaqa-validation-4737", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4876", "mrqa_triviaqa-validation-4890", "mrqa_triviaqa-validation-5158", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5361", "mrqa_triviaqa-validation-5457", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5706", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5820", "mrqa_triviaqa-validation-5832", "mrqa_triviaqa-validation-5851", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6239", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6329", "mrqa_triviaqa-validation-642", "mrqa_triviaqa-validation-6540", "mrqa_triviaqa-validation-6636", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-6729", "mrqa_triviaqa-validation-6985", "mrqa_triviaqa-validation-7031", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7219", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7350", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-958"], "OKR": 0.720703125, "KG": 0.50859375, "before_eval_results": {"predictions": ["Mount Rainier, Washington", "Douglas Jackson", "Austral L\u00edneas A\u00e9reas", "Mach number", "Craig William Macneill", "USS Essex", "8,648", "three", "Jeffrey Adam \"Duff\" Goldman", "Minnesota", "global peace", "Oregon State Beavers", "Arkansas", "the 2011 Pulitzer Prize in General Nonfiction", "Golden Gate National Recreation Area", "Pain Language", "Broadcasting House in London", "London Tipton", "Barney Miller", "Lily Hampton", "President of the United States", "Big Machine Records", "constant support from propaganda campaigns", "The Heirs", "Saturday Night Live", "Greek mythology,", "Tumi Holdings, Inc.", "Black Ravens", "commercial", "Suspiria", "Silvia Navarro", "22,500", "Warsaw, Poland", "Nelson County", "Kang", "25 million", "Cleopatra VII Philopator", "James G. Kiernan", "the MC5", "James City County", "Tunisian", "Linda Ronstadt", "the United Kingdom", "August 19, 2013", "the Americas and the entire South American temperate zone", "Sister, Sister", "five", "Mark Radcliffe", "13 May 2018", "Kevin Spacey", "Stalybridge Celtic", "The term was first used in tennis", "Frank Zappa", "1989", "apples", "Fred Trueman", "Scotland", "Chesley \"Sully\" Sullenberger", "Fernando Torres", "Friday,", "Lifeboat", "a bit", "stock-broker", "Benazir Bhutto"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6474158653846154}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.3333333333333333, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.05128205128205128, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-5715", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-1798", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-3197", "mrqa_hotpotqa-validation-2282", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-4109", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-2156", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-934", "mrqa_triviaqa-validation-7095", "mrqa_searchqa-validation-572"], "SR": 0.5625, "CSR": 0.5334375, "EFR": 0.8214285714285714, "Overall": 0.6695669642857143}]}