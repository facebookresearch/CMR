{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.2', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.2_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.2.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4180, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["positive divisors", "quietist/non-political", "Jonathan Stewart", "surface condensers", "Anglo-Saxons", "one of the first peer-to-peer network architectures", "Tanzania", "structure", "ABC Cable News", "Turner and Vernon", "$2 million", "German-language publications", "-40%", "BBC 1", "the \"blurring of theological and confessional differences in the interests of unity.\"", "pamphlets on Islam", "mad dogs", "Mnemiopsis", "both Kenia and Kegnia", "electricity", "student tuition, endowments, scholarship/voucher funds", "Mongolian patrimonial feudalism and the traditional Chinese autocratic-bureaucratic system", "European Council", "826", "1999", "Latin", "semantical problems", "$2 million", "committee", "(trunnion", "South Pacific", "Spanish moss", "1850s", "Abercrombie was recalled and replaced by Jeffery Amherst", "saturating them unconsciously with electricity", "slightly more than normal sea-level O2 partial pressure", "Associating forces with vectors", "showmanship", "social networking support", "Children of Earth", "Soviet", "Brock Osweiler", "San Diego", "Economist", "liquid", "Jerricho Cotchery", "suggested it for use in the ARPANET", "disrupting their plasma membrane", "Genghis Khan", "Robert Boyle", "feigned retreat", "Rotterdam", "the problem of multiplying two integers", "he was illiterate in Czech", "the Monarch", "4.7 yards per carry", "Sports Programs, Inc.", "only pharmacists", "ideological", "behavioral and demographic data", "Kuviasungnerk/Kangeiko", "94", "October 16, 2012", "transportation, sewer, hazardous waste and water"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7783752554812338}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.08695652173913045, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.33333333333333337, 0.11111111111111112, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3846153846153846, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8917", "mrqa_squad-validation-9575", "mrqa_squad-validation-4676", "mrqa_squad-validation-10430", "mrqa_squad-validation-5326", "mrqa_squad-validation-2291", "mrqa_squad-validation-1530", "mrqa_squad-validation-7086", "mrqa_squad-validation-8412", "mrqa_squad-validation-2478", "mrqa_squad-validation-3590", "mrqa_squad-validation-1913", "mrqa_squad-validation-3771", "mrqa_squad-validation-6293", "mrqa_squad-validation-1766", "mrqa_squad-validation-1187", "mrqa_squad-validation-288"], "SR": 0.734375, "CSR": 0.734375, "EFR": 1.0, "Overall": 0.8671875}, {"timecode": 1, "before_eval_results": {"predictions": ["1929", "the lack of a Parliament of Scotland", "islands", "US$10 a week", "a force is required to maintain motion, even at a constant velocity", "Horace Walpole", "any object can be, essentially uniquely, decomposed into its prime components", "1968", "straight", "complexity classes", "smaller, weaker swimmers such as rotifers and mollusc and crustacean larvae", "80%", "leaders who held secular leanings or who had introduced or promoted Western/foreign ideas and practices into Islamic societies", "Informal", "seven", "aircraft manufacturing", "1671", "the highest duty of a citizen", "the headwaiter", "a comb jelly", "expositions", "1784", "terrorist organisation", "\"winds up\"", "Golden Gate Bridge", "Hulu", "National Galleries of Scotland", "Northern Rhodesia", "Budapest Telephone Exchange", "Joanna Lumley", "Gateshead", "tentacles", "soluble components (molecules)", "East Smithfield burial site in England", "Jerome Schurf", "satellite television", "we are neither making maximum effort nor achieving results necessary", "Isaac Newton", "dangerous enemies", "Robert Underwood Johnson", "a protest", "kinetic friction", "X-rays", "Roger NFL", "Abu al-Rayhan al-Biruni", "colonial powers", "Spreading throughout the Mediterranean and Europe, the Black Death is estimated to have killed 30\u201360% of Europe's total population", "almost a month", "\"cellular\" and \"humoral\"", "traditional old boy network", "anti-Semitic policies", "the Scottish Government", "the Lisbon Treaty", "emerging market", "Bible", "24\u201310", "cellular respiration", "The \"Big Five\"", "computer problems", "Her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries ; the producers like to imagine that she was directing an episode", "From Russia", "Balvenie Castle", "geological evidence shows that this 5000-mile mountain chain may extend south into Antarctica", "Annie Ida Jenny No\u00eb Haesendonck"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7216039913510987}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08695652173913045, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1290322580645161, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1273", "mrqa_squad-validation-10338", "mrqa_squad-validation-9525", "mrqa_squad-validation-2704", "mrqa_squad-validation-3113", "mrqa_squad-validation-6759", "mrqa_squad-validation-739", "mrqa_squad-validation-1277", "mrqa_squad-validation-4902", "mrqa_squad-validation-10410", "mrqa_squad-validation-85", "mrqa_squad-validation-9732", "mrqa_squad-validation-4856", "mrqa_squad-validation-2497", "mrqa_squad-validation-9488", "mrqa_squad-validation-3516", "mrqa_newsqa-validation-911", "mrqa_naturalquestions-validation-1802", "mrqa_triviaqa-validation-1415", "mrqa_searchqa-validation-187", "mrqa_hotpotqa-validation-3155"], "SR": 0.671875, "CSR": 0.703125, "EFR": 0.9523809523809523, "Overall": 0.8277529761904762}, {"timecode": 2, "before_eval_results": {"predictions": ["Body of Proof", "estimated 16,000 to 35,000", "second", "phagocytes", "Jochi", "Alsace", "West Lothian question", "Wiesner", "representatives elected to either house of parliament", "patient care skills", "trial division", "August 2004", "noble brother or blood brother", "merchantnought", "non-discriminatory, \"justified by imperative requirements in the general interest\" and proportionately applied", "lower sixth", "2002", "Organizational", "the number of social services that people can access wherever they move", "cytokines", "The majority may be powerful but it is not necessarily right", "civil disobedience", "eighteenth century", "glaucophyte", "jellyfish", "existing level of inequality", "well before Braddock's departure for North America", "un earned property income", "Hughes Hotel", "Golden Gate Bridge", "Annual Status of Education Report", "seven", "Pauli exclusion principle", "1 September 1939", "Mexico", "Battle of Dalan Balzhut", "relative", "Russell T Davies", "Innate", "1875", "photosynthesis", "private research university", "article 49", "wine", "Arabic numerals", "the application of electricity", "bilaterians", "risen with increased income inequality", "life on Tyneside", "student-teacher relationships", "external combustion engines", "that each side is capable of performing the obligations set out", "the Russian defense ministry said Wednesday.Russia's Tupolev TU-160, pictured here in 2003, is a long-range strategic bomber.", "\"wipe out\" the United States if provoked", "Wake County, it lies just north of the state capital, Raleigh", "Speaker of the House of Representatives shall, upon his resignation as Speaker and as Representative in Congress, act as President", "were the first ever winners of the FA Cup, in 1872", "communion", "Robert Noyce", "Eliot Spitzer", "from local pubs got him dubbed \"Beer Drinker of the Year 2002\"", "often includes fasting", "\"big ringing it\"", "rambling thoughts"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7540010476389553}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8000000000000002, 0.0, 0.88, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.8695652173913044, 0.4166666666666667, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5928", "mrqa_squad-validation-9697", "mrqa_squad-validation-8909", "mrqa_squad-validation-6106", "mrqa_squad-validation-3378", "mrqa_squad-validation-4427", "mrqa_squad-validation-6970", "mrqa_squad-validation-7514", "mrqa_squad-validation-2724", "mrqa_squad-validation-10428", "mrqa_newsqa-validation-3489", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-3648", "mrqa_triviaqa-validation-6170", "mrqa_triviaqa-validation-2265", "mrqa_hotpotqa-validation-1174", "mrqa_searchqa-validation-9740", "mrqa_searchqa-validation-12652", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-4178"], "SR": 0.6875, "CSR": 0.6979166666666667, "EFR": 0.95, "Overall": 0.8239583333333333}, {"timecode": 3, "before_eval_results": {"predictions": ["New Orangery", "parish churches", "Michael Mullett", "Tibetan Buddhism", "North American", "phagosome", "the Smalcald Articles", "beg his son to return home", "1788", "The Rankine cycle", "Several thousand", "Get Carter", "the source of most of the chemical energy released", "the college", "bones", "Roger Goodell", "fever, cough, and blood-tinged sputum", "Apollo 17", "1562", "cilia", "a majority of all MEPs (not just those present) to block or suggest changes", "Blum complexity axioms", "the Diffie\u2013Hellman key exchange", "America's Funniest Home Videos", "16", "seven-layer OSI-compliant networking protocol", "the Rhine extended its watershed southward", "0 \u00b0C (32 \u00b0F)", "May 1754", "infected corpses", "2002", "Australian public X.25 network operated by Telstra", "even greater inequality", "the Association of American Universities", "economic utility in society from resources devoted on high-end consumption", "cut in half", "uncertain", "1835", "720p high definition", "mainline Protestant Methodist denomination", "comb-bearing", "Tate Britain", "CBS", "Treaty of Rome 1957 and the Maastricht Treaty 1992", "M. Theo Kearney", "offering a higher wage the best of their labor", "Russian leader Lenin", "most awarded female act of all-time", "the Mayor of the City of New York", "Vishal Bhardwaj", "Congress passed the Chinese Exclusion Act in 1882 which targeted a single ethnic group by specifically limiting further Chinese immigration", "a network connection device", "The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "Antoine Lavoisier", "CeeLo Green", "Ted Stillwell", "Zelaya and Roberto Micheletti", "Chinese nationals", "Sen. Evan Bayh", "robinson", "A.A. Milne", "a type of large cushion", "rural California", "World leaders"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7679789941338855}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.4444444444444444, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4347826086956522, 0.0, 0.8666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 0.42857142857142855, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1146", "mrqa_squad-validation-3530", "mrqa_squad-validation-4814", "mrqa_squad-validation-4074", "mrqa_squad-validation-4675", "mrqa_squad-validation-9103", "mrqa_squad-validation-2914", "mrqa_squad-validation-7502", "mrqa_squad-validation-7300", "mrqa_squad-validation-545", "mrqa_squad-validation-10044", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-2250", "mrqa_triviaqa-validation-1676", "mrqa_newsqa-validation-3041", "mrqa_newsqa-validation-1834", "mrqa_searchqa-validation-4680", "mrqa_searchqa-validation-4405", "mrqa_searchqa-validation-7746"], "SR": 0.6875, "CSR": 0.6953125, "EFR": 1.0, "Overall": 0.84765625}, {"timecode": 4, "before_eval_results": {"predictions": ["9\u201318", "Norman Foster", "Battle of Hastings", "9 October 2006", "Apollo 13", "Robert R. Gilruth", "hermaphroditism and early reproduction", "Moscone Center", "French Navy failed in the 1759 naval battles at Lagos and Quiberon Bay. In one piece of good fortune, some French supply ships managed to depart France, eluding the British blockade of the French coast", "1994", "patients' prescriptions and patient safety issues", "December 12", "June 6, 1951", "three", "John Wesley", "a project that fails to adhere to codes does not benefit the owner", "21 January 1788", "Gryphon", "Italian and French Renaissance", "ESPN Deportes", "1784", "LeGrande", "St. Lawrence", "informal rule", "Golden Super Bowl", "TEU articles 4 and 5", "a Standard Model", "Westinghouse Electric", "Sayyid Abul Ala Maududi", "$5,000,000", "Mike Tolbert", "phagocytes", "glaucophyte", "in case of a shared physical medium (such as radio or 10BASE5), the packets may be delivered according to a multiple access scheme", "the Magnetophon tape recorder", "random access machines", "a trade magazine for the construction industry", "noisiest", "wars", "Ogr\u00f3d Saski", "Zygons", "Dorothy Skerrit", "Stratfor", "The new Touch, now the most popular iPod, will be available in both black and white and get a $30 price cut, to $199 for 8GB of storage", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients", "stashing Room Cannes special on CNN at the following dates and times: Wednesday 27 May: 0230, 1730, Saturday 30 May: 0430, 1800, Sunday 31 May:", "The Abolition of Corporal Punishment Act, 1997 ( Act No. 33 of 1997 ) is an act of the Parliament of South Africa that abolished judicial corporal punishment", "Colonists objected to the Tea Act because they believed that it violated their rights as Englishmen to `` No taxation without representation ''", "Rent", "James Intveld", "Collective Noun", "Lorelei", "Salvador Dal\u00ed", "the Combination Acts", "Ricky Marco", "Frank Sinatra", "Little Richard", "Gerry Adams", "G Kessler", "German Shepherd", "a hat", "Anubis", "Frank Sinatra", "Amy & Chip"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6955425546663492}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12765957446808512, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2105263157894737, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6206896551724138, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 0.0, 0.08333333333333333, 0.5614035087719298, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10295", "mrqa_squad-validation-6877", "mrqa_squad-validation-9810", "mrqa_squad-validation-8643", "mrqa_squad-validation-4658", "mrqa_squad-validation-718", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-3012", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-1226", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-1403", "mrqa_hotpotqa-validation-3653", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-265", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-4712", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-6600"], "SR": 0.65625, "CSR": 0.6875, "EFR": 1.0, "Overall": 0.84375}, {"timecode": 5, "before_eval_results": {"predictions": ["atmospheric", "the constituting General Conference in Dallas, Texas", "central business district", "third", "The mermaid", "Mick Mixon", "1992", "a course of study", "Mansfeld", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "peptidoglycan", "in this life", "15", "in 1017", "84 hours", "Jordan Norwood", "Doritos", "four", "the American Revolutionary War", "the League of Nations", "the temperance movement", "the Albany Congress", "British", "Giovanni Branca", "Tiffany & Co.", "NASA discontinued the manned Block I program", "Andy Warhol", "2011", "five", "The ability to make probabilistic decisions", "the Divine Right of Kings", "teaching", "research, exhibitions and other shows", "case law by the Court of Justice", "the United States Census Bureau", "The Deadly Assassin", "Seventy percent", "Taih\u014d Code (701) and re-stated in the Y\u014dr\u014d Code", "order", "a place for another non-European Union player in Frank Rijkaard's squad", "John McCain", "The National Telecommunications and Information Administration offered a program to help people buy converter boxes that make old TVs work in the new era", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations", "1983", "Geraldine Margaret Agnew - Somerville", "Woodrow Wilson", "December 1886", "1993", "the Federal Reserve System", "Pontiac Matiz", "Chicago", "polio", "The Spanish Armada", "Kinnairdy Castle", "the first", "Heathrow", "Wildhorn, Bricusse and Cuden", "the County Executive", "a man with a fragile false identity", "The Gettysburg Address", "a tilt or a tilting board", "lion", "Sammy Sosa", "tiger's milk"], "metric_results": {"EM": 0.625, "QA-F1": 0.6814411132345914}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, true, false, true, false, false, true, false, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615383, 0.0, 0.08695652173913042, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2675", "mrqa_squad-validation-1877", "mrqa_squad-validation-2408", "mrqa_squad-validation-1061", "mrqa_squad-validation-7288", "mrqa_squad-validation-2961", "mrqa_squad-validation-3971", "mrqa_squad-validation-1824", "mrqa_squad-validation-4260", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-1425", "mrqa_naturalquestions-validation-1975", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-4466", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3228", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-12103", "mrqa_searchqa-validation-11543", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-1203"], "SR": 0.625, "CSR": 0.6770833333333333, "EFR": 1.0, "Overall": 0.8385416666666666}, {"timecode": 6, "before_eval_results": {"predictions": ["Wahhabism or Salafism", "Del\u00fc\u00fcn Boldog", "forceful taking of property", "220 miles", "jiggle TV", "On the Councils and the Church", "a Western Union superintendent", "mid-Eocene", "\"do not disturb\" sign", "bounding", "Maling company", "sunlight", "avionics, telecommunications, and computers", "five", "individuals", "Robert Boyle", "new and enlarged bridges, a shuttle service and/or a tram", "1997", "Presiding Officer", "Michelle Gomez", "Laszlo Babai and Eugene Luks", "Wesleyan Holiness Consortium", "Aristotle", "1894", "average workers", "cholecalciferol", "1524\u201325", "religious", "original series serials", "historical era", "2011", "1665", "closure temperature", "light reactions", "Brown", "ecosolutions", "Les Bleus", "a spurned suitor", "a man's lifeless, naked body", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "10 logarithm", "Dan Stevens, Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Nathan Mack, Ian McKellen, and Emma Thompson", "Sanchez Navarro", "Vincent Price", "Article Two", "2001", "Popeye", "celsius", "Donna Summer", "Vladimir Putin", "Blue Riband", "Vietnam", "Atlantic Coast Conference", "Prince of Cambodia Norodom Sihanouk", "Don Hahn", "Constance M. Burge", "400", "heavy metal", "drake", "Tokyo", "drake", "drake", "Weehawken", "the Chesapeake Bay"], "metric_results": {"EM": 0.71875, "QA-F1": 0.76058836996337}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-9592", "mrqa_squad-validation-4631", "mrqa_squad-validation-3518", "mrqa_squad-validation-6968", "mrqa_squad-validation-7767", "mrqa_newsqa-validation-716", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1643", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-1325", "mrqa_triviaqa-validation-6477", "mrqa_hotpotqa-validation-5703", "mrqa_searchqa-validation-11121", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-12747", "mrqa_searchqa-validation-1435", "mrqa_searchqa-validation-748"], "SR": 0.71875, "CSR": 0.6830357142857143, "EFR": 0.8888888888888888, "Overall": 0.7859623015873016}, {"timecode": 7, "before_eval_results": {"predictions": ["two", "no French regular army troops were stationed in North America", "Ed Mangan", "German", "c1750", "bones", "not an empire", "since 1951", "immunoglobulins and T cell receptors", "average workers", "Brandon McManus", "journal Science", "33 feet (10.1 m)", "88", "monophyletic", "Bible", "largest", "Sports Night", "superheaters", "Mercedes-Benz Superdome", "Wang Zhen", "dioxygen", "1953", "Capitol Hill, Washington, D.C.", "Parliament Square, High Street and George IV Bridge in Edinburgh", "important", "Mnemiopsis", "varied", "the Tesla coil", "National Galleries of Scotland", "The site currently houses three cinemas", "Bright Automotive", "up to $5,600", "wacko", "due to a shortage of landing fields available for practice", "more and more suspicious of the way their business books were being handled", "38", "Because 10 percent of women who have a mammogram could find 100 percent of breast cancers", "only drivers who were Daytona Pole Award winners, former Clash race winners, former Daytona 500 pole winners who competed full - time in 2017, and drivers who qualified for the 2017 Playoffs", "Spain ceded Puerto Rico, along with the Philippines and Guam, then under Spanish sovereignty, to the U.S.", "Aristotle", "Peking", "Dylan Walters", "My Summer Story", "Barry Taylor", "roast goose", "42 All-Time Classics", "New Zealand", "Colorado", "Inigo Jones", "Jonathan Katz", "Flushed Away", "Cuban", "Sam Waterston", "test pilot", "Chief Strategy Officer", "Fernando Rey", "Ulsan, Korea", "the Kaaba", "Dan Cryer", "Sri Lanka", "the Baltic Sea", "Kiss And Kill Killers", "The narwhal"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6481577841619762}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, false, false, true, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.15384615384615383, 0.0, 1.0, 0.0, 0.5365853658536585, 0.125, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-174", "mrqa_squad-validation-9898", "mrqa_squad-validation-8576", "mrqa_squad-validation-3909", "mrqa_squad-validation-2881", "mrqa_squad-validation-2789", "mrqa_squad-validation-5249", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-358", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-283", "mrqa_naturalquestions-validation-5355", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-1689", "mrqa_triviaqa-validation-370", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-720", "mrqa_hotpotqa-validation-64", "mrqa_hotpotqa-validation-4606", "mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-11561", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15236", "mrqa_searchqa-validation-14908"], "SR": 0.59375, "CSR": 0.671875, "EFR": 1.0, "Overall": 0.8359375}, {"timecode": 8, "before_eval_results": {"predictions": ["bars, caf\u00e9s and clubs", "T\u00f6regene Khatun", "Science Magazine", "3.6%", "the Second Republic", "highly diversified", "Beijing", "screw stoking mechanism", "the type of reduction being used", "quickly", "chloroplast division", "historians", "downward pressure", "Ali Shariati", "an algorithm", "Higher Real Gymnasium", "four", "events and festivals", "the Apollo 1 backup crew", "the Ikh Zasag (Great Administration)", "1883", "independent schools", "Sophocles", "rare and desired skills", "One in five", "breaches of law in protest against international organizations and foreign governments.", "dynamo electric machine commutators", "Christianized Yamasee", "six Africans dead", "two", "revelry", "Dan Brown", "tennis", "$250,000", "Matt Kuchar and Bubba Watson", "a pair of compasses", "18th century", "Erica Rivera", "Davos", "breaking the single - season record", "As Chicano studies programs began to be implemented at universities", "a limited period of time", "Portugal", "SpongeBob", "photographer", "The Breakfast Club", "Thames Street", "Columbus Day", "Amerigo Vespucci", "Reginald Engelbach", "Robert \"Bobby\" Germaine, Sr.", "Nip/Tuck", "Manchester United", "John Faso", "The Rite of Spring", "a pioneer", "The Devil's Dictionary", "dwts", "a fever", "Titan", "dwts", "9 to 5", "Thailand", "dwts.com"], "metric_results": {"EM": 0.53125, "QA-F1": 0.621382593101343}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.4, 1.0, 0.5, 1.0, 0.2222222222222222, 0.8333333333333333, 0.0, 0.16, 1.0, 0.16666666666666669, 0.0, 0.0, 0.30769230769230765, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3390", "mrqa_squad-validation-7189", "mrqa_squad-validation-6330", "mrqa_squad-validation-10303", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-2859", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-710", "mrqa_naturalquestions-validation-688", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-431", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-1178", "mrqa_searchqa-validation-12800", "mrqa_searchqa-validation-9818", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-3588", "mrqa_triviaqa-validation-4077"], "SR": 0.53125, "CSR": 0.65625, "EFR": 0.9666666666666667, "Overall": 0.8114583333333334}, {"timecode": 9, "before_eval_results": {"predictions": ["Ed Lee", "non-Mongol physicians", "the Acasta gneiss of the Slave craton in northwestern Canada", "26", "cabin depressurization", "a restaurant situated at a Grade I-listed 16th century merchant's house at 28\u201330 Close", "Super Bowl L", "private", "nineteenth-century cartographic techniques", "democracy", "ten minutes", "balance of parties across Parliament", "very rare", "areas cleared of forest", "a lute", "Chagatai", "Tesla Electric Company", "WABM-DT2", "The Newcastle Beer Festival", "the western end of the second east-west shipping route", "land and housing", "Warsaw University of Technology", "the plague was present somewhere in Europe in every year between 1346 and 1671", "three", "in herring barrels", "Donkey", "Judy Collins", "An aging group of outlaws look for one last big score as the \"traditional\" American West is disappearing around them", "coffee beans", "The Flight to France", "a fracas", "Paul McCartney", "Buffalo Bill", "Moton Field, the Tuskegee Army Air Field,", "120 m ( 390 ft )", "panning", "Marty J. Walsh", "Andrew Gold", "Brooke Wexler", "$657.4 million in North America and $1.528 billion in other countries, for a worldwide total of $2.187 billion", "Caracas", "Vienna", "Wawrinka", "Bear Grylls", "Harry Shearer", "1879", "Dian Fossey", "the E Street Band", "Cyclic Defrost", "Nathan Bedford Forrest", "Annales de chimie et de physique", "Faith", "40 million", "Columbia Records", "last week", "September", "the piracy problem", "Noida, located in the outskirts of the capital New Delhi", "Tuesday", "1960", "Nairobi, Kenya", "John Arthur Johnson", "Vito Corleone", "Mickey Mantle"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7463850933423302}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5234", "mrqa_squad-validation-5137", "mrqa_squad-validation-477", "mrqa_squad-validation-6059", "mrqa_squad-validation-9310", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-1072", "mrqa_naturalquestions-validation-3722", "mrqa_naturalquestions-validation-4547", "mrqa_triviaqa-validation-2082", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-4698", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3503", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-7775"], "SR": 0.65625, "CSR": 0.65625, "EFR": 1.0, "Overall": 0.828125}, {"timecode": 10, "UKR": 0.80078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-1563", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-2957", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3155", "mrqa_hotpotqa-validation-3228", "mrqa_hotpotqa-validation-3259", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-4698", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5391", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-64", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-834", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-283", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3722", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-4108", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5355", "mrqa_naturalquestions-validation-6486", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-7033", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-9685", "mrqa_naturalquestions-validation-9866", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1227", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-2291", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-2928", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3041", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-3882", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-535", "mrqa_newsqa-validation-716", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1072", "mrqa_searchqa-validation-11121", "mrqa_searchqa-validation-11543", "mrqa_searchqa-validation-11561", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-1203", "mrqa_searchqa-validation-12103", "mrqa_searchqa-validation-12652", "mrqa_searchqa-validation-12800", "mrqa_searchqa-validation-1320", "mrqa_searchqa-validation-1435", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15018", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-3588", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4405", "mrqa_searchqa-validation-4680", "mrqa_searchqa-validation-4712", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7672", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7988", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-8999", "mrqa_squad-validation-10027", "mrqa_squad-validation-10044", "mrqa_squad-validation-10090", "mrqa_squad-validation-10103", "mrqa_squad-validation-10106", "mrqa_squad-validation-10125", "mrqa_squad-validation-10136", "mrqa_squad-validation-10192", "mrqa_squad-validation-10211", "mrqa_squad-validation-10223", "mrqa_squad-validation-10293", "mrqa_squad-validation-10295", "mrqa_squad-validation-10299", "mrqa_squad-validation-10303", "mrqa_squad-validation-10309", "mrqa_squad-validation-10338", "mrqa_squad-validation-10346", "mrqa_squad-validation-10428", "mrqa_squad-validation-10430", "mrqa_squad-validation-10438", "mrqa_squad-validation-1061", "mrqa_squad-validation-1123", "mrqa_squad-validation-1146", "mrqa_squad-validation-1187", "mrqa_squad-validation-1211", "mrqa_squad-validation-1218", "mrqa_squad-validation-1226", "mrqa_squad-validation-1253", "mrqa_squad-validation-1277", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1349", "mrqa_squad-validation-1367", "mrqa_squad-validation-1391", "mrqa_squad-validation-1411", "mrqa_squad-validation-143", "mrqa_squad-validation-1530", "mrqa_squad-validation-1530", "mrqa_squad-validation-1531", "mrqa_squad-validation-1539", "mrqa_squad-validation-1584", "mrqa_squad-validation-1611", "mrqa_squad-validation-1644", "mrqa_squad-validation-1645", "mrqa_squad-validation-1657", "mrqa_squad-validation-1664", "mrqa_squad-validation-1690", "mrqa_squad-validation-1695", "mrqa_squad-validation-1720", "mrqa_squad-validation-173", "mrqa_squad-validation-174", "mrqa_squad-validation-1766", "mrqa_squad-validation-1794", "mrqa_squad-validation-1824", "mrqa_squad-validation-1872", "mrqa_squad-validation-1877", "mrqa_squad-validation-1908", "mrqa_squad-validation-1913", "mrqa_squad-validation-1980", "mrqa_squad-validation-1980", "mrqa_squad-validation-2049", "mrqa_squad-validation-2060", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2321", "mrqa_squad-validation-2324", "mrqa_squad-validation-2361", "mrqa_squad-validation-2382", "mrqa_squad-validation-2402", "mrqa_squad-validation-2408", "mrqa_squad-validation-2439", "mrqa_squad-validation-2475", "mrqa_squad-validation-2478", "mrqa_squad-validation-2497", "mrqa_squad-validation-2533", "mrqa_squad-validation-2636", "mrqa_squad-validation-2658", "mrqa_squad-validation-2675", "mrqa_squad-validation-2704", "mrqa_squad-validation-2724", "mrqa_squad-validation-2807", "mrqa_squad-validation-2819", "mrqa_squad-validation-2849", "mrqa_squad-validation-288", "mrqa_squad-validation-2881", "mrqa_squad-validation-2955", "mrqa_squad-validation-2961", "mrqa_squad-validation-3061", "mrqa_squad-validation-3113", "mrqa_squad-validation-3141", "mrqa_squad-validation-3147", "mrqa_squad-validation-3168", "mrqa_squad-validation-3197", "mrqa_squad-validation-3255", "mrqa_squad-validation-3288", "mrqa_squad-validation-3355", "mrqa_squad-validation-3378", "mrqa_squad-validation-3388", "mrqa_squad-validation-3400", "mrqa_squad-validation-3447", "mrqa_squad-validation-3457", "mrqa_squad-validation-3516", "mrqa_squad-validation-3518", "mrqa_squad-validation-3530", "mrqa_squad-validation-3559", "mrqa_squad-validation-3566", "mrqa_squad-validation-3590", "mrqa_squad-validation-3601", "mrqa_squad-validation-3628", "mrqa_squad-validation-3657", "mrqa_squad-validation-3771", "mrqa_squad-validation-3799", "mrqa_squad-validation-38", "mrqa_squad-validation-3806", "mrqa_squad-validation-3813", "mrqa_squad-validation-3898", "mrqa_squad-validation-3909", "mrqa_squad-validation-3915", "mrqa_squad-validation-3916", "mrqa_squad-validation-3942", "mrqa_squad-validation-3971", "mrqa_squad-validation-3986", "mrqa_squad-validation-405", "mrqa_squad-validation-4054", "mrqa_squad-validation-4074", "mrqa_squad-validation-4080", "mrqa_squad-validation-409", "mrqa_squad-validation-4100", "mrqa_squad-validation-4127", "mrqa_squad-validation-4137", "mrqa_squad-validation-4149", "mrqa_squad-validation-4192", "mrqa_squad-validation-42", "mrqa_squad-validation-4260", "mrqa_squad-validation-4262", "mrqa_squad-validation-4320", "mrqa_squad-validation-4332", "mrqa_squad-validation-437", "mrqa_squad-validation-4425", "mrqa_squad-validation-4427", "mrqa_squad-validation-4439", "mrqa_squad-validation-4475", "mrqa_squad-validation-4488", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-453", "mrqa_squad-validation-4629", "mrqa_squad-validation-4642", "mrqa_squad-validation-4658", "mrqa_squad-validation-4675", "mrqa_squad-validation-4676", "mrqa_squad-validation-4701", "mrqa_squad-validation-4711", "mrqa_squad-validation-477", "mrqa_squad-validation-477", "mrqa_squad-validation-4795", "mrqa_squad-validation-4801", "mrqa_squad-validation-4840", "mrqa_squad-validation-4856", "mrqa_squad-validation-4902", "mrqa_squad-validation-4930", "mrqa_squad-validation-5013", "mrqa_squad-validation-503", "mrqa_squad-validation-5063", "mrqa_squad-validation-509", "mrqa_squad-validation-5129", "mrqa_squad-validation-5137", "mrqa_squad-validation-5156", "mrqa_squad-validation-5197", "mrqa_squad-validation-5208", "mrqa_squad-validation-5226", "mrqa_squad-validation-5234", "mrqa_squad-validation-5249", "mrqa_squad-validation-5260", "mrqa_squad-validation-5300", "mrqa_squad-validation-5320", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-545", "mrqa_squad-validation-551", "mrqa_squad-validation-5531", "mrqa_squad-validation-5535", "mrqa_squad-validation-5550", "mrqa_squad-validation-5597", "mrqa_squad-validation-5620", "mrqa_squad-validation-5631", "mrqa_squad-validation-5715", "mrqa_squad-validation-5721", "mrqa_squad-validation-5721", "mrqa_squad-validation-5736", "mrqa_squad-validation-5891", "mrqa_squad-validation-5908", "mrqa_squad-validation-5951", "mrqa_squad-validation-5958", "mrqa_squad-validation-5975", "mrqa_squad-validation-5991", "mrqa_squad-validation-6059", "mrqa_squad-validation-6106", "mrqa_squad-validation-6119", "mrqa_squad-validation-612", "mrqa_squad-validation-6156", "mrqa_squad-validation-6166", "mrqa_squad-validation-6191", "mrqa_squad-validation-6293", "mrqa_squad-validation-6326", "mrqa_squad-validation-6347", "mrqa_squad-validation-6389", "mrqa_squad-validation-6404", "mrqa_squad-validation-6409", "mrqa_squad-validation-6471", "mrqa_squad-validation-6471", "mrqa_squad-validation-6473", "mrqa_squad-validation-6610", "mrqa_squad-validation-6614", "mrqa_squad-validation-6639", "mrqa_squad-validation-6644", "mrqa_squad-validation-6650", "mrqa_squad-validation-6674", "mrqa_squad-validation-6681", "mrqa_squad-validation-6692", "mrqa_squad-validation-6753", "mrqa_squad-validation-6759", "mrqa_squad-validation-677", "mrqa_squad-validation-6810", "mrqa_squad-validation-6813", "mrqa_squad-validation-6877", "mrqa_squad-validation-6889", "mrqa_squad-validation-6896", "mrqa_squad-validation-694", "mrqa_squad-validation-6968", "mrqa_squad-validation-6970", "mrqa_squad-validation-6978", "mrqa_squad-validation-6988", "mrqa_squad-validation-6990", "mrqa_squad-validation-7029", "mrqa_squad-validation-7086", "mrqa_squad-validation-7133", "mrqa_squad-validation-7154", "mrqa_squad-validation-718", "mrqa_squad-validation-7189", "mrqa_squad-validation-7233", "mrqa_squad-validation-7270", "mrqa_squad-validation-7288", "mrqa_squad-validation-7293", "mrqa_squad-validation-7300", "mrqa_squad-validation-739", "mrqa_squad-validation-742", "mrqa_squad-validation-7446", "mrqa_squad-validation-7466", "mrqa_squad-validation-7490", "mrqa_squad-validation-7502", "mrqa_squad-validation-7504", "mrqa_squad-validation-7508", "mrqa_squad-validation-7526", "mrqa_squad-validation-754", "mrqa_squad-validation-7563", "mrqa_squad-validation-7609", "mrqa_squad-validation-7653", "mrqa_squad-validation-7707", "mrqa_squad-validation-7718", "mrqa_squad-validation-7726", "mrqa_squad-validation-7727", "mrqa_squad-validation-7731", "mrqa_squad-validation-7744", "mrqa_squad-validation-7751", "mrqa_squad-validation-7767", "mrqa_squad-validation-778", "mrqa_squad-validation-7789", "mrqa_squad-validation-7813", "mrqa_squad-validation-7926", "mrqa_squad-validation-794", "mrqa_squad-validation-7945", "mrqa_squad-validation-7954", "mrqa_squad-validation-7997", "mrqa_squad-validation-8107", "mrqa_squad-validation-811", "mrqa_squad-validation-8154", "mrqa_squad-validation-8204", "mrqa_squad-validation-8212", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8269", "mrqa_squad-validation-8278", "mrqa_squad-validation-83", "mrqa_squad-validation-8350", "mrqa_squad-validation-8409", "mrqa_squad-validation-8412", "mrqa_squad-validation-8443", "mrqa_squad-validation-85", "mrqa_squad-validation-8500", "mrqa_squad-validation-8575", "mrqa_squad-validation-8576", "mrqa_squad-validation-8617", "mrqa_squad-validation-8643", "mrqa_squad-validation-8649", "mrqa_squad-validation-8658", "mrqa_squad-validation-8695", "mrqa_squad-validation-8779", "mrqa_squad-validation-8871", "mrqa_squad-validation-8909", "mrqa_squad-validation-8917", "mrqa_squad-validation-9038", "mrqa_squad-validation-9103", "mrqa_squad-validation-916", "mrqa_squad-validation-9189", "mrqa_squad-validation-930", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-934", "mrqa_squad-validation-9376", "mrqa_squad-validation-9378", "mrqa_squad-validation-9405", "mrqa_squad-validation-9438", "mrqa_squad-validation-9465", "mrqa_squad-validation-9476", "mrqa_squad-validation-9488", "mrqa_squad-validation-9498", "mrqa_squad-validation-9505", "mrqa_squad-validation-9525", "mrqa_squad-validation-9575", "mrqa_squad-validation-9590", "mrqa_squad-validation-9592", "mrqa_squad-validation-9596", "mrqa_squad-validation-9628", "mrqa_squad-validation-9717", "mrqa_squad-validation-9731", "mrqa_squad-validation-9732", "mrqa_squad-validation-975", "mrqa_squad-validation-9762", "mrqa_squad-validation-9776", "mrqa_squad-validation-9787", "mrqa_squad-validation-9810", "mrqa_squad-validation-9853", "mrqa_squad-validation-9859", "mrqa_squad-validation-9898", "mrqa_squad-validation-9920", "mrqa_squad-validation-9962", "mrqa_triviaqa-validation-1415", "mrqa_triviaqa-validation-1489", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-1675", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1867", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-2106", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-3050", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-333", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-370", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4846", "mrqa_triviaqa-validation-512", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-5984", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6170", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7472", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-770", "mrqa_triviaqa-validation-7735", "mrqa_triviaqa-validation-7775"], "OKR": 0.890625, "KG": 0.43671875, "before_eval_results": {"predictions": ["Article 5", "0.3 to 0.6 \u00b0C", "lawful protest demonstration, nonviolent civil disobedience, and violent civil disobedience", "Meredith Vieira", "extremely difficult, if not impossible", "london", "5,000 years of art", "Utopia", "hydrogen and helium", "Central Pacific Railroad", "equality", "Jan Andrzej Menich", "private conferences", "The View and The Chew", "Louis Adamic", "Roger Goodell", "lower bounds", "over 90", "message routing methodology", "as soon as they enter into force", "Sakya", "1688\u20131692", "French and Belgian delegates urged occupying the Ruhr as a way of forcing Germany to pay more, while the British delegate urged a lowering of the payments", "first to develop lethal injection as a method of execution, which has since been adopted by five other countries", "a major fall in stock prices", "claims handler", "Dasharatha", "Grand Inquisition", "more than a million members", "disputes between two or more states", "soybean", "the shoulder", "Orson Welles", "Razor", "the plains bison", "bitter almond", "show business", "Gloucestershire", "Wilhelmus Simon Petrus Fortuijn", "Mr. Tumnus", "Countess of Lovelace", "703", "Mauritian", "Yubin, Yeeun", "music genres of electronic rock, electropop and R&B", "Hawaii", "Bill Gates & Melinda Gates Foundation", "56", "Jared Polis", "gehland", "Seminole", "girls around 11 or 12", "last week", "sculptures", "london", "london", "the French Revolution", "lumbini", "gehon", "london", "laura l. Gold Flakes", "Anthony trollope", "london", "Thelma & Louise"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6244047619047619}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.12500000000000003, 0.5, 0.0, 0.5, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.125, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6811", "mrqa_squad-validation-5505", "mrqa_squad-validation-5494", "mrqa_squad-validation-3667", "mrqa_squad-validation-4588", "mrqa_squad-validation-1568", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-8092", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-2213", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-4133", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-3504", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-3890", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-7853", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-201"], "SR": 0.53125, "CSR": 0.6448863636363636, "EFR": 1.0, "Overall": 0.7546022727272728}, {"timecode": 11, "before_eval_results": {"predictions": ["the expulsion of the Acadians", "Sinclair", "1974", "the Uighurs of the Kingdom of Qocho", "15", "Invocavit Sermons", "1899\u20131900", "microorganisms", "Ealy", "Von Miller", "10", "23.9%", "cattle and citrus", "establish, equip, manage and maintain national and public libraries in the country", "In the 1060s", "1862", "William the Lion", "external combustion engines", "Ten", "a proper legal basis", "a Islamic shrine", "the utopian novels of H.G. Wells", "Albert Einstein", "drizzle, rain, sleet, snow, graupel and hail", "Richard Wright and non-lexical vocals by Clare Torry", "September 8, 2017", "( Schwarzenegger ) and his companion, the thief Malak ( Walter )", "In the 1920s", "Haiti", "Jackie Robinson", "James Brown", "a bear (losing) market which is a normal infrequent part of long  term investing", "Darby and Joan", "Bronx Mowgli", "the Basque separatist organization, the ETA (Euskadi Ta Askatasuna)", "Jules Verne", "the United Kingdom and China", "the Cheshire Phoenix", "Gweilo", "Nick on Sunset theater in Hollywood", "various", "2006", "1943", "The Design Inference", "from 1836 until his death in 1864", "John Lennon and George Harrison", "if Israel's conviction is correct that Iran could develop a nuclear weapon and it is close to achieving that desire, does that mean it would use it against Israel?", "One of Osama bin Laden's sons", "the inspector-general", "seven", "through Greece, the birthplace of the Olympics, before being transported to Canada", "Jean Van de Velde", "a drug test after taking a medicine that contained the banned substance cortisone", "New Yankee Workshop", "(Dr. Walter Reed, MD, MA, LLD - Gloucester County Virginia", "Like a Rock", "Indiana", "Canada", "Edgar Rice Burroughs", "Frank Lloyd Wright", "a submarine", "Russia", "James Lillywhite, Alfred Shaw and Arthur Shrewsbury", "Lesley Garrett"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6210227272727273}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.25, 1.0, 0.13333333333333333, 0.2222222222222222, 0.0, 0.0, 0.16666666666666669, 0.0, 0.18181818181818182, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6060", "mrqa_squad-validation-1437", "mrqa_squad-validation-2705", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-7715", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-3588", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-4171", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-5891", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-1654", "mrqa_searchqa-validation-16321", "mrqa_searchqa-validation-13474", "mrqa_searchqa-validation-10138", "mrqa_searchqa-validation-13198", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2684"], "SR": 0.546875, "CSR": 0.63671875, "EFR": 1.0, "Overall": 0.75296875}, {"timecode": 12, "before_eval_results": {"predictions": ["The Knowledge School", "OneDrive", "\"Into your hand I commit my spirit; you have redeemed me, O Lord, faithful God\" (Ps. 31:5)", "September 5, 1985", "62", "Jean Ribault", "James Bryant Conant", "journalism", "Western", "a green algal derived chloroplast", "electromagnetic", "in the kingdom", "Chicago Bears", "observer", "requiring his arrest", "giving her brother Polynices a proper burial", "26", "against governmental entities.", "Iowa", "Justin Timberlake", "of gold", "Curtis Armstrong", "Hem Chandra Bose", "Parker's pregnancy", "of Paris", "Afonso IV", "len\u00e9 Lavan", "ostrich", "Jean Alexander", "John Enoch Powell", "the British Royal Air Force", "Haystacks", "McDonnell Douglas", "3000m", "John Masefield", "1966", "The Prodigy", "\u00c6thelstan", "Sean Yseult", "the Salzburg Festival", "Les Miles", "World War I", "County of York", "Charles Quinton Murphy", "Floridians", "of Haiti", "to step down as majority leader.", "Wednesday", "of sumo wrestling", "a African-American woman", "seven", "could see the lifeboat where pirates have been holding Capt. Richard Phillips", "Araceli Valencia", "Sabina Guzzanti", "Lois", "students", "Wild Bill Hickok", "Mario Puzo", "united Nations", "bicarbonate", "bashing", "Salieri", "Rudyard Kipling", "Bangladesh"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6502409596981966}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.1818181818181818, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.923076923076923, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2550", "mrqa_squad-validation-249", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-9939", "mrqa_naturalquestions-validation-1925", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-5120", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-59", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-633", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-1146", "mrqa_searchqa-validation-14081"], "SR": 0.578125, "CSR": 0.6322115384615384, "EFR": 0.9629629629629629, "Overall": 0.7446599002849001}, {"timecode": 13, "before_eval_results": {"predictions": ["that \"the ones who are violating the greater law are the members of the Navy\"", "can produce both eggs and sperm at the same time.", "Necessity-based entrepreneurship", "Tesla coil", "2008", "spin", "Capability deprivation", "San Jose State", "1954", "Rhenus", "Xbox One", "the bishop has read the appointments at the session of the Annual Conference", "1996", "Italian Plague of 1629\u20131631", "Northumbria University", "calcitriol", "2015", "Merrimen", "Andy Serkis", "Robin", "Andrew Johnson", "2012", "1807", "1975", "England", "enemy", "16.5 feet", "Avestan z\u0101\u014d", "yellow", "second", "Nigel Short", "judo", "Manchester", "leprosy", "Harris", "Ouse and Foss", "Basilica", "Government of Ireland", "Mathieu Kassovitz", "Moonstruck", "Slaughterhouse-Five", "three", "December 24, 1973", "1865", "the wars in Iraq and Afghanistan", "almost 100", "his mother", "second", "Ozzy Osbourne", "Bob Dole", "composer", "strife in Somalia", "105", "fibula", "pASTA", "Newton", "Trinity", "Brigham Young", "China", "Cardinal Roger Mahony", "Korea", "Sideways", "Romance language", "Hawaii Republican"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7136904761904762}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, false, false, true, true, true, false, false, true, true, false], "QA-F1": [0.14285714285714288, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-6735", "mrqa_squad-validation-4645", "mrqa_squad-validation-9247", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-74", "mrqa_triviaqa-validation-6585", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-5153", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1811", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-2453", "mrqa_searchqa-validation-8451", "mrqa_searchqa-validation-3849", "mrqa_searchqa-validation-6636", "mrqa_searchqa-validation-12056", "mrqa_hotpotqa-validation-5808"], "SR": 0.671875, "CSR": 0.6350446428571428, "EFR": 0.9523809523809523, "Overall": 0.7431101190476189}, {"timecode": 14, "before_eval_results": {"predictions": ["as far back as the early Cambrian, about 515 million years ago.", "tentilla", "a problem instance", "Knights Templar", "Algeria", "Edinburgh Pentlands", "828,000", "1206", "thylakoid", "WLS", "about half of Naples' 300,000 inhabitants.", "at other locations throughout Scotland.", "in Algeria.", "Cadeby stone", "colonizing empires", "9 February 2018", "Government House at New Delhi", "until the 1960s", "19 October -- 22 November 1914", "Skat", "Governor Al Smith", "During Hanna's recovery masquerade celebration", "John Young", "Reuben Kincaid", "lighter fluid", "\"Mr Loophole\"", "Homo floresiensis.", "Humber", "1881", "NASCAR", "Organisation", "Roger Black", "Nova Scotia", "Eee-haw", "Tampa", "Mineola", "Sofia the First", "Hong Kong", "was an American painter and writer who wrote the autobiography \"The Bite in the Apple\" about her relationship with Apple co-founder Steve Jobs.", "Hong Kong", "Chief of the Operations Staff of the Armed Forces High Command", "Massapequa", "Kansas City crime family", "iPods", "death squad killings carried out during his rule in the 1990s.", "Ken Choi", "Esteban Alvarado", "Garth Brooks", "1,700 year old Roman mosaic", "nuclear weapon.", "at least nine", "it has not intercepted any", "John Bunyan", "Steven Spielberg", "koan", "Diogenes", "Intira Gandhi", "Daniel Defoe", "the Louvre", "Catherine the Great", "Fairness Doctrine", "on a remote highway in Michoacan state", "the abduction of minors.", "an Italian and six Africans"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6520258387445887}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, true, false, true, true], "QA-F1": [0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.7499999999999999, 1.0, 0.0, 0.7272727272727272, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.1904761904761905, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.22222222222222224, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4474", "mrqa_squad-validation-9480", "mrqa_squad-validation-9680", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-7754", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-7154", "mrqa_hotpotqa-validation-3620", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2383", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3538", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1675", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-12292", "mrqa_newsqa-validation-2821"], "SR": 0.53125, "CSR": 0.628125, "EFR": 0.9333333333333333, "Overall": 0.7379166666666667}, {"timecode": 15, "before_eval_results": {"predictions": ["the 1980s", "Fresno", "a net", "energize electrons", "if they arrest fully informed jury leafleters, the leaflets will have to be given to the leafleter's own jury as evidence.", "20th", "Lower Lorraine", "wave speeds", "three", "steady stream", "a system of many biological structures and processes within an organism that protects against disease", "1606", "1951", "denunciation", "the Central Intelligence Agency", "pale ale", "George Bernard Shaw", "Clarence Thomas", "Washington Irving", "Nickelodeon", "Brooklyn", "David Lee Roth", "Abraham Lincoln", "Under normal conditions", "200,564, with an estimated population of 204,408 in 2013.", "the same number of electron shells.", "June 27, 2008", "a specific task", "the Gospel of Matthew in the middle of the Sermon on the Mount", "united states", "Staci Keanan", "egg through parthenogenesis", "Donna", "dice", "Elizabeth Taylor", "Western Australia", "Sergeant-Major Bullimore", "Green Bay", "perfume", "1882", "authorizing a grant", "Cambridge, Massachusetts", "Dick Turpin", "10th Cavalry Regiment", "Central Park", "early 20th-century Europe", "2,664", "Anna Clyne", "Caesars Entertainment Corporation", "13 May 2018", "Dhivehi Raa'jeyge Jumhooriyya \"", "Sam the Sham", "Lalit", "elephants, and only a handful of media members are able to visit each year, in an effort to make the animals' lives as natural as possible.", "15", "the Ministry of Defense", "Kenneth Cole", "anaphylaxis,", "10 to 15 percent", "was back on the set at \"E! News\" on Tuesday.", "he would legally be able to intervene in the case if it is transferred from a judge in the eastern city of Abeche, where the children were taken", "Troy Livesay", "in the Angeles National Forest", "Jaime Andrade"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6143262987012987}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5454545454545454, 0.5714285714285715, 0.0, 0.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3602", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-14881", "mrqa_searchqa-validation-8463", "mrqa_searchqa-validation-3026", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-9852", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-1193", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-7215", "mrqa_triviaqa-validation-5920", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-7010", "mrqa_triviaqa-validation-4180", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-1016", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-67", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-3621"], "SR": 0.515625, "CSR": 0.62109375, "EFR": 1.0, "Overall": 0.7498437499999999}, {"timecode": 16, "before_eval_results": {"predictions": ["Rhin", "4", "chloroplasts", "neuronal dendrites", "evenly round the body", "Switzerland", "BSkyB", "British", "James E. Webb", "nitroaereus", "tidal delta", "DC traction motor", "the west bank", "a \"royal fork\"", "(Emma) Thompson", "eggshells", "President Nixon", "mass mass libraries", "massachusetts", "Cleveland", "Delaware", "The Ruged-A thriller", "alveolar process", "a snappy and spiteful tone", "medical abnormalities, activation level, or recruitment order, or to analyze the biomechanics of human or animal movement", "moral", "Kanawha River", "Luke Luke 18 : 1 - 8", "brothers Henry, Jojo and Ringo Garza", "Mohammad Reza Pahlavi", "Pakistan", "H.G. Wells", "saccharides", "the Bank of England", "Brussels", "Gary Oldman", "algae", "John Carpenter", "Midnight Cowboy", "E in A-Level art", "two days", "Vienna", "Rice University", "Mexico", "homebrew", "September 29, 2017", "postmodern schools", "five", "2006", "the Salzburg Festival", "basketball", "Hibbing, Minnesota", "200", "The word \"tuatara\" is derived from a Maori word meaning \"spiny back.\"", "5,600", "the California Highway Patrol.", "a 57-year old male", "Citizens are picking members of the lower house of parliament,", "\"Empire of the Sun\"", "a Yemeni cleric and his personal assistant", "Jacob", "(Jared) Polis", "Herbert Hoover", "two reservoirs in the eastern Catskill Mountains"], "metric_results": {"EM": 0.453125, "QA-F1": 0.520776098901099}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.888888888888889, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6153846153846153, 1.0, 0.14285714285714288, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10416", "mrqa_squad-validation-8792", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-2874", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-7115", "mrqa_searchqa-validation-11438", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-7705", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-713", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-4263", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-307", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-813", "mrqa_newsqa-validation-3174"], "SR": 0.453125, "CSR": 0.6112132352941176, "EFR": 1.0, "Overall": 0.7478676470588235}, {"timecode": 17, "before_eval_results": {"predictions": ["certifying, governing and enforcing the standards of practice for the teaching profession", "SAP Center in San Jose", "\"ash tree\"", "$5 million", "the number of foot-pounds of work delivered by burning one bushel (94 pounds) of coal", "mercuric oxide (HgO)", "Pacific", "Esel (\"Donkey\")", "a modern canalized section", "John Fox", "Sava Kosanovi\u0107", "the Twin Towers", "Alaska", "Spain and Portugal", "the 19th century", "klezmer", "Tiffany", "a locking pin", "Edith Wharton", "morose", "Indira Gandhi", "property", "the members of the actual club with the parading permit as well as the brass band", "the base of the right ventricle", "Indian Standard Time", "the Allied victory in World War II,", "Road / Track", "1920", "the need to repent in time", "the intersection of Del Monte Blvd and Esplanade Street", "used as a pH indicator, a color marker, and a dye", "William Chatterton Dix", "fish", "Austria", "The History Boys", "the Gambia", "the Kentucky Derby", "the Netherlands", "Robert Stroud", "William Lamb", "Malcolm Bradbury", "Pat Cash", "chalk quarry", "John II Casimir Vasa", "200,167", "American", "The Social Network", "2001", "Abbey Road", "the oil platforms in the North Sea", "Dan Brandon Bilzerian", "YouTube", "jobs up and down the auto supply chain", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "\"peregruzka\"", "Nicole", "breast cancer.", "the Cowardly Lion", "4 meters (13 feet) high", "the auto supply chain", "more than 1.2 million people", "Thabo Mbeki", "George I", "wake"], "metric_results": {"EM": 0.53125, "QA-F1": 0.634616267107076}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, false, false, false, false, false, true, true], "QA-F1": [0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.1, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.45454545454545453, 0.9411764705882353, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.8, 1.0, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.888888888888889, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2022", "mrqa_squad-validation-457", "mrqa_squad-validation-4634", "mrqa_squad-validation-3300", "mrqa_squad-validation-9190", "mrqa_searchqa-validation-8891", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-13117", "mrqa_searchqa-validation-5201", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-9094", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-7849", "mrqa_triviaqa-validation-6384", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-3931", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-2353", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-1380"], "SR": 0.53125, "CSR": 0.6067708333333333, "EFR": 0.9666666666666667, "Overall": 0.7403124999999999}, {"timecode": 18, "before_eval_results": {"predictions": ["if the Treaty provisions have a direct effect and they are sufficiently clear, precise and unconditional", "ownership of private industries", "tentilla", "Levi's Stadium", "a drug treatment for an individual", "Charlesfort", "2000", "five", "24 September 2007", "Infrastructure", "Sylvia F. Porter", "Missouri River", "the 1960s.", "a bow bridge with 16 arches shielded by ice guards", "more than 80 tank\u014dbon volumes", "on the southeastern coast of the Commonwealth of Virginia in the United States", "Gregor Mendel", "United Nations Peacekeeping Operations", "Certificate of Release or Discharge from Active Duty", "Latitude", "20 locations all within the Pittsburgh metropolitan area", "Hyundai", "vis melbourne melbourne", "Mexico", "group", "John Barbirolli", "Tahrir Square", "formic acid", "100 years", "Paul C\u00e9zanne", "a litter of pipes", "Germany", "Philip Livingston", "The Hawai\u02bbi State Senate is the upper chamber of the Hawaii State Legislature.", "DreamWorks Animation", "Humphrey Goodman", "London", "1926", "Lindka Rosalind Wanda Cierach", "Nebraska Cornhuskers women's basketball team", "Wu-Tang Clan", "$125.4 million", "two", "Caylee, who was 2", "Beijing", "11", "mental health treatment", "from Amsterdam, in the Netherlands, to Ankara, Turkey", "October 19", "an open window that fits neatly around him", "maintain an \"aesthetic environment\" and ensure public safety", "This syndicated TV show about show biz", "the Cricket in Times Square", "R2-D2", "Meet the Press", "dampers", "Samuel Burl \"Sam\" Kinison", "The Vandellas", "Sisyphus", "The New Adventures of Old Christine", "Elvis Presley", "Elizabeth Taylor", "fish", "Viking feet"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6938829399766899}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, false, true, false, true, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.7499999999999999, 1.0, 1.0, 0.0, 0.46153846153846156, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.8, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-6282", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-3721", "mrqa_triviaqa-validation-6670", "mrqa_triviaqa-validation-5122", "mrqa_triviaqa-validation-712", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-5222", "mrqa_hotpotqa-validation-3773", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-999", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-1889", "mrqa_newsqa-validation-3858", "mrqa_searchqa-validation-6221", "mrqa_searchqa-validation-5488", "mrqa_searchqa-validation-13223", "mrqa_searchqa-validation-7989", "mrqa_searchqa-validation-10860", "mrqa_searchqa-validation-7646", "mrqa_triviaqa-validation-3215"], "SR": 0.578125, "CSR": 0.6052631578947368, "EFR": 0.9629629629629629, "Overall": 0.73927022417154}, {"timecode": 19, "before_eval_results": {"predictions": ["Brandon McManus", "the Compromise of 1850", "1884", "Seven Stories", "1332", "Morgan", "Space", "rococo", "12.5", "Florence", "Canada goose", "the American Civil War", "King Edward III", "a plant", "Star Trek", "the circulatory system", "Yitzhak Rabin", "A Narrative of Adventures in the South Seas", "1", "Providence", "California", "landowner", "July 14, 2017", "at birth", "Chris Rea", "viking Calvano", "outside cultivated areas", "the United States", "senior enlisted sailor", "the buttock", "Ledger", "Mexico", "Funchal", "Geena Davis", "puffer skippers", "a text-based MUD", "eagle", "a board that has lines and pads that connect various points together", "Gaston Leroux", "Whitney Houston", "a number of spots", "Scotland", "local South Australian and Australian produced content", "basketball", "Animorphs", "MMA", "Vic Chesnutt", "William Douglas", "The Timekeeper", "Julia Gillard", "poetry, theater, art, music, the media, and books", "east\u2013west United States highway", "Native American tribes", "CNN's best ten golf movies ever made", "contact us", "onto the college campus.", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "Yemen", "the second missing person", "American icon's", "of a 15-year-old boy", "the use of torture and indefinite detention", "at least 25", "Fareed Zakaria"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5706330128205128}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5005", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-15149", "mrqa_searchqa-validation-6804", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-10378", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-7443", "mrqa_triviaqa-validation-1838", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-796", "mrqa_hotpotqa-validation-5253", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-3806", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-3891"], "SR": 0.53125, "CSR": 0.6015625, "EFR": 0.9666666666666667, "Overall": 0.7392708333333333}, {"timecode": 20, "UKR": 0.80859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2383", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-2821", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-2957", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3082", "mrqa_hotpotqa-validation-3166", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3620", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3773", "mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-4282", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4513", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-4685", "mrqa_hotpotqa-validation-4698", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5253", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-5891", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4410", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7705", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7754", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-8137", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-8754", "mrqa_naturalquestions-validation-8862", "mrqa_naturalquestions-validation-8883", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9852", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1889", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-2016", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-2127", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2353", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-3676", "mrqa_newsqa-validation-3719", "mrqa_newsqa-validation-3882", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-535", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-716", "mrqa_newsqa-validation-726", "mrqa_newsqa-validation-813", "mrqa_newsqa-validation-90", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-97", "mrqa_newsqa-validation-980", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10138", "mrqa_searchqa-validation-10364", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-1146", "mrqa_searchqa-validation-11543", "mrqa_searchqa-validation-12652", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-13198", "mrqa_searchqa-validation-13920", "mrqa_searchqa-validation-13987", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-14795", "mrqa_searchqa-validation-14908", "mrqa_searchqa-validation-15018", "mrqa_searchqa-validation-15057", "mrqa_searchqa-validation-15552", "mrqa_searchqa-validation-16581", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-187", "mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-2534", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-2996", "mrqa_searchqa-validation-3026", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3588", "mrqa_searchqa-validation-410", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-4551", "mrqa_searchqa-validation-5005", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5169", "mrqa_searchqa-validation-5201", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7672", "mrqa_searchqa-validation-7692", "mrqa_searchqa-validation-7988", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-8463", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-9748", "mrqa_searchqa-validation-9818", "mrqa_squad-validation-10087", "mrqa_squad-validation-10102", "mrqa_squad-validation-10103", "mrqa_squad-validation-10192", "mrqa_squad-validation-1021", "mrqa_squad-validation-10223", "mrqa_squad-validation-10293", "mrqa_squad-validation-10299", "mrqa_squad-validation-10303", "mrqa_squad-validation-10307", "mrqa_squad-validation-10309", "mrqa_squad-validation-10319", "mrqa_squad-validation-10428", "mrqa_squad-validation-10436", "mrqa_squad-validation-10438", "mrqa_squad-validation-1061", "mrqa_squad-validation-1176", "mrqa_squad-validation-1277", "mrqa_squad-validation-1299", "mrqa_squad-validation-132", "mrqa_squad-validation-1349", "mrqa_squad-validation-1410", "mrqa_squad-validation-143", "mrqa_squad-validation-1530", "mrqa_squad-validation-1539", "mrqa_squad-validation-1577", "mrqa_squad-validation-1584", "mrqa_squad-validation-1611", "mrqa_squad-validation-1644", "mrqa_squad-validation-1645", "mrqa_squad-validation-1688", "mrqa_squad-validation-1766", "mrqa_squad-validation-1767", "mrqa_squad-validation-1787", "mrqa_squad-validation-1850", "mrqa_squad-validation-1877", "mrqa_squad-validation-1980", "mrqa_squad-validation-2049", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2382", "mrqa_squad-validation-2408", "mrqa_squad-validation-2478", "mrqa_squad-validation-2489", "mrqa_squad-validation-2497", "mrqa_squad-validation-2533", "mrqa_squad-validation-2636", "mrqa_squad-validation-2658", "mrqa_squad-validation-2704", "mrqa_squad-validation-2709", "mrqa_squad-validation-2810", "mrqa_squad-validation-2819", "mrqa_squad-validation-2854", "mrqa_squad-validation-2955", "mrqa_squad-validation-2956", "mrqa_squad-validation-299", "mrqa_squad-validation-3061", "mrqa_squad-validation-3113", "mrqa_squad-validation-3141", "mrqa_squad-validation-3147", "mrqa_squad-validation-3168", "mrqa_squad-validation-3197", "mrqa_squad-validation-3300", "mrqa_squad-validation-3302", "mrqa_squad-validation-3447", "mrqa_squad-validation-3516", "mrqa_squad-validation-3518", "mrqa_squad-validation-3530", "mrqa_squad-validation-3590", "mrqa_squad-validation-3601", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3628", "mrqa_squad-validation-3667", "mrqa_squad-validation-3806", "mrqa_squad-validation-3812", "mrqa_squad-validation-3829", "mrqa_squad-validation-3898", "mrqa_squad-validation-3909", "mrqa_squad-validation-4054", "mrqa_squad-validation-4063", "mrqa_squad-validation-4074", "mrqa_squad-validation-409", "mrqa_squad-validation-4097", "mrqa_squad-validation-4121", "mrqa_squad-validation-4137", "mrqa_squad-validation-4142", "mrqa_squad-validation-4173", "mrqa_squad-validation-42", "mrqa_squad-validation-4260", "mrqa_squad-validation-4262", "mrqa_squad-validation-437", "mrqa_squad-validation-4439", "mrqa_squad-validation-453", "mrqa_squad-validation-457", "mrqa_squad-validation-4623", "mrqa_squad-validation-4631", "mrqa_squad-validation-4642", "mrqa_squad-validation-4676", "mrqa_squad-validation-4772", "mrqa_squad-validation-4840", "mrqa_squad-validation-4856", "mrqa_squad-validation-49", "mrqa_squad-validation-4954", "mrqa_squad-validation-4957", "mrqa_squad-validation-509", "mrqa_squad-validation-5129", "mrqa_squad-validation-5137", "mrqa_squad-validation-5156", "mrqa_squad-validation-5197", "mrqa_squad-validation-5211", "mrqa_squad-validation-5229", "mrqa_squad-validation-526", "mrqa_squad-validation-5272", "mrqa_squad-validation-5477", "mrqa_squad-validation-5492", "mrqa_squad-validation-5505", "mrqa_squad-validation-551", "mrqa_squad-validation-5550", "mrqa_squad-validation-5592", "mrqa_squad-validation-5631", "mrqa_squad-validation-5721", "mrqa_squad-validation-5758", "mrqa_squad-validation-5951", "mrqa_squad-validation-5958", "mrqa_squad-validation-5975", "mrqa_squad-validation-6060", "mrqa_squad-validation-6071", "mrqa_squad-validation-6106", "mrqa_squad-validation-6119", "mrqa_squad-validation-612", "mrqa_squad-validation-6231", "mrqa_squad-validation-6254", "mrqa_squad-validation-6282", "mrqa_squad-validation-6404", "mrqa_squad-validation-6471", "mrqa_squad-validation-6472", "mrqa_squad-validation-6505", "mrqa_squad-validation-6564", "mrqa_squad-validation-6674", "mrqa_squad-validation-6681", "mrqa_squad-validation-6695", "mrqa_squad-validation-6750", "mrqa_squad-validation-6753", "mrqa_squad-validation-677", "mrqa_squad-validation-6810", "mrqa_squad-validation-6812", "mrqa_squad-validation-6813", "mrqa_squad-validation-6877", "mrqa_squad-validation-6916", "mrqa_squad-validation-6938", "mrqa_squad-validation-694", "mrqa_squad-validation-6968", "mrqa_squad-validation-6970", "mrqa_squad-validation-6990", "mrqa_squad-validation-7029", "mrqa_squad-validation-704", "mrqa_squad-validation-7086", "mrqa_squad-validation-7090", "mrqa_squad-validation-7133", "mrqa_squad-validation-7154", "mrqa_squad-validation-718", "mrqa_squad-validation-7201", "mrqa_squad-validation-7233", "mrqa_squad-validation-7270", "mrqa_squad-validation-7273", "mrqa_squad-validation-7288", "mrqa_squad-validation-7322", "mrqa_squad-validation-739", "mrqa_squad-validation-742", "mrqa_squad-validation-7480", "mrqa_squad-validation-7490", "mrqa_squad-validation-7514", "mrqa_squad-validation-7718", "mrqa_squad-validation-7723", "mrqa_squad-validation-7726", "mrqa_squad-validation-7783", "mrqa_squad-validation-7789", "mrqa_squad-validation-7886", "mrqa_squad-validation-794", "mrqa_squad-validation-7945", "mrqa_squad-validation-7958", "mrqa_squad-validation-7988", "mrqa_squad-validation-799", "mrqa_squad-validation-8012", "mrqa_squad-validation-8107", "mrqa_squad-validation-8154", "mrqa_squad-validation-8202", "mrqa_squad-validation-823", "mrqa_squad-validation-8278", "mrqa_squad-validation-83", "mrqa_squad-validation-8342", "mrqa_squad-validation-8352", "mrqa_squad-validation-839", "mrqa_squad-validation-8412", "mrqa_squad-validation-8443", "mrqa_squad-validation-85", "mrqa_squad-validation-8500", "mrqa_squad-validation-8600", "mrqa_squad-validation-8643", "mrqa_squad-validation-8695", "mrqa_squad-validation-8779", "mrqa_squad-validation-8871", "mrqa_squad-validation-8909", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9103", "mrqa_squad-validation-912", "mrqa_squad-validation-916", "mrqa_squad-validation-9190", "mrqa_squad-validation-932", "mrqa_squad-validation-9365", "mrqa_squad-validation-9405", "mrqa_squad-validation-9438", "mrqa_squad-validation-9465", "mrqa_squad-validation-9488", "mrqa_squad-validation-9525", "mrqa_squad-validation-9534", "mrqa_squad-validation-9592", "mrqa_squad-validation-9596", "mrqa_squad-validation-9628", "mrqa_squad-validation-9643", "mrqa_squad-validation-9675", "mrqa_squad-validation-9680", "mrqa_squad-validation-9700", "mrqa_squad-validation-9701", "mrqa_squad-validation-9717", "mrqa_squad-validation-9732", "mrqa_squad-validation-9762", "mrqa_squad-validation-9776", "mrqa_squad-validation-9859", "mrqa_squad-validation-9869", "mrqa_squad-validation-9920", "mrqa_squad-validation-999", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1415", "mrqa_triviaqa-validation-1466", "mrqa_triviaqa-validation-1489", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1675", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1838", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-2060", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3048", "mrqa_triviaqa-validation-3050", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3132", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3932", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-4634", "mrqa_triviaqa-validation-4846", "mrqa_triviaqa-validation-4916", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-5785", "mrqa_triviaqa-validation-584", "mrqa_triviaqa-validation-6107", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-6270", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-6562", "mrqa_triviaqa-validation-6670", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7337", "mrqa_triviaqa-validation-7443", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7472", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-839"], "OKR": 0.87109375, "KG": 0.45859375, "before_eval_results": {"predictions": ["2004", "extremely old", "liquid", "greater equality but not per capita income", "green algal derived chloroplast", "3.55 inches", "Troika Design Group", "an Islamic rebellion", "a second lieutenant", "The Rolling Stones", "\"House\"", "a toga", "a blow", "The Daily Hockey", "C. S. Lewis", "furlong", "a baby goat", "E. de Selincourt", "The Beatles", "Jenny and Eric", "parthenogenesis", "The Cornett family", "Zhu Yuanzhang", "Bob Dylan", "Sam Waterston", "a scythe", "101.325 kPa", "Eddie Murphy", "the band's logo in gold lettering over black sleeve", "1700 Cascadia earthquake", "para", "Manchester United", "\"peasant,\"", "kendo", "St. Ives", "300", "\"The Famous Toll House cookie\"", "1944", "the Federal Reserve System", "Marlon Brando", "Neighbours", "England", "\"evangelicalism\u2019s flagship magazine", "five", "3,384,569", "Steve Coogan", "\"Marcella\"", "25", "Nashville", "2006", "Chris Anderson", "Saddle Rock Elementary School", "I, the chief executive officer, the one on the very top", "almost 100", "the Dalai Lama's", "opened considerably higher Tuesday and saw an unprecedented wave of buying amid the elections.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East", "Body Tap", "the reclusive North Korean leader", "Saturday", "The social and political vitality of the nation", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "\"Housewives of Atlanta\"", "Renoir\u00b4s"], "metric_results": {"EM": 0.5, "QA-F1": 0.6218022963467338}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.823529411764706, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5454545454545454, 1.0, 0.5, 0.4, 0.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.9565217391304348, 0.8571428571428571, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3689", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-13503", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-14127", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-13719", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-16866", "mrqa_naturalquestions-validation-6658", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-3556", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-7549", "mrqa_triviaqa-validation-190", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-1038", "mrqa_newsqa-validation-250", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-88", "mrqa_triviaqa-validation-1423"], "SR": 0.5, "CSR": 0.5967261904761905, "EFR": 0.96875, "Overall": 0.7407514880952382}, {"timecode": 21, "before_eval_results": {"predictions": ["non-violent", "$2 million", "specialised education and training", "Middle Miocene", "debased", "Robert Maynard Hutchins", "five", "Mark Anthony \"Baz\" Luhrmann", "Christine MacIntyre", "Michele Bachmann", "French", "beer and soft drinks", "\" Theme Park\"", "Everything Is wrong", "Congo River", "the fictional city of Quahog, Rhode Island", "\"Grimjack\" (from First Comics) and \"Firestorm\", \"The Spectre\", and \"Martian Manhunter\"", "December 5, 1991", "the efferent nerves that directly innervate muscles", "renoir", "gamma ray emission", "Nancy Jean Cartwright", "seven", "the Indian Ocean", "Santa Fe, New Mexico, USA", "May 2017", "$5.4 trillion", "the 1970 triple album All Things Must Pass", "Emma Thompson", "Alexei Kosygin", "Saudi Arabia", "Ed Woodward", "Llanberis", "Cyclops", "addiction and behavior change/issues", "Oklahoma", "John Alec Entwistle", "Anna Mae Bullock", "Haiti", "Madonna", "2050", "Graeme Smith", "three", "does not grant full health-care coverage, which would require an act of Congress,", "The meter reader", "prostate cancer,", "Haiti.", "A witness", "246", "the creation of a long-term plan to help Haiti recover from the devastating effects of the earthquake and Argentina's conflict with Great Britain", "NATO's Membership Action Plan,", "the Way of St. James", "the Wu-Tang Clan", "Coral Reef", "chain reaction", "Jean Valjean", "Crossword", "The Treasure of the Sierra Madre", "re-telling of the Nanabozho stories", "Moscow", "The New York Tribune", "Olympia", "present - day southeastern Texas", "The Republic of Tecala"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6083104754440961}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7272727272727272, 0.4, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13793103448275862, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2412", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-2910", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-7685", "mrqa_triviaqa-validation-117", "mrqa_triviaqa-validation-3352", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-2885", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-852", "mrqa_searchqa-validation-4626", "mrqa_searchqa-validation-15310", "mrqa_searchqa-validation-8781", "mrqa_searchqa-validation-16839", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-14446", "mrqa_naturalquestions-validation-9327"], "SR": 0.515625, "CSR": 0.5930397727272727, "EFR": 1.0, "Overall": 0.7462642045454545}, {"timecode": 22, "before_eval_results": {"predictions": ["three", "ex-Luxembourg Prime Minister, Jean-Claude Juncker", "18", "utility", "Scottish Parliament", "Maling company", "Cash and Jennings", "DI Humphrey Goodman", "ITV", "31 July 1975", "Edmonton, Alberta", "ONTV", "Blackpool Gazette", "in 1877", "\"Tom Jones\"", "The Killer", "Kew Gardens", "\"Sully\" Sullenberger III", "Walter Brennan", "the 2010 World Series", "Freddie Highmore", "1952", "American colonies, then at war with the Kingdom of Great Britain", "endocytosis", "the 14th most common surname in Wales and 21st most common in England", "unknown origin", "the semilunar pulmonary valve", "30 October 1918", "360", "Addis Ababa", "Tigger", "Persia", "\"nyah nyah,", "bofrot", "Alberich", "david moyD", "Jeffrey Archer", "dinghy", "Montezuma", "Secretary of State", "job training", "Libyan leader,", "former Pakistani Prime Minister Benazir Bhutto", "an Italian and six Africans", "Mexico,", "Another high tide", "Camorra has been blamed for about 60 killings this year in Naples and its surrounding county.", "0-0 draw", "the release of the four men", "November 1", "the Sodra nongovernmental organization,", "Chen Lu", "cow's", "physician", "Mexico", "Arthur Pitney", "the ear", "John F. Kennedy", "British rock band", "Kilkenny cats", "Aleph", "shiatsu", "former Clash race winners", "BC Jean and Toby Gad"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5729549963924964}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.5714285714285715, 1.0, 0.4, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444444, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4295", "mrqa_squad-validation-7427", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-509", "mrqa_hotpotqa-validation-4934", "mrqa_naturalquestions-validation-5000", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-3175", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-4556", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-2396", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-6018", "mrqa_newsqa-validation-1545", "mrqa_newsqa-validation-2382", "mrqa_newsqa-validation-1098", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-534", "mrqa_searchqa-validation-8489", "mrqa_searchqa-validation-1620", "mrqa_searchqa-validation-4282", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-15331", "mrqa_searchqa-validation-12153"], "SR": 0.515625, "CSR": 0.5896739130434783, "EFR": 0.9354838709677419, "Overall": 0.732687806802244}, {"timecode": 23, "before_eval_results": {"predictions": ["antigens", "the courts of member states and the Court of Justice of the European Union", "its contents were sold two years later to satisfy a debt.", "pastors", "Wales", "\u00dcberseering BV v Nordic Construction GmbH", "9", "Graham Payn", "Democratic", "Charles L. Clifford", "Ludwig van Beethoven", "Jacking", "Japan", "Viscount Cranborne", "Kew", "Kim Bauer", "Citizens for a Sound Economy", "Gian Carlo Menotti", "in the 1820s", "genome", "an address bar", "country", "1998", "Lex Luger and Rick Rude", "the biblical name of a Canaanite god associated with child sacrifice", "Presley Smith", "foreign investors", "Utah, Arizona, Wyoming, and Oroville, California", "Aalika Sheikh", "Dublin", "George H. W.", "Italy", "Margaret Thatcher", "Ascot", "Tennis", "Brussels", "christopher", "Uruguay", "Carousel", "Apollon", "the used-luxury market", "At least seven deaths were attributed to the storm, five caused by falling trees,", "Karen Floyd", "has contacted the drug maker \"about a specific lot number, and that lot number is not from the two we are recalling.\"", "his record breaking victory as he claimed his sixth world title at a different weight by beating Cotto on Saturday night.", "for security reasons and not because of their faith.", "on the family's blog", "$81,88010", "Ryder Russell", "district Attorney Larry Abrahamson", "armed members of the radical Islamist group Jund Ansar Allah surround a group representative in Rafah on Friday.", "Sweden", "the King's Men", "Free Morrison Beloved Ghost Essays", "Canada", "George Orwell", "Lincoln Cents", "Queen Margrethe", "Pillsbury", "metric", "Northwestern University", "Angelina Jolie", "Sergei Rachmaninoff", "adrenaline"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6062694099378882}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0, 0.0, 0.08695652173913045, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1406", "mrqa_hotpotqa-validation-5136", "mrqa_hotpotqa-validation-1886", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-8484", "mrqa_triviaqa-validation-4047", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-7265", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-2631", "mrqa_triviaqa-validation-1428", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-4198", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-2736", "mrqa_searchqa-validation-9995", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-8672", "mrqa_searchqa-validation-3315", "mrqa_searchqa-validation-12618", "mrqa_searchqa-validation-5110", "mrqa_searchqa-validation-10183"], "SR": 0.53125, "CSR": 0.5872395833333333, "EFR": 1.0, "Overall": 0.7451041666666666}, {"timecode": 24, "before_eval_results": {"predictions": ["38", "inertia", "Edmonton, Canada", "Islam", "conservative Muslims", "the 1st Parachute Battalion", "south-north motorway", "Ford Island", "Europe", "Amber Heard", "Matthieu Vaxivi\u00e8re", "the Rose Garden", "the House of Hohenstaufen", "the larger of two groups of cooperative banks in Austria", "2011", "\" Terry the Tomboy\"", "The Vaudevillains", "the Catholic Monarchs of Castile and Aragon", "Kelly Osbourne", "the right hand", "1804", "Steve Mazzaro", "Lady Gaga", "James Hutton", "1901", "number of times a pitcher pitches in a season", "a god of the Ammonites", "1959", "Rose-Marie", "Desdemona", "The Ten Commandments", "france", "Kraft's", "Adolphe Adam", "Brooklyn", "Paris", "southeasterly", "Today", "Lady Gaga", "Afghanistan", "Canada", "an upper respiratory infection,", "ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons,", "the Muslim Eid-ul-Adha", "10 to 15 percent", "al-Shabaab", "south of the border", "Collier County Sheriff Kevin Rambosk", "the European Commission", "Meredith Kercher", "jack Sprat", "Hollywood Canteen", "the Arabian Peninsula", "orangutans", "Heathrow", "the THX surround sound system", "the BBC series", "Mount Everest", "a junkyard dog", "Napoleon", "sphincter", "Maine", "Wigan Athletic", "Sapporo"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5105902777777778}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5555555555555556, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10328", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-2128", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-4696", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-3609", "mrqa_hotpotqa-validation-4890", "mrqa_naturalquestions-validation-9985", "mrqa_naturalquestions-validation-8476", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-1602", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-960", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-6214", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-3918", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-5165", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-7147", "mrqa_searchqa-validation-12372", "mrqa_triviaqa-validation-5278", "mrqa_triviaqa-validation-5890"], "SR": 0.484375, "CSR": 0.583125, "EFR": 1.0, "Overall": 0.74428125}, {"timecode": 25, "before_eval_results": {"predictions": ["Oirads", "via electron microscopy", "complete addressing information", "a particular algorithm with running time at most T(n)", "\"We are beggars,\"", "a fictional South American country", "JackScanlon", "the coast of Guant\u00e1namo Bay in Cuba", "Western Australia", "non-ferrous", "the Veterans Committee", "1978", "Pakistan", "the National League ( NL ) champion Chicago Cubs", "94 by 50 feet", "Joe Spano", "the passing of the year", "chinook", "a bacteria", "Macleans", "plants", "John Poulson", "one", "foods", "the Penguin", "Israel", "Southwest Airlines", "james hogg", "the People's Republic of China", "the International Conference on LGBT Human Rights", "Premier Division", "Terry Malloy", "Weare", "Rigel VII", "M Rookie Blaylock", "1988", "1959", "40 million", "Leonard Cohen", "President Bush", "Empire of the Sun", "Afghan lawmakers", "these cars are \"totaled,\" which might give buyers the peace of mind knowing they will get a replacement vehicle.", "some of the best stunt ever pulled off", "kite boards", "\"associate\" of the family,", "the two bodies out of the plant, which makes Slim Jim food products.", "a 9-week-old", "Australian officials", "Barack Obama sent a message that fight against terror will respect America's values.", "Jack London", "Philadelphia", "NYPD Shield", "a to Zed", "clouds", "a vacuum", "Jane Eyre", "Tom Sennett", "Catholicism", "\"It's regarded as the very worst example of a New Orleans accent\"", "Mikhail Gorbachev", "four months", "Polo", "\"Rin Tin Tin: The Life and the Legend\""], "metric_results": {"EM": 0.515625, "QA-F1": 0.603710222069597}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, false, false, true, true, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2, 0.5714285714285715, 1.0, 0.3076923076923077, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.049999999999999996, 0.6666666666666666, 0.0, 0.0, 0.625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1784", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-7827", "mrqa_triviaqa-validation-6910", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-3818", "mrqa_triviaqa-validation-7353", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-2151", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-2627", "mrqa_newsqa-validation-2765", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-3354", "mrqa_searchqa-validation-6579", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-1003"], "SR": 0.515625, "CSR": 0.5805288461538461, "EFR": 0.9354838709677419, "Overall": 0.7308587934243176}, {"timecode": 26, "before_eval_results": {"predictions": ["Disneyland", "Scandinavia", "back to an Earth ocean landing", "as soon as they enter into force,", "Charlene Holt", "25 years after the release of their first record", "the case of disputes between two or more states", "NIRA", "April 2010", "an epiphany song", "Atlanta", "1920s", "every 23 hours, 56 minutes, and 4 seconds with respect to the stars", "novella", "Javier Fern\u00e1ndez", "Luke 6 : 67 -- 71", "Washington, D.C.", "\"Wild Thing\"", "Socrates", "darts", "President Nixon", "the Diamondbacks", "surf", "king of Cyprus", "Verdi and his librettist Antonio Somma", "jules Verne", "Wikipedia", "leopons", "1884", "nursery rhyme", "Sully", "Thomas Christopher Ince", "quantum mechanics", "more than 26,000", "The entity", "John de Mol Jr.", "KBS2", "on the shore", "Harold Edward Holt", "Adelaide", "Islamic militants", "Dan Brown's", "a 22-year-old college student in Boston, Massachusetts,", "killed in an attempted car-jacking as he dropped his children off at a relative's house,", "citizenship", "broken pelvis", "\"illegitimate.\"", "Friday, a federal magistrate said Monday.", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Isabella", "Kurdish militant group in Turkey", "Brad Blauser,", "leeches", "The Rolling Stones", "Cheddar", "the human breast", "the Mesozoic Era", "King Arthur", "commission", "coral", "one small step", "butter", "abacus", "elixir"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6402758546600458}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, false, false, true, false, true, true, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.09523809523809525, 0.9411764705882353, 1.0, 0.6666666666666666, 0.19999999999999998, 1.0, 0.33333333333333337, 0.13333333333333333, 1.0, 0.0, 0.75, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 0.3333333333333333, 1.0, 0.33333333333333337, 0.1212121212121212, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3879", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-5300", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-3770", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-4113", "mrqa_triviaqa-validation-1115", "mrqa_triviaqa-validation-3902", "mrqa_triviaqa-validation-1858", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-7662", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-2121", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1143", "mrqa_searchqa-validation-11394", "mrqa_searchqa-validation-8544", "mrqa_searchqa-validation-12936"], "SR": 0.53125, "CSR": 0.5787037037037037, "EFR": 0.9666666666666667, "Overall": 0.7367303240740741}, {"timecode": 27, "before_eval_results": {"predictions": ["Herodotus", "13th", "A probabilistic Turing machine", "Donald Davies", "two tickets to Italy", "\"should be thrown out of their bloody jobs.\"", "outside influences in next month's run-off election,", "The tape was given to authorities in September 2007 by a man who said he had found it in the desert five months before.", "allergies to peanuts, nuts, shellfish, peanuts, tree nuts, wheat and soy,", "immigration judge with the U.S. Justice Department", "150", "j. Crew outfits", "changed the business of music,", "Miami Beach, Florida,", "more than 100", "police", "Dirk Benedict", "flawed democracy", "1986", "the adrenal medulla", "July 2, 1776", "parthenogenesis", "pilgrimages to Jerusalem", "Daniel A. Dailey", "the Allies", "74", "Nashville, Tennessee", "Billy Colman", "Sicily", "skanderbeut", "gda\u0144sk", "e pluribus unum", "Eddie Cochran", "the witch trials", "Ghana", "El Hiero", "pppaea Sabina", "heineken", "fish acting strange at the surface", "Andover, Maine", "Vaisakhi List", "West Africa", "Tranquebar", "Citizens for a Sound Economy", "Tie Domi", "the Rolls of Ol\u00e9ron", "bank of China", "The University Debating and Literary Club", "dice", "activist", "February 10, 1927", "Interstate 22", "Dr. Seuss", "a cardinal", "Paris", "ear infections", "giuseppe", "Sally Margaret Field", "rhodium, ruthenium", "apples", "William Shakespeare", "lala lala la lala", "Herbert Hoover", "Bulgaria"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6126628278122843}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.1739130434782609, 0.8571428571428571, 0.0, 0.16666666666666669, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.22222222222222224, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9274", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-829", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-828", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-2782", "mrqa_triviaqa-validation-5892", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-3755", "mrqa_triviaqa-validation-1188", "mrqa_triviaqa-validation-3032", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3973", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-351", "mrqa_searchqa-validation-14465", "mrqa_searchqa-validation-14772", "mrqa_searchqa-validation-494", "mrqa_searchqa-validation-411", "mrqa_searchqa-validation-11551", "mrqa_searchqa-validation-9272", "mrqa_searchqa-validation-1119"], "SR": 0.484375, "CSR": 0.5753348214285714, "EFR": 1.0, "Overall": 0.7427232142857142}, {"timecode": 28, "before_eval_results": {"predictions": ["X.25 and the terminal interface to X.29", "cyclic photophosphorylation", "without destroying", "\"The Da Vinci Code\"", "Gustav's top winds weakened to 110 mph", "President Obama", "\"Walk -- Don't Run\" and \"Diamond Head.\"", "dancing in short pants.", "2006", "to put a lid on the marking of Ashura", "Satsuma, Florida", "her son has strong values.", "17 Again", "3-2", "Cameroon", "Atlantic Ocean", "Super Bowl LII", "the nucleus with densely coiled chromatin fibres, surrounded anteriorly by an acrosome, which contains enzymes used for penetrating the female egg", "Andaman and Nicobar Islands", "July 25, 2017", "December 12, 2017", "Montreal Canadiens", "increased productivity, trade, and secular economic trends", "c. 1000 AD", "The onset of rigor mortis and its resolution", "across the Strait of Magellan", "1904", "yayin", "claudia", "Rugby School", "Dutch", "David Frost", "albania", "claudia", "Macbeth", "telephone", "a symbol of position and title", "Turkey", "judo", "puppy", "Hee Haw", "2012", "Coyote Ugly", "Matthew Tishler", "Brady Haran", "Squam Lake", "British singer and \"Britain's Got Talent\"", "Don Swayze", "East Is East", "Washington, D.C.", "Burning Man", "Joseph", "Harriet Tubman", "\"My Ding-a-Ling.\"", "Colorado", "60 Minutes", "Colorado River", "Spain", "the First Tree", "femur", "Machiavelli", "Heracles", "bone fracture", "the uvula"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6016531211843712}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true, false, false, true], "QA-F1": [0.8, 1.0, 0.6666666666666666, 0.0, 0.4444444444444445, 1.0, 0.0, 0.25, 1.0, 0.923076923076923, 0.28571428571428575, 0.7692307692307693, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.2, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4834", "mrqa_squad-validation-4029", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-1670", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-8450", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-1041", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-7011", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-664", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-4148", "mrqa_searchqa-validation-3220", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-14848", "mrqa_searchqa-validation-6598", "mrqa_searchqa-validation-862", "mrqa_searchqa-validation-15667"], "SR": 0.484375, "CSR": 0.572198275862069, "EFR": 0.9696969696969697, "Overall": 0.7360352991118078}, {"timecode": 29, "before_eval_results": {"predictions": ["rediscovery of \"Christ and His salvation\"", "$2.50 per AC horsepower royalty", "recant his writings", "pregnancy.", "at least 15", "\"The Cycle of Life,\"", "Tim Clark, Matt Kuchar and Bubba Watson", "at a house adjacent to the park,", "high-strength steel and boron", "made one of his strongest statements to date on the sex abuse scandal sweeping the Roman Catholic Church,", "Body Tap", "how health care can affect families.", "2-1", "National Indigenous Organization of Colombia,", "India", "Roman Reigns", "Nationalists", "April 10, 2018", "September 2017", "December 19, 1971", "Professor Kantorek", "646.38 metres ( 2,120.7 ft )", "1932", "If These Dolls Could Talk", "a solitary figure who is not understood by others, but is actually wise", "T.S. Eliot", "Ludacris", "dinghies", "Midsomer Murders", "rare islands of Micronesia", "sulfuric and nitric acids", "red-green colorblindness", "Nicky Morgan", "published at least once a week", "Director General of the Security Service", "rare", "Ghana", "Jan van Eyck", "achromatopsia", "Wake Island", "poetry", "in 1884", "Tampa Bay Lightning", "Madonna Louise Ciccone", "Bonkyll Castle", "Dan Rowan", "Nazi Party", "Nikita Khrushchev", "Speedway World Championship", "47,818", "1903", "a compound", "a host", "Fergie", "a young", "the Third Crusade", "Redblush", "the Hurt Locker", "Lady Sings the Blues", "a alligator", "a saltire", "a priest", "the long jump", "Asiana Town building"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5227430555555554}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.75, 0.0, 1.0, 1.0, 0.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.4, 0.6666666666666666, 0.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-1036", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-3633", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-824", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-1851", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-6726", "mrqa_searchqa-validation-14390", "mrqa_searchqa-validation-13080", "mrqa_searchqa-validation-14914", "mrqa_searchqa-validation-15706", "mrqa_searchqa-validation-15346", "mrqa_searchqa-validation-14664", "mrqa_searchqa-validation-7212"], "SR": 0.46875, "CSR": 0.56875, "EFR": 1.0, "Overall": 0.74140625}, {"timecode": 30, "UKR": 0.76953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1683", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-2412", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2821", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3055", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3082", "mrqa_hotpotqa-validation-3166", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-3378", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-351", "mrqa_hotpotqa-validation-3516", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3776", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-4092", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4171", "mrqa_hotpotqa-validation-4282", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-4513", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4698", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5222", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-5502", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-834", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1222", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-2479", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2600", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2960", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-5000", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-8137", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-8862", "mrqa_naturalquestions-validation-8883", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1098", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1217", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1889", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2016", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2127", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3400", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-3676", "mrqa_newsqa-validation-3719", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4162", "mrqa_newsqa-validation-4174", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-535", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-726", "mrqa_newsqa-validation-813", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-980", "mrqa_newsqa-validation-99", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-1017", "mrqa_searchqa-validation-10179", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-10364", "mrqa_searchqa-validation-10930", "mrqa_searchqa-validation-11121", "mrqa_searchqa-validation-1119", "mrqa_searchqa-validation-1146", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-12936", "mrqa_searchqa-validation-13117", "mrqa_searchqa-validation-13198", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13719", "mrqa_searchqa-validation-13920", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-14848", "mrqa_searchqa-validation-15018", "mrqa_searchqa-validation-15219", "mrqa_searchqa-validation-15331", "mrqa_searchqa-validation-15346", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-16581", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-16866", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-187", "mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-2136", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-2534", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-4282", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5165", "mrqa_searchqa-validation-5201", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6388", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-6726", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-694", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7672", "mrqa_searchqa-validation-7692", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-8489", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8672", "mrqa_searchqa-validation-9797", "mrqa_squad-validation-10103", "mrqa_squad-validation-10106", "mrqa_squad-validation-10223", "mrqa_squad-validation-10299", "mrqa_squad-validation-10303", "mrqa_squad-validation-10307", "mrqa_squad-validation-10319", "mrqa_squad-validation-10428", "mrqa_squad-validation-10436", "mrqa_squad-validation-10438", "mrqa_squad-validation-10449", "mrqa_squad-validation-1126", "mrqa_squad-validation-1299", "mrqa_squad-validation-132", "mrqa_squad-validation-143", "mrqa_squad-validation-1539", "mrqa_squad-validation-1611", "mrqa_squad-validation-1644", "mrqa_squad-validation-1645", "mrqa_squad-validation-1657", "mrqa_squad-validation-1688", "mrqa_squad-validation-1695", "mrqa_squad-validation-1787", "mrqa_squad-validation-1850", "mrqa_squad-validation-1877", "mrqa_squad-validation-1980", "mrqa_squad-validation-2382", "mrqa_squad-validation-2478", "mrqa_squad-validation-2489", "mrqa_squad-validation-2497", "mrqa_squad-validation-2636", "mrqa_squad-validation-2658", "mrqa_squad-validation-2704", "mrqa_squad-validation-2709", "mrqa_squad-validation-2810", "mrqa_squad-validation-2854", "mrqa_squad-validation-288", "mrqa_squad-validation-2949", "mrqa_squad-validation-2955", "mrqa_squad-validation-299", "mrqa_squad-validation-3061", "mrqa_squad-validation-3113", "mrqa_squad-validation-3141", "mrqa_squad-validation-3147", "mrqa_squad-validation-3168", "mrqa_squad-validation-3197", "mrqa_squad-validation-3300", "mrqa_squad-validation-3302", "mrqa_squad-validation-3516", "mrqa_squad-validation-3559", "mrqa_squad-validation-3566", "mrqa_squad-validation-3590", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3812", "mrqa_squad-validation-3829", "mrqa_squad-validation-3898", "mrqa_squad-validation-3909", "mrqa_squad-validation-4063", "mrqa_squad-validation-4097", "mrqa_squad-validation-4121", "mrqa_squad-validation-4137", "mrqa_squad-validation-4142", "mrqa_squad-validation-415", "mrqa_squad-validation-42", "mrqa_squad-validation-4262", "mrqa_squad-validation-4439", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-4623", "mrqa_squad-validation-4631", "mrqa_squad-validation-4676", "mrqa_squad-validation-4801", "mrqa_squad-validation-4834", "mrqa_squad-validation-4840", "mrqa_squad-validation-4856", "mrqa_squad-validation-4951", "mrqa_squad-validation-509", "mrqa_squad-validation-5156", "mrqa_squad-validation-5190", "mrqa_squad-validation-5229", "mrqa_squad-validation-5272", "mrqa_squad-validation-5505", "mrqa_squad-validation-5758", "mrqa_squad-validation-5951", "mrqa_squad-validation-5958", "mrqa_squad-validation-5975", "mrqa_squad-validation-6071", "mrqa_squad-validation-6106", "mrqa_squad-validation-612", "mrqa_squad-validation-6221", "mrqa_squad-validation-6254", "mrqa_squad-validation-6404", "mrqa_squad-validation-6472", "mrqa_squad-validation-6505", "mrqa_squad-validation-6674", "mrqa_squad-validation-6681", "mrqa_squad-validation-6750", "mrqa_squad-validation-6795", "mrqa_squad-validation-6812", "mrqa_squad-validation-6813", "mrqa_squad-validation-6916", "mrqa_squad-validation-6938", "mrqa_squad-validation-694", "mrqa_squad-validation-6968", "mrqa_squad-validation-704", "mrqa_squad-validation-7133", "mrqa_squad-validation-718", "mrqa_squad-validation-7233", "mrqa_squad-validation-7273", "mrqa_squad-validation-7322", "mrqa_squad-validation-742", "mrqa_squad-validation-7427", "mrqa_squad-validation-7490", "mrqa_squad-validation-7718", "mrqa_squad-validation-7723", "mrqa_squad-validation-7726", "mrqa_squad-validation-7731", "mrqa_squad-validation-7767", "mrqa_squad-validation-7783", "mrqa_squad-validation-7789", "mrqa_squad-validation-7886", "mrqa_squad-validation-799", "mrqa_squad-validation-8012", "mrqa_squad-validation-8107", "mrqa_squad-validation-8154", "mrqa_squad-validation-8202", "mrqa_squad-validation-8212", "mrqa_squad-validation-8278", "mrqa_squad-validation-8352", "mrqa_squad-validation-85", "mrqa_squad-validation-8600", "mrqa_squad-validation-8695", "mrqa_squad-validation-8792", "mrqa_squad-validation-8909", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9103", "mrqa_squad-validation-9189", "mrqa_squad-validation-9190", "mrqa_squad-validation-932", "mrqa_squad-validation-9405", "mrqa_squad-validation-9438", "mrqa_squad-validation-9465", "mrqa_squad-validation-9488", "mrqa_squad-validation-9519", "mrqa_squad-validation-9525", "mrqa_squad-validation-9534", "mrqa_squad-validation-9629", "mrqa_squad-validation-9639", "mrqa_squad-validation-9675", "mrqa_squad-validation-9698", "mrqa_squad-validation-9700", "mrqa_squad-validation-9701", "mrqa_squad-validation-9717", "mrqa_squad-validation-9732", "mrqa_squad-validation-975", "mrqa_squad-validation-9762", "mrqa_squad-validation-9859", "mrqa_squad-validation-9898", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-1489", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1675", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-219", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-2396", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2757", "mrqa_triviaqa-validation-2885", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3163", "mrqa_triviaqa-validation-3279", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3352", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-3932", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-4047", "mrqa_triviaqa-validation-4077", "mrqa_triviaqa-validation-4113", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4275", "mrqa_triviaqa-validation-4338", "mrqa_triviaqa-validation-434", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-4846", "mrqa_triviaqa-validation-5178", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5330", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5785", "mrqa_triviaqa-validation-5879", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6270", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-637", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6562", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6910", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-960"], "OKR": 0.849609375, "KG": 0.47734375, "before_eval_results": {"predictions": ["Sierra Sky Park Airport", "second in terms of GSP per capita", "Recognized Student Organizations", "fractured pelvis and sacrum", "Saturday", "lower house of parliament,", "drama that pulls in the crowds", "Daniel Nestor,", "40", "livingston", "him to step down as majority leader.", "Christopher Savoie", "Missouri", "Mitt Romney", "South Africa inflicted the first home series defeat on Australia in almost 16 years", "Robin", "1990", "broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "no embryo", "Sarah Silverman", "St. Mary's County", "the fovea centralis", "Christina Giles", "Dorothy Gale", "John Prine", "11 January 1923", "2015", "hair and fur", "bituminous", "Craggy Island", "Jaime Jaime", "Sodor", "major use of radio in tracking down the suspects", "crossbred", "Cadbury", "Wharton's club", "chlorine", "cabinet Office Briefing Rooms", "Big Brother", "Whoopi Goldberg", "Harlow Cuadra and Joseph Kerekes", "\"How to Train Your Dragon\"", "Lundbeck", "timeline of Shakespeare criticism", "311", "2012", "Belarus", "\"The Braes o' Bowhether\"", "Marvel Comics", "stein", "Gmail, Hangouts, Google Calendar, Drive", "stiletto", "Dune", "The Jane Eyre", "Ron Paul", "The Wall Street Journal", "the assassination of President John F.", "Tahiti", "\"Joliet\"", "the Bolshoi Ballet", "Stphane Mallarm", "The 76-year-old Bowden", "Armenia", "Don Garlits"], "metric_results": {"EM": 0.34375, "QA-F1": 0.43871640512265514}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, true], "QA-F1": [0.8571428571428571, 0.25, 1.0, 0.0, 0.0, 1.0, 0.1818181818181818, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8666666666666666, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4698", "mrqa_squad-validation-2956", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-4091", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-3087", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-195", "mrqa_triviaqa-validation-1340", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-6254", "mrqa_triviaqa-validation-1465", "mrqa_triviaqa-validation-1852", "mrqa_triviaqa-validation-6834", "mrqa_triviaqa-validation-4267", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2966", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-3241", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-89", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-5514", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-3231", "mrqa_searchqa-validation-314", "mrqa_searchqa-validation-15451", "mrqa_searchqa-validation-8649", "mrqa_searchqa-validation-3330"], "SR": 0.34375, "CSR": 0.561491935483871, "EFR": 1.0, "Overall": 0.7315952620967743}, {"timecode": 31, "before_eval_results": {"predictions": ["article 30", "Seine", "a house party in Crandon, Wisconsin", "Jaime andrade", "South Africa", "Goa", "the Frank case seemed to press every hot-button issue of the time: North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "246", "consumer confidence", "Remains", "Siri", "Touma", "little blue booties", "Communist Party of Nepal", "Luke 6 : 12 -- 16", "Nazi Germany and Fascist Italy", "George II", "Andrew Lloyd Webber", "five", "gastrocnemius", "accomplish the objectives of the organization", "Narendra Modi", "1960", "Marie Fredriksson", "John Frank Stevens", "Jesus Christ", "gebrselassie", "9", "Radio City Music Hall", "geyser", "the Battle of Camlann", "hawkeye", "France", "Maxwell", "piano", "florida", "180", "Jack Kennedy", "Bill Walton", "peatland", "Homer Hickam, Jr.", "1901", "the 250cc world championship", "Kinnairdy Castle", "1999 Odisha", "EBSCO", "Sylvia Pankhurst", "Hillary Clinton", "Roseann O'Donnell", "Iranian government\u2019s propaganda channel", "Bessie Coleman", "marooned", "Atlanta Hawks", "California, New York, Texas", "coral snake", "Green Lantern", "Hawaii", "The Greatest Show on Earth", "James V, King of Scotland", "blue state", "the snow comes down in June", "Henry James", "Matt Monro", "the Vital Records Office of the states, capital district, territories and former territories"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6768962272408964}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.28571428571428575, 1.0, 0.5, 1.0, 0.7058823529411764, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 0.4, 1.0, 0.5714285714285715, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.5, 1.0, 0.8, 0.5, 0.0, 0.0, 0.5, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.625]}}, "before_error_ids": ["mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3729", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-960", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-6084", "mrqa_triviaqa-validation-2300", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-5362", "mrqa_triviaqa-validation-821", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-5763", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-4995", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-3389", "mrqa_searchqa-validation-8819", "mrqa_searchqa-validation-4660", "mrqa_searchqa-validation-6423", "mrqa_searchqa-validation-10097", "mrqa_naturalquestions-validation-6998"], "SR": 0.53125, "CSR": 0.560546875, "EFR": 1.0, "Overall": 0.73140625}, {"timecode": 32, "before_eval_results": {"predictions": ["banded iron formations", "multiplication", "seven relief pitchers", "SuperX - 13", "Christopher Jones", "In land most phosphorus is found in rocks and minerals", "1988", "the Seton Hall Pirates men's basketball team", "Manhattan Project", "former Clash race winners", "October 29, 2015", "Norman Pritchard", "Lou Stallman", "Kim Basinger", "Charles manson", "The Life of a Great Sinner", "sebills", "Hindu Wisdom", "The Green Mile", "Paris", "Labrador retrievevers", "having or seeing nosebleeds or bleeding to death", "pickled peppers", "Jennifer Ellison", "Liszt Strauss Wagner Dvorak", "Pakistan", "the Kentucky Music Hall of Fame", "3730 km", "\"Margu\u0161\"", "the theme park song \"It's a Small World (After All)", "the Neotropical realm", "the Moselle", "\"The Concubine\"", "actor", "Drowning Pool", "\"Odorama\"", "\"Beauty and the Beast\"", "creeks, fringing the southwest mouth of Lagos Lagoon,", "Anne Frank,", "speaking out about a cause someone feels passionate about.", "the river will crest Saturday about 20 feet above flood stage.", "51 percent of the U.S. public consider China a military threat,", "his two-floor home in the St. Louis suburb of Columbia, Illinois", "At least 33", "two were in 1986, three in 1995, one in 1997 and one in 2007.", "Sri Lanka's Tamil rebels to \"release\" civilians,", "near Garacad, Somalia,", "\"Nothing But Love\"", "Afghanistan's restive provinces", "\"When you're going into a restaurant environment, you're putting your child's safety and livelihood into other hands,\"", "the Cyt p450 system", "John Hersey", "Solomon", "The Lord of the Rings", "Thai", "a statue", "a solecism", "Rome", "Dragons", "the Black Rock", "lyrebirds", "Joseph Bonaparte", "Bangkok", "MGM Resorts International"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5117333391690009}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5, 0.5714285714285715, 1.0, 0.5882352941176471, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.25, 0.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-2812", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-10213", "mrqa_triviaqa-validation-5008", "mrqa_triviaqa-validation-1582", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-3505", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-3300", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-2715", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-2282", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3974", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-1313", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-3736", "mrqa_searchqa-validation-12171", "mrqa_searchqa-validation-3946", "mrqa_searchqa-validation-13044", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-16682", "mrqa_searchqa-validation-10131"], "SR": 0.421875, "CSR": 0.556344696969697, "EFR": 1.0, "Overall": 0.7305658143939395}, {"timecode": 33, "before_eval_results": {"predictions": ["behind the foot of the mast of a moving ship", "Business Connect", "the Arctic Ocean", "ase", "the Deathly Hallows", "Daniel A. Dailey", "Jeff Bezos", "Sharecropping", "anastomosis of a single epididymal tubule ( luminal diameter. 15 -. 25 mm ) to the lumen of the vas deferens", "Nodar Kumaritashvili", "the disk", "Veronica", "inverted - drop - shaped icon", "Greek \u1f61\u03c3\u03b1\u03bd\u03bd\u03ac, h\u014dsann\u00e1", "\u201cMy dear, I don\u2019t give a damn\u201d and \u201cAfter all, tomorrow is another day\u201d", "australian", "Jackie Kennedy", "narcolepsy", "Bangladesh", "winton", "fluid", "blanc", "watson", "eagle", "the Book of Esther", "linda evans", "the State House in Augusta", "Pan Am Railways", "a coaxial cable with RCA connectors or a fibre optic cable with TOSLINK connectors", "National Collegiate Athletic Association (NCAA)", "Guillermo del Toro", "journal", "major intersections", "Sleeping Beauty", "Atat\u00fcrk Museum Mansion", "Martin Joseph O'Malley", "Deep Purple", "Atlas ICBM", "Tuesday", "Brian Smith.", "the Russian air force,", "nearly $2 billion in stimulus funds", "three", "Genocide Prevention Task Force", "NATO", "more than 30", "Republican Gov. Jan Brewer", "Muslim festival of Eid al-Adha", "$81,880", "lula Bell Houston Laundry,", "Hercules", "Connecticut", "elementary", "Nixon's next nominee, Judge Harrold Carswell of the Fifth Circuit", "FDR", "the cob", "the San Francisco 49ers", "a fisheye lens", "The Blues Brothers", "Anne Rice", "polygon", "dishwasher", "Jet Republic,", "The group, Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6457481856378915}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, true, true, true, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.125, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.125, 0.14285714285714288, 0.0, 0.0, 0.3529411764705882, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0909090909090909, 0.25, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-5951", "mrqa_triviaqa-validation-1766", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-597", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-6480", "mrqa_triviaqa-validation-6731", "mrqa_triviaqa-validation-7634", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-1350", "mrqa_newsqa-validation-2445", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3051", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-5304", "mrqa_newsqa-validation-1095"], "SR": 0.546875, "CSR": 0.5560661764705883, "EFR": 1.0, "Overall": 0.7305101102941177}, {"timecode": 34, "before_eval_results": {"predictions": ["he did not want disloyal men in his army.", "world classic of epoch-making oratory.", "1960", "Ernest Hemingway", "Master Christopher Jones", "Pradyumna", "National Football League Kickoff", "one day, while listening to what seems to be a crossed telephone connection, she hears two men planning a woman's murder", "Hirschman", "United States customary units", "Zeus", "after initially peaking at number 41 in the UK, it re-entered the charts after the group performed the track at Nelson Mandela's 70th Birthday concert", "an anembryonic gestation", "The Divergent Series : Ascendant was never made", "Wizard of Oz", "Andes", "us", "Barack Obama", "lord lord lord byron", "Super Bowl Sunday", "Elizabeth II", "Republican National Committee's", "baseball cards", "cigar", "labrador", "Microsoft", "1955", "The ones Who Walk Away from Omelas", "Province of New York", "historic buildings, arts, and published works", "Jim Jones", "Amberley Village", "Colonial colleges", "Shooter", "Eisenhower Executive Office Building", "Oklahoma Sooners", "Mike Greenwell", "Kurt Vonnegut Jr.", "Jena 6", "French army helicopter taking off from French frigate Nivose,  on patrol in the Gulf of Aden.", "researchers have developed technology that makes it possible to use thoughts to operate a computer, maneuver a wheelchair or even use Twitter.", "fighting for his life in a Buenos Aires hospital after being shot in the head during an armed robbery.", "wanted to change the music on the CD player and the 34-year-old McGee said the football star had acted aggressively in trying to grab the device.", "if you immediately think about saving the rainforests, you could find your environmental efforts make even more impact than Harrison Ford's chest.", "Monday night", "CNN's \"Piers Morgan Tonight\"", "tomorrow, the Hanford site is a virtual ghost town and those involved in the clean-up project say they will need every dollar of the federal stimulus funds.", "Louvre", "planned attacks", "housing, business and infrastructure repairs,", "us", "to know where the word came from", "the Boston Red Sox", "a beehive", "Walter Cronkite", "Twenty", "Jon Heder", "green olives", "that regardless of what we do with freed slaves, slavery is wrong and should stop.", "lordn hesse", "Baby Gays", "Rick Springfield", "David Bowie", "\"Cruisin'\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.5533542035235146}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, false, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.8, 0.0, 0.0, 0.0, 0.3157894736842105, 0.0, 0.0, 1.0, 0.4, 0.29411764705882354, 0.5, 0.4, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-2333", "mrqa_triviaqa-validation-1230", "mrqa_triviaqa-validation-1424", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-603", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-4845", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-4700", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2609", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3369", "mrqa_searchqa-validation-13153", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-7948", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-1952", "mrqa_searchqa-validation-7246", "mrqa_searchqa-validation-2411", "mrqa_searchqa-validation-12158", "mrqa_triviaqa-validation-6414"], "SR": 0.453125, "CSR": 0.553125, "EFR": 1.0, "Overall": 0.729921875}, {"timecode": 35, "before_eval_results": {"predictions": ["Doctorin' the Tardis", "religious", "punk rock band", "Aaron Hall", "Biola University", "1916 Easter Rising", "A.S. Roma", "Arena of Khazan", "Dungeness crab", "the Man Booker Prize", "England", "Texas's 27th congressional district", "1969", "Mel Blanc", "Jerry Ekandjo", "Sargon II", "warmth", "the American Revolutionary War", "Gertrude Niesen", "Gibraltar", "Peking", "Union's forces", "the nucleus", "the 1960s", "Wendy Fraser", "Michael Phelps", "Nikola Tesla", "humidity", "us", "hastings", "Tennessee", "Shirley Bassey", "hastings", "Pegida", "hastings", "frottage", "the United States", "Peter Kay", "Felipe Massa.", "the captain of a nearby ship", "his health", "Hamas rocket attacks on Israel", "three years", "President Paul Biya,", "2-1", "President-elect Barack Obama", "a nuclear weapon", "2008", "natural gas", "Mehsud", "Henry Hudson", "geometria", "hastings", "President of Ukraine", "gold", "Mork & Mindy", "anthrax", "Indiana's", "Michael Clayton", "the Waves", "William Jennings Bryan", "Hairspray", "Herman Cain,", "Hakeemullah Mehsud"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6443824404761904}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, false, true, false, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-4399", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-10182", "mrqa_triviaqa-validation-3272", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-493", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-5556", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-44", "mrqa_searchqa-validation-16347", "mrqa_searchqa-validation-16364", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-3697"], "SR": 0.578125, "CSR": 0.5538194444444444, "EFR": 1.0, "Overall": 0.730060763888889}, {"timecode": 36, "before_eval_results": {"predictions": ["some 27-30%", "Pat McCormick", "1984 Summer Olympics in Los Angeles", "heads of federal executive departments who form the Cabinet of the United States", "reservoirs at high altitudes", "Tim Passmore", "a simple majority vote", "the court from its members for a three - year term", "bacteria", "NCIS Special Agent in Charge", "congressional districts", "The Mexican Seismic Alert System", "the root cells", "March 16, 2018", "madonna", "hare", "nicaragua", "Heston Blumenthal", "color blindness", "jonathan", "Shirley Bassey", "cahawba", "Jupiter", "nicaragua", "Crippen", "the Mendip Hills", "Tallaght, South Dublin", "Tropical Storm Ann", "Christina Ricci", "ten", "Derry", "Koch Industries", "YouTube", "1978", "Arthur Miller", "over 1 million", "Allies of World War I, or Entente Powers", "town in Grafton County, New Hampshire, United States", "African National Congress Deputy President Kgalema Motlanthe", "Thursday night,", "Red Cross, UNHCR and UNICEF", "\"People of Palestine\"", "the Arab world to use the Internet for fun and not interfere with government and serious issues,", "nicaragua", "authorizing killings and kidnappings by paramilitary death squads.", "forgery and flying without a valid license,", "Intensifying", "more than 200", "\"Public Enemies\"", "MDC offshoot", "Dick Cheney's", "Norah Jones", "king David", "Casablanca", "Sympathy for the Devil", "macaroon", "jutted out into a body of water", "Risk", "Alien", "New York Harbor", "Robber Barons or Captains of Industry", "the Caucasus Mountains", "Vichy", "So You"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4711084602531971}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, true, false, true, false, false, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, true, false, false, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.4444444444444444, 0.0, 0.3076923076923077, 1.0, 0.8, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2105263157894737, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-3347", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-8982", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-975", "mrqa_naturalquestions-validation-1682", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6220", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-888", "mrqa_triviaqa-validation-1718", "mrqa_triviaqa-validation-1857", "mrqa_triviaqa-validation-4679", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-5740", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-3392", "mrqa_searchqa-validation-11138", "mrqa_searchqa-validation-10163", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-7350", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-171", "mrqa_searchqa-validation-2085"], "SR": 0.390625, "CSR": 0.5494087837837838, "EFR": 1.0, "Overall": 0.7291786317567568}, {"timecode": 37, "before_eval_results": {"predictions": ["Wisdom, Compassion, Justice and Integrity", "Alex Ryan", "16,801", "1939", "Detroit Red Wings", "Dalveer Bhandari", "Payaya Indians", "Gibraltar", "over 38 million", "4,840", "Mirabilis", "ideology", "23 November 1996", "Swedish figure skater Gillis Grafstr\u00f6m ( 3 gold, 1 silver )", "Pontiac Silverdome", "cricket", "2005", "telstar", "Nikkei", "jack Nicholson", "Munich", "tenerife", "Papua New Guinea", "bridge", "King Idris", "July 14th 1789", "The Rural Electrification Act of 1936", "Thomas Allen", "Adolfo Rodr\u00edguez Sa\u00e1", "20 July 1981", "Peterhouse, Cambridge", "Visigoths", "34", "\"Kill Your Darlings\"", "Christopher Nolan", "David May", "April 8, 1943", "Leucippus", "two", "Nick Adenhart", "\"We're not going to forget you in Washington, D.C.\"", "New York City", "a drug reportedly found after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "Barack Obama", "\"I'm just getting started.\"", "Omar bin Laden", "five Texas A&M University crew mates", "Sharon Bialek", "Henry,", "Antichrist", "inch", "herbicides", "the Chief", "Tom Cruise", "Philippines", "tomato", "Ben Siegel", "Oliver!", "arrows", "Wolfgang Johannes Puck", "Chile", "Mar 18, 2015", "Alien", "Ezra Jennings's opium addiction"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6044270833333333}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-9163", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-6486", "mrqa_triviaqa-validation-633", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-236", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3997", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-1972", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-4027", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-10939", "mrqa_searchqa-validation-6565", "mrqa_searchqa-validation-3831", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-4180", "mrqa_searchqa-validation-65", "mrqa_triviaqa-validation-2605"], "SR": 0.515625, "CSR": 0.5485197368421053, "EFR": 1.0, "Overall": 0.7290008223684211}, {"timecode": 38, "before_eval_results": {"predictions": ["194", "Sunset Publishing Corporation", "16 April 1922", "Thrushcross Grange", "September 10, 1993", "Zara Kate Bate", "their unusual behavior", "Ray Romano", "Bundesliga", "Overijssel, Netherlands", "Budget Rent a Car", "the Atlanta Athletic Club", "1986", "Sir Seretse Goitsebeng Maphiri Khama,", "Richie Cunningham", "kia", "March 31, 2018", "March 1995", "September 21, 2017", "January 17, 1899", "Super Bowl LII,", "Heather Stebbins", "Arkansas", "1,228 km / h ( 763 mph )", "Andaman and Nicobar Islands -- Port Blair", "Austria - Hungary", "Canada", "Nicolas Sarkozy", "kyu", "a sentence", "the Ordovices", "anesthetic", "90%", "South Africa", "Epiphany", "Emmy", "Eleanor Roosevelt", "Charlotte's Web", "Dennis Davern,", "17", "misdemeanor assault charges", "take on greenhouse gas emissions.", "animated films", "1994", "\"Turkey is one of the few -- possibly the only -- NATO member that has deep religious, cultural and historic knowledge of both Afghanistan and Pakistan.", "ketamine, an animal tranquilizer, can put users in a dazed stupor for about two hours,", "The Al Nisr Al Saudi", "French trimaran l'Hydroptere", "3 p.m. Wednesday", "Iran", "codemonkey13981", "volcanic eruptions", "a needle", "Rhizo", "Popular Science magazine", "Xerox", "Mount Rushmore", "Michelle Pfeiffer", "a transit vehicle", "Louis Brandeis", "May", "Morgan", "the International Campaign to Abolish Nuclear Weapons ( ICAN )", "the President"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5835565476190476}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5000000000000001, 0.0, 0.5, 0.16666666666666663, 0.14285714285714288, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-1218", "mrqa_naturalquestions-validation-6634", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-7415", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-1480", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-6002", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-1451", "mrqa_searchqa-validation-1559", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-11590", "mrqa_searchqa-validation-1349", "mrqa_searchqa-validation-4537", "mrqa_searchqa-validation-9322", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-9745", "mrqa_searchqa-validation-2980"], "SR": 0.484375, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.7286718750000001}, {"timecode": 39, "before_eval_results": {"predictions": ["Stephen Greenblatt", "Billy J. Kramer", "2.1 million", "British", "Happy Death Day", "ITV", "Jean de Florette", "Harvard", "\"novel with a key\"", "an Anglo-Saxon saint", "Animorphs", "Caesars Entertainment Corporation", "Romeo", "Autopia", "the financial statement showing a firm's assets, liabilities and equity ( capital ) at a set point in time, usually the end of the fiscal year reported on the accompanying income statement", "al - Mamlakah al - \u02bbArab\u012byah", "Saint Alphonsa", "Sophia Akuffo", "John Travolta", "New England", "Daya", "Antigonon leptopus", "the referee", "1959", "Nick Kroll", "Morgan Freeman", "Andy Warhol", "rugby", "Betelgeuse", "eucalyptus", "Canada", "h. H. Asquith", "lillian Randolph", "Madagascar", "tempera", "Holly Johnson", "Rossano Brazzi", "al Abyad", "Chinese President Hu Jintao.", "school, their books burned, as the hard-core Islamic militants spread their reign of terror across parts of Pakistan.", "\"momentous discovery\"", "for World War II in killings at a Nazi German death camp in Poland.", "Arsene Wenger", "Gov. Mark Sanford", "bronze medal in the women's figure skating final,", "seven", "$40 billion", "some of the world and Africa in particular?\"", "ISLAMABAD,", "\"Oprah is an angel, she is God-sent,\"", "an 8.52-carat diamond", "The Jetsons", "the Battle of Ypres", "a trumpet", "Riga", "Thelonious Monk", "Queen Mary II", "minority stockholders", "shrew", "Gangbusters", "an 'unconscious' mental process or event,", "some really good steaks", "HMS Amethyst", "Spearchucker"], "metric_results": {"EM": 0.5, "QA-F1": 0.5577256944444444}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.1111111111111111, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-1901", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-4321", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-212", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-4302", "mrqa_triviaqa-validation-1615", "mrqa_triviaqa-validation-7252", "mrqa_triviaqa-validation-3228", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-5839", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-6396", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-2114", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-1134", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-7000", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-7978", "mrqa_searchqa-validation-5470", "mrqa_searchqa-validation-11357", "mrqa_searchqa-validation-15555", "mrqa_triviaqa-validation-7264"], "SR": 0.5, "CSR": 0.545703125, "EFR": 0.96875, "Overall": 0.7221875000000001}, {"timecode": 40, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1037", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1377", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1563", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1595", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1740", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1901", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2715", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3166", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3718", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3773", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-3828", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-4675", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5170", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-5827", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-664", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10561", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1222", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-371", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4559", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-6417", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8937", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9939", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1969", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-2218", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-3495", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4162", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-658", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-99", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-1119", "mrqa_searchqa-validation-11190", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12195", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-12663", "mrqa_searchqa-validation-12694", "mrqa_searchqa-validation-12857", "mrqa_searchqa-validation-1320", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-14095", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-15057", "mrqa_searchqa-validation-15219", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15788", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-1604", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-16321", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-16866", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-2534", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3315", "mrqa_searchqa-validation-3389", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-3797", "mrqa_searchqa-validation-3849", "mrqa_searchqa-validation-3890", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-411", "mrqa_searchqa-validation-5165", "mrqa_searchqa-validation-5285", "mrqa_searchqa-validation-5352", "mrqa_searchqa-validation-5442", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6423", "mrqa_searchqa-validation-65", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-6637", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-6783", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-7115", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7589", "mrqa_searchqa-validation-7643", "mrqa_searchqa-validation-7646", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-8475", "mrqa_searchqa-validation-8528", "mrqa_searchqa-validation-8999", "mrqa_searchqa-validation-9118", "mrqa_searchqa-validation-9143", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-9272", "mrqa_searchqa-validation-9486", "mrqa_squad-validation-10106", "mrqa_squad-validation-10223", "mrqa_squad-validation-10410", "mrqa_squad-validation-10416", "mrqa_squad-validation-1049", "mrqa_squad-validation-1176", "mrqa_squad-validation-1226", "mrqa_squad-validation-1299", "mrqa_squad-validation-1391", "mrqa_squad-validation-1530", "mrqa_squad-validation-1568", "mrqa_squad-validation-1690", "mrqa_squad-validation-173", "mrqa_squad-validation-1766", "mrqa_squad-validation-1784", "mrqa_squad-validation-1850", "mrqa_squad-validation-1872", "mrqa_squad-validation-2171", "mrqa_squad-validation-2324", "mrqa_squad-validation-2361", "mrqa_squad-validation-2402", "mrqa_squad-validation-2489", "mrqa_squad-validation-2840", "mrqa_squad-validation-2914", "mrqa_squad-validation-2956", "mrqa_squad-validation-3197", "mrqa_squad-validation-3218", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3300", "mrqa_squad-validation-3378", "mrqa_squad-validation-3410", "mrqa_squad-validation-353", "mrqa_squad-validation-3530", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3799", "mrqa_squad-validation-3938", "mrqa_squad-validation-4003", "mrqa_squad-validation-4127", "mrqa_squad-validation-415", "mrqa_squad-validation-4320", "mrqa_squad-validation-4439", "mrqa_squad-validation-4484", "mrqa_squad-validation-4486", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-4708", "mrqa_squad-validation-4764", "mrqa_squad-validation-4801", "mrqa_squad-validation-4954", "mrqa_squad-validation-5129", "mrqa_squad-validation-5197", "mrqa_squad-validation-5260", "mrqa_squad-validation-5272", "mrqa_squad-validation-5320", "mrqa_squad-validation-5326", "mrqa_squad-validation-5360", "mrqa_squad-validation-5480", "mrqa_squad-validation-551", "mrqa_squad-validation-5597", "mrqa_squad-validation-5631", "mrqa_squad-validation-5941", "mrqa_squad-validation-5951", "mrqa_squad-validation-6119", "mrqa_squad-validation-6282", "mrqa_squad-validation-6480", "mrqa_squad-validation-6505", "mrqa_squad-validation-6610", "mrqa_squad-validation-6639", "mrqa_squad-validation-6750", "mrqa_squad-validation-677", "mrqa_squad-validation-6795", "mrqa_squad-validation-6810", "mrqa_squad-validation-6812", "mrqa_squad-validation-6896", "mrqa_squad-validation-6978", "mrqa_squad-validation-7090", "mrqa_squad-validation-7154", "mrqa_squad-validation-718", "mrqa_squad-validation-7201", "mrqa_squad-validation-7293", "mrqa_squad-validation-7300", "mrqa_squad-validation-742", "mrqa_squad-validation-7480", "mrqa_squad-validation-7490", "mrqa_squad-validation-7565", "mrqa_squad-validation-770", "mrqa_squad-validation-7723", "mrqa_squad-validation-7788", "mrqa_squad-validation-7789", "mrqa_squad-validation-7945", "mrqa_squad-validation-7982", "mrqa_squad-validation-8012", "mrqa_squad-validation-811", "mrqa_squad-validation-8213", "mrqa_squad-validation-8269", "mrqa_squad-validation-8498", "mrqa_squad-validation-8500", "mrqa_squad-validation-8658", "mrqa_squad-validation-8744", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9038", "mrqa_squad-validation-912", "mrqa_squad-validation-916", "mrqa_squad-validation-934", "mrqa_squad-validation-9476", "mrqa_squad-validation-9480", "mrqa_squad-validation-9498", "mrqa_squad-validation-9590", "mrqa_squad-validation-9643", "mrqa_squad-validation-9694", "mrqa_squad-validation-9697", "mrqa_squad-validation-9717", "mrqa_squad-validation-972", "mrqa_squad-validation-9732", "mrqa_squad-validation-9776", "mrqa_squad-validation-9810", "mrqa_squad-validation-9818", "mrqa_squad-validation-9859", "mrqa_squad-validation-9900", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1158", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1852", "mrqa_triviaqa-validation-1857", "mrqa_triviaqa-validation-190", "mrqa_triviaqa-validation-1923", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3175", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3279", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-3818", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4381", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4703", "mrqa_triviaqa-validation-4844", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-5168", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5615", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-584", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-596", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6396", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-6814", "mrqa_triviaqa-validation-6910", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7549", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-7662", "mrqa_triviaqa-validation-7665", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-839"], "OKR": 0.83984375, "KG": 0.46796875, "before_eval_results": {"predictions": ["within a few hundred feet of each other", "vitamin injections that promise to improve health and beauty.", "30", "an upper respiratory infection,\"", "two Metro transit trains that crashed the day before, killing nine,", "Manmohan Singh", "in Jaipur", "in the military, yet they fought on opposing sides.", "The father of Haleigh Cummings,", "in the 20 years since the Berlin Wall has fallen there has been a renaissance of the game in the region.", "more than 15,000", "Samoa's case seems to be unique because it is steering in the opposite direction.", "seven", "glass shards", "1991", "the head of the Imperial Family", "a bolus of food to enter", "hydrological cycle or the hydrologic cycle", "Mike Czerwien", "18 types of Pok\u00e9mon, only eleven exist in the TCG", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad", "food rationing", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins", "defense against rain rather than sun", "18", "two 1.6 loz (45.4 g) 100 per cent beef patties", "the S&DR", "South Africa", "tunisia", "tuncak Jaya (Carstenz Pyramid) beats Mount Kosciusko", "lorne Greene", "lance", "my favorite martian,", "Portugal", "conchita wurst", "Dodge Ram cars", "telstar", "Reverend Timothy \"Tim\" Lovejoy", "luchadora", "Girls' Generation", "Anna Pavlova", "6'5\"", "Martha Coolidge", "16 November 1973", "Centennial Olympic Stadium", "David Anthony O'Leary", "Andalusia, Spain", "Washington", "Prince Antoni Radziwi\u0142\u0142", "epsilon", "Sweden", "Madagascar", "Tom Sennett", "constellation", "champagne", "Like Water for Chocolate", "Sputnik I", "Joseph Smith", "the DASB Executive Advisory Committee", "Hangman", "Poldark", "Budapest", "Best Buy"], "metric_results": {"EM": 0.5, "QA-F1": 0.5940231643356644}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.4444444444444444, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.36363636363636365, 0.3333333333333333, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.22222222222222224, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.7999999999999999, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-1617", "mrqa_newsqa-validation-1459", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1319", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-6810", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-7425", "mrqa_triviaqa-validation-4722", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-4437", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-1492", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-4155", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-9355", "mrqa_searchqa-validation-44"], "SR": 0.5, "CSR": 0.5445884146341464, "EFR": 0.9375, "Overall": 0.7001676829268293}, {"timecode": 41, "before_eval_results": {"predictions": ["Industry and manufacturing", "possible victims of physical and sexual abuse.", "Brian David Mitchell,", "$250,000 for Rivers' charity: God's Love We Deliver.", "Ferraris, a Lamborghini and an Acura NSX", "Larry Ellison,", "Nkepile M abuse", "Seminole Indian Tribe", "18", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "Ross Perot", "byproducts emitted during the process of burning and melting raw materials.", "\"utterly baseless,\"", "Dolgorsuren Dagvadorj,", "the town of Acolman, just north of Mexico City", "1898", "performers must receive the highest number of votes, and also greater than 50 % of the votes", "Isaiah Amir Mustafa", "political ideology", "different philosophers and statesmen have designed different lists of what they believe to be natural rights ; almost all include the right to life and liberty as the two highest priorities", "crowned the dome of the U.S. Capitol building", "In the 1979 -- 80 season", "the nature of Abraham Lincoln's war goals", "the breast or lower chest of beef or veal", "Teri Hatcher", "in capillaries, alveoli, glomeruli, outer layer of skin", "six", "Alex Murphy", "the Lyrical Ballads", "Martin Luther King", "Rio de Janeiro", "piano", "Pyrrhus", "Nigel Short", "German Home Guard", "georgia", "charlie georgia", "brash", "Scott Mosier", "Leslie Knope", "between 1535\u201336", "the Prescription Drug User Fee Act", "University of Oxford", "Bruce Grobbelaar", "Rockhill Furnace, Pennsylvania", "the Chicago Bears", "Brookhaven", "Juventus", "'valley of the hazels'", "January 2016", "Mickey Spillane", "Thurman Munson", "Exodus", "The 82nd Annual Academy Awards", "Monica Samille Lewinsky", "the Pacific and Atlantic oceans", "Amish", "coffee", "the foot", "the United States", "Fettuccine", "baseball", "Benjam\u00edn", "pubs, bars and restaurants"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6377645502645503}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, false, true, false, true, false, false, true, false, true, true, false, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 0.962962962962963, 1.0, 1.0, 0.0, 0.2222222222222222, 0.8571428571428571, 0.2222222222222222, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-1122", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-2133", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-1677", "mrqa_triviaqa-validation-5985", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-1963", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5747", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-797", "mrqa_searchqa-validation-16897", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-10356", "mrqa_searchqa-validation-16878", "mrqa_hotpotqa-validation-655"], "SR": 0.515625, "CSR": 0.5438988095238095, "EFR": 0.967741935483871, "Overall": 0.706078149001536}, {"timecode": 42, "before_eval_results": {"predictions": ["The Dornbirner Ach", "cricket fighting", "XVideos", "William Harold \"Bill\" Woodfull", "Pseudolus", "various bigfoot-like sightings, giant snakes and \"thunderbirds.\"", "The Grandmaster", "Miss Universe 2010 Ximena Navarrete", "Charles Hastings Judd", "1952", "Squam Lake", "MGM Resorts International", "Golden Gate National Recreation Area", "William Corcoran Eustis", "sedimentary", "ideology", "an idiom for the most direct path between two points", "126", "March 5, 2014", "The genome", "Kit Harington", "April 26, 2005", "since 3, 1, and 4", "Upon closure at birth", "aiding the war effort", "Vincenzo Peruggia", "south Holland", "gold", "shallow seas", "dassler", "dumfries house", "deoxyribonucleic acid", "rachmaninoff", "british", "an \"on each.\"", "Galileo Galilei", "Rocky Horror Show", "Abbey Theatre", "16", "Ralph Lauren", "Sen. Barack Obama", "2,700-acre", "Jeanne Tripplehorn", "served in the military,", "Karen Floyd", "tie salesman", "bard", "a cease-fire", "5 1/2-year-old son, Ryder Russell,", "Arsene Wenger", "Boston", "Odysseus", "Nine to Five", "the polar bear", "the French and Indian War", "Henry Hudson", "Double Jeopardy", "Calais", "Lois Lane", "falafel", "an island", "treats", "Haiti", "is the world's second most populous country after the People's Republic of China"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6052324054621848}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false, false, false, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.7499999999999999, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.4705882352941177]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-3909", "mrqa_hotpotqa-validation-3426", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-169", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9799", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-3398", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-5163", "mrqa_triviaqa-validation-2777", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-436", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-544", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-5233", "mrqa_searchqa-validation-9276", "mrqa_searchqa-validation-14554", "mrqa_naturalquestions-validation-8420"], "SR": 0.515625, "CSR": 0.5432412790697674, "EFR": 0.9354838709677419, "Overall": 0.6994950300075018}, {"timecode": 43, "before_eval_results": {"predictions": ["November 2006", "an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "the Firth of Forth", "Brigadier General Raden Panji Nugroho Notosusanto", "band", "\"Lonely\"", "Anne Perry", "\"SOS\"", "932", "Sam Roberts Band", "Smoothie King Center", "Indooroopilly Shoppingtown", "1963\u201393", "\"Traumnovelle\"", "Brooklyn Heights", "1939", "the body - centered cubic ( BCC ) lattice", "a donor molecule", "Hank Williams", "Homer Banks, Carl Hampton and Raymond Jackson", "2010", "Kyla Coleman", "the Hongwu Emperor of the Ming Dynasty", "Lewis Hamilton", "the Holy See", "John Goodman", "netherlands", "\"I Wanna Be Like You\"", "Sardinian", "the Herald of Free Enterprise", "Poland", "the Magic Circle", "netherlands", "Leicester", "nether", "John le Carr\u00e9", "netherlands", "netherlands", "Los Alamitos Joint Forces Training Base", "Leaders of more than 30 Latin American and Caribbean nations", "his enjoyment of sex and how he lost his virginity at age 14.", "to alert patients of possible tendon ruptures and tendonitis.", "when daughter Sasha exhibited signs of potentially deadly meningitis,", "bedrooms of their two-floor home", "12", "President Obama", "2002.", "the FDA's", "\"Toy Story\"", "about 1,300 meters in the Mediterranean Sea", "overwhelm", "savings", "jurianus", "diesel", "the 'Great Game'", "friedrich", "South Africa", "Steven Spielberg", "Pitney Bowes", "a microwave oven", "\"Hercule Poirot\"", "Elvis Presley", "shared pairs or bonding pairs", "The Hudson River"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4891327217190704}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false, false, false, true, false, false, false, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.4210526315789474, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.125, 0.27272727272727276, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-2853", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-2954", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-3048", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-722", "mrqa_triviaqa-validation-1419", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-2269", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-613", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-202", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-1800", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-800", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-14422", "mrqa_searchqa-validation-7587", "mrqa_searchqa-validation-11850", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-2017", "mrqa_searchqa-validation-11892", "mrqa_naturalquestions-validation-7107"], "SR": 0.421875, "CSR": 0.5404829545454546, "EFR": 1.0, "Overall": 0.7118465909090909}, {"timecode": 44, "before_eval_results": {"predictions": ["the Warsaw Uprising Museum", "the Saddledome", "once", "Scandinavian design", "2008 Summer Olympic Games", "Juventus", "1968", "FX", "Sid Vicious", "1998", "John Anthony \"Jack\" White", "Boyd Gaming", "Double Agent", "Thon MarialMaker", "Acid rain", "Around 1200", "toys or doorbell installations", "China", "1992 to 2013", "Morgan Freeman", "`` a blind fury ''", "2001", "foreign investors", "Sir Rowland Hill", "Bonnie Aarons as Valak", "Dan Stevens", "france", "rugby school", "france", "pangrams", "Gower Peninsula", "Narragansett Bay", "Zork", "Peshtigo, Wisconsin", "Today newspaper", "Home Alone 2: Lost in New York", "drag club", "pink", "as he tried to throw a petrol bomb at the officers,", "Three French journalists, a seven-member Spanish flight crew and one Belgian", "Acura MDXA", "July 4.", "GM and Chrysler,", "Roger Federer", "around 1610,", "Nazi Party members", "Yusuf Saad Kamel", "July 1999.", "Chris Robinson,", "Democratic National Convention", "A Few Good Men", "Brown University", "Fidelio", "bone", "Macaulay Culkin", "Billie Joe Armstrong", "Northern Exposure", "American romantic comedy sports film", "Henry Cisneros", "Jacqueline Kennedy", "Marie Antoinette", "William Conrad", "sydney and Hobart", "reckless arson"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5883689435919295}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.17391304347826084, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.47058823529411764, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-375", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-839", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-2648", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-5613", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-4430", "mrqa_triviaqa-validation-3275", "mrqa_triviaqa-validation-3244", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-123", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-2421", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-423", "mrqa_searchqa-validation-12912", "mrqa_searchqa-validation-10831", "mrqa_searchqa-validation-14862", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-7791", "mrqa_triviaqa-validation-1457", "mrqa_triviaqa-validation-7370"], "SR": 0.4375, "CSR": 0.5381944444444444, "EFR": 1.0, "Overall": 0.7113888888888888}, {"timecode": 45, "before_eval_results": {"predictions": ["Germany and Austria", "Djibouti and Yemen", "Zengo", "Patrick Henry", "harry atlantic", "Matlock", "Belgium", "can any one help me", "Peter Ackroyd", "carbon dioxide", "11", "obtaining and proper handling of human blood", "Baluch rugs", "Anna Caterina Antonacci", "Golden Gate Bridge", "April 7, 2016", "early 20th century", "Eurasian Plate", "Michael Crawford", "the heart", "mathematical model", "111", "a theory", "President Lyndon Johnson", "electric potential generated", "Michael Moriarty", "Barbara Niven", "Supremes", "1974", "\"Love the Way You Lie\"", "archery bow", "Tsung-Dao Lee", "1949", "St. Vincent", "1899", "Gal Gadot", "John Lennon", "2009", "Grayback forest-firefighters", "five female pastors", "Darrel Mohler", "natural gas", "Friday,", "protective shoes", "Swiss holders Alinghi", "Ryder Russell,", "Russian residents and worldwide viewers, in English or in Russian,", "issued his first military orders as leader of North Korea", "Brazil", "eight", "Iconoclasm", "Tiger", "Exxon Corporation", "chancellor", "food combining", "General Andrew Jackson", "pumice", "megabytes", "eucalyptus", "Maria Montessori", "pot roast", "astrological", "Brooke Hogan", "Over the Rainbow"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6240214646464646}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-943", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-89", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-3552", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-6243", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-1507", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-961", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-4069", "mrqa_searchqa-validation-13958", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-572", "mrqa_searchqa-validation-8152"], "SR": 0.578125, "CSR": 0.5390625, "EFR": 1.0, "Overall": 0.7115625}, {"timecode": 46, "before_eval_results": {"predictions": ["The best-known legend", "a squid", "Sir Anthony Eden", "Glory", "Jackie Kennedy", "Tanzania", "Don Quixote", "Buddhism", "IBM", "Mercury", "Menninger", "a groom", "PEZ", "a hollaback girl", "1986", "$2.187 billion", "Donna", "10 June 1940", "Americans who served in the armed forces and as civilians during World War II", "a Boeing 767 - 200ER", "Andrew Michael Harrison", "Bob Dylan", "a component of starch", "the Sui", "Gustav Bauer", "1913", "biathlon", "embellish", "Mohandas Karamchand Gandhi", "Big Ben", "the solar system", "tomb raiser", "marsupial", "Alice", "Toll House cookies", "tosca", "tunis", "atlantic", "Tainted Love", "sixth", "R\u00edo Grande", "Everton", "Tom Kitt", "Demi Lovato, Rainn Wilson", "11,791", "pronghorn", "KULR-TV", "Big 12 Conference", "1976", "My Beautiful Dark Twisted Fantasy", "for death squad killings carried out during his rule in the 1990s.", "Switzerland", "North Korea", "second-degree attempted murder and conspiracy,", "debris", "Pakistan's", "a city of romance, of incredible architecture and history.", "The Sopranos", "Ketchum, Idaho.", "16", "on the Ohio River near Warsaw, Kentucky,", "Adam Lambert and Kris Allen", "HMS Thunderbolt", "the French & Indian War"], "metric_results": {"EM": 0.546875, "QA-F1": 0.607998511904762}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10673", "mrqa_searchqa-validation-9659", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-15080", "mrqa_searchqa-validation-6632", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-7910", "mrqa_triviaqa-validation-4300", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-6264", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-6150", "mrqa_triviaqa-validation-6480", "mrqa_hotpotqa-validation-1103", "mrqa_hotpotqa-validation-5052", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-3992", "mrqa_hotpotqa-validation-5621", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-2573", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-4334"], "SR": 0.546875, "CSR": 0.5392287234042553, "EFR": 1.0, "Overall": 0.711595744680851}, {"timecode": 47, "before_eval_results": {"predictions": ["Thomas Merton", "Morocco", "Douglas MacArthur", "Okinawa", "Lake Victoria", "the oil industry", "Walter Reed", "the gallbladder", "Out of Africa", "Dublin", "Marseille", "a light year", "archery", "awarded to the team that lost the pre-game coin toss", "The Massachusetts Compromise", "the southwestern part of the island", "Garfield Sobers", "the Portuguese common surname Pereira", "Burj Khalifa", "Aristotle", "1908", "54 Mbit / s, plus error correction code", "Jack Barry", "Bill Hayes", "Wisconsin", "argentina", "british state", "mongoose", "nine", "gwynne", "extreme romanticism", "Vietnam", "tunisia", "John McCarthy", "tobacco", "playing cards", "Leonard Nimoy", "Rana Daggubati", "Philadelphia", "Bart Conner", "About 200", "Tamaulipas", "1986", "private", "The Dragon School in Oxford", "Aamir Khan", "Arrowhead Stadium", "Standard Oil", "Matt Groening", "2000.", "voluntary manslaughter", "10,000", "Robert Mugabe", "animal rights activist Jasmin Singer", "different women coping with breast cancer", "338", "ultra-high-strength steel and boron", "Malawi", "North Korea", "Belfast's", "Symbionese Liberation Army", "p Larson Brown", "Dick Van Dyke", "Pope Benedict XVI"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6622767857142857}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, false, true, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.28571428571428575, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-121", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-6603", "mrqa_triviaqa-validation-3680", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-6284", "mrqa_triviaqa-validation-1650", "mrqa_triviaqa-validation-517", "mrqa_triviaqa-validation-3984", "mrqa_triviaqa-validation-1101", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-827", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-3946", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-442", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3097", "mrqa_triviaqa-validation-7750"], "SR": 0.578125, "CSR": 0.5400390625, "EFR": 1.0, "Overall": 0.7117578124999999}, {"timecode": 48, "before_eval_results": {"predictions": ["bears", "the Vice President of the United States", "roundabout", "gravity", "a Ouija board", "salsus", "Larry King", "the pipa", "the Nez Perce", "\"9 To 5\"", "Thor", "Mount Hood", "Superman Homepage", "Isley Brothers", "Freddie Highmore", "March 16, 2018", "Nepal", "ulcerative colitis", "Ferm\u00edn Francisco de Lasu\u00e9n", "9 January 2018", "1933", "Humphrey Bogart, Ingrid Bergman, and Paul Henreid", "the biblical Book of Exodus", "23 % of GDP", "1930s", "tunzanian", "mushrooms", "Japan", "Francis Drake", "the largest and most dramatic coastline cave in Britain", "Perkins Chapel", "Jean-Paul Gaultier", "Vienna", "Sheffield United", "\" Lost in Translation\"", "Dawn French", "murder/mystery series", "Matt Groening", "Bulgarian-Canadian", "Valeri Vladimirovich \"Val\" Bure", "Get Him to the Greek", "Balloon Street, Manchester", "New York Islanders", "\" training Day\"", "his exploration and settlement of what is now Kentucky", "Burny Mattinson", "the Paris Peace Treaties", "Tucum\u00e1n", "August 19, 2013", "held in a trust fund", "\"Nu au Plateau de Sculpteur,\"", "record deal.", "Amanda Knox's aunt", "a ban on inflatable or portable signs and banners on public property.", "DNA evidence", "Alan Graham", "allegations that a dorm parent mistreated students at the school.", "1616.", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Fullerton, California,", "the boy's Brazilian mother, Bruna Bianchi Carneiro Ribeiro, told Goldman -- to whom she was then married", "sent an e-mail to reporters", "Karl Kr\u00f8yer", "The patient, who prefers to be anonymous,"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5381535947712418}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, true, true, true, true, false, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, true, true, false, true, false, false, false, false], "QA-F1": [0.4, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.8571428571428571, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.2222222222222222, 0.8, 0.0, 1.0, 1.0, 0.35294117647058826, 1.0, 0.28571428571428575, 0.5714285714285715, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9690", "mrqa_searchqa-validation-12336", "mrqa_searchqa-validation-13769", "mrqa_searchqa-validation-12473", "mrqa_searchqa-validation-4046", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-2991", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-5483", "mrqa_triviaqa-validation-7656", "mrqa_triviaqa-validation-2540", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-1476", "mrqa_triviaqa-validation-4694", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-2396", "mrqa_hotpotqa-validation-3083", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-3197", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-56", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1092"], "SR": 0.421875, "CSR": 0.5376275510204082, "EFR": 1.0, "Overall": 0.7112755102040815}, {"timecode": 49, "before_eval_results": {"predictions": ["Driving Miss Daisy", "Margaret", "Bronchoconstriction", "Legoland", "William Henry Harrison", "Abraham Lincoln", "Floyd Mayweather Jr", "Isadora Duncan", "Beth Israel Deaconess Medical Center", "Venice", "Scarlet fever", "fog", "the BBC", "January 1, 2016", "George III's German - born wife, Charlotte of Mecklenburg - Strelitz", "Andrew Lloyd Webber", "supported modern programming practices and enabled business applications to be developed with Flash", "The Geography of Oklahoma", "Rashida Jones", "Representatives", "fascia surrounding skeletal muscle", "Harold Godwinson", "its vast territory was divided into several successor polities", "Robin Cousins, Jason Gardiner, Barber and Ashley Roberts", "Buddhism", "Howard Hoagland \"Hoagy\" Carmichael", "Secretary of State", "1960", "about 300 miles (480 km)", "Macbeth", "Cold Comfort Farm", "Prussian 2nd Army", "jackstones", "a horizontal desire", "Annie Lennox", "leaf", "Don Quixote", "the Mayor of the City of New York", "more than 110 films", "The Killer", "Dundalk", "Lawton Mainor Chiles", "right-hand batsman", "the adult webcam site LiveJasmin", "1997", "Mauthausen-Gusen concentration camp", "French", "Inverness", "Joshua Rowley", "Tupolev TU-160, pictured here in 2003, is a long-range strategic bomber.", "producing rock music with a country influence.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Buenos Aires.", "Heshmat Tehran Attarzadeh", "some of the most hostile war zones,", "Teresa Hairston", "two years,", "free enterprise in history", "the \"face of the peace initiative has been attacked.\"", "a tanker", "Nouri al-Maliki", "GM", "Bagel set", "graffiti art"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6246203449328449}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, false, true, false, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.8, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-14051", "mrqa_searchqa-validation-16940", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-2525", "mrqa_searchqa-validation-246", "mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1786", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-7501", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7262", "mrqa_hotpotqa-validation-4004", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-1315", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-5267", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2475", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-3671", "mrqa_triviaqa-validation-2662", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-3671"], "SR": 0.484375, "CSR": 0.5365625, "EFR": 1.0, "Overall": 0.7110624999999999}, {"timecode": 50, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1563", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1595", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-1740", "mrqa_hotpotqa-validation-1773", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-2919", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3718", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3773", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4233", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5170", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5702", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5827", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-5848", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-5882", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-664", "mrqa_hotpotqa-validation-957", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-3738", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4488", "mrqa_naturalquestions-validation-4559", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6399", "mrqa_naturalquestions-validation-6417", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9939", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1969", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3495", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3801", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-658", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-99", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-10939", "mrqa_searchqa-validation-11190", "mrqa_searchqa-validation-11459", "mrqa_searchqa-validation-11470", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-12694", "mrqa_searchqa-validation-12955", "mrqa_searchqa-validation-1320", "mrqa_searchqa-validation-13311", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-14095", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-1506", "mrqa_searchqa-validation-15219", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-16321", "mrqa_searchqa-validation-16588", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-16866", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-171", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-2525", "mrqa_searchqa-validation-3007", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3315", "mrqa_searchqa-validation-3389", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-3785", "mrqa_searchqa-validation-3849", "mrqa_searchqa-validation-3890", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-411", "mrqa_searchqa-validation-498", "mrqa_searchqa-validation-5285", "mrqa_searchqa-validation-5352", "mrqa_searchqa-validation-5442", "mrqa_searchqa-validation-5470", "mrqa_searchqa-validation-5676", "mrqa_searchqa-validation-5705", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6423", "mrqa_searchqa-validation-6465", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-6651", "mrqa_searchqa-validation-6783", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7115", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7646", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-8528", "mrqa_searchqa-validation-8999", "mrqa_searchqa-validation-9118", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-9272", "mrqa_squad-validation-10106", "mrqa_squad-validation-10223", "mrqa_squad-validation-10410", "mrqa_squad-validation-1049", "mrqa_squad-validation-1226", "mrqa_squad-validation-1299", "mrqa_squad-validation-1391", "mrqa_squad-validation-1530", "mrqa_squad-validation-1568", "mrqa_squad-validation-1690", "mrqa_squad-validation-173", "mrqa_squad-validation-1766", "mrqa_squad-validation-1784", "mrqa_squad-validation-1850", "mrqa_squad-validation-1872", "mrqa_squad-validation-2171", "mrqa_squad-validation-2324", "mrqa_squad-validation-2402", "mrqa_squad-validation-2914", "mrqa_squad-validation-2956", "mrqa_squad-validation-3197", "mrqa_squad-validation-3218", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3378", "mrqa_squad-validation-3410", "mrqa_squad-validation-353", "mrqa_squad-validation-3530", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3799", "mrqa_squad-validation-3938", "mrqa_squad-validation-4003", "mrqa_squad-validation-415", "mrqa_squad-validation-4439", "mrqa_squad-validation-4484", "mrqa_squad-validation-4486", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-4764", "mrqa_squad-validation-4801", "mrqa_squad-validation-4954", "mrqa_squad-validation-5197", "mrqa_squad-validation-5260", "mrqa_squad-validation-5272", "mrqa_squad-validation-5320", "mrqa_squad-validation-5326", "mrqa_squad-validation-5360", "mrqa_squad-validation-5480", "mrqa_squad-validation-551", "mrqa_squad-validation-5631", "mrqa_squad-validation-5941", "mrqa_squad-validation-5951", "mrqa_squad-validation-6119", "mrqa_squad-validation-6480", "mrqa_squad-validation-6505", "mrqa_squad-validation-6639", "mrqa_squad-validation-6750", "mrqa_squad-validation-6795", "mrqa_squad-validation-6810", "mrqa_squad-validation-6812", "mrqa_squad-validation-6896", "mrqa_squad-validation-6978", "mrqa_squad-validation-7090", "mrqa_squad-validation-718", "mrqa_squad-validation-7201", "mrqa_squad-validation-7293", "mrqa_squad-validation-7490", "mrqa_squad-validation-7565", "mrqa_squad-validation-770", "mrqa_squad-validation-7723", "mrqa_squad-validation-7789", "mrqa_squad-validation-7945", "mrqa_squad-validation-8012", "mrqa_squad-validation-811", "mrqa_squad-validation-8498", "mrqa_squad-validation-8500", "mrqa_squad-validation-8658", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9038", "mrqa_squad-validation-912", "mrqa_squad-validation-916", "mrqa_squad-validation-9240", "mrqa_squad-validation-934", "mrqa_squad-validation-9476", "mrqa_squad-validation-9480", "mrqa_squad-validation-9643", "mrqa_squad-validation-9694", "mrqa_squad-validation-9810", "mrqa_squad-validation-9818", "mrqa_squad-validation-9859", "mrqa_squad-validation-9900", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-1476", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-190", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3108", "mrqa_triviaqa-validation-3175", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3279", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-392", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4381", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4694", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5615", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-584", "mrqa_triviaqa-validation-5848", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-596", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6085", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-6814", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-6956", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-7117", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7501", "mrqa_triviaqa-validation-7514", "mrqa_triviaqa-validation-7549", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-7665", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-943"], "OKR": 0.853515625, "KG": 0.49765625, "before_eval_results": {"predictions": ["the skull & crossbones", "a swan", "Swamp Fox", "a Jelly Belly", "Eva Gabor", "Eurydice", "the eye", "foot", "piano", "Papua New Guinea", "William Jennings Bryan", "Apocrypha", "Gallipoli", "Network - Protocol driver ( Middleware driver)", "Sesel Zvidzai", "1773", "The President pro tempore", "Sebastian Lund", "Burbank", "the bank's own funds", "lumbar cistern", "Missouri River", "a line of committed and effective Sultans", "a moral tale", "semi solid and loses its flow characteristics", "shotguns", "China", "white", "Moon River", "Bronx Mowgli", "driving Miss Daisy", "Jeremy Bates", "corporal", "pyramids", "Wimbledon", "zagreb", "700ES", "Ashley Leggat", "\"Sleeping Beauty\"", "ten years of probation", "\"Empire Falls\"", "Imagine", "Patricia Hearst", "Louis \"Louie\" Zamperini", "baeocystin", "Venice", "Ghanaian", "Kirk Humphreys", "the Mayor of the City of New York", "went to listen to the music in 5.1 and we go 'Whoa, listen to that,'", "managing his time.", "Bobby Jindal", "Dr. Jennifer Arnold and husband Bill Klein", "gasoline", "The station", "56", "Lieberman", "Roy Foster's", "three teens", "Don Draper", "April 28", "Brenda", "Jean - Jacques Rousseau's Confessions, his autobiography ( whose first six books were written in 1765, when Marie Antoinette was nine years of age, and published in 1782 )", "April 1979"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5778608309730701}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5454545454545454, 0.0, 1.0, 0.0, 0.0, 0.16666666666666669, 0.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.10526315789473682, 0.5, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.13793103448275862, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8164", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-12114", "mrqa_searchqa-validation-2548", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-5940", "mrqa_triviaqa-validation-689", "mrqa_triviaqa-validation-5385", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-443", "mrqa_hotpotqa-validation-480", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-974", "mrqa_hotpotqa-validation-5070", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-2262", "mrqa_newsqa-validation-2121", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-879", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-493"], "SR": 0.515625, "CSR": 0.5361519607843137, "EFR": 0.967741935483871, "Overall": 0.721403779253637}, {"timecode": 51, "before_eval_results": {"predictions": ["cherries", "Belgium", "Dennis Haysbert", "the Netherlands", "god of god", "time", "a large plate of crayfish", "time", "Jupiter's", "Shropshire", "a red light laser", "timeanddate.com", "Jean Foucault", "1986", "Shreya Bhushan Pethewala", "Database - Protocol driver ( Pure Java driver )", "atomic numbers 1 ( hydrogen ) to 118 ( oganesson )", "1917", "2002", "Johnny Logan", "tradeable entity used to avoid the inconvenienceiences of a pure barter system", "1955", "an instant messaging client", "France", "the physical link between the mRNA and the amino acid sequence of proteins", "Space Oddity", "Maria Ouspenskaya", "red", "Stereophonics", "Greyfriars Bobby", "mater", "Monet", "curling", "Melpomene", "vitamin K", "Wat Tyler", "climatology", "USS Essex (CV-9)", "February 20, 1978", "Kirkcudbright", "dance partner", "Port of Boston", "Lucas Grabeel", "first train robbery,", "Scarface", "evangelical Christian periodical", "the Tallahassee City Commission in February 2003", "The Rebirth", "press conference", "Las Vegas", "Henry Ford", "returning combat veterans", "are set to take up the vacant slot alongside Cameroon international Samuel Eto'o and Ivory Coast midfielder Yaya Toure in the non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "sexual assault on a child.", "top designers, such as Stella McCartney,", "in the west African nation later this year.", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "in the southern port city of Karachi,", "a fractured pelvis and sacrum -- the triangular bone within the pelvis.", "in a public housing project, not too far from the stadium of her favorite team -- the New York Yankees.", "President Omar Bongo,", "port", "Mel Brooks", "feet"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6085305823209048}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, false, true, false, false, true, true, false, true, false, false, true, false, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.5, 1.0, 0.6666666666666666, 0.9, 1.0, 1.0, 1.0, 0.3225806451612903, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5, 0.0, 1.0, 0.8, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.05714285714285715, 0.0, 1.0, 0.0, 0.0, 0.9090909090909091, 0.6666666666666666, 0.4, 0.25, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-13259", "mrqa_searchqa-validation-355", "mrqa_searchqa-validation-4975", "mrqa_searchqa-validation-4573", "mrqa_searchqa-validation-9522", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-7003", "mrqa_triviaqa-validation-4491", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-1088", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-3001", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-3076", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2572", "mrqa_newsqa-validation-3925", "mrqa_triviaqa-validation-6748"], "SR": 0.46875, "CSR": 0.5348557692307692, "EFR": 0.9411764705882353, "Overall": 0.7158314479638009}, {"timecode": 52, "before_eval_results": {"predictions": ["President George Washington", "the Headless Horseman", "William Howard Taft", "the Family Stone", "Saudi Arabia", "Absalom", "the Odyssey", "Xinjiang-Uygur", "Wales", "the Galapagos", "drum", "Big Brown", "Patrick Henry", "the Naturalization Act of 1790", "the forex market", "6 January 793", "Camping World Stadium in Orlando, Florida", "at Tandi, in Lahaul", "Nagar Haveli", "in the 1820s", "Vasoepididymostomy", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "flood barrier", "the nucleus", "Gillen Simone Vangsness", "The Rocky Horror Picture Show", "larges", "Virginia", "China", "Moscow", "sports agent", "Wisconsin", "wickets", "astronomy", "Mary Jane Grant", "Brett Favre", "per annum exclusive", "Vilnius Old Town", "Sam Kinison", "Nova Scotia", "DI Humphrey Goodman", "Mika H\u00e4kkinen", "top division", "Kevin Peter Hall", "a \"homeward bounder\"", "Martha Wainwright", "Chicago", "Blue Origin", "Canada's first train robbery", "education about rainforests.", "a song about freedom of speech.", "worked for Wackenhut Security Inc.", "their emergency plans and consider additional security measures in light of Wednesday's shooting,", "The poster boy of Indian action films", "Paktika province in southeastern Afghanistan,", "the return of a fallen U.S. service member", "unemployment benefits", "South Africa", "July 8", "Vernon Forrest,", "\"Britain's Got Talent\"", "embroidered cloth", "Harry Potter", "Jeremy Bates"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6222284226190475}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, true, false, false, false, true, true, false, false, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.3333333333333333, 0.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666666, 0.375, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12283", "mrqa_searchqa-validation-7845", "mrqa_searchqa-validation-555", "mrqa_searchqa-validation-1640", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-5170", "mrqa_triviaqa-validation-6044", "mrqa_triviaqa-validation-1569", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-1611", "mrqa_hotpotqa-validation-5596", "mrqa_hotpotqa-validation-981", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-283", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-1875", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-3652", "mrqa_triviaqa-validation-412"], "SR": 0.546875, "CSR": 0.5350825471698113, "EFR": 0.9655172413793104, "Overall": 0.7207449577098244}, {"timecode": 53, "before_eval_results": {"predictions": ["the Mormon Tabernacle", "The llama", "Jennifer Lopez", "Cheddar", "Rudy Giuliani", "Ramen", "Wilhelm II", "Hausas", "Arkansas", "the phi phenomenon", "Bowl Championship Series", "nomadic", "Wichita", "Ernest Rutherford", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W \ufeff / \u1eff 26.617 \u00b0 N 81.617", "CBS Television City", "Donald Fauntleroy Duck", "10,000 BC", "24", "Harishchandra", "an expression of unknown origin, that means `` and there it is '' or ``", "January 2017", "1989", "Peter Andrew Beardsley MBE", "the 1980s", "1951", "leicestershire", "gnes Wickfield", "As You Like It", "bees", "Canada", "Buenos Aires", "heart", "World War I", "troopship", "Harry Redknapp", "an earthquake", "roller coaster", "Hawaii", "1826", "5 January 1921", "Miriam Margolyes", "Edward II", "Charles Eug\u00e8ne Jules Marie Nungesser", "film", "Valley Falls", "Lucas Stephen Grabeel", "1966", "Mark Sinclair", "Orlando police", "alert patients of possible tendon ruptures and tendonitis.", "seven or eight", "Inter Milan striker Adriano", "Iran's nuclear program.", "bipartisan", "Mashhad", "Charlotte Gainsbourg and Willem Dafoe", "Joe Jackson's", "The Rev. Alberto Cutie", "Jeannie Longo-Ciprelli", "tells stories of different women coping with breast cancer in five vignettes.", "Clio", "Spanish", "Andes Mountains of Chile and Argentina"], "metric_results": {"EM": 0.5, "QA-F1": 0.614023782589959}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true, false, true, false, false, false, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, false, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.823529411764706, 0.4615384615384615, 1.0, 0.0, 1.0, 0.0, 0.3076923076923077, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.4, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12611", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-10902", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-1988", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-1770", "mrqa_triviaqa-validation-4364", "mrqa_triviaqa-validation-6795", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-3532", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-3363", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-4129", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-3807", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-40", "mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-3984", "mrqa_newsqa-validation-153", "mrqa_triviaqa-validation-5844", "mrqa_triviaqa-validation-3547"], "SR": 0.5, "CSR": 0.5344328703703703, "EFR": 0.96875, "Overall": 0.721261574074074}, {"timecode": 54, "before_eval_results": {"predictions": ["pilgrim", "Montague", "the Pirates of Penzance", "Paul Joseph Goebbels", "berkshire", "the Blue", "the Republican party", "Vipers", "Darren Star", "Catalina", "Alaska", "Cleveland", "Marilyn Monroe", "Nicole Gale Anderson", "James Watson and Francis Crick", "New Orleans", "when the forward reaction proceeds at the same rate as the reverse reaction", "over 74", "Sasha Banks", "Bill McPherson", "free floating", "up to 100,000", "of left coronary artery", "thylakoid membranes", "1800", "berkshire", "cr\u00e8me", "tonsure", "1990", "copper", "1949", "robinson crusoe", "HMS Conqueror", "morphine", "vinegar", "wist", "four red stars", "California", "1941", "Larry Eustachy", "Oregon State Beavers", "Mexico", "shock cavalry", "The Books", "Cecily Legler Strong", "1933", "Steve Carell", "BBC's MediaCityUK", "first baseman", "Most of the 103 children that a French charity attempted to take to France", "Gulf of Aden,", "Adidas", "Authorities in Fayetteville, North Carolina,", "military trials for some Guantanamo Bay detainees.", "seven", "first", "Former Mobile County Circuit Judge Herman Thomas", "poems", "Stansted", "illegitimate.", "Arthur E. Morgan III", "Utena", "$10\u201320 million", "September 25, 2017"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5973892120950943}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, true, true, true, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.8, 0.3333333333333333, 1.0, 0.5, 0.4, 0.8, 0.4, 0.0, 0.0, 1.0, 0.11764705882352941, 0.0, 1.0, 0.888888888888889, 0.10256410256410256, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4626", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-1877", "mrqa_searchqa-validation-15700", "mrqa_searchqa-validation-13612", "mrqa_searchqa-validation-874", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-5582", "mrqa_triviaqa-validation-6660", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-397", "mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-2107", "mrqa_triviaqa-validation-678", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-4616", "mrqa_hotpotqa-validation-891", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-1279", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-867", "mrqa_hotpotqa-validation-1196"], "SR": 0.46875, "CSR": 0.5332386363636363, "EFR": 1.0, "Overall": 0.7272727272727273}, {"timecode": 55, "before_eval_results": {"predictions": ["Caleb", "Ohio", "the person compelled to pay for reformist programs", "c. 3000 BC", "Donna Mills", "for the red - bed country of its watershed", "SIP ( Session Initiation Protocol )", "Thirty years after the Galactic Civil War", "a graded basis, consisting of pass grades 1 ( threshold 85 %, a distinction ), 2 ( 70 -- 84 % ) & 4 ( 40 -- 54 % )", "Carol Ann Susi", "DeWayne Warren", "a qui tam provision that allows people who are not affiliated with the government, called `` relators ''", "Yugoslavia", "nigel", "Emily Davison", "islands", "a nerve cell cluster", "Tripoli", "in the fortified grounds of an old mission known as the Alamo", "Oklahoma", "Schengen Area", "peregrines", "Wadsworth", "Stars on 45 Medley", "nigel", "a retired three-star rank admiral in the Pakistan Navy, former diplomat, and a defence analyst.", "Daniel Andre Sturridge", "Texas Longhorns", "Omega SA", "\"The Gang\"", "Delilah Rene", "Italian", "WB", "Reese Witherspoon", "British Labour Party", "Christopher Tin", "a Dutch", "President Barack Obama,", "a bipartisan rhetoric", "a woman", "a weight-loss show", "Briton", "$250,000 for Rivers' charity: God's Love We Deliver.", "Nook", "Agent Mark Steinberg", "800,000", "the oldest daughter of an incestuous relationship between Elisabeth, 43, andElisabeth's", "101", "we will be back,\"", "Brass", "birds", "a boat", "Easter", "the West Point", "Joe DiMaggio", "law", "quasars", "Japan", "1852", "resuscitation", "Jumbo", "Sarek", "berks", "Apollo 11"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5980282738095237}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, false, true, false, true, false, true, true, true, true, false, true, true, false, false, false], "QA-F1": [0.4, 0.0, 1.0, 0.8, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-993", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-6436", "mrqa_triviaqa-validation-2999", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-6852", "mrqa_hotpotqa-validation-1240", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1435", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-679", "mrqa_newsqa-validation-428", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-3759", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-3319", "mrqa_searchqa-validation-14981", "mrqa_searchqa-validation-10216", "mrqa_searchqa-validation-9641", "mrqa_searchqa-validation-16358", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-125"], "SR": 0.484375, "CSR": 0.5323660714285714, "EFR": 0.9696969696969697, "Overall": 0.7210376082251082}, {"timecode": 56, "before_eval_results": {"predictions": ["Michael Phelps", "scurvy", "isosceles", "lewis", "1802", "willow", "perfume", "fertilization", "with a definite, visible tail joint are transferred to the Any Other Variety class.", "willy", "by Brian Wheeler", "Peter Stuyvesant", "Botham", "Ed Sheeran", "Jackie Robinson", "erosion", "Kelly Osbourne", "T'Pau", "October 1, 2014", "more than 80", "Emile Berliner", "CBS All Access", "Andrew Garfield", "the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On ''", "the ulnar nerve", "Kareena Kapoor", "Francisco Antonio Zea", "Salman Iqbal", "1967", "a particular nation", "Lowestoft", "Antonio Salieri", "McKinsey & Company", "British Labour Party", "Camille Saint-Sa\u00ebns", "Spiro Agnew", "In a Better World", "Dr. Maria Siemionow,", "second time since the 1990s", "blind Majid Movahedi,", "weight-loss", "the estate", "one count of attempted murder in the second degree in the October 12 attack in Deerfield Beach, Florida.", "three different videos", "1995", "death squad killings carried out during his rule in the 1990s.", "foreplay, sexual conquests and how he picks up women,", "make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "Paul Schlesselman", "Worcestershire", "Isaac Newton", "titanium", "Robert Browning", "St. Augustine", "(plural staff)", "Sicilian pizza", "Samuel Johnson", "silence", "Dalmatian", "duck", "Grover Cleveland.", "Jasenovac concentration camp", "Kohlberg K Travis Roberts", "John Robert Cocker"], "metric_results": {"EM": 0.5, "QA-F1": 0.59558928135037}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, false, false, true, false, true, true, true, true, false, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.5, 1.0, 0.8387096774193548, 1.0, 0.8, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.22222222222222224, 0.09090909090909093, 0.22222222222222218, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-392", "mrqa_triviaqa-validation-3403", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-7556", "mrqa_triviaqa-validation-405", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-993", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-9741", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-4869", "mrqa_hotpotqa-validation-5490", "mrqa_hotpotqa-validation-5110", "mrqa_hotpotqa-validation-3512", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-1146", "mrqa_searchqa-validation-1319", "mrqa_searchqa-validation-14560", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13085", "mrqa_searchqa-validation-4014", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4926"], "SR": 0.5, "CSR": 0.5317982456140351, "EFR": 1.0, "Overall": 0.726984649122807}, {"timecode": 57, "before_eval_results": {"predictions": ["scored a hat-trick", "a long-range missile", "bipartisan", "10", "The show went on without the self-proclaimed \"King of the South,\"", "stylish clothes", "Himalayan kingdom", "Stratfor", "tells stories of different women coping with breast cancer in five vignettes.", "Ron Howard", "the Obama chief of staff", "it is provocative action,\"", "their culture, religion and national identity.", "November 6, 2017", "piety", "It plays a key role in chain elongation in fatty acid biosynthesis and polyketide biosynthesis", "coffee shop Monk's", "Richard Carpenter", "Richard Masur", "October 2012", "Matthew Gregory Wise", "2003", "Noah Schnapp", "about 13,000 astronomical units ( 0.21 ly )", "May 26, 2017", "1973", "Toy Story 3", "athletics", "Charlie Sheen", "Egypt", "tony elton", "blue", "Prophet Joseph Smith", "sprite", "Ohio", "Bayer", "Madagascar", "British", "Cartoon Network", "Chester", "a large green dinosaur", "Zander", "the 45th Infantry Division", "Knoxville", "Lionsgate", "superhero roles", "A123 Systems", "Groom Lake Valley", "Jenji Kohan", "Oliver Twist", "January 20, 2013", "Big Brown", "a torpedo", "Parkinson's disease", "\"The PCH\"", "potential energy", "Mark Twain", "Kenny Rogers", "ice hockey", "Minnesota", "Appomattox", "Las Vegas", "Dublin", "Moses"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5620535714285714}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, false, true, true, false, true, true, true, false, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-177", "mrqa_newsqa-validation-182", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-481", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-7262", "mrqa_triviaqa-validation-3695", "mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-3137", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-1395", "mrqa_hotpotqa-validation-5649", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-2749", "mrqa_searchqa-validation-13976", "mrqa_searchqa-validation-13713", "mrqa_searchqa-validation-9484", "mrqa_searchqa-validation-495", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-16044", "mrqa_searchqa-validation-13336", "mrqa_searchqa-validation-7078", "mrqa_searchqa-validation-1462"], "SR": 0.421875, "CSR": 0.5299030172413793, "EFR": 0.9459459459459459, "Overall": 0.715794792637465}, {"timecode": 58, "before_eval_results": {"predictions": ["June 2007", "Dialogues of the Carmelites", "Dr. Alberto Taquini", "England", "Mary Harron", "Bank of China Tower", "Fidenza", "Japan Airlines Flight 123", "electric field", "Ben Stokes", "Robert Frost's former home in Franconia, New Hampshire, United States", "2006", "Tianhe Stadium", "Mickey Rourke", "Gayla Peevey", "2016", "husky", "Otis Timson", "Richard Masur", "the sperm and ova", "the church sexton Robert Newman and Captain John Pulling", "commemorating fealty and filial piety", "February 16, 2018", "West African traditions", "pop ballad", "taxonomic", "James Dean", "nirvana", "United States", "NOW Magazine", "Canada", "sugarloaf Mountain", "battle of Hastings", "smith", "basket", "Jews", "USS Maine", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "German Chancellor Angela Merkel", "\"Quiet Nights,\"", "flooding", "15,000", "20,000", "British", "leftist Workers' Party.", "World leaders", "Asashoryu", "$1.45 billion", "Dr. Cade", "Columbia River", "copper", "( Alan) HaleBopp", "Dolly Parton", "Terry Bradshaw", "Scotland", "Tunis", "Kunta Kinte", "Italy", "a cappella", "Daffodils", "the FBI's Criminal Justice Information Services Division", "21-year-old", "$12.9 million", "playing"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5576672676282051}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0625, 0.5, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-820", "mrqa_hotpotqa-validation-733", "mrqa_naturalquestions-validation-8209", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-4925", "mrqa_triviaqa-validation-3907", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-2612", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-3518", "mrqa_triviaqa-validation-7492", "mrqa_newsqa-validation-3364", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-1190", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-1128", "mrqa_searchqa-validation-6023", "mrqa_searchqa-validation-7395", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-6939", "mrqa_searchqa-validation-2045", "mrqa_searchqa-validation-13194", "mrqa_searchqa-validation-190", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1348"], "SR": 0.453125, "CSR": 0.5286016949152542, "EFR": 0.9714285714285714, "Overall": 0.7206310532687652}, {"timecode": 59, "before_eval_results": {"predictions": ["February 9, 1994", "Bank of China Building", "John Snow", "the Battelle Energy Alliance", "John of Gaunt", "Croatian", "1946", "35,402", "voice-work", "Mwabvi river", "\"Rich Girl\"", "StubHub Center", "Nickelodeon Animation Studio", "the utopian novels of H.G. Wells", "President Gerald Ford", "1957", "160km / hour", "March 16, 2018", "Lalo Schifrin", "The Cornett family", "Baaghi ( English : Rebel )", "the Old Testament", "March 11, 2016", "The Third Five - year Plan", "Chandigarh", "6", "speedway", "chairs", "la traviata", "british leachman", "midsomer Murders", "Norman Mailer", "British yachtswoman Dee Caffari", "Harry S. Truman", "on, in, and above the Earth,", "mars", "bamboozled", "Sri Lanka", "romantic e-mails", "parents", "Nigeria, Africa's largest", "Fullerton, California,", "a certain carrier based in Texas.", "In August, a lone 50-year-old man was bit by a grizzly bear,", "back at work,\"", "World-renowned security expert Gavin de Becker", "under the speed limit on his or her way to save the world.", "a review of state government practices completed in 100 days.", "Sunday.", "Dodecanese", "Tartarus", "climbing", "Illinois", "Wings", "birds", "Mindanao", "stigma", "M&M's", "quicksand", "tanks", "\"9 To 5\"", "Eleven", "Elin Nordegren,", "Shanghai"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6816468253968254}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.2222222222222222, 0.5, 1.0, 0.888888888888889, 0.16666666666666669, 1.0, 0.6666666666666666, 0.09523809523809525, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-4974", "mrqa_hotpotqa-validation-3020", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-3319", "mrqa_triviaqa-validation-3282", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-4362", "mrqa_triviaqa-validation-2374", "mrqa_triviaqa-validation-2709", "mrqa_newsqa-validation-2896", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-1178", "mrqa_newsqa-validation-2401", "mrqa_searchqa-validation-209", "mrqa_searchqa-validation-10663", "mrqa_searchqa-validation-5352", "mrqa_searchqa-validation-16642", "mrqa_searchqa-validation-2826"], "SR": 0.578125, "CSR": 0.5294270833333333, "EFR": 1.0, "Overall": 0.7265104166666666}, {"timecode": 60, "UKR": 0.736328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1196", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1563", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1595", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-1740", "mrqa_hotpotqa-validation-1773", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2354", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2475", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-2919", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3197", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3573", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3718", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4233", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4743", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-480", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5070", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5170", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5464", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5702", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5827", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-5848", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-5882", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-664", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-881", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-3008", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-3738", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4559", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6399", "mrqa_naturalquestions-validation-6417", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7705", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9939", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1969", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2421", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3495", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3984", "mrqa_newsqa-validation-40", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-56", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-658", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-99", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-10939", "mrqa_searchqa-validation-11190", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-12694", "mrqa_searchqa-validation-12955", "mrqa_searchqa-validation-13259", "mrqa_searchqa-validation-13311", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-14095", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-14560", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-14834", "mrqa_searchqa-validation-1506", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-15437", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-16321", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-171", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-3007", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-355", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-3785", "mrqa_searchqa-validation-3890", "mrqa_searchqa-validation-4046", "mrqa_searchqa-validation-411", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-498", "mrqa_searchqa-validation-5058", "mrqa_searchqa-validation-5442", "mrqa_searchqa-validation-5470", "mrqa_searchqa-validation-5676", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6465", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-6651", "mrqa_searchqa-validation-6783", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7845", "mrqa_searchqa-validation-7943", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-8528", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-8999", "mrqa_searchqa-validation-9118", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-9272", "mrqa_searchqa-validation-9518", "mrqa_squad-validation-10106", "mrqa_squad-validation-10223", "mrqa_squad-validation-10410", "mrqa_squad-validation-1049", "mrqa_squad-validation-1226", "mrqa_squad-validation-1299", "mrqa_squad-validation-1391", "mrqa_squad-validation-1530", "mrqa_squad-validation-1568", "mrqa_squad-validation-1690", "mrqa_squad-validation-173", "mrqa_squad-validation-1766", "mrqa_squad-validation-1784", "mrqa_squad-validation-1872", "mrqa_squad-validation-2171", "mrqa_squad-validation-2324", "mrqa_squad-validation-2402", "mrqa_squad-validation-2914", "mrqa_squad-validation-2956", "mrqa_squad-validation-3197", "mrqa_squad-validation-324", "mrqa_squad-validation-3410", "mrqa_squad-validation-353", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3799", "mrqa_squad-validation-3938", "mrqa_squad-validation-4003", "mrqa_squad-validation-415", "mrqa_squad-validation-4484", "mrqa_squad-validation-4486", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-4764", "mrqa_squad-validation-4801", "mrqa_squad-validation-4954", "mrqa_squad-validation-5272", "mrqa_squad-validation-5320", "mrqa_squad-validation-5360", "mrqa_squad-validation-551", "mrqa_squad-validation-5941", "mrqa_squad-validation-5951", "mrqa_squad-validation-6119", "mrqa_squad-validation-6480", "mrqa_squad-validation-6505", "mrqa_squad-validation-6639", "mrqa_squad-validation-6750", "mrqa_squad-validation-6795", "mrqa_squad-validation-6810", "mrqa_squad-validation-6812", "mrqa_squad-validation-6896", "mrqa_squad-validation-6978", "mrqa_squad-validation-7090", "mrqa_squad-validation-718", "mrqa_squad-validation-7201", "mrqa_squad-validation-7293", "mrqa_squad-validation-7490", "mrqa_squad-validation-770", "mrqa_squad-validation-7723", "mrqa_squad-validation-7789", "mrqa_squad-validation-7945", "mrqa_squad-validation-8012", "mrqa_squad-validation-8498", "mrqa_squad-validation-8500", "mrqa_squad-validation-8658", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9038", "mrqa_squad-validation-912", "mrqa_squad-validation-916", "mrqa_squad-validation-9240", "mrqa_squad-validation-934", "mrqa_squad-validation-9476", "mrqa_squad-validation-9480", "mrqa_squad-validation-9643", "mrqa_squad-validation-9694", "mrqa_squad-validation-9810", "mrqa_squad-validation-9818", "mrqa_squad-validation-9859", "mrqa_squad-validation-9900", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1104", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-1694", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-190", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-2540", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2999", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3175", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3279", "mrqa_triviaqa-validation-3573", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-392", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5615", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-596", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6085", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-6814", "mrqa_triviaqa-validation-682", "mrqa_triviaqa-validation-6956", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7514", "mrqa_triviaqa-validation-7537", "mrqa_triviaqa-validation-7549", "mrqa_triviaqa-validation-7665", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-943"], "OKR": 0.8046875, "KG": 0.4796875, "before_eval_results": {"predictions": ["the British Catuvellauni tribe", "Timo Hildebrand", "Samuel Beckett's \"Eleuth\u00e9ria\"", "constant support", "Marvel's The Defenders", "Philadelphia", "vegetarian", "11 June 1959", "Native American tribes", "Marktown", "45%", "Volvo Cars", "1942", "bicameral Congress", "Horace Lawson Hunley", "Human fertilization", "3D modeling", "President Richard Nixon", "Germany", "- ase", "Andreas Vesalius", "Mary Elizabeth Patterson", "the fictional town of West Egg on prosperous Long Island in the summer of 1922", "Clarence Darrow", "Minneapolis", "5", "arthur", "Sherlock Holmes", "Djibouti and Yemen", "Pallenberg", "hay fever", "bacallini", "Lincoln Logs", "the French", "Styal", "Sicily", "Gorky", "Brian Mabry", "United Front for Democracy Against Dictatorship", "Somali forces and Islamic insurgents.", "eight", "Nairobi, Kenya,", "United States", "Both", "Stratfor,", "183", "Sub-Saharan Africa", "jazz", "B-movie queen Lana Clarkson", "toothpaste", "Penn State Nittany Lions", "Bacon", "the Shrew", "shopping center", "Washi Emporium", "Universal Studios Hollywood", "visible", "Fairfax", "the Clark bar", "diameter", "mammoth", "Ma Khin Khin Leh,", "protect ocean ecology, address climate change and promote sustainable ocean economies.", "Woods"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5869656385281385}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, false, true], "QA-F1": [0.0, 0.0, 0.4, 0.5714285714285715, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8181818181818181, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-126", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3617", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-525", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-2799", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-5674", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-5273", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-4829", "mrqa_triviaqa-validation-2879", "mrqa_newsqa-validation-2672", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-2200", "mrqa_searchqa-validation-2213", "mrqa_searchqa-validation-924", "mrqa_searchqa-validation-1024", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10411", "mrqa_searchqa-validation-10482", "mrqa_searchqa-validation-2378", "mrqa_searchqa-validation-4850", "mrqa_newsqa-validation-4168"], "SR": 0.484375, "CSR": 0.5286885245901639, "EFR": 1.0, "Overall": 0.7098783299180328}, {"timecode": 61, "before_eval_results": {"predictions": ["\"King of Cool\"", "Rhode Island", "Lake Placid, New York", "the Dominican Republic", "Roslin", "Bolshoi", "Franconia, New Hampshire", "Tie Domi", "Charice", "Ian Fleming", "17 December 1998", "30", "Bill McCutcheon", "Nodar Kumaritashvili ( Georgian)", "Aaron Lewis", "the United States", "Havana Harbor", "`` Deadman's Gun ''", "Ticket to Ride", "British and French Canadian fur traders", "Georgia", "1997", "Moscow, Russia", "Mary Elizabeth Patterson", "Hank J. Deutschendorf II", "Catherine of Aragon", "housemaid", "Ruth Ellis", "Batman & Robin", "CAPTCHA", "Manifest Destiny", "bacalla", "Gary Oldman", "Washington, D.C.", "patallica", "Rats", "arthur", "Hu Jintao", "ties", "the vicious brutality which accompanied the murders of his father and brother.\"", "saying Chaudhary's death was warning to management.", "Tsvangirai", "Manmohan Singh's", "southern port city of Karachi,", "is the U.N. nuclear watchdog agency's strongest warning yet that Iran could be aiming to build a nuclear bomb.", "Ashley \"A.J.\" Jewell,", "Seoul,", "Sri Lanka", "10 percent", "Reuben", "John Deere", "Sarah", "a mathematical phrase", "Victor Hugo", "a pram", "Get Back", "Herman Wouk", "the Lottery", "Linkin Park", "Jupiter", "a Zigall", "1765", "Tigris and Euphrates rivers", "Agra Cantonment - H. Nizamuddin Gatimaan Express"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6426846590909091}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, false, false, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1818181818181818]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-941", "mrqa_hotpotqa-validation-3151", "mrqa_naturalquestions-validation-10078", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-2870", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-7365", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-5968", "mrqa_triviaqa-validation-2427", "mrqa_triviaqa-validation-6378", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6870", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3944", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-2375", "mrqa_searchqa-validation-6699", "mrqa_searchqa-validation-6028", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-14344", "mrqa_naturalquestions-validation-3416"], "SR": 0.5625, "CSR": 0.529233870967742, "EFR": 0.9285714285714286, "Overall": 0.6957016849078341}, {"timecode": 62, "before_eval_results": {"predictions": ["Ukraine", "Ethel Merman", "Poems : Series 1", "Julie Karen Kavner", "Iraq", "1937", "French Canadian", "1997", "2014 Winter Olympics in Sochi, Russia", "Guantanamo Bay Naval Base", "Thomas Mundy Peterson", "Walter Egan", "Mona Vanderwaal", "Sicily", "Todd Rigney", "12", "Ford Motor Company", "Columbia", "someone named Godot", "Sony Interactive Entertainment", "Hanseatic League", "Gibraltar", "mercury", "tasyana", "kenny Everett", "1937", "Tommy Cannon", "Charles Reed Bishop", "D\u00e2mbovi\u021ba River", "400", "supernatural psychological horror", "Martin Truex Jr.", "848", "blue Ridge Parkway", "Blender", "My Boss, My Hero", "Grave Digger", "an \"unnamed international terror group\"", "voluntary misdemeanor", "AS", "oys And Girls alone", "the former Massachusetts governor", "CNN", "Iowa's critical presidential caucuses", "Yemeni port city of Aden", "137", "missile", "Democrats and Republicans are saying Meehan shouldn't be using a 9/11 image to make a political point.", "Government Accountability Office report", "John Glenn", "Bering Strait", "Nora", "(Ben) Kingsley", "fructose", "Knocked Up", "upright", "the Rhine", "cabbage", "posterior", "grain", "xenon", "Kentucky, Virginia, and Tennessee", "1892 Auburn Tigers football team", "Nana Patekar"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6387946428571428}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.24000000000000002, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-7409", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-2900", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7633", "mrqa_triviaqa-validation-7555", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-6243", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-5174", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-3413", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-1662", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-3856", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-1923", "mrqa_searchqa-validation-11966", "mrqa_searchqa-validation-2682", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-9465", "mrqa_hotpotqa-validation-1317"], "SR": 0.546875, "CSR": 0.5295138888888888, "EFR": 0.9310344827586207, "Overall": 0.6962502993295019}, {"timecode": 63, "before_eval_results": {"predictions": ["Saint Michael, Barbados", "Danish", "rickyard", "Richard Arthur", "Blue Grass Airport", "France", "1872", "The Livingston family", "General Sir John Monash", "Margarine Unie", "July 8, 2014", "Dragons: Riders of Berk", "Trent Alexander-Arnold", "his friends, Humpty Dumpty and Kitty Softpaws", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "erosion", "assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "depending on the gender of the reigning monarch", "1923", "The weekly Torah portion", "Oklahoma native Major General Clarence L. Tinker", "Daren Maxwell Kagasoff", "the courts", "16", "Beorn", "shark", "pints", "9", "April", "German", "adventure", "Anne Boleyn", "pony troughton", "isosceles triangle", "petticoat", "Vienna", "umbrella", "The Drug Enforcement Administration said Wednesday it's considering tighter restrictions on propofol,", "heart,", "NASCAR.", "Defense of Marriage Act", "the immorality of these deviant young men", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "The Tinkler.", "Ninety-two", "normal maritime traffic", "sculptures", "as many as 250,000 unprotected civilians", "We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "the Five Orders of Architecture", "wheat", "(Shannon) Lucid", "Paganini", "The Untouchables", "Hinduism", "Pennsylvania", "The Don Quixote", "opera buffa", "Ezra Pound", "ponies", "Homicide: Life on the Street", "Baku", "about Eve", "carbohydrates"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6787586731145914}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.782608695652174, 1.0, 1.0, 0.11764705882352941, 0.7058823529411764, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.23076923076923078, 1.0, 0.6666666666666666, 0.8, 1.0, 0.2857142857142857, 0.0625, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1676", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-2577", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-3546", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-1640", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-4448", "mrqa_triviaqa-validation-6810", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1273", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-370", "mrqa_newsqa-validation-3865", "mrqa_searchqa-validation-12690", "mrqa_searchqa-validation-963", "mrqa_searchqa-validation-14562", "mrqa_searchqa-validation-8778", "mrqa_searchqa-validation-9664", "mrqa_triviaqa-validation-6234"], "SR": 0.5625, "CSR": 0.530029296875, "EFR": 1.0, "Overall": 0.7101464843749999}, {"timecode": 64, "before_eval_results": {"predictions": ["sony", "Anne of Cleves", "germany", "tosca", "tosca", "backgammon", "Denver", "judy holliday", "an apple", "to crush it into one solid lump, then melt it down to a liquid and have the metal to sell.", "Brazil", "raw hides", "bannockburn", "Cheryl Campbell", "a crust of mashed potato", "Thomas Jefferson", "Donald Gets Drafted ( May 1, 1942 ) ( shown in his Selective Service Draft Card close up, we learn Donald's full name : Donald Fauntleroy Duck )", "Wednesday, September 21, 2016, on NBC", "a mashed potato crust", "Matthew Gregory Wise", "Magnavox Odyssey", "Dark", "Taron Egerton", "Mankombu Sambasivan Swaminathan", "Western Australia", "Northwestern Hawaiian Islands", "Kathleen O'Brien", "John John Florence", "Jean-Marie Pfaff", "John Lennon/Plastic Ono Band", "Konstant\u012bns Raudive", "1694", "Ben R. Guttery", "Singapore", "1999 Galt\u00fcr avalanche", "a community southwest of the Pensacola city limits", "Christopher Lloyd Smalling", "$10 billion", "nearly 28 years", "heavy brush,", "Defense of Marriage", "Eight American", "up three", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "45 minutes, five days a week.", "a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the difference,\"", "The minister later apologized,", "Caylee Anthony's", "10 to 15 percent", "a zero - Search-ID.com", "a claymore", "elk", "Solidarity", "Christopher Columbus", "Shalom", "a mammal", "a machinist", "Jean-Paul Sartre", "a diamond", "Peter Shaffer", "a magnetic field", "Palatine Hill", "a mad cow", "a bassoon"], "metric_results": {"EM": 0.4375, "QA-F1": 0.608367673992674}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true], "QA-F1": [0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.23076923076923078, 0.6666666666666666, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.4, 1.0, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-3089", "mrqa_triviaqa-validation-3900", "mrqa_triviaqa-validation-394", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-7504", "mrqa_triviaqa-validation-6752", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-6100", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-10617", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-8298", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4673", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-4883", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-3561", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-780", "mrqa_searchqa-validation-122", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-16034", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-16119", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-13757"], "SR": 0.4375, "CSR": 0.5286057692307693, "EFR": 0.9722222222222222, "Overall": 0.7043062232905982}, {"timecode": 65, "before_eval_results": {"predictions": ["282,846", "DeWayne Warren", "Warren Hastings", "Colman", "Dougie MacLean", "Saint Alphonsa", "LII,", "Carroll O'Connor", "Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "A simple majority", "Jenny Slate", "western Cuba", "It was held in England and Wales from 1 June to 18 June 2017", "cotswold", "evonne goolagong cawley", "ferans", "ferator", "elytra", "sonja Henie", "cyclops", "John Constable", "le Parisien", "Baroness Karren Brady", "farce", "ryan hanks", "filibuster and scathing rhetoric", "Stephen Ireland", "The United States presidential election of 2016", "ten", "\"Authors anonymous\"", "d\u00eds", "The Washington Post", "Sippin' on Some Syrup", "playback singer, director, writer and producer", "27 January 1974", "February 14, 1859", "Patton Oswalt", "Michael Krane,", "Jaime Andrade", "Larry Ellison,", "teenage", "severe", "breast cancer", "10,000 refugees,", "Marines", "drug cartels", "has not fully implemented appropriate security practices to protect the control systems used to operate its critical infrastructures,\"", "16th grand Slam title.", "Katherine", "Henry David Thoreau", "action thriller", "Afghanistan", "Bizarro", "Catherine", "the bass viol", "Jawaharlal Nehru", "ice ages", "crunching", "man", "crone", "frequency", "Toni Morrison", "Type A", "Vampire"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5400297619047618}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.9166666666666666, 1.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.14285714285714288, 1.0, 0.08333333333333333, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-462", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-8944", "mrqa_triviaqa-validation-6689", "mrqa_triviaqa-validation-4955", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-1666", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-5170", "mrqa_hotpotqa-validation-5564", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-367", "mrqa_hotpotqa-validation-2525", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-1154", "mrqa_searchqa-validation-2935", "mrqa_searchqa-validation-11036", "mrqa_searchqa-validation-15729", "mrqa_searchqa-validation-14525", "mrqa_searchqa-validation-10278", "mrqa_searchqa-validation-1890", "mrqa_searchqa-validation-3848"], "SR": 0.453125, "CSR": 0.5274621212121212, "EFR": 1.0, "Overall": 0.7096330492424242}, {"timecode": 66, "before_eval_results": {"predictions": ["around 2011", "1956", "in a counter clockwise direction around the Sun", "2018", "Olivia Olson", "1987", "a humid subtropical climate, with hot summers and mild winters", "agriculture", "13 February", "eleven", "Robyn", "Jesus'birth", "Kyla Pratt", "Nile", "orangutans", "Emeril Lagasse", "one", "solitaire", "Bono", "George Herbert Walker Bush", "cuckoo", "land between two rivers", "Jasper Fforde", "Paris", "margaret", "Marine Corps", "Rhode Island School of Design", "Ector County", "Major League Soccer", "Gatwick Airport", "Sam Raimi", "J. K. Rowling", "The President's Volunteer Service Award", "Colin Vaines", "11 November 1821", "The Crowned Prince of the Philadelphia Mob/Mafia", "the Championnat National 3", "a hospital in Amstetten,", "Tuesday afternoon.", "Mohammed Mohsen Zayed,", "Marie-Therese Walter.", "Michael Jackson", "the reality he has seen is \"terrifying.\"", "a facility in Salt Lake City, Utah,", "his past and his future", "27-year-old's", "38,", "Ameneh Bahrami", "Spc. Megan Lynn Touma, 23,", "the ecliptic", "33.4%", "Jacob", "the urban forest", "Margaret Harlow", "the Hesperides", "Carthage", "Otto von Bismarck", "the Amazon", "Donna Summer", "Ralph Lauren", "Colorado", "is not our daughters anymore,", "Opry Mills,", "flooding from Hurricane Irene that pummeled the East Coast last August and for damages from Tropical Storm Lee in Schoharie, Tioga, Broome, Greene, and Orange counties."], "metric_results": {"EM": 0.515625, "QA-F1": 0.647670732045732}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 0.8000000000000002, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.1818181818181818, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-5787", "mrqa_naturalquestions-validation-7227", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-1805", "mrqa_triviaqa-validation-5010", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-2289", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-242", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-906", "mrqa_newsqa-validation-2517", "mrqa_searchqa-validation-15537", "mrqa_searchqa-validation-14101", "mrqa_searchqa-validation-11663", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-12874", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-536", "mrqa_searchqa-validation-8381", "mrqa_searchqa-validation-4370", "mrqa_searchqa-validation-9653", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3369"], "SR": 0.515625, "CSR": 0.527285447761194, "EFR": 1.0, "Overall": 0.7095977145522387}, {"timecode": 67, "before_eval_results": {"predictions": ["coercivity", "branch roots", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad", "Redenbacher family, namesake of the famous popcorn brand", "James Corden", "often linked to high - ranking ( though not necessarily royalty ) in China", "four", "Hellenismos", "1878", "Randy Newman", "2007", "Panthalassa", "In the early 1800s, John Dalton used the concept of atoms to explain why elements always react in ratios of small whole numbers ( the law of multiple proportions )", "Peter Butterworth", "orangish tinge", "robin", "Germany", "south American country of Brazil", "Tomorrow Never Dies", "Caracas", "magnetism", "Madagascar", "zulu", "south Seas territory of Kyrgystan", "cats", "motor ships", "Arlo Looking Cloud", "Francis Egerton", "Max Kellerman", "Rothschild", "10 June 1921", "Scotiabank Saddledome", "\"Slaughterhouse-Five\"", "Indian", "Socrates", "1942", "National Lottery", "two remaining crew members", "a deceased organ donor,", "The Wall Street Journal Europe", "Alwin Landry", "2,800", "Sgt. Barbara Jones", "Salafist", "\"Body Works\"", "Afghanistan,", "the body of the aircraft", "hank Moody", "between 1917 and 1924", "diamond", "the house of detention", "Turkey", "Sweet Home Alabama", "the black market", "the National Hockey League", "a Virgin", "Y", "Mariska Hargitay", "Germany", "a big stick", "Moses", "Kafka", "compost", "Dante"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5775752314814815}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, false], "QA-F1": [0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.14814814814814814, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6811", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1492", "mrqa_triviaqa-validation-4601", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-1636", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-4237", "mrqa_triviaqa-validation-6541", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-830", "mrqa_hotpotqa-validation-5604", "mrqa_newsqa-validation-3587", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2732", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-1086", "mrqa_searchqa-validation-13647", "mrqa_searchqa-validation-3197", "mrqa_searchqa-validation-1520", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-11335", "mrqa_searchqa-validation-16220"], "SR": 0.515625, "CSR": 0.5271139705882353, "EFR": 1.0, "Overall": 0.7095634191176471}, {"timecode": 68, "before_eval_results": {"predictions": ["Fran\u00e7ois Hollande", "Broadway musical South Pacific", "sony", "UPS", "( Kevin) Painter", "east", "sixty-one", "british", "(Adolphe) Adam", "cubed", "lamb", "november", "Sandi Toksvig", "in the vascular bundles", "Coton in the Elms", "the government - owned Panama Canal Authority", "Sir Donald Bradman", "Killer Within", "Ella Eyre", "Profit maximization", "Nalini Negi", "The Hunger Games : Mockingjay -- Part 2 ( 2015 )", "Vasoepididymostomy", "John Goodman", "Eddie Murphy", "Morocco", "John Starks", "Kurt Vonnegut", "an Albanian political party", "Comodoro Arturo Merino Ben\u00edtez International Airport", "Mike Mills", "May 20, 1862", "250 million", "\" Terry the Tomboy\"", "Umina Beach", "Taeko Ikeda", "Chevy", "barter", "General Motors'", "The Kirchners", "undergoing a double mastectomy and reconstructive surgery,", "Robert Mugabe", "Val d'Isere, France", "free laundry service.", "\"theoretically\"", "33-year-old", "took her own life.", "federal ocean planning.", "a one-shot victory in the Bob Hope Classic", "(Daisy) Buchanan", "Katharine McPhee", "the Eiffel Tower", "a pepperoni", "(Casey) Stengel", "Hodgkin\\'s disease", "( Marcia) Clark", "cement", "(John) Madden", "foolish", "hubris", "(Bolivia)", "Tuesday", "1967", "\"Canadien(ne)s fran\u00e7ais(es)\""], "metric_results": {"EM": 0.53125, "QA-F1": 0.6089486034798535}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.3846153846153846, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4159", "mrqa_triviaqa-validation-2488", "mrqa_triviaqa-validation-2237", "mrqa_triviaqa-validation-1044", "mrqa_triviaqa-validation-1332", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-986", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-9753", "mrqa_hotpotqa-validation-3156", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-4650", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1593", "mrqa_newsqa-validation-719", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-1731", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-2858", "mrqa_searchqa-validation-10150", "mrqa_searchqa-validation-14255", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-15325", "mrqa_hotpotqa-validation-179"], "SR": 0.53125, "CSR": 0.5271739130434783, "EFR": 1.0, "Overall": 0.7095754076086956}, {"timecode": 69, "before_eval_results": {"predictions": ["the mind eraser", "a novel", "Honolulu", "(Thomas) Paine", "the heron", "Universal Studios", "coal", "a (spinning) top", "spelunking", "walnuts", "Finding Nemo", "a catalog", "a mariachi", "1837", "2010", "New South Wales", "The play is a farcical black comedy revolving around the Brewster family, descended from the Mayflower, but now composed of insane homicidalicidals", "Achal Kumar Jyoti", "dorsally on the forearm", "Darren McGavin", "orbit", "the Speaker of the House of Representatives", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "energy loss", "October 29, 2015", "Ellesmere Port", "sugar baby love", "step by step", "Wisconsin", "buddhists", "Richard Strauss", "islands in the south east", "8", "January 1971", "French", "buddhist", "king Edward III", "Four Weddings and a Funeral", "Los Angeles", "Karl Johan Schuster", "elderships", "Rose Byrne", "The Division of Cook", "eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music", "1894", "Walt Disney Productions", "KlingStubbins", "Bambi, a Life in the Woods", "Australian", "Arsene Wenger", "two women", "two years,", "the child might still be alive,", "cars", "a series of wildfires", "Jeddah, Saudi Arabia,", "$50", "act,", "protective shoes", "British author J.G. Ballard,", "the 50-year-old King of Pop has agreed to a series of summer concerts at the O2.", "Just Dropped In ( To See What condition Mycondition Was In )", "April 29, 2009", "Terry Reid"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6699130639097745}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.47619047619047616, 1.0, 1.0, 0.5, 0.2, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.631578947368421, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3934", "mrqa_searchqa-validation-4535", "mrqa_searchqa-validation-11195", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-3440", "mrqa_triviaqa-validation-6905", "mrqa_triviaqa-validation-5706", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-913", "mrqa_triviaqa-validation-4594", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-4257", "mrqa_hotpotqa-validation-1506", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-444", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-522"], "SR": 0.609375, "CSR": 0.5283482142857143, "EFR": 1.0, "Overall": 0.7098102678571429}, {"timecode": 70, "UKR": 0.71875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1562", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1659", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-3276", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3974", "mrqa_hotpotqa-validation-3990", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5201", "mrqa_hotpotqa-validation-5222", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5253", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5405", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-733", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4145", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-44", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7268", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7846", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1545", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2091", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2550", "mrqa_newsqa-validation-2556", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-27", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3265", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-10138", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-10179", "mrqa_searchqa-validation-10216", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-10357", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11663", "mrqa_searchqa-validation-11668", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-11892", "mrqa_searchqa-validation-12136", "mrqa_searchqa-validation-12153", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12351", "mrqa_searchqa-validation-12473", "mrqa_searchqa-validation-12539", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12833", "mrqa_searchqa-validation-12874", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-1462", "mrqa_searchqa-validation-14914", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15232", "mrqa_searchqa-validation-15294", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16161", "mrqa_searchqa-validation-1623", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-16958", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3937", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-4352", "mrqa_searchqa-validation-4537", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-539", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-553", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7225", "mrqa_searchqa-validation-7236", "mrqa_searchqa-validation-7369", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-7672", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-7978", "mrqa_searchqa-validation-8884", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-9653", "mrqa_searchqa-validation-9659", "mrqa_squad-validation-10076", "mrqa_squad-validation-10102", "mrqa_squad-validation-10295", "mrqa_squad-validation-10307", "mrqa_squad-validation-10309", "mrqa_squad-validation-1176", "mrqa_squad-validation-1657", "mrqa_squad-validation-1794", "mrqa_squad-validation-1908", "mrqa_squad-validation-1980", "mrqa_squad-validation-2114", "mrqa_squad-validation-2133", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2675", "mrqa_squad-validation-2819", "mrqa_squad-validation-2849", "mrqa_squad-validation-3147", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3355", "mrqa_squad-validation-3590", "mrqa_squad-validation-3628", "mrqa_squad-validation-4127", "mrqa_squad-validation-4192", "mrqa_squad-validation-4429", "mrqa_squad-validation-4623", "mrqa_squad-validation-4698", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4834", "mrqa_squad-validation-4840", "mrqa_squad-validation-5197", "mrqa_squad-validation-5410", "mrqa_squad-validation-551", "mrqa_squad-validation-5592", "mrqa_squad-validation-5721", "mrqa_squad-validation-5951", "mrqa_squad-validation-6106", "mrqa_squad-validation-6166", "mrqa_squad-validation-6409", "mrqa_squad-validation-6471", "mrqa_squad-validation-6564", "mrqa_squad-validation-6681", "mrqa_squad-validation-6692", "mrqa_squad-validation-6812", "mrqa_squad-validation-6916", "mrqa_squad-validation-6988", "mrqa_squad-validation-7270", "mrqa_squad-validation-7300", "mrqa_squad-validation-7480", "mrqa_squad-validation-7565", "mrqa_squad-validation-7707", "mrqa_squad-validation-7751", "mrqa_squad-validation-7813", "mrqa_squad-validation-7917", "mrqa_squad-validation-794", "mrqa_squad-validation-8042", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8575", "mrqa_squad-validation-8592", "mrqa_squad-validation-8679", "mrqa_squad-validation-8767", "mrqa_squad-validation-8917", "mrqa_squad-validation-9103", "mrqa_squad-validation-9575", "mrqa_squad-validation-9643", "mrqa_squad-validation-9697", "mrqa_squad-validation-9732", "mrqa_squad-validation-9787", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-1615", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-1820", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2986", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-333", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-3509", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3798", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4448", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5114", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-5330", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5977", "mrqa_triviaqa-validation-6053", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-613", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7102", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-732", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-898"], "OKR": 0.775390625, "KG": 0.4515625, "before_eval_results": {"predictions": ["Genesis", "Quebec", "acceleration", "Roger Federer", "Roald Dahl", "Legally Blonde", "Voyager 1", "Peach", "the Naval Academy", "the Renaissance", "Blake Lively", "a prism schism", "the Society of Jesus ( Jesuits)", "a premalignant flat ( or sessile ) lesion of the colon", "John Joseph Patrick Ryan", "Jason Lee", "February 14, 2015", "homicidal thoughts of a troubled youth", "Hodel", "1560s", "supervillains who pose catastrophic challenges to the world", "the Alamodome and city of San Antonio", "Bobby Beathard", "The Gupta Empire", "caused by chlorine and bromine from manmade organohalogens", "wis warfield Simpson", "ovid", "upside down", "restless leg syndrome", "Hindenburg", "Sicily", "dinar", "Henley Royal Regatta", "6", "hawthole", "hand gun", "pickwick", "Sun Belt Conference", "47", "1 December 1948", "Manchester", "Ronnie Schell", "composer of both secular and sacred music", "Lochaber, Highland, Scotland", "Motorised quadric recycling", "the field of science", "PlayStation 4", "A compact car (North America)", "Elton Hercules John", "and former EPA administrator Carol Browner", "U.S. senators who couldn't resist taking the vehicles for a spin.", "Nirvana", "one", "22", "August 19, 2007.", "Russia", "southwestern", "Nigeria,", "an animal tranquilizer,", "Barnes & Noble", "American", "a canyon in the path of the blaze", "75", "Angola"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6919146825396825}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.22222222222222224, 0.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.8, 0.5, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8708", "mrqa_searchqa-validation-9530", "mrqa_searchqa-validation-14312", "mrqa_searchqa-validation-10021", "mrqa_searchqa-validation-8777", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-654", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-2342", "mrqa_hotpotqa-validation-1837", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-2558", "mrqa_hotpotqa-validation-4322", "mrqa_hotpotqa-validation-4573", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4121", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-3610"], "SR": 0.578125, "CSR": 0.5290492957746479, "EFR": 0.9629629629629629, "Overall": 0.6875430767475221}, {"timecode": 71, "before_eval_results": {"predictions": ["28,", "him to step down as majority leader.", "would effectively eliminate union elections, removing a fundamental hard-earned right of workers and forcing some workers into unions they don't want to join.", "in the Philippines", "off the coast of Dubai", "45 minutes, five days a week.", "Londoners", "This will be the second", "rebels", "Asashoryu", "Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,", "a national telephone survey", "Australian Open,", "Charles Dickens's novel Oliver Twist", "Longline fishing", "eight", "in 2009", "The eighth", "September of that year", "5 -- Etienne de Mestre", "the American League ( AL ) champion Cleveland Indians", "North Atlantic Ocean", "September 1959", "a young girl", "May 2016", "stained glass", "axe", "whist", "Caribbean", "eros", "2240", "kong", "Patrick McGoohan", "cobbler", "New Zealand", "Arctic Monkeys", "peter sellers", "Montagues and Capulets", "its variety of shops ranging from upscale boutiques to national and international chain store outlets", "Argentine", "United States Auto Club", "Australian", "44", "LA Galaxy", "World Championship Wrestling", "Apatosaurus", "white officers", "supply chain management", "June 1975", "Florence Nightingale", "deuterium", "Oregon", "asteroids", "artesian", "The Silence of the Lambs", "Danish Yoga instructor", "Dizzy", "Louise", "Virginia", "the battle of Marathon", "Cairo", "H CO ( equivalently OC (OH )", "Effy", "The won"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6117108585858586}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.2222222222222222, 0.8, 0.0, 0.7272727272727272, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-2809", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-6987", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-4087", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-1732", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-3412", "mrqa_hotpotqa-validation-1001", "mrqa_searchqa-validation-11421", "mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-16380", "mrqa_naturalquestions-validation-9675"], "SR": 0.546875, "CSR": 0.529296875, "EFR": 0.9655172413793104, "Overall": 0.6881034482758621}, {"timecode": 72, "before_eval_results": {"predictions": ["Max Martin and Shellback", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "1976", "head of the Cabinet of Bluhme II", "the capital of the province", "Royal Albert Hall", "Kansas", "Marshal of France", "The village lies just west of the A350 road about 9 mi south of Warminster and 5 mi north of Shaftesbury, Dorset", "Hungarian", "Arizona", "Captain", "Adam Karpel", "Acts 15, 7 -- 11 and Acts 15 : 14", "Karen Gillan", "775 rooms", "William Whewell", "Cheap Trick", "Experimental neuropsychology", "card verification value", "Katharine Hepburn -- Ethel Thayer", "2 %", "red", "Watson and Crick", "historical fiction", "viii lafferty", "spiral", "apples", "baulk line", "zanzibar", "bohemian city", "Monica Seles", "henna", "Bugsy Malone", "(Rosario Dawson)", "Francis Noble", "1919", "Rambosk", "misdemeanor assault charges", "Brazil", "can play an important role in Afghanistan as a reliable NATO ally.", "\"He is obviously very relieved and grateful that the pardon was granted,\"", "one", "12-1", "Juan Martin Del Potro.", "AS", "Casalesi Camorra clan", "1969", "British capital's other two airports,", "Blackberry", "an imaginary menagerie", "(Dan Morrison)", "John Lennon", "(Peter) Connolly", "the owl", "the Panama Canal", "the Washburn camp loop", "Tulle", "the saber-tooth cat", "Wall Street", "the Kensington Palace", "2009", "British colonial government", "the placing of repentance ashes on the foreheads of participants to either the words `` Repent, and believe in the Gospel '' or the dictum `` Remember that you are dust, and to dust you shall return"], "metric_results": {"EM": 0.375, "QA-F1": 0.46328125}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, true, true, false, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.16666666666666669, 1.0, 0.2, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.1, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8333333333333333, 0.3, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-3576", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-2902", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-7134", "mrqa_triviaqa-validation-4235", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-301", "mrqa_triviaqa-validation-1939", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-1483", "mrqa_searchqa-validation-3666", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-571", "mrqa_searchqa-validation-11097", "mrqa_searchqa-validation-1927", "mrqa_searchqa-validation-8587", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-259"], "SR": 0.375, "CSR": 0.5271832191780822, "EFR": 0.975, "Overall": 0.6895772688356165}, {"timecode": 73, "before_eval_results": {"predictions": ["Elizabeth I", "C. W. Grafton", "4,972", "Hugh Hefner", "July 8, 2014", "Julia Kathleen McKenzie", "sarod", "James Gay-Rees", "\"Two Is Better Than One\"", "The Battle of Tannenberg", "Jack Kilby", "Scotiabank Saddledome", "Balvenie Castle", "the Russian army", "pigs", "The ulnar nerve", "an outlaw motorcycle club", "from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "Carlos Alan Autry Jr.", "Nick Kroll", "New York University", "An elevator with a counterbalance", "the Ravi River", "James Madison", "at the hour of death or in the presence of the dying", "26 miles", "dennis pop", "Robert De Niro", "de Havilland Moth", "l'ardente flamme", "hokkaido", "The Shard", "darth viii", "jonathan kouri", "daniel", "Jaguar Land Rover", "george", "Sandro Bondi", "India", "The United States", "a level of autonomy that will allow them to protect and preserve their culture, religion and national identity.", "Isabella", "Monday and Tuesday", "15-year-old's", "two", "Lana Clarkson", "customers are lining up for vitamin injections that promise", "Donald Trump.", "Mugabe's opponents", "the Dolphins", "Zombies", "Detroit", "Saint Nicholas", "India", "3", "cable cars", "the People of America", "The Close Encounters of the Third Kind", "the Process Art", "World War I", "the Nominative", "(Ken) Russell", "Sadat", "a passenger-only ferry"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5862847222222223}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3357", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-656", "mrqa_hotpotqa-validation-3902", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-5396", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-1421", "mrqa_triviaqa-validation-4949", "mrqa_triviaqa-validation-3501", "mrqa_triviaqa-validation-5215", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-2245", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-1587", "mrqa_searchqa-validation-14657", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-8984", "mrqa_searchqa-validation-4550", "mrqa_searchqa-validation-3371", "mrqa_searchqa-validation-11000", "mrqa_searchqa-validation-11082", "mrqa_searchqa-validation-7619", "mrqa_searchqa-validation-11444"], "SR": 0.53125, "CSR": 0.5272381756756757, "EFR": 1.0, "Overall": 0.6945882601351351}, {"timecode": 74, "before_eval_results": {"predictions": ["Hampton's hump and Hampton's line", "102,984", "Nickelodeon", "848", "Georgia", "Cushman", "Mark Neveldine and Brian Taylor", "Harold Edward Holt", "Michael Crawford", "Fort Valley, Georgia", "Netflix", "Edward R. Murrow", "Ashridge Park", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "to ordain presbyters / bishops and to exercise general oversight, telling him to `` rebuke with all authority '' ( Titus 2 : 15 )", "Michael Rooker", "Patris et Filii et Spiritus Sancti", "Asuka", "2010", "a set of components that included charting, advanced UI, and data services ( Flex Data Services )", "South Asia", "Rajendra Prasad", "Ariana Clarice Richards", "President Yahya Khan", "Colon Street", "Roman legions", "Arkansas", "photography", "Henry Hunt", "Petain", "robinsons", "Gretzky", "IT Crowd", "Jim Peters", "sewing machines", "smallfaces", "Mike Tyson", "New York City Mayor Michael Bloomberg", "Ross Perot", "depression", "the Employee Free Choice act", "they couldn't accept an offer", "suppress the memories and to live as normal a", "southern port city of Karachi,", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "supermodel and philanthropist", "Bollywood", "16", "Steven Gerrard", "Dugong", "Buffalo", "Peter Sellers", "Herod", "the tuna", "Chuck Berry", "the Polaroid Corporation", "petits fours", "Smallville", "Mike Nichols", "a volcano", "15", "the eye", "Toyota", "Brundisium"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7562635281385282}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5925925925925926, 1.0, 1.0, 0.0, 0.6666666666666666, 0.07407407407407408, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.2857142857142857, 1.0, 0.4, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-4089", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-851", "mrqa_naturalquestions-validation-2161", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-3484", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-4928", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-5100", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1390", "mrqa_searchqa-validation-1139", "mrqa_triviaqa-validation-6581"], "SR": 0.6875, "CSR": 0.5293749999999999, "EFR": 1.0, "Overall": 0.695015625}, {"timecode": 75, "before_eval_results": {"predictions": ["Bologna", "Big Ben", "The Bronx", "14", "chicken livers", "a jump ball", "Prouse", "he", "a prey drive", "natural selection", "The Communist Party", "God", "University of Exeter", "the Boston Red Sox", "C\u03bc and C\u03b4", "\"He brought back Samuel so that they could help him find Purgatory, the afterlife of monsters, and that Samuel has been working for him", "orogenic belt", "butane", "James Hutton", "The Pittsburgh Steelers", "Ming dynasty", "20 March 2011", "Daniel A. Dailey", "The Maginot Line", "the euro", "burt Kwouk", "david bowart", "carthaginian", "Andr\u00e9s Iniesta", "Joy Division", "vegemian", "kingdom of united kingdom", "manfred von Richthofen", "Monster M*A*S*H", "12", "Dublin", "Barack Obama", "Major League Soccer", "$7.3 billion", "Michael Tippett", "Sutton Hoo helmet", "Art Bell", "Valhalla Highlands Historic District", "19th and early 20th centuries", "37", "Central Avenue", "Laura Jeanne Reese Witherspoon", "2015 Baylor Bears football team", "Alistair Grant", "snow, sleet, freezing drizzle or rain.", "Saturday", "Philippines", "renew registration until the manufacturer's fix has been made.", "1975", "97 years of age.", "raping and killing a 14-year-old Iraqi girl.", "Workers'", "two", "glamour", "246", "Eleven people", "carver", "henley", "Otto von Bismarck"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6142231466450216}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.06666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.14285714285714288, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4441", "mrqa_searchqa-validation-11658", "mrqa_searchqa-validation-7898", "mrqa_searchqa-validation-9920", "mrqa_searchqa-validation-10720", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-665", "mrqa_searchqa-validation-10897", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-250", "mrqa_triviaqa-validation-7017", "mrqa_triviaqa-validation-1823", "mrqa_triviaqa-validation-3604", "mrqa_triviaqa-validation-455", "mrqa_triviaqa-validation-2162", "mrqa_triviaqa-validation-935", "mrqa_hotpotqa-validation-4036", "mrqa_hotpotqa-validation-4882", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-2049", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-334", "mrqa_triviaqa-validation-5331", "mrqa_triviaqa-validation-2782"], "SR": 0.53125, "CSR": 0.5293996710526316, "EFR": 0.9333333333333333, "Overall": 0.681687225877193}, {"timecode": 76, "before_eval_results": {"predictions": ["Ben Johnston", "tempo", "New Hampshire Route 124", "Francis Nethersole", "Bill Walton", "The Little Match Girl", "Disney-Pixar", "Wu-Tang Clan", "Debbie Reynolds", "Mark \"Chopper\" Read", "north-east Lithuania", "Aly Raisman", "Harry Booth", "to manage the characteristics of the beer's head", "In Time", "mid - to late 1920s", "Cee - Lo", "Vicente Fox", "Zedekiah", "Renhe Sports Management Ltd", "often linked to high - ranking ( though not necessarily royalty ) in China", "rubidium - 85", "in 1967, Celtic became the first British team to win the competition", "forested parts of the world", "The Mandate of Heaven", "my fair Lady", "cato", "leukocytes", "Lady Gaga", "peacock", "Antarctica", "duke of morny", "cat Pancras", "david smith", "Jim Jones", "pini di Roma", "Cyndi Lauper", "100", "one", "one", "Peshawar", "the job bill's controversial millionaire's surtax,", "Muslims", "4,", "Matthew Fisher", "Laura Ling and Euna Lee", "snow, which continued to fall Wednesday,", "Basel", "\"He knew the owner of the home, a Vietnam veteran who had given him permission to enter the house and take painkillers or other pills whenever he wanted.", "kryptonite", "The Sixth Sense", "Mao Zedong", "the belle epoque structure that houses Colombo Confectionery", "the Pierian Spring", "break", "Andy Warhol", "Rocky", "Independence", "the Vietnam War", "The Thorn Birds", "the frostbite", "\"He hosted a short - lived talk show in WCW called A Flair for the Gold", "helps scientists better understand the spread of pollution around the globe", "the Pir Panjal Range"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6050680222555223}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, false, true, true, false, true, false, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7692307692307692, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.7272727272727273, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.5, 0.5714285714285715, 1.0, 0.6666666666666666, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-489", "mrqa_hotpotqa-validation-1196", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-3940", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-1705", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-7377", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-5068", "mrqa_triviaqa-validation-5589", "mrqa_triviaqa-validation-1368", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-2839", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4264", "mrqa_searchqa-validation-12071", "mrqa_searchqa-validation-14003", "mrqa_naturalquestions-validation-9003", "mrqa_naturalquestions-validation-1848"], "SR": 0.46875, "CSR": 0.528612012987013, "EFR": 0.9705882352941176, "Overall": 0.6889806746562261}, {"timecode": 77, "before_eval_results": {"predictions": ["Morgan", "Mahalia Jackson", "the outer skin", "a Bunsen burner", "King", "Cuba", "watermelon", "Gatsby", "milk", "the Black Sea", "Prime Time", "New Mexico", "Black sheep", "Michael Rosen", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "an adopted daughter of Thanos", "Sir Hugh Beaver", "The Satavahanas", "Francisco Pizarro", "September 27, 2017", "Real Madrid", "in Paradise, Nevada", "the flag of Hungary", "The western end of the line", "Barbara Eve Harris", "the banjo", "Philippines", "inflammation of the throat", "Volcanoes", "south africa", "Moaning Myrtle", "The Seven Year Itch", "the Holy Roman Empire", "forage", "Vichy", "your phone", "Jim Smith", "1935", "Premier League club Manchester City", "Joseph Cheshire Cotten", "Washington metropolitan area", "Christopher Tin", "cruiserweight", "the reigning monarch of the United Kingdom", "Les Temps modernes", "Kaep", "Luke Bryan", "Francisco Rafael Arellano F\u00e9lix", "Dolly Records", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry", "Draquila -- Italy Trembles.", "seven-time Formula One world champion Michael Schumacher", "South Africa's", "a murderer.", "new DNA evidence", "late Thursday", "Bill Stanton", "Natalie Cole", "11,", "the strawberry Family,\"", "public opinion.", "home games at the Cow Palace, before they moved to their present home, the SAP Center at San Jose in 1993", "Audrey II", "a couple broken apart by the Iraq War"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6363681457431458}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, true, true, false, false, true, true, false, false, false, true, false, true, false, true, false, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7878787878787877, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.5714285714285715, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.4444444444444445, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 0.1904761904761905, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1816", "mrqa_searchqa-validation-8530", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-5936", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3802", "mrqa_triviaqa-validation-2316", "mrqa_triviaqa-validation-7475", "mrqa_triviaqa-validation-2665", "mrqa_triviaqa-validation-5438", "mrqa_hotpotqa-validation-1623", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-5701", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-4445", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-2474", "mrqa_newsqa-validation-3833", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-5583"], "SR": 0.53125, "CSR": 0.5286458333333333, "EFR": 0.9, "Overall": 0.6748697916666667}, {"timecode": 78, "before_eval_results": {"predictions": ["Copenhagen", "BraveStarr", "\"The 8th Habit\"", "Picric acid", "Quentin Coldwater", "1940s and 1950s", "Brickyard", "Hong Kong", "Rabies", "M. Night Shyamalan", "You Can Be a Star", "NBA Slam Dunk Contest", "Currer Bell", "the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere", "the libretto", "Chuck Noland", "King Willem - Alexander", "an exultation of spirit", "the senior-most judge of the supreme court", "Cyrus", "Hon July Moyo", "Bonnie Aarons", "the right side of the heart", "1984", "his waterfront home on Chesapeake Bay, south of Annapolis in Maryland", "braille", "United Republic of Tanganyika and Zanzibar", "blue", "h Henry Kelly", "Turin", "taka", "Polish", "Kenya", "silks", "billy tea", "the Colossus of Rhodes", "albania", "Old Trafford", "The station", "President Robert Mugabe's", "Malawi.", "Les Bleus", "Europe,", "Tillakaratne Dilshan scored his sixth Test century", "removal of his diamond-studded braces.", "Kurt Cobain's", "Pakistan's", "\"Body Works\"", "businessman", "the Bad Beginning", "English", "Ghost Recon", "Spanish", "Peter Shaffer", "mirror", "the Daeodon", "The Count of Monte Cristo", "Picasso", "David H. Petraeus", "Dreamgirls", "the Lincoln Memorial", "powerful anesthetic and sedative.", "the release of the four men", "Michael Schumacher"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7807291666666667}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7777777777777778, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-3533", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6851", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-1673", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3362", "mrqa_searchqa-validation-674", "mrqa_searchqa-validation-3510", "mrqa_searchqa-validation-766", "mrqa_searchqa-validation-13704", "mrqa_searchqa-validation-12666"], "SR": 0.734375, "CSR": 0.53125, "EFR": 0.9411764705882353, "Overall": 0.6836259191176471}, {"timecode": 79, "before_eval_results": {"predictions": ["Ted Bundy", "2017", "David Naughton", "June 1975", "September 26, 2010", "The Prodigy", "Oryzomyini", "119", "Her mother Woizero Aselefech Wolde Hanna", "James Weldon Johnson", "Manhattan Project", "\"Green Chair\"", "17", "2013", "1961", "Andy Serkis", "January 1, 1976", "Alex Ryan", "Massachusetts", "neuropsychology", "10 May 1940", "a fictional character from the book Alice's Adventures in Alice by the writer Lewis Carroll", "the Washington metropolitan area", "the 2017 season", "U.S. service members who have died without their remains being identified", "pierre holliday", "eight-pocket", "david mitchell", "pierzelknirps", "Honolulu, Hawaii", "Diego Velazquez", "the Comitium", "10", "magic", "Istanbul", "24", "earldom", "The Rosie Show,\"", "Trevor Rees", "a remote part of northwestern Montana", "15-year-old's", "skeletal dysplasia,", "reached an agreement late Thursday to form a government of national reconciliation.", "tax credits to companies hiring jobless veterans", "Ingrid Betancourt,", "30", "open heart surgery,", "\"We see pictures from online and on TV, but it's just, it's much different when you see it up front,\"", "Chinese", "Tallahassee", "February 2, 1886", "Madonna", "the Prius", "Anja Prson", "Cannoli", "Los Angeles", "William Henry Harrison", "Charlie Sheen", "pierre london", "Jeannette Rankin", "Theodore Roosevelt", "Trinity", "chili", "Harvard"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5938289141414141}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.060606060606060615, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-1148", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-2588", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-2448", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-7641", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-820", "mrqa_triviaqa-validation-5695", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-1545", "mrqa_newsqa-validation-752", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-1433", "mrqa_searchqa-validation-6746", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-9856", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-15319", "mrqa_searchqa-validation-7235", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-510"], "SR": 0.53125, "CSR": 0.53125, "EFR": 0.8666666666666667, "Overall": 0.6687239583333333}, {"timecode": 80, "UKR": 0.70703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1259", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1659", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-3974", "mrqa_hotpotqa-validation-3990", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-4121", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5201", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5253", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5387", "mrqa_hotpotqa-validation-5405", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-5697", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-44", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-7846", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-851", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2091", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2550", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2600", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2995", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-10138", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-10357", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-11082", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11545", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11668", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-11892", "mrqa_searchqa-validation-12136", "mrqa_searchqa-validation-12153", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12351", "mrqa_searchqa-validation-12473", "mrqa_searchqa-validation-12539", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12833", "mrqa_searchqa-validation-12874", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13615", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14577", "mrqa_searchqa-validation-1462", "mrqa_searchqa-validation-14914", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15294", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15886", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16161", "mrqa_searchqa-validation-1623", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-16380", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-16958", "mrqa_searchqa-validation-1814", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3937", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-4352", "mrqa_searchqa-validation-4537", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-5157", "mrqa_searchqa-validation-539", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-553", "mrqa_searchqa-validation-571", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-674", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7037", "mrqa_searchqa-validation-7225", "mrqa_searchqa-validation-7235", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-7645", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-788", "mrqa_searchqa-validation-8884", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-9653", "mrqa_searchqa-validation-9659", "mrqa_squad-validation-10076", "mrqa_squad-validation-10295", "mrqa_squad-validation-10307", "mrqa_squad-validation-10309", "mrqa_squad-validation-1176", "mrqa_squad-validation-1657", "mrqa_squad-validation-1908", "mrqa_squad-validation-1980", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2675", "mrqa_squad-validation-2849", "mrqa_squad-validation-3147", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3355", "mrqa_squad-validation-3590", "mrqa_squad-validation-3628", "mrqa_squad-validation-4127", "mrqa_squad-validation-4192", "mrqa_squad-validation-4429", "mrqa_squad-validation-4623", "mrqa_squad-validation-4698", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4840", "mrqa_squad-validation-5410", "mrqa_squad-validation-5592", "mrqa_squad-validation-5951", "mrqa_squad-validation-6106", "mrqa_squad-validation-6166", "mrqa_squad-validation-6409", "mrqa_squad-validation-6564", "mrqa_squad-validation-6681", "mrqa_squad-validation-6812", "mrqa_squad-validation-6916", "mrqa_squad-validation-7270", "mrqa_squad-validation-7300", "mrqa_squad-validation-7480", "mrqa_squad-validation-7565", "mrqa_squad-validation-7707", "mrqa_squad-validation-7813", "mrqa_squad-validation-7917", "mrqa_squad-validation-794", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8592", "mrqa_squad-validation-8679", "mrqa_squad-validation-8917", "mrqa_squad-validation-9103", "mrqa_squad-validation-9575", "mrqa_squad-validation-9643", "mrqa_squad-validation-9697", "mrqa_squad-validation-9787", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-1820", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-333", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-3509", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3798", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-5330", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5977", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6634", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7102", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-732", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-7588", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-820", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-898"], "OKR": 0.818359375, "KG": 0.47578125, "before_eval_results": {"predictions": ["Super Saiyan 4", "a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "the Khoisan language of the \u01c0Xam people", "Pakistan", "$1.06 trillion", "Mesopotamia, the land in and around the Tigris and Euphrates rivers", "December 1972", "18", "Scar's henchmen", "Jane Addams, Grace Abbott, Edith Abbott and Sophonisba Breckinridge", "Howard Caine", "Buddhist missionaries first reached Han China via the maritime or overland routes of the Silk Road", "Roman Reigns", "Tennessee Whiskey", "Joan Rivers", "fish", "4-digit A-road in GB at 53 miles from Long Sutton to Bury St Edmunds", "the South Saskatchewan River", "placebo", "three", "greez", "Hispaniola", "Sophora microphylla", "balustrade", "tartan", "University of Georgia", "the Qin dynasty", "CBS", "South America", "the Milwaukee Bucks", "Prince Ioann Konstantinovich of Russia", "Selinsgrove", "The Tempest", "Los Angeles", "Pacific Place", "Franconia, New Hampshire", "small family car", "\"We are a nation of Christians and Muslims, Jews and Hindus", "Sri Lanka's", "a motor scooter", "Nechirvan Barzani,", "Derek Mears", "President Obama's", "Hamas", "at Sea World in San Antonio,", "Mubarak,", "Six members of Zoe's Ark", "More than 15,000", "\"Quiet Nights,\"", "Eli Lilly", "Austen", "the zebra", "the Charleston", "parrot", "the Sky", "The Prince and the Pauper", "taxicab", "Toby Keith", "meter", "Oahu", "Thor Heyerdahl", "lesser island", "Calcium carbonate", "nuts"], "metric_results": {"EM": 0.625, "QA-F1": 0.6754297785547786}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.6153846153846153, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-3018", "mrqa_naturalquestions-validation-5808", "mrqa_triviaqa-validation-1408", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-4205", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-3126", "mrqa_hotpotqa-validation-5714", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3620", "mrqa_searchqa-validation-1158", "mrqa_searchqa-validation-16149", "mrqa_searchqa-validation-5623", "mrqa_searchqa-validation-143", "mrqa_triviaqa-validation-598", "mrqa_triviaqa-validation-1488"], "SR": 0.625, "CSR": 0.5324074074074074, "EFR": 0.9583333333333334, "Overall": 0.6983825231481482}, {"timecode": 81, "before_eval_results": {"predictions": ["michael walsh", "apprentice", "egremont", "mercury", "othello", "fungi", "wigan Warriors", "a figure", "Pontiac Silverdome", "hogmanay", "acetone", "Virgil", "The Battle of Austerlitz", "a shot play", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "Bob Gaudio and his future wife Judy Parker", "ideology", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "in contemporary Earth, where the sudden appearance of a worldwide storm causes 98 % of the world's population to disappear, and zombie - like creatures rise to attack the remainder", "script", "1773", "the White Sox", "Meri", "a role in synthesizing vitamin B and vitamin K as well as metabolizing bile acids", "before 1986", "Telugu", "Paris", "the Tampa Bay Lightning", "Melville, NY, USA", "11 Grands Prix", "Argentine cuisine", "Comodoro Arturo Merino Ben\u00edtez International Airport, Santiago, Chile", "Ready Player One", "Fred \"Sonic\" Smith", "City of Ghosts", "a lack of any perceptible change in an adult female (for instance, a change in appearance or scent)", "331 episodes", "two-state solution", "12 hours", "allegations that a dorm parent mistreated students at the school.", "any abuse that occurred in his diocese.", "\"A sense of community has emerged in Section 60,", "Revolutionary Armed Forces of Colombia,", "two people", "southern Bangladesh,", "a violent government crackdown seeped out.\"", "Las Vegas.", "education", "Illinois Reform Commission", "a perimeter", "Gianni Versace", "a Supper", "Wings", "Croatian", "salmon", "a horse", "a public research university", "a monkey", "the monsoon", "Marie Antoinette", "Google", "Montreal", "left - sided heart failure", "the brain and spinal cord"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5580296207922825}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 0.8275862068965518, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.5, 1.0, 0.8571428571428571, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 0.1, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4252", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-3691", "mrqa_triviaqa-validation-4808", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-1630", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-10576", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-9487", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-1343", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-1652", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-4305", "mrqa_hotpotqa-validation-2781", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2268", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-3629", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15107", "mrqa_searchqa-validation-12769", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-8219", "mrqa_searchqa-validation-6702", "mrqa_searchqa-validation-3577", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-7342"], "SR": 0.453125, "CSR": 0.5314405487804879, "EFR": 0.9714285714285714, "Overall": 0.7008081990418118}, {"timecode": 82, "before_eval_results": {"predictions": ["a heart", "Space Cadet", "anvil", "Judy Garland", "Miranda", "Absinthe", "toadstool", "Belize", "Ban Ki-moon", "surface-to-air", "Babe Ruth", "improvisation", "a Sons of Liberty", "1834", "Ra\u00fal Eduardo Esparza", "Hellenism", "Leonardo da Vinci", "five", "16.5 quadrillion BTUs of primary energy to electric power plants in 2013, which made up nearly 92 % of coal's contribution to energy supply", "to have been domesticated by humans, and there is evidence of sheep farming in Iranian statuary dating to that time period", "Panic! at the Disco", "John Smith", "Daman and Diu", "American country music duo The Bellamy Brothers", "4.37 light - years", "my favorite martian", "jordan", "colette", "Monopoly", "toadog", "michael ancram", "a-j", "goose green", "horseradish", "jordan", "tepuis", "Forbes", "21st Century Fox", "at the Wanda Metropolitano", "67,575", "USS Essex", "Towards the Sun", "Japan", "7 February 14786", "1981 World Rowing Championships", "philanthropist", "2 March 1972", "Angel Parrish", "Buck Owens", "Chinese tourists", "at Eintracht Frankfurt", "authorizing killings and kidnappings by paramilitary death squads.", "84-year-old", "to open a restaurant in the inverted glass pyramid of the Louvre.", "not", "Larry Ellison,", "Ameneh Bahrami", "France's famous Louvre", "Carol Browner", "CNN's", "10 years", "bart Mendoza", "1768", "dwarf antelopes"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7361512874380522}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23076923076923078, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-10740", "mrqa_searchqa-validation-133", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-3319", "mrqa_triviaqa-validation-3580", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-5638", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-3700", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-3091", "mrqa_hotpotqa-validation-83", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-2144", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-3264", "mrqa_triviaqa-validation-4939", "mrqa_triviaqa-validation-589"], "SR": 0.65625, "CSR": 0.5329442771084337, "EFR": 0.9090909090909091, "Overall": 0.6886414122398685}, {"timecode": 83, "before_eval_results": {"predictions": ["Woods", "MDC head Morgan Tsvangirai.", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "Lana Clarkson", "Wednesday.", "MOSCOW, Russia", "Thursday night", "Republican presidential candidate", "five other Christians", "more than 1.2 million", "Madeleine K. Albright", "travel four hours to reach a government-run health facility that provides her with free drug treatment.", "Iran, and arrested online activists in an effort to stop the spread of dissenting information and opinions,\"", "$2.187 billion", "to symbolize his guilt in killing the bird", "gastrocnemius", "August 15, 1971", "Tropical cyclones", "never made", "the center of the Northern Hemisphere", "1977", "the President", "on a sound stage in front of a live audience in Burbank, California", "a crown cutting of the fruit, possibly flowering in five to ten months and fruiting in the following six months", "HTTP / 1.1", "a Treaty of Waitangi", "basketball", "lactic acid", "Moldova", "Rosetta Stone", "tonyanmen", "ethiopia", "henry ford", "archery", "byker Grove", "barley", "stained glass", "energy trading company", "English", "2006", "her translation of and commentary on Isaac Newton's book \"Principia\"", "Portsea", "two", "Taeko Ikeda", "Salford, Lancashire", "15,000 people for basketball matches and 15,500 for concerts (with standing public ramp)", "Sam Walton", "25", "Adrian Lyne", "Ted Turner", "GILBERT & SULLIVAN", "tartar", "cheddar", "Pierian Spring", "travel", "Herod", "bone marrow transplant", "Joe Montana", "Jewelry", "duster", "Max Factor", "March 15, 1945", "The Parlement de Bretagne", "2"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5850095396701}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, true, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, false, true, true, false, false, false, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0689655172413793, 0.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.6666666666666666, 0.4, 0.0, 1.0, 0.14285714285714288, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-4083", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-7398", "mrqa_naturalquestions-validation-9275", "mrqa_triviaqa-validation-2675", "mrqa_triviaqa-validation-5820", "mrqa_triviaqa-validation-6967", "mrqa_triviaqa-validation-6953", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-5700", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-3672", "mrqa_hotpotqa-validation-5288", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-5315", "mrqa_searchqa-validation-15829", "mrqa_searchqa-validation-7789", "mrqa_searchqa-validation-12451", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-7021"], "SR": 0.515625, "CSR": 0.5327380952380952, "EFR": 1.0, "Overall": 0.706781994047619}, {"timecode": 84, "before_eval_results": {"predictions": ["360", "to feel close to his son", "Geothermal gradient", "Low or moderate inflation", "Procol Harum", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "September 1973", "to prevent further offense by convincing the offenders that their conduct was wrong", "American Indian allies", "( Star Wars )", "the eurozone", "the right to vote", "the longest rotation period ( 243 days )", "turkey", "patrick maxwell", "20", "robert", "aragonite", "jonathan maxwell", "cogs", "robert", "South Africa", "helium", "Superman", "crow", "Kal Ho Naa Ho", "Volvo 850", "Clara Petacci", "season three", "Critics' Choice Television Award for Best Supporting Actress in a Comedy Series", "Cholas of the Chola empire in Indian subcontinent", "\"the Docile Don\"", "North Dakota", "magnus", "Mark Dayton", "No. 17", "public", "fight back against Israel in Gaza.", "Mother's Day poems", "Gov. Jan Brewer.", "Dube, 43, was killed", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.\"", "lost his job", "Nairobi, Kenya,", "its part to improve the environment by taking on greenhouse gas emissions.", "I", "the captain of a nearby ship", "84-year-old", "Virginia, West Virginia, the Carolinas, Tennessee, Kentucky and Arkansas.", "Three Mile Island", "ragweed", "Charles I", "Daniel Boone", "Hank Aaron", "Annie", "The Hunt for Bat Boy", "Boulders Beach", "rhythm", "fiber optics", "John", "Latin", "Manchester", "the sun", "Exodus"], "metric_results": {"EM": 0.421875, "QA-F1": 0.493187256292095}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.25, 0.0, 0.0, 1.0, 0.8387096774193548, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.15384615384615388, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-9728", "mrqa_triviaqa-validation-4563", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-1217", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-4977", "mrqa_triviaqa-validation-7520", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-3648", "mrqa_triviaqa-validation-1947", "mrqa_hotpotqa-validation-4919", "mrqa_hotpotqa-validation-406", "mrqa_hotpotqa-validation-3514", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-405", "mrqa_newsqa-validation-1846", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-2654", "mrqa_newsqa-validation-3208", "mrqa_searchqa-validation-14853", "mrqa_searchqa-validation-15550", "mrqa_searchqa-validation-12766", "mrqa_searchqa-validation-9351", "mrqa_searchqa-validation-10344", "mrqa_searchqa-validation-13197", "mrqa_searchqa-validation-1968"], "SR": 0.421875, "CSR": 0.5314338235294118, "EFR": 1.0, "Overall": 0.7065211397058823}, {"timecode": 85, "before_eval_results": {"predictions": ["red", "the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "Mason Alan Dinehart III", "red, white, and blue", "two", "7 m ( 23.0 ft )", "two", "methyl bromide", "2013", "Raya Yarbrough", "Sir Henry Cole", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "IX", "Dick Advocaat", "jonathan", "niece", "stilgian Larsson", "cuthbert", "mulberry", "$200 million", "ad nausea", "cereal", "Operation Overlord", "cumbria", "bowie", "Tom Shadyac", "actress", "The Weeknd", "Walldorf", "the Beatles", "Alfred Preis", "the Czech Kingdom", "third", "Elliot Fletcher", "$1 million", "Marika Nicolette Green", "Javed Miandad", "Sunday,", "\"Nude, Green Leaves and Bust\"", "Africa", "Florida's Everglades,", "Olivia Newton-John", "\"Empire of the Sun,\"", "Monday", "1,500 Marines", "Kris Allen,", "\"I have a strong objection to the genre of mixing fact with fiction,\"", "police to question people if there's reason to suspect they're in the United States illegally.", "india", "Forrest", "Achilles", "Bright & Schuster", "Timex", "landfills", "Harvard", "James Bond", "butterflies", "Sybil", "A Canticle for Leibowitz", "John Harvard", "Julia Roberts", "queens", "Johnny Mathis", "cox"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6457683100414078}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, true, false, true, true, true, false, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 0.6086956521739131, 0.8571428571428571, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 0.0, 0.3, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.25, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.2666666666666667, 1.0, 1.0, 0.08, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-1198", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-5851", "mrqa_triviaqa-validation-1658", "mrqa_triviaqa-validation-4892", "mrqa_triviaqa-validation-6304", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-5193", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-2450", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-1932", "mrqa_searchqa-validation-368", "mrqa_searchqa-validation-4565", "mrqa_searchqa-validation-4298", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-3227", "mrqa_triviaqa-validation-3052"], "SR": 0.53125, "CSR": 0.5314316860465116, "EFR": 1.0, "Overall": 0.7065207122093022}, {"timecode": 86, "before_eval_results": {"predictions": ["741 weeks", "Speaker of the House of Representatives", "on the two tablets", "Massachusetts", "ancient Athens", "Kevin Spacey", "6 March 1983", "innermost in the eye", "Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Hellenic Polytheistic Reconstructionism", "The Los Angeles Lakers", "September 8, 2017", "1989 album Sleeping with the Past", "lillian", "1875", "mary Briggs", "Bobbi Kristina Brown", "denim jeans", "wheel", "Philippines", "adventure", "india", "tide-wise", "birthday", "travel sickness", "Kim Sung-su", "John Davies", "Steel Venom", "Richard DeVos", "Archduke of Austria", "Reinhard Heydrich", "\"What's My Line?", "a hand", "OutKast", "Big Bad Wolf", "China Airlines", "American pharmaceutical company", "Tibet's independence", "around 10:30 p.m. October 3,", "If huge hunks of ice -- such as parts of Greenland and the western shelf of Antarctica -- melt, then the rise is expected to be more dramatic.", "British Prime Minister Gordon Brown's", "Unseeded Frenchwoman Aravane Rezai", "humans", "businessman", "The son of Gabon's former president", "The switch had been scheduled for February 17, but Congress delayed the conversion", "President Sheikh Sharif Sheikh Ahmed", "22 felony counts", "President Obama", "Robert Anthony Snow", "Frank Sinatra", "Auschwitz-Birkenau", "garlic", "the radius", "Anna", "Archie Bunker", "a quadrant", "Hawaii", "Punky Night", "Al Czervik", "a hat", "Times Square", "Immanuel Kant", "Australia"], "metric_results": {"EM": 0.390625, "QA-F1": 0.48267609126984123}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.5, 0.5, 0.0, 0.888888888888889, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-9242", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-2467", "mrqa_triviaqa-validation-5537", "mrqa_triviaqa-validation-4741", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-1473", "mrqa_triviaqa-validation-4787", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-3158", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-4506", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-3282", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-830", "mrqa_searchqa-validation-12870", "mrqa_searchqa-validation-16008", "mrqa_searchqa-validation-5992", "mrqa_searchqa-validation-15600", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-1863", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-12332"], "SR": 0.390625, "CSR": 0.5298132183908046, "EFR": 1.0, "Overall": 0.706197018678161}, {"timecode": 87, "before_eval_results": {"predictions": ["prince Igor", "conchita wurst", "Jessica Simpson", "The port of Terneuzen", "isle of man", "bacino", "London", "marx starkey", "Thailand", "sports agent", "Friday", "antelope", "massively multiplayer online games", "non-voters", "the Gospel of Matthew", "Antigonon leptopus", "British citizens", "Rick Marshall", "a section of the Torah ( Five Books of Moses ) used in Jewish liturgy during a single week", "2014", "Haliaeetus", "Kepner", "usually in May", "The Uralic languages", "$66.5 million", "12 countries", "Ice Princess", "1939", "JoJo", "Paris", "The Division of Cook", "24 hours a day and 7 days a week", "its air-cushioned sole", "January 4, 1821", "alcoholic drinks for consumption on the premises", "twenty-three", "TD Garden", "generally from an older generation", "Diego Milito's", "Ricardo Valles de la Rosa,", "Summer", "a skilled hacker", "Manmohan Singh's", "more than 4,000 commercial farmers", "development of two courses on the Black Sea coast in Bulgaria.", "American Hugo Vihlen", "sovereignty over them.", "flipped and landed on its right side,", "183 people,", "The Cold Mountain", "Texas A&M", "the Crossword", "Charlotte", "Delaware", "the sostenuto", "Hammurabi", "Ohio State", "a Hoosier", "the Rhine", "veterans", "San Francisco", "Teen Titan Go!", "Love", "number 5"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6336371527777778}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.375, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-2955", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3242", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-3881", "mrqa_naturalquestions-validation-7464", "mrqa_hotpotqa-validation-5868", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-5327", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4457", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-2377", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-3536", "mrqa_newsqa-validation-780", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-12503", "mrqa_searchqa-validation-13271", "mrqa_searchqa-validation-14831", "mrqa_hotpotqa-validation-5840"], "SR": 0.5625, "CSR": 0.5301846590909092, "EFR": 1.0, "Overall": 0.7062713068181818}, {"timecode": 88, "before_eval_results": {"predictions": ["almost entirely in Wake County, it lies just north of the state capital, Raleigh", "a `` house edge '', a statistical advantage for the casino that is built into the game", "John Adams", "is the span of historic events from approximately 1945 that are immediately relevant to the present time", "1995 Dodge Stealth - Seen in The Turbo Charged Prelude for 2 Fast 2 Furious", "Kansas", "Kansas and Oklahoma", "September 9, 2010", "Isthmus of Corinth", "awarded to the team that lost the pre-game coin toss", "San Antonio", "Peter Andrew Beardsley MBE", "Spacewar", "jia", "city of Savannah", "Jan van Eyck", "mausoleum of giza", "trade union", "zhejiang", "from Russia with Love", "blackfriars", "dior", "my Fair Lady", "migration", "ynys m\u00f4n", "Pamela Chopra", "December 19, 1998", "Black Swan", "Wes Unseld", "a minor basilica", "Hampton University", "Illinois", "Dr. Nefarious", "Interstate 95", "\"Nebo Zovyot\"", "1993", "1st Duke of Marlborough", "It's a place where elephants can roam freely, largely feed and shelter themselves and interact with others, often after years living alone in captivity.", "almost 9 million", "The plane", "Kenneth Cole", "Hussein's Revolutionary Command Council.", "two bodies out of the plant,", "scooter", "an average of 25 percent", "The United Nations", "through a facility in Salt Lake City, Utah,", "Nineteen", "hardship for terminally ill patients and their caregivers,", "the local", "repent", "Vespa", "Johnson", "Dmitri Mendeleev", "on or directly in front of the pitching rubber", "the Red Sea", "the Dauphin", "Ulysses S. Grant", "the Oakland Raiders", "cashmere wool", "tongue", "break up ice jams.", "Haiti.", "police"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5839640827922077}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, false, false, false, true, false, true, true, true, false, false, false, false, true, true, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.1, 0.375, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.7272727272727273, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-1449", "mrqa_triviaqa-validation-2660", "mrqa_triviaqa-validation-7069", "mrqa_triviaqa-validation-6377", "mrqa_triviaqa-validation-1999", "mrqa_triviaqa-validation-856", "mrqa_triviaqa-validation-5477", "mrqa_triviaqa-validation-5417", "mrqa_hotpotqa-validation-4087", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-5614", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3219", "mrqa_newsqa-validation-1086", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-908", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-5185", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-4967", "mrqa_newsqa-validation-3460"], "SR": 0.453125, "CSR": 0.5293188202247191, "EFR": 1.0, "Overall": 0.7060981390449438}, {"timecode": 89, "before_eval_results": {"predictions": ["Ricardo Valles de la Rosa,", "Katherine Jackson, his three children and undisclosed charities.", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.", "The Swiss art heist follows the recent theft in Switzerland of two paintings", "Kgalema Motlanthe,", "it -- you know -- black is beautiful,\"", "a complicated man", "Henrik Stenson", "executive director of the Americas Division of Human Rights Watch,", "refusal or inability to \"turn it off\"", "12", "surgical anesthetic propofol", "Mandarin", "`` women and children are vulnerable to violence because of their unequal social, economic, and political status in society", "the Colony of Virginia", "April 1, 2016", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "Munich, Bavaria", "the +, -, *, and / keys, respectively", "Kirstjen Nielsen", "British R&B girl group Eternal", "Cyndi Grecco", "Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "The fossilized remains were originally called Plesippus shoshonensis", "Antony", "salsa rojo", "Margaret Beckett", "\u4e2d\u570b", "Nowhere Boy", "conterminous United States", "auk", "'The Lone Ranger'", "Buddhist", "17 May 2013", "steel", "sharpening steels", "2011", "Nick Cassavetes", "John Singleton and Stephanie Allain", "the National Basketball Development League (NBDL)", "Frank Fertitta, Jr.", "2,627", "life insurance", "Kareena Kapoor Khan", "Hopeless Records", "1971", "J. Cole", "the 2012 Summer Olympics", "Anzio", "Lawrence Taylor", "cvicus", "Mr. Fred Rogers", "Hillary Rodham Clinton", "Texas", "occipital lobe", "the divine right of kings", "Uranus", "cauliflower", "a kettledrum", "Reform", "asparagus beetles", "The Office", "Elmore Leonard"], "metric_results": {"EM": 0.5, "QA-F1": 0.5979189213564213}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 0.2, 0.04761904761904762, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.8, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1950", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-1552", "mrqa_newsqa-validation-843", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-7027", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-154", "mrqa_triviaqa-validation-6501", "mrqa_triviaqa-validation-5535", "mrqa_triviaqa-validation-428", "mrqa_triviaqa-validation-7008", "mrqa_triviaqa-validation-3067", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2928", "mrqa_searchqa-validation-4669", "mrqa_searchqa-validation-11910", "mrqa_searchqa-validation-14695", "mrqa_searchqa-validation-10083", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-5328", "mrqa_searchqa-validation-14645", "mrqa_searchqa-validation-926"], "SR": 0.5, "CSR": 0.5289930555555555, "EFR": 0.96875, "Overall": 0.6997829861111111}, {"timecode": 90, "UKR": 0.685546875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1259", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1659", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2320", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-3974", "mrqa_hotpotqa-validation-3990", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-4121", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5288", "mrqa_hotpotqa-validation-5289", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5387", "mrqa_hotpotqa-validation-5405", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5697", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-817", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1449", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-44", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4895", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-851", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-8754", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9581", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2091", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2550", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2600", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3779", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-10357", "mrqa_searchqa-validation-10483", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-11082", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-1128", "mrqa_searchqa-validation-11545", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11703", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-12136", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12332", "mrqa_searchqa-validation-12351", "mrqa_searchqa-validation-12473", "mrqa_searchqa-validation-12539", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12833", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13615", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14577", "mrqa_searchqa-validation-1462", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15886", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16161", "mrqa_searchqa-validation-1623", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-16380", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-16958", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1968", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-2334", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3937", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-4352", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-5157", "mrqa_searchqa-validation-539", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-571", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-6702", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7037", "mrqa_searchqa-validation-7225", "mrqa_searchqa-validation-7235", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-788", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8884", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-9653", "mrqa_searchqa-validation-9659", "mrqa_squad-validation-10076", "mrqa_squad-validation-10295", "mrqa_squad-validation-10307", "mrqa_squad-validation-10309", "mrqa_squad-validation-1176", "mrqa_squad-validation-1657", "mrqa_squad-validation-1908", "mrqa_squad-validation-1980", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2675", "mrqa_squad-validation-2849", "mrqa_squad-validation-3147", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3355", "mrqa_squad-validation-4127", "mrqa_squad-validation-4429", "mrqa_squad-validation-4623", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4840", "mrqa_squad-validation-5410", "mrqa_squad-validation-5592", "mrqa_squad-validation-5951", "mrqa_squad-validation-6106", "mrqa_squad-validation-6166", "mrqa_squad-validation-6409", "mrqa_squad-validation-6564", "mrqa_squad-validation-6681", "mrqa_squad-validation-6812", "mrqa_squad-validation-7270", "mrqa_squad-validation-7300", "mrqa_squad-validation-7480", "mrqa_squad-validation-7707", "mrqa_squad-validation-7917", "mrqa_squad-validation-794", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8592", "mrqa_squad-validation-8679", "mrqa_squad-validation-8917", "mrqa_squad-validation-9575", "mrqa_squad-validation-9643", "mrqa_squad-validation-9697", "mrqa_squad-validation-9787", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-1362", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-1820", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2792", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3149", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-3509", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3745", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3798", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4252", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-4863", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-5417", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5535", "mrqa_triviaqa-validation-5537", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5977", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6082", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-6377", "mrqa_triviaqa-validation-641", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6634", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-7588", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-884"], "OKR": 0.814453125, "KG": 0.453125, "before_eval_results": {"predictions": ["David Joseph Madden", "Garbi\u00f1e Muguruza", "the reactor core", "Wingstop", "9.7 m", "Andy Serkis", "Andrew Garfield", "cut off close by the hip", "Ali Daei", "the Sun", "a action role - playing video game developed and published by Nippon Ichi Software for the PlayStation 4", "Nala", "Germany", "doves", "charlie b Boyd", "copper", "sugar", "Joan Rivers", "croquet", "coelacanth", "Mel Blanc", "b Baron Kinnock", "Ken Platt", "charlie lom", "steveland hardaway jkins", "Edmonton, Alberta", "Chelsea", "Indiana", "Christina Ricci", "The Treaty of Gandamak", "Newell Highway", "Sports Illustrated", "Iceal \"Gene\" Hambleton", "1730", "1903", "Oregon Ducks football", "Nye", "Thabo Mbeki,", "28", "$81,92012", "40", "Sabina Guzzanti", "Lindsey oil refinery,", "\"Charles Lock is one of an estimated 400 farmers who have remained in the country", "U.S. Court of Appeals for the District of Columbia.", "heavy flannel or wool", "Chester Stiles,", "a nuclear weapon.", "Maj. Nidal Malik Hasan, MD,", "It's My Party", "the Haunted Mansion", "the Moon", "the Patriarch Tree", "China", "the Grand Canyon", "Judges", "Judas", "Edgar Rice Burroughs", "the Seine", "Toy Story", "Samuel Morse", "9", "a relic", "Kansas State"], "metric_results": {"EM": 0.609375, "QA-F1": 0.690922619047619}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.6666666666666666, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.6666666666666666, 0.2666666666666667, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8653", "mrqa_naturalquestions-validation-6169", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-422", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-2973", "mrqa_triviaqa-validation-5299", "mrqa_hotpotqa-validation-3528", "mrqa_hotpotqa-validation-3657", "mrqa_hotpotqa-validation-2301", "mrqa_newsqa-validation-3390", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-827", "mrqa_newsqa-validation-1308", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-2316", "mrqa_searchqa-validation-7599", "mrqa_searchqa-validation-13576"], "SR": 0.609375, "CSR": 0.5298763736263736, "EFR": 1.0, "Overall": 0.6966002747252747}, {"timecode": 91, "before_eval_results": {"predictions": ["1612", "reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors", "Wyatt and Dylan Walters", "in the blood to the liver", "Coton in the Elms", "2015", "2007", "12.65 m", "French Canadian", "counter clockwise", "272", "Narendra Modi", "Vice President", "charlie Dickens's major novels", "charlie boyd", "tony blair", "harmolodic life", "Judges 16", "Islam", "Steve Biko", "the Warren Commission", "charlie boyd", "sow", "Donald Trump", "david copperfield", "queen consort of Hanover", "Central Park", "over 281", "Craig William Macneill", "evangelical Christian periodical", "Prussia", "Dutch", "Bangkok", "Springfield, Massachusetts", "1983", "Sam Kinison", "Ghostbusters Spooktacular attraction", "The sheriff's offices, the Idaho Fish and Game Department, and the U.S. Fish and Wildlife Service", "Super Mario Kart.", "Mary Phagan Kean", "Brett Cummins,", "$40 and a bread.", "Al-Aqsa mosque", "Pro-democracy leader and Nobel Peace Prize winner Aung San Suu Kyi", "Former U.S. soldier Steven Green", "June 2004", "British Prime Minister Gordon Brown's wife, Sarah,", "Los Angeles.", "\"Oprah is an angel, she is God-sent,\"", "hockey", "the Larch", "achates", "Wolfgang Amadeus Mozart", "the Graceland", "Castle Rock", "North Korea & South Korea", "Wrigley", "Daytona Beach", "the Damned", "Inuit", "Lord Henry", "Panama Canal", "iron", "J.R. Tolkien"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5277405753968254}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, false], "QA-F1": [0.0, 0.2777777777777778, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.8, 1.0, 0.7499999999999999, 0.0, 0.5333333333333333, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-5537", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-5512", "mrqa_triviaqa-validation-6129", "mrqa_hotpotqa-validation-3092", "mrqa_hotpotqa-validation-1149", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-618", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-2380", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-3803", "mrqa_searchqa-validation-11577", "mrqa_searchqa-validation-5807", "mrqa_searchqa-validation-11389", "mrqa_searchqa-validation-14018", "mrqa_searchqa-validation-15683", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-13792", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-5261", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-12632"], "SR": 0.40625, "CSR": 0.5285326086956521, "EFR": 0.9736842105263158, "Overall": 0.6910683638443935}, {"timecode": 92, "before_eval_results": {"predictions": ["mitosis", "Detective Superintendent Dave Kelly", "2011", "Lana Del Rey", "George Strait", "126 by Wilt Chamberlain from October 19, 1961 -- January 19, 1963", "Benzodiazepines", "U.S. service members who have died without their remains being identified", "due to Parker's pregnancy at the time of filming", "Malvolio", "James Corden", "November 17, 1800", "`` Psychomachia, '' an epic poem written in the fifth century", "michael croker", "Robben Island", "pumas", "John J. Pershing", "flowers", "Bjorn Borg", "Romania", "Baton Rouge", "Prince Andrew", "john johnson johnson", "December 7, 1941", "harrods", "Polka", "Ford Falcon", "Vernon Kay", "third baseman", "1926", "Irish Chekhov", "Adrian Peter McLaren", "London", "bioelectromagnetics", "pneumatic tyres", "Pakistan", "Hampton University", "1994", "leftist rebels", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "Serie A", "a crocodile", "A New York City crackdown on suspects allegedly involved in forged credit cards and identity theft led authorities to a $13 million global crime ring,", "President Obama", "the coalition", "Ignazio La Russa", "581 points", "George Washington Bridge,", "CNN's", "James Bond", "Massachusetts", "John Madden", "50 First Dates", "the Left Bank", "Nikita Khrushchev", "Newport", "South Carolina", "Harold Ramis", "1914", "Cairo", "E", "Tainy Sledstviya", "Stalybridge Celtic", "written for \"The New York Times\" and \"Popular Mechanics\", and is a regular contributor to various CNBC shows such as \"On the Money\""], "metric_results": {"EM": 0.578125, "QA-F1": 0.6697826396183474}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.823529411764706, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.21875, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-7507", "mrqa_triviaqa-validation-4332", "mrqa_triviaqa-validation-7096", "mrqa_triviaqa-validation-6411", "mrqa_triviaqa-validation-5975", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-124", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-277", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-524", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-11228", "mrqa_searchqa-validation-9478", "mrqa_searchqa-validation-10237", "mrqa_searchqa-validation-2094", "mrqa_hotpotqa-validation-3429", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-5211"], "SR": 0.578125, "CSR": 0.5290658602150538, "EFR": 0.9259259259259259, "Overall": 0.6816233572281959}, {"timecode": 93, "before_eval_results": {"predictions": ["comedy", "Richard Strauss", "Bow River", "is widely believed to have been the helmet of King R\u00e6dwald of East Anglia", "gull-wing", "12", "Dutch", "40 Days and 40 Nights", "Las Vegas", "Ealdorman", "benjamin ciaramello", "green and yellow", "Skegness", "season seven", "Speaker of the House of Representatives", "A 30 - something man ( XXXX )", "from the Anglo - Norman French waleis", "in interstellar space", "the chairman ( more usually called the `` chair '' or `` chairperson '' )", "Harlem River", "Simon Callow", "Prafulla Chandra Ghosh", "May 5, 1904", "based on sovereign states", "portal tomb", "circuit board", "mercury", "cricketer", "doe", "left book club", "giant", "baryon number", "december", "ghana", "Tony Blair", "freemen", "Camellia sinensis", "peanuts, nuts, shellfish, peanuts, tree nuts, wheat and soy", "NASCAR,", "Asashoryu", "Many Marines we talked to in this coastal, scrub pine-covered North Carolina base", "release a notorious killer", "Mugabe's opponents", "Flying a space shuttle is a little more challenging than what we did in the movie,", "The Sopranos", "three empty vodka bottles,", "Charlie Chaplin, Cary Grant, Clark Gable, Gregory Peck, Carol Lombard and Hearst's mistress Marion Davies", "Four", "education", "A Beautiful Mind", "Reader's Digest", "John Wesley", "The Simpsons", "40-Year-Old Virgin", "William Faulkner", "chocolate", "Clinton", "Canada", "The Sound of Silence", "Nadia Comneci", "General Lafayette", "one", "Melbourne", "\"Let it Roll:"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6671894639265963}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true], "QA-F1": [0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.4, 0.0, 0.7499999999999999, 0.11764705882352941, 1.0, 1.0, 0.6, 0.3333333333333333, 0.07692307692307693, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3252", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-5627", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-3559", "mrqa_triviaqa-validation-3056", "mrqa_triviaqa-validation-3116", "mrqa_triviaqa-validation-2398", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-6635", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-350", "mrqa_newsqa-validation-2633", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-9649", "mrqa_searchqa-validation-1090", "mrqa_searchqa-validation-11066", "mrqa_searchqa-validation-15122"], "SR": 0.5625, "CSR": 0.5294215425531915, "EFR": 0.9642857142857143, "Overall": 0.6893664513677812}, {"timecode": 94, "before_eval_results": {"predictions": ["rash", "(George Frederick Ernest Albert; 3 June 1865 \u2013 20 January 1936)", "full-sized nameplates", "secondary school study", "three", "first team", "Prince of Cambodia Norodom Sihanouk", "Kevin Smith", "Kansas", "My Cat from Hell", "February 12, 2014", "novelist and poet", "Bergen County", "$2.187 billion", "Whiskey Shivers", "India", "Super Bowl LII", "Ludacris", "1939", "Barbara Windsor", "Universal Pictures and Focus Features", "normally show IIII for four o'clock", "restricted naturalization to `` free white persons '' of `` good moral character ''", "Jonny Buckland", "The ladies'single figure skating competition of the 2018 Winter Olympics", "in California", "robert", "Leonardo di ser Piero da Vinci", "Severn", "strychnine", "hercule poirot", "Colombians", "zina", "Hans Lippershey", "Il Divo", "Omid Djalili", "the little dog", "response by raising its alert level, while the country's media went into overdrive trying to predict how this oblique and erratic state would respond.", "Casa de Campo International Airport", "at a depth of about 1,300 meters in the Mediterranean Sea.", "Michelle Rounds", "in the 1950s,", "the southern city of Naples", "clubs and bars", "Employee Free Choice act", "north-south", "a suicide bomber", "10-person", "a six-month amnesty period,", "the Shuttle", "Morse code", "My Fair Lady", "the Kingdom of the Crystal Skull", "the funny bone", "John Updike", "the Bay of Bengal", "lm", "(John) Boehner", "The Original 1941 Maltese Falcon", "William Blake", "Trojan prince & tragic hero", "Pringles", "negligent", "a hat"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5923726158101158}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, false, true, false, true, false, false], "QA-F1": [0.0, 0.16666666666666669, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.29629629629629634, 0.7692307692307693, 1.0, 1.0, 0.6666666666666666, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-60", "mrqa_hotpotqa-validation-2485", "mrqa_hotpotqa-validation-1189", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1864", "mrqa_hotpotqa-validation-1269", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-2743", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3746", "mrqa_triviaqa-validation-7582", "mrqa_triviaqa-validation-3094", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-273", "mrqa_searchqa-validation-9025", "mrqa_searchqa-validation-2624", "mrqa_searchqa-validation-3826", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-11460", "mrqa_searchqa-validation-11646", "mrqa_searchqa-validation-5274", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-14791"], "SR": 0.515625, "CSR": 0.5292763157894738, "EFR": 1.0, "Overall": 0.6964802631578948}, {"timecode": 95, "before_eval_results": {"predictions": ["6,396", "23 July 1989", "twin sister", "James Mitchum", "yellow fever", "right-hand", "Mika H\u00e4kkinen", "Todd McFarlane", "Republic of Ireland", "1942", "Germanic", "Bolan", "\"Realty Bites\"", "amphetamines", "Louis XV", "Steve Goodman", "the Bellas", "Thomas Jefferson's", "Miami Heat", "1920", "Lieutenant Templeton `` Faceman '' Peck", "1983", "November 5, 2017", "The vascular cambium", "Neil Young", "portugal", "Let It Snow!", "m Madrid", "Lithium", "Wednesday's", "Rajasthan", "a jumper", "mustard", "James Gang", "Dick Fosbury", "sebecc", "peter Nichols", "The Kirchners", "Friday,", "vegan bake", "Muslim revolutionary named Malcolm X", "are not for sale,", "10", "Pacific Ocean territory of Guam", "\"prostitute\"", "Colombia.", "Rihanna", "Dancing With the Stars.", "Kyra and Violet,", "Python", "pi", "Rio de Janeiro", "Chuck Yeager", "the tsuba", "fats", "Central Park", "Mary", "whales", "the Battle of Fort Donelson", "Bech", "Aaron Burr", "Dougie MacLean", "victims of rape", "2010"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6525669642857144}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-988", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-3503", "mrqa_hotpotqa-validation-2374", "mrqa_naturalquestions-validation-6789", "mrqa_naturalquestions-validation-5299", "mrqa_triviaqa-validation-2690", "mrqa_triviaqa-validation-723", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-4649", "mrqa_newsqa-validation-3125", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-3380", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-15280", "mrqa_searchqa-validation-11026", "mrqa_naturalquestions-validation-8037", "mrqa_naturalquestions-validation-1856"], "SR": 0.578125, "CSR": 0.52978515625, "EFR": 0.9629629629629629, "Overall": 0.6891746238425925}, {"timecode": 96, "before_eval_results": {"predictions": ["The Shropshire Union Canal", "\"Big Fucking German\"", "American", "1982", "Harlem", "Jeff Meldrum", "private", "gorillas", "Henry Lau", "her performance", "best winning percentage", "Marktown, Clayton Mark's planned worker community in Northwest Indiana", "Western District", "the variety b -- while short - haired type is listed as the variety a", "Boat lifts", "a castle", "Alison", "David Gahan", "Ernest Rutherford", "The Parlement de Bretagne ( Administrative and judicial centre of Brittany, Breton : Breujo\u00f9 Breizh )", "the Baltic Fleet of 41 sail under convoy of the HMS Serapis and HM hired armed vessel Countess of Scarborough near Flamborough Head", "a person has the usual two copies of chromosome 21, plus extra material from chromosome 21 attached to another chromosome", "mashed potato", "1986", "Nicole DuPort", "Driving Miss Daisy", "dogs", "michael Thatcher", "le Leicester", "tony jacksons", "Wash", "pussia", "rugby", "bone", "viii", "Jean-Paul Sartre", "branson", "rolled over", "MBA in finance", "people", "murder", "hiring of hundreds of foreign workers", "Wednesday.", "golf", "Roberto Micheletti,", "Kuranyi", "40 militants and six Pakistan soldiers", "engineering and construction", "July 18, 1994,", "A Tale of Murder", "The Communist Manifesto", "the bees", "Israel", "the yottabyte", "Jack Nicholson", "Danny Elfman", "arsenic", "Pyotr Ilyich Tchaikovsky", "salmon", "staff", "Senegal", "Thai soldiers crossed into Cambodian territory Wednesday near a disputed border temple that was the site of clashes last year,", "Muqtada al-Sadr,", "two"], "metric_results": {"EM": 0.484375, "QA-F1": 0.583094527598838}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 0.0, 0.19999999999999998, 0.0, 0.13333333333333333, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.25, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.06896551724137931, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-5188", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-871", "mrqa_hotpotqa-validation-4307", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-234", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-10625", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-637", "mrqa_triviaqa-validation-684", "mrqa_triviaqa-validation-6166", "mrqa_triviaqa-validation-259", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-2401", "mrqa_triviaqa-validation-6407", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-3219", "mrqa_searchqa-validation-3905", "mrqa_searchqa-validation-572", "mrqa_searchqa-validation-10303", "mrqa_searchqa-validation-14156", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-2036"], "SR": 0.484375, "CSR": 0.5293170103092784, "EFR": 1.0, "Overall": 0.6964884020618557}, {"timecode": 97, "before_eval_results": {"predictions": ["11 to 12 year old", "a bronze medal in the women's figure skating final,", "an independent homeland since 1983.", "Ryder Russell,", "three-time road race world champion,", "Pakistan", "137", "23 million square meters (248 million square feet)", "Frank,", "Gadahn,", "Barbara Dainton-West,", "President Bush", "Dennis Davern,", "The Sun", "Joseph Heller", "in all cases affecting ambassadors, other public ministers and consuls, and those in which a state shall be party", "A monocot related to lilies and grasses", "Joan Collins", "IBM", "Kida", "the heart", "by fermenting dietary fiber into short - chain fatty acids ( SCFAs ), such as acetic acid and butyric acid", "over a 20 - year period", "permanently absorbed the superhuman powers and the psyche of Carol Danvers, the original Ms. Marvel", "a committee under Institute of Chartered Accountants of India ( ICAI )", "polonium", "aircraft", "Cole Porter", "calf", "gaseous", "Clint Eastwood", "yichang", "Arthur Ashe", "cribbage", "darts", "Jane Seymour", "france", "American", "February 20, 1978", "Donna Paige Helmintoller", "London", "Hugh Grosvenor, 3rd Marquess of Westminster", "Bentley Twins", "2.1 million", "Bridgetown", "\"Baa, Baa, Black sheep\"", "Six Flags Great Adventure", "ABC1 and ABC2", "Pontins", "subtraction", "the FBI", "white wine", "heraldry", "Sesame Street", "Aphrodite", "French", "Asvalayana", "The Washington Post", "Mowgli", "Cyrano de Bergerac", "India", "Chopin", "Ohio", "Blackbeard"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5721595505189255}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, false, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true], "QA-F1": [0.0, 0.22222222222222224, 0.0, 1.0, 0.0, 0.0, 1.0, 0.15384615384615385, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.85, 0.8571428571428571, 0.9600000000000001, 0.8750000000000001, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-289", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-7253", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-10411", "mrqa_triviaqa-validation-6380", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-7444", "mrqa_triviaqa-validation-5466", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-1086", "mrqa_hotpotqa-validation-4285", "mrqa_hotpotqa-validation-4637", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2961", "mrqa_searchqa-validation-2295", "mrqa_searchqa-validation-6936", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-3746", "mrqa_searchqa-validation-15968", "mrqa_searchqa-validation-5183", "mrqa_searchqa-validation-15549", "mrqa_searchqa-validation-9260"], "SR": 0.46875, "CSR": 0.5286989795918368, "EFR": 1.0, "Overall": 0.6963647959183674}, {"timecode": 98, "before_eval_results": {"predictions": ["hawaii", "Los Angeles", "john heston", "yorkshire terriers", "Francois Mitterrand", "phrenology", "london", "hard Times", "John Travolta", "arson", "IKEA", "the Colossus of Rhodes", "the Greek Goddess of Revenge", "The Walking Dead", "euro", "the Raiders", "Georges Auguste Escoffier", "Arthur `` The President '' Flanders", "Originally written in Spanish, the book is more commonly published and read in the Philippines in either Tagalog or English", "the altitude", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "Watson", "Sohrai", "Before 1923", "eight", "40 Acres and a Mule Filmworks", "Lake Wallace", "Captain Beefheart & His Magic Band", "Clark County, Nevada", "Brookhaven", "atomic bomb", "the Americas and the entire South American temperate zone", "the Rose Garden", "Marvel Comics", "the Marx Brothers film", "Umar Israilov", "number 1", "$40 and a loaf of bread.", "a new GI Bill that expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of unemployment benefits and more than $2 billion in disaster assistance.", "an auxiliary lock", "Ity Bitsy Teeny Weeny Yellow Polka Dot Bikini.", "UNICEF", "Ali Larijani", "opposition candidate Morgan Tsvangirai,", "the annual White House Correspondents' Association dinner", "NATO", "Daniel Wozniak,", "hardship for terminally ill patients and their caregivers,", "the Arctic north of Murmansk down to the southern climes of Sochi", "Rudolph W. Giuliani", "a coyote", "Punch", "a teacher", "an apple-tree", "The Little Prince", "Garfield", "repent", "the drum", "Horatio Nelson", "Moscow", "tabula rasa", "March 2016", "159", "New York City"], "metric_results": {"EM": 0.640625, "QA-F1": 0.728482872664616}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.10526315789473684, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5714285714285715, 0.8, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.787878787878788, 1.0, 0.875, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-892", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2806", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-225", "mrqa_hotpotqa-validation-1555", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4237", "mrqa_hotpotqa-validation-2282", "mrqa_hotpotqa-validation-4662", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-3064", "mrqa_searchqa-validation-5194", "mrqa_searchqa-validation-15462", "mrqa_searchqa-validation-11428", "mrqa_naturalquestions-validation-3558"], "SR": 0.640625, "CSR": 0.5298295454545454, "EFR": 0.9565217391304348, "Overall": 0.687895256916996}, {"timecode": 99, "UKR": 0.693359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1259", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1659", "mrqa_hotpotqa-validation-1800", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2320", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-2485", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-3974", "mrqa_hotpotqa-validation-3990", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-4121", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5211", "mrqa_hotpotqa-validation-5288", "mrqa_hotpotqa-validation-5289", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-5387", "mrqa_hotpotqa-validation-5405", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5697", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-817", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-44", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4895", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-851", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-8754", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9581", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-994", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2091", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2600", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-350", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3779", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-827", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-10357", "mrqa_searchqa-validation-10483", "mrqa_searchqa-validation-11082", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-1128", "mrqa_searchqa-validation-11389", "mrqa_searchqa-validation-11545", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11703", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-11865", "mrqa_searchqa-validation-12136", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12332", "mrqa_searchqa-validation-12351", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12833", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13615", "mrqa_searchqa-validation-13792", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14577", "mrqa_searchqa-validation-14843", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15366", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-15683", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15886", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16044", "mrqa_searchqa-validation-1623", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-16380", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-16958", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1968", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-2334", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3905", "mrqa_searchqa-validation-3937", "mrqa_searchqa-validation-3963", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-4352", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-5157", "mrqa_searchqa-validation-539", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-571", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-6702", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-6990", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7037", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7235", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-788", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8884", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-9515", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-9653", "mrqa_searchqa-validation-9659", "mrqa_squad-validation-10076", "mrqa_squad-validation-10295", "mrqa_squad-validation-10307", "mrqa_squad-validation-1176", "mrqa_squad-validation-1657", "mrqa_squad-validation-1908", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2675", "mrqa_squad-validation-2849", "mrqa_squad-validation-3147", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3355", "mrqa_squad-validation-4127", "mrqa_squad-validation-4429", "mrqa_squad-validation-4623", "mrqa_squad-validation-4764", "mrqa_squad-validation-4840", "mrqa_squad-validation-5410", "mrqa_squad-validation-5592", "mrqa_squad-validation-5951", "mrqa_squad-validation-6106", "mrqa_squad-validation-6166", "mrqa_squad-validation-6409", "mrqa_squad-validation-6564", "mrqa_squad-validation-6681", "mrqa_squad-validation-6812", "mrqa_squad-validation-7270", "mrqa_squad-validation-7300", "mrqa_squad-validation-7480", "mrqa_squad-validation-7707", "mrqa_squad-validation-7917", "mrqa_squad-validation-794", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8592", "mrqa_squad-validation-8679", "mrqa_squad-validation-8917", "mrqa_squad-validation-9575", "mrqa_squad-validation-9643", "mrqa_squad-validation-9697", "mrqa_squad-validation-9787", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-1362", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-1820", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-2498", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2792", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3149", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3358", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3745", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3798", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-425", "mrqa_triviaqa-validation-4252", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-4863", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-5375", "mrqa_triviaqa-validation-5417", "mrqa_triviaqa-validation-545", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5535", "mrqa_triviaqa-validation-5537", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-5977", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-6082", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-6377", "mrqa_triviaqa-validation-641", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-723", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-7310", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-7588", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-799", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-884"], "OKR": 0.8515625, "KG": 0.4796875, "before_eval_results": {"predictions": ["dh", "Dublin", "Pilgrim's Progress", "seaweed", "wren", "Pakistan", "south bank", "a widow", "south pacific", "Massachusetts", "black", "Melbourne", "opera libretti", "International Orange", "sunny", "Claudia Grace Wells", "Jules Shear", "a compiler can derive machine code -- a form consisting of instructions that the computer can directly execute", "Meeting Sweet at The Bronze", "under Herod", "\u00c9mile Gagnan and Naval Lieutenant ( `` lieutenant de vaisseau '' ) Jacques Cousteau", "Florida", "a sweet alcoholic drink made with rum, fruit juice, and syrup or Grenadine", "by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "1922", "two", "Broadway musicals", "Guangzhou", "lower Manhattan", "GE Appliances", "The Keeping Hours", "The Life of Charlotte Bront\u00eb", "Peter Pan Live!", "2007", "President of the United States", "Girls' Generation", "we Need a Little Christmas", "183", "Venus Williams", "Lifeway's 100-plus stores nationwide", "a number of calls,", "Marie-Therese Walter.", "alcohol", "attacked L.K. Chaudhary, the chief executive of an Italian car parts manufacturing company.", "56,", "A Colorado prosecutor", "11th year in a row.", "Michael Krane,", "The University of California San Diego has suspended a student who admitted to hanging a noose in a campus library,", "\"A Connecticut Yankee in King Arthur's Court", "horse", "The Bravados", "Northwest Airlines", "China", "South Africa", "Alien", "Gossage", "a comic", "Sephora", "Fletcher Christian", "Latter-day Saints", "Doctor Dolittle", "Over the Rainbow", "The Vagabond"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6520407751916556}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.125, 0.0, 0.6666666666666666, 0.8235294117647058, 1.0, 0.7368421052631579, 0.8, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-1069", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-640", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-9885", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4294", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-1911", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-5163", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1749", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-3562", "mrqa_newsqa-validation-397", "mrqa_searchqa-validation-12378", "mrqa_searchqa-validation-3858", "mrqa_searchqa-validation-2151", "mrqa_searchqa-validation-2348", "mrqa_searchqa-validation-2773", "mrqa_searchqa-validation-9449", "mrqa_searchqa-validation-11184", "mrqa_searchqa-validation-15555"], "SR": 0.515625, "CSR": 0.5296875, "EFR": 0.967741935483871, "Overall": 0.7044077620967741}]}