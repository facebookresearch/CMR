{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=100_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[3]', diff_loss_weight=0, ewc_gamma=1.0, ewc_lambda=100.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=100_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[3]_result.json', stream_id=3, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4100, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["The Jones et al. and Briffa reconstructions", "help transfer and dissipate excess energy", "Historical and Critical Dictionary", "human settlement and development of the land", "over three days", "eight", "polynomial algebra", "60 days", "American Association of University Women", "mid-Cambrian period", "1926", "The outcome of most votes can be predicted beforehand", "Newton", "a disaster", "between the 1960s and 1990s", "James Hutton", "British", "growth and investment", "Each step had to be successfully accomplished before the next ones could be performed", "Downtown Riverside", "plague of Athens in 430 BC", "expelled Jews", "interleukin 1", "DuMont Television Network", "The city of Fresno", "1762", "colloblasts", "self", "Alta California", "300 km long and up to 40 km wide", "two-phased system", "On the Councils and the Church", "Super Bowl LI", "1968", "\"right\", \"just\", or \"true\"", "pastors", "meritocracy", "Vistula River", "arrows, swords, and leather shields", "23 November 1963", "since the 1960s", "zero net force", "2012", "CD4", "income inequality", "Tuition Fee Supplement", "April 1523", "the California State Automobile Association and the Automobile Club of Southern California", "Von Miller", "lymphocytes or an antibody-based humoral response", "unsuccessful", "faith alone, whether fiduciary or dogmatic, cannot justify man", "Percy Shelley", "education, sanitation, and traffic control", "six divisions", "six series of theses", "Pi\u0142sudski", "Disney\u2013ABC Television Group", "SAP Center", "Genghis Khan", "materials melted near an impact crater", "William the Silent", "182 million tons", "John G. Trump"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8592347756410257}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.08333333333333334, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4191", "mrqa_squad-validation-9426", "mrqa_squad-validation-368", "mrqa_squad-validation-3776", "mrqa_squad-validation-4769", "mrqa_squad-validation-9194", "mrqa_squad-validation-10305", "mrqa_squad-validation-2812", "mrqa_squad-validation-6559", "mrqa_squad-validation-2153", "mrqa_squad-validation-7246"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["occupational stress", "Sunni Arabs from Iraq and Syria", "The Lower Rhine", "Mike Tolbert", "silt up the lake", "cigarette advertising from all television and radio networks", "Dorothy and Michael Hintze", "climate change", "northwest", "Turkey", "Economist", "The Tyneside flat", "73", "Derek Jacobi", "ten minutes", "Sydney", "Basel", "ideal strings", "unstable molecules", "lasting damage", "December 2014", "other locations throughout Scotland", "eastern", "Newton", "Westwood One", "2008", "the colonies of British America and New France", "early 1546", "lower bounds", "2011", "became the University of Northumbria at Newcastle", "between 1859 and 1865", "8\u20134\u20134 system", "a green algal derived chloroplast", "over 200", "William the Conqueror", "The Service Module was discarded", "18 million volumes", "north", "detention", "collenchyma tissue", "Louis Adamic", "cytokine T IGF-\u03b2", "England", "Yuri Gagarin", "antigenic variation", "Knaurs Lexikon", "Chester, South Carolina", "1992", "three", "9.1 million", "rich and well socially standing", "pharynx", "1969", "Wardenclyffe Tower", "Tower Theatre", "700,000", "Pittsburgh", "jeopardy/2516_Qs.txt at master  jedoublen/jeopardy", "Heroes struggle... Animated, Action, Adventure, Fantasy, Sci-Fi.... Lego Star Wars: The Yoda Ch", "American baseball, until the late 1940s, excluded, with some big exceptions in... The color line was broken for good when Jackie Robinson signed with the Brooklyn", "the hand of cards which he supposedly held at the time of his death... killed by the assassin Jack McCall in Deadwood, Black Hills, August 2, 1876", "His... (initial capital letter) a German-built enciphering machine developed for commercial", "Europe"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8128918650793651}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714285, 0.16, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9752", "mrqa_squad-validation-5549", "mrqa_squad-validation-8718", "mrqa_squad-validation-8891", "mrqa_squad-validation-9744", "mrqa_squad-validation-5337", "mrqa_squad-validation-3885", "mrqa_squad-validation-1568", "mrqa_squad-validation-6523", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-4596"], "SR": 0.78125, "CSR": 0.8046875, "EFR": 1.0, "Overall": 0.90234375}, {"timecode": 2, "before_eval_results": {"predictions": ["John Sutcliffe", "can also concentrate wealth, pass environmental costs on to society, and abuse workers and consumers", "6800", "medical treatment for the sick and wounded French soldiers", "Ticonderoga Point", "October 16, 1973", "1980s", "seven", "books and articles for magazines and journals", "Roger NFL", "the oceans and seas", "60,000 European settlers", "by over 100%", "1350", "North America", "Euclid's fundamental theorem of arithmetic", "Maria de la Queillerie", "to encourage investment", "Julia Butterfly Hill", "farming assets, ranging from mobile phones to productive land and livestock, and is opening pathways for them to move out of poverty", "587,000 square kilometres", "March 1974", "quickly", "end of the 19th century", "DeMarcus Ware", "University of North Florida team", "motivated students", "platyctenids", "elementary school education certificate", "1220", "two", "30.0%", "Raghuram Rajan", "VHF channel 7", "The Pink Triangle", "ideological", "Methodist institutions", "Ticonderoga", "six daughters", "Los Angeles Dodgers", "19th", "a course of study and lesson plan", "a delay costs money", "Funchess", "Watt", "1,000 m3/s (35,000 cu ft/s)", "Thuringia", "rivers", "anti-Semitic policies of the National Socialists", "visor helmet", "Catholic", "Hollywood", "Long Island Sound", "Sweden's", "an invaluable service as usurers in medieval society", "an African American", "first woman governor", "an athlete who plays cricket", "conifer", "Alaska", "an Austrian and American film actress and inventor", "Daniel Defoe", "the Association of American Universities", "50JJB Sports Fitness Clubs"], "metric_results": {"EM": 0.625, "QA-F1": 0.7018229166666666}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 0.6, 0.5, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666665]}}, "before_error_ids": ["mrqa_squad-validation-7525", "mrqa_squad-validation-10258", "mrqa_squad-validation-1565", "mrqa_squad-validation-85", "mrqa_squad-validation-802", "mrqa_squad-validation-9061", "mrqa_squad-validation-8322", "mrqa_squad-validation-4257", "mrqa_squad-validation-1960", "mrqa_squad-validation-5678", "mrqa_squad-validation-6101", "mrqa_squad-validation-2786", "mrqa_squad-validation-1877", "mrqa_squad-validation-2497", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-5540", "mrqa_searchqa-validation-8844", "mrqa_searchqa-validation-13452", "mrqa_searchqa-validation-12341", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-5603"], "SR": 0.625, "CSR": 0.7447916666666667, "EFR": 0.9166666666666666, "Overall": 0.8307291666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["public schools", "connection id in a table", "males", "electrical, water, sewage, phone, and cable facilities", "level of the top tax rate", "\"Wise up or die.\"", "VideoGuard UK", "highly-paid", "pump this into the mesoglea", "Fred Silverman", "atmospheric engine", "force", "ctenophores", "trial and rehabilitation of Joan of Arc", "John Hurt", "an Australian public X.25 network operated by Telstra", "Arizona Cardinals", "Von Miller", "Indianapolis Colts", "42%", "1957", "imprisonment", "orogenic wedges", "one", "Catholic", "Edict of Fontainebleau", "Fort Caroline", "Pittard Sullivan", "wealth", "Niagara Falls", "Hugh Downs", "House of Hohenstaufen", "3D printing technology", "Daniel Burke", "internal strife", "Matthew Murray", "400 AD", "the United States", "Satya Nadella", "the difference between a problem and an instance", "Richard E. Grant", "Inner Mongolia", "cortisol and catecholamines", "a third group of pigments found in cyanobacteria", "isopentenyl pyrophosphate synthesis", "1963", "hotel room", "Italy", "Joan Hughes", "workhouse", "Khartoum", "William Henry Harrison", "Playboy rabbit", "Dutch metaphysicians", "Puerto Rico", "Court TV", "King Tut", "The Prairie Wolf", "Inhospitable Sea", "Moshe Dayan", "Joan Van Dinh", "active athletes", "helicopters and boats", "$17,000"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6802522130647131}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, false, false, false, true, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.9333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.4444444444444445, 0.4615384615384615, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7103", "mrqa_squad-validation-7357", "mrqa_squad-validation-4452", "mrqa_squad-validation-3247", "mrqa_squad-validation-230", "mrqa_squad-validation-363", "mrqa_squad-validation-10083", "mrqa_squad-validation-3139", "mrqa_squad-validation-1045", "mrqa_squad-validation-5542", "mrqa_squad-validation-1670", "mrqa_squad-validation-6263", "mrqa_squad-validation-8839", "mrqa_squad-validation-7711", "mrqa_searchqa-validation-8170", "mrqa_searchqa-validation-12199", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-16725", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-6624", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-3588"], "SR": 0.609375, "CSR": 0.7109375, "EFR": 1.0, "Overall": 0.85546875}, {"timecode": 4, "before_eval_results": {"predictions": ["whether he stood by their contents", "1850s", "Troika", "complexity classes", "Parliament of the United Kingdom at Westminster", "teaching", "Desperate Housewives, Lost and Grey's Anatomy", "8 November 2010", "the Y. pestis was spread from fleas on rats", "coastal beaches and the game reserves", "1524", "2p \u2212 1", "horizontal", "Crash the Super Bowl", "collenchyma tissue", "around a billion years ago", "Croatia", "Long Beach", "Edinburgh", "both teams", "Papin", "Cricket", "William Morris", "T. J. Ward", "the Daleks", "San Diego", "1017", "heat and pressure", "1072", "Chevron", "Africa", "New York City", "Marshall Cohen", "Hypersensitivity", "Business Connect", "Henry Young Darracott Scott", "not break any law", "income inequality", "Bainbridge's", "1294", "chao", "Apollo 17", "Ghirardelli", "Ghirardelli", "Stephen Hawking", "Ghirardelli", "Europe", "Ghirardelli", "Ghirardelli", "Ghirardelli", "Ghirardelli", "Moscow", "crystal anniversary", "prairie crocus", "Detroit River", "Seth Taft", "New Testament", "Dublin", "Ghirardelli", "Ghirardelli", "Albert Einstein", "Doctor Who", "1990", "Zimbabwe"], "metric_results": {"EM": 0.640625, "QA-F1": 0.670217803030303}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5878", "mrqa_squad-validation-5029", "mrqa_squad-validation-2765", "mrqa_squad-validation-821", "mrqa_squad-validation-5344", "mrqa_squad-validation-7615", "mrqa_squad-validation-2679", "mrqa_squad-validation-1061", "mrqa_squad-validation-4147", "mrqa_searchqa-validation-6952", "mrqa_searchqa-validation-9601", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-7713", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-4300"], "SR": 0.640625, "CSR": 0.696875, "EFR": 1.0, "Overall": 0.8484375}, {"timecode": 5, "before_eval_results": {"predictions": ["November 2006", "achievement-oriented", "a malfunction in the chameleon circuit", "SAP Center", "March 2011", "above the top of the range", "12th", "1226", "The Late Late Show", "bird", "P", "theNP-complete knapsack problem", "1928", "when the immune system is less active than normal", "disturbances", "Fraud", "1562 to 1598", "Emmerich Rhine Bridge", "ten", "1993", "chloroplasts", "lung tissue", "US$100,000", "Bakersfield", "Bruno Mars", "patients' prescriptions and patient safety issues", "2009", "the Common Core", "Recognized Student Organizations", "a cubic interpolation formula", "Thomas Edison", "the Swiss Reformation", "phagocytes", "15", "Satyagraha", "two", "microorganisms", "1968", "Aloha \u02bbOe", "1.6 million", "Eric Whitacre", "gourd-bows", "Angus Young", "New York", "the waltz Gunstwerber", "Cherokee River", "Odisha", "January 28, 2016", "138,535", "Adam Rex", "1933", "Sivakumar", "1968", "a Chaplain to the Forces", "astronomer", "Warrington", "1866", "Chattahoochee", "the Provisional Irish Republican Army", "pneumonoultramicroscopicsilicovolcanoconiosis", "Boutros Ghali", "Vertikal-T", "The Wizard of Oz", "he was mad at the U.S. military"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6799278846153846}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.4, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8531", "mrqa_squad-validation-434", "mrqa_squad-validation-9185", "mrqa_squad-validation-1759", "mrqa_squad-validation-1860", "mrqa_squad-validation-8696", "mrqa_squad-validation-3496", "mrqa_squad-validation-2634", "mrqa_squad-validation-664", "mrqa_squad-validation-3240", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-2158", "mrqa_newsqa-validation-1468", "mrqa_searchqa-validation-13492", "mrqa_newsqa-validation-3290"], "SR": 0.59375, "CSR": 0.6796875, "EFR": 1.0, "Overall": 0.83984375}, {"timecode": 6, "before_eval_results": {"predictions": ["UHF", "muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins", "Africa", "1976", "inverse", "non-deterministic time", "60", "Warsaw", "5,000 years", "Edgar Atheling", "Writers Guild of America", "UNEP", "Conservative", "11 million", "a water-cooled undergarment", "the Queen", "3 in 1,000,000", "2009 onwards", "Super Bowl Opening Night", "Fresno Street and Thorne Ave", "southern and central parts of France", "2014", "40%", "John F. Kennedy", "innate immune system", "15\u20131", "history of arms", "Industry and manufacturing", "this contact with nature made him stronger, both physically and mentally.", "1543", "\u015ar\u00f3dmie\u015bcie", "Hmong or Laotian", "Stromules", "oxygen will act as a fuel;", "Johnny Herbert", "\"jus sanguinis\"", "Pharrell Williams", "James Dearden", "J\u00f3zsef Pulitzer", "June 17, 2007", "\"The Frost Report\"", "National Basketball Development League", "Danish", "24 January 76", "Kona coast", "Dave Lee Travis", "Northrop P-61 Black widow", "People v. Turner", "\"Veyyil\"", "Helena Sternlicht", "Highlands Course", "Illinois", "Cartoon Cartoon Fridays", "Timo Hildebrand", "2016", "Ginger Rogers", "Corps of Discovery", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "801,200", "giraffe", "navy", "Ecuador", "Abraham Lincoln", "a final contest"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7132575757575758}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10424", "mrqa_squad-validation-1730", "mrqa_squad-validation-1114", "mrqa_squad-validation-8523", "mrqa_squad-validation-4070", "mrqa_squad-validation-7770", "mrqa_squad-validation-3764", "mrqa_squad-validation-1232", "mrqa_squad-validation-3483", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2183", "mrqa_naturalquestions-validation-4953", "mrqa_newsqa-validation-3782", "mrqa_searchqa-validation-5374"], "SR": 0.671875, "CSR": 0.6785714285714286, "EFR": 1.0, "Overall": 0.8392857142857143}, {"timecode": 7, "before_eval_results": {"predictions": ["convection of the mantle", "Continental Edison Company in France", "South", "Trevathan", "25 minutes of transmission length", "1903", "1993", "King George III", "occupancy permit", "Hereford", "a trusted friend, who may hold any office, from Elder to Bishop, or no office at all", "Arts & Entertainment Television (A&E)", "NASA", "Albert Einstein", "Karl von Miltitz", "a matter of custom or expectation,", "Silk Road", "a war", "39", "Bolshevik leaders", "University of Aberdeen", "transportation, sewer, hazardous waste and water", "during the plague of Athens in 430 BC", "Pedro Men\u00e9ndez de Avil\u00e9s", "expansion", "a deterministic Turing machine", "linear", "when the oxygen concentration is too high", "1290", "17,786,419,", "smallest state on the Australian mainland", "Montreal Montreal", "7000301604928199000", "was an incident on March 5, 1770, in which British Army soldiers shot and killed people while under attack by a mob", "member states on a voluntary basis", "2000", "Christina Calvano", "Samantha Jo", "the human hands and face", "Martin Lawrence", "Steve Hale", "a multilayer", "January 2017 patch", "Idaho", "September 6, 2019", "multinational retail corporation", "Joely Richardson", "Jack Gleeson", "claims adjusters", "when the forward reaction proceeds at the same rate as the reverse reaction", "heart", "writ of certiorari", "A standard form contract", "Mr. Feeny", "September 30", "Kelly Osbourne, Ian'' Dicko '' Dickson, Christina Monk and Eddie Perfect", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "a Californio nobleman and master living in Los Angeles during the era of Spanish rule", "\"Household Words\",", "56", "Hindu scriptures", "St. Augustine", "a vowel", "gold"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6656906864937389}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.07692307692307691, 0.8333333333333333, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.046511627906976744, 0.0, 0.5, 0.4615384615384615, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1239", "mrqa_squad-validation-764", "mrqa_squad-validation-2315", "mrqa_squad-validation-6878", "mrqa_squad-validation-360", "mrqa_squad-validation-1819", "mrqa_squad-validation-2881", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-3491", "mrqa_triviaqa-validation-7579", "mrqa_searchqa-validation-12472", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-3744"], "SR": 0.578125, "CSR": 0.666015625, "EFR": 0.9629629629629629, "Overall": 0.8144892939814814}, {"timecode": 8, "before_eval_results": {"predictions": ["ivory", "The Dornbirner Ach", "a certain number of teacher's salaries are paid by the State", "non-deterministic time", "five", "December 2014", "a inauspicious typhoon", "four", "Zwickau prophet", "10 July 1856 \u2013 7 January 1943", "1999", "digital streams of the game via CBS Sports apps on tablets, Windows 10, Xbox One and other digital media players", "3\u20132.7 billion years ago", "New Testament from Greek", "Von Miller", "economists with the Standard & Poor's rating agency", "Germany", "a vestigial red algal derived chloroplast", "two or more teachers working harmoniously to fulfill the needs of every student in the classroom", "a computer network funded by the U.S. National Science Foundation (NSF)", "type III secretion system", "worker, capitalist/business owner, landlord", "tungsten", "the state", "oxidant", "the signals could come from Mars, Venus, or other planets", "the American Revolution", "Book of Discipline", "Fat Albert", "1943", "\"Big Fucking German\"", "Chelmsford City", "William Novak", "22,500 acres", "1951", "Abu Dhabi, United Arab Emirates", "playing as members of the Big Eight Conference", "American", "Firth of Forth Site of Special Scientific Interest", "one live album", "#364", "\"The Birds\"", "Battle of the Rosebud", "Homebrewing", "Pablo Escobar", "Hanna-Barbera", "26 June 2013", "25 million", "twice", "Geraldine Sue Page", "Rochdale", "Charles Reed Bishop", "marine applications", "Marco Fu Ka-chun", "2015", "Dusty Springfield", "her translation of and commentary on Isaac Newton's book \"Principia\"", "BeBe Winans", "Henry VIII", "July", "purple", "Dumont d'Urville Station", "Under normal conditions", "a spiritual conversion"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7371647662688263}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.34782608695652173, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_squad-validation-1156", "mrqa_squad-validation-531", "mrqa_squad-validation-2272", "mrqa_squad-validation-1912", "mrqa_squad-validation-1529", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-391", "mrqa_triviaqa-validation-4490", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-5851"], "SR": 0.640625, "CSR": 0.6631944444444444, "EFR": 1.0, "Overall": 0.8315972222222222}, {"timecode": 9, "before_eval_results": {"predictions": ["1999", "zero", "Mao Zedong", "Chebyshev", "1960", "introduction of Beroe", "1000 CE", "arthur", "Sunspot, New Mexico", "Sonderungsverbot", "amending", "the environment in which they lived", "a genetic disease such as severe combined immunodeficiency, acquired conditions such as HIV/AIDS, or the use of immunosuppressive medication", "C. J. Anderson", "Cadeby", "Warraghiggey", "starts accidentally adding oxygen to sugar precursors", "World Meteorological Organization", "Sunni pan-Islamism", "11 points", "yes or no, or alternately either 1 or 0", "black", "the Tangut relief army", "English", "Newton", "1940s and 1950s", "arthur of Ord and Tore on the Black Isle, in Ross and Cromarty, Scotland", "Broadway musicals", "Taoiseach of Ireland", "Duval County", "Bill Ponsford", "$10\u201320 million", "Manasseh Cutler Hall", "Denmark", "Hindi", "the \"Pour le M\u00e9rite\"", "Giuseppe Fortunino Francesco Verdi", "Edward Trowbridge Collins Sr.", "1946 and 1947", "Christopher McCulloch", "2016\u201317", "Carson City", "Wembley Stadium", "19th", "Cesar Millan", "Bob Dylan", "Michael Lewis Greenwell", "from 20 March to 1 May 2003", "The Life of Charlotte Bront\u00eb", "2015", "Bill Curry", "Jack White", "Kim Yoon-seok and Ha Jung-woo", "superhero roles", "arthur", "18th congressional district", "the BBC", "2008", "arthur's", "arthur", "Logar province", "arvaquin, Avelox, Noroxin and Floxin", "arthur", "Jamaica"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7036991517254674}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 0.7368421052631579, 0.4615384615384615, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.923076923076923, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5714285714285715, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9897", "mrqa_squad-validation-1025", "mrqa_squad-validation-3981", "mrqa_squad-validation-6443", "mrqa_squad-validation-8832", "mrqa_squad-validation-8782", "mrqa_squad-validation-260", "mrqa_squad-validation-1652", "mrqa_squad-validation-6185", "mrqa_hotpotqa-validation-2670", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-5723", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3089", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4397", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-2645", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_searchqa-validation-13221"], "SR": 0.578125, "CSR": 0.6546875, "EFR": 1.0, "Overall": 0.82734375}, {"timecode": 10, "UKR": 0.771484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2168", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-2928", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3089", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3894", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4397", "mrqa_hotpotqa-validation-451", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4633", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5145", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-5474", "mrqa_hotpotqa-validation-5527", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5723", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-690", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-599", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8381", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-9687", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-28", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-11252", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-12625", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-13492", "mrqa_searchqa-validation-13863", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14454", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-16753", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-4300", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5540", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6624", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-6952", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-7713", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-8844", "mrqa_searchqa-validation-9383", "mrqa_squad-validation-10007", "mrqa_squad-validation-10010", "mrqa_squad-validation-10031", "mrqa_squad-validation-10083", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10130", "mrqa_squad-validation-10153", "mrqa_squad-validation-10173", "mrqa_squad-validation-10216", "mrqa_squad-validation-10234", "mrqa_squad-validation-10249", "mrqa_squad-validation-10258", "mrqa_squad-validation-10305", "mrqa_squad-validation-10345", "mrqa_squad-validation-10424", "mrqa_squad-validation-1045", "mrqa_squad-validation-10478", "mrqa_squad-validation-10496", "mrqa_squad-validation-1061", "mrqa_squad-validation-1073", "mrqa_squad-validation-1075", "mrqa_squad-validation-1075", "mrqa_squad-validation-1096", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1183", "mrqa_squad-validation-1232", "mrqa_squad-validation-1239", "mrqa_squad-validation-1254", "mrqa_squad-validation-1296", "mrqa_squad-validation-1372", "mrqa_squad-validation-1529", "mrqa_squad-validation-1543", "mrqa_squad-validation-1586", "mrqa_squad-validation-1632", "mrqa_squad-validation-1652", "mrqa_squad-validation-1681", "mrqa_squad-validation-1723", "mrqa_squad-validation-1730", "mrqa_squad-validation-1731", "mrqa_squad-validation-1759", "mrqa_squad-validation-1783", "mrqa_squad-validation-1819", "mrqa_squad-validation-185", "mrqa_squad-validation-1860", "mrqa_squad-validation-1877", "mrqa_squad-validation-1940", "mrqa_squad-validation-1960", "mrqa_squad-validation-1976", "mrqa_squad-validation-1985", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2111", "mrqa_squad-validation-2153", "mrqa_squad-validation-2181", "mrqa_squad-validation-2189", "mrqa_squad-validation-2246", "mrqa_squad-validation-2247", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2320", "mrqa_squad-validation-2359", "mrqa_squad-validation-2413", "mrqa_squad-validation-2442", "mrqa_squad-validation-2474", "mrqa_squad-validation-2475", "mrqa_squad-validation-2489", "mrqa_squad-validation-2497", "mrqa_squad-validation-2568", "mrqa_squad-validation-2591", "mrqa_squad-validation-260", "mrqa_squad-validation-2617", "mrqa_squad-validation-2628", "mrqa_squad-validation-2634", "mrqa_squad-validation-2644", "mrqa_squad-validation-2679", "mrqa_squad-validation-2721", "mrqa_squad-validation-2723", "mrqa_squad-validation-2765", "mrqa_squad-validation-2808", "mrqa_squad-validation-2812", "mrqa_squad-validation-2881", "mrqa_squad-validation-2941", "mrqa_squad-validation-2949", "mrqa_squad-validation-2975", "mrqa_squad-validation-2977", "mrqa_squad-validation-30", "mrqa_squad-validation-3111", "mrqa_squad-validation-3139", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3248", "mrqa_squad-validation-3269", "mrqa_squad-validation-3296", "mrqa_squad-validation-3377", "mrqa_squad-validation-3429", "mrqa_squad-validation-3483", "mrqa_squad-validation-3496", "mrqa_squad-validation-3534", "mrqa_squad-validation-3552", "mrqa_squad-validation-360", "mrqa_squad-validation-363", "mrqa_squad-validation-368", "mrqa_squad-validation-3705", "mrqa_squad-validation-3743", "mrqa_squad-validation-3764", "mrqa_squad-validation-3767", "mrqa_squad-validation-3776", "mrqa_squad-validation-3787", "mrqa_squad-validation-3810", "mrqa_squad-validation-3885", "mrqa_squad-validation-3952", "mrqa_squad-validation-3981", "mrqa_squad-validation-4067", "mrqa_squad-validation-4070", "mrqa_squad-validation-4070", "mrqa_squad-validation-4095", "mrqa_squad-validation-4107", "mrqa_squad-validation-4121", "mrqa_squad-validation-4121", "mrqa_squad-validation-4147", "mrqa_squad-validation-4191", "mrqa_squad-validation-421", "mrqa_squad-validation-4223", "mrqa_squad-validation-4228", "mrqa_squad-validation-4257", "mrqa_squad-validation-4327", "mrqa_squad-validation-434", "mrqa_squad-validation-4423", "mrqa_squad-validation-451", "mrqa_squad-validation-4516", "mrqa_squad-validation-457", "mrqa_squad-validation-460", "mrqa_squad-validation-4609", "mrqa_squad-validation-4689", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4758", "mrqa_squad-validation-4797", "mrqa_squad-validation-480", "mrqa_squad-validation-4840", "mrqa_squad-validation-4898", "mrqa_squad-validation-4988", "mrqa_squad-validation-4997", "mrqa_squad-validation-502", "mrqa_squad-validation-5029", "mrqa_squad-validation-5061", "mrqa_squad-validation-5096", "mrqa_squad-validation-5108", "mrqa_squad-validation-5200", "mrqa_squad-validation-5222", "mrqa_squad-validation-5270", "mrqa_squad-validation-5272", "mrqa_squad-validation-5287", "mrqa_squad-validation-531", "mrqa_squad-validation-5337", "mrqa_squad-validation-5344", "mrqa_squad-validation-5347", "mrqa_squad-validation-5382", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5549", "mrqa_squad-validation-5551", "mrqa_squad-validation-556", "mrqa_squad-validation-5621", "mrqa_squad-validation-5670", "mrqa_squad-validation-5741", "mrqa_squad-validation-5794", "mrqa_squad-validation-5839", "mrqa_squad-validation-5878", "mrqa_squad-validation-5881", "mrqa_squad-validation-5895", "mrqa_squad-validation-5922", "mrqa_squad-validation-5952", "mrqa_squad-validation-6000", "mrqa_squad-validation-6027", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6252", "mrqa_squad-validation-6259", "mrqa_squad-validation-6260", "mrqa_squad-validation-6263", "mrqa_squad-validation-6277", "mrqa_squad-validation-6295", "mrqa_squad-validation-6347", "mrqa_squad-validation-6389", "mrqa_squad-validation-6441", "mrqa_squad-validation-6443", "mrqa_squad-validation-6468", "mrqa_squad-validation-65", "mrqa_squad-validation-6500", "mrqa_squad-validation-6505", "mrqa_squad-validation-6538", "mrqa_squad-validation-6548", "mrqa_squad-validation-6569", "mrqa_squad-validation-6589", "mrqa_squad-validation-6600", "mrqa_squad-validation-6612", "mrqa_squad-validation-6624", "mrqa_squad-validation-664", "mrqa_squad-validation-6656", "mrqa_squad-validation-6657", "mrqa_squad-validation-6666", "mrqa_squad-validation-6695", "mrqa_squad-validation-6749", "mrqa_squad-validation-6858", "mrqa_squad-validation-6861", "mrqa_squad-validation-6878", "mrqa_squad-validation-6880", "mrqa_squad-validation-6888", "mrqa_squad-validation-689", "mrqa_squad-validation-6898", "mrqa_squad-validation-6900", "mrqa_squad-validation-6951", "mrqa_squad-validation-6953", "mrqa_squad-validation-6967", "mrqa_squad-validation-7018", "mrqa_squad-validation-7021", "mrqa_squad-validation-7033", "mrqa_squad-validation-7036", "mrqa_squad-validation-7062", "mrqa_squad-validation-7123", "mrqa_squad-validation-7125", "mrqa_squad-validation-7135", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7246", "mrqa_squad-validation-7268", "mrqa_squad-validation-7302", "mrqa_squad-validation-7312", "mrqa_squad-validation-7323", "mrqa_squad-validation-7357", "mrqa_squad-validation-7362", "mrqa_squad-validation-7373", "mrqa_squad-validation-738", "mrqa_squad-validation-7391", "mrqa_squad-validation-741", "mrqa_squad-validation-7450", "mrqa_squad-validation-7458", "mrqa_squad-validation-7466", "mrqa_squad-validation-7470", "mrqa_squad-validation-755", "mrqa_squad-validation-7562", "mrqa_squad-validation-7603", "mrqa_squad-validation-764", "mrqa_squad-validation-767", "mrqa_squad-validation-7686", "mrqa_squad-validation-7711", "mrqa_squad-validation-7736", "mrqa_squad-validation-7744", "mrqa_squad-validation-7765", "mrqa_squad-validation-7770", "mrqa_squad-validation-782", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7892", "mrqa_squad-validation-7902", "mrqa_squad-validation-7957", "mrqa_squad-validation-7970", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8194", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8224", "mrqa_squad-validation-8232", "mrqa_squad-validation-8236", "mrqa_squad-validation-8391", "mrqa_squad-validation-8391", "mrqa_squad-validation-85", "mrqa_squad-validation-8523", "mrqa_squad-validation-8531", "mrqa_squad-validation-8572", "mrqa_squad-validation-8612", "mrqa_squad-validation-8668", "mrqa_squad-validation-8680", "mrqa_squad-validation-8685", "mrqa_squad-validation-8696", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-8743", "mrqa_squad-validation-8760", "mrqa_squad-validation-8763", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8794", "mrqa_squad-validation-8797", "mrqa_squad-validation-8832", "mrqa_squad-validation-8837", "mrqa_squad-validation-8839", "mrqa_squad-validation-8891", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-8965", "mrqa_squad-validation-904", "mrqa_squad-validation-9185", "mrqa_squad-validation-9191", "mrqa_squad-validation-9236", "mrqa_squad-validation-9268", "mrqa_squad-validation-9300", "mrqa_squad-validation-9314", "mrqa_squad-validation-9317", "mrqa_squad-validation-9330", "mrqa_squad-validation-938", "mrqa_squad-validation-9401", "mrqa_squad-validation-9426", "mrqa_squad-validation-9507", "mrqa_squad-validation-9546", "mrqa_squad-validation-9579", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9628", "mrqa_squad-validation-9744", "mrqa_squad-validation-9752", "mrqa_squad-validation-9815", "mrqa_squad-validation-9817", "mrqa_squad-validation-9890", "mrqa_squad-validation-9892", "mrqa_squad-validation-9897", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_squad-validation-9947", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-2645", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-7579"], "OKR": 0.90625, "KG": 0.3453125, "before_eval_results": {"predictions": ["Thermochemical techniques", "executive producer", "1987", "James O. McKinsey", "North", "one-eighth", "coastal beaches and the game reserves", "Vicodin", "\u00a34.2bn", "Katharina von Bora", "9 March 1508", "Shoushi Li", "his means of seizing power", "ideal strings", "after the Franco-German War", "German", "San Andreas Fault", "NYPD Blue", "Northern Chinese", "Oireachtas funds", "the Marconi Company", "countries with bigger income inequalities", "John Robert Cocker", "King Kelly", "a United States Army Medical Corps psychiatrist", "Richard Masur", "1988", "Bergen County", "The Ones Who Walk Away from Omelas", "hiphop", "Lithuania", "historic buildings, arts, and published works", "Disney Parks Christmas Day Parade", "Esp\u00edrito Santo Financial Group", "Guillermo del Toro", "Wolf Creek", "YouTube", "onset and progression of Alzheimer's disease", "San Francisco 49ers", "at the 1980 Winter Olympics in Lake Placid, New York", "Iron Man 3", "Suffolk", "singer, songwriter, actress, and radio and television presenting", "Chicago", "Martin \"Marty\" McCann", "247,597", "Mandalay Entertainment", "actor", "a specialized version of the two-seat F/A-18F Super Hornet", "Barnoldswick", "Pacific War", "A41", "Heather Langenkamp", "Leona Lewis", "The Ministry of Utmost Happiness", "BAFTA TV Award Best Actor winner", "Rodney Crowell", "Andy Serkis", "a specific phobia", "aida", "Musharraf", "cancer", "aida", "aida"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6858315295815296}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7887", "mrqa_squad-validation-2976", "mrqa_squad-validation-1480", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-3822", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-739", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-1133", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-7417", "mrqa_newsqa-validation-850", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-5418"], "SR": 0.609375, "CSR": 0.6505681818181819, "EFR": 1.0, "Overall": 0.7347230113636363}, {"timecode": 11, "before_eval_results": {"predictions": ["1887", "semi-arid savanna", "2014", "Levi's Stadium", "misguided", "San Jose Marriott", "1972", "second-largest", "Decision Time", "Victorian Government", "the American Revolutionary War", "pep rally", "human", "the Treaties establishing the European Union", "the Command Module's heat shield", "Amazonia", "Daniel Andrews", "UNESCO's World Heritage list", "Richard E. Grant", "49\u201315", "NCAA's Division I", "Mark Helfrich", "Wal-Mart Canada Corp.", "\"Louie\" Zamperini", "\"Che\" Guevara", "Carol Ann Duffy", "Karl-Anthony Towns Jr.", "1978", "Danish", "Ukrainian", "1954", "\"John\" Alexander Florence", "\"brainwash\")", "9Lives", "\"valley of the hazels'", "Art Bell", "\"Lady Frederick Windsor\"", "Eminem", "Point Place", "Knowlton School", "Delilah Rene", "Don Bluth", "Columbus", "the Czech Kingdom", "Anne Fletcher", "Sacramento Kings", "South Asian Games", "Tufts College", "Harrods", "Flamingo Las Vegas", "\"Girl Meets World\"", "Dakota Johnson", "City of Newcastle", "Japan", "Canada", "in the pancreas", "privatized", "silver", "insects", "about 9 p.m.", "\"the most dangerous precedent in this country, violating all of our due process rights,\"", "\"Annie Get Your Gun\"", "New York", "Hebrew"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7173363095238094}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1219", "mrqa_squad-validation-8447", "mrqa_squad-validation-7288", "mrqa_squad-validation-4015", "mrqa_squad-validation-234", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-369", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-3881", "mrqa_hotpotqa-validation-4371", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-953", "mrqa_triviaqa-validation-1604", "mrqa_triviaqa-validation-818", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-3580", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-7583"], "SR": 0.609375, "CSR": 0.6471354166666667, "EFR": 1.0, "Overall": 0.7340364583333334}, {"timecode": 12, "before_eval_results": {"predictions": ["late 1545", "several hundred thousand", "five", "every two years", "two-man", "The Hoppings", "tuberculosis", "C. J. Anderson", "Harvey Martin", "stratigraphic", "environmental determinism", "Wellington", "problem instance", "inertia of the ball associated with its constant velocity in the direction of the vehicle's motion that ensures the ball continues to move forward even as it is thrown up and falls back down", "a sin", "for translation initiation in most chloroplasts and prokaryotes", "Alemannic dialect", "Edmonton, Canada", "Anthony Stephen Burke", "Eugene O'Neill", "Max Kellerman", "created the American Land-Grant universities and colleges", "VfL Wolfsburg", "July 22, 1946", "Julia Verdin", "Pendlebury", "McLaren-Honda", "Bismarck", "Comedy Film Nerds", "2016 World Indoor Championships", "MG Cars", "January 18, 1977", "North Greenwich Arena", "The Soloist", "Nikita Khrushchev", "(born Harold Lipshitz, March 20, 1931)", "the fourth President of Pakistan", "October 23, 1989", "automobiles", "Republican", "Allison J71", "Chad", "NBA season was the 16th season for the Minnesota Timberwolves in the National Basketball Association", "Emilia Fox", "Freeform", "Mark Masons' Hall", "Cristiano Ronaldo", "American", "Chief Strategy Officer", "November 15, 1903", "De La Soul", "American", "Archbishop of Canterbury", "Via Vai", "1985", "After releasing Xander from the obligation to be Sweet's `` bride '', tells the group how much fun they have been", "when the cell is undergoing the metaphase of cell division", "California", "(multi-user dungeon)", "Gulf of Aden", "Iran", "Halle Berry", "Sindbad", "Abid Ali Neemuchwala"], "metric_results": {"EM": 0.625, "QA-F1": 0.7079159708755297}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20512820512820512, 1.0, 0.9411764705882353, 0.0, 1.0, 0.4, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809523, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-914", "mrqa_squad-validation-6654", "mrqa_squad-validation-10328", "mrqa_squad-validation-8852", "mrqa_squad-validation-9190", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-452", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-2364", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-8159", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-3242", "mrqa_newsqa-validation-642", "mrqa_searchqa-validation-13537"], "SR": 0.625, "CSR": 0.6454326923076923, "EFR": 1.0, "Overall": 0.7336959134615385}, {"timecode": 13, "before_eval_results": {"predictions": ["Tower District", "computational", "\"social and political action,\"", "Duran Duran", "30", "NYPD Blue", "chemical bonds", "John and Benjamin Green", "Lippe", "between AD 0\u20131250", "2 million", "a statement to the chamber", "40,000", "Citadel Broadcasting", "$45,000", "stream capture", "735 feet", "about a young husband and wife and how they deal with the challenge of buying secret Christmas gifts for each other with very little money", "Robert Remak", "Eddie Murphy", "oregon", "Human fertilization", "Asset = Liabilities + Equity", "The terrestrial biosphere", "Peter Andrew Beardsley MBE", "Leo Arnaud", "lumbar cistern", "not being pushed around by big labels, managers, and agents and being told what to do", "1 US dollar worth close to 5,770 guaranies", "digitization of social systems", "the League of Communists of Yugoslavia", "Middlesex County", "long - standing policy of neutrality was tested on many occasions during the 1930s", "in 1972", "lightning strike", "they were weaker when it came to training and tertiary education", "instant messenger", "George Strait", "silk, hair / fur", "Anakin Luke", "New York", "Paspahegh Indians", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "Spain", "Toronto and locations in Canada and the United States", "Manchuria", "Ben Savage", "at a given temperature", "Max", "Magyarorsz\u00e1g z\u00e1szlaja", "the church at Philippi", "2003", "the star", "northern China", "Mackinac Bridge", "Barbarella", "Bergen", "Balvenie Castle", "Some money he made from trading sugar bought at a discount from the supermarket where he worked, and funds borrowed from friends to secure a visitor's visa and bus ticket to Johannesburg.", "repression and dire economic circumstances", "Leon Trotsky", "oregon", "Elizabeth Gaskell", "American radiologist remembered for describing Hampton's hump and Hampton's line"], "metric_results": {"EM": 0.5, "QA-F1": 0.6158964070039437}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true, true, false, true, true, true, false, true, false, false, true, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9777777777777777, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.8205128205128205, 1.0, 1.0, 0.33333333333333337, 0.1904761904761905, 0.5882352941176471, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.5, 0.5, 0.0, 0.0, 0.18181818181818182, 0.0, 0.125, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1600", "mrqa_squad-validation-3599", "mrqa_squad-validation-4304", "mrqa_squad-validation-9483", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-5608", "mrqa_hotpotqa-validation-877", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1879", "mrqa_searchqa-validation-16103", "mrqa_hotpotqa-validation-3149"], "SR": 0.5, "CSR": 0.6350446428571428, "EFR": 1.0, "Overall": 0.7316183035714285}, {"timecode": 14, "before_eval_results": {"predictions": ["reaffirmed Catholicism as the state religion of France, but granted the Protestants equality with Catholics under the throne and a degree of religious and political freedom within their domains", "reached an all-time high between 2005 and 2010", "Shropshire", "a desired social goal", "cartels", "Anglo-Saxon populations", "Ancient Egypt", "Battle of Olustee", "patrimonial feudalism", "Daniel arap Moi", "near Millingen aan de Rijn", "an electric lighting company in Tesla's name, Tesla Electric Light & Manufacturing", "1303", "oxidant", "three", "Lance Cpl. Maria Lauterbach", "1994", "Empire of the Sun", "54 bodies", "Roger Federer", "sodium dichromate", "helicopters and boats, as well as vessels from other agencies,", "July", "citizenship", "40", "18 years to life in prison", "to alleviate the flooding", "Expedia", "Kabul", "\"I'm just getting started,\"", "Communist Party of Nepal", "Bob Dole", "Eden Park", "in her home", "12.3 million people worldwide", "as soon as 2050", "from the shelves in all of Lifeway's 100-plus stores nationwide", "Shanghai", "18", "National Park Service", "AS Roma beat Lecce 3-2", "Bob Bogle", "40 militants and six Pakistan soldiers dead", "1959", "Pakistan's High Commission in India", "his father", "Obama's", "President-elect Barack Obama", "the Obama administration", "Howard Bragman", "a peace sign", "Muslim festival of Eid al-Adha", "Larry Ellison", "Revolutionary Armed Forces of Colombia", "a unknown recipient", "Jules Shear", "the Soviet Union", "Vienna", "2008\u201309 UEFA Champions League", "310", "West Arlington", "Achaemenid Empire", "2017", "Algernod Lanier Washington"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6237608745421246}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false], "QA-F1": [0.8095238095238095, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3163", "mrqa_squad-validation-1886", "mrqa_squad-validation-1064", "mrqa_squad-validation-7017", "mrqa_squad-validation-1287", "mrqa_squad-validation-3532", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-324", "mrqa_hotpotqa-validation-2080", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-1723", "mrqa_hotpotqa-validation-5370"], "SR": 0.53125, "CSR": 0.628125, "EFR": 1.0, "Overall": 0.730234375}, {"timecode": 15, "before_eval_results": {"predictions": ["Robert R. Gilruth", "Arthur Woolf", "Ten", "multiple revisions", "seven", "Funchess", "St. George's United Methodist Church", "1914", "Wales", "more than 70,000", "Nikita Khrushchev", "increasing unemployment", "X reduces to Y", "February 5", "he acted in self defense in punching businessman Marcus McGhee.", "anyone", "Superman had been fighting crime in print since 1938", "Peruvians", "sanctions 17 entities, including three government-owned or controlled companies used by Mugabe and his government \"to illegally siphon revenue and foreign exchange from the Zimbabwean people,\" as well as one individual.\"", "Senate Democrats", "15,000", "Kim Jong Un", "Tim Baker", "Philip Markoff", "Democratic", "the IV cafe", "North Korea intends to launch a long-range missile in the near future,", "the District of Columbia National Guard", "forgery and flying without a valid license", "digging ditches", "14", "are accused along with a third teen, Jesus Mendez, 16, of being in a group that poured alcohol over $40, a video game and a bicycle.", "school", "Monday and Tuesday", "27-year-old", "almost 100 vessels off Somalia's coast", "American third seed Venus Williams", "allergies", "as spies for more than two years,", "Manchester United", "President Obama", "Robert Barnett", "military commissions", "Alfredo Astiz,", "a Muslim with Lebanese heritage, but her family is \"not defined by religion,\"", "Illness", "procedures", "Jaime Andrade", "56", "Michael Jackson", "Raymond Thomas", "racially motivated", "Adam Lambert", "placed near the grave.", "The U.S. state of Georgia", "Redwood National Park ( established 1968 ) and California's Del Norte Coast, Jedediah Smith, and Prairie Creek Redwoods State Parks ( dating from the 1920s )", "citric", "Denali", "The New Yorker", "death", "the MasonDixon line", "high and dry", "a palace", "New Orleans Saints"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6163911088911089}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, true, false, true, true, true, true, false, false, true, true, false, false, false, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05128205128205128, 0.0, 0.5, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.888888888888889, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.0, 0.7272727272727273, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.4, 0.08, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-378", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3395", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-2561", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-4202", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-2265", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-6596", "mrqa_triviaqa-validation-210", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-3685"], "SR": 0.515625, "CSR": 0.62109375, "EFR": 0.967741935483871, "Overall": 0.7223765120967742}, {"timecode": 16, "before_eval_results": {"predictions": ["\"exterminate\" all non-Dalek beings.", "San Diego", "30\u201375%", "\"to implement Islamic values in all spheres of life.\"", "James Watt", "comb jellies", "NewcastleGateshead", "1989", "priest", "Denmark, Iceland and Norway", "the need for alliances", "Anderson", "1969", "Debbie Gibson", "to collect menstrual flow", "at least 18 or 21 years old ( or have a legal guardian present )", "10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "the UK", "In 1917", "The Fellowship of the Ring ( 2001 ), The Two Towers ( 2002 ) and The Return of the King ( 2003 )", "Montgomery", "Upon closure at birth", "to last four years unless renewed by the Reichstag", "December 1, 2009", "Miami Heat", "Timothy B. Schmit", "Alan Shearer", "The 1700 Cascadia earthquake", "the Central and South regions", "The Man", "in the blood to the liver", "in 1936", "two - year terms", "students", "September 19, 2017", "Andy Serkis", "The Abbott and Costello Show", "John Smith", "Idaho", "Ali", "Abraham Gottlob Werner", "Jason Paige", "Jackie Robinson", "Rufus and Chaka Khan", "the body - centered cubic ( BCC ) lattice", "Edward J. Scott", "a merengue", "3", "Olivia Olson", "erosion", "Department of Health and Human Services", "an integral membrane protein", "Conrad Lewis", "Justice Harlan", "a person employed to write or type what another dictates or to copy what has been written by another,", "Purple Rain", "Oakland, California", "World Famous Gold & Silver Pawn Shop", "Crandon, Wisconsin,", "loyalty and the way that I stood by this guy through thick and thin...\"", "Psycho", "sapphire", "Harry S Truman", "yellow"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6431795634920634}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, false, false], "QA-F1": [0.888888888888889, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.4, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7778", "mrqa_squad-validation-9610", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9811", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3436", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-5207", "mrqa_naturalquestions-validation-3385", "mrqa_triviaqa-validation-1394", "mrqa_hotpotqa-validation-5030", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2813", "mrqa_searchqa-validation-3751", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-262"], "SR": 0.546875, "CSR": 0.6167279411764706, "EFR": 1.0, "Overall": 0.7279549632352941}, {"timecode": 17, "before_eval_results": {"predictions": ["diversity", "\"Provisional Registration\"", "the Tyne and wear Metro", "phlogiston", "Creon", "QuickBooks", "1892", "The coordinating lead authors", "Six", "a set-up that uses movable pulleys", "\u00d6gedei Khan", "Princes Park", "between the 1950 and 2001 general elections", "Polk", "Mrs. Eastwood & Company", "first train robbery", "Las Vegas", "General Manager", "Owsley Stanley III", "The visit", "Hong Kong First Division League", "Unbreakable", "He can play as a striker or left striker", "\"How to Train Your Dragon\"", "Agra", "1.6 million passengers", "actress", "Gaius Julius Caesar Augustus Germanicus", "Jeffrey William Van Gundy", "Joachim Trier", "Tamil", "1972", "2013", "Golden Globe Award for Best Actor", "Ronald Lyle \" Ron\" Goldman", "David Allen", "Cleopatra VII Philopator", "political", "Tomorrowland", "16,116", "footballer", "Bishop's Stortford", "Joanna No\u00eblle Levesque", "late eighteenth century", "The School Boys", "Operation Iceberg", "Dallas Cowboys", "Hordaland", "January 2017", "30", "Polka", "October 22, 2012", "Soldier in Truck", "Noah Schnapp", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive", "Neptune", "France", "70,000 or so", "Miami Beach, Florida", "the second pilot episode of the science fiction television series Star Trek", "Karan", "Copenhagen", "the life of ancient Athens", "Civil War"], "metric_results": {"EM": 0.53125, "QA-F1": 0.651871992147923}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.4, 1.0, 0.8936170212765957, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2788", "mrqa_squad-validation-10479", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-2410", "mrqa_hotpotqa-validation-3874", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-4865", "mrqa_hotpotqa-validation-4419", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1308", "mrqa_naturalquestions-validation-7408", "mrqa_triviaqa-validation-4029", "mrqa_newsqa-validation-1720", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-3630", "mrqa_searchqa-validation-6753"], "SR": 0.53125, "CSR": 0.6119791666666667, "EFR": 1.0, "Overall": 0.7270052083333334}, {"timecode": 18, "before_eval_results": {"predictions": ["Lagos and Quiberon Bay", "seven months", "Sava Kosanovi\u0107", "return to his side", "\"Monte Carlo\")", "large compensation pools", "Graz", "5,792", "Lucas\u2013Lehmer", "Wahhabist", "February 26, 1948", "Hordaland", "the town of El Nacimiento in M\u00fazquiz Municipality", "Stephen James Ireland", "Koch Industries", "Washington", "Isle of Man", "Children's Mercy Park", "High Falls Brewery", "technical director", "Mike Holmgren", "Nathan Bedford Forrest", "Kim Hyun-ah", "green and yellow", "Isobel", "Urijah Faber", "Barack Obama's", "Guthred", "Peel Holdings", "6, 1967", "College Football Scoreboard", "Marko Tapani \" Marco\" Hietala", "In 2017, Pachulia won his first NBA Championship as a member of the Warriors.", "Sarah Hurst", "An invoice, bill or tab", "jimmy carter", "Clarence Nash", "Golden Valley, Minnesota", "Marco Fu", "Syracuse", "Durban International Convention Centre", "Ryan Babel", "Bob Dylan", "an \"out and back\" wooden roller ride", "Luca Guadagnino", "Jennifer Lynne", "11 Grands Prix wins", "National Collegiate Athletic Association", "Bill Cosby", "thirteen", "Argentinian", "Dana Fox", "Sunday, November 2, 2003", "northwest Washington", "London", "Romanian athlete Nadia Comaneci", "The Mayor of Casterbridge", "Friday", "Luca di Montezemolo", "Social Democratic", "The first European sugar preserves made use of that seemingly magical substance, honey.", "competing claims", "President Obama and Britain's Prince Charles", "a woman"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5734623015873016}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, true, true, false, false, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.5, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 0.14285714285714285, 0.5714285714285715, 0.4, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-653", "mrqa_squad-validation-6117", "mrqa_squad-validation-9057", "mrqa_squad-validation-8020", "mrqa_squad-validation-9592", "mrqa_hotpotqa-validation-1211", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-3491", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-1855", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-4366", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-4757", "mrqa_naturalquestions-validation-6658", "mrqa_triviaqa-validation-4403", "mrqa_newsqa-validation-2163", "mrqa_searchqa-validation-14240", "mrqa_searchqa-validation-3464", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2844"], "SR": 0.453125, "CSR": 0.6036184210526316, "EFR": 1.0, "Overall": 0.7253330592105264}, {"timecode": 19, "before_eval_results": {"predictions": ["the CEPR", "the Commission and Council", "14,000", "scoil phr\u00edobh\u00e1ideach", "90 to 95 percent", "the famous rock Lorelei", "56.2%", "time", "Christ's message and teachings", "Bayern Munich", "the fifth level", "five times", "A123 Systems", "\"the backside.\"", "Bothtec", "the VHF Global Lightning", "18 December 1975", "3,000", "the youngest TV director ever", "Thom Yorke", "About 200", "Marco Fu", "Orfeo ed Euridice", "\"She of Little Faith\"", "Kristin Scott Thomas, Anne Bancroft, James Fox, Derek Jacobi, and Sean Penn.", "Golden Calf", "Houston Rockets", "Summerlin, Nevada", "Argentinian", "Europe", "Don't Look Back in Anger", "Savannah River Site", "a family member", "Switzerland", "second largest", "Frank Lowy", "Fat Man", "Robert Marvin \"Bobby\" Hull, OC", "Nye County", "Herman's Hermits", "Mark Alan Dacascos (born February 26, 1964)", "Pendlebury, Lancashire", "300 km north west", "1932", "Arrowhead Stadium", "1885", "House of Borromeo", "a system of processing conflicts in which outcomes depend on what participants do, but no single force controls what occurs and its outcomes.", "Sydney", "Michael Redgrave", "KlingStubbins", "Big 12", "a kind of oil lamp", "2017", "RAF", "John McEnroe", "Matricide", "helicopters and robotic surveillance craft to the \"border states\"", "a one-shot victory", "apartheid", "Dune", "the zodiac", "\"teenagers in Versailles.\"", "Jupiter"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6621121066433566}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.3636363636363636, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7001", "mrqa_squad-validation-9093", "mrqa_squad-validation-1665", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-920", "mrqa_hotpotqa-validation-4172", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-119", "mrqa_naturalquestions-validation-954", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-2858", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-2686"], "SR": 0.609375, "CSR": 0.60390625, "EFR": 1.0, "Overall": 0.725390625}, {"timecode": 20, "UKR": 0.728515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1582", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1808", "mrqa_hotpotqa-validation-1855", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2022", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2295", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-240", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2806", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3571", "mrqa_hotpotqa-validation-3575", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3693", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3874", "mrqa_hotpotqa-validation-3881", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4419", "mrqa_hotpotqa-validation-447", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-451", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-4640", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5718", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5756", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4155", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5207", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7311", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8389", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-9707", "mrqa_newsqa-validation-1038", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3278", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3395", "mrqa_newsqa-validation-3421", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-392", "mrqa_newsqa-validation-402", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-642", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-12472", "mrqa_searchqa-validation-13492", "mrqa_searchqa-validation-14710", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-1723", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4596", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-7051", "mrqa_searchqa-validation-7076", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10031", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10115", "mrqa_squad-validation-10249", "mrqa_squad-validation-1025", "mrqa_squad-validation-10305", "mrqa_squad-validation-10328", "mrqa_squad-validation-10424", "mrqa_squad-validation-1045", "mrqa_squad-validation-10496", "mrqa_squad-validation-1061", "mrqa_squad-validation-1064", "mrqa_squad-validation-1064", "mrqa_squad-validation-1114", "mrqa_squad-validation-1162", "mrqa_squad-validation-1177", "mrqa_squad-validation-1183", "mrqa_squad-validation-1255", "mrqa_squad-validation-1296", "mrqa_squad-validation-1366", "mrqa_squad-validation-1480", "mrqa_squad-validation-1529", "mrqa_squad-validation-1565", "mrqa_squad-validation-1568", "mrqa_squad-validation-1597", "mrqa_squad-validation-1723", "mrqa_squad-validation-1748", "mrqa_squad-validation-1759", "mrqa_squad-validation-1783", "mrqa_squad-validation-185", "mrqa_squad-validation-1860", "mrqa_squad-validation-1940", "mrqa_squad-validation-1960", "mrqa_squad-validation-2040", "mrqa_squad-validation-2059", "mrqa_squad-validation-2111", "mrqa_squad-validation-2144", "mrqa_squad-validation-2153", "mrqa_squad-validation-2315", "mrqa_squad-validation-234", "mrqa_squad-validation-2475", "mrqa_squad-validation-2536", "mrqa_squad-validation-2568", "mrqa_squad-validation-2628", "mrqa_squad-validation-2701", "mrqa_squad-validation-2786", "mrqa_squad-validation-287", "mrqa_squad-validation-2898", "mrqa_squad-validation-2976", "mrqa_squad-validation-3139", "mrqa_squad-validation-3155", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3198", "mrqa_squad-validation-3240", "mrqa_squad-validation-3483", "mrqa_squad-validation-3532", "mrqa_squad-validation-3534", "mrqa_squad-validation-361", "mrqa_squad-validation-363", "mrqa_squad-validation-3743", "mrqa_squad-validation-3764", "mrqa_squad-validation-3767", "mrqa_squad-validation-3787", "mrqa_squad-validation-3810", "mrqa_squad-validation-3813", "mrqa_squad-validation-4015", "mrqa_squad-validation-4070", "mrqa_squad-validation-410", "mrqa_squad-validation-4121", "mrqa_squad-validation-4162", "mrqa_squad-validation-421", "mrqa_squad-validation-4228", "mrqa_squad-validation-4257", "mrqa_squad-validation-4259", "mrqa_squad-validation-4263", "mrqa_squad-validation-4327", "mrqa_squad-validation-434", "mrqa_squad-validation-4423", "mrqa_squad-validation-4545", "mrqa_squad-validation-457", "mrqa_squad-validation-4598", "mrqa_squad-validation-460", "mrqa_squad-validation-4609", "mrqa_squad-validation-4646", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4769", "mrqa_squad-validation-480", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-4929", "mrqa_squad-validation-5089", "mrqa_squad-validation-5200", "mrqa_squad-validation-5276", "mrqa_squad-validation-531", "mrqa_squad-validation-5344", "mrqa_squad-validation-5382", "mrqa_squad-validation-5483", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5542", "mrqa_squad-validation-556", "mrqa_squad-validation-5611", "mrqa_squad-validation-5655", "mrqa_squad-validation-5678", "mrqa_squad-validation-5878", "mrqa_squad-validation-5881", "mrqa_squad-validation-5895", "mrqa_squad-validation-5954", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6263", "mrqa_squad-validation-6351", "mrqa_squad-validation-6389", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-6441", "mrqa_squad-validation-6443", "mrqa_squad-validation-6468", "mrqa_squad-validation-6500", "mrqa_squad-validation-6500", "mrqa_squad-validation-6505", "mrqa_squad-validation-6523", "mrqa_squad-validation-6538", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6654", "mrqa_squad-validation-6657", "mrqa_squad-validation-6861", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-689", "mrqa_squad-validation-6951", "mrqa_squad-validation-6953", "mrqa_squad-validation-7001", "mrqa_squad-validation-7021", "mrqa_squad-validation-7035", "mrqa_squad-validation-7056", "mrqa_squad-validation-7062", "mrqa_squad-validation-7125", "mrqa_squad-validation-7135", "mrqa_squad-validation-7152", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7302", "mrqa_squad-validation-7302", "mrqa_squad-validation-7323", "mrqa_squad-validation-7391", "mrqa_squad-validation-7403", "mrqa_squad-validation-7450", "mrqa_squad-validation-7603", "mrqa_squad-validation-7692", "mrqa_squad-validation-7732", "mrqa_squad-validation-7744", "mrqa_squad-validation-7788", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7950", "mrqa_squad-validation-7957", "mrqa_squad-validation-7974", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8077", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8219", "mrqa_squad-validation-8225", "mrqa_squad-validation-8253", "mrqa_squad-validation-8322", "mrqa_squad-validation-8391", "mrqa_squad-validation-8391", "mrqa_squad-validation-8403", "mrqa_squad-validation-8447", "mrqa_squad-validation-8494", "mrqa_squad-validation-85", "mrqa_squad-validation-8523", "mrqa_squad-validation-8531", "mrqa_squad-validation-8553", "mrqa_squad-validation-8612", "mrqa_squad-validation-864", "mrqa_squad-validation-8668", "mrqa_squad-validation-8685", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-876", "mrqa_squad-validation-8836", "mrqa_squad-validation-8839", "mrqa_squad-validation-8891", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-9057", "mrqa_squad-validation-9061", "mrqa_squad-validation-9076", "mrqa_squad-validation-9101", "mrqa_squad-validation-9185", "mrqa_squad-validation-9191", "mrqa_squad-validation-9240", "mrqa_squad-validation-9268", "mrqa_squad-validation-9300", "mrqa_squad-validation-939", "mrqa_squad-validation-9401", "mrqa_squad-validation-9475", "mrqa_squad-validation-9506", "mrqa_squad-validation-9507", "mrqa_squad-validation-9546", "mrqa_squad-validation-959", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9652", "mrqa_squad-validation-9716", "mrqa_squad-validation-9744", "mrqa_squad-validation-9779", "mrqa_squad-validation-9890", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_squad-validation-9947", "mrqa_squad-validation-9956", "mrqa_squad-validation-9991", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1346", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2645", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-4403", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-7642"], "OKR": 0.880859375, "KG": 0.415625, "before_eval_results": {"predictions": ["Bento de Moura Portugal", "Mongolia", "polynomial-time", "All India Muslim League", "CBS Sports apps on tablets, Windows 10, Xbox One and other digital media players", "1967", "an antigen from a pathogen", "Encoded Archival description (EAD)", "Trey Parker and Matt Stone", "International Federation of Competitive eating", "Democratic Unionist Party", "local South Australian and Australian produced content", "\"Naked Killer\" (1992)", "7 June 1926 to 17 December 1926", "1900 Summer Olympics", "1937", "John Lee Hancock", "Hordaland", "Agent 99", "Bonnie Franklin", "Edmonton, Alberta", "Skyscraper", "Love Actually", "a creek", "Orange County, California", "URO VAMTAC", "The bald eagle", "32", "fifty-word", "Philadelphia Naval Shipyard", "The Books", "Rochdale, North West England", "2013\u201314 Premier League", "Clara Petacci", "Jamie Fraser", "Lionel Brockman Richie Jr.", "The Two Noble Kinsmen", "Eucritta melanolimnetes", "Jenji Kohan", "Johnson & Johnson", "PewDiePie", "Germanic", "2002", "Tamara Ecclestone Rutland", "Thomas Joseph \"T. J. Lavin", "Lancashire Combination side Stalybridge Celtic", "Adelaide Lightning", "Kohlberg K Travis Roberts", "\"My Love from the Star\"", "Tottenham ( ) or Spurs", "Sam Bettley", "Don DeLillo", "Nia Kay", "The Fixx", "The Crossing", "1919", "dynamite", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "a head injury", "The Humayun's Tomb", "teeth", "Profit maximization", "electron shells", "1901"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7100694444444444}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.0, 0.25, 0.8, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.7499999999999999, 0.4444444444444445, 1.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1764", "mrqa_squad-validation-525", "mrqa_hotpotqa-validation-698", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-5608", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-1994", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-5115", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-4819", "mrqa_naturalquestions-validation-8584", "mrqa_searchqa-validation-9565"], "SR": 0.59375, "CSR": 0.6034226190476191, "EFR": 1.0, "Overall": 0.7256845238095238}, {"timecode": 21, "before_eval_results": {"predictions": ["an Orthodox priest", "December 12", "the Hostmen", "Apollo 8", "Antigone", "a pyrenoid and thylakoids", "Sugarfoot", "\"Preacher\"", "Forbes, New South Wales", "Mitsubishi Motors Corporation", "Tabasco", "1 January 1788", "Lowestoft", "Christopher Nolan", "Taylor Swift", "Al Horford", "Johan Leysen", "1854", "American", "Pittsburgh Steelers", "professional footballer", "Logar", "Lauren Alaina", "Ian Fleming", "Mary Bonauto, Susan Murray, and Beth Robinson", "27 November 1956", "a split 7\"", "Hindi", "United States Auto Club", "The Clash of Triton", "Albany High School", "BAFTA TV Award Best Actor", "\"The Bob Edwards Show\"", "the heaviest album of all", "James Hill", "15,024", "Prime Minister of Denmark 1852\u20131853 as head of the Cabinet of Bluhme I (the \"January Cabinet\")", "the title character in Rodgers and Hammerstein\u2019s television version of \"Cind Cinderella\"", "\"Alberta\", a small-town girl who assumes the false identity of her former babysitter and current dominatrix", "2007", "3,000", "\"Chinese Coffee\"", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller", "Mineola", "KWPW", "John Richard Schlesinger, CBE", "the U.S. Representative for Oklahoma's 4 congressional district", "Blue", "Esperanza Emily Spalding", "the Kree", "Big & Rich", "1916", "American", "18", "The Golden Gate Bridge", "David Jason", "olea europaea", "Turkey", "Michael Jackson may soon return to the stage, at least for a \"special announcement.\"", "the Entourage", "the Tet Offensive", "Erica Rivera", "Spanish missionaries, ranchers and troops", "Madison's"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6613169510228334}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, false, false], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5714285714285715, 0.5, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.11764705882352942, 0.0, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1258", "mrqa_squad-validation-8651", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-3234", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-5216", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-2367", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-1961", "mrqa_newsqa-validation-444", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-6307"], "SR": 0.53125, "CSR": 0.6001420454545454, "EFR": 0.9333333333333333, "Overall": 0.7116950757575757}, {"timecode": 22, "before_eval_results": {"predictions": ["during the later decades of the 17th century", "WLQP-LP", "Sunni extremist groups", "mid-Eocene", "leftist/communist/nationalist insurgents/opposition", "enthusiasm", "the 34th President of the United States", "receive the benefits of the Morrill Acts of 1862 and 1890", "Prussian", "Violet", "German", "the British series of the same name", "Indianola", "the Vampire Intelligences", "Nassau County", "What Are Little Boys Made Of", "Andries Jonker", "President John F. Kennedy and First Lady Jacqueline Kennedy", "Mollie King", "1959", "129,007", "San Francisco 49ers", "Big Machine Records", "the Parthian Empire", "almost 3 million people", "Matt Groening", "June 10, 1982", "Philip K. Dick", "John Anthony \"Jack\" White", "Samuel Burl \"Sam\" Kinison", "Boston", "Lisa", "perjury and obstruction of justice", "Galleria Vittorio Emanuele II", "Paul Avery", "31 October 1783", "Puli Alam", "the Peninsular War in Spain during the Napoleonic Wars", "1838", "Margaret Thatcher", "Ashland is a town in Grafton County, New Hampshire, United States", "Manchester Victoria station in air rights space", "the east of Ireland", "Estadio de L\u00f3pez Cort\u00e1zar", "March 21, 2004", "Jesus", "Agent Carter", "Plies", "Tim \"Ripper\" Owens", "Walt Disney Productions", "chalk quarry", "Rhode Island", "electron donors", "President since creation of the office in 1789", "Nala", "Prince of Snowdon", "the River Thames", "The Sunday Post", "Citizens are picking members of the lower house of parliament, which will be tasked with drafting a new constitution after three decades of Mubarak's rule.", "Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "a six-year veteran of the museum's security staff", "the dugout canoe", "we/wee", "the firebrand Shi'ite cleric became a major"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7061990926421073}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.1818181818181818, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.23529411764705882, 0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.29629629629629634, 0.14285714285714288, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-968", "mrqa_squad-validation-3754", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-4981", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-1726", "mrqa_hotpotqa-validation-4954", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-4656", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-5555", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-1531", "mrqa_hotpotqa-validation-3736", "mrqa_naturalquestions-validation-4370", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-2945", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-6928"], "SR": 0.609375, "CSR": 0.6005434782608696, "EFR": 1.0, "Overall": 0.725108695652174}, {"timecode": 23, "before_eval_results": {"predictions": ["MBH99 reconstruction", "the Neckar", "static discs", "Pittsburgh Steelers", "14,000", "Scandinavia and northern Europe", "1911", "ESPN's \"SportsCenter\"", "Prince Antoni Radziwi\u0142\u0142", "northern Italy's Lombardy", "Caesars Palace Grand Prix", "Ford Field in Detroit, Michigan", "Wilton Mall at Saratoga", "the Sun", "Point of Entry", "Wilmette, Illinois", "Malayalam", "Pendlebury, Lancashire", "Leona Lewis", "Laura Dern", "Melbourne", "democracy and personal freedom", "Cool Runnings", "Indian", "Labour Party", "Thor", "Copa Airlines", "Ashridge Park", "Rudebox", "Washington", "Massachusetts", "Edward James Olmos", "ghaedheil", "Slaughterhouse-Five", "Nashville", "quarterly", "Telugu and Tamil", "\"Peshwa\"", "Joseph I", "Oracle Corporation", "1999", "William Shakespeare", "1898", "1953", "Bergen", "Scribner", "\"Apprendi v. New Jersey\"", "Oregon State Beavers", "Jack Elam", "1907", "The Design Inference", "R&B", "pilgrimages to Jerusalem", "art pottery", "a sequel to the song `` Because of You ''", "California Chrome", "Moby Dick", "Christine Keeler", "137", "group of diehard fans", "Sunday", "Jackie Robinson", "a string of continental defence radar", "a snake"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7414930555555556}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5, 0.4, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8702", "mrqa_hotpotqa-validation-2721", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-4089", "mrqa_hotpotqa-validation-994", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-354", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-2866", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-636", "mrqa_triviaqa-validation-3186", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-1059", "mrqa_searchqa-validation-6978"], "SR": 0.65625, "CSR": 0.6028645833333333, "EFR": 0.9545454545454546, "Overall": 0.7164820075757576}, {"timecode": 24, "before_eval_results": {"predictions": ["Los Angeles", "variously combustion chamber", "13th-century", "spin", "ACL", "Bury F.C.", "Las Vegas", "Suzuki YZF-R6", "Ahold N.V.", "east", "Gettysburg Address", "science", "Robert \"Bobby\" Germaine, Sr.", "American 3D computer-animated comedy", "the Asia-Pacific War", "Amy Poehler", "professional footballer", "British Labour", "USC Marshall School of Business", "Theme Aquarium", "1936", "Martin Scorsese", "Maxwell Smart", "The Walking Dead", "2008", "Yasir Hussain", "Let's Make Sure We Kiss Goodbye", "Ronald Ryan", "Elena Verdugo", "soccer", "Peel Holdings", "Chechen Republic", "alcoholic drinks", "Zaire", "Debbie Harry", "Barbara Lee Alexander", "novelist and poet", "1986", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "Hilary Duff", "100 million", "a Albanian political party", "2015 Masters Tournament", "John Schlesinger", "Venice", "Rockstar San Diego", "A.S. Roma", "Genderqueer", "Gothic Revival", "Melbourne's City Centre", "South West Peninsula League", "Bury St Edmunds", "1608", "metamorphic rock", "Rugrats in Paris : The Movie", "auk", "a distilled beverage (such as, gin, pisco, vodka, whiskey, tequila, or rum)", "Rose-Marie", "Sen. Barack Obama", "A Colorado prosecutor Friday asked a judge to dismiss the first-degree murder charge against Tim Masters,", "Spanish Davis Cup hero Fernando Verdasco,", "pink", "calcium", "Si-Tchun"], "metric_results": {"EM": 0.578125, "QA-F1": 0.699875992063492}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.8, 0.25, 1.0, 0.0, 0.8571428571428571, 0.8571428571428571, 1.0, 0.0, 0.0, 0.8, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2666666666666667, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3202", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-1741", "mrqa_hotpotqa-validation-4561", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-4573", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-1864", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-3934", "mrqa_naturalquestions-validation-6940", "mrqa_triviaqa-validation-6356", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-1361", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-14782"], "SR": 0.578125, "CSR": 0.6018749999999999, "EFR": 1.0, "Overall": 0.7253749999999999}, {"timecode": 25, "before_eval_results": {"predictions": ["August 15, 1971", "Geordie", "Peter Howell", "many elements", "formalize a unified front in trade and negotiations with various Indians", "French", "New York City", "2017", "Logan International Airport", "Pain Language", "Dzyha Vertov", "no. 3", "John John Florence", "Two Is Better Than One", "July 16, 1971", "Microsoft Office", "Baldwin is a hamlet and census-designated place (CDP) located in the town of Hempstead in Nassau County, New York, United States", "Elton John", "Firestorm", "the Ruul", "March 14, 2000", "David Wells", "Philip Pullman", "the Chengdu Aircraft Corporation (CAC) of China", "Michael Cremo", "Minnesota", "Oklahoma", "Jim Aaron Diamond", "Smithfield", "Julie Taymor", "29 September\u20132 October", "Columbia Records", "1943", "Maria Brink", "Scottish folk song", "Cody Miller", "Darkroom", "Tainted Love", "Christopher Nolan", "The Blue Album", "1996", "2016 United States elections", "bushwhackers", "Princes Park", "The Late Late Show", "2012", "1978", "Patsy Swayze", "John Morgan", "May 1801", "an organ", "Macau", "49 cents", "Certificate of Release or Discharge from Active Duty", "Jocelyn Flores", "Hyperbole", "Egypt", "tiger", "1292", "June 2002", "1991-1993", "Thomas Nast", "ice hockey", "Colombian telenovela"], "metric_results": {"EM": 0.625, "QA-F1": 0.6754407051282052}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.46153846153846156, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2081", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-5393", "mrqa_hotpotqa-validation-3386", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-1064", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1718", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-2092", "mrqa_triviaqa-validation-3925", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-1230", "mrqa_searchqa-validation-172"], "SR": 0.625, "CSR": 0.6027644230769231, "EFR": 0.9166666666666666, "Overall": 0.708886217948718}, {"timecode": 26, "before_eval_results": {"predictions": ["Italian physicist", "illiberal Islamic regimes", "The Judicial Council", "CBS", "bercs", "Blue Jean", "Ibex", "Hebride", "prostate", "Vitus Bering", "fuel", "Burundi", "larval", "baleen", "Der Zauberberg", "double-reeded zhaleika", "Canada", "Komodo", "a laparoscope", "Billy Bob Thornton", "a good natured Ice Cream Salesman", "Gulliver's Travels; Of Mice and Men", "Last Summer", "radio waves", "an anagram", "Isis", "Eliza Doolittle", "Tendinitis", "En banc", "Franklin D. Roosevelt", "six ounces", "Violeta Chamorro", "Take My Breath Away", "Rafael Nadal", "Bizkaia", "Ich bin ein Berliner", "Caesar", "Good fiction", "Neverbeen Kissed", "Antichrist", "\"Butcher\"", "Day-O", "Nanjing", "asparagus beetles", "\"auf Wiedersehen\"", "blubber", "catalysts", "the L-M version", "Germany", "Deep Purple", "Most important religious leader who performed miracles.", "Grandma Klump", "to solve its problem of lack of food self - sufficiency", "Speaker of the House of Representatives, President pro tempore of the Senate", "the Bulgarian 2nd Army", "20 numbered,", "Time Bandits", "a native species or because they're regarded as an agricultural pest", "Tracy Turnblad", "35", "three centuries", "Consumer Product Safety Commission", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "the United Nations"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4971354166666666}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.25, 0.8, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.16666666666666666, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15646", "mrqa_searchqa-validation-10928", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-2775", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-10174", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-2659", "mrqa_searchqa-validation-9629", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-14776", "mrqa_searchqa-validation-4299", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-6143", "mrqa_searchqa-validation-7883", "mrqa_searchqa-validation-13623", "mrqa_searchqa-validation-10252", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-4190", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-7196", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-2819", "mrqa_triviaqa-validation-1217", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-405", "mrqa_newsqa-validation-1748"], "SR": 0.390625, "CSR": 0.5949074074074074, "EFR": 1.0, "Overall": 0.7239814814814814}, {"timecode": 27, "before_eval_results": {"predictions": ["May 18, 1756", "canceled", "modern fashion", "more than half of the global wealth", "Lismore", "STS-51-L", "Paradise, Nevada", "German", "Newcastle upon Tyne", "Colonel", "Cody Miller", "Virginia", "The Hertz Corporation", "Wiz Khalifa", "Disney California Adventure", "Maria Brink", "The Sound of Music", "G\u00f6tene", "Argentine", "novelty songs, comedy, and strange or unusual recordings", "Perfect 10", "6teen", "South America", "Princes Park", "Glenn Close", "the Knight Company", "My Gorgeous Life", "Ashanti", "the Dutch Empire", "Culiac\u00e1n, Sinaloa", "Northampton, England", "Black Panthers", "Fred Willard", "beer", "Dara Torres", "nine", "1909", "the E22", "3,384,569", "an anvil", "House of Hohenstaufen", "James G. Kiernan", "Johnnie Ray", "Forrest Gump", "2009", "1919", "a skerry", "1620 to 1691", "Charles Edward Stuart", "Mickey Gilley", "Blue Origin", "December 31, 2015", "2017", "Eleanor Roosevelt", "Utah, Arizona, Wyoming, and Oroville, California", "Google", "the earth", "the Kinks", "a lesser charge of threatening behavior.", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997", "1975", "The sperm whale", "libraries", "NASA"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7368303571428572}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7559", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-4942", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-878", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-1233", "mrqa_hotpotqa-validation-296", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-5185", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-2926", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4650", "mrqa_naturalquestions-validation-8484", "mrqa_triviaqa-validation-2829", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-2586"], "SR": 0.609375, "CSR": 0.5954241071428572, "EFR": 0.96, "Overall": 0.7160848214285714}, {"timecode": 28, "before_eval_results": {"predictions": ["British patrons", "Laverne & Shirley", "South", "Roger Bacon", "protozoa", "11307 Universities", "Pitney Bowes", "apochauns", "12:24", "September 20, 1934", "Quebec City", "Edith Piaf", "the old Krntnertor Theatre", "Sappho", "Saatchi 17", "Colorado River", "Hershey's", "Timothy Leary", "3800", "FBI", "John Grisham", "cheeks", "Alice Addertongue", "Bridges of Madison County", "a 1.5 km swim", "calcium", "Mikel Arteta", "Wisconsin", "Raphael", "To Build a Fire", "auk", "The Opera Guild's annual... football tournament", "transept", "centigrade", "silver", "British Broadcasting Corporation", "Jackass penguin", "Jack Lewis", "Blackwater USA", "apogee", "Nicky Hilton", "December", "Cleopatra", "Hadrosaurus foulkii", "E-T", "Contra Costa County", "C&D Canal", "asthma", "Mihama Ya", "a trumpet", "Narcissus", "Marion", "12,000 metres per second", "its population", "Sarah Silverman", "Anwar Sadat", "Great and Most Fortunate Navy", "Barings", "Esp\u00edrito Santo Financial Group", "Earvin \"Magic\" Johnson Jr.", "John DeMita", "a free laundry service", "the first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship.\"", "north coast of Puerto Rico"], "metric_results": {"EM": 0.390625, "QA-F1": 0.44749999999999995}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.23999999999999996, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12695", "mrqa_searchqa-validation-2360", "mrqa_searchqa-validation-11819", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-2190", "mrqa_searchqa-validation-7239", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-5756", "mrqa_searchqa-validation-851", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-8603", "mrqa_searchqa-validation-4398", "mrqa_searchqa-validation-4388", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-6355", "mrqa_searchqa-validation-9666", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-3781", "mrqa_searchqa-validation-11293", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-12173", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-841", "mrqa_searchqa-validation-4866", "mrqa_searchqa-validation-6787", "mrqa_searchqa-validation-7159", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-3848", "mrqa_triviaqa-validation-4466", "mrqa_hotpotqa-validation-4972", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3961"], "SR": 0.390625, "CSR": 0.5883620689655172, "EFR": 0.9487179487179487, "Overall": 0.7124160035366931}, {"timecode": 29, "before_eval_results": {"predictions": ["X Games", "11 million", "GTE", "secondary school", "John Lee Hancock", "2007", "Westfield Tea Tree Plaza", "Philadelphia", "237 square miles", "Wonder Woman", "1860", "Eddie Izzard", "1966 US tour", "Miracle", "James Ellison", "studied Arabic grammar", "Humberside Airport", "\"Charmed\"", "2012", "Kind Hearts and Coronets", "Centennial Olympic Stadium", "Leatherheads", "Polka", "Sinngedichte", "James G. Kiernan", "Australia", "heavy metal band", "Anah\u00ed", "private", "the Tampa Bay Lightning", "tabasco peppers", "Patricia Arquette", "ABC", "William Shand Kydd", "coca wine", "Square Enix", "Geraldine Page", "pornographicstar", "Europe", "179", "three", "Sam the Sham", "pinball", "Genesee Brewing Company", "Las Vegas", "Pittsburgh", "Charles", "J35", "Engirundho Vandhaal", "Romance", "Bohemia", "Macomb County", "birth", "biochemistry", "provides the public with financial information about a nonprofit organization", "Elgar's Enigma Variations", "Stockholm syndrome", "magnetism", "Morgan Tsvangirai.", "Empire of the Sun", "Amanda Knox's aunt", "Robert Bruce", "Donna Reed", "gulls"], "metric_results": {"EM": 0.5, "QA-F1": 0.5895833333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 0.4, 0.3333333333333333, 1.0, 0.5, 0.0, 1.0, 1.0, 0.4, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4094", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-190", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5637", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-1455", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-2004", "mrqa_naturalquestions-validation-9088", "mrqa_triviaqa-validation-5397", "mrqa_triviaqa-validation-3393", "mrqa_newsqa-validation-3391", "mrqa_searchqa-validation-5455", "mrqa_searchqa-validation-9860"], "SR": 0.5, "CSR": 0.5854166666666667, "EFR": 1.0, "Overall": 0.7220833333333333}, {"timecode": 30, "UKR": 0.71484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-1055", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1180", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1342", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1488", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1562", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-172", "mrqa_hotpotqa-validation-1726", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1808", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1896", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1994", "mrqa_hotpotqa-validation-2004", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2199", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-240", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2721", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2806", "mrqa_hotpotqa-validation-2825", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3013", "mrqa_hotpotqa-validation-3071", "mrqa_hotpotqa-validation-3074", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3191", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3402", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3575", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4285", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4467", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4640", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4830", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4954", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5185", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5247", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-5393", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-5471", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-5561", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-5713", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5756", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-704", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-98", "mrqa_hotpotqa-validation-992", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1199", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1503", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1869", "mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-392", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-894", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-11434", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-12625", "mrqa_searchqa-validation-12695", "mrqa_searchqa-validation-13018", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-13831", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14240", "mrqa_searchqa-validation-14710", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-16", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-1680", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-2360", "mrqa_searchqa-validation-2489", "mrqa_searchqa-validation-2621", "mrqa_searchqa-validation-2686", "mrqa_searchqa-validation-3516", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4596", "mrqa_searchqa-validation-4866", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6045", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-7051", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-8245", "mrqa_searchqa-validation-851", "mrqa_searchqa-validation-9130", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9601", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10031", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10223", "mrqa_squad-validation-1025", "mrqa_squad-validation-10305", "mrqa_squad-validation-10424", "mrqa_squad-validation-10449", "mrqa_squad-validation-1045", "mrqa_squad-validation-10496", "mrqa_squad-validation-1064", "mrqa_squad-validation-1096", "mrqa_squad-validation-1114", "mrqa_squad-validation-1177", "mrqa_squad-validation-1232", "mrqa_squad-validation-1255", "mrqa_squad-validation-1258", "mrqa_squad-validation-1296", "mrqa_squad-validation-1480", "mrqa_squad-validation-1565", "mrqa_squad-validation-1568", "mrqa_squad-validation-1723", "mrqa_squad-validation-1940", "mrqa_squad-validation-1976", "mrqa_squad-validation-2153", "mrqa_squad-validation-2272", "mrqa_squad-validation-2301", "mrqa_squad-validation-234", "mrqa_squad-validation-2474", "mrqa_squad-validation-2475", "mrqa_squad-validation-2591", "mrqa_squad-validation-287", "mrqa_squad-validation-2976", "mrqa_squad-validation-306", "mrqa_squad-validation-3139", "mrqa_squad-validation-3155", "mrqa_squad-validation-3296", "mrqa_squad-validation-3532", "mrqa_squad-validation-3534", "mrqa_squad-validation-361", "mrqa_squad-validation-363", "mrqa_squad-validation-3716", "mrqa_squad-validation-3767", "mrqa_squad-validation-3810", "mrqa_squad-validation-3813", "mrqa_squad-validation-4026", "mrqa_squad-validation-4070", "mrqa_squad-validation-410", "mrqa_squad-validation-4121", "mrqa_squad-validation-4162", "mrqa_squad-validation-421", "mrqa_squad-validation-4228", "mrqa_squad-validation-4259", "mrqa_squad-validation-4286", "mrqa_squad-validation-4423", "mrqa_squad-validation-451", "mrqa_squad-validation-4545", "mrqa_squad-validation-4598", "mrqa_squad-validation-460", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4769", "mrqa_squad-validation-480", "mrqa_squad-validation-4810", "mrqa_squad-validation-4898", "mrqa_squad-validation-4929", "mrqa_squad-validation-5089", "mrqa_squad-validation-5276", "mrqa_squad-validation-5344", "mrqa_squad-validation-5382", "mrqa_squad-validation-5483", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5532", "mrqa_squad-validation-556", "mrqa_squad-validation-5578", "mrqa_squad-validation-5611", "mrqa_squad-validation-5779", "mrqa_squad-validation-5839", "mrqa_squad-validation-5881", "mrqa_squad-validation-5954", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6252", "mrqa_squad-validation-6351", "mrqa_squad-validation-6389", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-6443", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6888", "mrqa_squad-validation-6967", "mrqa_squad-validation-7021", "mrqa_squad-validation-7035", "mrqa_squad-validation-7056", "mrqa_squad-validation-7125", "mrqa_squad-validation-7152", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7323", "mrqa_squad-validation-7403", "mrqa_squad-validation-7458", "mrqa_squad-validation-7603", "mrqa_squad-validation-7744", "mrqa_squad-validation-7788", "mrqa_squad-validation-782", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7950", "mrqa_squad-validation-7957", "mrqa_squad-validation-7974", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8194", "mrqa_squad-validation-8198", "mrqa_squad-validation-8219", "mrqa_squad-validation-8253", "mrqa_squad-validation-8322", "mrqa_squad-validation-8391", "mrqa_squad-validation-8403", "mrqa_squad-validation-8494", "mrqa_squad-validation-85", "mrqa_squad-validation-8531", "mrqa_squad-validation-8553", "mrqa_squad-validation-864", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8839", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-9061", "mrqa_squad-validation-9191", "mrqa_squad-validation-9300", "mrqa_squad-validation-9314", "mrqa_squad-validation-939", "mrqa_squad-validation-9401", "mrqa_squad-validation-9475", "mrqa_squad-validation-9506", "mrqa_squad-validation-9507", "mrqa_squad-validation-9521", "mrqa_squad-validation-959", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9647", "mrqa_squad-validation-9652", "mrqa_squad-validation-9744", "mrqa_squad-validation-9779", "mrqa_squad-validation-9890", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1346", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-1604", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2654", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-7266", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-830"], "OKR": 0.853515625, "KG": 0.4578125, "before_eval_results": {"predictions": ["the Scottish Government", "quadratic", "1879", "Xiu Li Dai and Yongge Dai", "St. Theodosius Russian Orthodox Cathedral", "northwest Washington", "1924", "England", "Bud Light", "the status line", "Virginia farmers", "December 2, 2013, and the third season concluded on October 1, 2017", "into the intermembrane space", "Chinese cooking", "Philadelphia", "the U.S. Senate", "electrical activity produced by skeletal muscles", "thick skin", "an Islamic shrine", "Sylvester Stallone", "Anakin Skywalker", "September 27, 2017", "convert single - stranded genomic RNA into double - stranded cDNA", "the economy", "Victory gardens", "Paul Hogan", "961", "northern China", "gathering money from the public, which circumvents traditional avenues of investment", "a beach in Malibu, California", "Humpty Alexander Dumpty", "homicidal thoughts of a troubled youth", "a part of the continent of North America, Greenland has been politically and culturally associated with Europe ( specifically Norway and Denmark, the colonial powers, as well as the nearby island of Iceland ) for more than a millennium", "Sun Tzu", "18th century in the United Kingdom", "Setsuko Thurlow", "the temporal lobes", "the fictional town of Ramelle", "interactions between DNA and other molecules that mediate the function of the genome", "mining", "Keith Thibodeaux", "six - hoop game", "West Africa", "butane", "Julia Roberts", "12 November 2010", "Brazil, China, France, Germany, India, Indonesia, Italy", "Aaron Harrison", "Panning", "CBS Television City", "Mike Higham", "James Chadwick", "the Swirral Edge ridge", "Nutrient enrichment", "Worcester Cathedral", "Germany", "Rachel, Nevada", "Atlanta, Georgia", "more than 30 Latin American and Caribbean nations", "two women killed in a stampede at one of his events in Angola on Saturday,", "police dogs", "a Observatory", "Frank Borzage", "Hannibal of Carthage"], "metric_results": {"EM": 0.40625, "QA-F1": 0.550564140373923}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, true, false, false], "QA-F1": [0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4864864864864865, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8695652173913044, 0.6666666666666666, 1.0, 1.0, 0.30769230769230765, 1.0, 0.4166666666666667, 0.0, 1.0, 0.7272727272727273, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.25, 1.0, 1.0, 0.5454545454545454, 0.13333333333333333, 0.6666666666666666, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9540", "mrqa_squad-validation-1818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-6224", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-5651", "mrqa_naturalquestions-validation-2498", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-8072", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-1988", "mrqa_naturalquestions-validation-9773", "mrqa_triviaqa-validation-6465", "mrqa_triviaqa-validation-173", "mrqa_triviaqa-validation-782", "mrqa_hotpotqa-validation-4986", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-414", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-4098"], "SR": 0.40625, "CSR": 0.5796370967741935, "EFR": 1.0, "Overall": 0.7211617943548387}, {"timecode": 31, "before_eval_results": {"predictions": ["lighter and seem to lose something in the process", "In the genus Beroe the juveniles, like the adults, lack tentacles and tentacle sheaths", "September 19 - 22, 2017", "social commentary", "John Roberts", "in contemporary Earth, where the sudden appearance of a worldwide storm causes 98 % of the world's population to disappear, and zombie - like creatures rise to attack the remainder", "between the Eastern Ghats and the Bay of Bengal", "a bow bridge with 16 arches shielded by ice guards", "12 to 36 months old", "Tom Brady", "Chelsea", "Darlene Cates", "fascia surrounding skeletal muscle", "Jerry Leiber", "in a Norwegian town circa 1879", "Lisa Stelly", "Heroes and Villains", "a contract between two parties, where the terms and conditions of the contract are set by one of the parties, and the other party has little or no ability to negotiate more favorable terms and is thus placed in a `` take it or", "Robin", "Dan Enright", "Missouri River", "new character Randy", "August 18, 1945", "19 June 2018", "Daniel A. Dailey", "to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "King James Bible of the biblical phrase in saecula saeculorum in Ephesians 3 : 21", "many hospitals, nursing homes, home health agencies, hospice providers, health maintenance organizations ( HMOs ), and other health care institutions to provide information about advance health care directives to adult patients upon their admission to the healthcare facility", "international educational foundation headquartered in Geneva, Switzerland and founded in 1968", "October 28, 2007", "Jaydev Shah", "in ancient Mesopotamia", "Action Jackson", "in a thousand years", "a state or other organizational body that controls the factors of production", "the north pole", "in Ephesus in AD 95 -- 110", "1984", "sport utility vehicles", "Americans", "1916", "2015 American epic space opera film produced, co-written and directed by J.J. Abrams", "Jack Lord", "an American musical group founded by Marcus Bowens and Jermaine Fuller, with the later addition of J.J. O' Neal and Dougy Williams", "Rashidun Caliphs", "off the rez", "Woody Paige", "Ren\u00e9 Descartes", "2007 via Valve's Steam content distribution platform", "Asuka", "A Turtle's Tale : Sammy's Adventures", "three", "Steve Biko", "Rudolph", "janggi", "Anne Fletcher", "October 21, 2016", "Centers for Medicare & Medicaid Services (CMS)", "14", "Robert Barnett, a prominent Washington attorney,", "Oaxaca, Mexico", "Algeria", "an egg in a bottle", "pipa"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6096780198826275}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, false, false], "QA-F1": [0.8421052631578948, 0.0, 1.0, 0.3076923076923077, 1.0, 0.0625, 1.0, 0.3636363636363636, 0.5714285714285715, 0.0, 0.5, 1.0, 0.888888888888889, 0.5714285714285715, 0.5714285714285715, 1.0, 1.0, 0.961038961038961, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.8125000000000001, 1.0, 0.9859154929577464, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8333333333333334, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3494", "mrqa_squad-validation-4566", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-802", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-714", "mrqa_triviaqa-validation-2748", "mrqa_hotpotqa-validation-1803", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-3785", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-4046"], "SR": 0.4375, "CSR": 0.5751953125, "EFR": 0.9166666666666666, "Overall": 0.7036067708333333}, {"timecode": 32, "before_eval_results": {"predictions": ["September 1565", "Anglo-Saxon populations who migrated to and conquered much of England after the end of Roman Imperial rule", "Pin the Tail on the Donkey", "the martini", "the Wiener Sangerknaben", "cinnamon", "a big bang", "Halloween", "R.E.M.", "Gale Eugene Sayers", "French Presidential Power and the Stability of the French Fifth Republic", "Georgia", "Abraham Lincoln", "the Yangtze River", "school", "Angelina Jolie", "Sharon Epatha Merkerson", "air pressure", "Harold Macmillan", "dUG UP", "shark", "the Deaf President Now protest", "school", "school", "the orangutan", "anaphylactic shock", "camels", "gangrene", "ex", "Bonnie Elizabeth Parker", "John Harvard", "the Italic branch", "``The Partridge Family", "Henry", "Guatemala", "\"JK\"", "Barack Obama", "Jos Joaqun de Olmedo", "Albert Einstein", "go over to Monument Valley", "school at Barnsdall Art Park", "Louisa May Alcott", "Tulip", "rely", "Providence", "Tasmanian devil John Quincy", "Mother Vineyard", "South Africa", "Howard Athenaeum", "dry ice", "`` 1947 Birthday - Avril \" Kim\" Campbell", "tooth", "Gibraltar", "1999", "9 February 2018", "Argentina", "Sarah Sawyer", "Aviva plc", "Russian Empire", "1967", "44,300", "228", "Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "``Buying a Prius shows the world that you love the environment and hate using fuel,\""], "metric_results": {"EM": 0.5, "QA-F1": 0.6009424603174602}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false], "QA-F1": [0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3088", "mrqa_squad-validation-5276", "mrqa_searchqa-validation-10043", "mrqa_searchqa-validation-5405", "mrqa_searchqa-validation-15328", "mrqa_searchqa-validation-3698", "mrqa_searchqa-validation-16138", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-5348", "mrqa_searchqa-validation-12676", "mrqa_searchqa-validation-1108", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-12978", "mrqa_searchqa-validation-10759", "mrqa_searchqa-validation-9970", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-7460", "mrqa_searchqa-validation-4645", "mrqa_searchqa-validation-8650", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-2559", "mrqa_searchqa-validation-10513", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-554", "mrqa_triviaqa-validation-4432", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-2395"], "SR": 0.5, "CSR": 0.5729166666666667, "EFR": 0.96875, "Overall": 0.7135677083333334}, {"timecode": 33, "before_eval_results": {"predictions": ["11", "R\u00fcdesheim am Rhein", "Dirty Diana", "Tennessee Williams", "the Ring magazine", "a pancake", "John Henry", "Zombies", "Colombia", "Judges", "Friday Night Lights", "Halloween", "the Mummy", "a port-wine stain", "a Skyscraper", "Pinta", "Czechoslovakia", "\"Godbuster\"", "Mike Judge", "Unforgiven", "Court TV", "the Galaxy", "Germany", "Gunsmoke", "an astronomer", "Candy", "AT&T", "asthma", "Microsoft", "tequila", "Puerto Rico", "Singapore Airlines Flight 21", "a flying saucer", "Shakespeare", "a liter", "Edward", "The Silence of the Lambs", "Prymaat Conehead", "stuffing", "3", "carbonite", "Spain", "the phi phenomenon", "obelisk", "Sam Kinison", "Katharine Hepburn", "(Kansas City)", "Kublai Khan", "Georgia", "Edison", "a bow", "Newfoundland", "538", "a narcissistic ex-lover", "Lee Baldwin", "a googol", "Laos", "the UK", "1995", "Champion Jockey", "Boeing 757", "the Bush administration", "200", "around 8 p.m. local time"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5975378787878788}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, false, true, false, true, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.9090909090909091]}}, "before_error_ids": ["mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-7868", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-6554", "mrqa_searchqa-validation-5519", "mrqa_searchqa-validation-10095", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-9672", "mrqa_searchqa-validation-2404", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-7035", "mrqa_searchqa-validation-533", "mrqa_searchqa-validation-227", "mrqa_searchqa-validation-10490", "mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-10902", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11808", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-5234", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6140", "mrqa_triviaqa-validation-6808", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-3445", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-591"], "SR": 0.546875, "CSR": 0.5721507352941176, "EFR": 1.0, "Overall": 0.7196645220588236}, {"timecode": 34, "before_eval_results": {"predictions": ["Hisao Yamada", "Percy Shelley", "3D computer-animated comedy", "aluminum foil", "Montreal", "Lego", "Daniil Shafran", "Doc Hollywood", "Richard L. Thompson", "the Virgin label", "James Edward Kelly", "26 June 2013", "Freddie Jackson", "Michael Swango", "Roman Polanski", "322,520", "1754", "Cate Blanchett", "Westfield Marion", "Montana State University", "1961", "Eisenhower Executive Office Building", "137th", "Mohsin Fani", "High Falls Brewery", "2016", "1998", "Sean", "2015", "Mel Blanc", "Corendon Airlines", "Tamil", "number five", "Champion Jockey", "University of Columbia", "Jennifer Aniston", "Larry Eustachy", "Anne Perry", "March 17, 2015", "Julie Taymor", "Mika H\u00e4kkinen", "nine", "Bass", "Buck Owens", "January 1788", "Lord Chancellor of England", "MGM Resorts International", "Cleveland, Ohio", "The song also features rap parts from Darryl, RB Djan and Ryan Babel", "Nellee Hooper", "Syracuse University", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\"", "1969", "georgia", "the referee", "Botticelli", "the American Circus Parade", "Keats", "Dr. Maria Siemionow,", "it has not intercepted any Haitians attempting illegal crossings", "Arsene Wenger", "atoms", "Bellerophon", "Monica Lewinsky"], "metric_results": {"EM": 0.734375, "QA-F1": 0.79375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5530", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-421", "mrqa_hotpotqa-validation-810", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-2989", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-4302", "mrqa_triviaqa-validation-2977", "mrqa_newsqa-validation-1675", "mrqa_searchqa-validation-7800"], "SR": 0.734375, "CSR": 0.5767857142857142, "EFR": 0.9411764705882353, "Overall": 0.70882681197479}, {"timecode": 35, "before_eval_results": {"predictions": ["June 4, 2014", "highly diversified", "Walter Pauk", "2018", "a noble gas", "International Campaign to Abolish Nuclear Weapons ( ICAN )", "Ross Elliott", "Ishaan Anirudh Sinha", "the Charlotte Hornets of the National Basketball Association", "one person", "Orographic lift", "approximately in half", "6 March 1983", "An alternative is to cool all the atmosphere by spraying the whole atmosphere as if drawing letters in the air", "Around 1200", "April 21, 2015", "1854", "Mount Mannen in Norway", "1937", "the Kennedy Space Center ( KSC ) in Florida", "Rocky Dzidzornu -- congas", "employment in which a person works a minimum number of hours defined as such by his / her employer", "Monastic orders, especially the Cistercians and the Carthusians", "since the early 20th century", "Authority ( derived from the Latin word auctoritas )", "a yellow background instead of a white one", "Magnetically soft ( low coercivity ) iron", "southern Anatolia", "1992", "ulcerative colitis", "September 1995", "Turducken", "the brain and spinal cord", "abdicated in November 1918", "Massachusetts", "Hans Zimmer, Steve Mazzaro & Missi Hale", "c. 1000 AD", "through 13 states", "Jerry Leiber and Mike Stoller", "111", "49 cents", "December 1, 1969", "economic and cultural region in Germany", "1978", "Javier Fern\u00e1ndez", "the motion of the continents", "art", "the five permanent states", "peninsular mainland", "Santo Domingo", "season two", "28, 1973", "the Somme", "Frederick William III of Prussia", "Majorca", "National Football League", "Arlo Looking Cloud", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "Pervez Musharraf", "to spend billions to improve America's education, infrastructure, energy and health care systems", "wigan Athletic", "Ireland", "asteroids", "Colorado"], "metric_results": {"EM": 0.421875, "QA-F1": 0.569836913360762}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.5, 1.0, 0.8, 0.09090909090909091, 0.0, 0.8571428571428571, 0.2857142857142857, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4210526315789474, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.6666666666666666, 0.0, 0.0, 0.0, 0.1111111111111111, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.22222222222222224, 0.0, 0.5, 1.0, 1.0, 0.0, 0.375, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3469", "mrqa_naturalquestions-validation-9165", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-6300", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-7342", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-8625", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-2556", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-6276", "mrqa_hotpotqa-validation-3613", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-1970"], "SR": 0.421875, "CSR": 0.5724826388888888, "EFR": 1.0, "Overall": 0.7197309027777778}, {"timecode": 36, "before_eval_results": {"predictions": ["a crust and lithosphere", "Kanun\u00ee Sultan S\u00fcleyman", "Skatoony", "number 1", "Satchmo, Satch or Pops", "San Antonio", "Polish-Jewish", "Danish", "Milwaukee Bucks", "1994", "\"Glee\"", "1965", "over 100 million", "Oneida Limited", "New Hanover County in coastal southeastern North Carolina, United States", "Pieter van Musschenbroek", "Southbank", "London", "Australian", "Rochdale, North West England", "Bardot", "Mario Lemieux", "To SquarePants or Not to SquarePant", "The Sun", "Sydney, New South Wales", "Crist\u00f3v\u00e3o de Magallanes", "King of France", "1694", "Michael Fassbender", "Nanna Popham Britton", "Minette Walters", "leopard", "Moselle", "Anne and Georges", "Republic Plaza", "American playwright and Nobel laureate in Literature", "Cheshire", "Robert Gibson", "1770", "1974", "Columbia River Sub", "Woody Woodpecker", "2", "Edward Albert Heimberger", "IFFHS World's Best Goalkeeper", "three", "1989 until 1994", "Pittsburgh Steelers", "9 venues", "1993 to 2001", "Double Crossed", "United States economy first went into an economic recession", "needle - like teeth", "Bacon", "greece", "constantine i = 1", "1812", "energy propels the boat that travels between 5 and 10 knots an hour", "the Casalesi Camorra clan", "mind-blowing structures", "Queen Wilhelmina", "greece", "Santa Fe", "1992"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6354252518315018}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, false, false, false, true, true], "QA-F1": [0.25, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4952", "mrqa_hotpotqa-validation-1312", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-3342", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-3139", "mrqa_hotpotqa-validation-3417", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-2319", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-5119", "mrqa_triviaqa-validation-2468", "mrqa_triviaqa-validation-994", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-743", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-6519"], "SR": 0.53125, "CSR": 0.5713682432432432, "EFR": 0.9666666666666667, "Overall": 0.712841356981982}, {"timecode": 37, "before_eval_results": {"predictions": ["lasting damage", "Favre", "Bob Turner", "Wool Sack dress", "Billy the Kid", "Oliver Twist", "Hans Christian Andersen", "Topaz", "hydrothermal", "Cameroon English", "x", "x", "Bishop of Rome", "California", "Ocean's Eleven", "einherjar", "Little Women", "\"Ich bin ein Berliner\"", "x", "the Japanese navy", "difference", "Gogol", "Malcolm X", "blue", "Phonetics", "Michigan", "Sigmund Freud", "red", "T. S. Eliot", "Dumpling", "Alexander Hamilton", "New Zealand", "rum", "Theology of God", "x", "Stephen Decatur", "x", "Paraguay", "R2-D2", "3", "tense", "Vassar College", "forensic medicine", "National Air and Space Museum", "Vespa", "Warren G. Harding", "Emma Peel", "Tennessee", "Richard I", "Will Rogers", "Bee Gees", "Honor\u00e9 Mirabeau", "2018", "Hanna Alstr\u00f6m", "National Secretary of Union Employees", "the first web browser", "Scotland", "1898", "1887", "Bonkyll Castle", "more and more suspicious of the way their business books were being handled.", "\"Top Gun\"", "Dame Elizabeth,", "Chester Leland Brewer"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5534722222222221}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, true, false, false, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-2897", "mrqa_searchqa-validation-16031", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-7936", "mrqa_searchqa-validation-7058", "mrqa_searchqa-validation-2666", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-10534", "mrqa_searchqa-validation-7080", "mrqa_searchqa-validation-3714", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-11481", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-8720", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-15083", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-10345", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-13049", "mrqa_searchqa-validation-8295", "mrqa_searchqa-validation-8604", "mrqa_searchqa-validation-10821", "mrqa_searchqa-validation-2859", "mrqa_searchqa-validation-14390", "mrqa_searchqa-validation-8890", "mrqa_naturalquestions-validation-1455", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-1283", "mrqa_hotpotqa-validation-1884", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-2552", "mrqa_hotpotqa-validation-4203"], "SR": 0.453125, "CSR": 0.5682565789473684, "EFR": 1.0, "Overall": 0.7188856907894736}, {"timecode": 38, "before_eval_results": {"predictions": ["the butcher Market", "Mardi Gras", "henin", "Soundgarden", "a pew", "Russia", "the Penguin", "cardiac glycosides", "Canada", "sopra", "Pole vault", "California", "Jordan", "an offset printing", "the Battle of Waterloo", "Ukraine", "goombah", "Paris", "David Geffen", "American Express Corporation", "Joan of Arc", "John Tyler", "Simile", "La-Z-Boy", "water", "a phylum", "Narnia", "East Germany", "Ginger Rogers", "the Book of the Wars of Jehovah", "Vlad Tepes", "Marlee Matlin", "a female", "Qatar", "debts", "Jane", "yellow fever", "Days Inn", "Guatemala", "Harold Edward \"Red\" Grange", "Peter", "an printing press", "Bambara", "1917", "Colonel (Tom) Parker", "the lilac", "American Pie", "a diamond", "a bowhead", "Ohio State", "Sweet Home Alabama", "John Prine and Roger Cook", "Toledo", "1960", "\"The Nutcracker\"", "Dodoma", "the Red Lion", "martial arts action films", "Black Swan", "the Sun", "girls", "Vernon Forrest", "President Barack Obama", "Lewiston"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6380208333333333}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-582", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-6090", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-14171", "mrqa_searchqa-validation-12204", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-4408", "mrqa_searchqa-validation-3768", "mrqa_searchqa-validation-5033", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-7368", "mrqa_searchqa-validation-10244", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-6757", "mrqa_searchqa-validation-3197", "mrqa_naturalquestions-validation-3087", "mrqa_triviaqa-validation-6420", "mrqa_hotpotqa-validation-1192", "mrqa_newsqa-validation-1371", "mrqa_triviaqa-validation-700"], "SR": 0.578125, "CSR": 0.5685096153846154, "EFR": 1.0, "Overall": 0.7189362980769232}, {"timecode": 39, "before_eval_results": {"predictions": ["substantially increased the asking price", "preserved corpses having sex", "\"To My Mother\"", "Reggae star Lucky Dube", "grocery store", "fallen comrades lost in the heat of battle.", "Samuel Herr, 26, and Juri Kibuishi", "around 1918-1919.", "participate in Iraq's government.", "political dead-end", "Honduras", "Hearst Castle.", "FBI", "201-262-2800", "concern that taking the product off the market would result in hardship for terminally ill patients and their caregivers", "were directly involved in an Internet broadband deal with a Chinese firm.", "Iraq", "40 militants and six Pakistan soldiers", "1973", "Argentina", "Washington", "his injuries,", "Laura Ling and Euna Lee", "January 24, 2006.", "\"Why are our kids so messing up?\"", "Haleigh", "Casalesi Camorra", "four wickets.", "an annual road trip,", "11 countries", "two", "going out of business for one reason or another,", "Kurt Cobain", "genocide", "Hartsfield-Jackson International Airport", "barter -- trading goods and services without exchanging money", "\"Dance Your Ass Off.\"", "1994", "Larry Zeiger", "the United States", "Illness", "\"He's crying like a baby,\"", "(Charles Reisner, 1928) Stuntman", "African National Congress", "abuse", "blew himself up.", "two", "girls who go to school and women who dare to teach them", "14", "11", "Royal Navy servicemen", "Clarence Darrow", "sugar", "to discover first principles --'those universal principles which are the condition of the possibility of the existence of anything and everything", "1768", "chariots", "(St.) George Orwell", "\"Rated R\"", "1955", "\"I'm Shipping Up to Boston\"", "a lock", "(Portrait of the Artist\\'s Mother)", "a war bond", "email"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5423943974519632}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, true, false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.2222222222222222, 0.6666666666666666, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.9714285714285714, 0.9473684210526316, 0.0, 1.0, 0.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.08333333333333333, 1.0, 1.0, 0.8, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-3773", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-2946", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-1259", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-4185", "mrqa_naturalquestions-validation-4211", "mrqa_triviaqa-validation-7637", "mrqa_hotpotqa-validation-1585", "mrqa_searchqa-validation-16932", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-11411", "mrqa_triviaqa-validation-90"], "SR": 0.421875, "CSR": 0.56484375, "EFR": 0.972972972972973, "Overall": 0.7127977195945946}, {"timecode": 40, "UKR": 0.716796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1348", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1608", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-2214", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2749", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2860", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3171", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4371", "mrqa_hotpotqa-validation-4397", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5327", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-543", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3678", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4110", "mrqa_naturalquestions-validation-4211", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4382", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6680", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1371", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2462", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-28", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3421", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-351", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-526", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-103", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10534", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-11376", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11937", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-16", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-1706", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-2423", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-2883", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3647", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7583", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8604", "mrqa_searchqa-validation-8890", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-9163", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9967", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-138", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3341", "mrqa_squad-validation-3532", "mrqa_squad-validation-3599", "mrqa_squad-validation-3787", "mrqa_squad-validation-3813", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-3954", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4107", "mrqa_squad-validation-421", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4609", "mrqa_squad-validation-4802", "mrqa_squad-validation-4887", "mrqa_squad-validation-5061", "mrqa_squad-validation-5089", "mrqa_squad-validation-5222", "mrqa_squad-validation-5237", "mrqa_squad-validation-525", "mrqa_squad-validation-5269", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-531", "mrqa_squad-validation-5319", "mrqa_squad-validation-5483", "mrqa_squad-validation-5501", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5741", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-6873", "mrqa_squad-validation-689", "mrqa_squad-validation-7001", "mrqa_squad-validation-7002", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-741", "mrqa_squad-validation-7458", "mrqa_squad-validation-7470", "mrqa_squad-validation-7525", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7736", "mrqa_squad-validation-7788", "mrqa_squad-validation-7885", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8224", "mrqa_squad-validation-8233", "mrqa_squad-validation-8403", "mrqa_squad-validation-8612", "mrqa_squad-validation-8668", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9057", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9579", "mrqa_squad-validation-9613", "mrqa_squad-validation-9744", "mrqa_squad-validation-9845", "mrqa_squad-validation-9892", "mrqa_squad-validation-9894", "mrqa_squad-validation-9969", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2468", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-830"], "OKR": 0.859375, "KG": 0.4921875, "before_eval_results": {"predictions": ["deflate", "Gov. Jan Brewer.", "Casey Anthony,", "Tehran,", "Britain's", "\"We exercise all around the globe and have joint exercises with countries all over the world. So do many other nations.\"", "saying they did not receive a fair trial.", "Gordon Brown", "regulators in the agency's Colorado office", "The 544-page book, \"Oprah: A Biography,\"", "Vivek Wadhwa", "\"stand tall, stand firm.\"", "Former Mobile County Circuit Judge Herman Thomas", "eight-day", "rocket", "Iggy Pop invented punk rock.", "committed to equality,", "Harry Nicolaides,", "in central", "neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,", "opium poppies", "all animal products.", "firefighter", "the end of a biology department faculty meeting at the University of Alabama in Huntsville,", "at the age of 23", "secretary of defense", "\"The Real Housewives of Atlanta\"", "dancing", "June 2004", "Jennifer Arnold and husband Bill Klein,", "Waterloo Bridge", "two", "ties", "defensive measures around Hawaii.", "Nicole", "found Julissa Brisman, 26, unconscious with multiple gunshot wounds on April 14.", "Drew Kesse,", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "a man wearing a baseball cap, dark jersey, blue jeans and running shoes entering a store,", "14", "Alan Graham", "intravenous vitamin \"drips\" are part of the latest quick-fix, health fad catching on in Japan:", "delivers a big speech", "Ennis, County Clare", "$17,000", "Swedish Prime Minister Fredrik Reinfeldt", "on Anjuna beach in Goa", "who prefers to be anonymous,", "South African", "apparently died after shooting himself three times in the head", "a rally at the State House", "J. Presper Eckert and John William Mauchly", "Latitude", "excessive growth", "Saint Cecilia", "One Thousand and One", "King George I", "in May 2011", "Violet", "Kinnairdy Castle", "the hippopotamus", "Boll weevil", "Aleksandr Solzhenitsyn", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5691618396766454}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8125000000000001, 0.923076923076923, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6, 1.0, 0.0, 0.0, 0.25, 1.0, 0.5, 0.9333333333333333, 0.0, 0.8, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.17391304347826086, 1.0, 1.0, 1.0, 0.20000000000000004, 0.33333333333333337, 0.0, 0.5, 0.5882352941176471, 1.0, 0.2, 0.0, 0.22222222222222224, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-816", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-4004", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-788", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-2729", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-2320", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-705", "mrqa_hotpotqa-validation-3836"], "SR": 0.40625, "CSR": 0.5609756097560976, "EFR": 0.9736842105263158, "Overall": 0.7206038390564827}, {"timecode": 41, "before_eval_results": {"predictions": ["Jim Nantz and Phil Simms", "Texas", "Detroit", "birds", "george Fleming", "Taps", "Seal", "Dr. Strangelove", "a cat", "Atlanta", "on a brief comment made by Carnap in the... metaphysics are without meaning because they are not deducible", "Kevin Costner", "Coors Field", "Oregon", "Doc Holliday", "Chicken Run", "Hercules", "\"Rama, #1\"", "hydrogen", "Svengali", "Magda", "Mammoth Cave National Park", "a sousaphone", "S-waves", "Poseidon", "Queen Elizabeth II", "The 39 Steps", "Cynic", "Judges", "oreo", "St. Lawrence", "the seashore", "Dr. Irina Spalko", "America", "Bill Clinton", "Cloverfield", "Paraguay", "Zenda", "Gulf of Tonkin", "(to lay at rest, quiet, also nourish, intransitive", "himself", "on", "a calculator", "Tuesday", "Olivia Newton-John", "Robert Cohn", "oil", "South Africa", "george Vermeer", "Arnold J. Toynbee", "george W. Bush", "January 2004", "Rachel Sarah Bilson", "Pradyumna", "Cheshire", "George H. W. Bush", "blood", "on a last day to determine if they will spend eternity in Gehenna or heaven for their sin", "Indooroopilly Shopping Centre", "the first freshman to finish as the runner-up", "opium", "Adam Yahiye Gadahn,", "found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "Tarzan"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6175347222222222}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, false, true], "QA-F1": [0.20000000000000004, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-597", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-9543", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-8164", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-2509", "mrqa_searchqa-validation-6563", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-3826", "mrqa_searchqa-validation-2758", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-4268", "mrqa_searchqa-validation-16017", "mrqa_searchqa-validation-12223", "mrqa_searchqa-validation-2756", "mrqa_searchqa-validation-15316", "mrqa_triviaqa-validation-6163", "mrqa_triviaqa-validation-5356", "mrqa_hotpotqa-validation-3713", "mrqa_newsqa-validation-3404"], "SR": 0.546875, "CSR": 0.5606398809523809, "EFR": 0.9310344827586207, "Overall": 0.7120067477422003}, {"timecode": 42, "before_eval_results": {"predictions": ["the Divine Right of Kings", "Mussolini", "Madonna", "deuce", "Tarsus", "Charles Chaplin", "Dancing On Ice", "Paris", "Korea", "foil", "Agatha Christie", "Cold Blood", "laugh", "Jackie Joyner", "a whale", "Nelson Mandela", "the Taurid", "Cuba Libre", "Thomas Jefferson", "Tanzania", "Oscar Wilde", "Puebla", "Pennsylvania", "Borneo", "Cupcake", "Walla Walla", "Netflix", "Roger Bannister", "Le Corbusier", "(Scott) Peterson", "the action of Russia", "Franco", "Bolivia", "rugby", "Ireland", "Vanna White", "Catherine II", "blue", "the ignition coil", "ROE", "Elizabeth Cady Stanton", "the Eastern Alps", "Francis Ford Coppola", "wives", "meander", "The Wind in the Willows", "Jack Dempsey", "hexadecimal", "The Two Gentlemen of Verona", "chimpanzee", "the Red Cross", "pigs", "August 2012", "Jack Nicklaus", "the Central African Republic", "six", "Yalta", "Emad Hashim", "Brenda Song", "The Daily Stormer", "Bayern Munich", "the United States", "cancer", "mckinair"], "metric_results": {"EM": 0.515625, "QA-F1": 0.596875}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-16519", "mrqa_searchqa-validation-11329", "mrqa_searchqa-validation-11452", "mrqa_searchqa-validation-14303", "mrqa_searchqa-validation-519", "mrqa_searchqa-validation-2589", "mrqa_searchqa-validation-16809", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-1094", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-9664", "mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-7658", "mrqa_searchqa-validation-10935", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-16153", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-12644", "mrqa_searchqa-validation-12772", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-7323", "mrqa_searchqa-validation-14283", "mrqa_naturalquestions-validation-7702", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-2135", "mrqa_hotpotqa-validation-5307", "mrqa_newsqa-validation-3131", "mrqa_triviaqa-validation-6337"], "SR": 0.515625, "CSR": 0.559593023255814, "EFR": 1.0, "Overall": 0.7255904796511627}, {"timecode": 43, "before_eval_results": {"predictions": ["Orthogonal components", "Henry III", "Tomorrow Never Dies", "Monaco", "the Leon-Delaney,", "Jonathan", "Columbus", "Brett Favre", "South African", "Brian Deane", "Pinter", "Argentina", "William Conrad", "1875", "Lloyd Webber", "Iran", "Fairey Swordfish", "the Isle of Arran", "London County", "Playboy", "a man`s and boys` hair", "Matalan", "Chesney Wold", "boise", "a griffin", "red", "The Pink Panther", "Stocznia", "The Labour Party", "rings", "Karl Marx and Friedrich Engels", "Utrecht", "\u201cin support of the mine workers against the attack on their standard of life by the coalowners.\u201d", "Strangeways", "Carousel", "14", "Richard Wagner", "the brain and the spinal cord", "\"Garp\"", "(Fred) William Herschel", "Belgium", "October 31st", "a beetle", "(Deacon) Blues", "Pompey", "Denali", "auctions", "haddock", "L. P. Hartley", "Italy", "a snake", "Toby Kebbell", "2001", "the human hands", "the Distinguished Service Cross", "Shenae Grimes", "five-time", "prostate cancer,", "the U.S. Holocaust Memorial Museum,", "on its final scheduled voyage this week.", "Dale", "a game show", "London", "Uru-Salim"], "metric_results": {"EM": 0.5, "QA-F1": 0.6005208333333333}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, false], "QA-F1": [0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10394", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-2870", "mrqa_triviaqa-validation-6459", "mrqa_triviaqa-validation-5057", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-4052", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-6442", "mrqa_naturalquestions-validation-8177", "mrqa_hotpotqa-validation-3137", "mrqa_hotpotqa-validation-4109", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2244", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-9414"], "SR": 0.5, "CSR": 0.5582386363636364, "EFR": 0.96875, "Overall": 0.7190696022727272}, {"timecode": 44, "before_eval_results": {"predictions": ["co-chair", "the Flying Pickets", "iceland", "Evita", "Victoria", "Sikhism", "Dick Turpin", "Sinclair Lewis", "Argentina", "neck", "Guatemala", "olive", "Munich", "violin", "a sash", "Paul Nash", "placentals", "first among equals", "a robin", "Indira Priyadarshini Gandhi", "Colombia", "demand-siders", "Uranus", "Prince Igor", "h2g2", "monaco", "watt", "The Wicker Man", "last of the Mohicans", "nizhny Novgorod", "South Africa", "hovercraft", "Pete Sampras", "white", "Jimmy Boyd", "Tina Turner", "flowers", "brash", "bees", "Harold Wilson", "Spice Girls", "Seattle", "henry", "detergent", "Wolfgang Amadeus Mozart", "\"Bubba\" Watson, Jr.", "Utah", "Richard Lester", "December", "peregrines", "steel", "1 October 2006", "Cee - Lo", "Britain", "Marcus T. Reynolds", "35,000", "Tel Aviv University", "response to a civil disturbance call", "the foyer of the BBC building in Glasgow,", "\"Hillbilly Handfishin'\"", "a man", "Roosevelt & this leader", "flanker", "an integral membrane protein that builds up a proton gradient across a biological membrane"], "metric_results": {"EM": 0.625, "QA-F1": 0.6800852793040293}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, false, true], "QA-F1": [0.2857142857142857, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.7499999999999999, 0.923076923076923, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8581", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6195", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-7543", "mrqa_triviaqa-validation-5547", "mrqa_naturalquestions-validation-5476", "mrqa_hotpotqa-validation-773", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-220", "mrqa_searchqa-validation-9143", "mrqa_searchqa-validation-13219", "mrqa_searchqa-validation-9558"], "SR": 0.625, "CSR": 0.5597222222222222, "EFR": 0.9583333333333334, "Overall": 0.7172829861111112}, {"timecode": 45, "before_eval_results": {"predictions": ["the Apollo spacecraft", "1853", "Daniel A. Dailey", "adrenal medulla", "anakin Skywalker", "Plank", "Ann Gillespie", "near Chesapeake Bay", "related to the Common Germanic word guma `` man ''", "March 26, 1973", "drizzle", "Tommy Shaw", "1803", "Charles Perrault", "John Daly", "1998", "Elizabeth Dean Lail", "March 31, 2017", "Victor Salva", "Aristotle", "2007", "Continental drift", "eight", "more than a million", "the last book accepted into the Christian biblical canon", "1995", "Rock Island, Illinois", "1926", "2006 -- 06", "the west and the Americas in the east", "on the pelvic floor", "The Osmonds", "Ace", "from the Greek \u0392\u03bf\u03ce\u03c4\u03b7\u03c2, Bo\u014dt\u0113s, meaning `` herdsman '' or `` plowman ''", "Teddy Randazzo", "brain", "Christopher Jones", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "current day", "Bill Belichick", "The balance sheet", "London, United Kingdom", "Hellenism", "232", "January 2, 1971", "Andy Cole", "\u20b9 39.50 lakh", "natural killer cells", "a crust of potatoes", "the team", "4.5 pounds or 2.04 kg", "de Havilland moth", "12", "bukwus", "the International Hotel", "Gloria Trevi", "postal delivery", "was killed in an attempted car-jacking as he dropped his children off at a relative's house,", "the leader of a drug cartel that set off two grenades during a public celebration in September,", "\"Slumdog Millionaire\"", "Clifford Odets", "Margaret Mitchell", "Microsoft", "november jarman"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6605753282044042}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, false, false, false, true, true, false, false, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5454545454545454, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5, 0.2857142857142857, 0.8, 1.0, 1.0, 0.6956521739130436, 0.0, 1.0, 0.5, 0.9, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-8763", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-2812", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1552", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-7509", "mrqa_triviaqa-validation-1421", "mrqa_triviaqa-validation-2110", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-3832", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-2476", "mrqa_searchqa-validation-947"], "SR": 0.546875, "CSR": 0.5594429347826086, "EFR": 0.9655172413793104, "Overall": 0.7186639102323837}, {"timecode": 46, "before_eval_results": {"predictions": ["Gamal Abdul Nasser", "2016", "a line of committed and effective Sultans", "Germany", "to United Nations Peacekeeping Operations", "travis", "King Saud University", "Parashara", "John Dalton", "Vienna", "pepsin", "Carol Worthington", "pagan custom", "1928", "April 29, 2009", "19 June 2018", "Todd Bridges", "Ben Savage", "the senior-most judge of the supreme court", "multinational retail corporation", "Tex - Mex", "Andrea Brooks", "sometime between 124 and 800 CE, with some theories dating the earliest Polynesian settlements to the 10th or even 13th century", "Britney Spears", "2010", "232", "many forested parts", "about 3.5 mya", "2010", "The Union", "mitosis", "Henry", "Texhoma", "officiant", "Nat Finkelstein", "The Royalettes", "from the Latin centum, which means 100, and gradus", "Vienna", "Andrew Johnson", "microfilament", "four distinct levels", "if the concentration of a compound exceeds its solubility", "Cleveland Indians", "travis", "August Darnell", "Stephen Graham -- Detective Superintendent Dave Kelly", "Anna Maria Jaffrey", "the plane crash", "Tatsumi", "Ernest Hemingway", "1999", "Venus", "Vietnam", "nastase", "Bennett Cerf", "Scotty Grainger", "uncle", "Steve Jobs", "a preliminary injunction against a Mississippi school district and high school in federal court Tuesday", "ordered the release of the four men -- Jesus Ortiz, 19; Stalin Felipe, 19", "the poverty line", "uterus", "I tried to cool her off with a wet rag as much as i could.", "( Colonal) Sanders"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5378977793040293}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.923076923076923, 0.5, 0.0, 0.0, 0.5, 0.0, 0.19999999999999998, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.7500000000000001, 0.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-10682", "mrqa_naturalquestions-validation-3771", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-5943", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-5936", "mrqa_naturalquestions-validation-8759", "mrqa_triviaqa-validation-2205", "mrqa_triviaqa-validation-3418", "mrqa_triviaqa-validation-959", "mrqa_hotpotqa-validation-4468", "mrqa_hotpotqa-validation-1475", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3806", "mrqa_searchqa-validation-14439", "mrqa_searchqa-validation-15864"], "SR": 0.453125, "CSR": 0.5571808510638299, "EFR": 0.9714285714285714, "Overall": 0.7193937594984803}, {"timecode": 47, "before_eval_results": {"predictions": ["2014", "Ren\u00e9 Descartes", "their bearers", "Dante Pastula", "Havana Harbor", "sedimentary rock", "April 10, 2018", "Tracy McConnell", "the North Atlantic Ocean", "inland", "111", "around 2 %", "an Irish feminine name", "the head of the Imperial Family and the traditional head of state of Japan", "November 17, 2017", "an instant messaging client that was first developed and popularized by the Israeli company Mirabilis", "appellate courts", "A young woman who flees her abusive husband and meets Alex", "84", "William Chatterton Dix", "Killer Within", "Broken Hill and Sydney", "appendicular skeleton", "a database maintained by the United States federal government, listing the telephone numbers of individuals and families who have requested that telemarketers not contact them", "The song's music video depicts a couple broken apart by the Iraq War", "Johannes Gutenberg", "National Industrial Recovery Act ( NIRA )", "all within the Pittsburgh metropolitan area", "the little finger", "the Nationalists", "31 October 1972", "twelve Wimpy Kid books have been released, plus one do - it - yourself book and two movie diaries", "Matt Flinders", "The person who has existence in two paradise", "to aid the access of multimedia and voice applications from wireless and wireline terminals", "Ra\u00fal Eduardo Esparza", "The post translational modification of proinsulin to mature insulin only occurs in the beta cells of the islets of Langerhans", "4 September 1936", "the field is limited to drivers who meet more exclusive criteria", "H.L. Hunley", "Orangeville, Ontario", "Bactrian", "the Gentiles", "James Hutton", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "decades after its initial release", "2005", "March 18, 2005", "Fats Waller", "the fovea centralis", "the Russian army", "in Viking times if you died, they would typically send you on your way in one of two ways, cremation or inhumation", "Rudyard Kipling", "the Penguin", "Otto Eduard Leopold", "Ukrainian", "237", "Apple employees", "\"We connected meaningfully about the important issues that have emerged over recent days, and I offered him my sincere apologies for any offense to our veterans caused by this report.", "in July", "Margaret Mitchell", "crucifixion", "Lisa Lisa Lisa", "right-hand batsman"], "metric_results": {"EM": 0.5, "QA-F1": 0.6676841362491034}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5263157894736842, 1.0, 0.375, 0.5, 0.4615384615384615, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8108108108108109, 0.0, 1.0, 0.888888888888889, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7999999999999999, 0.0, 1.0, 0.3, 1.0, 0.25, 0.4, 0.8, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10256410256410257, 0.6666666666666666, 1.0, 0.0, 0.8, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-8179", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-10321", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-10618", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-7358", "mrqa_triviaqa-validation-3828", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-272", "mrqa_searchqa-validation-14084", "mrqa_searchqa-validation-16263", "mrqa_hotpotqa-validation-181"], "SR": 0.5, "CSR": 0.5559895833333333, "EFR": 0.96875, "Overall": 0.7186197916666666}, {"timecode": 48, "before_eval_results": {"predictions": ["Magdalen College", "Rana Daggubati", "13 people", "Udhayam NH4", "1", "Kristy Lee Cook", "LA Galaxy", "The Volvo 850", "February 14, 1859", "'Tis the Fifteenth Season", "Biola University in La Mirada, California", "Best Art Direction", "2012 NBA draft", "October 13, 1980", "tomato", "Gracie Mansion", "Arsenal Football Club", "Operation Neptune", "Steve Prohm", "a super-regional shopping mall", "Lady Charlotte Elliot", "The 2000 World Rally Championship", "seven players", "28 June 1945", "University of California", "Miami Gardens", "Indian origin in space", "Paige O'Hara", "Graham McLaren", "The Emperor of Japan", "Hillary Clinton presidential campaign, 2016", "1896", "Edward Michael \" Mike\"/\"Spanky\" Fincke", "the D\u00e2mbovi\u021ba River", "Philip Quast", "Pierce County", "PPG Paints Arena", "8 November 1978", "Rodrick Heffley", "Operation Julin", "BAFTA TV Award Best Actor", "the First Balkan War", "1641", "Marty Ingels", "mathematician, physicist, and spectroscopist", "1941", "June 2, 2008", "Charice", "Sleepy Brown", "Waimea Bay", "Ustad Vilayat Khan", "the part of the brain", "his teenage role as the title character on the Disney Channel television series The Famous Jett Jackson", "restoring someone's faith in love and family relationships", "Eddie", "gold anniversary", "corvidae", "$162 billion in war funding without the restrictions congressional Democrats vowed to put into place", "out in the woods", "The Sopranos", "Winslow Homer", "Elizabeth II", "the Crimean War", "Alexey Pajitnov"], "metric_results": {"EM": 0.5, "QA-F1": 0.6524509803921569}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, true, false, false, false, true, true, false, false, false, false, true, false, true, false, true, true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.4, 0.4, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.823529411764706, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-2480", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-2335", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-4551", "mrqa_hotpotqa-validation-913", "mrqa_hotpotqa-validation-1757", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-438", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-636", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-5535", "mrqa_newsqa-validation-163", "mrqa_searchqa-validation-14284"], "SR": 0.5, "CSR": 0.5548469387755102, "EFR": 0.96875, "Overall": 0.718391262755102}, {"timecode": 49, "before_eval_results": {"predictions": ["The Yardbirds", "Dubai", "Silverstone", "Ted", "Triumph", "1655", "King Henry V", "Darren Aronofsky", "beetles", "platomy", "100 years", "Rudyard Kipling", "dragonflies", "Cole Porter", "liriope", "plenet", "Big Brother", "kawasaki", "Beaujolais", "Ashburton", "Paul Dukas", "Tom Watson", "38", "the dog's middle ear", "Tokyo", "low", "the keeper of the Longstone (Fame Islands) lighthouse", "God bless America, My home sweet home.", "Dangerous Minds", "psychology", "Apollon", "Sorry", "South Korea", "Boxing Day", "St Pancras", "fish", "wainscoting", "plaphylococcus", "Scarborough", "Alan Turing", "Newton", "Calcium carbonate", "chhatrapati Shivaji", "Anna", "Roy Rogers", "Cyclades", "tennis", "Hitachi", "king of Phrygia", "plgrade", "the gizzard", "Detroit Tigers", "counter clockwise direction", "3.5 million years old", "Tampa Bay Lightning", "Theatre Ventures, Inc.", "Battle of Dresden", "U.S. relief effort in Haiti.", "$150 billion", "root out terrorists within its borders.", "Latvia", "alligator", "Jerry Lee Rice", "JUNGLE JIM"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5063244047619048}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true, false, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 0.6, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6533", "mrqa_triviaqa-validation-3651", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3276", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-1944", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-6874", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-2114", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-6241", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-7216", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-5431", "mrqa_triviaqa-validation-3904", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-293", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-6612", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-4647", "mrqa_hotpotqa-validation-1029", "mrqa_newsqa-validation-93", "mrqa_searchqa-validation-2086", "mrqa_searchqa-validation-11198", "mrqa_searchqa-validation-5649"], "SR": 0.40625, "CSR": 0.551875, "EFR": 1.0, "Overall": 0.724046875}, {"timecode": 50, "UKR": 0.693359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1455", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1608", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-2214", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2584", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-438", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4468", "mrqa_hotpotqa-validation-4499", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5443", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4278", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-4853", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5405", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-670", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1005", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2373", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2846", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3559", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11937", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-14232", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15316", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-16409", "mrqa_searchqa-validation-16501", "mrqa_searchqa-validation-16791", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3647", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-4337", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-592", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6519", "mrqa_searchqa-validation-6657", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7018", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7260", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7658", "mrqa_searchqa-validation-7973", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-8906", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9543", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2382", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3532", "mrqa_squad-validation-3599", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-3954", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4609", "mrqa_squad-validation-4802", "mrqa_squad-validation-4887", "mrqa_squad-validation-5061", "mrqa_squad-validation-5089", "mrqa_squad-validation-5237", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-5483", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-689", "mrqa_squad-validation-7002", "mrqa_squad-validation-7183", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-7458", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7788", "mrqa_squad-validation-7885", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8233", "mrqa_squad-validation-8403", "mrqa_squad-validation-8612", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9579", "mrqa_squad-validation-9613", "mrqa_squad-validation-9892", "mrqa_squad-validation-9894", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-21", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-2696", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-3391", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3761", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4537", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5098", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5624", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6195", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-96"], "OKR": 0.84375, "KG": 0.4609375, "before_eval_results": {"predictions": ["Medusa", "m Maui", "Easy Rider", "scrabble", "Percy Bysshe Shelley", "Billy Joel", "pardon", "New York City", "phoebus", "Roman Polanski", "Dogberry", "Battle Creek", "the Red Sea", "Mary Todd Lincoln", "Mary Poppins", "isaac new york", "phonetics", "The Naked Brothers Band", "Marcia Cross", "trailgator bars", "Holly Golightly", "quilt", "anemoi", "butter", "The Tagus", "the CIO", "acting", "nautilus", "belize", "Denmark", "student loan", "quiz", "the fife", "Seattle", "Michael Jordan.", "Hillary Clinton", "the French Legion of Honour", "phoebus", "kimchi", "December 23, 1777", "chancellor of West Germany", "Washington Irving", "Crimean", "phoebus", "the White House", "a gastropod shell", "Julius Caesar", "one dollar and eighty-seven cents", "Dean Acheson", "Pittsburgh Steelers", "jury trials in certain civil cases.", "Malina Weissman", "Kyla Pratt", "Jonathan Goldstein", "9 imperial gallons", "not", "Spain", "Sean Yseult", "1754", "Robert Harper", "outside the Iranian consulate in Peshawar", "in his native Philippines", "she's in love,", "September 21, 2014"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5946056547619047}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14794", "mrqa_searchqa-validation-6167", "mrqa_searchqa-validation-12751", "mrqa_searchqa-validation-13002", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-11463", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-241", "mrqa_searchqa-validation-4385", "mrqa_searchqa-validation-14430", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-9201", "mrqa_searchqa-validation-9334", "mrqa_searchqa-validation-11794", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-9809", "mrqa_searchqa-validation-16024", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-15599", "mrqa_searchqa-validation-9986", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-674", "mrqa_hotpotqa-validation-3612", "mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-3509", "mrqa_newsqa-validation-551", "mrqa_hotpotqa-validation-4911"], "SR": 0.53125, "CSR": 0.5514705882352942, "EFR": 1.0, "Overall": 0.7099034926470589}, {"timecode": 51, "before_eval_results": {"predictions": ["Reddi-wip", "the dachshund", "Saturn", "Women in the Work Force", "Risk", "a Bar Mitzvah", "cauliflower ear", "Clark Gable", "Katharine Hepburn", "Metacom", "surrender", "Tarsus", "the Niagara River", "Hannibal Lecter", "The Man Without A Country", "the Arc de Triomphe", "George Frideric Handel", "john john Combs", "Indonesia", "Florence Henderson", "Linus Pauling", "gold", "the English Channel", "a whelped", "water", "Ohio", "Million Dollar Baby", "beans", "violin", "Papua New Guinea", "Macy's Thanksgiving Day Parade", "Jeb Bush", "the Arctic Ocean", "gold", "Port-au-Prince", "the Barbary Coast", "humility", "Michael Phelps", "rice", "gas masks", "\"to look like\"", "\"The Jinx\"", "breast", "jets of water", "Louis XIV of France", "suspension bridge", "petticoats", "trudge", "JetBlue", "Ryan Seacrest", "a key", "Lake Michigan", "Spanish colonies", "in the Southern United States", "piscinae", "the house sparrows", "Islamabad", "Indianapolis Motor Speedway", "$26 billion", "Deftones", "Adam Lambert and Kris Allen", "27-year-old's", "more than two years,", "Michael Madhusudan Dutta"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6177083333333333}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-14737", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-13086", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-16829", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10836", "mrqa_searchqa-validation-9669", "mrqa_searchqa-validation-10726", "mrqa_searchqa-validation-3994", "mrqa_searchqa-validation-7613", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-10574", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-10180", "mrqa_searchqa-validation-8660", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-2254", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-554", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-2705"], "SR": 0.546875, "CSR": 0.5513822115384616, "EFR": 1.0, "Overall": 0.7098858173076923}, {"timecode": 52, "before_eval_results": {"predictions": ["Alfred Preis", "\"Nip/Tuck\"", "Adelaide Lightning", "Homeland", "1983", "3.9 mi", "Hilux", "\"Rude Boy\"", "Snowball II", "Anna Clyne", "Flushed Away", "the Elbow River", "Mickey\\'s Christmas Carol", "Ellie Kemper", "Aamir Khan", "Eugene Levy", "25 million", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu", "Julianne Moore", "drummer Seb Rochford", "Samantha Spiro", "Martin O'Neill", "Los Angeles Galaxy", "Faysal Qureshi", "Total Nonstop Action Wrestling", "Don Hahn", "Nobel Prize in Physics", "ballets", "the east of Ireland", "Blue Grass Airport", "Tim Whelan", "Cleveland", "Ashley Jensen", "Franc Roddam", "Ben R. Guttery", "Pieter van Musschenbroek", "ABC", "Roseann O'Donnell", "\"online hub for Clinton backers so that they can find easy-to-share facts, stats and other information you can take out to social media when you\u2019re having debates on key issues people are discussing\"", "1902", "the USS \"Enterprise\"", "Las Vegas", "Todd Emmanuel Fisher", "supply chain management", "attorney", "June 26, 2018", "MGM Grand Las Vegas", "Harold Edward Holt", "Clara Petacci", "from 1986 to 2013", "Bill Ponsford", "as one of Jesus'disciples", "Ricardo Chavira", "the Gaget, Gauthier & Co. workshop", "Martin Van Buren", "Robert Boyle", "Vienna", "Harrison Ford", "the campus of a school", "27-year-old's", "the cat that ate the canary", "Moses", "condensation", "Kitty Kelley"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6316134316134316}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.10810810810810813, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-1584", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-3504", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-3321", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-5010", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-617", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-3502", "mrqa_hotpotqa-validation-4515", "mrqa_hotpotqa-validation-3286", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-3391", "mrqa_triviaqa-validation-2082", "mrqa_newsqa-validation-2415", "mrqa_searchqa-validation-11381", "mrqa_searchqa-validation-12184"], "SR": 0.546875, "CSR": 0.5512971698113207, "EFR": 1.0, "Overall": 0.7098688089622641}, {"timecode": 53, "before_eval_results": {"predictions": ["London", "2018", "Welch, West Virginia", "800", "2009", "Mark Jackson", "Indonesia", "December 24, 1836", "2 September 1990", "BC Jean", "Toronto", "off the southernmost tip of the South American mainland", "Roger Dean Stadium", "Jesse Wesley Williams", "19 June 2018", "Prince William", "Sanchez Navarro", "Nick Kroll", "pigs", "in Pittsburgh", "2018", "to the left of the dinner plate", "headdresses", "a Norwegian town", "1960", "1840s", "heavy tank", "semi-automatic", "Jack and Jill", "displacement", "methane, ethane, and propane", "200 to 500 mg", "Mike Gabriel", "November 5, 2017", "vasoconstriction of most blood vessels, including many of those in the skin, the digestive tract, and the kidneys", "Firoz Shah Tughlaq", "1868 war veterans, such as Polish internationalist General Carlos Roloff and Seraf\u00edn S\u00e1nchez in Las Villas", "muscles", "Robin", "March 26, 1973", "New England Patriots", "New York City", "S", "31 - member Senate and a 150 - member House of Representatives", "Ajay Tyagi", "from the heraldic crest carved in the lintel on St. Ignatius'family home in Azpeitia, Spain", "Brooklyn Heights", "book and architecture", "19 June 2018", "Efren Manalang Reyes", "California", "Barcelona", "nickel", "Dodi Fayed", "January 4, 1976", "1921", "Edmund Allenby", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery", "coral reefs,", "22", "altitude", "Timbaland", "Foo Fighters", "Jennifer Grey"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6190364793015721}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, false, false, false, false, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false, true, true, true, true, true], "QA-F1": [1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.2222222222222222, 0.923076923076923, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.11764705882352941, 0.33333333333333337, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.631578947368421, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6137", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-2337", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-9384", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8873", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-2381", "mrqa_naturalquestions-validation-1533", "mrqa_naturalquestions-validation-3801", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-3837", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-6364", "mrqa_hotpotqa-validation-5720", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-1638"], "SR": 0.515625, "CSR": 0.5506365740740741, "EFR": 0.967741935483871, "Overall": 0.703285076911589}, {"timecode": 54, "before_eval_results": {"predictions": ["the martini", "a final contest", "lisbons", "(Arseniy) Yatsenyuk", "paul paul", "Tony Gwynn", "Atonement", "North Carolina", "Collagen", "Just say no", "Typewriter", "Diane Arbus", "Cincinnati", "Cleopatra, Queen of Denial", "the Suez Canal", "Planet of the Apes", "garret", "Adam Sandler", "north (N), east (E), south (S), west (W)", "Erasmus", "member", "William Shakespeare", "phobias", "San Jose", "piano", "the Byzantine Empire", "Dunkirk", "white", "Psalms", "pearls", "gelato", "Jesus", "dengue fever", "George Balanchine", "Alfred Stieglitz", "Bryan Adams", "Africa", "Marcus Junius Brutus", "Applebee\\'s", "Mercator", "Robin Hood", "sauropods", "(Boris) Godunov", "Daniel Boone", "William Tecumseh Sherman", "Prison Break", "a hippopotamus", "Black Beauty", "Catherine of Braganza", "Sinclair Lewis", "Leo", "\u00a3 3.2 billion ( US $5.1 billion )", "Jamestown", "Argentina", "cat", "Mr. Humphries", "Boulder Dam", "Personal History", "Hellenism", "Barnoldswick", "bodies and heads", "the U.S. Holocaust Memorial Museum", "BET", "Havana Harbor"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6373263888888889}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-12810", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-1885", "mrqa_searchqa-validation-13725", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-1876", "mrqa_searchqa-validation-451", "mrqa_searchqa-validation-308", "mrqa_searchqa-validation-7376", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15511", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-10640", "mrqa_searchqa-validation-14717", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5271", "mrqa_triviaqa-validation-1657", "mrqa_newsqa-validation-23"], "SR": 0.609375, "CSR": 0.5517045454545455, "EFR": 1.0, "Overall": 0.7099502840909091}, {"timecode": 55, "before_eval_results": {"predictions": ["a zebra", "Sarah McLachlan", "half", "Japan", "C Daryl Chessman", "grade point average", "grapefruit", "Detroit", "Tiger Woods", "John Paul II", "Blackbeard", "a goose", "Jane Goodall", "Big Ben", "Ethiopian", "3", "Stephen Crane", "Luxor", "gung ho", "a nickel", "Bill Clinton", "Wyoming", "a septum", "nantucket", "The Joys and Dilemmas of Wealth", "Never let me go", "Gianlorenzo Bernini", "mosquitoes", "Frank Sinatra", "Mark Knopfler", "photons", "the Jefferson Building", "low blood pressure", "Mousehunt", "Israel", "honey", "Rugby Football Union", "The Taming of the Shrew", "a palace", "coffee", "Knott\\'s Berry Farm", "Phaedra", "Carl Linnaeus", "Australia", "Jodie Foster", "Ventricular Fibrillation", "Barbary pirates", "cinnamon rolls", "The Monkees", "Matilda", "Don Juan", "1923", "Master Christopher Jones", "T.J. Miller", "7", "Mark Renton", "Jerry Mouse", "AVN Adult Entertainment Expo", "England and Ireland", "The Beatles", "two bodyguards", "detention operations,", "the 11th anniversary of the September 11, 2001, terror attacks.", "Philip Billard Municipal Airport"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6813205891330891}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 0.3076923076923077, 0.888888888888889]}}, "before_error_ids": ["mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-2778", "mrqa_searchqa-validation-15701", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-9006", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7646", "mrqa_searchqa-validation-14500", "mrqa_searchqa-validation-2662", "mrqa_searchqa-validation-6240", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-12914", "mrqa_searchqa-validation-4022", "mrqa_searchqa-validation-11601", "mrqa_searchqa-validation-14248", "mrqa_searchqa-validation-7849", "mrqa_searchqa-validation-12284", "mrqa_triviaqa-validation-948", "mrqa_triviaqa-validation-7361", "mrqa_hotpotqa-validation-3169", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-2215", "mrqa_hotpotqa-validation-2840"], "SR": 0.609375, "CSR": 0.552734375, "EFR": 0.96, "Overall": 0.70215625}, {"timecode": 56, "before_eval_results": {"predictions": ["Jean Lafitte", "nymphodorus", "argyle", "the Pacific Ocean", "Easter", "\"I will forgive me; that's his business\"", "Dalai Lama", "a heptathlon", "a tuba", "Thomas L. Friedman", "tea", "a vein", "Nicholas II", "Vespucci", "Patrick Henry", "Essen", "punk rock band", "Ho Chi Minh", "the pituitary gland", "Ben Johnson", "Kentucky", "the Byzantine Empire", "9 to 5", "Cambodia", "lunch and dunch", "Velvet Revolver", "Sears", "flavonoids", "a cherries", "Florence", "Ma Barker", "Joe DiMaggio", "Tie", "Naples", "Nick (Isa) Dennings", "the Baruch Plan", "the Big Dipper", "wine", "silk", "\"The Safety Dance\"", "(Felis catus)", "the Moors", "a GPS", "North Carolina", "M&M'S Pe peanut Chocolate Candies", "a cake knife", "Tchaikovsky", "Versailles", "the Panama Canal", "Mathias Rust", "McClellan", "Andy Serkis", "parthenogenesis", "Pangaea or Pangea ( / p\u00e6n\u02c8d\u0292i\u02d0\u0259 / )", "lully", "mule", "60", "Silvia Navarro", "26 November", "John Morgan", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\" Kristal Kraft, a real estate agent in Denver,", "Mary Phagan, a northern Jew who'd moved to Atlanta to supervise the National Pencil Company factory.", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "a freighter seized by pirates off east  Africa"], "metric_results": {"EM": 0.5625, "QA-F1": 0.654718978937729}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1, 0.25, 0.4615384615384615, 0.25]}}, "before_error_ids": ["mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-665", "mrqa_searchqa-validation-5765", "mrqa_searchqa-validation-3160", "mrqa_searchqa-validation-16234", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-10639", "mrqa_searchqa-validation-9458", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-11186", "mrqa_searchqa-validation-6009", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-795", "mrqa_searchqa-validation-11589", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-2151", "mrqa_searchqa-validation-15973", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-2230", "mrqa_triviaqa-validation-3262", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-1022"], "SR": 0.5625, "CSR": 0.5529057017543859, "EFR": 1.0, "Overall": 0.7101905153508772}, {"timecode": 57, "before_eval_results": {"predictions": ["a spectator", "French toast", "Mexico", "\"to cut the wall, you could use a razor knife (box cutter), a jab saw for drywall, or a jigsaw for", "William Faulkner", "Patty Duke", "Franklin Pierce", "Hindu", "(Seth) Rogan", "Intel", "Hank Williams Jr.", "George C. Wallace", "a mall", "a battle", "West Virginia", "Edward Hopper", "asteroids", "Mark Twain", "the Large Orbiting Telescope", "Pop-Tarts", "Jimi Hendrix", "John H. Miller", "(Adam) Linnaeus", "Tootsie", "a shrub", "albinism", "Bonn", "a germany", "chinchillas", "Tennessee", "No Child Left Behind", "William S. Hart", "the Wiggles", "Francisco Franco", "Tennessee Williams", "germany", "Douglas Fairbanks, Jr.", "West Point", "Revolver", "Steely Dan", "in # Quiz # Question", "Norway", "the Kotleta po-kyivsky", "George Clooney", "a diamond", "the Baltimore Orioles", "postcards", "Kentucky", "Skateboarding", "Gaul", "blasters", "appropriates ( gives to, sets aside for ) money to specific federal government departments, agencies, and programs", "Havana Harbor", "Melbourne", "Atticus Finch", "a elephant", "Max Planck", "Ella Jane Fitzgerald", "Ed \"Ed\" O'Neill", "Darkroom", "Dancing With the Stars", "about 75 miles east of Yakima where nine nuclear reactors were eventually built.", "February\\'s Winter Games in Vancouver", "Upstairs Downstairs"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5983893557422969}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.058823529411764705, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12562", "mrqa_searchqa-validation-14344", "mrqa_searchqa-validation-2264", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-14318", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-6362", "mrqa_searchqa-validation-10372", "mrqa_searchqa-validation-14269", "mrqa_searchqa-validation-1372", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-13105", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-644", "mrqa_searchqa-validation-6688", "mrqa_searchqa-validation-806", "mrqa_searchqa-validation-5196", "mrqa_searchqa-validation-14142", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-12764", "mrqa_searchqa-validation-8729", "mrqa_naturalquestions-validation-10533", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-1161", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-1727"], "SR": 0.546875, "CSR": 0.552801724137931, "EFR": 1.0, "Overall": 0.7101697198275863}, {"timecode": 58, "before_eval_results": {"predictions": ["Gov. Andrew Jackson", "Sri Lanka", "John Glenn", "Hinduism", "Lady Sings the Blues", "wedlock", "trans fat (i.e.)", "Leslie", "Edward", "Hello, Dolly!", "the Mesozoic Era", "Gettysburg", "Martin Lawrence", "plantain", "Heracles", "Bob Fosse", "stem cells", "a cutlass", "the Bodleian Library", "Pupils", "a front", "James Franco", "salmon", "The Crow", "sheep\\'s", "James Watt", "1945", "a birthstone", "Ichabod Crane", "An Old Man, a Young Man", "Heather Locklear", "noun", "Holden Caulfield", "hazelnut", "Saudi Arabia", "lamb", "LaDainian Tomlinson", "wheat", "Duke", "a photoelectric cell", "Cape Town", "sperm", "\" Fear & Loathing in Las Vegas\"", "sourdough", "mo Nissanite", "run as Eisenhower\\'s running mate", "Joseph Stalin", "La Guardia", "Chastity", "Turandot", "the Texas Rangers", "Henri Fantin", "ice giants", "Amenhotep IV", "Zimbabwe", "colony", "phobia", "Haiti", "Argand lamp", "1891", "Dick Cheney", "1-1", "an upper respiratory infection", "Austria - Hungary"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6526785714285714}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, true, false, false, true, true], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-16405", "mrqa_searchqa-validation-5240", "mrqa_searchqa-validation-482", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-4326", "mrqa_searchqa-validation-8772", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5137", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-14926", "mrqa_searchqa-validation-10741", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-7781", "mrqa_searchqa-validation-11795", "mrqa_searchqa-validation-13956", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-6896", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-2685", "mrqa_hotpotqa-validation-1482", "mrqa_newsqa-validation-2402", "mrqa_newsqa-validation-2472"], "SR": 0.59375, "CSR": 0.5534957627118644, "EFR": 1.0, "Overall": 0.7103085275423728}, {"timecode": 59, "before_eval_results": {"predictions": ["the Monet Bar", "Mary Magdalene", "The Pillow Book", "General Paulus", "the Grail", "butcher", "the Double", "Dr. Samuel Johnson", "Jessica Simpson", "Humble Pie", "the gallbladder", "peterloo", "Amram and Jochebed", "Leo Tolstoy", "Birmingham", "the Penrose triangle", "Magnificent Seven", "Australian shearers' strike", "Theodore Roosevelt", "a cormorant", "John of Gaunt", "typhoid fever", "germanium", "Microsoft", "John Galliano", "the Big Bang", "Willie Nelson", "horseracing", "Stars on 45 Medley", "Lundy", "Guinea", "Kerri Strug", "Belgium", "mavis", "Beau Brummell", "non-Orthodox synagogues", "Amnesio", "Herbert Henry Asquith, 1st Earl of Oxford", "Nirvana and Kiss", "Mr. Humphries", "Paul Gauguin", "Connochaetes", "French", "50", "Charlie Harper", "nirvana", "Pet Sounds", "purple", "Bob Ferris", "aardvark", "Charles Darwin", "5.7 million", "Oklahoma", "Luke Luke 18 : 1 - 8", "8,515", "villanelle", "Awake", "2008", "China, Taiwan, Hong Kong and Mongolia,", "Patrick McGoohan,", "a coyote", "heating", "A Tale of Two Cities", "Charlie Wilson"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.888888888888889, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-4273", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-2414", "mrqa_triviaqa-validation-3489", "mrqa_triviaqa-validation-6759", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-229", "mrqa_triviaqa-validation-6132", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-577", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-1182", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4592", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-4197", "mrqa_newsqa-validation-2061"], "SR": 0.609375, "CSR": 0.5544270833333333, "EFR": 0.96, "Overall": 0.7024947916666667}, {"timecode": 60, "UKR": 0.638671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1121", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-122", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1396", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3074", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4518", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5443", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-5828", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-159", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4278", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4853", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-670", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1005", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2846", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3152", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-1071", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10905", "mrqa_searchqa-validation-11120", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11198", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11794", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-12032", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-12468", "mrqa_searchqa-validation-13002", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-13669", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-14142", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14344", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-15309", "mrqa_searchqa-validation-15316", "mrqa_searchqa-validation-15373", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16409", "mrqa_searchqa-validation-16501", "mrqa_searchqa-validation-16776", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16915", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2254", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-241", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3160", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3329", "mrqa_searchqa-validation-3454", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4232", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-4337", "mrqa_searchqa-validation-451", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5481", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-592", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6009", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6187", "mrqa_searchqa-validation-644", "mrqa_searchqa-validation-6519", "mrqa_searchqa-validation-6657", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7658", "mrqa_searchqa-validation-7973", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-806", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8373", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8733", "mrqa_searchqa-validation-8772", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-8906", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9201", "mrqa_searchqa-validation-9334", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9543", "mrqa_searchqa-validation-9809", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2382", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3240", "mrqa_squad-validation-3532", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4887", "mrqa_squad-validation-5089", "mrqa_squad-validation-5237", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-5483", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-689", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-7458", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7788", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8233", "mrqa_squad-validation-8612", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9613", "mrqa_squad-validation-9894", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-1078", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-2230", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-2696", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-3391", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3761", "mrqa_triviaqa-validation-3904", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4074", "mrqa_triviaqa-validation-4497", "mrqa_triviaqa-validation-4537", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5098", "mrqa_triviaqa-validation-5183", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5624", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6874", "mrqa_triviaqa-validation-690", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-998"], "OKR": 0.775390625, "KG": 0.42265625, "before_eval_results": {"predictions": ["15", "G\u00f6tene in Sweden", "Muir Mathieson", "Rensselaer County", "more than 20", "\"Beauty and the Beast\"", "extreme nationalist, and nativist ideologies, as well as authoritarian tendencies.", "penny bun", "Overtime", "south-north", "Hockey Club Davos", "2 April 1940", "Domingo \"Sam\" Samudio", "Sunday", "Taylor Alison Swift", "James Gay-Rees", "to steal the plans for the Death Star, the Galactic Empire's super weapon", "graffiti", "ESPN", "Dow Air Force Base", "October 29, 1985", "Point of Entry", "\"Mickey Mouse\" series", "the Harpe brothers", "the 1940s and 1950s", "Port Clinton", "deadpan sketch group", "Bharat Ratna", "George Hodson", "made into a TV series", "the 2011 Pulitzer Prize in General Nonfiction", "Eliot Cutler", "IT products and services", "American", "in 1865", "the Critics' Choice Television Award for Best Supporting Actress in a Comedy Series", "Jeff Meldrum", "Picric acid", "23 March 1991", "1979", "Scarborough, Maine", "1968", "the Caesar Julian", "Rigoletto", "William Jefferson Clinton", "Twister...Ride it Out", "94 episodes", "horror", "\"Baa, Baa, Black sheep\"", "Law no. 91", "28,776", "a Canaanite god associated with child sacrifice", "on the microscope's stage", "commemorating fealty and filial piety", "Cain", "video", "colonel", "New Zealand", "U.N. agencies", "al-Maqdessi,", "the Thames", "Pamela Anderson", "Henry Ford", "methane"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6038938492063493}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.25, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4, 1.0, 0.3333333333333333, 0.4, 0.8, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-804", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-5589", "mrqa_hotpotqa-validation-2263", "mrqa_hotpotqa-validation-2128", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-5359", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-5715", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-1958", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-618", "mrqa_hotpotqa-validation-4186", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-1570", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-182", "mrqa_triviaqa-validation-3314", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-2732"], "SR": 0.484375, "CSR": 0.5532786885245902, "EFR": 0.9696969696969697, "Overall": 0.6719388816443119}, {"timecode": 61, "before_eval_results": {"predictions": ["Awake", "Law Adam", "Daniel Craig", "Magnus Carlsen", "Volvo 850", "400 MW", "1991", "Dra\u017een Petrovi\u0107", "Lord's Resistance Army", "Andrew Joseph \" Andy\" Cohen", "Manhattan", "IFFHS World's Best Goalkeeper", "David May", "New Jersey", "Sir Derek George Jacobi", "Waimea", "Willie Nelson and Kris Kristofferson", "the Mikoyan design bureau", "Nickelodeon on Sunset", "Terry the Tomboy", "an Anglo-Saxon saint", "Give Up", "Matt Winer", "Karl-Anthony Towns", "WB Television Network", "Ice Princess", "Boxing Day, 2004", "liberty as its main idea, promoting free expression, freedom of choice, other social freedoms, and \"laissez-faire\" capitalism", "Australian", "Norse mythology", "Konstant\u012bns Raudive", "Melville", "5,922", "White Horse", "Black Abbots", "Secretly, Greatly", "Kentucky, Virginia, and Tennessee", "2011", "five", "Veronica Hamel", "poetry collections", "French, English and Spanish", "Edward James Olmos", "John R. Leonetti", "\"Alceste\" and \"Orfeo ed Euridice\"", "Perth", "Cersei Lannister", "Baltimore and Ohio Railroad", "stopwatch feature", "Tayeb Salih", "\"The Simpsons\"' thirteenth season", "Ferm\u00edn Francisco de Lasu\u00e9n", "22 \u00b0 00 \u2032 N 80 \u00b0 00\u2032 W \ufeff / \ufefb\ufffd 22.000 \u00b0 W", "Yuzuru Hanyu", "Jose Antonio Reyes", "Jordan", "lulu", "sniff out cell phones", "18", "paintings", "Prison Break", "Oscar Wilde", "Sicily", "Yukon"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7224972943722943}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 1.0, 0.28571428571428575, 0.19047619047619047, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.5, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-3542", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-237", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-4171", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-4367", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-4563", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-625", "mrqa_hotpotqa-validation-1670", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3255", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-2187", "mrqa_naturalquestions-validation-5451", "mrqa_newsqa-validation-4032"], "SR": 0.609375, "CSR": 0.5541834677419355, "EFR": 0.96, "Overall": 0.6701804435483871}, {"timecode": 62, "before_eval_results": {"predictions": ["at home, attending every soccer game and knowing what his kids like to eat for breakfast", "refused Wednesday to soften the Vatican's ban on condom use", "Den of Spies", "tie salesman", "punish", "Venezuela's Libertador military airfield", "Dore Gold, former Israeli ambassador to the United Nations", "The European Union", "Belfast, Northern Ireland", "India", "The worst snowstorm to hit Britain in 18 years", "A member of the group dubbed the \"Jena 6\"", "fire", "the charter mandated the English king to cede certain basic rights to his citizens, ensuring that no man is above the law.", "Dead Weather's \"Horehound\"", "Cash for Clunkers", "San Diego", "returning combat veterans", "Jesus Christ", "The Mexican military", "fossils", "Torget", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship", "Thursday", "his wife's name", "$17,000", "to try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "Bob Johnson", "Matthew Fisher,", "26", "angel", "$31,000", "the \"race for the future... and it won't be won with a president who is stuck in the past.\"", "American Civil Liberties Union", "commitment to regain the trust of customers in his first interview after appearing on Capitol Hill on Wednesday.\"", "1994", "High Court Judge Justice Davis", "to provide security as needed.", "$83,03013", "$250,000", "Christos Winter", "Islamabad", "hundreds", "North Korean newspaper", "Osama", "A judge dismissed all charges Wednesday night and ordered the release of the four men", "\"exceptional circumstances surround these memos and require their release.\"", "the capital city of Harare", "Vernon Forrest, 38,", "sexual assault with a minor", "the District of Columbia National Guard,", "in March 1930", "1961", "Rajendra Prasad", "lancaster", "Secretary of State William H. Seward", "conterminous", "\"Black Christmas\"", "Hispania Racing F1 Team", "small family car", "delete", "Kansas", "Louis XVII", "Kim Basinger"], "metric_results": {"EM": 0.359375, "QA-F1": 0.45195804255610295}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, true, false, true], "QA-F1": [0.0, 0.5333333333333333, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.5, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.06896551724137932, 0.0, 0.8, 0.0, 0.888888888888889, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-1481", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1917", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-3801", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-504", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-829", "mrqa_newsqa-validation-1289", "mrqa_naturalquestions-validation-7628", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-6501", "mrqa_hotpotqa-validation-2703", "mrqa_hotpotqa-validation-1346", "mrqa_searchqa-validation-5326"], "SR": 0.359375, "CSR": 0.5510912698412698, "EFR": 0.975609756097561, "Overall": 0.6726839551877661}, {"timecode": 63, "before_eval_results": {"predictions": ["Israel", "billboards with an image of the burning World Trade Center", "Saturn", "the commissions as a legitimate forum for prosecution, while bringing them in line with the rule of law,\"", "Kgalema Motlanthe,", "Ken Choi,", "part of the proceeds go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "travel with privately armed guards.", "up to $50,000 for her,", "gun charges", "January 24, 2006.", "U.S. program to assassinate terrorists in Iraq.", "Philippines", "used cars", "July", "her home", "natural gas", "the shooting death of B-movie queen Lana Clarkson after a night out in the clubs of Hollywood.", "jazz", "the Rockies", "threatened to rape her if she didn't tell him where Andrade hid his money,", "40 lash for the incident which is said to have taken place in the capital Khartoum on August 21.", "KBR", "Ralph Lauren", "Dubai", "Al-Shabaab", "the classic \"The Wonderful Wizard of Oz\" tale,", "269,000", "eight", "Dube, 43, was killed", "North Korea", "Tuesday.", "super-yacht designers", "Alina Cho", "WTA Tour titles at Strasbourg and Bali prior to Madrid", "the nose, cheeks, upper jaw and facial tissue", "1983", "he gave the victims \"assurances of the church's action\" after the April 18 meeting.", "collaborating with the Colombian government,", "three-time road race world champion,", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "Yemen,", "11", "Matthew Fisher,", "Afghanistan's restive provinces", "Rob Lehr,", "insect stings,", "Tennessee", "\"release\" civilians,", "the chemical at the Qarmat Ali water pumping plant in southern Iraq shortly after the U.S. invasion in 2003.", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "Sleeping with the Past", "on the two tablets", "the pachytene stage of prophase I of meiosis", "the Great Chicago Fire", "4", "Edward III", "fourth", "Academy Award for Best Art Direction", "March 19, 1958", "abraham", "Bronx Park", "draft- horse breed", "November"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5449035396041009}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, true, true, false, true, true, false, false, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.9411764705882353, 1.0, 0.10526315789473685, 0.4444444444444445, 0.8, 0.2857142857142857, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.125, 1.0, 1.0, 0.0, 0.10526315789473684, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4615384615384615, 0.8750000000000001, 1.0, 0.11764705882352942, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.0, 1.0, 0.0, 0.23529411764705882, 1.0, 1.0, 0.10526315789473684, 0.9166666666666666, 1.0, 0.0, 0.7777777777777778, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-851", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-3049", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-7035", "mrqa_triviaqa-validation-1159", "mrqa_hotpotqa-validation-3183", "mrqa_hotpotqa-validation-3383", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1794", "mrqa_searchqa-validation-10510"], "SR": 0.40625, "CSR": 0.548828125, "EFR": 1.0, "Overall": 0.677109375}, {"timecode": 64, "before_eval_results": {"predictions": ["\"We Found Love\"", "The 19-year-old woman", "\"We have received three people from the blast at Rabbani's house. Among the injured are Masoom Stanikzai, one bodyguard and an assistant\" to Rabbani.", "40", "opened considerably higher Tuesday and saw an unprecedented wave of buying amid the elections.", "helped make the new truck safer,", "19", "(l-r) Paul McCartney, Yoko Ono Lennon, Olivia Harrison and Ringo Starr,", "great jazz music", "homicide.", "Sodra nongovernmental organization,", "on the family's blog", "computer-generated", "first", "the 12th on the Blue Monster course at Doral", "The three men loaded the paintings", "signed a power-sharing deal with the opposition party's breakaway faction,", "\"executed\" eight people on February 6 in the town of Rio Bravo because the Indians were gathering information about the rebels to give to the Colombian military.", "the Russian air force,", "Rod Blagojevich,", "Fiona MacKeown", "50", "the legitimacy of that race.", "President Obama", "John Lennon and George Harrison,", "aron Bialek", "1998.", "45 minutes, five days a week.", "Israel", "Monday.", "Frank Ricci,", "Sixteen", "international aid agencies", "EU naval force", "Al-Shabaab", "Daytime Emmy Lifetime Achievement Award", "since 1983.", "the foyer of the BBC building", "The UNHCR", "EU naval force", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "\"We don't see at this point any indication of an individual out in the neighborhoods committing additional crimes or homicides,", "that he knew the owner of the home, a Vietnam veteran who had given him permission to enter the house and take painkillers or other pills whenever he wanted.", "two", "6-2 6-1", "to the U.S. Consulate in Rio de Janeiro,", "John Demjanjuk", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "\"He is obviously very relieved and grateful that the pardon was granted,\"", "citizenship to a man because he allegedly forced his wife to wear a full Islamic veil,", "to secure more funds from the region.", "Clay Morrow", "Drew Barrymore", "difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "the board base for physically supporting and wiring the", "Richard Wagner", "H. H. Asquith", "Centers for Medicare and Medicaid Services", "J. Edgar Hoover", "Macomb County", "Custer", "Louis XIV", "a waterbed", "Risk"], "metric_results": {"EM": 0.46875, "QA-F1": 0.582080303955304}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.3636363636363636, 0.0, 1.0, 0.4, 0.5714285714285715, 1.0, 0.16666666666666669, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.07407407407407408, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.923076923076923, 0.0, 0.0, 0.3, 0.13333333333333333, 0.8571428571428571, 0.0, 0.0, 0.14285714285714288, 0.22222222222222224, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-4188", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-300", "mrqa_newsqa-validation-574", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-7443", "mrqa_triviaqa-validation-5934", "mrqa_hotpotqa-validation-2837"], "SR": 0.46875, "CSR": 0.5475961538461538, "EFR": 1.0, "Overall": 0.6768629807692308}, {"timecode": 65, "before_eval_results": {"predictions": ["North West England", "Carol Ann Duffy", "moth", "Fredric Warburg", "Battleship", "Hurricane Faith", "the Slavic women accompanying their husbands in the First Balkan War.", "Teutonic Knights", "9", "James Harrison", "Germany", "Jonathan Katz", "Ford Island", "2011", "NCAA Division I", "Tim Allen", "Latium", "Paul Avery", "Berea College", "Christopher Lloyd Smalling", "January 4, 1976", "an aerial maneuver", "1971", "Clovis I", "Tie Domi", "2007", "poet, and writer", "Quasimodo, the deformed bell-ringer of Notre Dame,", "Savin Yeatman-Eiffel", "Pieter van Musschenbroek", "23 July 1989", "actress", "Attorney General and as Lord Chancellor of England", "Socrates", "St Andrews, Fife, Scotland", "Henry Mills", "ribosomal RNA (rRNA) molecules", "Ronald Ryan", "A Hard Day's Night", "Humberside", "Dumfries and Galloway,", "\"A Charlie Brown Christmas\"", "from 1989 until 1994", "Cecily Legler Strong", "Polish", "Philip Aaberg", "in 2005", "Levon Helm", "Chengdu Aircraft Corporation", "White Knights of the Ku Klux Klan", "Reunited Worlds", "Lou Rawls", "the first year begins", "Michael English", "Parkinson's disease", "\"Explain\"", "Yeats", "Casa de Campo International Airport", "11", "any mom or sister worth her salt", "Carmen", "Pisa", "Massachusetts", "in July"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7154418498168498}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.7692307692307693, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-64", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-1244", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-1378", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-830", "mrqa_hotpotqa-validation-1433", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-4616", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-4711", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-6912", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-1180", "mrqa_newsqa-validation-271"], "SR": 0.609375, "CSR": 0.548532196969697, "EFR": 0.92, "Overall": 0.6610501893939394}, {"timecode": 66, "before_eval_results": {"predictions": ["Song of Songs", "alligator", "best pediatric centers", "a throwing game", "Ramona", "Tobacco Road", "The Honeymooners", "Opportunity seldom knocks twice", "Smokey Robinson", "a frog", "Gladiator", "combination colors", "ET tube", "Cairo", "The Cotton Club", "a sandstorm", "george Byron", "neutrino", "jodie Foster", "George Eliot", "clouds", "The Lost World", "The PCH", "Auschwitz-Birkenau", "China", "Uganda", "low cal", "Edward the Black Prince", "Garnet", "Bali", "Montmartre", "2004 Athens Games", "Elizabeth II", "kings", "Hollywood Ten", "take a small boat", "a jumper", "Jean", "Yogi Berra", "China", "Hawaii", "birds", "combination vision", "Turkey", "Delacorte", "head", "Vanessa Williams", "Buttercream", "potential energy", "the Byzantine Empire", "Delaware", "16 seasons", "photoelectric", "Moscow, Russia", "Joan Crawford", "Bassenthwaite Lake", "the moon passes directly between the sun and Earth", "Vanilla Air Inc.", "Jack Ridley", "diplomat", "Lion -- the motherless cub defended by Elphaba in \"Wicked.\"", "Six people", "attempted burglary", "missile"], "metric_results": {"EM": 0.4375, "QA-F1": 0.4763020833333334}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-995", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-2806", "mrqa_searchqa-validation-13562", "mrqa_searchqa-validation-14186", "mrqa_searchqa-validation-8242", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-14972", "mrqa_searchqa-validation-5294", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-10375", "mrqa_searchqa-validation-13713", "mrqa_searchqa-validation-16008", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-15880", "mrqa_searchqa-validation-16918", "mrqa_searchqa-validation-14516", "mrqa_searchqa-validation-488", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-1198", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-3230", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-15925", "mrqa_searchqa-validation-15274", "mrqa_searchqa-validation-9484", "mrqa_searchqa-validation-6543", "mrqa_searchqa-validation-14831", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-2870", "mrqa_triviaqa-validation-3020", "mrqa_hotpotqa-validation-684", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-5467", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-3829"], "SR": 0.4375, "CSR": 0.546875, "EFR": 0.9722222222222222, "Overall": 0.6711631944444445}, {"timecode": 67, "before_eval_results": {"predictions": ["Steven Spielberg", "Baton Rouge", "Wilbur Wright", "Charles Lindbergh", "George III", "Stephen Sondheim", "515 ft", "a calculating machine", "Bill Wyman", "the Surgeon", "T.S. Eliot", "lead", "Kevens Island", "French", "gravitational", "Fisherman\\'s Wharf", "Santa Fe", "Kenya Barris", "Sex Pistols", "chess", "Michael Jordan", "Roustabout", "doughboy", "Brge Rosenbaum", "Ali", "a rabbit", "Secretariat", "the soup stand", "a tooth", "citric", "Homer", "oar", "a woman scorned", "John Paul II", "Will Rogers", "Hairspray", "Orlando", "the pen of John", "Old Ironsides", "River Phoenix", "Sydney", "mutton", "royal icing", "Napoleon", "the flag of Mongolia", "Peter the Great", "a barn", "ibrik", "Missouri", "Sweeney Todd", "Paris", "1956", "Ritchie Cordell", "the town of Beaverton to be destroyed", "jujitsu", "Salvador Dali", "Jerry Zaks", "1993", "October 17, 2017", "from 1986 to 2013", "Afghanistan,", "a mammoth", "co-writing credits", "Gary Grimes"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6640625}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15468", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-6634", "mrqa_searchqa-validation-7771", "mrqa_searchqa-validation-5298", "mrqa_searchqa-validation-626", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7053", "mrqa_searchqa-validation-1250", "mrqa_searchqa-validation-2305", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-15818", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-1882", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-512", "mrqa_triviaqa-validation-6594", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-2152"], "SR": 0.609375, "CSR": 0.5477941176470589, "EFR": 0.96, "Overall": 0.6689025735294118}, {"timecode": 68, "before_eval_results": {"predictions": ["the \"Fisherman's ring", "Omaha", "Antwerp", "the Matterhorn", "Loch Lomond", "Alaska", "Frasier", "a temporary need", "Denmark", "the \"ball in tube\" or electromechanical crash sensor", "antonyms", "Pygmalion", "cholera", "\"E.E. Cummings\"", "Wilhelm Conrad Roentgen", "Kennedy Townsend", "\"Ento the lens\"", "the Green Hornet", "\"People, people who need\"", "yolk", "amniotic", "\"300\"", "Barry Levinson", "Cleopatra", "shepherd\\'s", "St. Petersburg", "Japan", "the Jordan River", "Derek Jeter", "Hans Christian Andersen", "an optional value", "\"Millions for defense, but not one cent for tribute\"", "\"The Tyger\"", "Percy Shelley", "a diamond", "carbon dioxide", "an earthquake", "Jr.", "Citizen Kane", "zero-g", "Mathew B. Brady", "Clinton", "the opponent\\'s court", "Tasmania", "Wyoming", "the Fellowship of the Ring", "the quick brown fox", "Denmark", "wheat", "\"Free Bird\"", "Chesapeake", "http://www.example.com/index.HTML", "presidential representative democratic republic", "the Pir Panjal Railway Tunnel", "Barcelona", "China", "Leander", "the Corps of Discovery", "Kingdom of Morocco", "September 6, 1998", "club managers,", "heavy flannel or wool", "several weeks,", "2011"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6223484848484848}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.8, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.36363636363636365, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16502", "mrqa_searchqa-validation-11862", "mrqa_searchqa-validation-16760", "mrqa_searchqa-validation-15586", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-12041", "mrqa_searchqa-validation-13013", "mrqa_searchqa-validation-7221", "mrqa_searchqa-validation-9738", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-16500", "mrqa_searchqa-validation-4154", "mrqa_searchqa-validation-12701", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-4617", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-15210", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-1342", "mrqa_searchqa-validation-2004", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8857", "mrqa_searchqa-validation-10953", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-15704", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-1848", "mrqa_hotpotqa-validation-3841"], "SR": 0.53125, "CSR": 0.5475543478260869, "EFR": 1.0, "Overall": 0.6768546195652174}, {"timecode": 69, "before_eval_results": {"predictions": ["KFC", "a dickson", "Follies", "Streisand", "Andrew Jackson", "Agamemnon", "spurs", "Edward Winslow", "\"Monkees come\"", "a canton", "Louisiana", "Mall", "percussus", "the pardon power", "Diana", "cherry", "Constellations", "Abraham", "Fox Network", "20", "Mendel", "Maria Callas", "Hulk Hogan", "Margaret Tobin", "a horse", "A Hard Day\\'s Night", "Making the Band", "Judy Garland", "Autumn in New York", "telephone operator", "Franklin D. Roosevelt", "William Shakespeare", "\"I Have No Mouth, and I Must\"", "La Salle", "lattice", "a penny", "Succotash", "the retina", "a prayer", "Lake Coeur d'Alene", "The Sopranos", "\"Hark\"", "Huguenots", "the Brooklyn Dodgers", "king", "yellow", "Mascara", "Rooster Cogburn", "ponderosa pine", "Homestead Act", "Lawrence Wien", "Kyrie Irving", "Mexico", "Aaron Harrison", "Crete", "Miles Morales", "Peter Nichols", "the Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide", "1858", "McComb, Mississippi", "Newcastle", "eight years", "\" Unfortunately, this is not an anomaly in Naples and in that neighborhood.\"", "mermaid"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6296296296296295}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 0.0, 0.2962962962962963, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1217", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-6437", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-3747", "mrqa_searchqa-validation-5203", "mrqa_searchqa-validation-843", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-10815", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-3221", "mrqa_searchqa-validation-5237", "mrqa_searchqa-validation-12476", "mrqa_searchqa-validation-9111", "mrqa_searchqa-validation-4606", "mrqa_searchqa-validation-705", "mrqa_searchqa-validation-5659", "mrqa_searchqa-validation-12330", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-6323", "mrqa_hotpotqa-validation-2762", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-2727"], "SR": 0.578125, "CSR": 0.5479910714285714, "EFR": 1.0, "Overall": 0.6769419642857143}, {"timecode": 70, "UKR": 0.72265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-890", "mrqa_hotpotqa-validation-947", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-777", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10000", "mrqa_searchqa-validation-10122", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11681", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-12727", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15925", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16953", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2004", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3655", "mrqa_searchqa-validation-3747", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-3869", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4642", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6910", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7212", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7221", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7593", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9643", "mrqa_searchqa-validation-9666", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10091", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10173", "mrqa_squad-validation-10216", "mrqa_squad-validation-10254", "mrqa_squad-validation-10328", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1075", "mrqa_squad-validation-1183", "mrqa_squad-validation-1319", "mrqa_squad-validation-1568", "mrqa_squad-validation-1681", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-2975", "mrqa_squad-validation-3139", "mrqa_squad-validation-3168", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3599", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4147", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5532", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5826", "mrqa_squad-validation-5922", "mrqa_squad-validation-5952", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6624", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7202", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-821", "mrqa_squad-validation-8253", "mrqa_squad-validation-8410", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9061", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9540", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1681", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5488", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-6565", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.857421875, "KG": 0.5234375, "before_eval_results": {"predictions": ["Polk", "a band", "delta", "baroque", "St. Petersburg", "Australia", "Prohibition", "Onomastic Sobriquets", "The Godfather", "Maria Sharapova", "Dairy Queen", "a boxer", "11", "\"The Stars and Stripes Forever\"", "Jackie Moon", "Pulp Fiction", "expunge", "the Rhine", "a bazooka", "dilithium", "Schwarzenegger", "Epstein-Barr virus", "helium", "Double time", "U.S. Naval Academy", "Iowa", "invocation", "a circle", "Pussycat Dolls Present", "Shakespeare", "a lump", "Vin Diesel", "Hitler", "Heath", "the Odyssey", "(Mark) Phelps", "Annapolis", "the Maccabees", "Rolls Royce", "a doses", "the Caucasus Mountains", "Lafayette", "a gopher", "Paul", "the Coca-Cola Company", "Warren", "apogee", "a bull", "a mirror", "david archuleta", "Union Carbide", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "Ireland", "1949", "March", "Stockholm syndrome", "Grassington", "Geographical Indication tag", "just off the northwest tip of Canisteo Peninsula in Amundsen Sea", "Voyager 2", "President Nicolas Sarkozy and Canada's Prime Minister Stephen Harper", "she was third after the short program -- 7.14 points behind Kim Yu-Na of South Korea and 2.42 points behind Mao Asada of Japan", "1000 square meters in forward deck space", "Larry Ellison,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6272726086048455}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, true], "QA-F1": [0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.5555555555555556, 0.2105263157894737, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16561", "mrqa_searchqa-validation-8833", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-9097", "mrqa_searchqa-validation-4798", "mrqa_searchqa-validation-6173", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-15266", "mrqa_searchqa-validation-958", "mrqa_searchqa-validation-8333", "mrqa_searchqa-validation-10678", "mrqa_searchqa-validation-11878", "mrqa_searchqa-validation-555", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-5212", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-11798", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-2363", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-4050", "mrqa_naturalquestions-validation-6489", "mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-5511", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-2486", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-1701"], "SR": 0.515625, "CSR": 0.5475352112676056, "EFR": 1.0, "Overall": 0.7302101672535211}, {"timecode": 71, "before_eval_results": {"predictions": ["Funki Porcini", "119", "560", "Nine Inch Nails", "Klasky Csupo", "the music genres of electronic rock, electropop and R&B", "the \"Home of the Submarine Force\"", "Luc Besson", "River Shiel", "\"Hush\u2026 Hush, Sweet Charlotte\"", "Lommel", "Harry Booth", "Westland", "1.23 million", "Boston, Massachusetts", "281", "Northern Ireland", "1916 Easter Rising", "Park Hyung-Sik", "Conservative Party", "quantum mechanics", "gamecock", "Theo James Walcott", "April 8, 1943", "\"Spoorloos, literally \"Traceless\" or \"Without a Trace\"", "their unusual behavior", "11", "Victoria Peak", "Taylor Swift's single \"Back to December\"", "850 saloon", "Hindi", "Statutory List of Buildings of Special Architectural or Historic Interest", "High Falls Brewery", "one child, Lisa Brennan-Jobs", "Frederick Louis, Prince of Wales", "Tomorrowland and Fantasyland", "Hindi", "Art Deco-style", "Jon Walker", "\"Big Blood\"", "Mani", "Green Chair", "Walker Smith Jr.", "Waimea Bay", "Juan Manuel Mata Garc\u00eda", "Umina Beach, New South Wales", "Mickey Mouse cup", "Kinnairdy Castle", "Antigua & Barbuda, Argentina, South Africa, Fernando P\u00f3, S\u00e3o Tom\u00e9, Madagascar, Mauritius,", "Stephen James Ireland", "Lola Dee", "1924", "the cell nucleus", "the inner core", "Helen of Troy", "Vince Cable", "30", "\"A good vegan cupcake has the power to transform everything for the better,\"", "Sporting Lisbon", "Illness", "programming", "bowling", "Gin Rummy", "enamel"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6792348710317461}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.125, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-1397", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-5147", "mrqa_hotpotqa-validation-1799", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-1037", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-2678", "mrqa_hotpotqa-validation-1716", "mrqa_naturalquestions-validation-2502", "mrqa_searchqa-validation-12240", "mrqa_searchqa-validation-3082", "mrqa_searchqa-validation-12267"], "SR": 0.578125, "CSR": 0.5479600694444444, "EFR": 1.0, "Overall": 0.730295138888889}, {"timecode": 72, "before_eval_results": {"predictions": ["U2 360\u00b0 Tour", "Brent Wilson was fired", "Peter Kay's Car Share", "Michael Crawford", "Brazilian Jiu-Jitsu", "2012 Olympic bronze medalist", "CMYKOG process", "Colonel", "the first month of World War I", "Germany", "River Clyde", "George Washington Bridge", "Argand lamp", "Lowestoft, Suffolk", "The Ethics of Ambiguity", "Bishop's Stortford", "Rural Electrification Act", "Vitor Vieira Belfort", "Carlos Coy", "Hawaii", "Cuban descent", "35,000 members", "24 NCAA sports", "the Bahamian island of Great Exuma", "Musicology", "Bonnie Franklin", "Cheshire League Premier Division", "Kelly Bundy", "Canada's first train robbery", "Carson City was the county seat of Ormsby County", "Ben R. Guttery", "director-General of the BBC", "Montreal", "New Zealand", "from August 14, 1848,", "Peel Holdings", "1692", "Indian epic historical drama film", "Teddy Riley", "672 km2", "140 million", "Lamar Hunt", "Vienna", "Alemannic", "San Antonio", "SpongeBob SquarePants 4-D", "Seti I", "\"Agent Vinod\"", "Joseph E. Grosberg", "January 15, 1975", "2015", "the true horrors of human history derive not from orc and Dark Lords, but from ourselves", "energy loss", "Gautamiputra Satakarni", "was the fourth son of King George III: Edward, duke of Kent.", "Audi", "Malta", "the most effective way to curb the drug trade was tackling the insurgency head-on.", "Heshmatollah Attarzadeh", "drug cartels", "New York City", "Turandot", "Champagne", "seabirds"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7142924783549784}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.09090909090909091, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-656", "mrqa_hotpotqa-validation-3344", "mrqa_hotpotqa-validation-1320", "mrqa_hotpotqa-validation-4088", "mrqa_hotpotqa-validation-1695", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-1260", "mrqa_hotpotqa-validation-4365", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-3483", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4489", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-6234", "mrqa_triviaqa-validation-6188", "mrqa_triviaqa-validation-2795", "mrqa_newsqa-validation-2194", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-3199"], "SR": 0.578125, "CSR": 0.5483732876712328, "EFR": 1.0, "Overall": 0.7303777825342466}, {"timecode": 73, "before_eval_results": {"predictions": ["Bathsheba", "Poland", "Hillary Clinton", "Hannibal", "Elysium", "Birmingham", "syndicate", "Hansel and Gretel", "J.M.W. Turner", "Heisenberg", "astronaut", "a glockenspiel", "David Hockney", "Kyoto Protocol", "alexandrina", "Croatia", "the Phillies", "South Carolina", "Survivor Series", "taxis", "peppers", "piscina", "Edward III", "Bruce Wayne", "lamps", "Tesco", "Cologne", "the northern prawn", "New York", "Nikola Tesla", "near field communication", "Tennessee", "Grimbsy", "New Cheshire Way", "Robert Guerrero", "Virginia Plain", "Columbia", "Scotland", "Freema Agyeman", "Spain", "a toilet", "alexandrina", "2011", "alexandrina", "World Heavyweight", "ArcelorMittal Orbit", "Madness", "Mao Zedong", "Sweet Home Alabama", "Greece", "Saints", "a valuable way to feed the poor, and would relieve some pressure of the land redistribution process", "required many hospitals, nursing homes, home health agencies, hospice providers, health maintenance organizations ( HMOs ), and other health care institutions to provide information about advance health care directives to adult patients", "biscuit", "High Knob", "Sacramento Kings", "200", "African National Congress Deputy President Kgalema Motlanthe,", "Michelle Obama", "near the village of Dara Bazar in the Bajaur Agency,", "a belfry", "Bret Maverick", "Ronald McDonald House", "#364"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5793087121212122}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, false, false, true, true, false, false, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3099", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-5341", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-5672", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-131", "mrqa_triviaqa-validation-3294", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-2277", "mrqa_triviaqa-validation-6598", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-1673", "mrqa_triviaqa-validation-4776", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-5719", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-4819", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-4414", "mrqa_hotpotqa-validation-5878", "mrqa_newsqa-validation-3358", "mrqa_searchqa-validation-16624", "mrqa_searchqa-validation-6760"], "SR": 0.515625, "CSR": 0.5479307432432432, "EFR": 0.967741935483871, "Overall": 0.7238376607454228}, {"timecode": 74, "before_eval_results": {"predictions": ["iPod Classic", "3-2", "Kiriakou said Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "lower house of parliament,", "Simon Cowell", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith.", "1983", "$40 and a loaf of bread.", "syria army's answer to the rebels has been to viciously attack civilians in the Ogaden,", "are \"active athletes,\" far from couch potatoes,", "pirates", "10,000", "his business dealings for possible securities violations", "former Procol Harum bandmate", "California, Texas and Florida,", "morphine sulfate oral solution", "\"it should stay that way.\"", "Iran", "and censorship remain rife across the Middle East and North Africa,", "Rawalpindi", "he walked up to a roadblock set up by a local anti-kidnapping task force", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "peanuts", "United States", "Samoa", "syria", "Six", "the Irish capital.", "customers", "At least 13", "Whitney Houston", "Ferraris", "free fixes for the consumer.", "nine-wicket", "10 below", "Madhav Kumar Nepal", "her landlord defaulted on the mortgage and the house fell into foreclosure.", "black is beautiful,", "fifth", "1979 Islamic Revolution.", "U.S. State Department and British Foreign Office", "JBS Swift Beef Company,", "Hurricane Gustav", "U.S. troops", "murder", "Manny Pacquiao", "The American Civil Liberties Union", "1,073 immigration detainees", "flying without a valid license,", "Alfredo Astiz,", "in Los Angeles", "Super Bowl VIII", "3", "Steve Ford", "Sharpening stones", "Edinburgh", "true", "Norbertine", "Anheuser-Busch", "Beauty and the Beast", "the Chiefs", "Cleopatra", "Philip", "Sparafucile"], "metric_results": {"EM": 0.5, "QA-F1": 0.6275222173659674}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, true, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6, 0.0, 0.0, 0.8, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727273, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4109", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2155", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-12", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-1514", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-1373", "mrqa_triviaqa-validation-3067", "mrqa_searchqa-validation-14806"], "SR": 0.5, "CSR": 0.5472916666666667, "EFR": 1.0, "Overall": 0.7301614583333333}, {"timecode": 75, "before_eval_results": {"predictions": ["1-1 draw", "Ten South African ministers and the deputy president", "Herman Cain", "finished last year after giving birth to baby daughter Jada,", "voluntary manslaughter", "The Kirchners", "Sharon Bialek", "Roy Foster", "March 24,", "African-Americans", "Bill Haas", "David Beckham", "Rwanda", "Caster Semenya", "nuclear weapon", "$627,", "Turkish President Abdullah Gul.", "People Against Switching Sides (PASS)", "sedative.", "26", "Bob Dole,", "White Hills, Arizona, near Hoover Dam.", "the assassination of President Mohamed Anwar al-Sadat", "Australia", "re-examine other regions where similar blowback might take place.", "Josef Fritzl", "Galveston, Texas, to Veracruz, Mexico,", "Sharon Bialek", "About 100,000 workers", "was separated", "San Simeon, California,", "\"Sesame Street\" Grover, how to make gnocchi with Mario Batali, and the ins and outs of prettying up your home with any number of programs on HGTV.", "Fiona Mac Keown", "2001", "Walt Disney Studios", "a violent government crackdown seeped out.\"", "9 percent", "Jezebel.com", "Kenner, Louisiana", "Saturday", "that the deadly attack on India's financial capital last month was planned inside Pakistan,", "in 2005", "35,000 kilometers", "Marco Polo", "stressed out.", "71 percent of Americans consider China an economic threat to the United States,", "\"Oprah: A Biography,\"", "the Bronx.", "President Idriss Deby hopes the journalists and the flight crew will be freed,", "in 1994.", "murder in the beating death of", "February 7, 2018", "1439", "Blue laws", "Siddhartha", "Prince Bumpo", "nitrogen", "Araminta Ross", "ABC1 and ABC2", "G\u00f6tene in Sweden", "the kitchen sink", "whey", "Benito Mussolini", "Anna Eleanor Roosevelt"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6006156629318394}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8235294117647058, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7777777777777778, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-801", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-4014", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-915", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-4012", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-406", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-9060", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-177", "mrqa_searchqa-validation-7856", "mrqa_triviaqa-validation-3733"], "SR": 0.46875, "CSR": 0.5462582236842105, "EFR": 0.9705882352941176, "Overall": 0.7240724167956657}, {"timecode": 76, "before_eval_results": {"predictions": ["cricket", "Alan Greenspan", "Bolivia", "Matalan", "Ub Iwerks", "Macbeth", "German Chancellor Angela Merkel", "Monopoly", "transsexual", "yellow", "skull, jaw, shoulder, rib cage, and pelvis", "doubles", "Paul Gauguin", "Ben Jonson", "convex quadrilaterals", "Willy Lott", "19-9", "Abu Dhabi", "london", "14", "ants", "palladium", "Hubble Space Telescope", "Van Allen Probes", "Rawalpindi", "Mexico", "Philip Glenister", "Miss Prism", "Emma Hamilton", "Beethoven", "Haystacks", "2016", "Margaret Thatcher", "Mauricio Pochettino", "USS Missouri", "Sensurround", "eAGLE", "Olympic Games", "blue", "Rihanna", "Tripoli", "euthanasia", "Eva Per\u00f3n", "Doctor Who", "pink", "lincoln Center Theater's Vivian Beaumont", "Illinois", "The Magnificent Seven", "Uranus", "typhoid fever", "Elvis Costello", "September 9, 2012", "Charles R Ranch, County Road 24, Las Vegas, New Mexico, USA", "seven", "Ricky Marco i Vives", "American", "Queens, New York", "Manchester United's perfect start to the English Premier League season came to a halt", "Damon Bankston", "intricate Flemish tapestries in an east-facing sitting room", "The Big Sleep", "Dairy Queen", "Paul the Apostle", "Confederate victory"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7291666666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-2144", "mrqa_triviaqa-validation-7293", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-2322", "mrqa_triviaqa-validation-6706", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-4855", "mrqa_triviaqa-validation-4595", "mrqa_triviaqa-validation-7636", "mrqa_triviaqa-validation-4242", "mrqa_triviaqa-validation-2047", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-6179", "mrqa_triviaqa-validation-890", "mrqa_hotpotqa-validation-3653", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2631", "mrqa_searchqa-validation-15341", "mrqa_naturalquestions-validation-767"], "SR": 0.671875, "CSR": 0.5478896103896104, "EFR": 0.9523809523809523, "Overall": 0.7207572375541125}, {"timecode": 77, "before_eval_results": {"predictions": ["nearly $162 billion in war funding", "poor.", "183", "\"I reject this course because it sets goals that are beyond what we can achieve at a reasonable cost, and what we need to achieve to secure our interests,\"", "\"I'm certainly not nearly as good of a speaker as he is.\"", "Transport Workers Union leaders", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "President Bush", "November 26,", "Wednesday.", "change course", "seven years' probation.", "Jackson sitting in Renaissance-era clothes and holding a book.", "off the coast of Dubai", "his past and his future", "Spc. Megan Lynn Touma,", "Hussein's Revolutionary Command Council.", "McDonald's' plans", "$1,500", "The noose incident", "in a tenement in the Mumbai suburb of Chembur,", "Monday.", "weren't taking it well.", "The island's dining scene", "22", "the foyer of the BBC building in Glasgow, Scotland", "Jenny Sanford", "The president, speaking at his palace in the capital, Mogadishu,", "Graham's wife", "Don Draper", "two", "ambassadors", "fritter his cash away on fast cars, drink and celebrity parties.", "Everton", "criminals", "The Ministry of Defense", "civilians,", "21-year-old", "Ignazio La Russa", "Passers-by", "Pixar's", "NATO fighters", "New York City Mayor Michael Bloomberg", "A Colorado prosecutor", "16 Indiana National Guard soldiers", "1940's", "A London, Ontario, hospital", "her husband", "South Africa", "\"I am sick of life -- what can I say to you?\"", "a tanker", "Rigor mortis is very important in meat technology", "Walter Pauk", "Louis Prima", "Alanis Morissette", "Uranus", "Margaret Thatcher", "February 16, 1944", "Fort Snelling, Minnesota", "Wilmette", "divisor", "Vermont", "the International Date Line", "Jay Van Andel"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6404011786824286}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, false, true, true, true], "QA-F1": [0.1818181818181818, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-162", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-9", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1145", "mrqa_naturalquestions-validation-2680", "mrqa_triviaqa-validation-6088", "mrqa_hotpotqa-validation-252", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-1874", "mrqa_searchqa-validation-11971"], "SR": 0.578125, "CSR": 0.5482772435897436, "EFR": 0.9629629629629629, "Overall": 0.7229511663105412}, {"timecode": 78, "before_eval_results": {"predictions": ["missing Florida toddler Caylee Anthony,", "the bombers", "\"He is a very special member of our family. We miss having his love and compassion in our home,\"", "Isabella", "5 1/2-year-old son, Ryder Russell,", "finance", "gun charges,", "forgery and flying without a valid license,", "can command a fine of $627,", "There's no chance of it being open on time.", "the peace with Israel", "two", "70,000", "rural California,", "Sabina Guzzanti", "650", "Expedia.", "i report form", "Virgin America", "The Italian government", "Arab Prince Ghazi bin Muhammad,", "helping on the sandbags lines", "four decades", "the captain of a nearby ship", "The e-mails", "papillomavirus", "helicopters and unmanned aerial vehicles", "two tickets to Italy", "Dr. Maria Siemionow,", "Hillary Clinton", "Barack Obama:", "NATO member states, Russia and India", "assassination of President Mohamed Anwar al-Sadat", "eradication of the Zetas cartel from the state of Veracruz, Mexico,", "June 6, 1944,", "Orbiting Carbon Observatory,", "black civil rights leaders and prominent Democrats", "the Airbus A330-200", "divorced Goldman and married a Brazilian lawyer.", "poems", "be silent.", "Sri Lanka", "Adidas,", "2,700-acre sanctuary in rural Tennessee.", "a drug lord with ties to paramilitary groups,", "can indeed help people with irritable bowel syndrome,", "martial arts", "a student who admitted to hanging a noose in a campus library,", "543 elected members, of which 58 are women.", "Barack Obama", "The Louvre", "Bay of Plenty, Taupo and Wellington", "The Beatles", "k\u0268mantsi ( enemy )", "johnson", "Count Basie Orchestra", "Billy Cox", "figure-eight variety", "Yoruba", "boxer", "a chrysanthemums", "John Wesley", "Colonel Sanders", "depicting multiple alternative realities rather than a novel"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5865685190413451}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 0.1904761904761905, 1.0, 0.0, 0.5, 1.0, 0.2857142857142857, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.75, 1.0, 1.0, 1.0, 0.5, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.25, 0.5, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.6956521739130436, 1.0, 0.5, 0.2222222222222222, 0.6666666666666666, 0.5, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3240", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-915", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2609", "mrqa_naturalquestions-validation-1636", "mrqa_naturalquestions-validation-74", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-5101", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-5784", "mrqa_hotpotqa-validation-2388", "mrqa_searchqa-validation-12910", "mrqa_naturalquestions-validation-2729"], "SR": 0.453125, "CSR": 0.5470727848101266, "EFR": 1.0, "Overall": 0.7301176819620253}, {"timecode": 79, "before_eval_results": {"predictions": ["`` Mercy Mercy Me ( The Ecology )", "1990", "Bill Irwin", "Hon July Moyo", "amino acids glycine and arginine", "38 - 7", "Agra Cantonment - H. Nizamuddin Gatimaan Express", "the Rolling Stones", "A Christmas Story", "In 2010", "in a 3 - 2 shootout", "in 1975", "the date on which the Constitution of India came into effect on 26 January 1950", "David Tennant", "the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "the Canadian Rockies continental divide east to central Saskatchewan", "Deuteronomy 5 : 4 -- 25", "the Executive branch and the Senate", "John Musker", "Andy Cole and Shearer", "each team", "in case of `` a national emergency created by attack upon the United States, its territories or possessions, or its armed forces", "Sauron", "February 1775", "Supplemental oxygen", "the level of the third lumbar vertebra, or L3, at birth", "'' is the slogan used to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "103", "two", "Reba McEntire and Linda Davis", "New Mexico", "Gamora on Ego", "In 1889", "SI joint", "cut off close by the hip", "Arkansas", "Mickey Rourke", "a circular movement of an object around a center ( or point ) of rotation", "Will Marshall -- Wesley Eure", "Number 4, Privet Drive, Little Whinging in Surrey, England", "Salisbury steak is a dish made from a blend of ground beef and other ingredients and is usually served with gravy or brown sauce", "Sunday evenings", "Charles R Ranch, County Road 24, Las Vegas, New Mexico, USA", "a major fall in stock prices", "April 1979", "Reverend J. Long", "2005", "`` save, rescue, savior ''", "as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "Mike Alstott", "Nicole Gale Anderson", "ten", "at the Battle of Edgehill", "James Taylor", "as the eastern border of the re-emerging sovereign Republic following the century of partitions", "goalkeeper", "the Emancipation Proclamation", "4.6 million", "executive director of the Americas Division of Human Rights Watch,", "Pope Benedict XVI", "Afghanistan", "Erie Canal", "the lion", "the Black Sea"], "metric_results": {"EM": 0.421875, "QA-F1": 0.580719482259469}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.1818181818181818, 1.0, 0.4, 0.6666666666666666, 0.0, 0.6666666666666666, 0.8695652173913044, 1.0, 0.7692307692307692, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.22857142857142856, 1.0, 0.6666666666666666, 0.0, 0.16666666666666666, 0.9302325581395349, 1.0, 0.1111111111111111, 0.5714285714285715, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.08695652173913045, 0.0, 1.0, 0.12500000000000003, 1.0, 1.0, 1.0, 0.5, 0.6956521739130435, 0.0, 1.0, 0.0, 0.4, 1.0, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-4108", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-3945", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-1979", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-2813", "mrqa_triviaqa-validation-752", "mrqa_triviaqa-validation-5693", "mrqa_hotpotqa-validation-4599", "mrqa_searchqa-validation-10525"], "SR": 0.421875, "CSR": 0.5455078125, "EFR": 0.918918918918919, "Overall": 0.7135884712837838}, {"timecode": 80, "UKR": 0.681640625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-849", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7167", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10000", "mrqa_searchqa-validation-10122", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12727", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12910", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15341", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3655", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-3869", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-555", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6910", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10091", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1568", "mrqa_squad-validation-1681", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3139", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3599", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7202", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9061", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9540", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1535", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.828125, "KG": 0.478125, "before_eval_results": {"predictions": ["prevent any contaminants in the sink from flowing into the potable water system by siphonage", "parthenogenesis", "late - September through early January", "Violante Visconti di Modrone", "Prem Lata Agarwal", "Cal", "1994", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas '', cf. English Andrew )", "Joe Pizzulo and Leeza Miller", "British Indian Association", "they signed with Simon Cowell's record label Syco Music", "Jenny Slate", "Gunpei Yokoi", "January 15, 2010", "the sixth season", "the leaves of the plant species Stevia rebaudiana", "Julie Deborah Kavner", "Vincent Price", "infection, irritation, or allergies", "Jewel Akens", "Watson", "March 11, 2018", "Sauron", "12", "Frank Langella", "Number 4, Privet Drive, Little Whinging in Surrey, England", "shared", "Southport, North Carolina", "John C. Reilly", "Wednesday, September 21, 2016", "the Washington metropolitan area", "the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho ( including the communities of Parma, Wilder, Greenleaf, and Notus )", "1885", "Kevin Kline", "Consular Report of Birth Abroad", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "Monk's Caf\u00e9", "Pittsburgh", "Spanish missionaries, ranchers and troops", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "butch", "Thorleif Haug", "directly elected", "1997", "Brooks & Dunn", "12 November 2010", "the homicidal thoughts of a troubled youth", "W. Edwards Deming", "convergent plate boundary", "Sylvester Stallone", "1967", "Harrods, London", "Fontane di Roma", "Steve Coogan", "Steven Weinberg", "Bay of Fundy", "Lincoln Riley", "The Rosie Show", "Tomas Olsson, the journalists' Swedish attorney.", "2,000 euros ($2,963)", "dishwasher", "letter", "(Albert) Einstein", "wheeze"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6171856998847927}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.1, 0.4, 1.0, 0.20000000000000004, 0.5, 0.4, 0.25, 0.33333333333333337, 0.967741935483871, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-9311", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-451", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2509", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-2842", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-5589", "mrqa_hotpotqa-validation-4160", "mrqa_newsqa-validation-2045", "mrqa_searchqa-validation-11496"], "SR": 0.515625, "CSR": 0.5451388888888888, "EFR": 0.967741935483871, "Overall": 0.700154289874552}, {"timecode": 81, "before_eval_results": {"predictions": ["Jason Momoa", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "1623", "March 14, 1942", "Lafayette", "April 3, 1973", "5 liters", "Diane", "13 to 22 June 2012", "2.5", "232", "in late November or early December", "Guwahati", "Nala", "1979 / 80", "2017", "Imperium R\u014dm\u0101num", "Leslie and Ben", "electron shells", "drizzle, rain", "compasses", "two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "1987", "in the eye", "Eagle Ridge Outdoor pool in Coquitlam, BC", "gave the German Cabinet -- in effect, Chancellor Adolf Hitler -- the power to enact laws without the involvement of the Reichstag", "Charlton Heston", "the fourth ventricle", "October 27, 2017", "John Cooper Clarke", "Saturday", "Cheryl Campbell", "Spanish", "0.1 to 0.3 mm", "January 4, 2016", "Jeff Bezos", "Erica Rivera", "sunny", "January 2, 1971", "Tracy McConnell", "gastrocnemius muscle", "Parker's pregnancy at the time of filming", "1 -- 3", "the Turco - Mongol Timurid dynasty of Central Asia", "1830", "Frank Oz", "Number 4, Privet Drive, Little Whinging in Surrey, England", "Celtic", "Guant\u00e1namo or GTMO", "Morgan Freeman", "two installments", "in the muscle tissue of vertebrates", "the toe-line", "synagogue", "Katarina Witt", "Bhaktivedanta Manor", "Lowe's", "Honduran", "Kenyan forces", "Columbian mammoth", "flee", "hateful", "churrasco", "the evaporator"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5706095553751804}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.375, 0.0, 0.5, 1.0, 0.5, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666667, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-2144", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-5214", "mrqa_naturalquestions-validation-6482", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5185", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-3302", "mrqa_hotpotqa-validation-5833", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-1512", "mrqa_searchqa-validation-7442", "mrqa_searchqa-validation-12256", "mrqa_searchqa-validation-12239"], "SR": 0.453125, "CSR": 0.5440167682926829, "EFR": 0.9714285714285714, "Overall": 0.7006671929442508}, {"timecode": 82, "before_eval_results": {"predictions": ["chisels", "Hans Christian Andersen", "swing", "Charles Lindbergh", "fructose corn syrup", "T.S. Eliot", "Superman Returns", "nokomis", "Yale", "tides", "The Nutcracker", "Over the hifls", "circumnavigate", "kennebunkport", "opar", "CFO", "the Cymric", "Spiced Rum", "Baroque", "pterodactyl", "licorice", "Count Dracula", "skating", "Sweden", "War", "Sarah Montana", "Van Allen", "Mitch McConnell", "bravery", "the gallbladder", "the Invisible Man", "the Himalayas", "Chile", "Sri Lanka", "the St. Valentine's Day Massacre", "Saturday Night Live", "Sayonara", "Oakland", "oregon e Giulietta", "E", "Andrew Johnson", "the knee", "NASA", "Gavin MacLeod", "a snake", "The Count of Monte Cristo", "Green Acres", "Equus", "Wyandotte County", "Cy Young", "Stephen Sondheim", "December 27, 2015", "digital transmission modes", "Andy Cole", "yellow", "Galileo Galilei", "oregon", "Afro-American", "Midtown Manhattan in New York City", "Awake", "sports cars", "The show which looks at how children as young as eight would cope without their parents for two weeks.", "Asian qualifying Group 2", "Washington"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6132291666666666}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.5, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.08, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5014", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16607", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-13546", "mrqa_searchqa-validation-11022", "mrqa_searchqa-validation-10399", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-795", "mrqa_searchqa-validation-11894", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-15074", "mrqa_searchqa-validation-8668", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-3227", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-1024", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-14521", "mrqa_searchqa-validation-14081", "mrqa_searchqa-validation-6416", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-2222", "mrqa_triviaqa-validation-1023", "mrqa_hotpotqa-validation-5173", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3226"], "SR": 0.515625, "CSR": 0.5436746987951807, "EFR": 1.0, "Overall": 0.7063130647590361}, {"timecode": 83, "before_eval_results": {"predictions": ["Coca-Cola", "Oklahoma", "a snout beetle", "Pippin", "Georgia", "Atlantic States", "a dugout", "cement", "heating", "(Tim) Robbins", "(James) Cooper", "Hannah and Her Sisters", "(Stine) Stine", "potato chips", "the Bay of Bengal", "the Clark bar", "o", "Dresden", "John Ashcroft", "Phil of the Future", "Newman", "California", "rings", "to pick up and hold until released", "(George) Eliot", "the Met Center", "The Twinkle Tales", "Tommy Allsup", "jaded", "Sgt. Pepper's Lonely Hearts Club Band", "palindrome", "a trapezoid", "Scrubs", "Henrik Ibsen", "Elizabeth", "Canticle", "Friedrich Nietzsche", "can we get along?", "Halloween Zone", "for the cold-blooded murder of the", "Tanzania", "Coca-Cola", "(Deepak) Chopra", "Rudolf Hess", "Siberia", "Rings Twice", "Sylvester Stallone", "(Sondheim) Rodgers", "Sketch", "safari", "Arkansas", "During his epic battle with Frieza", "in the middle of the eighth inning", "The stability, security, and predictability of British law and government", "Joseph Priestley", "Granada", "London", "1940s and 1950s", "Best Musical", "York County", "two", "Michelle Rounds", "Derek Mears", "Adidas"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6682291666666667}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12304", "mrqa_searchqa-validation-11933", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-16183", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-2282", "mrqa_searchqa-validation-8962", "mrqa_searchqa-validation-534", "mrqa_searchqa-validation-15025", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-16081", "mrqa_searchqa-validation-39", "mrqa_searchqa-validation-13348", "mrqa_searchqa-validation-16469", "mrqa_searchqa-validation-14197", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-15780", "mrqa_searchqa-validation-15171", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-13383", "mrqa_naturalquestions-validation-7702", "mrqa_naturalquestions-validation-7224", "mrqa_triviaqa-validation-2381", "mrqa_hotpotqa-validation-5309", "mrqa_newsqa-validation-490"], "SR": 0.59375, "CSR": 0.5442708333333333, "EFR": 1.0, "Overall": 0.7064322916666665}, {"timecode": 84, "before_eval_results": {"predictions": ["Portugal", "noreg", "a daisy", "stanley", "Belfast", "w W Jacobs", "the Kurguelen Island Group", "East of Eden", "John Buchan", "Doncaster Rovers", "Anne", "Edo", "9", "Supertramp", "Sky", "Joanne Harris", "an abacus", "Eriksson", "buffalo", "acceleration", "(Tom Baker)", "stanley", "the sun", "euporie", "White spirit", "an aglet", "lemurs", "Saskatchewan", "Fabio Capello", "Goofy", "cricket", "1973", "William Neil Connor", "Azerbaijan", "Logic", "Spain", "Ferdinand McLaren", "Inspector of Prisons", "Moulin Rouge", "golf", "a dog", "Devereux, 2nd Earl of Leicester", "Hamelin", "Prague", "George Osborne", "Oxygen", "Toyota", "a opossum", "HMS Amethyst", "hairdresser", "Antony", "Lali Lila", "Spektor", "San Francisco", "Deputy F\u00fchrer", "nearly 8 km", "Lake Buena Vista, Florida", "about 1,300 meters in the Mediterranean Sea.", "Afghanistan,", "Deutschneudorf,", "a chimp", "tin", "Wings of Desire", "absorbed the superhuman powers and the psyche of Carol Danvers"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5862723214285714}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8571428571428571, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.8, 0.8, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.25, 0.761904761904762]}}, "before_error_ids": ["mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-4980", "mrqa_triviaqa-validation-4027", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-2163", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-6937", "mrqa_triviaqa-validation-696", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-3431", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-6666", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4166", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-2685", "mrqa_newsqa-validation-796", "mrqa_searchqa-validation-1820", "mrqa_naturalquestions-validation-2309"], "SR": 0.484375, "CSR": 0.5435661764705882, "EFR": 0.9696969696969697, "Overall": 0.7002307542335116}, {"timecode": 85, "before_eval_results": {"predictions": ["the Grand Harbour", "Eurasia", "Rita Moreno", "California", "9,000", "My Beautiful Dark Twisted Fantasy", "secondary school study", "Shut Up", "Nazareth", "William Shakespeare", "Nic Cester", "Morocco", "Mountain goat", "Prince George's County", "Ariel Ram\u00edrez", "The Apple iPod+HP", "four months in jail", "Objectivism", "Mari Jiwe McCabe", "Rogue One: A Star Wars Story", "Stephen Crawford Young", "Peter Lawrence Buck", "The Three Caesars' Alliance", "Professor Frederick Lindemann, Baron Cherwell", "Indianola, Mississippi", "Alexandre Dimitri Song Billong", "Dutch", "Rabies", "Malayalam fantasy comedy", "Sim Theme Park", "Outside", "novelty songs", "Nationalism", "Nancy Dow", "the North Atlantic Conference", "Sarah Newlin", "Downtown", "Rockstar San Diego", "Strato of Lampsacus", "Columbus Crew SC", "Beno\u00eet Jacquot", "Manor of the More", "Anita Dobson", "500-room", "Rajmund Roman Thierry Pola\u0144ski", "the Space Shuttle \"Discovery\" on STS-51-C.", "\"Orchard County\"", "Saint Michael, Barbados", "Championnat National 3", "Oxford, UK", "Phil Silvers", "one person", "CH CO, or CH COO", "Gibraltar", "The Seine", "ferdinand", "daltonism", "Virgin America", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Zelaya", "a cause, principle, or system of beliefs", "Japan", "Stalin", "Rupert\\'s Land"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6402529761904762}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, false, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.28571428571428575, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4597", "mrqa_hotpotqa-validation-5200", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-5197", "mrqa_hotpotqa-validation-3156", "mrqa_hotpotqa-validation-2506", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-4781", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-2372", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-5371", "mrqa_hotpotqa-validation-1536", "mrqa_naturalquestions-validation-1202", "mrqa_triviaqa-validation-5716", "mrqa_searchqa-validation-4662", "mrqa_searchqa-validation-5450"], "SR": 0.578125, "CSR": 0.543968023255814, "EFR": 1.0, "Overall": 0.7063717296511627}, {"timecode": 86, "before_eval_results": {"predictions": ["the Joads, a poor family of tenant farmers driven from their Oklahoma home by drought", "the object is placed further away from the mirror / lens than the focal point", "any unfavourable and unintended sign ( including an abnormal laboratory finding ), symptom, or disease temporally associated with the use of a medicinal ( investigational ) product", "18 - season", "December 24, 1836", "the bank, rather than the purchaser, is responsible for paying the amount", "Carlos Alan Autry Jr.", "20 years from the filing date subject to the payment of maintenance fees", "the Italian mezzadria, the French m\u00e9tayage, the Spanish mediero, or the Islamic system of muqasat", "the 1940s", "American production duo The Chainsmokers", "Steve Russell", "the President pro tempore", "Donna", "the Dutch", "A turlough, or turlach", "San Francisco, California", "Elected Emperor of the Romans", "9 February 2018", "1996", "the 2013 -- 14 television season", "Paradise, Nevada", "supervillains who pose catastrophic challenges to the world", "two", "the Charbagh structure", "Elizabeth Dean Lail", "a password recovery tool for Microsoft Windows", "semi-autonomous organisational units within the National Health Service in England", "May 1979", "13 to 22 June 2012", "60", "electron donors", "the pouring rain", "lizards", "Abbot Suger", "the Mahalangur Himal sub-range of the Himalayas", "The Royalettes", "the winter solstice", "State Bar of Arizona", "Marie Fredriksson", "25 -- 30 \u00b0 C / km ( 28 -- 34 \u00b0 F / mi )", "2001", "Hellenion", "Urge Overkill", "2000", "blue", "the Mishnah", "1078", "April 1979", "around 1872", "Atlanta, Georgia", "glagolitic", "the US", "\"You're the) Devil in Disguise\"", "France", "American rapper", "November 10, 2017", "jobs up and down the auto supply chain:", "London", "France's famous Louvre", "arbutus", "Dragnet", "Harry Potter", "Fairfax County"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5531011574210103}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, false, true, true], "QA-F1": [0.14285714285714288, 0.0, 0.8333333333333333, 0.0, 1.0, 0.19999999999999998, 1.0, 0.5882352941176471, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 0.058823529411764705, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9523809523809523, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.15384615384615385, 0.6666666666666666, 1.0, 0.15384615384615383, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-916", "mrqa_triviaqa-validation-3891", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3197", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-3536", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-2614", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-1911"], "SR": 0.453125, "CSR": 0.5429238505747127, "EFR": 0.8857142857142857, "Overall": 0.6833057522577997}, {"timecode": 87, "before_eval_results": {"predictions": ["About $10 billion", "hank Moody", "Nafees A. Syed", "two pages -- usually high school juniors who serve Congress as messengers --", "Marcell J Hansen", "capital murder and three counts of attempted murder in the shootings at the University of Alabama in Huntsville last week.", "nuclear warheads", "California-based Current TV -- media venture launched by Clinton's former vice president, Al Gore.", "riders a 19th-century experience on cruises complete with the carnival-like sounds of the steam-whistle calliope.", "going somewhere very special, far away,", "allergen-free", "Piers Morgan Tonight", "Dubai", "Columbia, Illinois,", "jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "free services.", "Steven Chu", "Mark Obama Ndesandjo", "Hayden", "30,000", "Adam Lambert and Kris Allen,", "Ashura -- the 10th day of the month of Muharram", "more than $17,000", "250,000 unprotected civilians", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "the Juarez drug cartel.", "Muslim festival", "United States, NATO member states, Russia and India", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "his business dealings", "150", "cancer for several years.", "U.S. Chamber of Commerce", "a Muslim with Lebanese heritage, but her family is \"not defined by religion,\"", "the underprivileged.", "an open window", "Jiverly Wong,", "deciding the duties of the new prime minister has been a sticking point in the negotiations.", "bronze medal in the women's figure skating final,", "98 people,", "President Obama", "in late order, he had canceled the card, but not before his credit union checking account had been debited,", "Cologne, Germany,", "for former U.S. soldier Steven Green exhibited clear symptoms of acute stress disorder in Iraq and that a military psychiatric nurse-practitioner failed to diagnose the troubled infantryman and pull him out of combat.", "Arkansas", "five minutes before commandos descended from ropes that dangled from helicopters,", "legally be able to intervene in the case if it is transferred from a judge in the eastern city of Abeche,", "Mary Procidano,", "The son of Gabon's former president", "Samoa", "Derek Mears", "January to May 2014", "Frederick County", "RMS Titanic", "calcium carbonate", "the snow outside Mara\u2019s window slowed, spiky white stars melting into streaks on the pane.", "India", "Elisha Nelson Manning", "CBS", "\"Dr. Gr\u00e4sler, Badearzt\"", "a seal", "glaucoma", "Carl Sandburg", "Meriwether Lewis"], "metric_results": {"EM": 0.5, "QA-F1": 0.6021931168300653}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true], "QA-F1": [0.8, 0.0, 1.0, 0.3076923076923077, 0.4, 0.6153846153846153, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 0.26666666666666666, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.5, 1.0, 0.0, 1.0, 0.17647058823529413, 0.0, 0.625, 0.07692307692307691, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-3274", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-3260", "mrqa_newsqa-validation-1292", "mrqa_newsqa-validation-931", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-5141", "mrqa_hotpotqa-validation-3563"], "SR": 0.5, "CSR": 0.5424360795454546, "EFR": 1.0, "Overall": 0.7060653409090909}, {"timecode": 88, "before_eval_results": {"predictions": ["Caylee,", "two soldiers", "$22 million", "The federal officers' bodies", "\"We tortured (Mohammed al-) Qahtani,\"", "the area of the 11th century Preah Vihear temple", "police to question people if there's reason to suspect they're in the United States illegally.", "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "and Jquante Crews,", "France,", "poems", "in almost all [Middle East and North Africa] countries,", "Cleve Landsberg,", "20 feet above flood stage.", "Columbian mammoth", "the Afghan government.", "genocide, crimes against humanity, and war crimes.", "in July 1999,", "in the county jail in Spanish forks,", "304,000", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry", "28", "an independent homeland", "Miguel Cotto", "South Africa", "seven", "to best your own fuel economy achievements,\"", "returning combat veterans", "scientific reasons.", "dismissed all charges", "United States, NATO member states, Russia and India", "75 percent", "Ameneh Bahrami", "\" Reddit was able to force the House Budget Chair to reverse course -- shock waves will be felt throughout the establishment in Washington today, and other lawmakers will take notice.\"", "Ma Khin Khin Leh,", "12", "The Rosie Show", "The station", "Larry King", "The citizens won't accept if Ali Bongo wins, because that will mean the government stole the vote,\"", "in the Horn of Africa,", "Seoul.", "part of the proceeds", "to dream and celebrate,\"", "intravenous vitamin \"drips\"", "Donald Trump.", "Ciudad Juarez,", "Rolling Stone", "Indonesian", "Robert Park", "At least 14", "late 2018 or early 2019", "Graub\u00fcnden, in the eastern Alps region of Switzerland", "2,050 metres ( 6,730 ft )", "cutis anserina", "the Battle of Poitiers", "the Esmeralda's Barn night  club", "Belgian", "the Australian Football League (AFL) home and away season", "Richard L. Thompson", "a fisheye lens", "a clavichord", "pigeons", "John Knox"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5684794526214763}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, false, true, true, false, false, true], "QA-F1": [0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.08, 1.0, 1.0, 0.0, 1.0, 0.7142857142857143, 1.0, 0.0, 1.0, 0.0, 0.25, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.10256410256410256, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.4347826086956522, 0.7777777777777778, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-999", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-929", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-1631", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-4127", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-3922", "mrqa_newsqa-validation-3162", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1586", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-798", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-8794", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-277", "mrqa_hotpotqa-validation-342", "mrqa_searchqa-validation-14968", "mrqa_searchqa-validation-2827"], "SR": 0.453125, "CSR": 0.5414325842696629, "EFR": 0.9714285714285714, "Overall": 0.7001503561396468}, {"timecode": 89, "before_eval_results": {"predictions": ["Ariel Binns", "Ignazio La Russa", "collaborating with the Colombian government,", "collaborating with the Colombian government,", "potential revenues from oil and gas", "2001.", "$24.1 million,", "Two United Arab Emirates based companies", "Chadian President Idriss Deby", "FBI.", "two hunters -- one of whom witnessed the attack --", "U.S. State Department and British Foreign Office", "the hunt for Nazi Gold", "U.S. President-elect Barack Obama", "Web", "News of the World tabloid.", "Too many glass shards left by beer drinkers in the city center,", "not for sale,", "blind Majid Movahedi,", "for fear of losing their licenses to fly.", "Jenny Sanford,", "Kurdish militant group", "music, street dancing and revelry", "\"falling space debris,\"", "At Wilhelmina Kids,", "France", "581 points", "Robert Barnett,", "The Mexican military", "14", "Venezuela", "41,", "Wednesday at the age of 95.", "hopes the journalists and the flight crew will be freed,", "Iran could be secretly working on a nuclear weapon is a major development, but not one that should lead the U.S. to consider a military strike against the Tehran regime,", "\"illegitimate.\"", "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"", "twice", "Haeftling,", "Saturday.", "five minutes before commandos descended", "a severely disfigured woman", "cell phones are valuable contraband, fetching a greater asking price from convicts than some shipments of illegal drugs.", "homicide by undetermined means,", "forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "for the rest of the year", "July", "attacked L.K. Chaudhary, the chief executive of an Italian car parts manufacturing company.", "Dominic Adiyiah", "Glasgow, Scotland", "Caylee Anthony's", "2006 -- 03", "a 31 - member Senate and a 150 - member House of Representatives", "John Vincent Calipari", "New Zealand", "Jessica Simpson", "fox hunting", "Los Angeles", "Samsung Galaxy S6", "Hilux pickup truck", "eggshells", "Manchester", "Gertrude Stein", "Dick Van Dyke"], "metric_results": {"EM": 0.5, "QA-F1": 0.6270744239614369}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.125, 0.2857142857142857, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.05714285714285715, 1.0, 0.0, 0.0, 0.16666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 0.14634146341463414, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.19047619047619047, 0.4, 0.5217391304347826, 1.0, 0.6666666666666666, 0.42857142857142855, 0.0, 0.0, 0.0, 0.5, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1499", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-1684", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3562", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-1000", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-1533", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5380"], "SR": 0.5, "CSR": 0.5409722222222222, "EFR": 1.0, "Overall": 0.7057725694444443}, {"timecode": 90, "UKR": 0.6640625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-342", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-5005", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5362", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4845", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-7167", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11326", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12093", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15269", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15341", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15755", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-8553", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-8668", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1568", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2163", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.81640625, "KG": 0.5, "before_eval_results": {"predictions": ["eelagic", "peter the Great", "Stevie Wonder", "Sir Tom Finney", "Jackson Pollock", "Black Sea", "Jumping Jack Flash", "Joseph Priestley", "Dumbo", "New Zealand", "Call for the Dead", "Bill Bryson", "Hans Christian Andersen", "Christian Louboutin", "Vespa", "naked naked", "Laputa", "a Hungarian Horntail", "Jumanji", "Tramar Lacel Dillard", "at", "The Princess bride", "Word Options", "pig", "Dancing With the Stars", "Australia", "Leicester", "E.", "Andr\u00e9s Iniesta", "BATH, England", "1924", "a barred spiral galaxy", "Duty Free", "Mark Twain", "fruit", "carbon", "ffestiniog", "khalifa Abdullah", "dpurves Member", "Sergio Garc\u00eda Fern\u00e1ndez", "Chad", "Urien", "Amal Clooney", "E. Nesbit", "czar Alexander I of Russia,", "John F. Kennedy", "Sheree Murphy", "Jekyll", "the R34", "Yukon", "OUT MUTUAL FRIend", "Chlorofluorocarbons", "three levels", "the coffee shop Monk's", "Mani", "DreamWorks Animation", "Port Melbourne", "338", "Pakistan", "\"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\"", "the U.S. Marine Band", "Joe Biden", "Honor Daumier", "Austria"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5555253623188405}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.08695652173913043, 0.8, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3683", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-3154", "mrqa_triviaqa-validation-2698", "mrqa_triviaqa-validation-5650", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-982", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-3665", "mrqa_triviaqa-validation-3335", "mrqa_triviaqa-validation-181", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-268", "mrqa_triviaqa-validation-5252", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-2111", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-4011", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-1997", "mrqa_triviaqa-validation-6807", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-339", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-2687", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-724", "mrqa_searchqa-validation-6163", "mrqa_searchqa-validation-13048", "mrqa_searchqa-validation-8979", "mrqa_hotpotqa-validation-5203"], "SR": 0.4375, "CSR": 0.5398351648351649, "EFR": 0.9166666666666666, "Overall": 0.6873941163003663}, {"timecode": 91, "before_eval_results": {"predictions": ["Dumbo", "Ernest Hemingway", "Switzerland", "choisya ternata", "paul Drake", "england", "James I", "trapezium", "Canada", "my little 1937 Austin Seven Ruby Open Top Tourer", "seven", "Vancouver, Canada", "niger", "Switzerland", "the Union Gap", "england", "air Bud", "carol", "gin", "piano", "Dick Cheney", "tectonic uplift of land and volcanic eruptions", "Virginia", "menstrual cramps", "pasta joke", "witch trials", "sailor", "my Favorite Martian", "plutocracy", "Bahrain", "geographical", "Ace of Spades", "Frank Warner", "China", "doxycycline", "Venice", "n Zealand", "1973", "paul's", "in full Laurence Kerr Olivier,", "d.offical", "popes", "Jimmy Carter", "Bradley Walsh", "Argentina", "Genesis", "special sauce", "Nikita Khrushchev", "iron", "john Peel", "Sheffield Wednesday", "May 2017", "cytokinesis", "Morgan Freeman", "Mike Fiers", "non-alcoholic", "politician", "cars, drink and celebrity parties.", "calling on NATO to do more to stop the Afghan opium trade", "Trevor Rees,", "edvard gRIEG", "Buddhism", "(Thomas Francis Eagleton)", "water"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5625473484848484}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.2727272727272727, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6589", "mrqa_triviaqa-validation-7166", "mrqa_triviaqa-validation-61", "mrqa_triviaqa-validation-3421", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-4024", "mrqa_triviaqa-validation-6471", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-3775", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-5155", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-7582", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-7584", "mrqa_triviaqa-validation-742", "mrqa_triviaqa-validation-7242", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-7129", "mrqa_triviaqa-validation-637", "mrqa_triviaqa-validation-4722", "mrqa_triviaqa-validation-6212", "mrqa_naturalquestions-validation-9155", "mrqa_hotpotqa-validation-2210", "mrqa_hotpotqa-validation-2228", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2183", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-11019"], "SR": 0.515625, "CSR": 0.5395720108695652, "EFR": 0.967741935483871, "Overall": 0.6975565392706873}, {"timecode": 92, "before_eval_results": {"predictions": ["raping and killing a 14-year-old Iraqi girl.", "Karen Floyd", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "Krishna Rajaram,", "U.S. President-elect Barack Obama", "Courtney Love,", "Mahmoud Ahmadinejad", "a gym", "California-based Current TV", "gun", "E! News", "1983", "beaches.", "tennis", "we", "Twelve", "Max Foster,", "military trial system", "Iran of trying to build nuclear bombs,", "A Lion Among Men.", "as though there was an opportunity for intervention,\"", "surge", "to clean up Washington State's decommissioned Hanford nuclear site,", "delivers a big speech", "The island's dining scene", "Saturday,", "warns business owners to close their shops during daily prayers,", "Bismarck,", "gasoline", "they did not receive a fair trial.", "2.5 million copies,", "abducting each other for ransoms or retribution.", "Egypt", "1:55 a.m. PT", "Barack Obama's", "iTunes", "under president Robert Mugabe", "her boyfriend,", "12.3 million", "ties", "three", "to encourage readers to get involved in service and volunteerism in their communities.", "75 percent", "Hu Jintao", "1,500", "can be volatile and dangerous.", "two counts of murder.", "young self-styled anarchists", "first grand Slam,", "Los Angeles'", "Alberto Espinoza Barron,", "2013", "Vincenzo Peruggia", "1979", "taking of Pelham One Two Three", "hercule poirot", "1997", "Debbie Reynolds", "43rd", "USS Essex", "Daylight Saving Time", "Amelia Earhart", "uranium", "Pakistan"], "metric_results": {"EM": 0.546875, "QA-F1": 0.7044990344622697}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.9411764705882353, 1.0, 1.0, 0.6666666666666666, 0.8235294117647058, 0.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 1.0, 0.8571428571428571, 0.4444444444444445, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-45", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-2917", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1516", "mrqa_naturalquestions-validation-834", "mrqa_triviaqa-validation-4531", "mrqa_hotpotqa-validation-1316", "mrqa_searchqa-validation-3479"], "SR": 0.546875, "CSR": 0.5396505376344086, "EFR": 1.0, "Overall": 0.7040238575268817}, {"timecode": 93, "before_eval_results": {"predictions": ["Fabio Simplicio", "They are co-chairs of the Genocide Prevention Task Force.", "March 8", "\"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "Kearny, New Jersey.", "\"I haven't seen any violence. I know [Wimunc's husband] was not living here anymore, but that's all I know,\"", "five minutes before commandos descended", "Democratic", "Kim Il Sung", "Roy Foster's", "South African's", "managing his time.", "U.S. Consulate in Rio de Janeiro,", "Majid Movahedi,", "Molotov cocktails, rocks and glass.", "Facebook and Google,", "The 725-mile Veracruz regatta", "These intravenous vitamin \"drips\" are part of the latest quick-fix, health fad catching on in Japan:", "Oxbow,", "Sarah Brown's", "food plant heavily damaged", "summer", "Charles Jubert,", "Friday,", "\"a whole new treasure trove of fossils\"", "The plane had a crew of 14 people and was carrying an additional 98 passengers,", "producing rock music with a country influence.", "more than 78,000 parents of children ages 3 to 17.", "\"I think I think everything he has said about the tech world being a meritocracy. Lots of people believe it.", "Republican", "the children of street cleaners and firefighters.", "March 24,", "North Korea,", "Mexican military", "Bill Haas", "devoted to federal ocean planning.", "An undated photo of Alexandros Grigoropoulos,", "Somalia's piracy problem was fueled by environmental and political events.", "Swat Valley.", "Japanese Foreign Ministry spokesman Hidenobu Sobashima", "a crew of Grayback forest-firefighters", "Steven Gerrard", "three", "Olympic medal", "having an affair with a woman in Argentina.", "the Beatles", "At least 88", "the Illinois Reform Commission", "the use of torture and indefinite detention", "\"Watchmen\" (No. 4)", "severe flooding", "Pastoral farming", "Kaley Christine Cuoco", "Hermann Ebbinghaus", "Route 66", "wagner", "Black Wednesday", "Cambridge University", "early 20th-century", "American tour", "Mickey Mouse", "The Daiquiri Diving", "Israel", "UNESCO / ILO"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7108731944669444}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.05405405405405406, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.6666666666666666, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.21428571428571427, 0.0, 0.7777777777777777, 0.3846153846153846, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.22222222222222224, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 0.5, 0.18181818181818182, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1910", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3870", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-4092", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3303", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-3157", "mrqa_hotpotqa-validation-3326", "mrqa_searchqa-validation-6", "mrqa_searchqa-validation-343"], "SR": 0.59375, "CSR": 0.5402260638297872, "EFR": 1.0, "Overall": 0.7041389627659574}, {"timecode": 94, "before_eval_results": {"predictions": ["The gunmen also took hostage Lunsmann's 14-year-old son, Kevin,", "23-year-old", "Paul McCartney and Ringo Starr", "Thirty to 40", "Larry Ellison,", "in the U.S.", "Alberto Espinoza Barron,", "review their emergency plans", "4,000", "Whitney Houston", "Marines", "Lillo Brancato Jr.", "United States, NATO member states, Russia", "cash away on fast cars, drink and celebrity parties.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "riders love the trip route, which winds through the Rockies and climbs to 9,000 feet.", "$10 billion", "export value of this year's poppy harvest stood at around $4 billion,", "Casa de Campo International Airport in the Dominican Republic", "Pakistan's", "her dancing against a stripper's pole.", "First Stop Resource Center and Housing Program", "CNN's Campbell Brown", "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "Iraqi Prime Minister Nouri al-Maliki", "Alfredo Astiz,", "Senate Sotomayor,", "Majid Movahedi,", "majority leader.", "three", "1960", "he would pay for these programs by ending the war in Iraq, reducing government waste, charging polluters for greenhouse gas emissions and ending the Bush tax cuts for wealthy individuals.", "education to kids at no cost to the poorest host-country governments.", "Teen Patti\"", "Larry Zeiger", "Willem Dafoe", "public endorsement", "no other designer has had a greater impact, not only on the way American men and women dress", "Barack Obama sent a message that fight against terror will respect America's values.", "2005", "U.S. helicopter", "(l-r)", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Ghana", "Christopher Savoie", "246 passengers and most of the crew aboard the MS Columbus, currently at the start of an around-the-world cruise,", "London's Waterloo Bridge", "Chievo", "in the last few months,", "84-year-old", "a nuclear weapon", "2020 National Football League ( NFL ) season", "John Cooper Clarke", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "Chicago's", "kolkata", "painter", "MGM Grand Garden Special Events Center", "Royal Navy", "England national team", "Bangkok", "roof", "cana", "KXII"], "metric_results": {"EM": 0.5, "QA-F1": 0.6421336917158929}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, true, false, false, false, false, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true], "QA-F1": [0.6956521739130436, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 0.14285714285714288, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.08, 0.0, 0.5, 0.5714285714285715, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.0, 0.1212121212121212, 1.0, 1.0, 0.125, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9090909090909091, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-985", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-768", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-2219", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1781", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-640", "mrqa_newsqa-validation-788", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-1131", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-4069"], "SR": 0.5, "CSR": 0.5398026315789474, "EFR": 0.96875, "Overall": 0.6978042763157895}, {"timecode": 95, "before_eval_results": {"predictions": ["Dano-Nor Norwegian author Aksel Sandemose", "pubs, bars and restaurants", "Retina display", "Chick tract of the same name", "Thomas Jane", "second cousin", "defender", "Graffiti", "ARY Digital Network", "counter-steering", "a trio with his younger brothers Steve and Rudy", "1960s", "Key West, Florida", "its riverside location", "Scottish", "Skipton", "Atlas ICBM", "PPG Paints Arena", "Kelly Bundy", "John Joseph Travolta", "Love Letter", "SBS", "AVN Adult Entertainment Expo", "119", "50 million", "Intelligent Design", "Sinngedichte", "Hidden America with Jonah Ray", "Irish", "Metro-Goldwyn-Mayer", "RAF Tangmere, West Sussex", "The Summer Olympic Games", "1692", "Art Deco-style skyscraper", "American professional baseball left fielder", "25 November 2015", "Hyuna", "Mathieu Kassovitz", "Kentucky Music Hall of Fame", "Ray Teal", "Danish", "Hindi", "two", "McComb, Mississippi", "D\u00e2mbovi\u021ba River", "24800 mi", "saint", "143,007", "Michael Edward \" Mike\" Mills", "Edward Vincent Sullivan (September 28, 1901 \u2013 October 13, 1974) was an American television personality, sports and entertainment reporter", "fourth", "2001", "a single, implicitly structured data item", "Strabo", "Roberta Flack", "Sir Walter Scott", "Braille", "Caster Semenya", "diplomat in northwest Pakistan", "20,000-capacity O2 Arena.", "to be drummed out", "Wizard for Hire", "the Erie Canal", "Andy Warhol"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7388392857142857}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, false, false, true, false, true, true], "QA-F1": [0.5714285714285715, 1.0, 1.0, 0.28571428571428575, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.09523809523809525, 0.6666666666666666, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1777", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-4869", "mrqa_hotpotqa-validation-46", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-1221", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-2975", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-3492", "mrqa_hotpotqa-validation-2013", "mrqa_naturalquestions-validation-2956", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-3651", "mrqa_searchqa-validation-3482"], "SR": 0.609375, "CSR": 0.54052734375, "EFR": 1.0, "Overall": 0.70419921875}, {"timecode": 96, "before_eval_results": {"predictions": ["Black Abbots", "Audrey Mossom", "Nicolas Winding Refn", "PBS", "Ed Lee", "James Victor Chesnutt", "Lu\u00eds Carlos Almeida da Cunha", "South Asia and the Middle East", "Martin Ingerman", "Rice University", "I", "Tom Rob Smith", "Reich Chancellery", "Kim So-hyun", "pubs, bars and restaurants", "Brad Pitt", "Lombardy", "in 1885", "Pac-12 Conference", "Lamar Wyatt", "Homer Hickam, Jr.", "Straits of Gibraltar", "Port Clinton", "Patrick Dempsey", "Seattle", "Taylor Swift", "Appleby-in-Westmorland", "2000", "Mickey's Christmas Carol", "The American relay of Michael Phelps, Ryan Lochte, Peter Vanderkaay", "Orson Welles", "in 1902", "Brittany Snow", "Lapland", "Elvis' Christmas Album", "CD Castell\u00f3n", "Coronation Street", "Zimbabwe", "Lifestyle Cities", "Jack Elam", "1998", "Johnny Cash, Waylon Jennings", "Brad Silberling", "ITV", "1998", "the Marx Brothers film", "Kentucky", "Raabta", "Michael Jordan", "Harrods", "2007", "January 2004", "obliquely", "left - sided heart failure", "Karl Marx", "Laurence Olivier", "Indian Ocean", "Rodong Sinmun", "two-day,", "1,900-acre YFZ ranch,", "Vatican City", "a porch", "Eric Knight", "United Kingdom and Commonwealth countries"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6704387626262626}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 0.0, 1.0, 1.0, 0.4444444444444445, 0.5, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-1854", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-2033", "mrqa_hotpotqa-validation-2029", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-4948", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-1932", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-1006", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-3381", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7760", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-776", "mrqa_searchqa-validation-69", "mrqa_naturalquestions-validation-9071"], "SR": 0.5625, "CSR": 0.5407538659793815, "EFR": 1.0, "Overall": 0.7042445231958763}, {"timecode": 97, "before_eval_results": {"predictions": ["Louisiana", "poker", "Budapest", "hoppin' John", "capuchin", "bass", "El Cid", "Vestal Virgins", "contract", "Akihito", "lead", "Israel", "Matthew", "Nancy Astor", "imperative mood", "a bald eagle", "a bird of prey", "Bergen", "a leap year", "Little Miss Muffet", "aila", "The Hague", "Zyrtec", "Buddhism", "Carson City", "Syria", "Cherry, Cherry", "the Council of Better Business Bureaus", "Linda Tripp", "a car", "Aqua Teen Hunger Force", "the James Webb Space Telescope", "economics", "Korean War", "diseases", "Rocky Mountain Fever", "euros", "Lebanon", "typewriters", "Isadora Duncan", "Jaws 2", "George Armstrong Custer", "nag", "Homer", "Motor Trend", "the Pipeline and Hazardous", "Staten Island", "Naxos", "steel", "Xaymaca", "St. Elsewhere", "Lord Banquo", "12951 / 52 Mumbai Rajdhani Express", "Paul Lynde", "John Part", "March 10, 1997", "Hubble Space Telescope", "\"Household Words\"", "Rockland County", "Trilochanapala", "Col. Sansern Kaewkumnerd said the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "Turkey", "security on the streets, with backing from U.N. peacekeepers.", "Quintero"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7010664682539682}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6111111111111112, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1614", "mrqa_searchqa-validation-1155", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-16033", "mrqa_searchqa-validation-1443", "mrqa_searchqa-validation-2127", "mrqa_searchqa-validation-4397", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-12404", "mrqa_searchqa-validation-2730", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-10588", "mrqa_searchqa-validation-6714", "mrqa_searchqa-validation-9423", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-2784", "mrqa_searchqa-validation-16039", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-2927", "mrqa_naturalquestions-validation-6519", "mrqa_triviaqa-validation-3516", "mrqa_hotpotqa-validation-2278", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-2757", "mrqa_triviaqa-validation-5852"], "SR": 0.609375, "CSR": 0.5414540816326531, "EFR": 1.0, "Overall": 0.7043845663265306}, {"timecode": 98, "before_eval_results": {"predictions": ["Scary Movie 2", "Mall", "piety", "TIME", "the Annunciation", "the Thames", "Alyssa Milano", "drowsiness", "a crocus", "Alaska", "Yellowstone", "Duchamp", "Little Red Riding Hood", "New York", "the tongue", "the English Channel", "Michelin", "an event", "Simple Simon", "hot chocolate", "vibrations", "a metronome", "a hard disk", "the Phillie Phanatic", "GILBERT & SULLIVAN", "Pringles", "Blondes", "a Stratocaster", "anchors", "Romeo", "a SLR", "Pamela Anderson", "Trampoline", "\"King of the Hill\"", "Bahamas", "Tiger Woods", "dark places", "Elton John", "the Sphinx", "Toy Story", "lump", "density", "hockey", "Heather Locklear", "the Pong", "\"spokes\"", "the Holy Grail", "a Crone", "the pope", "whole hog", "Target", "Left Behind", "71 -- 74 \u00b0 C", "the Devastator", "cotton", "Mary Decker of the USA", "(Donatello) Maggiore", "1939", "Leafcutter John", "University of Vienna", "Acura NSX", "80 percent", "Michael Jackson", "north"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7479910714285714}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9200", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-11345", "mrqa_searchqa-validation-13276", "mrqa_searchqa-validation-10245", "mrqa_searchqa-validation-8717", "mrqa_searchqa-validation-136", "mrqa_searchqa-validation-8288", "mrqa_searchqa-validation-15535", "mrqa_searchqa-validation-15136", "mrqa_searchqa-validation-416", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-8451", "mrqa_searchqa-validation-13668", "mrqa_searchqa-validation-85", "mrqa_naturalquestions-validation-134", "mrqa_triviaqa-validation-7303", "mrqa_triviaqa-validation-6743", "mrqa_hotpotqa-validation-537", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-1683"], "SR": 0.65625, "CSR": 0.5426136363636364, "EFR": 0.9545454545454546, "Overall": 0.6955255681818182}, {"timecode": 99, "UKR": 0.70703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-154", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1673", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4845", "mrqa_hotpotqa-validation-5005", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5362", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4845", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8734", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2662", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3434", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11326", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-12093", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12845", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14027", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15269", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15755", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16039", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6260", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8234", "mrqa_searchqa-validation-8339", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-8668", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_searchqa-validation-999", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1568", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3353", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6448", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7028", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7166", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-959"], "OKR": 0.83984375, "KG": 0.515625, "before_eval_results": {"predictions": ["file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Bollywood-produced \"Teen Patti\"", "Miami Beach, Florida,", "Kevin Evans", "sixth world title", "the Sri Lankan cricket team in the Pakistani city of Lahore.", "\"The situation is pretty much resolved,\"", "Addis Ababa,", "Saturn", "piano", "China and Japan.", "two years,", "\"We cannot receive the help by plane,\"", "$17,000", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "Haleigh Cummings,", "The sailboat, named Cynthia Woods,", "telling CNN his comments had been taken out of context.", "violent separatist campaign", "Michoacan Family,\"", "two", "exotic sports", "Bryant Purvis", "role as a bride in the 2007 movie \"License to Wed\"", "Kurt Cobain,", "Janet Napolitano", "Gary Brooker", "E. coli bacteria", "July", "engineering and construction", "Sri Lanka,", "a one-shot victory in the Bob Hope Classic on the final hole", "Dubai", "U.S. Vice President Dick Cheney", "Rwanda", "Tehran,", "France's famous Louvre", "Daytime Emmy Lifetime Achievement Award.", "money or other discreet aid for the effort if it could be made available,", "checkposts and military camps in the Mohmand agency,", "Somalia's piracy problem was fueled by environmental and political events.", "five", "Monday.", "start a dialogue of peace based on the conversations she had with Americans along the way.", "compromise the public broadcaster's appearance of impartiality.", "President Obama", "The Ministry of Defense", "Wednesday.", "Osama bin Laden's sons", "Michael Arrington,", "Lexus, Lincoln, Infiniti", "1973", "1975", "1546", "Kaiser Chiefs", "rivers", "Ecuador", "Afghanistan", "University of North Staffordshire", "Nelson County,", "the bull", "Jean-Michel Basquiat", "the Coast Guard", "silicon oxide"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7023046398046399}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, true, false, true, false, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false], "QA-F1": [0.0, 0.5714285714285715, 1.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.33333333333333337, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4444444444444445, 1.0, 0.8, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 0.0, 1.0, 1.0, 0.0, 0.1, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-1822", "mrqa_newsqa-validation-2895", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-653", "mrqa_newsqa-validation-2968", "mrqa_hotpotqa-validation-3644", "mrqa_searchqa-validation-14651", "mrqa_searchqa-validation-13028"], "SR": 0.59375, "CSR": 0.5431250000000001, "EFR": 1.0, "Overall": 0.721125}]}