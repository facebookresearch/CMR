{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.95,beta=0.5,gamma=0.8', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.95,beta=0.5,gamma=0.8/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.95,beta=0.5,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.95,beta=0.5,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5210, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["UK", "cnidarians", "seven", "religious", "Mnemiopsis", "TEU articles 4 and 5", "Denver", "public-key encryption", "1939", "communications between Yuan dynasty and its ally and subordinate in Persia, the Ilkhanate", "Christian", "a painting", "2012", "Bruno Mars", "economic liberalism", "Einstein", "The Shah's army was split by diverse internecine feuds", "Lunar Roving Vehicle", "SoCal", "Block II spacesuit", "Dr. George E. Mueller", "a multi-party system", "attacks on teachers in Welsh schools which reached an all-time high between 2005 and 2010", "cartels", "Tony Hawk", "Broncos", "Stromatoveris", "Sonderungsverbot", "Vince Lombardi Trophy", "the need for alliances", "1525\u201332", "wealth", "a phylum of animals that live in marine waters worldwide", "The United Methodist Church", "A piece of paper", "concrete", "one", "opportunistic", "Duke Kent- Brown", "1910\u20131940", "Museum of Manufactures", "Downtown Los Angeles", "south", "1980s", "the Main Quadrangles", "the government and the National Assembly and the Senate", "48 hours", "Edgar Atheling", "Rhine knee", "materia medica", "three years", "An attorney", "11 million", "The Scotland Act 1998", "24 March 1879", "Albert Einstein", "an attack on New France's capital, Quebec", "the Common Core", "1851", "office of Patriarch", "life expectancy", "seven", "Jonathan Stewart", "Manakin Town"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8046130952380952}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.09523809523809525, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9086", "mrqa_squad-validation-8065", "mrqa_squad-validation-166", "mrqa_squad-validation-664", "mrqa_squad-validation-7400", "mrqa_squad-validation-6163", "mrqa_squad-validation-8316", "mrqa_squad-validation-1886", "mrqa_squad-validation-2513", "mrqa_squad-validation-4534", "mrqa_squad-validation-2476", "mrqa_squad-validation-6945", "mrqa_squad-validation-2634", "mrqa_squad-validation-2314"], "SR": 0.78125, "CSR": 0.78125, "EFR": 0.9285714285714286, "Overall": 0.8549107142857143}, {"timecode": 1, "before_eval_results": {"predictions": ["necessity", "The BBC was given the highlights of most of the matches, while BSkyB paying \u00a3304m for the Premier League rights", "locomotion", "Albert of Mainz", "nine", "bachelor's degree", "Duisburg", "Latin Mass", "#P", "soap sponge", "semantical problems", "O(n2)", "force", "Bermuda 419 turf", "1975", "Sunday Night Movie", "Chicago Theological Seminary", "The Daleks", "comedies and family-oriented series", "the Amboise plot of 1560", "Chinatown", "half", "2,000", "$2 million", "public official", "Gods of Egypt", "the next day", "six", "turbine type", "late 14th-century", "Wittenberg", "Stanford", "427,652", "four days", "Thomas Edison", "1979", "a tool of the devil", "high voltage", "warmest months from May through September, while the driest months are from November through April", "Dublin", "Madison Square Garden", "Catholic dogma of the time", "domestic Islamists", "1891", "God", "Henry Cole", "the Marburg Colloquy", "esoteric", "eastern", "The special was one of several special 3D programmes the BBC produced at the time, using a 3D system that made use of the Pulfrich effect requiring glasses with one darkened lens", "T. J. Ward", "George Stephenson", "wiring the walls of a schoolroom and, \"saturating [the schoolroom] with infinitesimal electric waves vibrating at high frequency.\"", "\"blurring of theological and confessional differences in the interests of unity.\"", "cantatas", "a Qutb", "John Fox", "feigned retreat to break enemy formations and to lure small enemy groups away from the larger group", "plants that are biologically contained", "the Court of Justice", "296", "it was made by this man who died in 1933", ",,, named after the Dutch explorer Adriaen Block.... The claimed territories extended from the Delmarva Peninsula to extreme", "1960"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7764042075163398}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.06666666666666667, 1.0, 1.0, 0.2, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2222222222222222, 0.25, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2925", "mrqa_squad-validation-6024", "mrqa_squad-validation-3048", "mrqa_squad-validation-2165", "mrqa_squad-validation-7211", "mrqa_squad-validation-1456", "mrqa_squad-validation-2140", "mrqa_squad-validation-2423", "mrqa_squad-validation-2122", "mrqa_squad-validation-9744", "mrqa_squad-validation-7835", "mrqa_squad-validation-1531", "mrqa_squad-validation-2419", "mrqa_squad-validation-6254", "mrqa_squad-validation-8896", "mrqa_squad-validation-3942", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-5692"], "SR": 0.71875, "CSR": 0.75, "EFR": 1.0, "Overall": 0.875}, {"timecode": 2, "before_eval_results": {"predictions": ["the mouth of the Monongahela River", "Rutherford Grammar School", "trying to recover market share", "Clair Cameron Patterson", "a tool of the devil", "Skyclad", "one MSP", "The European Commission", "nearly 10,000", "Peanuts", "Charles Darwin", "1906", "Protestantism", "Scottish rivers", "trust God's word", "French Rhin", "the rediscovery of \"Christ and His salvation\"", "form business partnerships with physicians or give them \"kickback\" payments", "Paul Marin de la Malgue", "(leptin, pituitary growth hormone, and prolactin", "The Talons of Weng-Chiang", "2005", "Karakorum", "Paris", "Ukraine", "the policies of major powers, or simply, general-purpose aggressiveness", "Energiprojekt AB in Sweden", "high inequality", "the introduction of the traditional Mongolian script and the creation of the Ikh Zasag (Great Administration)", "when the present amount of funding cannot cover the current costs for labour and materials", "Marconi successfully transmitted the letter S from England to Newfoundland", "Chuck Howley", "theatre", "the increase in tea drinking", "Istanbul", "The Scotland Act 1998", "a stolen Mark I Type 40 TARDIS", "DC traction motor", "17 seconds", "George Westinghouse", "Omnicare, Kindred Healthcare and PharMerica", "the 14th century", "Robert Koch and Emil von Behring", "Guglielmo Marconi", "the first Thursday in May", "article 49", "60 minutes", "the Cobham\u2013Edmonds thesis", "the nobles", "Mork & Mindy", "Los Angeles Kings", "Arabic", "the steam escapes", "Word and Image Department", "6 miles (9.7 km)", "McCrary", "Industrial", "three of his ribs were broken", "Drive them from.... with one Joan la Pucelie join'd", "(with Pictures)", "the Roman Empire", "(4) He was a son that David failed to discipline or reign in", "a Maraschino cherry", "step up"], "metric_results": {"EM": 0.75, "QA-F1": 0.8150229978354977}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.16666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5400", "mrqa_squad-validation-9247", "mrqa_squad-validation-6622", "mrqa_squad-validation-9807", "mrqa_squad-validation-3345", "mrqa_squad-validation-6333", "mrqa_squad-validation-635", "mrqa_squad-validation-7370", "mrqa_squad-validation-9569", "mrqa_squad-validation-478", "mrqa_squad-validation-5473", "mrqa_squad-validation-1625", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-10983", "mrqa_searchqa-validation-7494"], "SR": 0.75, "CSR": 0.75, "EFR": 0.875, "Overall": 0.8125}, {"timecode": 3, "before_eval_results": {"predictions": ["Genghis Khan", "Rutherford Grammar School", "Thomas Edison", "tidal currents", "32", "Newcastle University's student's union building", "11 July 1934", "15,000", "CBS", "4k + 3", "aboral organ", "Wojciech Bogus\u0142awski Theatre", "an antigen from a pathogen", "Maling", "NBC", "the level of the top tax rate", "October 16, 2012", "Metro Light Rail system", "staying home", "The European Commission", "pastors and teachers", "time and space hierarchy", "static discs", "contract", "US$100,000", "Cretaceous\u2013Paleogene extinction", "DNA", "Saturn V", "60%", "Weekend Aventure", "Happy Days", "seven-eighths", "1916", "Jonathan Stewart", "Ron Grainer", "Roone Arledge", "Scottish Government", "adapted quickly and often married outside their immediate French communities", "8 mm cine film", "MODES", "a mainline Protestant Methodist denomination", "Golden Gate Bridge", "magma", "31 October", "Necessity-based", "\"Guilt implies wrong-doing.", "Gallifrey", "twelve", "EU law", "ctenophores", "relationship contracting", "Mitochondria", "The Talons of Weng-Chiang", "discipline problems with the Flight Director's orders", "6.7+", "Paul Claudel", "The Free", "milion", "Light", "\"birdlike snout\"", "Brussels", "Saint Helena", "ccoli", "animals"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7763888888888889}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-589", "mrqa_squad-validation-2337", "mrqa_squad-validation-1684", "mrqa_squad-validation-8664", "mrqa_squad-validation-5903", "mrqa_squad-validation-9487", "mrqa_squad-validation-3069", "mrqa_squad-validation-6913", "mrqa_squad-validation-4060", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-15467", "mrqa_searchqa-validation-13132", "mrqa_searchqa-validation-2273", "mrqa_searchqa-validation-1654", "mrqa_searchqa-validation-16512", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-5998"], "SR": 0.734375, "CSR": 0.74609375, "retrieved_ids": ["mrqa_squad-train-76895", "mrqa_squad-train-56066", "mrqa_squad-train-24878", "mrqa_squad-train-43769", "mrqa_squad-train-44337", "mrqa_squad-train-1129", "mrqa_squad-train-16783", "mrqa_squad-train-19234", "mrqa_squad-train-50088", "mrqa_squad-train-22197", "mrqa_squad-train-45117", "mrqa_squad-train-59467", "mrqa_squad-train-171", "mrqa_squad-train-59098", "mrqa_squad-train-10975", "mrqa_squad-train-15973", "mrqa_squad-validation-7370", "mrqa_squad-validation-2634", "mrqa_squad-validation-1456", "mrqa_squad-validation-2925", "mrqa_squad-validation-1625", "mrqa_squad-validation-3345", "mrqa_squad-validation-1531", "mrqa_squad-validation-6333", "mrqa_squad-validation-8065", "mrqa_squad-validation-2165", "mrqa_squad-validation-2140", "mrqa_squad-validation-2314", "mrqa_squad-validation-6945", "mrqa_squad-validation-2122", "mrqa_squad-validation-9807", "mrqa_squad-validation-5400"], "EFR": 0.9411764705882353, "Overall": 0.8436351102941176}, {"timecode": 4, "before_eval_results": {"predictions": ["\u00a334m per year", "11.5 inches", "patient care rounds drug product selection", "494,665", "1,160,000 square miles", "highest", "Frederick W. Mote", "Plasmodium falciparum", "resolution", "many other herbs not listed", "an orbital scientific instrument package", "Hasar, Hachiun, and Tem\u00fcge", "Hamburg merchants and traders", "Ikh Zasag", "remote sensing", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "Building construction", "rapid expansion in telecommunication and financial activity", "Vampire bats", "theNP-complete Boolean satisfiability problem", "Rose", "socialist realism", "270,000", "not in Wittenberg", "2005\u20132010", "environmental determinism", "Samuel Phillips", "pulmonary fibrosis", "45\u201360 nanometers across", "Four thousand", "70", "a force is required to maintain motion", "addition, subtraction and multiplication", "Charly", "The Secret Life of Pets", "thousands", "1966", "13", "silicon dioxide", "2007", "160 kPa", "use the potential energy stored in an H+ or hydrogen ion gradient", "NFL owners", "Roger Goodell", "the packets may be delivered according to a multiple access scheme", "regeneration", "with the help of the military", "Genghis Khan", "national courts", "color confinement", "75,000 to 100,000 people", "males", "a major personal", "(18801946)", "Helen Hayes", "Tom Chaney", "a very interesting mathematical theorem", "Cape Guard Reserve Home Page", "Franklin D. Roosevelt", "The Royal Family", "a horse", "a raven", "Indianapolis", "an album or an EP record"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7675967261904761}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-4383", "mrqa_squad-validation-5891", "mrqa_squad-validation-1862", "mrqa_squad-validation-1640", "mrqa_squad-validation-10338", "mrqa_squad-validation-9047", "mrqa_squad-validation-600", "mrqa_squad-validation-3664", "mrqa_squad-validation-8902", "mrqa_squad-validation-3040", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-14126", "mrqa_searchqa-validation-5392", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-13360", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-16491", "mrqa_searchqa-validation-14679", "mrqa_hotpotqa-validation-965"], "SR": 0.703125, "CSR": 0.7375, "EFR": 1.0, "Overall": 0.86875}, {"timecode": 5, "before_eval_results": {"predictions": ["the Korean King", "the depths of the oceans and seas", "1562", "San Jose State", "John Sutcliffe", "a military coup d'\u00e9tat", "weekly screenings of all available classic episodes", "Manned Spacecraft Center", "dislodge the French", "July", "Catholic", "Yale", "middle of the 20th century", "1852", "uniting warring tribes", "private", "1815", "the mid-Cambrian period", "Bayeux Tapestry", "specialised education and training", "US$100,000", "Ollie Treiz", "1599", "John Fox", "an emerging Greater Sacramento region", "Venom", "eliminate the accusing law", "the Scotland Parliament", "Narrow alleys", "reciprocating steam engines", "an automobile radiator", "Japan", "NP-intermediate problems", "1542", "GPS", "Leverage", "a number of stages", "new laws or amendments to existing laws as a bill", "suburban", "Higher Real Gymnasium", "peer tuitions", "Asia", "Battle of Olustee", "Veni redemptor gentium", "the local church", "six", "Jurassic Period", "Lenin", "seven", "the 17th century", "Cliff Richard", "Fortnite has up to four players cooperating on various missions on randomly - generated maps to collect resources", "fluoride ions are present in plaque fluid along with dissolved hydroxyapatite", "18", "The main function of the PNS is to connect the CNS to the limbs and organs", "2018 in Pyeongchang County, Gangwon Province, South Korea", "In 1954, language FORTRAN was invented at IBM", "SAMCRO", "the biblical Book of Exodus", "538 electors", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "the Marx Brothers", "TNT", "Coach Carter"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7246465773809524}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.8571428571428571, 0.4, 0.125, 0.8571428571428571, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8244", "mrqa_squad-validation-3860", "mrqa_squad-validation-2835", "mrqa_squad-validation-9492", "mrqa_squad-validation-3369", "mrqa_squad-validation-3403", "mrqa_squad-validation-4311", "mrqa_squad-validation-7936", "mrqa_squad-validation-9452", "mrqa_squad-validation-7094", "mrqa_squad-validation-10041", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5396", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-14925"], "SR": 0.65625, "CSR": 0.7239583333333333, "EFR": 0.9545454545454546, "Overall": 0.8392518939393939}, {"timecode": 6, "before_eval_results": {"predictions": ["net force", "1948", "by qualified majority", "non-self molecules", "1893", "the Channel Islands", "Doctor Who \u2013 The Ultimate Adventure", "preventable diseases like malaria, HIV/AIDS, pneumonia, diarrhoea and malnutrition", "US$10 a week", "Einstein", "1988", "pyrenoid and thylakoids", "a third group of pigments found in cyanobacteria, and glaucophyte, red algal, and cryptophyte chloroplasts", "Ford", "Inalchuq", "51.6%", "NBC Red Network", "quantitative", "John 8:7", "rain", "the university Commons", "short-tempered and even harsher", "theology and philosophy", "KRFX", "annual NFL Experience", "Metro: All Change", "General Samuel C. Phillips", "McManus", "lawful protest demonstration, nonviolent civil disobedience, and violent civil disobedience", "Anheuser-Busch InBev", "Inflammation", "1313", "Rice University", "Dignity Health", "six", "December 1878", "force", "a trusted friend", "Start Here", "Karluk Kara-Khanid", "send aid and sometimes to go themselves to fight for their faith", "judicial branch", "that the Earth must be much older than had previously been supposed in order to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea", "inequitable taxes", "dangerous enemies", "peer tuitions", "Albert Einstein", "Vespa", "Al Yeganeh", "Durban", "pitched", "Antony", "Ronnie", "4", "Francis Matthews Dies", "197", "Lord Marmaduke", "Leon Baptiste", "Star Wars Episode III: Revenge of the Wars", "The o'jays", "the UK", "Derry City F.C.", "H. R. Haldeman", "consulting"], "metric_results": {"EM": 0.625, "QA-F1": 0.6817466085271318}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4651162790697675, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4074", "mrqa_squad-validation-1272", "mrqa_squad-validation-1550", "mrqa_squad-validation-8690", "mrqa_squad-validation-6161", "mrqa_squad-validation-7979", "mrqa_squad-validation-2486", "mrqa_squad-validation-821", "mrqa_squad-validation-2315", "mrqa_squad-validation-5054", "mrqa_triviaqa-validation-2990", "mrqa_triviaqa-validation-1289", "mrqa_triviaqa-validation-2928", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-3501", "mrqa_triviaqa-validation-1164", "mrqa_triviaqa-validation-3388", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-632"], "SR": 0.625, "CSR": 0.7098214285714286, "retrieved_ids": ["mrqa_squad-train-25167", "mrqa_squad-train-16699", "mrqa_squad-train-74063", "mrqa_squad-train-63454", "mrqa_squad-train-13206", "mrqa_squad-train-20817", "mrqa_squad-train-78268", "mrqa_squad-train-70607", "mrqa_squad-train-55839", "mrqa_squad-train-17038", "mrqa_squad-train-27844", "mrqa_squad-train-51268", "mrqa_squad-train-62171", "mrqa_squad-train-16831", "mrqa_squad-train-10464", "mrqa_squad-train-72387", "mrqa_naturalquestions-validation-5396", "mrqa_squad-validation-8065", "mrqa_squad-validation-3942", "mrqa_squad-validation-5400", "mrqa_squad-validation-5891", "mrqa_naturalquestions-validation-5069", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-13132", "mrqa_squad-validation-6254", "mrqa_squad-validation-6913", "mrqa_squad-validation-5473", "mrqa_squad-validation-8244", "mrqa_squad-validation-3345", "mrqa_squad-validation-3369", "mrqa_squad-validation-2835", "mrqa_squad-validation-9086"], "EFR": 0.9583333333333334, "Overall": 0.8340773809523809}, {"timecode": 7, "before_eval_results": {"predictions": ["confirmation and sometimes the profession of faith", "latent heat", "3,837", "Canada", "UK", "487", "Westinghouse Electric", "1957", "soap sponge", "Dynasty", "torn down", "by store and forward switching", "a university or college", "Charles River", "2008", "arrows", "The Victorian Alps", "Louis Comfort Tiffany and \u00c9mile Gall\u00e9", "the evolution of the German language and literature", "silent", "The Greens", "weak labor movements", "gas", "one way streets", "oppidum Ubiorum", "the middle", "private finance initiatives", "only people established in the Netherlands could give legal advice", "left foot", "1901", "poor application", "Industrial", "100% oxygen", "Madame de Pompadour", "Disney Media Networks", "The Saxon Garden", "political figures", "modern art by Polish and international artists", "Neoclassical economics", "membrane-bound granules in the cytoplasm of the euglenophyte", "immune surveillance", "pharmakon", "commerce, schooling and government", "commensal flora", "specific terminology has no more (or no less) meaning than the individual orator intends it to have", "Battle of Bunker Hill", "Odisha", "Samantha Spiro", "Croke Park", "Jesper Myrfors", "1995\u201396", "Vince Guaraldi", "M Rookie Blaylock", "Martha Coolidge", "Moda Center", "the Royal Navy", "Viacom Media Networks", "trans-Pacific flight from the United States to Australia", "Oklahoma City", "The Social Network", "1958", "South Africa", "poems", "jerry"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7468531162464986}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, false, true, false, false, false, true, false, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8571428571428571, 0.8571428571428571, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.4, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4891", "mrqa_squad-validation-5168", "mrqa_squad-validation-3686", "mrqa_squad-validation-9298", "mrqa_squad-validation-4287", "mrqa_squad-validation-6779", "mrqa_squad-validation-4460", "mrqa_squad-validation-8547", "mrqa_squad-validation-8654", "mrqa_squad-validation-6207", "mrqa_squad-validation-6814", "mrqa_hotpotqa-validation-1931", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-1790", "mrqa_searchqa-validation-10900"], "SR": 0.65625, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 8, "before_eval_results": {"predictions": ["extremely high", "Elton Rule", "those who already hold wealth", "Protestant clergy to marry", "between 1859 and 1865", "Robert of Jumi\u00e8ges", "two Japanese research teams", "1905", "gambling", "Mount Bogong", "Mark Twain", "1573", "17", "enthusiasm", "Roman law meaning 'empty land'", "a double displacement loop", "attempting to reconcile electromagnetic theory with two observations", "WBT-FM (99.3 FM)", "private individuals, private organizations or rarely, religious groups", "antiforms", "accepting Jesus as your personal Lord and Savior", "Compromise of 1850", "Keraite", "end of the Pleistocene (~11,600 BP)", "an invasion of Britain, to draw British resources away from North America and the European mainland", "1724", "Hendrix v Employee Insurance Institute", "500", "heat and pressure", "guerrilla warfare campaign", "temperance", "standard of pastoral care and Christian education", "Oirads", "automatically assigned addresses, updated the distributed namespace, and configured any required inter-network routing", "Golden Gate Bridge", "Larger Catechism", "a Saturn V", "2014", "Conan O'Brien", "North American theater", "around 1700", "its humoral system", "1812", "wrestler", "\"Spring Sonata\" (\"Fr\u00fchlingssonate\")", "Golden Gate National Recreation Area", "1938", "LA Galaxy", "Edward Robert Martin Jr.", "July 10, 2017", "Buckingham Palace", "punk rock", "Konstant\u012bns Raudive", "Rob Reiner", "Alain Robbe-Grillet", "Craig William Macneill", "German federal state of Brandenburg", "from 1969 until 1974", "2009", "Sri Lanka Nidahas Pakshaya", "Penelope Garcia", "Carbon", "it's a really dumb thing", "red & white"], "metric_results": {"EM": 0.6875, "QA-F1": 0.746391369047619}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.761904761904762, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5620", "mrqa_squad-validation-9761", "mrqa_squad-validation-10489", "mrqa_squad-validation-6981", "mrqa_squad-validation-5111", "mrqa_squad-validation-9989", "mrqa_squad-validation-10027", "mrqa_squad-validation-4789", "mrqa_squad-validation-10168", "mrqa_hotpotqa-validation-4537", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-278", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-5333", "mrqa_naturalquestions-validation-5170", "mrqa_newsqa-validation-3485", "mrqa_searchqa-validation-7920"], "SR": 0.6875, "CSR": 0.7013888888888888, "EFR": 1.0, "Overall": 0.8506944444444444}, {"timecode": 9, "before_eval_results": {"predictions": ["gilt bronze", "Charles I", "Christian Whiton", "39", "hymn-writer", "two", "MPEG-4", "1724", "the theme of \"grace alone\" more fully", "NewcastleGateshead", "a Polish general and hero of the American Revolutionary War", "Immunology", "Jesus Christ", "temperature", "friendship", "shipping toxic waste", "118", "2:45 a.m.", "Charles L. Hutchinson", "NFL Mobile", "English and Swahili", "a bachelor's degree", "Alvaro Martin", "Belgrade", "no French regular army troops were stationed in North America", "Hadrian's Wall", "American Association of University Women", "uncertain", "Charlesfort", "1650", "original star William Hartnell's poor health", "Three's Company", "October 6, 2004", "Dutch East India Company", "1060s", "Richard E. Grant, Jim Broadbent, Hugh Grant and Joanna Lumley", "coastal beaches and the game reserves", "10 July 1856", "Leonardo da Vinci's", "his lab and elsewhere", "Japan Airlines Flight 123", "13\u20133", "Rothschild", "riders are turned upside-down and then back upright", "47,818", "The High Mobility Multipurpose Wheeled Vehicle", "Schutzstaffel", "The 1984 South Asian Games", "2012", "Hoosick", "Shenae Grimes-Beech", "Marvel Comics characters Steve Rogers / Captain America in the Marvel Cinematic Universe and Johnny Storm / Human Torch in \"Fantastic Four\" and.", "The Seacoast Region", "Greater Washington, D.C.", "Dame Harriet Walter", "Fainaru Fantaj\u012b Tuerubu", "Pan Am Railways", "Taoiseach", "paternalistic policies enacted upon Native American tribes", "Jason Marsden", "Canary Islands", "Sudanese nor orphans", "Bananas are rich in this element that controls the body's fluid balance", "double-headed Japanese tsuzumi"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7411177407661782}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.6666666666666666, 0.888888888888889, 0.0625, 0.5, 0.6666666666666666, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16000000000000003, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2305", "mrqa_squad-validation-652", "mrqa_squad-validation-2329", "mrqa_squad-validation-7665", "mrqa_squad-validation-7885", "mrqa_squad-validation-5537", "mrqa_squad-validation-1613", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-653", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-4655", "mrqa_triviaqa-validation-6110", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-13081", "mrqa_searchqa-validation-8586"], "SR": 0.65625, "CSR": 0.696875, "retrieved_ids": ["mrqa_squad-train-67095", "mrqa_squad-train-32207", "mrqa_squad-train-9580", "mrqa_squad-train-64002", "mrqa_squad-train-13514", "mrqa_squad-train-859", "mrqa_squad-train-83137", "mrqa_squad-train-55384", "mrqa_squad-train-62656", "mrqa_squad-train-66171", "mrqa_squad-train-64149", "mrqa_squad-train-32824", "mrqa_squad-train-68261", "mrqa_squad-train-63055", "mrqa_squad-train-54270", "mrqa_squad-train-83466", "mrqa_triviaqa-validation-1164", "mrqa_squad-validation-8902", "mrqa_squad-validation-2337", "mrqa_hotpotqa-validation-1790", "mrqa_searchqa-validation-7690", "mrqa_triviaqa-validation-2131", "mrqa_squad-validation-2513", "mrqa_searchqa-validation-16491", "mrqa_squad-validation-5891", "mrqa_naturalquestions-validation-7845", "mrqa_squad-validation-5054", "mrqa_squad-validation-8664", "mrqa_squad-validation-9761", "mrqa_searchqa-validation-16512", "mrqa_squad-validation-1456", "mrqa_triviaqa-validation-5998"], "EFR": 1.0, "Overall": 0.8484375}, {"timecode": 10, "UKR": 0.771484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1063", "mrqa_hotpotqa-validation-1126", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1440", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1790", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-278", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-369", "mrqa_hotpotqa-validation-3718", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-384", "mrqa_hotpotqa-validation-3966", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-4537", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-604", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-634", "mrqa_hotpotqa-validation-635", "mrqa_hotpotqa-validation-653", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-965", "mrqa_naturalquestions-validation-13", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3910", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-7845", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-424", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10267", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-10900", "mrqa_searchqa-validation-13132", "mrqa_searchqa-validation-13360", "mrqa_searchqa-validation-14679", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15467", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-16491", "mrqa_searchqa-validation-16512", "mrqa_searchqa-validation-1654", "mrqa_searchqa-validation-2273", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-2676", "mrqa_searchqa-validation-3109", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-5392", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7494", "mrqa_searchqa-validation-8586", "mrqa_searchqa-validation-9529", "mrqa_squad-validation-10018", "mrqa_squad-validation-10022", "mrqa_squad-validation-10027", "mrqa_squad-validation-10041", "mrqa_squad-validation-10062", "mrqa_squad-validation-10078", "mrqa_squad-validation-10136", "mrqa_squad-validation-10168", "mrqa_squad-validation-10191", "mrqa_squad-validation-10203", "mrqa_squad-validation-10294", "mrqa_squad-validation-10337", "mrqa_squad-validation-10338", "mrqa_squad-validation-1035", "mrqa_squad-validation-10370", "mrqa_squad-validation-10406", "mrqa_squad-validation-10474", "mrqa_squad-validation-1068", "mrqa_squad-validation-1075", "mrqa_squad-validation-1110", "mrqa_squad-validation-1113", "mrqa_squad-validation-1131", "mrqa_squad-validation-1148", "mrqa_squad-validation-1182", "mrqa_squad-validation-1243", "mrqa_squad-validation-1272", "mrqa_squad-validation-1301", "mrqa_squad-validation-1320", "mrqa_squad-validation-1343", "mrqa_squad-validation-1349", "mrqa_squad-validation-135", "mrqa_squad-validation-1355", "mrqa_squad-validation-1371", "mrqa_squad-validation-1404", "mrqa_squad-validation-1456", "mrqa_squad-validation-1467", "mrqa_squad-validation-1478", "mrqa_squad-validation-1531", "mrqa_squad-validation-1548", "mrqa_squad-validation-1556", "mrqa_squad-validation-1612", "mrqa_squad-validation-1613", "mrqa_squad-validation-1625", "mrqa_squad-validation-1639", "mrqa_squad-validation-1640", "mrqa_squad-validation-1656", "mrqa_squad-validation-166", "mrqa_squad-validation-1684", "mrqa_squad-validation-1719", "mrqa_squad-validation-1755", "mrqa_squad-validation-1758", "mrqa_squad-validation-1769", "mrqa_squad-validation-1811", "mrqa_squad-validation-1862", "mrqa_squad-validation-1886", "mrqa_squad-validation-1889", "mrqa_squad-validation-195", "mrqa_squad-validation-1956", "mrqa_squad-validation-1977", "mrqa_squad-validation-1977", "mrqa_squad-validation-2073", "mrqa_squad-validation-2077", "mrqa_squad-validation-2079", "mrqa_squad-validation-2106", "mrqa_squad-validation-2112", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2133", "mrqa_squad-validation-2165", "mrqa_squad-validation-2165", "mrqa_squad-validation-2169", "mrqa_squad-validation-218", "mrqa_squad-validation-2283", "mrqa_squad-validation-2292", "mrqa_squad-validation-2292", "mrqa_squad-validation-2305", "mrqa_squad-validation-2315", "mrqa_squad-validation-2323", "mrqa_squad-validation-2326", "mrqa_squad-validation-2329", "mrqa_squad-validation-2337", "mrqa_squad-validation-237", "mrqa_squad-validation-2375", "mrqa_squad-validation-2390", "mrqa_squad-validation-2399", "mrqa_squad-validation-2423", "mrqa_squad-validation-2457", "mrqa_squad-validation-2476", "mrqa_squad-validation-2486", "mrqa_squad-validation-2489", "mrqa_squad-validation-2513", "mrqa_squad-validation-2540", "mrqa_squad-validation-2544", "mrqa_squad-validation-2573", "mrqa_squad-validation-258", "mrqa_squad-validation-2607", "mrqa_squad-validation-2616", "mrqa_squad-validation-2634", "mrqa_squad-validation-2648", "mrqa_squad-validation-2667", "mrqa_squad-validation-2709", "mrqa_squad-validation-271", "mrqa_squad-validation-2723", "mrqa_squad-validation-2736", "mrqa_squad-validation-2785", "mrqa_squad-validation-2798", "mrqa_squad-validation-2810", "mrqa_squad-validation-2835", "mrqa_squad-validation-2885", "mrqa_squad-validation-2911", "mrqa_squad-validation-2923", "mrqa_squad-validation-2925", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3010", "mrqa_squad-validation-3028", "mrqa_squad-validation-3040", "mrqa_squad-validation-3048", "mrqa_squad-validation-3069", "mrqa_squad-validation-3081", "mrqa_squad-validation-3097", "mrqa_squad-validation-3112", "mrqa_squad-validation-3125", "mrqa_squad-validation-313", "mrqa_squad-validation-3138", "mrqa_squad-validation-3141", "mrqa_squad-validation-3165", "mrqa_squad-validation-318", "mrqa_squad-validation-32", "mrqa_squad-validation-3207", "mrqa_squad-validation-3225", "mrqa_squad-validation-3233", "mrqa_squad-validation-3324", "mrqa_squad-validation-3345", "mrqa_squad-validation-3369", "mrqa_squad-validation-340", "mrqa_squad-validation-3403", "mrqa_squad-validation-343", "mrqa_squad-validation-3432", "mrqa_squad-validation-3554", "mrqa_squad-validation-3623", "mrqa_squad-validation-3636", "mrqa_squad-validation-3640", "mrqa_squad-validation-3641", "mrqa_squad-validation-3664", "mrqa_squad-validation-3751", "mrqa_squad-validation-381", "mrqa_squad-validation-3816", "mrqa_squad-validation-3836", "mrqa_squad-validation-3853", "mrqa_squad-validation-3860", "mrqa_squad-validation-3887", "mrqa_squad-validation-3921", "mrqa_squad-validation-3925", "mrqa_squad-validation-3937", "mrqa_squad-validation-3942", "mrqa_squad-validation-3965", "mrqa_squad-validation-3991", "mrqa_squad-validation-402", "mrqa_squad-validation-4023", "mrqa_squad-validation-4024", "mrqa_squad-validation-4057", "mrqa_squad-validation-4060", "mrqa_squad-validation-4074", "mrqa_squad-validation-4080", "mrqa_squad-validation-4122", "mrqa_squad-validation-415", "mrqa_squad-validation-4157", "mrqa_squad-validation-4162", "mrqa_squad-validation-4187", "mrqa_squad-validation-419", "mrqa_squad-validation-4267", "mrqa_squad-validation-4287", "mrqa_squad-validation-4291", "mrqa_squad-validation-4307", "mrqa_squad-validation-4311", "mrqa_squad-validation-4348", "mrqa_squad-validation-4358", "mrqa_squad-validation-4383", "mrqa_squad-validation-4421", "mrqa_squad-validation-4429", "mrqa_squad-validation-4437", "mrqa_squad-validation-4442", "mrqa_squad-validation-4477", "mrqa_squad-validation-4490", "mrqa_squad-validation-453", "mrqa_squad-validation-4534", "mrqa_squad-validation-4557", "mrqa_squad-validation-4575", "mrqa_squad-validation-462", "mrqa_squad-validation-4694", "mrqa_squad-validation-4724", "mrqa_squad-validation-4755", "mrqa_squad-validation-478", "mrqa_squad-validation-4802", "mrqa_squad-validation-4807", "mrqa_squad-validation-4891", "mrqa_squad-validation-4983", "mrqa_squad-validation-4986", "mrqa_squad-validation-5081", "mrqa_squad-validation-5134", "mrqa_squad-validation-5156", "mrqa_squad-validation-5162", "mrqa_squad-validation-5168", "mrqa_squad-validation-5179", "mrqa_squad-validation-519", "mrqa_squad-validation-5205", "mrqa_squad-validation-5256", "mrqa_squad-validation-5257", "mrqa_squad-validation-5269", "mrqa_squad-validation-530", "mrqa_squad-validation-5322", "mrqa_squad-validation-5351", "mrqa_squad-validation-537", "mrqa_squad-validation-5373", "mrqa_squad-validation-5396", "mrqa_squad-validation-5400", "mrqa_squad-validation-5400", "mrqa_squad-validation-5412", "mrqa_squad-validation-5423", "mrqa_squad-validation-5473", "mrqa_squad-validation-5474", "mrqa_squad-validation-5489", "mrqa_squad-validation-5502", "mrqa_squad-validation-5533", "mrqa_squad-validation-5537", "mrqa_squad-validation-5555", "mrqa_squad-validation-5557", "mrqa_squad-validation-5593", "mrqa_squad-validation-5620", "mrqa_squad-validation-5672", "mrqa_squad-validation-5684", "mrqa_squad-validation-5697", "mrqa_squad-validation-5754", "mrqa_squad-validation-5760", "mrqa_squad-validation-5846", "mrqa_squad-validation-5891", "mrqa_squad-validation-5939", "mrqa_squad-validation-5966", "mrqa_squad-validation-5967", "mrqa_squad-validation-6007", "mrqa_squad-validation-6024", "mrqa_squad-validation-6031", "mrqa_squad-validation-6034", "mrqa_squad-validation-604", "mrqa_squad-validation-6040", "mrqa_squad-validation-6086", "mrqa_squad-validation-6115", "mrqa_squad-validation-6122", "mrqa_squad-validation-6142", "mrqa_squad-validation-6161", "mrqa_squad-validation-6163", "mrqa_squad-validation-619", "mrqa_squad-validation-6214", "mrqa_squad-validation-6221", "mrqa_squad-validation-6222", "mrqa_squad-validation-6227", "mrqa_squad-validation-6254", "mrqa_squad-validation-626", "mrqa_squad-validation-6284", "mrqa_squad-validation-629", "mrqa_squad-validation-6318", "mrqa_squad-validation-6330", "mrqa_squad-validation-6333", "mrqa_squad-validation-6349", "mrqa_squad-validation-635", "mrqa_squad-validation-6373", "mrqa_squad-validation-6412", "mrqa_squad-validation-6447", "mrqa_squad-validation-652", "mrqa_squad-validation-6541", "mrqa_squad-validation-6606", "mrqa_squad-validation-6628", "mrqa_squad-validation-664", "mrqa_squad-validation-6653", "mrqa_squad-validation-6694", "mrqa_squad-validation-6718", "mrqa_squad-validation-6718", "mrqa_squad-validation-6738", "mrqa_squad-validation-6777", "mrqa_squad-validation-6807", "mrqa_squad-validation-6812", "mrqa_squad-validation-6813", "mrqa_squad-validation-6814", "mrqa_squad-validation-6831", "mrqa_squad-validation-686", "mrqa_squad-validation-6879", "mrqa_squad-validation-6887", "mrqa_squad-validation-6894", "mrqa_squad-validation-6913", "mrqa_squad-validation-6919", "mrqa_squad-validation-6919", "mrqa_squad-validation-6945", "mrqa_squad-validation-696", "mrqa_squad-validation-7046", "mrqa_squad-validation-7049", "mrqa_squad-validation-7094", "mrqa_squad-validation-7111", "mrqa_squad-validation-7135", "mrqa_squad-validation-7165", "mrqa_squad-validation-7198", "mrqa_squad-validation-7203", "mrqa_squad-validation-7204", "mrqa_squad-validation-721", "mrqa_squad-validation-7217", "mrqa_squad-validation-725", "mrqa_squad-validation-7263", "mrqa_squad-validation-7267", "mrqa_squad-validation-7312", "mrqa_squad-validation-7322", "mrqa_squad-validation-7327", "mrqa_squad-validation-7354", "mrqa_squad-validation-7369", "mrqa_squad-validation-7370", "mrqa_squad-validation-7400", "mrqa_squad-validation-7424", "mrqa_squad-validation-7448", "mrqa_squad-validation-7458", "mrqa_squad-validation-751", "mrqa_squad-validation-7547", "mrqa_squad-validation-7580", "mrqa_squad-validation-7637", "mrqa_squad-validation-7646", "mrqa_squad-validation-7649", "mrqa_squad-validation-7663", "mrqa_squad-validation-7665", "mrqa_squad-validation-767", "mrqa_squad-validation-7733", "mrqa_squad-validation-7752", "mrqa_squad-validation-7821", "mrqa_squad-validation-7835", "mrqa_squad-validation-7851", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7934", "mrqa_squad-validation-7936", "mrqa_squad-validation-7975", "mrqa_squad-validation-7979", "mrqa_squad-validation-7991", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8031", "mrqa_squad-validation-8073", "mrqa_squad-validation-8092", "mrqa_squad-validation-8163", "mrqa_squad-validation-820", "mrqa_squad-validation-823", "mrqa_squad-validation-8244", "mrqa_squad-validation-8245", "mrqa_squad-validation-8301", "mrqa_squad-validation-831", "mrqa_squad-validation-8316", "mrqa_squad-validation-8319", "mrqa_squad-validation-8337", "mrqa_squad-validation-8341", "mrqa_squad-validation-8401", "mrqa_squad-validation-8418", "mrqa_squad-validation-8455", "mrqa_squad-validation-8465", "mrqa_squad-validation-8466", "mrqa_squad-validation-8527", "mrqa_squad-validation-8547", "mrqa_squad-validation-8599", "mrqa_squad-validation-8654", "mrqa_squad-validation-8664", "mrqa_squad-validation-8682", "mrqa_squad-validation-8690", "mrqa_squad-validation-8700", "mrqa_squad-validation-878", "mrqa_squad-validation-8837", "mrqa_squad-validation-8837", "mrqa_squad-validation-8902", "mrqa_squad-validation-9041", "mrqa_squad-validation-9047", "mrqa_squad-validation-9082", "mrqa_squad-validation-9086", "mrqa_squad-validation-9110", "mrqa_squad-validation-9192", "mrqa_squad-validation-9276", "mrqa_squad-validation-929", "mrqa_squad-validation-9298", "mrqa_squad-validation-9325", "mrqa_squad-validation-9346", "mrqa_squad-validation-9427", "mrqa_squad-validation-9452", "mrqa_squad-validation-9456", "mrqa_squad-validation-9492", "mrqa_squad-validation-9543", "mrqa_squad-validation-9543", "mrqa_squad-validation-9569", "mrqa_squad-validation-9597", "mrqa_squad-validation-9623", "mrqa_squad-validation-9643", "mrqa_squad-validation-9653", "mrqa_squad-validation-967", "mrqa_squad-validation-9678", "mrqa_squad-validation-9682", "mrqa_squad-validation-9709", "mrqa_squad-validation-9719", "mrqa_squad-validation-9744", "mrqa_squad-validation-9770", "mrqa_squad-validation-9773", "mrqa_squad-validation-980", "mrqa_squad-validation-9807", "mrqa_squad-validation-9837", "mrqa_squad-validation-9844", "mrqa_squad-validation-9857", "mrqa_squad-validation-9889", "mrqa_squad-validation-9900", "mrqa_squad-validation-9940", "mrqa_squad-validation-9989", "mrqa_squad-validation-999", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1289", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2990", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-3501", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-6110", "mrqa_triviaqa-validation-6283", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-7276"], "OKR": 0.943359375, "KG": 0.40546875, "before_eval_results": {"predictions": ["5 to 15 years", "Chagatai", "the heavens", "Gottfried Fritschel", "1960s", "a supervisory church body", "The Emperor presented the final draft of the Edict of Worms on 25 May 1521", "2000", "Budget cuts", "Sky Digital", "carbohydrates", "powerful high frequency currents set up in them", "light reactions", "14th to 17th centuries", "an epidemiological account of the plague", "Doritos", "Stanford Stadium", "Sava Kosanovi\u0107", "A diaspora of French Australians", "diversified technology and industrial base", "ABC Inc.", "Super Bowl City", "ESPN Deportes", "Article 102", "1973", "Africa", "to elect and appoint bishops", "seven", "Beyonc\u00e9", "modular", "Wade Phillips", "when the immune system is less active than normal, resulting in recurring and life-threatening infections", "domestic legislation of the Scottish Parliament", "Aaron Spelling", "Emmerich Rhine Bridge", "music", "three", "unsustainable monetary stimulation", "orchestr the first movement piano sketch", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "Them", "Armada", "Boyd Gaming", "Field of Dreams", "October 25, 1881", "A Charlie Brown Christmas", "Quasimodo", "Canada's first train robbery", "1933", "Patrick Dempsey", "2016", "Ian Rush", "The Number Twelve", "A. R. Rahman", "ice hockey", "Lincoln Memorial University", "Vincent Anthony Guaraldi", "Romas Kalanta", "French", "May 1, 2018", "Charles Dickens", "marking of Ashura", "February", "father's parenting skills"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7467956349206348}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15999999999999998, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_squad-validation-2209", "mrqa_squad-validation-1474", "mrqa_squad-validation-8873", "mrqa_squad-validation-3063", "mrqa_squad-validation-2708", "mrqa_squad-validation-103", "mrqa_squad-validation-6442", "mrqa_squad-validation-7387", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-4338", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-4950", "mrqa_newsqa-validation-1058", "mrqa_searchqa-validation-8755", "mrqa_newsqa-validation-1949"], "SR": 0.703125, "CSR": 0.6974431818181819, "EFR": 0.9473684210526315, "Overall": 0.7530248205741626}, {"timecode": 11, "before_eval_results": {"predictions": ["the courts of member states", "on a phased basis", "Ukraine", "different viewpoints and political parties", "raised his arm", "1888", "Academy Awards", "August 1914", "1962", "the Chinese", "eastwards", "geghis", "detention", "main porch", "Imperial", "Matt Smith", "Wales", "New England Patriots", "San Jose Marriott", "2016", "AD 14", "encourage growth", "Buddhism", "c1600", "a supervisory church body", "theme", "northern Mokot\u00f3w", "to avoid trivialization", "1948", "article 49", "the butcher Market", "the highest terrace", "San Mateo", "Waterlogged", "service firms", "from 4 August 1915 until November 1918", "Queens", "Maryland", "Francis Egerton", "1910s", "eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music.", "Super Bowl XXIX", "Lascelles", "Helsinki", "1978", "Martin Scorsese", "Groupe PSA", "850 m", "50JJB Sports Fitness Clubs and the attached retail stores", "Nia Kay", "Massachusetts", "125 lb (57 kg)", "22 November 17615 July 1816", "Ericsson", "a classroom specialist", "1949", "Heinrich Himmler", "Sharman Joshi", "1985", "cigarettes", "in the Niger Delta", "If short winter days make your roommate sleep longer & eat all your potato chips", "Addis Ababa", "The Merry Wives of Windsor"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7019660027472527}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.5, 1.0, 1.0, 0.0, 0.47619047619047616, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 0.0, 1.0, 1.0, 0.5714285714285715, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6278", "mrqa_squad-validation-7818", "mrqa_squad-validation-2468", "mrqa_squad-validation-7758", "mrqa_squad-validation-1042", "mrqa_squad-validation-6766", "mrqa_hotpotqa-validation-1097", "mrqa_hotpotqa-validation-243", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-2763", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-693", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-686", "mrqa_hotpotqa-validation-303", "mrqa_naturalquestions-validation-8175", "mrqa_newsqa-validation-3429", "mrqa_searchqa-validation-10721"], "SR": 0.640625, "CSR": 0.6927083333333333, "EFR": 0.9565217391304348, "Overall": 0.7539085144927535}, {"timecode": 12, "before_eval_results": {"predictions": ["less than $1.25", "Brazil", "Oahu", "turbine type", "Pegasus satellites", "1708", "Zh\u00e8ng", "captured enemies", "US", "Paul Samuelson", "Construction", "baptism", "1227", "President", "Stockton and Darlington Railway", "Venus", "George Westinghouse", "the Santa Clara Marriott", "Maria Sk\u0142odowska-Curie Institute of Oncology", "from January 1964, until it achieved the first manned landing in July 1969", "Thomas Davis", "Blaydon Race", "45,000 pounds", "1972", "Between 1975 and 1990", "Denver Broncos", "Jacksonville", "the Song dynasty", "10", "United States", "HAMAS", "arranged marriages", "Algeria", "innate immune system versus the adaptive immune system", "three", "Gustav's top winds", "Vernon Forrest", "50,000", "persistent pain", "22", "women", "Karl Kr\u00f8yer", "preventing our public-owned seas from turning into sprawling, watery versions of Houston, Texas, or Atlanta, Georgia", "$1.5 million", "Dennis Davern", "American", "co-writing credits", "$250,000", "Kaka", "Laura Ling and Euna Lee", "1995", "\"disagreements\" with the Port Authority of New York and New Jersey", "because the Indians were gathering information about the rebels", "three", "recall communications", "a president who understands the world today, the future we seek and the change we need", "Jan Brewer", "15,000", "its confluence with the Paran\u00e1 River", "Mark Freeland", "Argand lamp", "Bantu", "Charles \"Lucky\" Luciano", "John Ford"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7290081521739131}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.9565217391304348, 0.8, 0.5, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6279", "mrqa_squad-validation-1424", "mrqa_squad-validation-1050", "mrqa_squad-validation-1", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-4145", "mrqa_naturalquestions-validation-3390", "mrqa_triviaqa-validation-1871", "mrqa_searchqa-validation-4954", "mrqa_hotpotqa-validation-3124"], "SR": 0.6875, "CSR": 0.6923076923076923, "retrieved_ids": ["mrqa_squad-train-51583", "mrqa_squad-train-14424", "mrqa_squad-train-33919", "mrqa_squad-train-34803", "mrqa_squad-train-46953", "mrqa_squad-train-73862", "mrqa_squad-train-2458", "mrqa_squad-train-76523", "mrqa_squad-train-3180", "mrqa_squad-train-75733", "mrqa_squad-train-37376", "mrqa_squad-train-29647", "mrqa_squad-train-75394", "mrqa_squad-train-462", "mrqa_squad-train-51849", "mrqa_squad-train-47138", "mrqa_squad-validation-3942", "mrqa_searchqa-validation-8586", "mrqa_squad-validation-5473", "mrqa_squad-validation-1474", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-4655", "mrqa_searchqa-validation-5692", "mrqa_hotpotqa-validation-686", "mrqa_squad-validation-10168", "mrqa_squad-validation-7979", "mrqa_hotpotqa-validation-4338", "mrqa_squad-validation-2925", "mrqa_hotpotqa-validation-1097", "mrqa_squad-validation-1640", "mrqa_squad-validation-1613", "mrqa_squad-validation-664"], "EFR": 1.0, "Overall": 0.7625240384615385}, {"timecode": 13, "before_eval_results": {"predictions": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "gilt copper", "antithetical", "young and the elderly", "because it has survived many wars, conflicts and invasions throughout its long history", "common flagellated", "anti-colonial movements", "five million", "John Pell, Lord of Pelham Manor", "expendable nature of the worker in relation to his or her particular job.", "five-year", "3%", "720p high definition", "their Annual Conference", "acular", "electric", "aspirational consumption", "Vistula River", "390 billion", "red algae red", "other ctenophores", "to protect the King's land in the Ohio Valley from the British.", "Department of Justice", "Barbara Walters", "Chicago Theological Seminary", "May", "1885", "William S. Paley", "six", "Iran", "shopping", "an extensive, electrified, passenger system throughout Melbourne and suburbs", "southern California coast", "1989", "Cathy Dennis and Rob Davis", "1971", "1st Earl Mountbatten of Burma remained Governor - General of India", "in a geographical coordinate system at which longitude is defined to be 0 \u00b0", "Detective Abigail Baker", "159 beats per minute", "50 states of the United States of America", "The eighth and final season", "Scheria ( / \u02c8sk\u025bri\u0259 /", "Hugh S. Johnson, a retired United States Army general and a successful businessman", "$75,000", "Rigg", "NFL coaches, general managers, and scouts", "Emma Watson and Dan Stevens as the eponymous characters with Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, and Emma Thompson in supporting roles", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "American Idol would return for the 2017 -- 18 television season", "nucleus, where the DNA is held", "The Vamps, McGregor Maynard, Bronnie, Ella Eyre, Sheppard and Louisa Johnson were the opening acts", "georgia", "total cost", "Michael Christopher McDowell", "this remake is set in China, and features Kung Fu instead of Okinawan Karate", "The first series premiered on BBC Two on 8 January 1999 and lasted for six episodes, concluding on 12 February 1999", "The flag of Hungary", "Argentina", "footballer", "safety issues in the company's cars", "turkey vulture", "warwick d(AACCCC)", "Magnolia grandiflora"], "metric_results": {"EM": 0.5, "QA-F1": 0.6569207894561657}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 0.588235294117647, 0.0, 0.4, 0.0, 0.16666666666666666, 0.6666666666666666, 0.6666666666666666, 0.42857142857142855, 1.0, 1.0, 1.0, 0.9152542372881356, 0.33333333333333337, 0.0, 0.25, 0.2666666666666667, 0.33333333333333337, 0.8, 1.0, 0.13333333333333333, 0.2608695652173913, 0.5, 1.0, 0.0, 0.33333333333333337, 0.6666666666666666, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-5889", "mrqa_squad-validation-89", "mrqa_squad-validation-4415", "mrqa_squad-validation-8840", "mrqa_squad-validation-9568", "mrqa_squad-validation-2900", "mrqa_squad-validation-2585", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-10347", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-10162", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9004", "mrqa_naturalquestions-validation-9986", "mrqa_naturalquestions-validation-6020", "mrqa_hotpotqa-validation-5578", "mrqa_newsqa-validation-247", "mrqa_searchqa-validation-12558", "mrqa_searchqa-validation-15590", "mrqa_searchqa-validation-5681"], "SR": 0.5, "CSR": 0.6785714285714286, "EFR": 0.9375, "Overall": 0.7472767857142857}, {"timecode": 14, "before_eval_results": {"predictions": ["Worldvision Enterprises", "the computational model used", "16,000 rpm", "scholars", "it infringed on democratic freedoms.", "a \"racket\"", "The Muslims in the semu class", "The Five Doctors", "high supply", "Tyneside's shipbuilding heritage, and inventions which changed the world", "events and festivals", "platyctenids", "increased blood flow into tissue", "Deformational events", "a kleptoplast", "organic", "three to five", "1996", "half", "Pushing against an object", "McManus", "Super Bowl XLIV", "San Jose State", "12 to 15 million", "the Carmichael numbers", "BPP, ZPP and RP", "Genghis Khan", "the Advanced Steam movement", "because Dutch law said only people established in the Netherlands could give legal advice.", "Alsace", "Vistula River", "the Buonapartes", "George W. Bush", "sarsaparilla", "Idaho State", "cardiac", "a full-blooded Navajo", "Tennessee", "the focal point", "James R. Garfield", "the United States Navy", "Stanley Nicholson", "Enchanted", "prostate", "Ireland's Four Courts,", "purple potatoes", "Willie Mays, Jr.", "Franklin D. Roosevelt", "Mussolini. 26.", "Stripes", "Israel", "Yogi Bear", "the Arctic Ocean", "Wyoming", "Crayola", "anticle", "pommel horse", "four", "the Poincar\u00e9 conjecture", "Elizabeth River", "Stratfor", "Joseph Heller", "commemorating fealty and filial piety", "September 2000"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6786458333333333}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.5, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1714", "mrqa_squad-validation-9865", "mrqa_squad-validation-7792", "mrqa_squad-validation-5226", "mrqa_squad-validation-680", "mrqa_squad-validation-8677", "mrqa_squad-validation-3653", "mrqa_squad-validation-10316", "mrqa_squad-validation-821", "mrqa_squad-validation-6044", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-9543", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-1243", "mrqa_searchqa-validation-9508", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-9604", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-12609", "mrqa_searchqa-validation-1374", "mrqa_searchqa-validation-6784", "mrqa_searchqa-validation-13966", "mrqa_searchqa-validation-4619", "mrqa_searchqa-validation-6365", "mrqa_searchqa-validation-10053", "mrqa_searchqa-validation-10017", "mrqa_naturalquestions-validation-5897"], "SR": 0.5625, "CSR": 0.6708333333333334, "EFR": 1.0, "Overall": 0.7582291666666666}, {"timecode": 15, "before_eval_results": {"predictions": ["7.8%", "Manakin Town", "three", "the city council", "the Monarch", "a structure called a Z-ring", "rapid expansion in telecommunication and financial activity", "Accountants", "Jim Nantz and Phil Simms", "phagolysosome", "George Westinghouse", "October 16, 2012", "February 9, 1832", "a universal Ku band LNB (9.75/10.600 GHz)", "June 4, 2014", "26", "$5 million", "trade unions", "Ronnie Hillman", "Chris Keates", "the murder of Christ", "connection-oriented operations", "the days, weeks and months", "complexity", "CBS", "prices", "random noise", "sorcery or even poison", "an Islamic state", "Hendrix v Employee Insurance Institute", "a quarter tone", "the Boston Marathon", "Home alone", "Tainted Love", "Suspicious Minds", "humans", "the Travers Stakes", "Duran Duran", "a grenade", "cinnamon", "Jack Nicklaus", "PT 109", "Uranus", "insulin", "Afrikaans", "the right whale", "May 16, 1836", "He don't smoke, he don't drink", "the bus", "we should forget old acquaintances", "Cy Young", "Kings", "the parrots", "genes", "the Antonine Wall", "The Pirates of Penzance", "the Whig Party", "frustration with the atmosphere in the group", "FunkFunk", "Australia", "the Internet", "86,112", "a pioneer in watch design, manufacturing and distribution", "Sweden, Norway and Denmark"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7017857142857143}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.7142857142857143, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-413", "mrqa_squad-validation-2743", "mrqa_squad-validation-4968", "mrqa_squad-validation-6933", "mrqa_searchqa-validation-9550", "mrqa_searchqa-validation-2002", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-1993", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-6370", "mrqa_searchqa-validation-1254", "mrqa_searchqa-validation-12771", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-9274", "mrqa_searchqa-validation-8628", "mrqa_searchqa-validation-9894", "mrqa_searchqa-validation-7138", "mrqa_searchqa-validation-16371", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-7748", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-5415"], "SR": 0.640625, "CSR": 0.6689453125, "retrieved_ids": ["mrqa_squad-train-61336", "mrqa_squad-train-78892", "mrqa_squad-train-40000", "mrqa_squad-train-53909", "mrqa_squad-train-42347", "mrqa_squad-train-35603", "mrqa_squad-train-14344", "mrqa_squad-train-39186", "mrqa_squad-train-7634", "mrqa_squad-train-21917", "mrqa_squad-train-8052", "mrqa_squad-train-42519", "mrqa_squad-train-71672", "mrqa_squad-train-43526", "mrqa_squad-train-21975", "mrqa_squad-train-49589", "mrqa_naturalquestions-validation-3390", "mrqa_newsqa-validation-1949", "mrqa_squad-validation-2468", "mrqa_hotpotqa-validation-418", "mrqa_triviaqa-validation-1558", "mrqa_squad-validation-6913", "mrqa_squad-validation-7665", "mrqa_hotpotqa-validation-1011", "mrqa_squad-validation-3369", "mrqa_hotpotqa-validation-4537", "mrqa_squad-validation-8654", "mrqa_searchqa-validation-7494", "mrqa_squad-validation-1050", "mrqa_naturalquestions-validation-2558", "mrqa_squad-validation-1550", "mrqa_squad-validation-7094"], "EFR": 1.0, "Overall": 0.7578515625}, {"timecode": 16, "before_eval_results": {"predictions": ["De Materia Medica", "an electrical generator", "force", "4,404.5", "time or space", "International Association of Methodist-related Schools, Colleges, and Universities", "ideological as well as financial reasons", "Jason Bourne", "48.8 \u00b0C", "0.52/sq mi", "Jamukha", "journalist", "The Day of the Doctor", "3 million", "eight", "2014", "Ugali with vegetables, sour milk, meat, fish or any other stew", "1285", "interleukins", "Parliament", "1978", "Not designed to fly through the Earth's atmosphere or return to Earth", "Hughes Hotel", "the North Sea in the Netherlands", "The Emperor", "Josh Norman", "government officials and climate change experts", "Thomas Commerford Martin", "The P Larson Russell Terrier", "Stephen King", "piano", "Captain James T. Kirk", "Carly Simon", "a capella", "Ida Noddack", "Japan", "Charlotte Elizabeth Diana", "Uganda", "Antonio Stradivari", "France", "Taiwan", "Liverpool", "E pluribus unum", "Budapest", "line coding", "a", "the Distin family", "Jupiter", "Chuck Hagel", "Victoria", "a period", "by her stage name \u201cTina Turner\u201d", "Margaret Thatcher", "a tapestry", "hypopituitarism", "George Osborne", "boudin", "a stem", "Bardney", "4 meters (13 feet)", "the Black Sea", "Todd Griffin", "1908", "active absorption of water"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6500372023809524}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25]}}, "before_error_ids": ["mrqa_squad-validation-6314", "mrqa_squad-validation-9800", "mrqa_squad-validation-601", "mrqa_squad-validation-7877", "mrqa_squad-validation-72", "mrqa_squad-validation-9482", "mrqa_triviaqa-validation-4654", "mrqa_triviaqa-validation-6026", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-1925", "mrqa_triviaqa-validation-3272", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-7612", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-4892", "mrqa_triviaqa-validation-2320", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-3542", "mrqa_naturalquestions-validation-2462", "mrqa_newsqa-validation-1214", "mrqa_naturalquestions-validation-1704"], "SR": 0.609375, "CSR": 0.6654411764705883, "EFR": 0.96, "Overall": 0.7491507352941176}, {"timecode": 17, "before_eval_results": {"predictions": ["life expectancy", "eleven", "1937", "biomass", "to avoid being targeted by the boycott", "Walter Reed", "September 1944", "two-page", "a new form", "an area of science where our scientific understanding is rapidly changing,", "12%", "six to nine percent", "The European Court of Justice", "2010", "that the law is no longer to be taught to Christians", "the Ardabil Carpet", "within the US", "Destiny of the Doctor", "much land and housing", "Pons Aelius", "hard highlight the difference between a problem and an instance", "slash and burn", "that since Luther's increasingly antisemitic views developed during the years his health deteriorated, it is possible they were at least partly the product of a declining state of mind", "131", "Hugh Downs", "Dillon, Read & Co.", "Euler's totient function", "Julian Fellowes", "The Lady of Shalott", "98", "Adam Werritty", "Four", "Ipswich Town", "Margaret Thatcher", "South Korea", "hard", "Lesotho", "India", "the 2014 Mille Miglia", "Papua New Guinea", "arraf", "skills", "Ramadan", "arthur ryron", "Leon Czolgosz", "of Creation", "Genghis Khan", "South Africa", "Edward de Bono", "127 Hours", "kyu", "arctic", "in the Andes Mountains of Chile and Argentina", "arthur", "the Nasdaq", "Nelson Mandela", "steel", "above the light source and under the sample", "9", "Sunday", "arthur", "1960s", "Ahmad Givens", "absolute temperature"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6194047619047618}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4799999999999999, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7949", "mrqa_squad-validation-8470", "mrqa_squad-validation-2473", "mrqa_squad-validation-2191", "mrqa_squad-validation-1670", "mrqa_squad-validation-2523", "mrqa_squad-validation-9416", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-5571", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2667", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-7675", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-1480", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-5219", "mrqa_triviaqa-validation-2246", "mrqa_naturalquestions-validation-4132", "mrqa_hotpotqa-validation-893", "mrqa_searchqa-validation-2851", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-365"], "SR": 0.546875, "CSR": 0.6588541666666667, "EFR": 0.9655172413793104, "Overall": 0.7489367816091954}, {"timecode": 18, "before_eval_results": {"predictions": ["jihadat al-Mahgoub", "the Swiss-Austrian border", "Michael P. Millardi", "the autumn of 1991", "MetroCentre", "force", "packet switching", "high wages", "Buena Vista International Television", "three hundred sixty", "patents", "near the Alps", "the Venetian Marco Polo", "January 2003", "1972", "the 1965\u201366 season", "Luther", "Storybook", "1981", "1264", "monophyletic", "complete the modules", "1981", "glowed even when turned off", "21 October 1512", "gobi", "dans", "the people", "gobi", "tartans", "douglas", "gobi", "gobi", "born to be Wild", "pakistan", "in the city of Leicester", "the skull", "ptolemy", "pakistan", "douglas", "pakistan", "Isle of Wight", "a murre", "pakistan", "douglas", "a fort", "omid dans", "douglas", "gobi", "tartar", "pakistan", "blood transfusion", "masa", "pompadour", "dans", "The World is Not Enough", "The Romantics", "2020", "The Pirate", "1968", "Citizens", "the widows of John Lennon and George Harrison", "The Gateway Arch", "The University of Exeter"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5187003968253968}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, true, false, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9649", "mrqa_squad-validation-9071", "mrqa_squad-validation-8171", "mrqa_squad-validation-2093", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-2560", "mrqa_triviaqa-validation-5885", "mrqa_triviaqa-validation-2092", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-4059", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-1213", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-6625", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-1413", "mrqa_triviaqa-validation-799", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-1741", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-1410", "mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2137", "mrqa_triviaqa-validation-3465", "mrqa_hotpotqa-validation-774", "mrqa_newsqa-validation-2128", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-10897"], "SR": 0.46875, "CSR": 0.6488486842105263, "retrieved_ids": ["mrqa_squad-train-11702", "mrqa_squad-train-75900", "mrqa_squad-train-22507", "mrqa_squad-train-61776", "mrqa_squad-train-57144", "mrqa_squad-train-34794", "mrqa_squad-train-63381", "mrqa_squad-train-27772", "mrqa_squad-train-3731", "mrqa_squad-train-16132", "mrqa_squad-train-78515", "mrqa_squad-train-11158", "mrqa_squad-train-55706", "mrqa_squad-train-48557", "mrqa_squad-train-35959", "mrqa_squad-train-54944", "mrqa_searchqa-validation-8586", "mrqa_squad-validation-2585", "mrqa_naturalquestions-validation-10406", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-4029", "mrqa_naturalquestions-validation-9004", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-1289", "mrqa_squad-validation-6622", "mrqa_searchqa-validation-9529", "mrqa_hotpotqa-validation-1548", "mrqa_searchqa-validation-9604", "mrqa_hotpotqa-validation-3247", "mrqa_naturalquestions-validation-4247", "mrqa_triviaqa-validation-7612", "mrqa_squad-validation-9568"], "EFR": 1.0, "Overall": 0.7538322368421053}, {"timecode": 19, "before_eval_results": {"predictions": ["\u03b2-defensins", "10.0%", "Josh Norman", "the smallest subfield", "Andrew Alper", "1990s", "CBS", "2003", "seven", "from southern China to Daidu in the north", "3.6%", "the park", "Book of Discipline", "optimisation of a drug treatment", "Neil Shubin and Paul Sereno", "Colorado Springs", "more convenient and private method", "Several procedures", "It is also the home of the Sunnyside Country Club", "antigenic variation", "c1110", "glaucophyte", "700,000", "Cestida", "basketball", "Copenhagen", "David Nixon", "basketball", "between Seventh Avenue and Broadway", "humbert", "john habert humbert", "Coldplay", "marea drummondii", "Stanislas Wawrinka", "Western Australia", "liza Barth", "Florence", "Thailand", "ponridge", "Strangeways", "Uranus", "Phil Redmond", "Benito Mussolini", "sebastian moran", "dV", "Montmorency", "South african", "Chillicothe", "Boyle", "south african", "Ytterby", "lonesome Polecat", "four", "oastler", "doe", "Algeria", "3D modeling", "February 7, 2018", "Mary Bonauto, Susan Murray, and Beth Robinson", "872 to 930", "Hugo Chavez", "Michoacan Family", "Parkinson's disease", "Crustacea"], "metric_results": {"EM": 0.625, "QA-F1": 0.6884469696969697}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, false, false, false, true, false, true, false, true, true, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.7272727272727273, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-225", "mrqa_squad-validation-8103", "mrqa_squad-validation-4662", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-4343", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-5148", "mrqa_triviaqa-validation-6129", "mrqa_triviaqa-validation-985", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-490", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-6312", "mrqa_triviaqa-validation-5318", "mrqa_triviaqa-validation-35", "mrqa_naturalquestions-validation-4879", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-2897", "mrqa_searchqa-validation-5795"], "SR": 0.625, "CSR": 0.64765625, "EFR": 0.9583333333333334, "Overall": 0.7452604166666666}, {"timecode": 20, "UKR": 0.79296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1063", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-1128", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-119", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3099", "mrqa_hotpotqa-validation-3115", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-384", "mrqa_hotpotqa-validation-3861", "mrqa_hotpotqa-validation-3966", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4338", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4699", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5415", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-5896", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-635", "mrqa_hotpotqa-validation-653", "mrqa_hotpotqa-validation-686", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-965", "mrqa_naturalquestions-validation-10162", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-13", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-8654", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9684", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-1243", "mrqa_searchqa-validation-1254", "mrqa_searchqa-validation-12558", "mrqa_searchqa-validation-12771", "mrqa_searchqa-validation-13966", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15467", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-15884", "mrqa_searchqa-validation-16371", "mrqa_searchqa-validation-23", "mrqa_searchqa-validation-2676", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-4220", "mrqa_searchqa-validation-4619", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-4997", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-5692", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7494", "mrqa_searchqa-validation-7935", "mrqa_searchqa-validation-801", "mrqa_searchqa-validation-9090", "mrqa_searchqa-validation-9604", "mrqa_searchqa-validation-9885", "mrqa_squad-validation-10009", "mrqa_squad-validation-10018", "mrqa_squad-validation-10022", "mrqa_squad-validation-10027", "mrqa_squad-validation-10062", "mrqa_squad-validation-10087", "mrqa_squad-validation-101", "mrqa_squad-validation-10168", "mrqa_squad-validation-10191", "mrqa_squad-validation-10206", "mrqa_squad-validation-10252", "mrqa_squad-validation-10264", "mrqa_squad-validation-10294", "mrqa_squad-validation-10316", "mrqa_squad-validation-10338", "mrqa_squad-validation-10383", "mrqa_squad-validation-10406", "mrqa_squad-validation-1042", "mrqa_squad-validation-10474", "mrqa_squad-validation-1048", "mrqa_squad-validation-1068", "mrqa_squad-validation-1075", "mrqa_squad-validation-1081", "mrqa_squad-validation-1110", "mrqa_squad-validation-1148", "mrqa_squad-validation-1156", "mrqa_squad-validation-1243", "mrqa_squad-validation-1272", "mrqa_squad-validation-1371", "mrqa_squad-validation-1394", "mrqa_squad-validation-1404", "mrqa_squad-validation-1441", "mrqa_squad-validation-1547", "mrqa_squad-validation-1550", "mrqa_squad-validation-1556", "mrqa_squad-validation-1557", "mrqa_squad-validation-1577", "mrqa_squad-validation-159", "mrqa_squad-validation-1613", "mrqa_squad-validation-1631", "mrqa_squad-validation-1639", "mrqa_squad-validation-167", "mrqa_squad-validation-1719", "mrqa_squad-validation-1758", "mrqa_squad-validation-1769", "mrqa_squad-validation-1862", "mrqa_squad-validation-1882", "mrqa_squad-validation-1889", "mrqa_squad-validation-192", "mrqa_squad-validation-195", "mrqa_squad-validation-1956", "mrqa_squad-validation-1977", "mrqa_squad-validation-1993", "mrqa_squad-validation-2033", "mrqa_squad-validation-2079", "mrqa_squad-validation-2112", "mrqa_squad-validation-2122", "mrqa_squad-validation-2140", "mrqa_squad-validation-2169", "mrqa_squad-validation-2191", "mrqa_squad-validation-225", "mrqa_squad-validation-2292", "mrqa_squad-validation-2329", "mrqa_squad-validation-2337", "mrqa_squad-validation-237", "mrqa_squad-validation-2375", "mrqa_squad-validation-2399", "mrqa_squad-validation-2411", "mrqa_squad-validation-2423", "mrqa_squad-validation-2453", "mrqa_squad-validation-2461", "mrqa_squad-validation-2467", "mrqa_squad-validation-2468", "mrqa_squad-validation-2521", "mrqa_squad-validation-2523", "mrqa_squad-validation-2540", "mrqa_squad-validation-256", "mrqa_squad-validation-2570", "mrqa_squad-validation-258", "mrqa_squad-validation-2585", "mrqa_squad-validation-2616", "mrqa_squad-validation-2625", "mrqa_squad-validation-2634", "mrqa_squad-validation-2648", "mrqa_squad-validation-2708", "mrqa_squad-validation-2736", "mrqa_squad-validation-2778", "mrqa_squad-validation-2834", "mrqa_squad-validation-2885", "mrqa_squad-validation-2900", "mrqa_squad-validation-2909", "mrqa_squad-validation-2923", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3028", "mrqa_squad-validation-305", "mrqa_squad-validation-3063", "mrqa_squad-validation-3081", "mrqa_squad-validation-3097", "mrqa_squad-validation-3125", "mrqa_squad-validation-3165", "mrqa_squad-validation-317", "mrqa_squad-validation-3193", "mrqa_squad-validation-32", "mrqa_squad-validation-3204", "mrqa_squad-validation-3225", "mrqa_squad-validation-3233", "mrqa_squad-validation-3233", "mrqa_squad-validation-3267", "mrqa_squad-validation-327", "mrqa_squad-validation-3324", "mrqa_squad-validation-3345", "mrqa_squad-validation-340", "mrqa_squad-validation-3403", "mrqa_squad-validation-343", "mrqa_squad-validation-3469", "mrqa_squad-validation-3661", "mrqa_squad-validation-3686", "mrqa_squad-validation-3740", "mrqa_squad-validation-3751", "mrqa_squad-validation-3759", "mrqa_squad-validation-3802", "mrqa_squad-validation-3806", "mrqa_squad-validation-381", "mrqa_squad-validation-3836", "mrqa_squad-validation-3860", "mrqa_squad-validation-3902", "mrqa_squad-validation-3925", "mrqa_squad-validation-3991", "mrqa_squad-validation-4080", "mrqa_squad-validation-4122", "mrqa_squad-validation-415", "mrqa_squad-validation-419", "mrqa_squad-validation-4267", "mrqa_squad-validation-4366", "mrqa_squad-validation-4383", "mrqa_squad-validation-4429", "mrqa_squad-validation-4429", "mrqa_squad-validation-4534", "mrqa_squad-validation-4557", "mrqa_squad-validation-4575", "mrqa_squad-validation-4607", "mrqa_squad-validation-462", "mrqa_squad-validation-4662", "mrqa_squad-validation-4670", "mrqa_squad-validation-468", "mrqa_squad-validation-4694", "mrqa_squad-validation-4807", "mrqa_squad-validation-4875", "mrqa_squad-validation-5003", "mrqa_squad-validation-5010", "mrqa_squad-validation-5012", "mrqa_squad-validation-5054", "mrqa_squad-validation-5077", "mrqa_squad-validation-5134", "mrqa_squad-validation-5162", "mrqa_squad-validation-5179", "mrqa_squad-validation-5185", "mrqa_squad-validation-519", "mrqa_squad-validation-5205", "mrqa_squad-validation-5256", "mrqa_squad-validation-5269", "mrqa_squad-validation-5282", "mrqa_squad-validation-5319", "mrqa_squad-validation-5351", "mrqa_squad-validation-5363", "mrqa_squad-validation-5382", "mrqa_squad-validation-5400", "mrqa_squad-validation-5428", "mrqa_squad-validation-5457", "mrqa_squad-validation-547", "mrqa_squad-validation-5473", "mrqa_squad-validation-5474", "mrqa_squad-validation-552", "mrqa_squad-validation-5537", "mrqa_squad-validation-5556", "mrqa_squad-validation-5557", "mrqa_squad-validation-5600", "mrqa_squad-validation-5672", "mrqa_squad-validation-5684", "mrqa_squad-validation-5718", "mrqa_squad-validation-5754", "mrqa_squad-validation-5797", "mrqa_squad-validation-5840", "mrqa_squad-validation-5846", "mrqa_squad-validation-5853", "mrqa_squad-validation-5889", "mrqa_squad-validation-5903", "mrqa_squad-validation-5927", "mrqa_squad-validation-5939", "mrqa_squad-validation-5948", "mrqa_squad-validation-596", "mrqa_squad-validation-6024", "mrqa_squad-validation-6026", "mrqa_squad-validation-6034", "mrqa_squad-validation-604", "mrqa_squad-validation-6086", "mrqa_squad-validation-6142", "mrqa_squad-validation-6151", "mrqa_squad-validation-6161", "mrqa_squad-validation-6209", "mrqa_squad-validation-6214", "mrqa_squad-validation-6254", "mrqa_squad-validation-6268", "mrqa_squad-validation-6278", "mrqa_squad-validation-6279", "mrqa_squad-validation-6284", "mrqa_squad-validation-6349", "mrqa_squad-validation-6415", "mrqa_squad-validation-6447", "mrqa_squad-validation-6457", "mrqa_squad-validation-6474", "mrqa_squad-validation-6492", "mrqa_squad-validation-6517", "mrqa_squad-validation-6541", "mrqa_squad-validation-6622", "mrqa_squad-validation-6624", "mrqa_squad-validation-6766", "mrqa_squad-validation-6777", "mrqa_squad-validation-680", "mrqa_squad-validation-6807", "mrqa_squad-validation-6831", "mrqa_squad-validation-686", "mrqa_squad-validation-6879", "mrqa_squad-validation-6919", "mrqa_squad-validation-6933", "mrqa_squad-validation-696", "mrqa_squad-validation-7005", "mrqa_squad-validation-7026", "mrqa_squad-validation-7046", "mrqa_squad-validation-7051", "mrqa_squad-validation-7111", "mrqa_squad-validation-7135", "mrqa_squad-validation-7165", "mrqa_squad-validation-7170", "mrqa_squad-validation-7198", "mrqa_squad-validation-7203", "mrqa_squad-validation-7204", "mrqa_squad-validation-7208", "mrqa_squad-validation-7211", "mrqa_squad-validation-725", "mrqa_squad-validation-7260", "mrqa_squad-validation-7284", "mrqa_squad-validation-729", "mrqa_squad-validation-7292", "mrqa_squad-validation-7370", "mrqa_squad-validation-7387", "mrqa_squad-validation-7400", "mrqa_squad-validation-7417", "mrqa_squad-validation-7435", "mrqa_squad-validation-7448", "mrqa_squad-validation-7458", "mrqa_squad-validation-748", "mrqa_squad-validation-7500", "mrqa_squad-validation-7508", "mrqa_squad-validation-751", "mrqa_squad-validation-7637", "mrqa_squad-validation-7665", "mrqa_squad-validation-767", "mrqa_squad-validation-7758", "mrqa_squad-validation-7794", "mrqa_squad-validation-7851", "mrqa_squad-validation-7859", "mrqa_squad-validation-7885", "mrqa_squad-validation-7934", "mrqa_squad-validation-7936", "mrqa_squad-validation-7949", "mrqa_squad-validation-7975", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8092", "mrqa_squad-validation-8123", "mrqa_squad-validation-8151", "mrqa_squad-validation-8171", "mrqa_squad-validation-8180", "mrqa_squad-validation-821", "mrqa_squad-validation-8210", "mrqa_squad-validation-8250", "mrqa_squad-validation-8301", "mrqa_squad-validation-8316", "mrqa_squad-validation-8338", "mrqa_squad-validation-8341", "mrqa_squad-validation-8349", "mrqa_squad-validation-835", "mrqa_squad-validation-8394", "mrqa_squad-validation-8401", "mrqa_squad-validation-8418", "mrqa_squad-validation-8428", "mrqa_squad-validation-8457", "mrqa_squad-validation-8478", "mrqa_squad-validation-8527", "mrqa_squad-validation-8599", "mrqa_squad-validation-862", "mrqa_squad-validation-8664", "mrqa_squad-validation-8677", "mrqa_squad-validation-8682", "mrqa_squad-validation-8690", "mrqa_squad-validation-8694", "mrqa_squad-validation-8700", "mrqa_squad-validation-8723", "mrqa_squad-validation-8724", "mrqa_squad-validation-8839", "mrqa_squad-validation-8840", "mrqa_squad-validation-8902", "mrqa_squad-validation-9036", "mrqa_squad-validation-9041", "mrqa_squad-validation-9047", "mrqa_squad-validation-9145", "mrqa_squad-validation-9247", "mrqa_squad-validation-929", "mrqa_squad-validation-9317", "mrqa_squad-validation-9456", "mrqa_squad-validation-9487", "mrqa_squad-validation-9492", "mrqa_squad-validation-9504", "mrqa_squad-validation-9524", "mrqa_squad-validation-9653", "mrqa_squad-validation-967", "mrqa_squad-validation-9678", "mrqa_squad-validation-9682", "mrqa_squad-validation-9699", "mrqa_squad-validation-9761", "mrqa_squad-validation-9770", "mrqa_squad-validation-9787", "mrqa_squad-validation-9837", "mrqa_squad-validation-9900", "mrqa_squad-validation-9971", "mrqa_squad-validation-9995", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-1111", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1413", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-1741", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-1780", "mrqa_triviaqa-validation-1871", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-2137", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2560", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-2990", "mrqa_triviaqa-validation-3140", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3748", "mrqa_triviaqa-validation-3881", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4059", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4590", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-4892", "mrqa_triviaqa-validation-490", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-5094", "mrqa_triviaqa-validation-5102", "mrqa_triviaqa-validation-5181", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-5660", "mrqa_triviaqa-validation-5780", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6026", "mrqa_triviaqa-validation-6110", "mrqa_triviaqa-validation-6129", "mrqa_triviaqa-validation-6244", "mrqa_triviaqa-validation-6312", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6784", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7450", "mrqa_triviaqa-validation-7612", "mrqa_triviaqa-validation-7675", "mrqa_triviaqa-validation-7748", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-799", "mrqa_triviaqa-validation-828"], "OKR": 0.931640625, "KG": 0.434375, "before_eval_results": {"predictions": ["Cuba", "The General Conference", "Satya Nadella", "24\u201310", "achieving crime control via incapacitation and deterrence", "Cosgrove Hall", "eleven", "issues under their jurisdiction", "article 49", "Corliss", "Doctor of Pharmacy (Pharm. D.)", "a simple majority vote", "the father of the house when in his home", "Battle of Dalan Balzhut", "since the 1970s", "higher returns", "US$10 a week raise over Tesla's US$18 per week salary", "1698", "Urgench", "clinical pharmacists", "exceeds any given number", "Enric Miralles", "178", "Franklin, Tennessee", "Grayback forest-firefighters", "two", "daisy robert", "Texas is among a growing number of state governments going after them", "March 24", "CEO of an engineering and construction company", "Olivia Newton-John", "Fernando Gonzalez", "July 4", "Nazi Germany", "269,000", "banned substance cortisone", "longest domestic torch relay", "collaborating with the Colombian government", "as many as 250,000", "Zapata Reyes", "Sporting Lisbon", "early Tuesday", "leftist Workers' Party", "afmado", "Arthur E. Morgan III", "head for Italy", "Gospel today", "Sheikh Sharif Sheikh Ahmed", "Jason Voorhees", "Two United Arab Emirates based companies", "people who don't even know me", "Jacob", "strife in Somalia", "insurgency", "changed the business of music", "two years", "2026", "the Anglo - Saxon King Harold Godwinson", "a fatty hump on their shoulders, drooping ears and a large dewlap", "Ethiopia", "every Rose Has Its Thorn", "every aspect of public and private life", "Tokyo", "cVS Caremark"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6929450757575757}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, true, true, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-2318", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-370", "mrqa_newsqa-validation-498", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-1261", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-3209", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-4142", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-10604", "mrqa_triviaqa-validation-2423", "mrqa_triviaqa-validation-3916", "mrqa_searchqa-validation-12902"], "SR": 0.59375, "CSR": 0.6450892857142857, "EFR": 1.0, "Overall": 0.7608147321428571}, {"timecode": 21, "before_eval_results": {"predictions": ["half", "the complexity of medications", "Vince Lombardi Trophy,", "CALIPSO", "1989", "an electrical generator", "events", "potential drug interactions, adverse drug reactions, and assess patient drug allergies", "international students", "the spread of diseases from Europe, such as smallpox.", "King Gilgamesh of Uruk and Atilla the Hun", "The Prospect Studios", "a school that has good, clear laws, fairly and democratically", "Warsaw Uprising Museum", "some extra costs are levied.", "Warsaw University of Technology", "Stanford University", "1978", "Citadel Media", "abolish the state of Israel", "2007", "continental liberalism", "Good Neighbors", "the principal front of a building", "Paul Anka", "Wings", "gleddyf Rhydderch Hael", "Leonardo", "seven", "a tin star.", "1982", "the fifth", "George Clooney", "Falkland Islands", "origami", "Wisconsin", "Norman Mailer", "Wikia", "Thank you", "Amnesty International", "parsnip", "\"Archer\"", "Justin Bieber", "The Merry Wives of Windsor", "play style", "26", "dominoes", "Hadrian", "phylloxera", "Indonesian", "green", "the Seven Year Itch", "Anita Brookner", "inflation", "Frogmore", "copper", "September 4, 2000 to February 25, 2003", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "U.S. Marshals", "Thomas Jefferson", "\"Hillbilly Handfishin'\"", "22-10.", "a frog", "the ceiling"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7180460164835165}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6361", "mrqa_squad-validation-4303", "mrqa_squad-validation-1831", "mrqa_squad-validation-8598", "mrqa_squad-validation-7400", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2069", "mrqa_triviaqa-validation-5289", "mrqa_triviaqa-validation-6743", "mrqa_triviaqa-validation-3521", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-7375", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-2081", "mrqa_naturalquestions-validation-5006", "mrqa_naturalquestions-validation-1722", "mrqa_hotpotqa-validation-2937", "mrqa_searchqa-validation-9630"], "SR": 0.65625, "CSR": 0.6455965909090908, "retrieved_ids": ["mrqa_squad-train-51651", "mrqa_squad-train-37932", "mrqa_squad-train-17951", "mrqa_squad-train-17947", "mrqa_squad-train-29099", "mrqa_squad-train-52496", "mrqa_squad-train-23338", "mrqa_squad-train-53898", "mrqa_squad-train-52167", "mrqa_squad-train-31580", "mrqa_squad-train-81091", "mrqa_squad-train-40818", "mrqa_squad-train-10820", "mrqa_squad-train-86569", "mrqa_squad-train-41360", "mrqa_squad-train-74779", "mrqa_searchqa-validation-9894", "mrqa_hotpotqa-validation-4080", "mrqa_triviaqa-validation-7405", "mrqa_newsqa-validation-498", "mrqa_searchqa-validation-9508", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-1243", "mrqa_squad-validation-10168", "mrqa_searchqa-validation-13132", "mrqa_squad-validation-600", "mrqa_triviaqa-validation-2423", "mrqa_searchqa-validation-1254", "mrqa_squad-validation-8654", "mrqa_triviaqa-validation-1925", "mrqa_searchqa-validation-7138", "mrqa_searchqa-validation-10053"], "EFR": 0.9545454545454546, "Overall": 0.7518252840909091}, {"timecode": 22, "before_eval_results": {"predictions": ["steam", "a freshwater lake", "a teacher occupying a transient or ongoing role,", "Rhin", "local producer prices by 20\u201325% in Nairobi and Mombasa.", "Queen Bees", "coal", "the LGBT community", "Sunday Service of the Methodists", "incentives for imperial and colonial powers to obtain \" information to fill in blank spaces on contemporary maps\".", "CBS and NBC", "differences in value added by labor, capital and land", "the forts Shirley had erected at the Oneida carry.", "Vince Lombardi Trophy", "Treaty provisions", "Albert Einstein", "X-ray", "as soon as they enter into force", "eighteenth century", "light-harvesting complexes", "one (or more)", "Bangladesh", "shark River Park", "1994", "adult reality show", "the nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "The Rosie Show", "Addis Ababa", "a remote part of northwestern Montana", "next year", "pirates", "Costa Rica", "Michael Partain", "terrorize", "2,700-acre", "Coptic Christians", "a strategy to reverse the Taliban's momentum and stabilize the country's government.", "Lindsey oil refinery", "musharraf", "Red Cross, UNHCR and UNICEF", "Jacob", "the Ministry of Defense", "41,280", "Larry Ellison", "Airbus A320-214", "$50", "a head injury.", "Venus Williams", "1912", "prostate cancer", "Larry Zeiger", "pro-democracy activists", "procedures", "two United Arab Emirates based companies announced on Tuesday that they will be investing in the Iraq's autonomous region of Kurdish.", "private homes", "Hot Wings", "used obscure languages as a means of secret communication during wartime", "Lois", "Newton", "Tony Burke", "47,818", "Dante", "Corinthian", "Queen Wilhelmina"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6561255344168575}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.08695652173913045, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.15384615384615385, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1863", "mrqa_squad-validation-8325", "mrqa_squad-validation-9977", "mrqa_squad-validation-9730", "mrqa_squad-validation-5764", "mrqa_squad-validation-10251", "mrqa_squad-validation-1407", "mrqa_squad-validation-8771", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-3412", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-387", "mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-2863", "mrqa_naturalquestions-validation-4496", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-5431", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-7611", "mrqa_searchqa-validation-4084"], "SR": 0.546875, "CSR": 0.6413043478260869, "EFR": 1.0, "Overall": 0.7600577445652175}, {"timecode": 23, "before_eval_results": {"predictions": ["Brompton district of the Royal Borough of Kensington and Chelsea", "Golden Gate Bridge", "Wars of Religion", "winter of 1973\u201374", "steam", "events", "15 February 1763", "major cities", "hotel room", "Oxygen therapy", "Tibetan Buddhism", "transubstantiation", "The Swahili", "left foot", "necessity", "Ancient Egypt", "catechism", "Court of Justice", "tenfold", "San Joaquin Light & Power Building", "Crips", "Overland Park, Kansas", "White Horse", "XXXHamster", "period dependent", "1987", "south", "twenty-three", "beer and soft drinks", "Swiss tourism boom", "east", "Patricia Arquette", "Robert Paul \"Robbie\" Gould III", "Dana Andrews", "The Onion", "Mary Bonauto, Susan Murray", "Malayalam", "Traugott von Sauberzweig", "the end of the 17th century", "Jenn Brown", "League of the Three Emperors", "Viacom Media Networks", "Oakland, California", "1993", "RKO", "Sullenberger III", "David Abelevich", "Bill Clinton", "Martin Scorsese", "Florence Foster Jenkins", "Haitian Revolution", "filibuster and scathing rhetoric", "600", "Macau Peninsula, Macau", "Kalahari Desert", "$1.84 billion", "2008", "The Peppercorn Pioneer", "Harold Wilson", "The father of Haleigh Cummings,", "curfew in Jaipur", "Sioux City", "Luxor", "Arnold Schoenberg"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6656622023809524}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, false, false, true, false, false, true, true, true, false, true, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.5, 1.0, 0.4, 1.0, 0.0, 0.25, 1.0, 0.28571428571428575, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.4, 0.5, 0.4, 0.0, 0.5, 0.0, 0.0, 1.0, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3053", "mrqa_squad-validation-4364", "mrqa_hotpotqa-validation-2880", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-3860", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1700", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5030", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-583", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-3003", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-5564", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-1394", "mrqa_hotpotqa-validation-4134", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-8696", "mrqa_triviaqa-validation-6557", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-1616"], "SR": 0.546875, "CSR": 0.6373697916666667, "EFR": 1.0, "Overall": 0.7592708333333335}, {"timecode": 24, "before_eval_results": {"predictions": ["suburban", "Thermochemical techniques", "was lost in the 5th Avenue laboratory fire of March 1895", "Orthodox Christians", "July 23, 1963", "twelve", "gain support from China", "lay servants", "Hitler's secret police demanded to know if they were hiding a Jew in their house", "The league", "Wednesdays", "17.5 million", "104 \u00b0F", "phycobilisomes attached to the outside of the thylakoid membranes", "seizures", "New England Patriots", "chemical bonds", "Harrods", "extra-legal", "HSH Nordbank Arena", "The two-hour finale", "Tuesday afternoon", "Sunday", "The new star is next to the iconic Hollywood headquarters of Capitol Records", "12 years after the discovery of Hettrick's stabbed and sexually mutilated corpse in a field near his trailer", "to share personal information", "David Beckham", "a strict interpretation of the law", "\"She was focused so much on learning that she didn't notice,\"", "three out of four", "Barack Obama", "Department of Homeland Security", "It takes one crooked prison worker to populate a whole prison unit with them", "Climatecare", "Venus Williams", "Graziano Transmissioni", "April 2010", "The version to be auctioned is one of fewer than 20 known copies of the Magna Carta", "Iran's parliament speaker", "purists", "Tigris and Euphrates", "The minister later apologized, telling CNN his comments had been taken out of context.", "the liberty to come and go with her face uncovered", "Bangladesh", "forgery and flying without a valid license", "surgeons", "Four", "\"Purvis continued to choke [the student] and told him, 'Don't you ever mess with my car again,'", "\"Slumdog Millionaire\"", "34", "It is \"deeply saddened\"", "Another billion people worldwide", "Amstetten, west of Vienna", "6-1", "former U.S. secretary of state", "Ciara Brady", "The standing rib roast", "isa berenices", "Scorpius", "George A. Romero", "\"50 best cities to live in.\"", "Whitehorse", "The Viga", "Tomorrowland"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6032303999719797}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, false, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.9032258064516129, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.7058823529411764, 0.0, 1.0, 1.0, 0.29629629629629634, 1.0, 0.4444444444444445, 1.0, 0.7272727272727273, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 0.0, 0.2666666666666667, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8452", "mrqa_squad-validation-6998", "mrqa_squad-validation-170", "mrqa_squad-validation-8780", "mrqa_squad-validation-3599", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-57", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-806", "mrqa_naturalquestions-validation-3206", "mrqa_naturalquestions-validation-6074", "mrqa_triviaqa-validation-424", "mrqa_triviaqa-validation-7252", "mrqa_searchqa-validation-12546"], "SR": 0.515625, "CSR": 0.6325000000000001, "retrieved_ids": ["mrqa_squad-train-16694", "mrqa_squad-train-30216", "mrqa_squad-train-79019", "mrqa_squad-train-77150", "mrqa_squad-train-74998", "mrqa_squad-train-20963", "mrqa_squad-train-58658", "mrqa_squad-train-79992", "mrqa_squad-train-32392", "mrqa_squad-train-63980", "mrqa_squad-train-51254", "mrqa_squad-train-26118", "mrqa_squad-train-82681", "mrqa_squad-train-42395", "mrqa_squad-train-79296", "mrqa_squad-train-42320", "mrqa_hotpotqa-validation-5178", "mrqa_naturalquestions-validation-1704", "mrqa_newsqa-validation-3316", "mrqa_triviaqa-validation-2519", "mrqa_squad-validation-2315", "mrqa_searchqa-validation-1654", "mrqa_squad-validation-9452", "mrqa_newsqa-validation-1261", "mrqa_triviaqa-validation-2745", "mrqa_squad-validation-1862", "mrqa_triviaqa-validation-3388", "mrqa_newsqa-validation-247", "mrqa_triviaqa-validation-6312", "mrqa_newsqa-validation-1648", "mrqa_hotpotqa-validation-407", "mrqa_triviaqa-validation-2693"], "EFR": 1.0, "Overall": 0.7582968750000001}, {"timecode": 25, "before_eval_results": {"predictions": ["more equality in the income distribution", "Lunar Module Pilot", "the Apollo 11 mission", "few British troops.", "linebacker", "unicellular", "high wages", "evenly round the body", "ABC", "the solution", "He was shown to be physically imposing, an equal in stature to the secular German princes with whom he would join forces to spread Lutheranism", "Sweden", "macrophages and lymphocytes", "14,000", "Tony Hawk", "C. J. Anderson", "biologically important energy-carrying", "average workers", "brigadier general", "1909", "psilocybin", "Scotland", "Raimond Gaita", "James Taylor", "Haitian Revolution", "saloon-keeper", "July", "Wayman Tisdale", "Bath, Maine", "26,000", "August 14, 1848", "The Dressmaker", "punk rock", "Dark Heresy", "North Holland", "Elena Verdugo", "from 1874 until 1994", "1698", "UFC 50: The War of '04", "a few minutes", "1951", "Mwabvi river", "John M. Dowd", "Amber Heard", "the 2011 Pulitzer Prize in General Nonfiction", "Reverend Timothy \"Tim\" Lovejoy", "1939", "Patterns of Sexual Behavior", "George Raft", "UFC Fight Pass", "McLean, Virginia", "Meryl Streep", "Odorama", "February 22, 1968", "Bourbon County", "Long Island", "Jacques Cousteau", "Bodhidharma", "Paul Robinson", "some of the Awa", "helped finance the insurgency against U.S. troops in Iraq with Iraqi funds he transferred to Syria before Hussein's government collapsed in April 2002.", "a large river inputs lots of fresh water", "Afghanistan", "1878"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6736271367521367}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.16, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.7499999999999999, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.3076923076923077, 0.25, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3997", "mrqa_squad-validation-6453", "mrqa_squad-validation-7791", "mrqa_squad-validation-2599", "mrqa_squad-validation-3624", "mrqa_hotpotqa-validation-2140", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-4245", "mrqa_hotpotqa-validation-1381", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-4974", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-5715", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-2681", "mrqa_naturalquestions-validation-5143", "mrqa_triviaqa-validation-310", "mrqa_newsqa-validation-1848", "mrqa_searchqa-validation-4623", "mrqa_naturalquestions-validation-7387"], "SR": 0.578125, "CSR": 0.6304086538461539, "EFR": 1.0, "Overall": 0.7578786057692308}, {"timecode": 26, "before_eval_results": {"predictions": ["gender roles and customs", "80,000", "decision problems", "9 March 1508", "reduce consumer costs", "2020", "their greatest common divisor is one", "Sophocles", "low latitude", "1726", "environmental factors", "backing for the uprising", "around 28,000", "The Reconstruction of Religious Thought in Islam", "July", "Harvey Martin", "a water pump", "state senators", "Pakistan's High Commission in India", "it", "Nearly all of Britain's troops in Iraq will have left by the week's end.", "Susan Atkins", "almost 100 vessels", "40 militants", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "The remains of Cologne's archive building", "he was one of 10 gunmen who attacked several targets in Mumbai", "the United States", "the toughest challenge yet in his nearly 28 years of rule.", "the 11th century Preah Vihear temple", "Monday night.", "the 45-year-old future president", "about 12 million", "Mark Fields", "the United States", "his grandfather was a \"Zionist\" and \"a zealous supporter of the usurper entity, and a prominent member of a number of Zionist hate organizations", "Arthur Mutambara.", "1995", "Turkey", "Prime Minister Stephen Harper", "piers Morgan", "an Omani national", "East Java", "little blue booties.", "Thaksin Shinawatra", "gasoline", "diplomatic relations", "auxiliary lock", "prostate cancer", "The ACLU", "fight back against Israel in Gaza.", "soldiers", "off the coast of Dubai", "cancer", "five minutes before commandos descended from ropes that dangled from helicopters, Zoabi said", "alcohol withdrawal", "the optic disc to the optic chiasma", "cotton", "Samson", "Jack Ridley", "bioelectromagnetics", "Norway", "p.m.", "downdraft"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6292270875830658}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, false, true, false, true, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08695652173913045, 0.0, 0.0, 1.0, 0.8, 0.5, 0.18181818181818182, 0.25000000000000006, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.7499999999999999, 1.0, 1.0, 0.10810810810810811, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5555555555555556, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1689", "mrqa_squad-validation-7041", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1552", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-12", "mrqa_newsqa-validation-1580", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-1846", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-1292", "mrqa_naturalquestions-validation-4442", "mrqa_naturalquestions-validation-3368", "mrqa_hotpotqa-validation-2944", "mrqa_searchqa-validation-8943", "mrqa_searchqa-validation-11260"], "SR": 0.53125, "CSR": 0.6267361111111112, "EFR": 1.0, "Overall": 0.7571440972222223}, {"timecode": 27, "before_eval_results": {"predictions": ["Milka, Angelina and Marica", "Working Group chairs", "nine nations", "Stanley Steamer", "nerves", "Luther's rediscovery of \"Christ and His salvation\"", "a program of coordinated, evolving projects sponsored by the National Science Foundation", "nervous breakdown", "NewcastleGateshead", "cholecalciferol", "a legitimate medical purpose by a licensed practitioner", "government and the National Assembly and the Senate", "vocational subjects", "four nations", "an eight-year term", "Calendar for Fixing the Seasons", "the Scilly Isles", "\"Good Morning to All\"", "The Simpsons", "Brazil", "salmon", "Annabel Croft", "Mary Pickford", "flowering plants", "Bond", "Chester", "a skunk", "Daniel Boone", "the Chartered Institute for the Management of Sport and Physical Activity", "Argentina", "penny", "Lyn Dershowitz", "Sandy Welch's", "couch", "Morten Skovsby", "the Nile", "West Ham", "Las Vegas", "Charlie Brown", "Joanne Harris", "a leopard seal", "On the Braden Beat", "parsnip", "port", "\"World\u2019s Greatest Athlete\u201d", "turbines", "waterfowl", "Walter Hagen", "Dirty Dancing", "Judy Cassab", "apples", "a bank", "Texas", "Tuscaloosa", "4 September 1936", "35 to 40 hours per week", "neuro-orthopaedic", "relationship with Apple co-founder Steve Jobs", "Bhutto was the de facto leader of her father's Pakistan People's Party.", "Iowa,", "cell of Cavour", "The Biggest Little City", "Douglas MacArthur", "flavonoids"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5521834935897436}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.4285714285714285, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2133", "mrqa_squad-validation-4846", "mrqa_squad-validation-6426", "mrqa_squad-validation-4404", "mrqa_triviaqa-validation-1538", "mrqa_triviaqa-validation-3021", "mrqa_triviaqa-validation-6610", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-6237", "mrqa_triviaqa-validation-4339", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-4183", "mrqa_triviaqa-validation-3946", "mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-1541", "mrqa_triviaqa-validation-3167", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-3385", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-7193", "mrqa_hotpotqa-validation-260", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-1072", "mrqa_searchqa-validation-16770", "mrqa_searchqa-validation-2631", "mrqa_searchqa-validation-7657"], "SR": 0.453125, "CSR": 0.6205357142857143, "retrieved_ids": ["mrqa_squad-train-73167", "mrqa_squad-train-25554", "mrqa_squad-train-31431", "mrqa_squad-train-56676", "mrqa_squad-train-1557", "mrqa_squad-train-57289", "mrqa_squad-train-23082", "mrqa_squad-train-47022", "mrqa_squad-train-44560", "mrqa_squad-train-60271", "mrqa_squad-train-15141", "mrqa_squad-train-69299", "mrqa_squad-train-38635", "mrqa_squad-train-76985", "mrqa_squad-train-70783", "mrqa_squad-train-56104", "mrqa_squad-validation-9063", "mrqa_squad-validation-2634", "mrqa_triviaqa-validation-2092", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-948", "mrqa_hotpotqa-validation-1179", "mrqa_newsqa-validation-2152", "mrqa_squad-validation-2743", "mrqa_triviaqa-validation-1164", "mrqa_squad-validation-4662", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-2544", "mrqa_squad-validation-5400", "mrqa_squad-validation-652", "mrqa_newsqa-validation-848", "mrqa_naturalquestions-validation-5417"], "EFR": 0.9714285714285714, "Overall": 0.7501897321428572}, {"timecode": 28, "before_eval_results": {"predictions": ["quantum electrodynamics", "Genghis Khan and his family", "westerns and detective series", "one astronaut", "The NBC Blue Network", "Friday", "CD40", "lion, leopard, buffalo, rhinoceros, and elephant", "high pressure", "\"exterminate\" all non-Dalek beings", "City council", "a second Bachelor's degree", "Miocene", "Rhin", "in the kingdom", "james bond", "Madagascar", "shoulders", "Muriel Spark", "Japan", "John Peel", "argument form", "Christine Keeler", "jerry Hancock", "leopons", "frogs", "The Verruckt", "bologna", "weather", "bihari cuisine", "jerry", "cytok", "bridge", "New Jersey", "fluorine", "the Northern line", "harold wilson", "Birmingham", "bbc", "\"black\"", "Venezuela", "leona water", "saxophone", "pot", "high noon", "1930", "Bosnian", "narwhals", "eagle", "63 to 144 inches", "Blind Beggar pub", "Vatican Crypt", "a tiger", "b\u00e9la Bart\u00f3k", "Andrew Garfield", "Narya", "Delilah Rene", "Tianhe Stadium", "Japan and Singapore,", "10 a.m.", "Rent", "james bond", "Washington", "Mineola"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6644097222222223}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5906", "mrqa_squad-validation-8278", "mrqa_squad-validation-7778", "mrqa_triviaqa-validation-1109", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-7662", "mrqa_triviaqa-validation-515", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-2087", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-4045", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-2011", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-5656", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-6264", "mrqa_naturalquestions-validation-19", "mrqa_hotpotqa-validation-1952", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2580", "mrqa_searchqa-validation-7462"], "SR": 0.578125, "CSR": 0.619073275862069, "EFR": 1.0, "Overall": 0.7556115301724138}, {"timecode": 29, "before_eval_results": {"predictions": ["ten times their own weight", "nine", "a year", "2003", "Africa", "Nederrijn", "a lack of understanding of the legal ramifications, or due to a fear of seeming rude", "Elie Metchnikoff", "domestic", "50%", "7\u20134\u20132\u20133", "Connectional Table", "2nd century BCE", "1060s", "oscar", "tksoulchild", "panther", "Vietnam", "vatican city", "leona", "pannier", "dollar", "cars", "oven", "diphthong", "vatican city", "FLORIDA", "Austria", "oscar", "Seventy-six trombones", "red", "bbc", "ER", "crustacean", "chicken run", "Robert Bork", "Luzon", "vainberg", "The Big Sleep", "quid", "bismarck", "the Cliffs of Dover", "Prague", "footnotes", "uncle vanya", "city", "Yitzhak Rabin", "Quiz", "The Boondocks", "vatican city", "bumblebee", "short-tailed weasel", "tariq jahan", "oven", "central plains", "49", "hydrogen", "oscar", "southwestern", "Jenji Kohan", "Turkey's foreign policy", "a monthly allowance", "Via Vai", "Black Abbots"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5791666666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7537", "mrqa_squad-validation-8490", "mrqa_searchqa-validation-28", "mrqa_searchqa-validation-13985", "mrqa_searchqa-validation-4675", "mrqa_searchqa-validation-8718", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-4467", "mrqa_searchqa-validation-2393", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-1650", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-893", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-14246", "mrqa_searchqa-validation-364", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-4904", "mrqa_searchqa-validation-1761", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-11505", "mrqa_searchqa-validation-13246", "mrqa_searchqa-validation-6317", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-1871", "mrqa_newsqa-validation-3833"], "SR": 0.5625, "CSR": 0.6171875, "EFR": 1.0, "Overall": 0.755234375}, {"timecode": 30, "UKR": 0.7734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1063", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-119", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-1440", "mrqa_hotpotqa-validation-1449", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-2140", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2293", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-243", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-303", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3317", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3502", "mrqa_hotpotqa-validation-3860", "mrqa_hotpotqa-validation-3861", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3966", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4419", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-4699", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-4974", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5247", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5516", "mrqa_hotpotqa-validation-5729", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-5896", "mrqa_hotpotqa-validation-635", "mrqa_hotpotqa-validation-653", "mrqa_hotpotqa-validation-686", "mrqa_hotpotqa-validation-774", "mrqa_hotpotqa-validation-88", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-965", "mrqa_naturalquestions-validation-10162", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-9367", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9986", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1552", "mrqa_newsqa-validation-1580", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1592", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2462", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-2912", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3152", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3267", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-3400", "mrqa_newsqa-validation-3412", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-370", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-498", "mrqa_newsqa-validation-542", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-806", "mrqa_newsqa-validation-868", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-1243", "mrqa_searchqa-validation-12558", "mrqa_searchqa-validation-12609", "mrqa_searchqa-validation-12771", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-13966", "mrqa_searchqa-validation-14126", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-14606", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-15590", "mrqa_searchqa-validation-16371", "mrqa_searchqa-validation-2917", "mrqa_searchqa-validation-3069", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-320", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-3841", "mrqa_searchqa-validation-4175", "mrqa_searchqa-validation-4619", "mrqa_searchqa-validation-4675", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5392", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-5692", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-7611", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-7920", "mrqa_searchqa-validation-801", "mrqa_searchqa-validation-9090", "mrqa_searchqa-validation-917", "mrqa_searchqa-validation-9508", "mrqa_searchqa-validation-9604", "mrqa_squad-validation-10009", "mrqa_squad-validation-10018", "mrqa_squad-validation-10025", "mrqa_squad-validation-10027", "mrqa_squad-validation-10064", "mrqa_squad-validation-101", "mrqa_squad-validation-10264", "mrqa_squad-validation-10294", "mrqa_squad-validation-10370", "mrqa_squad-validation-10474", "mrqa_squad-validation-10491", "mrqa_squad-validation-10500", "mrqa_squad-validation-1068", "mrqa_squad-validation-1081", "mrqa_squad-validation-1148", "mrqa_squad-validation-1156", "mrqa_squad-validation-1272", "mrqa_squad-validation-1273", "mrqa_squad-validation-1355", "mrqa_squad-validation-1371", "mrqa_squad-validation-1550", "mrqa_squad-validation-159", "mrqa_squad-validation-167", "mrqa_squad-validation-1684", "mrqa_squad-validation-1758", "mrqa_squad-validation-1769", "mrqa_squad-validation-1889", "mrqa_squad-validation-195", "mrqa_squad-validation-1977", "mrqa_squad-validation-1980", "mrqa_squad-validation-1996", "mrqa_squad-validation-2140", "mrqa_squad-validation-2146", "mrqa_squad-validation-2169", "mrqa_squad-validation-2191", "mrqa_squad-validation-2329", "mrqa_squad-validation-237", "mrqa_squad-validation-2411", "mrqa_squad-validation-2423", "mrqa_squad-validation-2453", "mrqa_squad-validation-2461", "mrqa_squad-validation-2467", "mrqa_squad-validation-2468", "mrqa_squad-validation-2473", "mrqa_squad-validation-2523", "mrqa_squad-validation-2540", "mrqa_squad-validation-256", "mrqa_squad-validation-258", "mrqa_squad-validation-2616", "mrqa_squad-validation-2634", "mrqa_squad-validation-2709", "mrqa_squad-validation-2778", "mrqa_squad-validation-2834", "mrqa_squad-validation-2885", "mrqa_squad-validation-2900", "mrqa_squad-validation-2909", "mrqa_squad-validation-2994", "mrqa_squad-validation-3028", "mrqa_squad-validation-3048", "mrqa_squad-validation-3063", "mrqa_squad-validation-3081", "mrqa_squad-validation-3125", "mrqa_squad-validation-3165", "mrqa_squad-validation-3166", "mrqa_squad-validation-317", "mrqa_squad-validation-318", "mrqa_squad-validation-319", "mrqa_squad-validation-32", "mrqa_squad-validation-3207", "mrqa_squad-validation-3233", "mrqa_squad-validation-3264", "mrqa_squad-validation-3267", "mrqa_squad-validation-327", "mrqa_squad-validation-3403", "mrqa_squad-validation-343", "mrqa_squad-validation-3469", "mrqa_squad-validation-3522", "mrqa_squad-validation-3641", "mrqa_squad-validation-3740", "mrqa_squad-validation-3751", "mrqa_squad-validation-3791", "mrqa_squad-validation-3837", "mrqa_squad-validation-3860", "mrqa_squad-validation-3921", "mrqa_squad-validation-4080", "mrqa_squad-validation-4122", "mrqa_squad-validation-4144", "mrqa_squad-validation-415", "mrqa_squad-validation-4267", "mrqa_squad-validation-4303", "mrqa_squad-validation-4325", "mrqa_squad-validation-4366", "mrqa_squad-validation-4429", "mrqa_squad-validation-4437", "mrqa_squad-validation-4534", "mrqa_squad-validation-4557", "mrqa_squad-validation-4662", "mrqa_squad-validation-4670", "mrqa_squad-validation-472", "mrqa_squad-validation-4795", "mrqa_squad-validation-4806", "mrqa_squad-validation-4875", "mrqa_squad-validation-4958", "mrqa_squad-validation-5003", "mrqa_squad-validation-5010", "mrqa_squad-validation-5054", "mrqa_squad-validation-5077", "mrqa_squad-validation-5078", "mrqa_squad-validation-5134", "mrqa_squad-validation-5162", "mrqa_squad-validation-5179", "mrqa_squad-validation-5185", "mrqa_squad-validation-5205", "mrqa_squad-validation-5256", "mrqa_squad-validation-5319", "mrqa_squad-validation-5351", "mrqa_squad-validation-5382", "mrqa_squad-validation-5400", "mrqa_squad-validation-5457", "mrqa_squad-validation-5473", "mrqa_squad-validation-5474", "mrqa_squad-validation-5557", "mrqa_squad-validation-5600", "mrqa_squad-validation-5607", "mrqa_squad-validation-5609", "mrqa_squad-validation-5611", "mrqa_squad-validation-5672", "mrqa_squad-validation-5684", "mrqa_squad-validation-5754", "mrqa_squad-validation-5760", "mrqa_squad-validation-5840", "mrqa_squad-validation-5906", "mrqa_squad-validation-5939", "mrqa_squad-validation-5948", "mrqa_squad-validation-596", "mrqa_squad-validation-5966", "mrqa_squad-validation-6024", "mrqa_squad-validation-604", "mrqa_squad-validation-6086", "mrqa_squad-validation-6142", "mrqa_squad-validation-6161", "mrqa_squad-validation-6224", "mrqa_squad-validation-6227", "mrqa_squad-validation-6254", "mrqa_squad-validation-6268", "mrqa_squad-validation-6278", "mrqa_squad-validation-6284", "mrqa_squad-validation-6292", "mrqa_squad-validation-6349", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6380", "mrqa_squad-validation-6415", "mrqa_squad-validation-643", "mrqa_squad-validation-6434", "mrqa_squad-validation-6447", "mrqa_squad-validation-6457", "mrqa_squad-validation-6474", "mrqa_squad-validation-6492", "mrqa_squad-validation-654", "mrqa_squad-validation-6622", "mrqa_squad-validation-6624", "mrqa_squad-validation-6650", "mrqa_squad-validation-6744", "mrqa_squad-validation-6766", "mrqa_squad-validation-6777", "mrqa_squad-validation-680", "mrqa_squad-validation-6807", "mrqa_squad-validation-6812", "mrqa_squad-validation-6879", "mrqa_squad-validation-6941", "mrqa_squad-validation-7005", "mrqa_squad-validation-7026", "mrqa_squad-validation-7049", "mrqa_squad-validation-7051", "mrqa_squad-validation-708", "mrqa_squad-validation-7162", "mrqa_squad-validation-7170", "mrqa_squad-validation-7198", "mrqa_squad-validation-7208", "mrqa_squad-validation-7217", "mrqa_squad-validation-725", "mrqa_squad-validation-7260", "mrqa_squad-validation-7263", "mrqa_squad-validation-7284", "mrqa_squad-validation-7370", "mrqa_squad-validation-7387", "mrqa_squad-validation-7400", "mrqa_squad-validation-7448", "mrqa_squad-validation-7458", "mrqa_squad-validation-7492", "mrqa_squad-validation-7500", "mrqa_squad-validation-7508", "mrqa_squad-validation-7550", "mrqa_squad-validation-7562", "mrqa_squad-validation-7665", "mrqa_squad-validation-767", "mrqa_squad-validation-7794", "mrqa_squad-validation-7851", "mrqa_squad-validation-7859", "mrqa_squad-validation-7934", "mrqa_squad-validation-7936", "mrqa_squad-validation-7949", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8123", "mrqa_squad-validation-8171", "mrqa_squad-validation-8180", "mrqa_squad-validation-821", "mrqa_squad-validation-8210", "mrqa_squad-validation-8234", "mrqa_squad-validation-8289", "mrqa_squad-validation-83", "mrqa_squad-validation-8319", "mrqa_squad-validation-8325", "mrqa_squad-validation-8338", "mrqa_squad-validation-8394", "mrqa_squad-validation-8418", "mrqa_squad-validation-8428", "mrqa_squad-validation-8476", "mrqa_squad-validation-8478", "mrqa_squad-validation-8527", "mrqa_squad-validation-8599", "mrqa_squad-validation-862", "mrqa_squad-validation-8652", "mrqa_squad-validation-8664", "mrqa_squad-validation-8723", "mrqa_squad-validation-8724", "mrqa_squad-validation-8771", "mrqa_squad-validation-8839", "mrqa_squad-validation-8840", "mrqa_squad-validation-8902", "mrqa_squad-validation-9020", "mrqa_squad-validation-9036", "mrqa_squad-validation-9243", "mrqa_squad-validation-9247", "mrqa_squad-validation-9247", "mrqa_squad-validation-9486", "mrqa_squad-validation-9492", "mrqa_squad-validation-9504", "mrqa_squad-validation-9653", "mrqa_squad-validation-9678", "mrqa_squad-validation-9682", "mrqa_squad-validation-9761", "mrqa_squad-validation-9762", "mrqa_squad-validation-9770", "mrqa_squad-validation-9837", "mrqa_squad-validation-9900", "mrqa_squad-validation-999", "mrqa_squad-validation-9995", "mrqa_triviaqa-validation-1111", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1213", "mrqa_triviaqa-validation-122", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1413", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1538", "mrqa_triviaqa-validation-1681", "mrqa_triviaqa-validation-1741", "mrqa_triviaqa-validation-1884", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-1977", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2376", "mrqa_triviaqa-validation-2618", "mrqa_triviaqa-validation-2667", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-310", "mrqa_triviaqa-validation-3140", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3385", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-3471", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3501", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3620", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3881", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-3946", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-4045", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-4590", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-5040", "mrqa_triviaqa-validation-5102", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-5318", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5571", "mrqa_triviaqa-validation-5655", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6006", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6110", "mrqa_triviaqa-validation-6244", "mrqa_triviaqa-validation-6312", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-7375", "mrqa_triviaqa-validation-7675", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-987"], "OKR": 0.88671875, "KG": 0.48203125, "before_eval_results": {"predictions": ["mid-Eocene", "2004", "the algorithm is said to accept the input string", "disciples", "data sampling is biased away from the center of the Amazon basin", "downward trend", "Westinghouse Electric", "captured Fort Beaus\u00e9jour", "Saxon chancellery", "Advanced Steam", "July 23, 1963", "Stanford University", "Cuba", "French power in North America meant the disappearance of a strong ally and counterweight to British expansion", "the OSS", "Elijah Muhammad", "bamboos", "Benjamin Franklin", "Guatemala", "Moscow", "the Monkees", "Mount Hood", "the Backstreet Boys", "the hand", "the French flag", "a magnetic compass", "the Black Maria", "Walt Kelly", "the evaporator", "the QWERTY keyboard", "kozo", "the trial", "Raytheon", "red", "a burn", "Thomas Hardy", "Signs", "the British", "the cold of the ice", "cream", "adultery", "Sunday, November 6, 2016", "Syracuse", "the Shoshone Indians", "(NCO)", "the breath", "Toulouse", "dualism", "(D) Griffith", "French Guiana", "Rembrandt van Rijn", "a fruitcake", "a violin", "plain and tall", "Bill Irwin", "Eric Clapton", "perfumer", "each year", "the 1999 Odisha cyclone", "15,000 people", "two remaining crew members", "France", "evidence of them having a child.", "the Gulf of Aden"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5330729166666667}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.4, 0.0, 0.16666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1655", "mrqa_squad-validation-4390", "mrqa_squad-validation-3666", "mrqa_squad-validation-10304", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-4646", "mrqa_searchqa-validation-11972", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-12239", "mrqa_searchqa-validation-1561", "mrqa_searchqa-validation-10370", "mrqa_searchqa-validation-5735", "mrqa_searchqa-validation-13224", "mrqa_searchqa-validation-11367", "mrqa_searchqa-validation-14007", "mrqa_searchqa-validation-16821", "mrqa_searchqa-validation-3933", "mrqa_searchqa-validation-5043", "mrqa_searchqa-validation-9552", "mrqa_searchqa-validation-960", "mrqa_searchqa-validation-4605", "mrqa_searchqa-validation-3782", "mrqa_searchqa-validation-2901", "mrqa_searchqa-validation-8256", "mrqa_searchqa-validation-9870", "mrqa_searchqa-validation-6586", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-3017", "mrqa_newsqa-validation-3587", "mrqa_newsqa-validation-929", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-2076"], "SR": 0.453125, "CSR": 0.6118951612903225, "retrieved_ids": ["mrqa_squad-train-65967", "mrqa_squad-train-57701", "mrqa_squad-train-16532", "mrqa_squad-train-26964", "mrqa_squad-train-61761", "mrqa_squad-train-50347", "mrqa_squad-train-20618", "mrqa_squad-train-83700", "mrqa_squad-train-70510", "mrqa_squad-train-72016", "mrqa_squad-train-65653", "mrqa_squad-train-42999", "mrqa_squad-train-13037", "mrqa_squad-train-58954", "mrqa_squad-train-69142", "mrqa_squad-train-71489", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-2990", "mrqa_searchqa-validation-15483", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10900", "mrqa_naturalquestions-validation-4784", "mrqa_triviaqa-validation-6548", "mrqa_squad-validation-10251", "mrqa_searchqa-validation-10053", "mrqa_triviaqa-validation-4655", "mrqa_searchqa-validation-7560", "mrqa_hotpotqa-validation-965", "mrqa_hotpotqa-validation-3481", "mrqa_squad-validation-821", "mrqa_squad-validation-9807", "mrqa_squad-validation-6913"], "EFR": 1.0, "Overall": 0.7508165322580644}, {"timecode": 31, "before_eval_results": {"predictions": ["Maria Fold and Thrust Belt", "OpenTV", "E.I. du Pont", "a shortage of male teachers", "water flow through the body cavity", "John D. Rockefeller", "a lot of waste", "his own men", "Gaelic", "New Orleans", "five", "half as much", "John Pell, Lord of Pelham Manor", "Cher", "a doses", "a treasure map", "San Antonio", "Cantor", "Hindu", "the AMISTAD", "Wings of Desire", "malaria", "Jamie Lee Curtis", "King Louis XV", "Shakira", "Yokohama", "Stanton Avery", "Morse Code", "bolivar", "Pete Rose", "Hannibal", "Mercury", "Parma", "Iran", "Oahu", "a dog's", "Lincoln", "Porcelain", "the Blue Nile", "insulin", "Robert Taft", "Alexander Hamilton", "the Colossus of Rhodes", "cereal", "Nepali", "Ernest Hemingway", "Stephen Hawking", "center", "guru", "lamb", "Beverly Hills", "the bumblebee", "FDR", "the Hermes", "season two", "spectroscopic notation", "Switzerland", "mustard", "Polese", "south africa", "\"It has never been the policy of this president or this administration to torture.\"", "\"brain hacking\"", "heads of federal executive departments who form the Cabinet of the United States", "it can refer to either peace between two entities ( especially between man and God or between two countries )"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6569131663292849}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, true, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.25, 1.0, 0.4, 0.0, 1.0, 0.0, 0.8, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.15384615384615385, 1.0, 0.0, 0.10526315789473684]}}, "before_error_ids": ["mrqa_squad-validation-6128", "mrqa_squad-validation-341", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-16920", "mrqa_searchqa-validation-460", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-10333", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-15114", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-11562", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-12500", "mrqa_searchqa-validation-3737", "mrqa_searchqa-validation-6966", "mrqa_searchqa-validation-11241", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-8067", "mrqa_searchqa-validation-15563", "mrqa_searchqa-validation-16250", "mrqa_naturalquestions-validation-585", "mrqa_triviaqa-validation-902", "mrqa_hotpotqa-validation-3953", "mrqa_newsqa-validation-3818", "mrqa_naturalquestions-validation-8982", "mrqa_naturalquestions-validation-645"], "SR": 0.578125, "CSR": 0.61083984375, "EFR": 1.0, "Overall": 0.75060546875}, {"timecode": 32, "before_eval_results": {"predictions": ["dioxygen", "one of the first peer-to-peer network architectures", "the Kenyan Coast", "SyFy", "1530", "the temperance movement", "12 December 1964", "18", "British troops", "Harvard Yard", "all war", "Mongol peace", "the thymus gland", "red", "rani", "the endive", "the 18th century", "the gypsy flamenco", "Nero", "the Manse", "( Kenneth) Gorelick", "Rio de Janeiro", "the earth", "the Lone Ranger", "( Helen) Hayes", "New Revised Standard Version", "the Bluetooth protocol", "(Ma) Barker", "the St. Valentine's Day Massacre", "Ratatouille", "Spain", "(Louis) Comfort Tiffany", "the pig", "Jericho", "(Giovanni) Cabot", "( George) Orwell", "the 1844 doctrines of Christendom", "eggs", "Christmas Story", "(U.S. president Grover) Cleveland", "Hawaii", "rhythmic", "a falcon", "a prayer", "Sicily", "the Lord of the Rings", "Georgia O'Keeffe", "Athens", "Roxanne", "the 18th century", "Olmedo", "the trans fat", "( Caspar) Weinberger", "the Ninja Turtles", "Amerigo Vespucci", "1979", "an eclipse", "I Will survive", "Girls' Generation", "Guangzhou, China", "Oxygen", "\"The oceans are kind of the last frontier for use and development,\"", "$627", "demolition crews blew up an ice jam"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6805253623188405}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 0.08695652173913045, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4676", "mrqa_squad-validation-8369", "mrqa_searchqa-validation-16221", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-7873", "mrqa_searchqa-validation-286", "mrqa_searchqa-validation-7410", "mrqa_searchqa-validation-2598", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-11603", "mrqa_searchqa-validation-1105", "mrqa_searchqa-validation-2747", "mrqa_searchqa-validation-11804", "mrqa_searchqa-validation-2560", "mrqa_searchqa-validation-5893", "mrqa_searchqa-validation-10006", "mrqa_searchqa-validation-2889", "mrqa_searchqa-validation-12476", "mrqa_searchqa-validation-16241", "mrqa_searchqa-validation-13887", "mrqa_searchqa-validation-1104", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-16405", "mrqa_searchqa-validation-6217", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3453"], "SR": 0.578125, "CSR": 0.6098484848484849, "EFR": 0.9629629629629629, "Overall": 0.7429997895622895}, {"timecode": 33, "before_eval_results": {"predictions": ["the Colony of Victoria Act", "1,345,591", "Cestum veneris (\"Venus' girdle\")", "the traditional Mongolian aristocracy", "nonfunctional pseudogenes", "Northern Europe and the Mid-Atlantic temperate climate", "to coordinate the response to the embargo", "the Pliocene period", "OMSF program controller", "yellow fever outbreaks", "the thylakoid network", "both fish larvae and organisms that would otherwise have fed the fish", "Jupiter", "Nyasaland", "regal", "Michael Holding", "Manfred von Richthofen", "( (Frankie) Laine", "Malteshoes", "folklore", "180", "the opossum", "Florence", "Rodgers and Hammerstein", "Washington", "plants that will do well with the sun, soil, and water", "South Africa", "the Tussauds", "the Hippety Hopper", "marries", "three", "Peter Paul Rubens", "Goran Ivanisevic", "the tomato paste-based orange borscht", "June", "\"Frances Ethel Gumm\"", "Mel Brooks", "Solo", "George Miller", "the Plantagenets", "Hydrogen", "Tesla", "copper", "David Frost", "Charlie Chaplin", "Edward VIII", "Saffron", "The Siberian tiger", "Sousa", "New Zealand", "the giraffe", "the Crusades", "( (Ernie)", "big house", "Kevin Garnett", "1980", "Led Zeppelin", "Marco Hietala", "Zed", "$60 billion", "\"Best Children's Hospitals\"", "1930s CINEMA", "Portugal", "5 liters"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6351934523809524}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, false, true, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.2857142857142857]}}, "before_error_ids": ["mrqa_squad-validation-7151", "mrqa_squad-validation-4580", "mrqa_squad-validation-9869", "mrqa_squad-validation-3855", "mrqa_squad-validation-4319", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-1261", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-2686", "mrqa_triviaqa-validation-3543", "mrqa_triviaqa-validation-6141", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-454", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-1956", "mrqa_naturalquestions-validation-2008", "mrqa_naturalquestions-validation-2732", "mrqa_hotpotqa-validation-2711", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-15800", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-4054"], "SR": 0.546875, "CSR": 0.6079963235294117, "retrieved_ids": ["mrqa_squad-train-43046", "mrqa_squad-train-70287", "mrqa_squad-train-68628", "mrqa_squad-train-11117", "mrqa_squad-train-61269", "mrqa_squad-train-8730", "mrqa_squad-train-67113", "mrqa_squad-train-61448", "mrqa_squad-train-17412", "mrqa_squad-train-27438", "mrqa_squad-train-32224", "mrqa_squad-train-46555", "mrqa_squad-train-4453", "mrqa_squad-train-54832", "mrqa_squad-train-79416", "mrqa_squad-train-30822", "mrqa_squad-validation-1", "mrqa_hotpotqa-validation-4080", "mrqa_searchqa-validation-3739", "mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-1035", "mrqa_squad-validation-1407", "mrqa_searchqa-validation-14373", "mrqa_squad-validation-1550", "mrqa_newsqa-validation-2040", "mrqa_hotpotqa-validation-2897", "mrqa_triviaqa-validation-3946", "mrqa_hotpotqa-validation-3984", "mrqa_triviaqa-validation-2087", "mrqa_newsqa-validation-2152", "mrqa_searchqa-validation-16035"], "EFR": 1.0, "Overall": 0.7500367647058823}, {"timecode": 34, "before_eval_results": {"predictions": ["powerful", "an inauspicious typhoon", "the municipal building inspector", "Battle of Hastings", "gold", "Victoria", "western European", "Muslim and Chinese", "four", "the town council", "tea or porridge with bread, chapati, mahamri, boiled sweet potatoes or yams", "admit he cheated on you", "January 24, 2006", "William Shakespeare", "George Lucas", "The Da Vinci Code", "the Russian air force,", "10 below in Chicago, Illlinois.", "18th time in their extensive history", "100% of its byproducts", "Switzerland", "\" learn how to dance and feel sexy,\"", "misdemeanor assault charges", "wings", "More than 22 million people in sub-Saharan Africa are infected with HIV,", "in a tenement in the Mumbai suburb of Chembur,", "nirvana", "firefighter", "1959", "he spent the first night in his car.\"", "American soldiers", "Empire of the Sun", "genocide", "Her husband and attorney, James Whitehouse,", "a bronze medal", "had his personal.40-caliber pistol,", "Alaska or Hawaii.", "south africa", "He's still got a lot of major surgery ahead of him,", "one count of attempted murder in the second degree in the October 12 attack.\"", "Intensifying violence, food shortages and widespread drought", "Kerstin", "India", "Hakeemullah Mehsud", "LEDs", "200 human bodies at various life stages", "Cambodian territory", "number ones", "red", "more than 2.5 million copies", "same-sex civil unions,", "Hezbollah", "London", "his brother, who died in action in the United States Army", "Derek Hough", "Lyonesse", "football", "Clovis I", "the Willis (Sears) Tower", "the First Amendment", "british", "himmelman", "tina turner", "Rudyard Kipling"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5336262695637696}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, false, false, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0909090909090909, 0.2857142857142857, 0.25, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8333333333333333, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6968", "mrqa_newsqa-validation-1180", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-3642", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-3859", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-1336", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-1408", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-1783", "mrqa_triviaqa-validation-2930", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-1674", "mrqa_searchqa-validation-7046", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-5716"], "SR": 0.453125, "CSR": 0.6035714285714286, "EFR": 1.0, "Overall": 0.7491517857142858}, {"timecode": 35, "before_eval_results": {"predictions": ["4,222,000", "1271", "17", "Falls", "Highly combustible", "Engineering News- Record (ENR)", "single-tape", "Pitt", "\u00a330m", "On the Councils and the Church", "Honorary freemen", "Congress", "he didn't know if Woods' wife, Elin Nordegren, would appear with her husband.", "Sri Lanka's Tamil rebels", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.\"", "Israeli Navy", "28", "Aldgate East", "Black and Hispanic students", "signed a bill that will pay for the wars in Iraq and Afghanistan through the remainder of his presidency and into spring 2009.", "one Iraqi soldier,", "Department of Homeland Security Secretary Janet Napolitano", "glamour and hedonism", "This will be the second time", "Math teacher Mawise Gumba", "Former U.S. soldier Steven Green", "more than 78,000 parents of children ages 3 to 17.", "March 22, the ACLU said.", "Afghanistan and India", "Tuesday's iPhone 4S news,", "we seek a new way forward, based on mutual interest and mutual respect.", "two", "publicly criticized his father's parenting skills.", "environmental", "Little Rock Central High School", "NATO's International Security Assistance Force", "U.S. senators who couldn't resist taking the vehicles for a spin.\"", "summer", "Bob Johnson", "Kim Il Sung", "63", "$250,000", "Communist Party of Nepal", "Diego Maradona", "April 22.", "She said Cain suggested meeting over dinner, then tried to reach up her skirt after the meal -- and when she protested, he told her, \"You want a job, right?\"", "Unseeded Frenchwoman", "MS Columbus", "California-based Current TV", "Sonia Sotomayor", "Rambosk", "15-month investigation,", "a president who understands the world today, the future we seek and the change we", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome ) as directed by a three - nucleotide sequence ( codon ) in a messenger RNA ( mRNA )", "the addition of the massive dome", "New Zealand artist Nicholas Garland", "james chadwick", "Patrick Dempsey", "Kristy Lee Cook", "Clint Eastwood", "Italy", "an all-female a cappella singing group", "General Motors", "Doctor of Philosophy"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6311344471500722}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.7499999999999999, 0.6666666666666666, 0.0, 0.0, 0.28571428571428575, 0.09523809523809523, 0.5, 0.4444444444444445, 0.375, 0.33333333333333337, 0.6666666666666666, 0.5714285714285715, 0.7777777777777777, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.25000000000000006, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-3490", "mrqa_squad-validation-6693", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-1296", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-381", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-2929", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-234", "mrqa_naturalquestions-validation-187", "mrqa_triviaqa-validation-4532", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-5297"], "SR": 0.484375, "CSR": 0.6002604166666667, "EFR": 1.0, "Overall": 0.7484895833333334}, {"timecode": 36, "before_eval_results": {"predictions": ["a new form", "Greg Brady", "the mid-sixties", "less than a year", "the best, worst and average case complexity", "the 7th century", "William Tyndale", "Henry Cole", "\"gentlemen's agreement\"", "military action", "MMA", "Rick and Morty", "Ringo Starr", "Lord Byron", "Resorts World Genting", "World of Wonder", "Vernier, Switzerland", "the Ruul", "Nebula Award, the Philip K. Dick Award, and the Hugo Award", "United States and Canada", "musician", "2004 Paris Motor Show", "Jonathan Daniel Hamm", "Abdul Razzak Yaqoob", "Vilnius", "Mr. Tumnus", "elderships", "Ruth Robinson Duccini", "Bolton, England", "Conservative", "1966", "Lindsey Islands", "Liverpool and England international player", "romantic comedy", "Mani", "Tunisian", "people working in film and the performing arts,", "Nicolas Vanier", "red, fallow and roe deer", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.,", "Bob Mould", "German", "very hot, very energetic excited matter", "Oklahoma Sooners", "books, films and other", "Galway", "Razor Ramon", "illnesses", "Russia", "southern (Dolomitic) Alps in the Veneto region of Northern Italy", "Bisexuality", "400 MW", "Germany", "allows the fuel pressure to be controlled via pulse - width modulation of the pump voltage", "International Baccalaureate", "a pepper", "Wikia", "15", "make sure water continues flow through the river channel and not spread out over land.", "the Mississippi River", "zesta-punta", "Michael Jackson and Lionel Richie", "George Halas", "St. Augustine"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6158234126984128}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714285, 1.0, 0.4, 0.33333333333333337, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.7142857142857143, 0.0, 1.0, 1.0, 0.7000000000000001, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1701", "mrqa_squad-validation-9567", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-5254", "mrqa_hotpotqa-validation-4963", "mrqa_hotpotqa-validation-1059", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-4931", "mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1716", "mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-820", "mrqa_hotpotqa-validation-4473", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1283", "mrqa_naturalquestions-validation-6148", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-6179", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3460", "mrqa_searchqa-validation-6003", "mrqa_searchqa-validation-2977", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-1223"], "SR": 0.515625, "CSR": 0.597972972972973, "retrieved_ids": ["mrqa_squad-train-1039", "mrqa_squad-train-43985", "mrqa_squad-train-40359", "mrqa_squad-train-22959", "mrqa_squad-train-84296", "mrqa_squad-train-44921", "mrqa_squad-train-43099", "mrqa_squad-train-52791", "mrqa_squad-train-65332", "mrqa_squad-train-39020", "mrqa_squad-train-20805", "mrqa_squad-train-6591", "mrqa_squad-train-812", "mrqa_squad-train-4198", "mrqa_squad-train-69134", "mrqa_squad-train-71612", "mrqa_naturalquestions-validation-1704", "mrqa_triviaqa-validation-1213", "mrqa_squad-validation-6945", "mrqa_squad-validation-4383", "mrqa_searchqa-validation-6370", "mrqa_naturalquestions-validation-3390", "mrqa_squad-validation-601", "mrqa_newsqa-validation-2902", "mrqa_searchqa-validation-14925", "mrqa_squad-validation-1", "mrqa_squad-validation-1670", "mrqa_searchqa-validation-16371", "mrqa_searchqa-validation-4619", "mrqa_searchqa-validation-15590", "mrqa_triviaqa-validation-2745", "mrqa_searchqa-validation-10897"], "EFR": 1.0, "Overall": 0.7480320945945945}, {"timecode": 37, "before_eval_results": {"predictions": ["not having a residence permit", "Torchwood", "the p-adic norm", "sedimentary", "Doritos", "30", "the dinophyte nucleus", "six", "Blum complexity axioms", "electromagnetic force", "15,000", "Boston, Massachusetts", "Saw II", "Aloha \u02bbOe", "Jeffrey Adam \"Duff\" Goldman", "69.7 million litres", "Ben Ainslie", "an English Grand Prix motorcycle road racer", "Kait Parker", "Revolution Studios", "Lord's Resistance Movement", "June 26, 1970", "Dorothy", "2008", "Cookstown", "1978", "Colonel", "Giuseppe Verdi", "Takura Tendayi", "Piedmont", "Love", "Arthur Freed", "Revolt of the Sergeants", "the lead roles of Timmy Sanders", "Maine", "Oklahoma Sooners", "the University of Keele", "more than 230", "Northern Irish", "1736", "Joachim Trier", "Barbara Niven", "Washington, D.C.", "Andrzej Go\u0142ota", "Patrick Dempsey", "Flaw", "president", "Derry City F.C.", "My Gorgeous Life", "1,382", "Nana Patekar", "KBS2", "2005", "Jack Gleeson", "18th century", "badminton", "Caernarfon", "Barack Obama", "Ben Freeth", "San Antonio", "opera", "Robert Mugabe's opponents", "More than 150,000", "around 3.5 percent of global greenhouse emissions."], "metric_results": {"EM": 0.640625, "QA-F1": 0.7094020562770562}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, false, true, true, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.5, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7626", "mrqa_squad-validation-8958", "mrqa_squad-validation-10483", "mrqa_hotpotqa-validation-5310", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-2855", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-4119", "mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4196", "mrqa_hotpotqa-validation-3644", "mrqa_hotpotqa-validation-87", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-1630", "mrqa_triviaqa-validation-5021", "mrqa_newsqa-validation-3529", "mrqa_searchqa-validation-11958", "mrqa_newsqa-validation-3393"], "SR": 0.640625, "CSR": 0.599095394736842, "EFR": 0.9565217391304348, "Overall": 0.7395609267734554}, {"timecode": 38, "before_eval_results": {"predictions": ["effects of deforestation", "21 to 11", "a commune", "lower lake", "the Yuan dynasty", "a quantity surveyor", "Squillace", "chloroplasts", "the wedding banquet", "Starlite", "David Yates", "pioneering New Zealand food writer", "an estimated 50 to 90 million inhabitants", "265 million", "the County of York", "Lev Ivanovich Yashin", "Caligula", "Leonard", "Edmonton, Alberta", "Brooklyn", "Symphony No. 7", "a champion dancer", "Indianapolis Motor Speedway", "Central University of India", "2004", "a profound contribution to Newtonian mechanics", "\"Barney Miller\"", "Daniel Sturridge", "supernatural psychological horror", "Philippe of Belgium", "Ardeth Bay", "Hans Rosenfeldt,", "January 15, 1975", "I Am Furious (Yellow)", "Nikolai Sergeyevich Trubetzkoy", "books, films and other media", "1770", "John Meston", "Tottenham", "Operation Neptune", "\"The Future\"", "Ready Player One", "Cecily Legler Strong", "My Own Worst Enemy", "Gillian Leigh Anderson", "SKUM", "Tom Ewell Tompkins", "the National Mall in Washington, D.C., across from the Washington Monument", "1976", "Vice President George Mifflin Dallas", "the Sun", "1987", "Thorgan", "quarterback", "hairpin corner", "flat oven-baked Italian bread product", "ABBA", "Fareed Zakaria", "use of torture and indefinite detention", "Indiana", "Nixon", "Vice President", "May 30, 2017", "Mahatma Gandhi"], "metric_results": {"EM": 0.53125, "QA-F1": 0.682132711038961}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true], "QA-F1": [0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.8, 1.0, 0.8, 0.3636363636363636, 1.0, 0.7499999999999999, 1.0, 1.0, 0.4, 0.25, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4446", "mrqa_squad-validation-973", "mrqa_squad-validation-8792", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-1958", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-4024", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-4563", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-5389", "mrqa_hotpotqa-validation-3064", "mrqa_hotpotqa-validation-1848", "mrqa_hotpotqa-validation-5590", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-4616", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-4597", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-727", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-9979", "mrqa_triviaqa-validation-3529", "mrqa_searchqa-validation-3074", "mrqa_naturalquestions-validation-5537"], "SR": 0.53125, "CSR": 0.5973557692307692, "EFR": 1.0, "Overall": 0.7479086538461538}, {"timecode": 39, "before_eval_results": {"predictions": ["11\u201313th century AD", "75%", "the defects justifying rebellion", "Ersatzschulen", "1543", "Edward the Confessor", "July 2013", "Warszawa", "became the University of Northumbria at Newcastle", "the plague", "a Zen monastery", "Dick & Jane", "an arcade", "Life of Pi", "the Circus World Museum", "Nixon", "zoos", "Garland", "The English Patient", "Chicago Cubs", "the Militia Act", "the Tame", "August Wilson", "accordion", "the Volkswagen Passat", "Mad About You", "a promissory note", "James Patterson", "Leinster", "soprano", "Maria Sharapova", "Daniel Defoe", "\"The Secrets of a Fire King\"", "floats", "lntracoastal Canal", "Suitcase", "the Minotaur", "the man who Mistook his wife for a Hat", "chlorine", "Elton John", "fishermen", "the Bordeaux region of France", "pennies", "the White Rabbit", "a worthless thing or endeavor", "Tila Tequila", "Ashton Kutcher", "Emancipation Proclamation", "the pottery industry", "Billy Joel", "the Linton", "the Caribbean Sea", "the Edict of Nantes", "1995", "Detective Superintendent Dave Kelly", "Leeds", "Angostura bitters", "pop music and popular culture", "4,530", "almost 100", "Womack Army Hospital at Fort Bragg,", "Vice President : Nickey Iyambo", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "dorsally on the forearm"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5683948863636363}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.3636363636363636, 0.0, 0.3636363636363636, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6794", "mrqa_squad-validation-5337", "mrqa_searchqa-validation-4961", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-14709", "mrqa_searchqa-validation-13399", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-15899", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-3324", "mrqa_searchqa-validation-116", "mrqa_searchqa-validation-13589", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-3436", "mrqa_searchqa-validation-16699", "mrqa_searchqa-validation-13735", "mrqa_searchqa-validation-16623", "mrqa_searchqa-validation-2218", "mrqa_searchqa-validation-12987", "mrqa_searchqa-validation-4513", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-11152", "mrqa_searchqa-validation-2794", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-4986", "mrqa_searchqa-validation-14332", "mrqa_searchqa-validation-14037", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-8851", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-1391", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-4351"], "SR": 0.46875, "CSR": 0.594140625, "retrieved_ids": ["mrqa_squad-train-25287", "mrqa_squad-train-10266", "mrqa_squad-train-41547", "mrqa_squad-train-76003", "mrqa_squad-train-3745", "mrqa_squad-train-74756", "mrqa_squad-train-38461", "mrqa_squad-train-52126", "mrqa_squad-train-55415", "mrqa_squad-train-32803", "mrqa_squad-train-50248", "mrqa_squad-train-56040", "mrqa_squad-train-31162", "mrqa_squad-train-56382", "mrqa_squad-train-84296", "mrqa_squad-train-70548", "mrqa_naturalquestions-validation-3206", "mrqa_newsqa-validation-1133", "mrqa_hotpotqa-validation-5030", "mrqa_searchqa-validation-14126", "mrqa_triviaqa-validation-3385", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-11442", "mrqa_squad-validation-5168", "mrqa_squad-validation-10251", "mrqa_searchqa-validation-1654", "mrqa_triviaqa-validation-5414", "mrqa_hotpotqa-validation-5745", "mrqa_squad-validation-7792", "mrqa_newsqa-validation-490", "mrqa_triviaqa-validation-1109", "mrqa_newsqa-validation-661"], "EFR": 1.0, "Overall": 0.747265625}, {"timecode": 40, "UKR": 0.7734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1063", "mrqa_hotpotqa-validation-1097", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-1449", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1509", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1625", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2164", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3163", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3317", "mrqa_hotpotqa-validation-336", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3437", "mrqa_hotpotqa-validation-3630", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3825", "mrqa_hotpotqa-validation-3865", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4024", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-4987", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5037", "mrqa_hotpotqa-validation-5052", "mrqa_hotpotqa-validation-5168", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5302", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5616", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-604", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-634", "mrqa_hotpotqa-validation-649", "mrqa_hotpotqa-validation-686", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-88", "mrqa_hotpotqa-validation-881", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-4532", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-9004", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-9367", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9778", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1296", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-1592", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2084", "mrqa_newsqa-validation-209", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3393", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3642", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-644", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-806", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-937", "mrqa_searchqa-validation-10053", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-1184", "mrqa_searchqa-validation-11854", "mrqa_searchqa-validation-1254", "mrqa_searchqa-validation-13081", "mrqa_searchqa-validation-13246", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-13634", "mrqa_searchqa-validation-13735", "mrqa_searchqa-validation-146", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-15114", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-15685", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-16512", "mrqa_searchqa-validation-16699", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-2218", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2598", "mrqa_searchqa-validation-2624", "mrqa_searchqa-validation-2631", "mrqa_searchqa-validation-2794", "mrqa_searchqa-validation-2795", "mrqa_searchqa-validation-2917", "mrqa_searchqa-validation-2977", "mrqa_searchqa-validation-3074", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4117", "mrqa_searchqa-validation-4228", "mrqa_searchqa-validation-4467", "mrqa_searchqa-validation-4675", "mrqa_searchqa-validation-4723", "mrqa_searchqa-validation-4741", "mrqa_searchqa-validation-5043", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5910", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-6365", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-7575", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-7777", "mrqa_searchqa-validation-785", "mrqa_searchqa-validation-7873", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-9274", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-9508", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9760", "mrqa_searchqa-validation-9784", "mrqa_searchqa-validation-9894", "mrqa_squad-validation-1", "mrqa_squad-validation-10020", "mrqa_squad-validation-10027", "mrqa_squad-validation-10041", "mrqa_squad-validation-10054", "mrqa_squad-validation-10137", "mrqa_squad-validation-10203", "mrqa_squad-validation-10206", "mrqa_squad-validation-1028", "mrqa_squad-validation-103", "mrqa_squad-validation-10316", "mrqa_squad-validation-10489", "mrqa_squad-validation-10500", "mrqa_squad-validation-1051", "mrqa_squad-validation-1098", "mrqa_squad-validation-1148", "mrqa_squad-validation-1311", "mrqa_squad-validation-1343", "mrqa_squad-validation-1379", "mrqa_squad-validation-1394", "mrqa_squad-validation-1424", "mrqa_squad-validation-1467", "mrqa_squad-validation-1474", "mrqa_squad-validation-1481", "mrqa_squad-validation-1506", "mrqa_squad-validation-1516", "mrqa_squad-validation-1544", "mrqa_squad-validation-157", "mrqa_squad-validation-1640", "mrqa_squad-validation-167", "mrqa_squad-validation-1670", "mrqa_squad-validation-1695", "mrqa_squad-validation-1736", "mrqa_squad-validation-1758", "mrqa_squad-validation-1771", "mrqa_squad-validation-1862", "mrqa_squad-validation-1956", "mrqa_squad-validation-1964", "mrqa_squad-validation-1977", "mrqa_squad-validation-1977", "mrqa_squad-validation-1980", "mrqa_squad-validation-2079", "mrqa_squad-validation-2122", "mrqa_squad-validation-218", "mrqa_squad-validation-2191", "mrqa_squad-validation-2208", "mrqa_squad-validation-225", "mrqa_squad-validation-2292", "mrqa_squad-validation-2292", "mrqa_squad-validation-2315", "mrqa_squad-validation-2354", "mrqa_squad-validation-2375", "mrqa_squad-validation-2628", "mrqa_squad-validation-2648", "mrqa_squad-validation-2719", "mrqa_squad-validation-2736", "mrqa_squad-validation-2739", "mrqa_squad-validation-3048", "mrqa_squad-validation-3069", "mrqa_squad-validation-3097", "mrqa_squad-validation-3112", "mrqa_squad-validation-3125", "mrqa_squad-validation-3146", "mrqa_squad-validation-3193", "mrqa_squad-validation-327", "mrqa_squad-validation-328", "mrqa_squad-validation-338", "mrqa_squad-validation-3420", "mrqa_squad-validation-3469", "mrqa_squad-validation-3599", "mrqa_squad-validation-3642", "mrqa_squad-validation-3653", "mrqa_squad-validation-3661", "mrqa_squad-validation-3664", "mrqa_squad-validation-3713", "mrqa_squad-validation-3725", "mrqa_squad-validation-3740", "mrqa_squad-validation-3759", "mrqa_squad-validation-3791", "mrqa_squad-validation-3853", "mrqa_squad-validation-3907", "mrqa_squad-validation-3921", "mrqa_squad-validation-3941", "mrqa_squad-validation-3942", "mrqa_squad-validation-402", "mrqa_squad-validation-4023", "mrqa_squad-validation-4168", "mrqa_squad-validation-4173", "mrqa_squad-validation-4267", "mrqa_squad-validation-4304", "mrqa_squad-validation-4404", "mrqa_squad-validation-4415", "mrqa_squad-validation-4429", "mrqa_squad-validation-4429", "mrqa_squad-validation-4442", "mrqa_squad-validation-4446", "mrqa_squad-validation-4478", "mrqa_squad-validation-4490", "mrqa_squad-validation-4646", "mrqa_squad-validation-4655", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-472", "mrqa_squad-validation-478", "mrqa_squad-validation-4802", "mrqa_squad-validation-4807", "mrqa_squad-validation-4846", "mrqa_squad-validation-4875", "mrqa_squad-validation-4896", "mrqa_squad-validation-4953", "mrqa_squad-validation-4958", "mrqa_squad-validation-5012", "mrqa_squad-validation-5077", "mrqa_squad-validation-5185", "mrqa_squad-validation-5256", "mrqa_squad-validation-5311", "mrqa_squad-validation-5322", "mrqa_squad-validation-5373", "mrqa_squad-validation-5396", "mrqa_squad-validation-5428", "mrqa_squad-validation-5457", "mrqa_squad-validation-547", "mrqa_squad-validation-5503", "mrqa_squad-validation-5537", "mrqa_squad-validation-5611", "mrqa_squad-validation-5635", "mrqa_squad-validation-5754", "mrqa_squad-validation-5846", "mrqa_squad-validation-5877", "mrqa_squad-validation-5927", "mrqa_squad-validation-5967", "mrqa_squad-validation-6086", "mrqa_squad-validation-61", "mrqa_squad-validation-6115", "mrqa_squad-validation-6122", "mrqa_squad-validation-6128", "mrqa_squad-validation-6224", "mrqa_squad-validation-6242", "mrqa_squad-validation-6279", "mrqa_squad-validation-6292", "mrqa_squad-validation-630", "mrqa_squad-validation-6312", "mrqa_squad-validation-6361", "mrqa_squad-validation-6380", "mrqa_squad-validation-6434", "mrqa_squad-validation-6453", "mrqa_squad-validation-6474", "mrqa_squad-validation-6520", "mrqa_squad-validation-6541", "mrqa_squad-validation-6726", "mrqa_squad-validation-6777", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-6871", "mrqa_squad-validation-6879", "mrqa_squad-validation-6887", "mrqa_squad-validation-6919", "mrqa_squad-validation-696", "mrqa_squad-validation-7006", "mrqa_squad-validation-7136", "mrqa_squad-validation-7165", "mrqa_squad-validation-7211", "mrqa_squad-validation-7217", "mrqa_squad-validation-725", "mrqa_squad-validation-7284", "mrqa_squad-validation-729", "mrqa_squad-validation-7537", "mrqa_squad-validation-7663", "mrqa_squad-validation-7665", "mrqa_squad-validation-7683", "mrqa_squad-validation-7752", "mrqa_squad-validation-7816", "mrqa_squad-validation-7835", "mrqa_squad-validation-7871", "mrqa_squad-validation-7877", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8031", "mrqa_squad-validation-8065", "mrqa_squad-validation-8103", "mrqa_squad-validation-820", "mrqa_squad-validation-821", "mrqa_squad-validation-8210", "mrqa_squad-validation-8244", "mrqa_squad-validation-8245", "mrqa_squad-validation-8250", "mrqa_squad-validation-8338", "mrqa_squad-validation-8401", "mrqa_squad-validation-8505", "mrqa_squad-validation-8554", "mrqa_squad-validation-8561", "mrqa_squad-validation-8566", "mrqa_squad-validation-8599", "mrqa_squad-validation-8969", "mrqa_squad-validation-8977", "mrqa_squad-validation-9020", "mrqa_squad-validation-9068", "mrqa_squad-validation-9102", "mrqa_squad-validation-9145", "mrqa_squad-validation-9151", "mrqa_squad-validation-929", "mrqa_squad-validation-9325", "mrqa_squad-validation-9346", "mrqa_squad-validation-9368", "mrqa_squad-validation-9541", "mrqa_squad-validation-9595", "mrqa_squad-validation-9623", "mrqa_squad-validation-9643", "mrqa_squad-validation-9701", "mrqa_squad-validation-9709", "mrqa_squad-validation-9744", "mrqa_squad-validation-9787", "mrqa_squad-validation-980", "mrqa_squad-validation-9800", "mrqa_squad-validation-9837", "mrqa_squad-validation-9900", "mrqa_squad-validation-9944", "mrqa_squad-validation-999", "mrqa_triviaqa-validation-101", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-1884", "mrqa_triviaqa-validation-1956", "mrqa_triviaqa-validation-2026", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2097", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-2928", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-3406", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3794", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4183", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-5040", "mrqa_triviaqa-validation-5102", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-561", "mrqa_triviaqa-validation-5655", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-6033", "mrqa_triviaqa-validation-6141", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-6610", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6912", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7531", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-91"], "OKR": 0.876953125, "KG": 0.50078125, "before_eval_results": {"predictions": ["England", "Jimmy Kimmel Live", "an invasion of Western Europe", "\"The Day of the Doctor\"", "mannerist architecture", "3", "the same message routing methodology as developed by Baran", "Jonathan Stewart", "two", "Balvenie Castle", "Vernier, Switzerland", "1966", "Selden", "Henry Kaiser", "841", "a creek", "20 March to 1 May 2003", "Newfoundland and Labrador", "9 November 1967", "Westchester County", "French Canadians", "Towards the Sun", "the Secret Intelligence Service", "Internet creators Guild", "Windigo", "aging issues", "1949", "churro", "Tom Kartsotis", "Terrence Jones", "Joseph Cotten", "Thomas Allen", "Eisstadion Davos", "Tampa Bay Lightning", "Vishal Bhardwaj", "15", "the University of Vienna", "four", "KlingStubbins", "the Fifteenth Season", "former pornographystar", "torpedoes", "1951", "Saint Motel", "Allan McNish", "1995", "1689", "David Villa", "Champion Jockey", "Microsoft Office", "(Radioisotopes and the Age of The Earth)", "He served as director of the Saint Petersburg Conservatory", "Mathew Sacks", "Sons of Anarchy Motorcycle Club", "Mel Tillis", "baseball", "sand", "skeletal dysplasia,", "A New York appeals court", "3800", "mirror", "sewer", "Fred Astaire", "Douglas MacArthur"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6505208333333333}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5979", "mrqa_squad-validation-9362", "mrqa_hotpotqa-validation-4721", "mrqa_hotpotqa-validation-5848", "mrqa_hotpotqa-validation-23", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-339", "mrqa_hotpotqa-validation-553", "mrqa_hotpotqa-validation-79", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3363", "mrqa_hotpotqa-validation-4257", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-4007", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-4288", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-815", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-10901"], "SR": 0.546875, "CSR": 0.5929878048780488, "EFR": 1.0, "Overall": 0.7488319359756097}, {"timecode": 41, "before_eval_results": {"predictions": ["c1600", "absolute value", "20", "James Hutton", "force-free magnetic fields", "six", "Basic formal education", "BBC Wales", "bass", "books, films and other media", "the City of Peace", "Ben Ainslie", "the Pac-12 Conference", "Rochdale", "Former leader Jay Park", "\"Diary of the Dead\"", "11 November 1918", "Fort Bragg, North Carolina", "The Drudge Report", "Homebrewing", "an anvil", "created the American Land-Grant universities and colleges", "Andy Garc\u00eda", "Eielson Air Force Base", "1 December 1948", "drummer Seb Rochford", "David Dunn", "\"Charmed\"", "White Horse", "postal delivery", "Europop", "private", "Leofric", "Germanic", "Prince Sung-won", "voicing Liquid Snake", "Schaffer", "Valhalla Highlands Historic District", "Texas, United States", "Walmart", "southern Jasper County and northern Newton County in the southwestern corner of the U.S. state of Missouri", "1964", "Richard Arthur", "Presbyterian Church", "CD Castell\u00f3n", "right-hand batsman", "the German Empire", "three", "River Welland", "Ben Stokes", "first train robbery", "Don Johnson", "Polish Army", "13 February", "on the microscope's stage", "Frederick and Rosemary West", "Kate Winslet", "February 2008", "Daniel Radcliffe and the Order of the Phoenix", "Nostradamus", "a tortoise", "Leo", "Pablo Picasso", "Kosovo"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7138578869047618}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.13333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.5, 0.125, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.4, 0.5, 0.0, 1.0, 0.8571428571428571, 0.4, 0.5, 1.0, 0.5714285714285715, 0.3333333333333333, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8472", "mrqa_hotpotqa-validation-2258", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-3571", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-2203", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-4363", "mrqa_hotpotqa-validation-1365", "mrqa_naturalquestions-validation-182", "mrqa_triviaqa-validation-2033", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-764", "mrqa_searchqa-validation-4054"], "SR": 0.578125, "CSR": 0.5926339285714286, "EFR": 0.9629629629629629, "Overall": 0.7413537533068784}, {"timecode": 42, "before_eval_results": {"predictions": ["Oxygen therapy", "in a series of New York hotels", "energy content", "Baiju", "July 24", "The Saturn IB", "over half of the planet's remaining rainforests", "New Orleans, Louisiana", "German", "GE Appliances", "Keelung", "John Joseph Travolta", "The authorship of Titus Andronicus", "National Football League", "L.M. Montgomery", "25 August 1949", "over 20 million records worldwide", "The Captain Matchbox Whoopee Band", "Some Sizzurp", "over 50 million singles", "22,500 acres", "Citizens for a Sound Economy", "Woodsy owl", "Hellenism", "The Lufthansa Heist", "Mauritian", "The LA Galaxy", "Oliver Parker", "first baseman and third baseman", "Rymill Park", "Ricardo L\u00f3pez Nava", "Tampa Bay Storm", "in the series \"Runaways\"", "Las Vegas", "House of Habsburg-Lorraine", "Tian Tan Buddha", "Hopi", "Band of Hanover", "Martin Truex Jr.", "Secretary of Defense", "twice", "Logar Province", "September 6, 1998", "Lionel Hollins", "Pamelyn Ferdin", "the Blue Ridge Parkway", "bioelectromagnetics", "VH1", "Kentucky", "Dra\u017een Petrovi\u0107", "SAS Technical Services", "Hawaii", "Scott Eastwood", "Isabella Palmieri", "July 1, 1923", "radionuclides", "The person who is naked, in the process of disrobing, or engaging in sexual activity.", "Marine Corps", "the sex scandal", "a tornado", "the \"baggage train\" of the", "60 Minutes", "Nixon", "jolly roger"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6388020833333333}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.5, 1.0, 1.0, 1.0, 0.8, 0.0, 0.5, 0.4, 0.5, 0.28571428571428575, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2, 0.7499999999999999, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1247", "mrqa_squad-validation-4405", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3130", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-1818", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-5125", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-2646", "mrqa_naturalquestions-validation-1446", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-7704", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-2811", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-14021", "mrqa_searchqa-validation-10521", "mrqa_searchqa-validation-7720"], "SR": 0.484375, "CSR": 0.5901162790697674, "retrieved_ids": ["mrqa_squad-train-50789", "mrqa_squad-train-59472", "mrqa_squad-train-31364", "mrqa_squad-train-27625", "mrqa_squad-train-4687", "mrqa_squad-train-53705", "mrqa_squad-train-36409", "mrqa_squad-train-3334", "mrqa_squad-train-31993", "mrqa_squad-train-75526", "mrqa_squad-train-20757", "mrqa_squad-train-72371", "mrqa_squad-train-23883", "mrqa_squad-train-42718", "mrqa_squad-train-43318", "mrqa_squad-train-32992", "mrqa_squad-validation-413", "mrqa_newsqa-validation-2435", "mrqa_triviaqa-validation-7660", "mrqa_searchqa-validation-13360", "mrqa_searchqa-validation-13224", "mrqa_squad-validation-1456", "mrqa_searchqa-validation-4675", "mrqa_searchqa-validation-10901", "mrqa_newsqa-validation-1948", "mrqa_searchqa-validation-4605", "mrqa_triviaqa-validation-2667", "mrqa_searchqa-validation-16699", "mrqa_hotpotqa-validation-4363", "mrqa_hotpotqa-validation-961", "mrqa_searchqa-validation-7327", "mrqa_triviaqa-validation-902"], "EFR": 1.0, "Overall": 0.7482576308139535}, {"timecode": 43, "before_eval_results": {"predictions": ["19", "Creon", "an unbalanced torque", "Oxygen storage methods", "April 1959", "the water level", "the Master", "hydrogen", "Lana Del Rey", "the Beldam / Other Mother", "Jason's ( Robert Wilfort )", "Brazil and Paraguay", "Adam", "Bryan Cranston", "1987", "Kanawha River", "September 9, 2012", "the magnetic stripe `` anomalies '' on the ocean floor", "Bart Cummings", "Hermann Ebbinghaus", "optic chiasma", "New Mexico", "The centuries - old Jedi Grand Master of an unknown species", "James Zeebo", "The decision effectively overturned the Plessy v. Ferguson decision of 1896", "the Intertropical Convergence Zone ( ITCZ )", "Butch or Killer", "2014", "Steve Lukather", "Parker's pregnancy at the time of filming", "lightning", "1955", "season five", "the North Atlantic Drift", "Abid Ali Neemuchwala", "Lord Banquo", "1998", "early 20th century", "March 31, 2017", "Louisiana, the company had garnered 35 % of the ice cream market", "Ernest Hemingway", "north of the Equator", "Sara Gilbert", "ABC", "silk, hair / fur ( including wool ) and feathers", "the Mongol Yuan Dynasty", "the eighth episode in the ninth season of the American animated television series", "Elena Anaya", "Gary Grimes as Hermie, Jerry Houser as his best friend Oscy", "The tuatara", "a chimera", "Greek \u03b3\u03b1\u03c3\u03c4\u03ae\u03c1 `` stomach '' and \u03ba\u03bd\u03ae\u03bc\u03b7 ( kn\u1e17m\u0113 ) `` leg ''", "the early to mid-2000s", "Charles Dickens", "aircraft carrier", "Vyto Ruginis", "Archie Andrews", "five", "tickets", "(John) Shaft", "The Laughing Cavalier", "Saturday Night Live", "Bono", "The Royal Ballet"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6359866695804196}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666667, 0.0, 0.0, 0.3333333333333333, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 1.0, 0.0, 0.25, 0.0, 0.6666666666666666, 1.0, 0.3076923076923077, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3688", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-3074", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-2966", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2953", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-7362", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-8072", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-6131", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-8908", "mrqa_triviaqa-validation-4420", "mrqa_newsqa-validation-824"], "SR": 0.546875, "CSR": 0.5891335227272727, "EFR": 0.9655172413793104, "Overall": 0.7411645278213166}, {"timecode": 44, "before_eval_results": {"predictions": ["a green algal derived chloroplast", "bilaterians", "Ward", "The Swiss cities", "the goals he receives from his superior", "7 January 1900", "Education", "in the $24,000-30,000 price range", "10 years", "UH-60 Blackhawk helicopters collided Saturday night", "millions of Americans", "arson", "Sunday's", "Citizens", "a music video", "five", "calling on NATO to do more to stop the Afghan opium trade", "flying", "Kerstin Fritzl", "amazed at their band's amazing impact", "in a canyon", "recanted her claims that she was lured to a dorm and assaulted in a bathroom stall", "Ronaldinho", "shock", "Illness", "Utah Valley Regional Medical Center", "additional information regarding actress Natalie Wood's 1981 drowning death,", "Frank Ricci", "200", "striker", "African National Congress", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "The charter mandated the English king to cede certain basic rights to his citizens, ensuring that no man is above the law.", "137", "New Year's Day", "South Africa", "Unseeded Frenchwoman Aravane Rezai", "Belfast, Northern Ireland", "after nine years.", "Tutsi and Hutu rivalry", "the island's dining scene", "sovereignty over them", "African National Congress Deputy President Kgalema Motlanthe", "1918-1919", "Newcastle", "\"disagreements\" with the Port Authority of New York and New Jersey,", "German Chancellor Angela Merkel", "EU naval force", "Genocide Prevention Task Force", "gay rights activists", "U.S. military bases in the Pacific Ocean territory of Guam", "Hong Kong and mainland China,", "201-262-2800", "Neuropsychology", "Rumplestiltskin", "Saturn", "Hyundai", "South America", "WB Television Network", "George P. Shultz", "parody", "cartoons", "decorate", "Jack Johnson"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6390533078033078}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, true, false, true, true, false, true, false, false, false, false, false, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.5, 0.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.5714285714285715, 1.0, 0.4, 0.5, 0.0, 0.28571428571428575, 0.8, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-844", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2121", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-1704", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-263", "mrqa_naturalquestions-validation-2839", "mrqa_searchqa-validation-5272", "mrqa_triviaqa-validation-4612"], "SR": 0.53125, "CSR": 0.5878472222222222, "EFR": 0.9666666666666667, "Overall": 0.7411371527777778}, {"timecode": 45, "before_eval_results": {"predictions": ["Jane Kim", "BBC National Orchestra of Wales", "to the southeast part of Khwarzemia to form, with the first division, a pincer attack on Samarkand.", "Algeria.", "Qara Khitai", "complexity", "Lorne Greene", "piano", "Ramen", "Zuckmantel", "hot peppers", "Edmund Burke", "\"Every Breath You Take\"", "Captain William Bligh", "Washington, District of Columbia, United States", "Se sitcom", "Medieval Times Dinner & Tournament", "Marcus Welby", "hot dog", "Marlon Brando", "\"The Hogan Family\"", "zoology", "hot dogs", "Judges", "Urban", "Howard Hughes", "Lake Michigan", "religious figures", "Hermann Rorschach", "the Golden Temple", "Teddy Roosevelt", "Vietnam", "the ghost", "Lance Armstrong", "It's a marvelous Life", "\"The Passing of Arthur\"", "(BORE)", "The Winds of War", "the new Italian flag", "the Hundred Years' War", "Wellington", "Bangkok", "Castle Rock", "carbon fiber", "Women in Love", "California's Classic Car Show", "Serer", "Donald Trump", "a helicopter", "aces", "\"The Raven\"", "May", "Marshall Sahlins", "more than 420 locations", "L\u00e9o Arnaud", "Jane Eyre", "Debbie Rowe", "\"GEORGES Bizet\u2019s Carmen premieres in Paris", "extended play", "Sulfur mustard", "Iceal Hambleton", "\"Top Gun\"", "fake his own death", "\"Iran's Green Movement of protesters against the regime indicates that waiting could have its benefits,"], "metric_results": {"EM": 0.40625, "QA-F1": 0.538045634920635}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, false, true, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.5, 1.0, 0.5, 0.2857142857142857, 1.0, 0.5, 0.5, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6125", "mrqa_squad-validation-1710", "mrqa_searchqa-validation-7347", "mrqa_searchqa-validation-10923", "mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-11405", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-16776", "mrqa_searchqa-validation-9674", "mrqa_searchqa-validation-15634", "mrqa_searchqa-validation-1698", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-3040", "mrqa_searchqa-validation-4957", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-5644", "mrqa_searchqa-validation-5759", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-3427", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-6209", "mrqa_searchqa-validation-9820", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-14904", "mrqa_searchqa-validation-4954", "mrqa_searchqa-validation-11073", "mrqa_searchqa-validation-13554", "mrqa_searchqa-validation-2849", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-1840", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-7659", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-3528", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-729"], "SR": 0.40625, "CSR": 0.5838994565217391, "retrieved_ids": ["mrqa_squad-train-62017", "mrqa_squad-train-76681", "mrqa_squad-train-61289", "mrqa_squad-train-72006", "mrqa_squad-train-78900", "mrqa_squad-train-29663", "mrqa_squad-train-5765", "mrqa_squad-train-181", "mrqa_squad-train-12074", "mrqa_squad-train-84163", "mrqa_squad-train-25134", "mrqa_squad-train-33976", "mrqa_squad-train-38638", "mrqa_squad-train-3238", "mrqa_squad-train-77862", "mrqa_squad-train-11599", "mrqa_triviaqa-validation-490", "mrqa_naturalquestions-validation-4054", "mrqa_searchqa-validation-6586", "mrqa_hotpotqa-validation-1290", "mrqa_searchqa-validation-4646", "mrqa_hotpotqa-validation-2681", "mrqa_squad-validation-3345", "mrqa_newsqa-validation-293", "mrqa_squad-validation-9487", "mrqa_triviaqa-validation-5219", "mrqa_squad-validation-4446", "mrqa_searchqa-validation-14126", "mrqa_hotpotqa-validation-4721", "mrqa_newsqa-validation-556", "mrqa_hotpotqa-validation-3800", "mrqa_newsqa-validation-2082"], "EFR": 1.0, "Overall": 0.7470142663043479}, {"timecode": 46, "before_eval_results": {"predictions": ["Ed McCaffrey", "oxygen-16", "December 1517", "a blue British police box", "Georgia", "Four thousand", "Uttar Pradesh", "Rich Mullins", "an investor couple", "gloria rowe", "Sara Gilbert", "the final years of the Third Republic", "into the Christian biblical canon", "1971", "Butter Island off North Haven, Maine in the Penobscot Bay", "15 December 2017", "between the stomach and the large intestine", "Todd Griffin", "Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "2017 / 18 Divisional Round", "New Zealand and Australia", "2018", "Cyndi Grecco", "carmen", "Catholic Monarchs of Castile and Aragon", "starch", "Grand Inquisition", "House of Representatives", "Christopher Jones", "Lynne", "June 8, 2009", "fertilization", "The management team", "The Enchantress", "Americans", "Herod", "Gibraltar", "13 February", "Bill Russell", "E Elaine Davidson", "the west - facing core of the crescent on Salamis Bay", "explosion", "the uppermost layer of the dermis", "Total Drama Action", "18", "food and clothing", "Empiricism", "senators", "in skeletal muscle", "March 29, 2018", "Acid rain", "The UN General Assembly", "South Pacific", "Mt Kenya", "vanilla", "1999", "John Snow", "March 13, 2013", "a financial or legal problem,", "Polo", "1,073 immigration detainees", "lily Allen", "a Purple Heart", "Caltech"], "metric_results": {"EM": 0.515625, "QA-F1": 0.631897095959596}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.20000000000000004, 1.0, 0.3636363636363636, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5714285714285715, 0.4, 0.13333333333333333, 0.6666666666666666, 1.0, 0.5714285714285715, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-3341", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-1304", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-9985", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-2812", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-9582", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-7264", "mrqa_naturalquestions-validation-5008", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-7312", "mrqa_naturalquestions-validation-692", "mrqa_triviaqa-validation-5309", "mrqa_hotpotqa-validation-4472", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-1003"], "SR": 0.515625, "CSR": 0.5824468085106382, "EFR": 0.967741935483871, "Overall": 0.7402721237989018}, {"timecode": 47, "before_eval_results": {"predictions": ["Pittsburgh Steelers", "Sainte Foy in Quebec", "Larry Ellison", "1930", "current Doctor Who logo", "productivity of each worker", "UTC \u2212 09 : 00", "three levels", "New England Patriots", "before November 1", "in Brooklyn Heights, New York", "hydrogen", "http://www.example.com/index.HTML", "Massachusetts", "Turkey", "Daniel A. Dailey", "the leaves of the plant species Stevia rebaudiana", "Captain Jones", "Freddie Highmore", "Canada", "Jack Barry", "Jason Marsden", "at Tandi, in Lahaul", "1912", "Etienne de Mestre", "Gaget, Gauthier & Co. workshop", "Lauren Tom", "more than 50 % of the total members of a house", "Miami Heat", "The Intolerable Acts", "constitutional principle", "pilgrimages to Jerusalem", "Shirley Mae Jones", "Haytham Kenway", "741 weeks", "head of the Imperial Family and the traditional head of state of Japan", "frontal lobe", "McKim Marriott", "Lizzy Greene", "PC2, a type II endoprotease, cleaves the C peptide - A chain bond", "Walter Brennan", "Tabaqui", "Keeley Clare Julia Hawes", "when they qualify as a medical practitioner following graduation with a Bachelor of Medicine, Bachelor of surgery degree and start the UK Foundation Programme", "Georgia Groome as Georgia Nicolson", "1773", "Dr. Rajendra Prasad", "Elizabeth Lail", "Mahatma Gandhi", "white rapper B - Rabbit ( Eminem ) and his attempt to launch a career in a genre dominated by African - Americans", "Jonathan Goldstein", "Malvolio", "formic acid", "transuranic elements", "western Caribbean Sea", "The final of 2011 AFC Asian Cup", "Angus Young", "Great Northern Railway transcontinental railway line", "The Stooges comedic farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "South America and Africa.", "seeking them to do jobs that Arizonans wouldn't do.", "Brazil", "rodeo", "William Safire"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6541661471497189}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, false, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.26666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.5263157894736842, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.09302325581395349, 1.0, 1.0, 1.0, 0.5, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.6666666666666666, 0.2666666666666667, 0.0, 0.8750000000000001, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7875", "mrqa_squad-validation-7188", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-215", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-8986", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2605", "mrqa_triviaqa-validation-1912", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-3417", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-1444"], "SR": 0.53125, "CSR": 0.5813802083333333, "EFR": 1.0, "Overall": 0.7465104166666666}, {"timecode": 48, "before_eval_results": {"predictions": ["More than 50%", "about 10,000", "1998", "materials melted near an impact crater", "the Rhine Gorge", "New Brunswick", "in Pashto and Persian as \u0647\u0646\u062f\u0648\u06a9\u0634 \u202c,", "3D modeling", "Kevin McKidd", "Abanindranath Tagore", "The Osmonds", "arm", "2014", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "pop ballad", "freedom of speech, the freedom of the press, the right to peaceably assemble, or to petition for a governmental redress of grievances", "Franklin Roosevelt", "China in American colonies without paying any taxes", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "art of the book and architecture ; and also including ceramics, metal, glass, and gardens", "Buddhist missionaries", "a greeting", "Siddharth Arora / Vibhav Roy as Ishaan Anirudh Sinha", "bicameral Congress", "1984", "Geoffrey Zakarian", "in Rome in 336", "2006", "Seven", "The Fixx", "ancient Rome", "Buffalo Lookout", "Charles Carson", "2012", "Spanish / Basque origin", "an idiom for the most direct path between two points", "March 15, 1945", "December 19, 1971", "a usually red oxide formed by the redox reaction", "the naos", "SURFACE AREA OF ROOTS", "movement of the Earth's continents relative to each other, thus appearing to `` drift '' across the ocean bed", "a competitor or team", "2.5 %", "Tony Curran", "to obtain a U.S. passport", "the endocrine ( hormonal ) systems", "the Royal Air Force ( RAF )", "It plays a key role in chain elongation", "Sharyans Resources", "Old Trafford", "Icarus", "Fiordland", "london", "Cambridge", "Chad", "Johnnie Ray", "William Corcoran Eustis", "\"it is impossible to turn back the tide of globalization.\"", "Afghan homes and compounds,", "Booches Billiard Hall,", "thunder", "Take the kids on a magical journey", "man"], "metric_results": {"EM": 0.390625, "QA-F1": 0.48335983552387374}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6792452830188679, 0.0, 0.0, 1.0, 0.14285714285714288, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.33333333333333337, 0.3636363636363636, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.11764705882352941, 0.05555555555555555, 0.0, 0.0, 0.0, 0.19354838709677416, 0.0, 0.4444444444444445, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.07142857142857142, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3026", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-9361", "mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-7948", "mrqa_triviaqa-validation-3594", "mrqa_triviaqa-validation-5057", "mrqa_triviaqa-validation-1314", "mrqa_hotpotqa-validation-4240", "mrqa_newsqa-validation-1973", "mrqa_searchqa-validation-10457"], "SR": 0.390625, "CSR": 0.5774872448979591, "retrieved_ids": ["mrqa_squad-train-39896", "mrqa_squad-train-27951", "mrqa_squad-train-37146", "mrqa_squad-train-84718", "mrqa_squad-train-27832", "mrqa_squad-train-51573", "mrqa_squad-train-35406", "mrqa_squad-train-61208", "mrqa_squad-train-28516", "mrqa_squad-train-69917", "mrqa_squad-train-19065", "mrqa_squad-train-27307", "mrqa_squad-train-15162", "mrqa_squad-train-8319", "mrqa_squad-train-45703", "mrqa_squad-train-44480", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-961", "mrqa_triviaqa-validation-3292", "mrqa_hotpotqa-validation-2914", "mrqa_searchqa-validation-10721", "mrqa_searchqa-validation-2901", "mrqa_squad-validation-5906", "mrqa_newsqa-validation-2669", "mrqa_squad-validation-821", "mrqa_searchqa-validation-13224", "mrqa_searchqa-validation-14332", "mrqa_hotpotqa-validation-5116", "mrqa_searchqa-validation-10900", "mrqa_squad-validation-8792", "mrqa_newsqa-validation-12", "mrqa_newsqa-validation-2040"], "EFR": 0.9230769230769231, "Overall": 0.7303472085949764}, {"timecode": 49, "before_eval_results": {"predictions": ["Leonard Bernstein", "Boolean", "Yale University", "Wednesdays", "with the help of the military", "Epithelium ( epi - + thele + - ium )", "alveolar process", "the court from its members for a three - year term", "an object", "in provinces along the Yangtze River and in provinces in the south", "200 to 500 mg up to 7 mg", "W. Edwards Deming", "Fa Ze Rug", "in the brain, muscles, and liver", "cartilage", "`` Nearer, My God, to Thee ''", "Jos\u00e9 Mart\u00ed", "Eurasian Plate", "Ukraine", "interstellar medium", "October 28, 2007", "2018", "Richard Masur", "Walmart", "Celtic", "Ra\u00fal Eduardo Esparza", "late as the 1890s", "2 Constant ( C\u03bc and C\u03b4 ) gene segments", "Ambulatory care or outpatient care", "Justice Harlan", "2017", "52 days", "Travis Tritt and Marty Stuart", "Missi Hale", "1939", "the base of the right ventricle", "Continental Congress", "New Mexico", "Bart Howard", "Jason Flemyng", "Brad Dourif", "Hans Zimmer", "a set of related data", "Kyla Pratt", "Ariana Clarice Richards", "glucose", "pathology", "Lionel Hardcastle", "Debbie Gibson", "on the bank's own funds and signed by a cashier", "2017", "Portuguese version of this surname is Tavares", "Dombey and Son", "bison", "The Duchess", "Puente Hills Mall", "Adam Levine", "Westchester", "\"It should stay that way.\"", "more than 200", "humans", "dobermann", "Chuck Schumer", "Elie Wiesel"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6151386807636808}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.5714285714285715, 0.2, 1.0, 0.4, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9056", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-8217", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-6987", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-2618", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-6052", "mrqa_triviaqa-validation-2988", "mrqa_hotpotqa-validation-745", "mrqa_newsqa-validation-2655", "mrqa_searchqa-validation-4712", "mrqa_searchqa-validation-5647"], "SR": 0.5625, "CSR": 0.5771875, "EFR": 1.0, "Overall": 0.745671875}, {"timecode": 50, "UKR": 0.734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1097", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-1245", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1625", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2035", "mrqa_hotpotqa-validation-2164", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2203", "mrqa_hotpotqa-validation-2258", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2368", "mrqa_hotpotqa-validation-2389", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3317", "mrqa_hotpotqa-validation-3345", "mrqa_hotpotqa-validation-336", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-3437", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3528", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-3825", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3865", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4519", "mrqa_hotpotqa-validation-4527", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5302", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-5389", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5616", "mrqa_hotpotqa-validation-5737", "mrqa_hotpotqa-validation-5882", "mrqa_hotpotqa-validation-604", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-634", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-88", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2280", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-3074", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6399", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-662", "mrqa_naturalquestions-validation-6987", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7480", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7517", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-7942", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-9004", "mrqa_naturalquestions-validation-9029", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-9367", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9684", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1296", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-1592", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3642", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-644", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-937", "mrqa_searchqa-validation-10521", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-1184", "mrqa_searchqa-validation-1254", "mrqa_searchqa-validation-12987", "mrqa_searchqa-validation-13081", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13246", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-13419", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-13634", "mrqa_searchqa-validation-13735", "mrqa_searchqa-validation-14037", "mrqa_searchqa-validation-146", "mrqa_searchqa-validation-14635", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-15114", "mrqa_searchqa-validation-15366", "mrqa_searchqa-validation-15685", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16512", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2598", "mrqa_searchqa-validation-2598", "mrqa_searchqa-validation-2624", "mrqa_searchqa-validation-2631", "mrqa_searchqa-validation-2917", "mrqa_searchqa-validation-2977", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4675", "mrqa_searchqa-validation-4723", "mrqa_searchqa-validation-4741", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-578", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5910", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7297", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-785", "mrqa_searchqa-validation-7873", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-9274", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-9442", "mrqa_searchqa-validation-9508", "mrqa_searchqa-validation-9760", "mrqa_searchqa-validation-9820", "mrqa_squad-validation-1", "mrqa_squad-validation-10020", "mrqa_squad-validation-10027", "mrqa_squad-validation-10041", "mrqa_squad-validation-10054", "mrqa_squad-validation-10137", "mrqa_squad-validation-10160", "mrqa_squad-validation-10206", "mrqa_squad-validation-1028", "mrqa_squad-validation-103", "mrqa_squad-validation-10316", "mrqa_squad-validation-10416", "mrqa_squad-validation-10500", "mrqa_squad-validation-1051", "mrqa_squad-validation-1098", "mrqa_squad-validation-1148", "mrqa_squad-validation-1247", "mrqa_squad-validation-1311", "mrqa_squad-validation-1379", "mrqa_squad-validation-1394", "mrqa_squad-validation-1424", "mrqa_squad-validation-1467", "mrqa_squad-validation-1474", "mrqa_squad-validation-1481", "mrqa_squad-validation-1516", "mrqa_squad-validation-1544", "mrqa_squad-validation-157", "mrqa_squad-validation-1640", "mrqa_squad-validation-167", "mrqa_squad-validation-1695", "mrqa_squad-validation-1736", "mrqa_squad-validation-1758", "mrqa_squad-validation-1771", "mrqa_squad-validation-1862", "mrqa_squad-validation-1956", "mrqa_squad-validation-1964", "mrqa_squad-validation-1977", "mrqa_squad-validation-2122", "mrqa_squad-validation-218", "mrqa_squad-validation-2191", "mrqa_squad-validation-2292", "mrqa_squad-validation-2292", "mrqa_squad-validation-2315", "mrqa_squad-validation-2648", "mrqa_squad-validation-267", "mrqa_squad-validation-2719", "mrqa_squad-validation-2736", "mrqa_squad-validation-3048", "mrqa_squad-validation-3069", "mrqa_squad-validation-3112", "mrqa_squad-validation-3125", "mrqa_squad-validation-3146", "mrqa_squad-validation-327", "mrqa_squad-validation-328", "mrqa_squad-validation-338", "mrqa_squad-validation-3469", "mrqa_squad-validation-3599", "mrqa_squad-validation-3642", "mrqa_squad-validation-366", "mrqa_squad-validation-3661", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3907", "mrqa_squad-validation-3921", "mrqa_squad-validation-3941", "mrqa_squad-validation-3942", "mrqa_squad-validation-402", "mrqa_squad-validation-4168", "mrqa_squad-validation-4173", "mrqa_squad-validation-4304", "mrqa_squad-validation-4404", "mrqa_squad-validation-4429", "mrqa_squad-validation-4442", "mrqa_squad-validation-4478", "mrqa_squad-validation-4646", "mrqa_squad-validation-4655", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-478", "mrqa_squad-validation-4802", "mrqa_squad-validation-4807", "mrqa_squad-validation-4875", "mrqa_squad-validation-4896", "mrqa_squad-validation-4953", "mrqa_squad-validation-4958", "mrqa_squad-validation-5077", "mrqa_squad-validation-5311", "mrqa_squad-validation-5322", "mrqa_squad-validation-5337", "mrqa_squad-validation-5396", "mrqa_squad-validation-5428", "mrqa_squad-validation-5457", "mrqa_squad-validation-547", "mrqa_squad-validation-5503", "mrqa_squad-validation-5537", "mrqa_squad-validation-5611", "mrqa_squad-validation-5635", "mrqa_squad-validation-5754", "mrqa_squad-validation-5846", "mrqa_squad-validation-5857", "mrqa_squad-validation-5877", "mrqa_squad-validation-5927", "mrqa_squad-validation-5967", "mrqa_squad-validation-6086", "mrqa_squad-validation-61", "mrqa_squad-validation-6115", "mrqa_squad-validation-6122", "mrqa_squad-validation-6125", "mrqa_squad-validation-6128", "mrqa_squad-validation-6224", "mrqa_squad-validation-6279", "mrqa_squad-validation-6292", "mrqa_squad-validation-630", "mrqa_squad-validation-6312", "mrqa_squad-validation-6361", "mrqa_squad-validation-6380", "mrqa_squad-validation-6434", "mrqa_squad-validation-6453", "mrqa_squad-validation-6474", "mrqa_squad-validation-6520", "mrqa_squad-validation-6541", "mrqa_squad-validation-6726", "mrqa_squad-validation-6777", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-6879", "mrqa_squad-validation-6887", "mrqa_squad-validation-6919", "mrqa_squad-validation-7006", "mrqa_squad-validation-7165", "mrqa_squad-validation-7211", "mrqa_squad-validation-7217", "mrqa_squad-validation-725", "mrqa_squad-validation-7284", "mrqa_squad-validation-729", "mrqa_squad-validation-7537", "mrqa_squad-validation-7615", "mrqa_squad-validation-7627", "mrqa_squad-validation-7665", "mrqa_squad-validation-7683", "mrqa_squad-validation-7816", "mrqa_squad-validation-7835", "mrqa_squad-validation-7864", "mrqa_squad-validation-7871", "mrqa_squad-validation-7877", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8031", "mrqa_squad-validation-8065", "mrqa_squad-validation-8103", "mrqa_squad-validation-8143", "mrqa_squad-validation-820", "mrqa_squad-validation-821", "mrqa_squad-validation-8210", "mrqa_squad-validation-8244", "mrqa_squad-validation-8245", "mrqa_squad-validation-8250", "mrqa_squad-validation-8338", "mrqa_squad-validation-8401", "mrqa_squad-validation-844", "mrqa_squad-validation-8472", "mrqa_squad-validation-8554", "mrqa_squad-validation-8561", "mrqa_squad-validation-8566", "mrqa_squad-validation-8599", "mrqa_squad-validation-8792", "mrqa_squad-validation-8958", "mrqa_squad-validation-8977", "mrqa_squad-validation-9068", "mrqa_squad-validation-9151", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9325", "mrqa_squad-validation-9346", "mrqa_squad-validation-9541", "mrqa_squad-validation-9595", "mrqa_squad-validation-9623", "mrqa_squad-validation-9643", "mrqa_squad-validation-9680", "mrqa_squad-validation-9701", "mrqa_squad-validation-9709", "mrqa_squad-validation-973", "mrqa_squad-validation-9787", "mrqa_squad-validation-9837", "mrqa_squad-validation-9900", "mrqa_squad-validation-9944", "mrqa_squad-validation-999", "mrqa_triviaqa-validation-101", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1956", "mrqa_triviaqa-validation-2026", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2097", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3794", "mrqa_triviaqa-validation-3805", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4183", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-5102", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-561", "mrqa_triviaqa-validation-5655", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-6044", "mrqa_triviaqa-validation-6141", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-6610", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7531", "mrqa_triviaqa-validation-867"], "OKR": 0.837890625, "KG": 0.44765625, "before_eval_results": {"predictions": ["Rankine cycle", "1849", "Bureau of Buddhist and Tibetan Affairs (Xuanzheng Yuan)", "More than 1 million", "October 16, 2012", "Liverpool Bay", "\"Little Dixie\"", "Sadar Bazaar", "Burning Man", "The Wachowskis", "Revengers Tragedy", "not born in the United States", "Jessica Lange", "Boeing", "thermal shielding material", "County Executive", "Urijah Faber", "1980", "Ben R. Guttery", "Wiz Khalifa", "Matthew Ryan Kemp", "January 15, 1975", "Gibraltar", "Valeri Vladimirovich \"Val\" Bure", "German Shepherd", "March 19, 2017", "Albert", "the Summer Olympics", "the Mikoyan design bureau", "Vyd\u016bnas", "Greg Hertz", "seven", "1999", "a few", "Venus", "the 924", "\"The Process\"", "16 November 1973", "House of Fraser", "Duval", "William Allen White", "Tel Aviv University", "Stephen Crawford Young", "through YouTube", "94", "Rabat", "Stu Henderson", "Centers for Medicare & Medicaid Services (CMS)", "2013", "Montague", "Mark Neveldine and Brian Taylor", "Canyon", "in Middlesex County, Province of Massachusetts Bay", "The ladies'single figure skating competition of the 2018 Winter Olympics", "merengue", "the Treaty of Utrecht", "EMI", "Mariette", "in a Nazi concentration camp,", "Paul Ryan", "destroyed and his business is shattered,", "Enigma", "Chicago", "pitic"], "metric_results": {"EM": 0.5, "QA-F1": 0.6408320539754363}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, false, true, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.8, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8333333333333334, 0.5, 0.0, 1.0, 0.0, 0.5384615384615384, 0.0, 1.0, 0.5, 0.0, 1.0, 0.3333333333333333, 1.0, 0.11764705882352942, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8351", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-739", "mrqa_hotpotqa-validation-188", "mrqa_hotpotqa-validation-3228", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-1634", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-5296", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-289", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-2306", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-453", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-1961", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-2743", "mrqa_triviaqa-validation-6409", "mrqa_triviaqa-validation-6055", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2853", "mrqa_searchqa-validation-4409", "mrqa_searchqa-validation-13144"], "SR": 0.5, "CSR": 0.5756740196078431, "EFR": 1.0, "Overall": 0.7191191789215686}, {"timecode": 51, "before_eval_results": {"predictions": ["356 \u00b1 47 tonnes per hectare", "acupuncture, moxibustion, pulse diagnosis, and various herbal drugs and elixirs", "more than half", "5K", "all-Gemini veteran crew", "Euna Lee,", "strawberry", "Hu Jintao", "$3 billion,", "Basel", "9-1", "Afghanistan's restive provinces", "Millvina Dean, is auctioning off her remaining mementos of the doomed ship to pay nursing home bills.", "Steven Green", "Fiorentina", "public opinion in Turkey.", "Honduran", "help women \" learn how to dance and feel sexy,\"", "45 minutes, five days a week.", "six", "95", "\"Quiet Nights,\"", "military personnel", "New Haven, Connecticut, firefighter Frank Ricci", "Stoke City.", "Australian officials", "Kim Il Sung", "Indonesian", "Karen Floyd", "a \" happy ending\" to the case.", "Brett Cummins", "workers have pulled a body from underneath the rubble of a collapsed apartment building", "three out of four questioned", "Anil Kapoor", "MS Columbus", "reached an agreement late Thursday to form a government of national reconciliation.", "The Wall Street Journal Europe", "refusal or inability to \"turn it off\"", "Sunday", "Nineteen", "off Somalia's coast.", "fake his own death by crashing his private plane into a Florida swamp.", "sailing", "Mugabe's opponents", "$24.1 million,", "Kaka", "south-central Washington, an area roughly half the size of Rhode Island.", "543", "Robert Park", "maintain an \"aesthetic environment\" and ensure public safety", "the FBI.", "Rolling Stone", "Branford College, the same residential college that her grandfather, Richard Gilmore, lived in, at the beginning of her sophomore year", "St. John's, Newfoundland and Labrador", "September 24, 2012", "Erinyes", "the liver", "tregarth", "21 kilometres south-east of Adelaide, in the Adelaide Hills.", "St James's Palace", "Salisbury", "The Saiga Antelope", "bush nut", "Livia"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6521798947580197}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.0, 0.0, 0.07692307692307693, 1.0, 0.0, 1.0, 1.0, 0.8750000000000001, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7272727272727272, 0.0, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2594", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-492", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-2446", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-5096", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-2306", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-3324", "mrqa_searchqa-validation-2005", "mrqa_searchqa-validation-1154", "mrqa_searchqa-validation-15795"], "SR": 0.578125, "CSR": 0.5757211538461539, "retrieved_ids": ["mrqa_squad-train-23912", "mrqa_squad-train-66694", "mrqa_squad-train-16954", "mrqa_squad-train-81459", "mrqa_squad-train-41176", "mrqa_squad-train-82721", "mrqa_squad-train-33073", "mrqa_squad-train-77529", "mrqa_squad-train-5890", "mrqa_squad-train-26036", "mrqa_squad-train-39682", "mrqa_squad-train-36217", "mrqa_squad-train-48977", "mrqa_squad-train-44591", "mrqa_squad-train-70545", "mrqa_squad-train-26805", "mrqa_hotpotqa-validation-2935", "mrqa_squad-validation-635", "mrqa_triviaqa-validation-6666", "mrqa_squad-validation-3688", "mrqa_squad-validation-1710", "mrqa_naturalquestions-validation-7387", "mrqa_searchqa-validation-3933", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-6141", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-16776", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-4145", "mrqa_hotpotqa-validation-1097", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-3341"], "EFR": 0.9629629629629629, "Overall": 0.7117211983618235}, {"timecode": 52, "before_eval_results": {"predictions": ["education, sanitation, and traffic control within the city limits", "\u00a342,090,", "Genghis Khan", "Emergency Highway Energy Conservation Act", "\"training Day\"", "Havenhurst", "24 hours", "1 April 1985", "the George Washington Bridge", "Red Rock West", "Larry Eustachy,", "shortstop", "Stephen King", "Sports Illustrated", "Chicago Bears", "43rd", "\"The School Boys\"", "In Pursuit", "British", "Memphis, Tennessee", "Fort Saint Anthony", "Martin Lee Truex Jr.", "The Onion", "\"Feels Like Love\"", "1993", "Saint Louis County", "Republican", "Anno 2053", "Wabanaki Confederacy", "Martin Ingerman", "New York Islanders", "Division of Cook", "Bigfoot", "\"The King of Chutzpah\".", "$10\u201320 million", "Julianne Moore", "beer", "Francis the Talking Mule", "Mexico", "model", "16,725", "\"Confessions of a Teenage Drama Queen\".", "superhero roles as the Marvel Comics characters Steve Rogers / Captain America in the Marvel Cinematic Universe and Johnny Storm / Human Torch in \"Fantastic Four\" and", "October 13, 1980", "HBO World Championship Boxing", "College Football Scoreboard", "January 1930", "Buffalo", "General Sir John Monash", "Godiva", "science fiction drama", "1978", "December 9, 2017", "the Deathly Hallows", "Tom Robinson", "Muhammad Ali", "Elvis Presley", "Sri Lanka", "April 6, 1994", "from Texas and Oklahoma to points east,", "Brown-Waite", "Abuja", "Cuba", "Judo"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7424851190476189}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 0.3571428571428571, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7246", "mrqa_squad-validation-6150", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-2791", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-3058", "mrqa_hotpotqa-validation-797", "mrqa_hotpotqa-validation-1316", "mrqa_hotpotqa-validation-2297", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-5155", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-5475", "mrqa_hotpotqa-validation-2577", "mrqa_naturalquestions-validation-10416", "mrqa_triviaqa-validation-5506", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-1016"], "SR": 0.609375, "CSR": 0.5763561320754718, "EFR": 0.96, "Overall": 0.7112556014150944}, {"timecode": 53, "before_eval_results": {"predictions": ["Johannes Bugenhagen and Philipp Melanchthon", "Renaissance", "Warszawa", "X-Men: Apocalypse", "Levon Helm", "three", "Zack Snyder", "the Qin dynasty", "the University of Vienna", "the University of Texas at Austin", "Oregon Ducks", "1970s and 1980s", "1983", "Headless Body in Topless Bar", "considered the father of modern flight training", "Gerard Marenghi", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "UDC", "University of Southern California", "20", "invoice", "October 13, 1980", "Russell T Davies", "nearly 8 km", "Tony Aloupis", "London", "Antonio Salieri", "USS Essex", "Lonestar", "deemed to be comparable to the seven Wonders of the World", "Merck & Co.,", "Dundalk", "A123 Systems, LLC", "Northern Ireland", "shock cavalry", "Robert L. Stone", "American", "Flushed Away", "Unibet", "four", "The Division of Cook", "Macau, China", "New York Shakespeare Festival", "Princess Muna al-Hussein", "1881", "John II Casimir Vasa", "Nina Stibbe", "1987", "1994", "Panic!", "Robert \"Bobby\" Bunda", "RAF Mount Pleasant", "Akshay Kumar", "Dr. Sachchidananda Sinha", "The Lykan Hypersport", "Flemish", "Jennifer Ellison", "China", "to do jobs that Arizonans wouldn't do.", "Robert Barnett", "more than a million residents who have been displaced by", "Mark Twain", "peameal", "Warren Covington"], "metric_results": {"EM": 0.625, "QA-F1": 0.7315949675324676}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 0.5, 0.45454545454545453, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 1.0, 0.28571428571428575, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-608", "mrqa_hotpotqa-validation-4578", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-309", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-1051", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-3708", "mrqa_hotpotqa-validation-2884", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-118", "mrqa_hotpotqa-validation-3784", "mrqa_hotpotqa-validation-4900", "mrqa_naturalquestions-validation-8326", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-4189", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-3178", "mrqa_searchqa-validation-14354", "mrqa_searchqa-validation-9008"], "SR": 0.625, "CSR": 0.5772569444444444, "EFR": 1.0, "Overall": 0.719435763888889}, {"timecode": 54, "before_eval_results": {"predictions": ["\u00a320,980", "more environmental degradation", "1876", "Standard Model", "The United States of America", "50JJB Sports Fitness Clubs", "Cherokee National Holiday", "Gatwick Airport", "Berea College", "Daniel Espinosa", "Bill Boyd", "John Monash", "1860", "25 November 2015", "Trilochanpala", "Charles Edward Stuart", "Martin Truex Jr.", "Objectivism", "Ronald Ryan", "United Healthcare", "Samuel Beckett", "Barbara Bush", "mathematician", "Grace Nail Johnson", "Tom Jones", "sarod", "Bank of China Tower", "two Grammy awards", "Enemy", "40,400", "more than 100", "for crafting and voting on legislation", "Flaw", "237", "Matthieu Vaxivi\u00e8re", "puzzle", "Jennifer Taylor", "Canada Goose", "Nye County", "Prospero", "Orlando Predators", "Gust Avrakotos", "The King of Chutzpah", "Pakistan", "Girls' Generation", "NCAA Division I", "1968", "Vernier, Switzerland", "Park Hyung-Sik", "Britain", "West Wyalong", "17 December 1998", "104 colonists and Discovery", "My Summer Story", "Samantha Jo `` Mandy '' Moore", "6", "jerry", "Ben Watson", "Osman Ali Ahmed,", "Mexico", "to \"wipe out\" the United States", "Stones from the River", "a sneer", "potato"], "metric_results": {"EM": 0.625, "QA-F1": 0.674905303030303}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666665, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727272, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-7629", "mrqa_squad-validation-5540", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-4584", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2531", "mrqa_hotpotqa-validation-4696", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-2005", "mrqa_hotpotqa-validation-3208", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-2762", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-4877", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-213", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-14967", "mrqa_searchqa-validation-3400"], "SR": 0.625, "CSR": 0.578125, "retrieved_ids": ["mrqa_squad-train-40355", "mrqa_squad-train-24094", "mrqa_squad-train-13038", "mrqa_squad-train-42936", "mrqa_squad-train-31236", "mrqa_squad-train-18112", "mrqa_squad-train-16157", "mrqa_squad-train-34794", "mrqa_squad-train-63823", "mrqa_squad-train-77109", "mrqa_squad-train-8990", "mrqa_squad-train-8024", "mrqa_squad-train-11501", "mrqa_squad-train-5335", "mrqa_squad-train-32849", "mrqa_squad-train-85058", "mrqa_searchqa-validation-6365", "mrqa_squad-validation-4303", "mrqa_squad-validation-8690", "mrqa_hotpotqa-validation-4578", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-3468", "mrqa_newsqa-validation-3587", "mrqa_searchqa-validation-9894", "mrqa_hotpotqa-validation-243", "mrqa_newsqa-validation-1949", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-5590", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-2905", "mrqa_newsqa-validation-2892", "mrqa_hotpotqa-validation-5333"], "EFR": 1.0, "Overall": 0.719609375}, {"timecode": 55, "before_eval_results": {"predictions": ["mantle", "deep spiritual despair", "Three", "the printing press", "closed on 366 for eight wickets on the opening day.", "Pixar's", "New Zealand against Argentina, and world champions South Africa versus Australia", "Brazil", "9 percent", "put the gun in Christofi's hand, and even jumped in his pool, hoping to wash away the evidence.", "reduce the cost of auto repairs and insurance premium for consumers", "his father", "\"release\" civilians", "1940's", "the oceans", "228", "23 years", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "Sharon Bialek", "More than 15,000", "cell phones", "her boyfriend", "a \"prostitute\" and threatening to oust another from his country.", "\"Chadian authorities immediately accused the charity of kidnapping the children and concealing their identities.", "North Korea intends to launch a long-range missile in the near future,", "21", "\"Friday the 13th,\"", "bring closure to the families of three missing military men,", "illegal immigrants", "150", "1994", "Al-Shabaab, the radical Islamist militia that controls the city", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "Secretary of State Hillary Clinton", "those traveling near the Somali coast", "Basilan", "a skilled hacker", "on supporting full marriage equality,\"", "onto the college campus.", "President Sheikh Sharif Sheikh Ahmed", "four", "returning combat veterans", "fighting charges of Nazi war crimes", "12-1", "second", "criminals who had fired on an army patrol shot and killed the students, the Interior Ministry said.", "Mashhad", "543", "Fullerton, California,", "love the environment and hate using fuel", "the Russian air force", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun,\"", "Kimberlin Brown", "Philippe Petit", "the year 2026", "Hugh Hefner", "Switzerland", "Ladee-Lo", "1998", "2012 Olympic bronze medalist", "Mary Astor", "(day of Mars)", "wildebeest", "Parris Island"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6828369904889959}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 0.0, 0.8181818181818182, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3870967741935484, 1.0, 1.0, 1.0, 1.0, 0.26666666666666666, 0.10526315789473685, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.25, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-19", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-2398", "mrqa_newsqa-validation-2850", "mrqa_triviaqa-validation-6942", "mrqa_hotpotqa-validation-5657", "mrqa_hotpotqa-validation-1632", "mrqa_searchqa-validation-9626"], "SR": 0.609375, "CSR": 0.5786830357142857, "EFR": 1.0, "Overall": 0.7197209821428572}, {"timecode": 56, "before_eval_results": {"predictions": ["thyroid hormone activity", "microscopic analysis of oriented thin sections of geologic samples", "$41 trillion", "in Eisleben, Saxony, then part of the Holy Roman Empire", "EU naval force", "Gyanendra,", "a head injury.", "Haitians", "by the time the Presidents Day holiday weekend", "Harrison Ford", "Kenyan Defense Minister Yusuf Haji,", "Don Draper", "\"A good vegan cupcake has the power to transform everything for the better,\"", "on a dangerous stretch of Highway 18 near Grand Ronde, Oregon.", "President Sheikh Sharif Sheikh Ahmed", "two satellites", "Sgt. Jason Bendett of the 3rd Platoon, A Company, 2nd Light armored Reconnaissance Battalion,", "150", "workers walked off the job January 28 to protest the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "CNN/Opinion Research Corporation", "Amitabh Bachchan", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.\"", "militants.", "Nothing But Love", "(Saturn) owners", "French army helicopter taking off from French frigate Nivose,", "two", "the job bill's controversial millionaire's surtax,", "California,", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "attempted murder,", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "because a new model is simply out of their reach.", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "Wigan Athletic", "Pakistani policemen", "Kris Allen,", "Chester Stiles, 38,", "President Bush", "1,073 immigration detainees", "Iggy and the Stooges", "\"Wicked.\"", "Monday and Tuesday", "opposition party", "Larry Ellison,", "9 million.", "Leo Frank,", "\"This is not something that anybody can reasonably anticipate,\"", "unknown,", "North Korea", "Jaime Andrade", "\"The Cycle of Life,\"", "actions taken by employers or unions that violate the National Labor Relations Act of 1935", "in late 1969 UTC", "24", "triathletes", "Thomas Chippendale", "Andy Murray", "Martin \"Marty\" McCann", "Captain B.J. Hunnicutt", "Jackson Storm", "arsenic", "the 'X'", "It is a dynamic, contemporary Australian university, proud of its reputation for quality teaching and the strength of its"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5876685830294577}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false], "QA-F1": [0.0, 0.9473684210526316, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.4615384615384615, 0.33333333333333337, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 0.15384615384615388, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.9411764705882353, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5384615384615384, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6649", "mrqa_squad-validation-5038", "mrqa_squad-validation-2212", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-3645", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-827", "mrqa_newsqa-validation-1031", "mrqa_newsqa-validation-862", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-2013", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-6470", "mrqa_hotpotqa-validation-2702", "mrqa_searchqa-validation-10114", "mrqa_searchqa-validation-6692"], "SR": 0.46875, "CSR": 0.5767543859649122, "EFR": 0.9705882352941176, "Overall": 0.713452899251806}, {"timecode": 57, "before_eval_results": {"predictions": ["Sunday Times University of the Year", "probabilistic Turing machine", "d'Hondt", "Gene Barry", "Muhammad", "Djokovic", "two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "Around 1200", "Atlanta, Georgia", "Jessica Simpson", "Kevin Spacey", "Bart Cummings ( 1965, 1966, 1967, 1974, 1975, 1977, 1979, 1990, 1991, 1996, 1999, 2008", "Jodie Foster", "Tenochtitlan", "Aaron Harrison", "virtual reality simulator", "Edward Seton", "Lake Powell", "September of that year", "two", "Lori Rom", "`` speed limit '' omitted and an additional panel stating the type of hazard ahead", "a stray wandering the streets of Moscow", "Daniel A. Dailey", "Antigonon leptopus", "James Chadwick", "green", "United Nations", "Mason Alan Dinehart", "skeletal muscle and the brain", "depicting multiple alternative realities rather than a novel", "Lou Rawls", "2017", "New York City", "Pat McCormick", "the season ten episode `` Nelson's Sparrow ''", "Michael Phelps", "Seattle", "five", "the four - letter suffix", "Steve Russell", "Steve Hale ( who was first introduced in the season five episode `` Sisters in Crime '' )", "Philippe Petit", "United States", "October 28, 2007", "the top 50 accounts", "post translational modification", "Hon July Moyo", "`` Audrey II ''", "Mankombu Sambasivan Swaminathan", "1994", "Chris Rea", "Smeagol", "Tim Peake", "knife", "Seretse Goitsebeng Maphiri Khama", "Waltham Abbey", "Kentucky River", "about 50", "Israel", "five female pastors", "Satan", "Narcissus", "a husband"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6343657159833631}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.35294117647058826, 0.0, 1.0, 0.0, 1.0, 1.0, 0.09090909090909091, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.6666666666666666, 0.16666666666666669, 0.0, 0.0, 1.0, 0.2666666666666667, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-10598", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-7502", "mrqa_triviaqa-validation-4753", "mrqa_triviaqa-validation-5211", "mrqa_triviaqa-validation-5598", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-323", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-2275", "mrqa_searchqa-validation-1213", "mrqa_searchqa-validation-3021"], "SR": 0.5625, "CSR": 0.5765086206896552, "retrieved_ids": ["mrqa_squad-train-14723", "mrqa_squad-train-79886", "mrqa_squad-train-69311", "mrqa_squad-train-43767", "mrqa_squad-train-69067", "mrqa_squad-train-24291", "mrqa_squad-train-85704", "mrqa_squad-train-12471", "mrqa_squad-train-60917", "mrqa_squad-train-44455", "mrqa_squad-train-28371", "mrqa_squad-train-72728", "mrqa_squad-train-43136", "mrqa_squad-train-37613", "mrqa_squad-train-32371", "mrqa_squad-train-13322", "mrqa_naturalquestions-validation-7362", "mrqa_newsqa-validation-3316", "mrqa_triviaqa-validation-2777", "mrqa_hotpotqa-validation-2646", "mrqa_searchqa-validation-13360", "mrqa_hotpotqa-validation-757", "mrqa_triviaqa-validation-4877", "mrqa_searchqa-validation-3427", "mrqa_triviaqa-validation-6026", "mrqa_hotpotqa-validation-339", "mrqa_naturalquestions-validation-8982", "mrqa_hotpotqa-validation-1365", "mrqa_newsqa-validation-847", "mrqa_triviaqa-validation-2306", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-1432"], "EFR": 0.8928571428571429, "Overall": 0.6978575277093596}, {"timecode": 58, "before_eval_results": {"predictions": ["Tomingaj, near Gra\u010dac", "Class II MHC molecules", "2011", "fall 2010", "Frankie Muniz", "De Wayne Warren", "The Continental Congress", "Tom Brady", "1997", "2016", "plant food", "modern random - access memory ( RAM )", "Andaman and Nicobar Islands", "W. Edwards Deming", "LED illuminated display", "1804", "the end of 1066", "Billie Jean King", "minor key symphonies", "a major fall in stock prices", "Since 1940", "Ben Findon", "Christy Plunkett ( Anna Faris )", "$19.8 trillion", "Walter ( Jonathan Goldstein )", "six degrees of freedom", "frontal lobe", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "MGM Resorts International", "Davos", "asphyxia", "$72", "Hans Christian Andersen", "January 1, 1976", "Bill Pullman", "July 4, 1776", "headdresses", "a theory", "eight", "two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "Athens", "American author Elizabeth George Speare", "a bank", "Austria - Hungary", "a mark that reminds of the Omnipotent Lord, which is formless", "two installments", "loosely on Eminem's actual upbringing, and follows white rapper B - Rabbit ( Eminem ) and his attempt to launch a career in a genre dominated by African - Americans", "the Chicago metropolitan area", "Will Champion", "the Reverse - Flash", "Natya Shastra", "fictional characters", "Triumph", "a type of electrified hybrid urban and suburban railway", "fibroblast", "1964", "A1 Recordings", "mathematician and physicist", "Josef Fritzl,", "gasoline", "\"Well, about time.\"", "ticks", "Joe Biden", "poison gas"], "metric_results": {"EM": 0.375, "QA-F1": 0.5269790488310708}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false], "QA-F1": [0.5, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.1904761904761905, 0.888888888888889, 0.8, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.4, 0.12500000000000003, 0.6666666666666666, 1.0, 0.0, 0.5, 0.8, 0.4, 1.0, 1.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.6153846153846153, 0.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1227", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-4219", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-1507", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-3246", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7751", "mrqa_hotpotqa-validation-4280", "mrqa_hotpotqa-validation-4856", "mrqa_newsqa-validation-509", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-7402"], "SR": 0.375, "CSR": 0.5730932203389831, "EFR": 0.975, "Overall": 0.7136030190677967}, {"timecode": 59, "before_eval_results": {"predictions": ["equal in magnitude and opposite in direction", "100", "productivity gap", "Toby Keith", "John Adams", "Abbot Suger", "senators", "1966", "Justice Lawrence John Wargrave", "Isaiah Amir Mustafa", "Effy", "MGM Resorts International", "Sunni Muslim family", "Sherwood Forest", "`` Nelson's Chamber '', `` Lady Arbuthnot's chamber ''", "538", "Six Degrees of Separation", "Turner Layton", "`` 1 lakh of people ''", "Richard Stallman", "Ferm\u00edn Francisco de Lasu\u00e9n", "manage the characteristics of the beer's head", "Sean O' Neal", "Brazil", "950 pesos ( approximately $ 18 ) in the Philippines or $60 abroad", "thirteen Academy Award nominations, including Best Picture, Best Director for Fincher, Best Actor for Pitt and Best Supporting Actress for Taraji P. Henson", "the ACU", "16.5 quadrillion BTUs of primary energy", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "`` G conduiter's Diet Beer, '' developed in 1967 by Joseph L. Owades, PhD, a biochemist working for New York's Rheingold Brewery", "Speaker of the House of Representatives", "cell - mediated, cytotoxic innate immunity )", "Miami Heat", "Terry Kath", "3, 1, and 4", "pathology", "reared in South Africa", "Sarah Silverman", "the Coriolis force", "Number 4, Privet Drive, Little Whinging in Surrey, England", "Warren Zevon", "produced with constant technology and resources per unit of time", "amino acids glycine and arginine", "1917", "regulatory site", "Jocelyn Flores", "October 27, 2017", "five girls, Payson, Lauren, and finally Colleen", "1912", "Zilphia Horton", "Bryan Cranston", "July 2, 1776", "India's Sachin Tendulkar", "Christian Dior", "\"Party of God\"", "Field Marshal Lord Gort", "841", "VH1", "September 21.", "Sen. Barack Obama", "Robert", "Wendell, North Carolina", "cinnamon", "a tigrinum"], "metric_results": {"EM": 0.53125, "QA-F1": 0.666920955882353}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false], "QA-F1": [0.6, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.8, 0.08, 0.0, 0.6666666666666666, 0.35294117647058826, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.33333333333333337, 0.5, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10333", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-5454", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-6091", "mrqa_triviaqa-validation-4570", "mrqa_hotpotqa-validation-992", "mrqa_newsqa-validation-1976", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-10417"], "SR": 0.53125, "CSR": 0.5723958333333333, "EFR": 0.9666666666666667, "Overall": 0.711796875}, {"timecode": 60, "UKR": 0.732421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1097", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-1245", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1574", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1625", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1806", "mrqa_hotpotqa-validation-1892", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2035", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2164", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2203", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2258", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2368", "mrqa_hotpotqa-validation-2389", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3228", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3317", "mrqa_hotpotqa-validation-3345", "mrqa_hotpotqa-validation-336", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-3437", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3528", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3649", "mrqa_hotpotqa-validation-3825", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3865", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4186", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4519", "mrqa_hotpotqa-validation-4527", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4811", "mrqa_hotpotqa-validation-4811", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5302", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-5389", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5616", "mrqa_hotpotqa-validation-5737", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-739", "mrqa_hotpotqa-validation-810", "mrqa_hotpotqa-validation-88", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-412", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-592", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-662", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7480", "mrqa_naturalquestions-validation-7502", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7517", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-7942", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8154", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8423", "mrqa_naturalquestions-validation-853", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8870", "mrqa_naturalquestions-validation-9029", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-9367", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-9931", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1296", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-1592", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2039", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-2210", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-2283", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2474", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-2594", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3103", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3642", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-492", "mrqa_newsqa-validation-644", "mrqa_newsqa-validation-644", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-862", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10521", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-1254", "mrqa_searchqa-validation-12987", "mrqa_searchqa-validation-13081", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13246", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-13634", "mrqa_searchqa-validation-13735", "mrqa_searchqa-validation-14037", "mrqa_searchqa-validation-146", "mrqa_searchqa-validation-14635", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-15114", "mrqa_searchqa-validation-15366", "mrqa_searchqa-validation-15685", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16512", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2598", "mrqa_searchqa-validation-2631", "mrqa_searchqa-validation-2917", "mrqa_searchqa-validation-2977", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4675", "mrqa_searchqa-validation-4741", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5647", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-578", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5910", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-785", "mrqa_searchqa-validation-7873", "mrqa_searchqa-validation-8393", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-9274", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-9442", "mrqa_searchqa-validation-9508", "mrqa_searchqa-validation-9760", "mrqa_searchqa-validation-9820", "mrqa_squad-validation-1", "mrqa_squad-validation-10020", "mrqa_squad-validation-10027", "mrqa_squad-validation-10054", "mrqa_squad-validation-10137", "mrqa_squad-validation-10160", "mrqa_squad-validation-10206", "mrqa_squad-validation-1028", "mrqa_squad-validation-10309", "mrqa_squad-validation-10316", "mrqa_squad-validation-10416", "mrqa_squad-validation-10500", "mrqa_squad-validation-1051", "mrqa_squad-validation-1098", "mrqa_squad-validation-1148", "mrqa_squad-validation-1311", "mrqa_squad-validation-1394", "mrqa_squad-validation-1424", "mrqa_squad-validation-1467", "mrqa_squad-validation-1474", "mrqa_squad-validation-1516", "mrqa_squad-validation-1544", "mrqa_squad-validation-167", "mrqa_squad-validation-1695", "mrqa_squad-validation-1736", "mrqa_squad-validation-1758", "mrqa_squad-validation-1771", "mrqa_squad-validation-1862", "mrqa_squad-validation-1956", "mrqa_squad-validation-1964", "mrqa_squad-validation-1977", "mrqa_squad-validation-2122", "mrqa_squad-validation-218", "mrqa_squad-validation-2191", "mrqa_squad-validation-2292", "mrqa_squad-validation-2292", "mrqa_squad-validation-2315", "mrqa_squad-validation-2648", "mrqa_squad-validation-2719", "mrqa_squad-validation-2736", "mrqa_squad-validation-3048", "mrqa_squad-validation-3069", "mrqa_squad-validation-3112", "mrqa_squad-validation-3146", "mrqa_squad-validation-327", "mrqa_squad-validation-328", "mrqa_squad-validation-338", "mrqa_squad-validation-3469", "mrqa_squad-validation-3599", "mrqa_squad-validation-3642", "mrqa_squad-validation-3661", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3907", "mrqa_squad-validation-3921", "mrqa_squad-validation-3942", "mrqa_squad-validation-402", "mrqa_squad-validation-4168", "mrqa_squad-validation-4173", "mrqa_squad-validation-4304", "mrqa_squad-validation-4404", "mrqa_squad-validation-4429", "mrqa_squad-validation-4442", "mrqa_squad-validation-4646", "mrqa_squad-validation-4655", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-478", "mrqa_squad-validation-4802", "mrqa_squad-validation-4875", "mrqa_squad-validation-4896", "mrqa_squad-validation-4953", "mrqa_squad-validation-4958", "mrqa_squad-validation-5077", "mrqa_squad-validation-5311", "mrqa_squad-validation-5322", "mrqa_squad-validation-5337", "mrqa_squad-validation-5396", "mrqa_squad-validation-5428", "mrqa_squad-validation-5457", "mrqa_squad-validation-547", "mrqa_squad-validation-5503", "mrqa_squad-validation-5537", "mrqa_squad-validation-5611", "mrqa_squad-validation-5635", "mrqa_squad-validation-5754", "mrqa_squad-validation-5857", "mrqa_squad-validation-5877", "mrqa_squad-validation-5927", "mrqa_squad-validation-5967", "mrqa_squad-validation-6086", "mrqa_squad-validation-61", "mrqa_squad-validation-6115", "mrqa_squad-validation-6122", "mrqa_squad-validation-6125", "mrqa_squad-validation-6128", "mrqa_squad-validation-6150", "mrqa_squad-validation-6224", "mrqa_squad-validation-6279", "mrqa_squad-validation-6312", "mrqa_squad-validation-6361", "mrqa_squad-validation-6380", "mrqa_squad-validation-6434", "mrqa_squad-validation-6453", "mrqa_squad-validation-6474", "mrqa_squad-validation-6726", "mrqa_squad-validation-6777", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-6879", "mrqa_squad-validation-6887", "mrqa_squad-validation-6919", "mrqa_squad-validation-7006", "mrqa_squad-validation-7165", "mrqa_squad-validation-7211", "mrqa_squad-validation-7217", "mrqa_squad-validation-725", "mrqa_squad-validation-7284", "mrqa_squad-validation-729", "mrqa_squad-validation-7537", "mrqa_squad-validation-7615", "mrqa_squad-validation-7627", "mrqa_squad-validation-7665", "mrqa_squad-validation-7683", "mrqa_squad-validation-7816", "mrqa_squad-validation-7835", "mrqa_squad-validation-7877", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8103", "mrqa_squad-validation-8143", "mrqa_squad-validation-820", "mrqa_squad-validation-8210", "mrqa_squad-validation-8244", "mrqa_squad-validation-8245", "mrqa_squad-validation-8250", "mrqa_squad-validation-8338", "mrqa_squad-validation-8401", "mrqa_squad-validation-8566", "mrqa_squad-validation-8599", "mrqa_squad-validation-8792", "mrqa_squad-validation-8958", "mrqa_squad-validation-9068", "mrqa_squad-validation-9151", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9325", "mrqa_squad-validation-9346", "mrqa_squad-validation-9541", "mrqa_squad-validation-9595", "mrqa_squad-validation-9643", "mrqa_squad-validation-9701", "mrqa_squad-validation-9709", "mrqa_squad-validation-973", "mrqa_squad-validation-9787", "mrqa_squad-validation-9796", "mrqa_squad-validation-9837", "mrqa_squad-validation-999", "mrqa_triviaqa-validation-101", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1956", "mrqa_triviaqa-validation-2026", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-2097", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3594", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3794", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4183", "mrqa_triviaqa-validation-4198", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-5102", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-5506", "mrqa_triviaqa-validation-561", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-6141", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6610", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7531", "mrqa_triviaqa-validation-867"], "OKR": 0.822265625, "KG": 0.42578125, "before_eval_results": {"predictions": ["1963", "The Judicial Council", "Sports Night", "April 1917", "Kim Basinger", "Fonzworth Bentley", "24", "henchmen", "the 1840s", "September 1980", "Landon Jones, in his book Great Expectations : America and the Baby Boom Generation ( 1980 ), defined the span of the baby - boom generation as extending from 1943 through 1960", "a Welsh privateer - turned - pirate", "2018", "IFN - \u03b5", "the Persian Gulf", "the spinal cord", "1999", "the common law", "Montreal Canadiens", "February 26, 2018", "Malina Weissman", "49", "2009", "Felix Baumgartner", "2019", "Chelsea", "Ra\u00fal Eduardo Esparza", "Shawn Wayans", "The National Legislature was moved to Washington prematurely, at the urging of President John Adams", "eleven", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "Lady Gaga", "Mitch Murray", "May 2017", "during the day", "July", "Barbara Windsor", "scrolls", "Charles Cavaroc v", "oxidant", "the flagship campus of the University of Wisconsin System", "Starscream", "gave the German Cabinet -- in effect, Chancellor Adolf Hitler -- the power to enact laws without the involvement of the Reichstag", "Macon Blair", "8 December 1985", "a hostname", "1998", "Tess", "Eukarya", "the development of electronic computers in the 1950s", "exceeds the normal resting rate", "a liquid crystal on silicon ( LCoS ) ( based on an LCo S chip from Himax ), field - sequential color system, LED illuminated display", "\u201cDance Stance\u201d", "Triumph and Disaster", "1500m", "1991", "Urijah Christopher Faber", "1993 to 1996", "Spanish Davis Cup hero Fernando Verdasco", "Teen Patti", "called Chu \"uniquely suited to be our next secretary of energy\"", "The Rio Grande", "the kidney", "Cobb salad"], "metric_results": {"EM": 0.5, "QA-F1": 0.5987139432451932}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, true, false, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.08333333333333334, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.2857142857142857, 0.0, 0.4, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.918918918918919, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 0.060606060606060615, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8368", "mrqa_naturalquestions-validation-3018", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-5997", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-3347", "mrqa_naturalquestions-validation-7300", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-754", "mrqa_triviaqa-validation-5198", "mrqa_triviaqa-validation-2359", "mrqa_hotpotqa-validation-2625", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-674", "mrqa_searchqa-validation-3656"], "SR": 0.5, "CSR": 0.5712090163934427, "retrieved_ids": ["mrqa_squad-train-61006", "mrqa_squad-train-51120", "mrqa_squad-train-59654", "mrqa_squad-train-63871", "mrqa_squad-train-7959", "mrqa_squad-train-2855", "mrqa_squad-train-76447", "mrqa_squad-train-1288", "mrqa_squad-train-45049", "mrqa_squad-train-33032", "mrqa_squad-train-80314", "mrqa_squad-train-35959", "mrqa_squad-train-32498", "mrqa_squad-train-5808", "mrqa_squad-train-40188", "mrqa_squad-train-78979", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-2232", "mrqa_naturalquestions-validation-3074", "mrqa_newsqa-validation-2780", "mrqa_triviaqa-validation-776", "mrqa_hotpotqa-validation-2800", "mrqa_squad-validation-5906", "mrqa_hotpotqa-validation-1316", "mrqa_searchqa-validation-3400", "mrqa_searchqa-validation-5795", "mrqa_newsqa-validation-1136", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-3074", "mrqa_triviaqa-validation-3117", "mrqa_squad-validation-7626", "mrqa_searchqa-validation-14988"], "EFR": 0.9375, "Overall": 0.6978355532786885}, {"timecode": 61, "before_eval_results": {"predictions": ["Charles Darwin's", "to manage the pharmacy department and specialised areas in pharmacy practice allowing pharmacists the time to specialise in their expert field as medication consultants spending more time working with patients and in research", "interacting and working directly with students", "Burma", "sorrow", "Red Bank", "\"Tracks of My Tears\"", "an overcast day", "Edward of Bordeaux", "Lost in America", "Louis XV", "mustard", "Nunavut", "a tape measure", "the Sea", "the turquoise", "horns", "Carousel", "mug", "jackson", "a barrel", "Eve", "the chakra", "the plague", "stocks.", "Sam Snead", "Thomas Becket", "Colorado", "Wall Street", "Eastman Kodak Company", "Juicy Couture", "the skirt", "You Are What You Are (Beautiful) odiai", "Istanbul", "Port Washington", "the Obelisk", "Brad Paisley", "an &", "The Beatles", "Massachusetts", "Theodore Roosevelt", "a violin", "The Lamb", "Stephen F. Austin", "Elvis Presley", "The Washington Post", "Man Ray", "the bassoon", "Juliette Binoche", "the Bay of Biscay", "The Mystery of Edwin Drood", "Gene Krupa", "Gorakhpur Junction", "about 12,000 years ago", "The Star Spangled Banner", "Spain", "peter", "Katy Perry", "Tool", "The entity", "Mark Helfrich", "Steven Green", "Saturday's Hungarian Grand Prix.", "People Against Switching Sides (PASS)"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5404761904761904}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true], "QA-F1": [0.5, 0.45000000000000007, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.25, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5179", "mrqa_squad-validation-6324", "mrqa_searchqa-validation-9701", "mrqa_searchqa-validation-9357", "mrqa_searchqa-validation-10839", "mrqa_searchqa-validation-9796", "mrqa_searchqa-validation-16918", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-16254", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-14750", "mrqa_searchqa-validation-12163", "mrqa_searchqa-validation-2710", "mrqa_searchqa-validation-7733", "mrqa_searchqa-validation-2006", "mrqa_searchqa-validation-1041", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-764", "mrqa_searchqa-validation-14742", "mrqa_searchqa-validation-2011", "mrqa_searchqa-validation-7030", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-4774", "mrqa_searchqa-validation-8059", "mrqa_searchqa-validation-174", "mrqa_searchqa-validation-5057", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-8268", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-4966", "mrqa_searchqa-validation-4354", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8014", "mrqa_newsqa-validation-1733"], "SR": 0.4375, "CSR": 0.5690524193548387, "EFR": 1.0, "Overall": 0.7099042338709678}, {"timecode": 62, "before_eval_results": {"predictions": ["Saul Alinsky", "spring of 1329", "to protect their tribal lands from commercial interests", "Legally Blonde", "Equus", "Shia LaBeouf", "the Teamsters Union", "New Zealand", "a microwave oven", "FDR", "Budapest", "feminism", "John Hersey", "Tofu", "a mansard roof", "seven months, and two days", "Judge Advocate General's Corps", "The Kingston Trio", "the Ottoman Empire", "the Narnia", "Vermont", "a rolling stone", "ex-wife", "Austria", "Dante", "a piccolo", "Kosher Wines", "The Hague", "(Mary) Martin", "All About Eve", "U2", "Laos", "St Andrew", "\"Walden\"", "a diamond", "Anthony Fokker", "an artist, dancer and choreographer who popularized Indian dance through his effective use of western theatrical techniques", "Flanders", "March 8, 1917", "Bots", "a carpet", "a lampoon", "hydrogen", "a centaur", "the Algonquin word for \"where the river narrows\"", "treasury bonds", "a necropolis", "Israel", "\"Little Women\"", "the ceiling", "Belgium", "the Vietnam War", "12,000 BC", "Jewish audiences", "October 6, 2017", "Japan", "Fred Gwynne", "Vancouver", "TD Garden", "Mitsubishi Motors Corporation", "zoonotic", "seeking help", "Zimbabwe President", "a fight outside of an Atlanta strip club"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6301288075959128}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.3157894736842105, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.30769230769230765]}}, "before_error_ids": ["mrqa_squad-validation-4314", "mrqa_searchqa-validation-14081", "mrqa_searchqa-validation-7735", "mrqa_searchqa-validation-2017", "mrqa_searchqa-validation-14524", "mrqa_searchqa-validation-959", "mrqa_searchqa-validation-13817", "mrqa_searchqa-validation-2666", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-13068", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-12070", "mrqa_searchqa-validation-6169", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-9729", "mrqa_searchqa-validation-8773", "mrqa_searchqa-validation-1728", "mrqa_searchqa-validation-13426", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-12122", "mrqa_searchqa-validation-12255", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-11763", "mrqa_searchqa-validation-2783", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-3027", "mrqa_triviaqa-validation-4487", "mrqa_hotpotqa-validation-620", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-85"], "SR": 0.53125, "CSR": 0.5684523809523809, "EFR": 0.9666666666666667, "Overall": 0.7031175595238095}, {"timecode": 63, "before_eval_results": {"predictions": ["\"villes de s\u00fbret\u00e9\"", "39", "easier credit", "Pedro Calomino", "May 5, 2015", "Shakespeare", "Dennis Potter", "1909 Cuban-American Major League Clubs Series", "Martin Ingerman", "\"The Godfather Part II\"", "seal hunting", "October 21, 2016", "Memphis", "William Corcoran Eustis", "Painter in Miniatures", "Oregon Ducks", "The club", "December 24, 1973", "Christian", "Polihale State Park", "the northwest tip of Canisteo Peninsula in Amundsen Sea", "its riverside location", "320,520", "The O2 Arena", "Ringo Starr", "Designed by KlingStubbins", "FC Bayern Munich", "four", "Elsie Audrey Mossom", "Rice", "the famous Albert Bridge", "Count Schlieffen", "Mexican War on Drugs", "Erreway", "Leinster", "Oracle", "Katherine Harris", "a Hong Kong actor", "Ice Princess", "Hellenism", "Iceland", "A. E. Housman", "Cyclic Defrost", "Losing My Religion", "the Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Believe", "Boston University", "dachshund", "Frank Sinatra", "Woking, England", "Army & Navy Stores", "2016\u201317", "Kate Walsh", "November 3, 2007", "no longer a fundamental right", "Cybill Shepherd", "Pamplona", "the narwhal", "$40 billion during", "provides nearly $162 billion in war funding", "Democratic VP candidate", "Haile", "cherry", "Gettysburg"], "metric_results": {"EM": 0.59375, "QA-F1": 0.712079326923077}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, false, false, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.8, 0.25, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.4, 0.923076923076923, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-639", "mrqa_hotpotqa-validation-2364", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-3897", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2862", "mrqa_hotpotqa-validation-3396", "mrqa_hotpotqa-validation-3544", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-2621", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-5723", "mrqa_naturalquestions-validation-2242", "mrqa_triviaqa-validation-51", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-163", "mrqa_searchqa-validation-585", "mrqa_searchqa-validation-14467"], "SR": 0.59375, "CSR": 0.56884765625, "retrieved_ids": ["mrqa_squad-train-22817", "mrqa_squad-train-19647", "mrqa_squad-train-4303", "mrqa_squad-train-64831", "mrqa_squad-train-11077", "mrqa_squad-train-5863", "mrqa_squad-train-43741", "mrqa_squad-train-11411", "mrqa_squad-train-75838", "mrqa_squad-train-13820", "mrqa_squad-train-28836", "mrqa_squad-train-80089", "mrqa_squad-train-82279", "mrqa_squad-train-76705", "mrqa_squad-train-9156", "mrqa_squad-train-31572", "mrqa_hotpotqa-validation-4839", "mrqa_newsqa-validation-1234", "mrqa_squad-validation-1625", "mrqa_squad-validation-608", "mrqa_hotpotqa-validation-2916", "mrqa_naturalquestions-validation-4054", "mrqa_newsqa-validation-1948", "mrqa_naturalquestions-validation-2605", "mrqa_triviaqa-validation-2314", "mrqa_searchqa-validation-10923", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-4720", "mrqa_naturalquestions-validation-1165", "mrqa_triviaqa-validation-3527", "mrqa_newsqa-validation-1580", "mrqa_naturalquestions-validation-8555"], "EFR": 1.0, "Overall": 0.70986328125}, {"timecode": 64, "before_eval_results": {"predictions": ["six", "Six-time", "rearview mirror", "German rock band Scorpions", "the RAF", "trying to fit in", "Kyrie Irving", "over 300,000", "Canada", "Jesse Frederick James Conaway", "B.R. Ambedkar", "1987", "a Malibu, California beach", "during the American Civil War", "moist temperate climates, with much of the world's production occurring near the 48th parallel north", "twelve", "Lana Del Rey", "Nicole Gale Anderson", "TLC -", "Le Petit Chaperon Rouge", "Djokovic", "rock music subgenres", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "declared neutrality", "1986", "between 1765 and 1783", "annuity", "John Garfield as Al Schmid", "Michael Crawford", "weaker when it came to training and tertiary education", "the New Testament canon", "4 September 1936", "Hollywood, Los Angeles, California", "sometime between 124 and 800 CE, with some theories dating the earliest Polynesian settlements to the 10th or even 13th century", "biscuit", "Scheria", "Audrey II", "Master Christopher Jones", "RMS Titanic", "the right side of the heart to the lungs", "Pittsburgh in 2008", "the sea witch character who appears in the fairy tale `` The Little Mermaid '' by Hans Christian Andersen", "15 February 1998", "Thomas Jefferson", "Arthur Chung", "advisory speed signs are classified as warning signs, not regulatory signs", "March 31, 2013", "semi-automatic, but not fully automatic, firearms by Swiss citizens and foreigners with permanent residence", "1987", "Rajendra Prasad", "Donna", "December 1800", "gold", "Western Samoa", "Labrador retrievingvers", "micronutrient-rich diet", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller", "December 13, 1920", "raping and killing a 14-year-old Iraqi girl.", "verify the authenticity of the voice on the tape.", "$627,", "A Portrait of the Artist", "Crete", "Brownie"], "metric_results": {"EM": 0.5, "QA-F1": 0.6550316003441004}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.4, 0.4, 0.0, 0.6153846153846153, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.22222222222222218, 0.6666666666666666, 1.0, 0.4, 0.16666666666666669, 0.5714285714285715, 1.0, 0.0, 0.6666666666666666, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.2, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-571", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8275", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-462", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-3342", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-1382", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-3505", "mrqa_hotpotqa-validation-5873", "mrqa_hotpotqa-validation-5688", "mrqa_searchqa-validation-1924"], "SR": 0.5, "CSR": 0.5677884615384615, "EFR": 0.9375, "Overall": 0.6971514423076923}, {"timecode": 65, "before_eval_results": {"predictions": ["1943", "CALIPSO", "Matt Flinders", "2026 -- the centenary of Gaud\u00ed's death", "in the pouring rain", "a compact layout to combine keys which are usually kept separate", "Ra\u00fal Eduardo Esparza", "1962", "April 1979", "1792", "Charles Carson", "November 5, 2017", "to universalize the topic of the song into something everyone could relate to and ascribe personal meaning to in their own way", "1974", "6,259", "the transcellular compartment", "O'Meara", "classical architecture", "September 19, 2017", "almost all officeholders annually", "Rashida Jones", "Nodar Kumaritashvili", "Olympia", "2000", "J.P. Zenger High", "Georges Auguste Escoffier", "after Shawn's kidnapping", "1904", "the American Kennel Club", "Liam Cunningham", "before the first year begins", "Amanda Bynes", "Woody Paige", "Albert Einstein", "the brain", "Tremont, Ohio", "David Ben - Gurion", "T.J. Miller", "1995", "toys", "Dirk Benedict", "New Croton Reservoir in Westchester and Putnam counties", "the source of the donor organ", "prevent further offense", "Bill Henderson", "humid subtropical climate", "Roger Federer", "the mid-1990s", "Paris", "St. John's, Newfoundland and Labrador", "Elvis Presley", "southern Anatolia", "Hugh Laurie", "All Things Must Pass", "bacall", "Salvatore Testa", "Newark, New Jersey", "National Mall", "75.", "Grand Ronde, Oregon.", "1994,", "Buddha", "Oslo", "Coretta Scott King"], "metric_results": {"EM": 0.5, "QA-F1": 0.5718290579974061}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, false, true, false, true, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, false, false, false, true, true, true, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.058823529411764705, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.13953488372093023, 0.0, 0.0, 0.28571428571428575, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.7272727272727272, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-1153", "mrqa_squad-validation-4228", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-9253", "mrqa_naturalquestions-validation-4520", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-6634", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-5259", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-9383", "mrqa_triviaqa-validation-5247", "mrqa_hotpotqa-validation-242", "mrqa_hotpotqa-validation-4896", "mrqa_searchqa-validation-3274"], "SR": 0.5, "CSR": 0.5667613636363636, "EFR": 0.9375, "Overall": 0.6969460227272728}, {"timecode": 66, "before_eval_results": {"predictions": ["24 March 1879", "shopping", "saliva", "fanny", "South African", "Ben Whishaw", "Neighbours", "toronto", "Wawrinka", "Vienna", "Dennis", "Brocks Hill", "Cyprus", "The Nutcracker", "temperature", "Take That", "Egypt", "Cole", "carburetors", "off the coast of Northumberland", "Denmark", "Conduction", "Willy Russell", "beets", "Fahrenheit", "dusseldorf", "\"Li'l Abner,\"", "Alex Murphy", "aromatherapy", "Lieutenant-General", "barbeor", "Fenn Street School", "Fran\u00e7ois Hollande", "Johannesburg", "abba", "Vernon Kay", "Madrid", "The Danelaw", "toronto", "Wine Director/Sommelier", "oven", "blue", "June Carter", "Ned Sherrin", "The Pillow Book", "Sally Ride", "archery", "wool", "1992", "Billy Fury", "California", "Anton Chekhov", "1898", "The Third Five - year Plan", "`` Killer Within ''", "Kathleen O'Brien", "Nan Britton", "Venice", "Haleigh Cummings", "These planning processes are urgently needed and have been a long time in coming.", "on Capitol Hill,", "the bark", "Plumeria rubra", "the Fertile Crescent"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5888888888888889}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, false, true, true, false, false, true, false, true, false, false, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 0.8, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-355", "mrqa_triviaqa-validation-1997", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-4013", "mrqa_triviaqa-validation-5529", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-6481", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-3407", "mrqa_triviaqa-validation-2682", "mrqa_triviaqa-validation-2164", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-4790", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-598", "mrqa_triviaqa-validation-4421", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-3673", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-4915", "mrqa_triviaqa-validation-4946", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-2926", "mrqa_searchqa-validation-11467"], "SR": 0.546875, "CSR": 0.566464552238806, "retrieved_ids": ["mrqa_squad-train-56166", "mrqa_squad-train-11999", "mrqa_squad-train-15644", "mrqa_squad-train-68915", "mrqa_squad-train-9655", "mrqa_squad-train-62158", "mrqa_squad-train-71576", "mrqa_squad-train-19099", "mrqa_squad-train-57273", "mrqa_squad-train-11551", "mrqa_squad-train-74070", "mrqa_squad-train-42550", "mrqa_squad-train-34974", "mrqa_squad-train-69065", "mrqa_squad-train-79532", "mrqa_squad-train-78145", "mrqa_hotpotqa-validation-5848", "mrqa_triviaqa-validation-5148", "mrqa_hotpotqa-validation-2855", "mrqa_searchqa-validation-12546", "mrqa_naturalquestions-validation-8986", "mrqa_squad-validation-9568", "mrqa_hotpotqa-validation-4472", "mrqa_searchqa-validation-9894", "mrqa_naturalquestions-validation-4351", "mrqa_newsqa-validation-2902", "mrqa_naturalquestions-validation-8591", "mrqa_newsqa-validation-929", "mrqa_searchqa-validation-10721", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-6577", "mrqa_newsqa-validation-3557"], "EFR": 1.0, "Overall": 0.7093866604477612}, {"timecode": 67, "before_eval_results": {"predictions": ["1850", "Ed McCaffrey", "Dutch", "the first integrated circuit", "Abu Dhabi, United Arab Emirates", "Spain, Mexico and France", "Humberside Airport", "Jeff Meldrum", "Central Avenue", "2013", "Martin Truex Jr.", "compact car", "Frank Ocean", "Germanicus", "Detroit, Michigan", "the 944", "Knoxville", "Sean", "Brian Friel", "Paul W. S. Anderson", "indie and metal", "Nebraska Cornhuskers", "Station Casinos", "beer and soft drinks", "Muriel", "Awake", "Helena Sternlicht", "British", "Nickelodeon", "2,615", "McG", "\"Invader (Invasor)\"", "the Rolling Stones", "Orfeo ed Euridice", "Sean Yseult", "The Kansas City Crime Family", "34", "Paris", "1907", "Jenji Kohan", "Sir William Collins", "Beatrice Muriel Hill Tinsley", "\"The Walking Dead\"", "Crossed", "singer, songwriter, actress, and radio and television", "1903", "Warsaw", "\"Slaughterhouse-Five\"", "2016", "The Spiderwick Chronicles", "Mediterranean", "1984", "Tom Brady", "on the left hand ring finger", "plant food, mainly grass and sedges, which were supplemented with herbaceous plants, flowering plants, shrubs, mosses, and tree matter", "How Many", "The Battle of Austerlitz", "ballet", "a rocket", "Apple co-founder", "Anonymous", "the horror", "plumbeus", "China"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6219618055555556}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, false, true, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.8, 0.7499999999999999, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.4, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.25, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9852", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-556", "mrqa_hotpotqa-validation-1399", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-529", "mrqa_hotpotqa-validation-3773", "mrqa_hotpotqa-validation-2766", "mrqa_hotpotqa-validation-5896", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3031", "mrqa_hotpotqa-validation-3752", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-3997", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-4276", "mrqa_hotpotqa-validation-2138", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-558", "mrqa_naturalquestions-validation-10093", "mrqa_triviaqa-validation-4195", "mrqa_triviaqa-validation-1630", "mrqa_newsqa-validation-1662", "mrqa_newsqa-validation-2617", "mrqa_searchqa-validation-7217"], "SR": 0.46875, "CSR": 0.5650275735294117, "EFR": 1.0, "Overall": 0.7090992647058824}, {"timecode": 68, "before_eval_results": {"predictions": ["the Miocene", "observer status", "Moton Field, the Tuskegee Army Air Field", "O'Meara", "digitization of social systems", "light utility vehicles", "Poems : Series 1", "South Dakota ( 30.3 % )", "Ali Daei", "Ukraine", "Woodrow Wilson", "a judicial officer, of a lower or puisne court, elected or appointed by means of a commission ( letters patent ) to keep the peace", "American country music duo The Bellamy Brothers", "the 1980s", "Hellenismos", "Batman", "Wakanda and the Savage Land", "August Darnell", "Lori Rom", "Nodar Kumaritashvili", "American blues electric guitar musician T - Bone Walker", "New Mexico", "The final venues were confirmed, along with the tournament's schedule, on 2 May 2013", "FIGG Bridge Engineers, a Tallahassee - based firm", "scrolls", "architecture", "Emmett Lathrop `` Doc '' Brown", "c. 1000 AD", "Baker, California, USA", "majority of members present at that time", "the passing of the year", "Christopher Allen Lloyd", "One Night in the Tropics", "Moscazzano", "the Lower Mainland in Vancouver", "Gare du Nord", "9th century", "a 5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "Wednesday, May 24, 2017", "A turlough, or turlach", "Glenn Close", "a substance that fully activates the receptor that it binds to ) while under other conditions, behaves as an antagonist", "Tommy Shaw", "All Hallows'Day", "the Red Sea", "sea water", "March 2018 in North America and Europe", "American singer Elvis Presley", "The pulmonary circulation is the portion of the circulatory system which carries deoxygenated blood away from the right ventricle of the heart, to the lungs", "Raja Dhilu", "1881", "1997", "the Riverwalk", "the Prussian 2nd Army", "Nessie", "Boyd Gaming", "\"Guardians of the Galaxy\"", "Love Actually", "five", "Washington", "managing his time.", "Robin Hood", "Lafayette", "Virgin Atlantic"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6780986201298701}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.24000000000000002, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2, 0.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 1.0, 0.8, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.72, 1.0, 0.28571428571428575, 0.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.1904761904761905, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-8183", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-9749", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-42", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-6843", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-6858", "mrqa_hotpotqa-validation-4345", "mrqa_newsqa-validation-4073", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-11403"], "SR": 0.546875, "CSR": 0.5647644927536232, "EFR": 0.9655172413793104, "Overall": 0.7021500968265867}, {"timecode": 69, "before_eval_results": {"predictions": ["Labor Party", "the hosts have the responsibility to ensure orderly delivery of packets", "900000 km2", "a minor basilica", "Immediate Media Company", "Royal College of Music", "Sullivan University", "Archbishop of Canterbury", "Canada's first train robbery", "The club will participate in the Premier League", "1941", "Portland, OR", "Chattahoochee", "Kennedy Road", "August 1973", "Hawaii County", "Summer Olympic Games", "Minnesota Timberwolves", "David Kossoff", "a priest", "England", "coaxial", "Tony Ducks", "striker", "Colin Vaines", "Bhushan Patel", "win world titles in four weight classes", "Dark Heresy", "Texas Tech University", "actress", "Scott Mosier", "Paradise, Nevada", "Adelaide", "Erinsborough", "interstate commerce", "Darci Kistler", "Philip Pullman's Trilogy \"His Dark Materials\"", "Molly Hatchet", "\"Alceste\"", "Prada", "Su\u00f0reyjar", "Mary Bonauto, Susan Murray", "David Wells", "Nayvadius DeMun Wilburn", "Macomb County", "wine", "Boston, Massachusetts", "Juventus", "23 June 1912", "Matthew Abraham \"Matt\" Groening ( ; born February 15, 1954) is an American cartoonist, writer, producer, animator, and voice actor.", "Helen Aberson", "Sulla", "1923", "Harrison Ford", "Bill Pullman", "sett", "Kwame Nkrumah", "docked", "\"Draquila -- Italy Trembles.", "positive signal", "ConAgra Foods plant", "the Bears", "Pikes Peak", "James Watt"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6377388784461153}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true], "QA-F1": [0.8, 0.4, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.21052631578947367, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2886", "mrqa_squad-validation-4777", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-2081", "mrqa_hotpotqa-validation-3491", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-3474", "mrqa_hotpotqa-validation-1008", "mrqa_hotpotqa-validation-5041", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-4037", "mrqa_naturalquestions-validation-622", "mrqa_triviaqa-validation-6018", "mrqa_newsqa-validation-627", "mrqa_searchqa-validation-9505"], "SR": 0.578125, "CSR": 0.5649553571428572, "retrieved_ids": ["mrqa_squad-train-76484", "mrqa_squad-train-76783", "mrqa_squad-train-73808", "mrqa_squad-train-62536", "mrqa_squad-train-248", "mrqa_squad-train-24349", "mrqa_squad-train-43914", "mrqa_squad-train-77947", "mrqa_squad-train-37842", "mrqa_squad-train-2976", "mrqa_squad-train-2586", "mrqa_squad-train-40215", "mrqa_squad-train-22866", "mrqa_squad-train-74608", "mrqa_squad-train-10886", "mrqa_squad-train-66241", "mrqa_hotpotqa-validation-5310", "mrqa_naturalquestions-validation-6091", "mrqa_searchqa-validation-2977", "mrqa_triviaqa-validation-2137", "mrqa_newsqa-validation-1035", "mrqa_searchqa-validation-3656", "mrqa_searchqa-validation-6209", "mrqa_newsqa-validation-1848", "mrqa_squad-validation-7629", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3485", "mrqa_naturalquestions-validation-7242", "mrqa_hotpotqa-validation-3971", "mrqa_newsqa-validation-234", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1364"], "EFR": 1.0, "Overall": 0.7090848214285714}, {"timecode": 70, "UKR": 0.767578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1012", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-1126", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1266", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1399", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1522", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1634", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1748", "mrqa_hotpotqa-validation-1790", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1892", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2138", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2306", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2703", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-289", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3396", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-3581", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-4128", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4276", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4410", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-4486", "mrqa_hotpotqa-validation-4537", "mrqa_hotpotqa-validation-4579", "mrqa_hotpotqa-validation-4704", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4721", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-556", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-5657", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5820", "mrqa_hotpotqa-validation-5836", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-771", "mrqa_hotpotqa-validation-846", "mrqa_hotpotqa-validation-904", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1263", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1840", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2349", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4597", "mrqa_naturalquestions-validation-475", "mrqa_naturalquestions-validation-4816", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5230", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-6300", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-6634", "mrqa_naturalquestions-validation-6951", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7461", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7564", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8072", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8928", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-9979", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1088", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1915", "mrqa_newsqa-validation-1949", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2863", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3014", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3351", "mrqa_newsqa-validation-337", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3570", "mrqa_newsqa-validation-381", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-636", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-94", "mrqa_searchqa-validation-10269", "mrqa_searchqa-validation-10839", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-10900", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-11241", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-1154", "mrqa_searchqa-validation-11562", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-12507", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-1363", "mrqa_searchqa-validation-14332", "mrqa_searchqa-validation-14354", "mrqa_searchqa-validation-14679", "mrqa_searchqa-validation-15078", "mrqa_searchqa-validation-15188", "mrqa_searchqa-validation-15590", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15832", "mrqa_searchqa-validation-15958", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-1761", "mrqa_searchqa-validation-1977", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2073", "mrqa_searchqa-validation-2273", "mrqa_searchqa-validation-2307", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-2560", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-3324", "mrqa_searchqa-validation-3400", "mrqa_searchqa-validation-364", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-3828", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-4409", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-4966", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-6784", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-7720", "mrqa_searchqa-validation-8431", "mrqa_searchqa-validation-8518", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-893", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-959", "mrqa_searchqa-validation-9626", "mrqa_searchqa-validation-9729", "mrqa_searchqa-validation-9894", "mrqa_searchqa-validation-9984", "mrqa_squad-validation-10064", "mrqa_squad-validation-10168", "mrqa_squad-validation-10203", "mrqa_squad-validation-10309", "mrqa_squad-validation-10333", "mrqa_squad-validation-10338", "mrqa_squad-validation-10370", "mrqa_squad-validation-10416", "mrqa_squad-validation-1042", "mrqa_squad-validation-1048", "mrqa_squad-validation-10491", "mrqa_squad-validation-1182", "mrqa_squad-validation-1247", "mrqa_squad-validation-1349", "mrqa_squad-validation-1404", "mrqa_squad-validation-157", "mrqa_squad-validation-1577", "mrqa_squad-validation-1655", "mrqa_squad-validation-1769", "mrqa_squad-validation-1862", "mrqa_squad-validation-1956", "mrqa_squad-validation-1964", "mrqa_squad-validation-1977", "mrqa_squad-validation-2112", "mrqa_squad-validation-2118", "mrqa_squad-validation-2140", "mrqa_squad-validation-225", "mrqa_squad-validation-237", "mrqa_squad-validation-2390", "mrqa_squad-validation-2468", "mrqa_squad-validation-2521", "mrqa_squad-validation-256", "mrqa_squad-validation-2573", "mrqa_squad-validation-258", "mrqa_squad-validation-2599", "mrqa_squad-validation-2628", "mrqa_squad-validation-2668", "mrqa_squad-validation-2684", "mrqa_squad-validation-2719", "mrqa_squad-validation-2778", "mrqa_squad-validation-3063", "mrqa_squad-validation-3138", "mrqa_squad-validation-3141", "mrqa_squad-validation-3165", "mrqa_squad-validation-3204", "mrqa_squad-validation-3460", "mrqa_squad-validation-3522", "mrqa_squad-validation-3554", "mrqa_squad-validation-3599", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3925", "mrqa_squad-validation-3958", "mrqa_squad-validation-3965", "mrqa_squad-validation-4072", "mrqa_squad-validation-4122", "mrqa_squad-validation-4157", "mrqa_squad-validation-4162", "mrqa_squad-validation-4164", "mrqa_squad-validation-4286", "mrqa_squad-validation-4304", "mrqa_squad-validation-4311", "mrqa_squad-validation-4421", "mrqa_squad-validation-4429", "mrqa_squad-validation-4490", "mrqa_squad-validation-4579", "mrqa_squad-validation-4615", "mrqa_squad-validation-4755", "mrqa_squad-validation-4795", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4807", "mrqa_squad-validation-4875", "mrqa_squad-validation-5010", "mrqa_squad-validation-5081", "mrqa_squad-validation-5156", "mrqa_squad-validation-519", "mrqa_squad-validation-5256", "mrqa_squad-validation-5373", "mrqa_squad-validation-5472", "mrqa_squad-validation-5555", "mrqa_squad-validation-5754", "mrqa_squad-validation-5839", "mrqa_squad-validation-5891", "mrqa_squad-validation-6019", "mrqa_squad-validation-6166", "mrqa_squad-validation-6214", "mrqa_squad-validation-6227", "mrqa_squad-validation-6266", "mrqa_squad-validation-6524", "mrqa_squad-validation-6611", "mrqa_squad-validation-6613", "mrqa_squad-validation-664", "mrqa_squad-validation-6854", "mrqa_squad-validation-692", "mrqa_squad-validation-6924", "mrqa_squad-validation-6959", "mrqa_squad-validation-708", "mrqa_squad-validation-7111", "mrqa_squad-validation-7162", "mrqa_squad-validation-7203", "mrqa_squad-validation-7211", "mrqa_squad-validation-7357", "mrqa_squad-validation-7407", "mrqa_squad-validation-751", "mrqa_squad-validation-7559", "mrqa_squad-validation-7588", "mrqa_squad-validation-7639", "mrqa_squad-validation-7683", "mrqa_squad-validation-7752", "mrqa_squad-validation-7793", "mrqa_squad-validation-7831", "mrqa_squad-validation-7864", "mrqa_squad-validation-7902", "mrqa_squad-validation-7934", "mrqa_squad-validation-7979", "mrqa_squad-validation-823", "mrqa_squad-validation-8245", "mrqa_squad-validation-8476", "mrqa_squad-validation-8527", "mrqa_squad-validation-8566", "mrqa_squad-validation-8664", "mrqa_squad-validation-867", "mrqa_squad-validation-8837", "mrqa_squad-validation-9082", "mrqa_squad-validation-9145", "mrqa_squad-validation-9247", "mrqa_squad-validation-9317", "mrqa_squad-validation-9362", "mrqa_squad-validation-9567", "mrqa_squad-validation-9569", "mrqa_squad-validation-9587", "mrqa_squad-validation-9643", "mrqa_squad-validation-9653", "mrqa_squad-validation-9944", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-1625", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1681", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-1871", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3081", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-3917", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-424", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4343", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4615", "mrqa_triviaqa-validation-4718", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5431", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5885", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-679", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-828", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-977"], "OKR": 0.884765625, "KG": 0.503125, "before_eval_results": {"predictions": ["Song dynasty", "manage the pharmacy department", "$106,482,500", "a president who understands the world today, the future we seek and the change we", "rape and murdered.", "45 minutes,", "the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Michael Jackson", "1000 square meters in forward deck space, allowing for such features as a full garden and pool, a tennis court, or several heli-pads.", "\"utterly baseless\"", "a bag", "procedures", "laundromats", "opposition supporters", "Kingdom City", "the killing of a 15-year-old boy", "Section 60.", "spend $60 billion on America's infrastructure.", "the challenges a pregnancy", "The EU naval force", "Bob Bogle", "composer", "22-10.", "$273 million", "O2 Arena.", "the idea that the Richmond students did nothing because of the \"bystander effect\"", "calls the Internet \"one of the most magnificent expressions of freedom and free enterprise in history\" and", "Lucky Dube,", "Unseeded", "the bill", "82", "150", "burns", "20-something woman at the tenteki 10 Caf\u00e9", "Ireland", "Jacob", "lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains", "flooding and debris", "more than 5,600", "the fact that the teens were charged as adults.", "one", "al-Maliki", "Mugabe's", "Kenyan and Somali", "snowstorm", "88-year-old", "Damon Bankston", "suicide bombing", "\"Watchmen\"", "The goal of the relief effort in the first 72 hours will be very focused on saving lives,\"", "a three-story residential building in downtown Nairobi.", "1998.", "Tom Tucker", "Indirect rule", "Kryptonite", "south Georgia", "jumbly", "James Stewart", "Robert Matthew Hurley", "Sulla", "Gondorian soldier", "Howard Hughes", "Harry Truman", "The Curse of the Black Pearl"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6655287114845938}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2985", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-3671", "mrqa_newsqa-validation-3393", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-2208", "mrqa_newsqa-validation-93", "mrqa_naturalquestions-validation-10523", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-3839", "mrqa_searchqa-validation-11618", "mrqa_searchqa-validation-10796"], "SR": 0.578125, "CSR": 0.5651408450704225, "EFR": 1.0, "Overall": 0.7441219190140845}, {"timecode": 71, "before_eval_results": {"predictions": ["playing cards", "the divinity of Jesus", "Rocky Down Mexico Way", "Festa di San Marco", "unrequited love", "Usher", "debts", "Cape Town", "Wikipedia", "yodeling", "the FBI", "Spartacus", "Bengal", "sphinx", "Prince Rogers Nelson", "Iraq", "the Senate", "Texas A&M University", "the Witch's Ride", "a snakes", "Moab", "Stanford", "a relationship with a man who proves to", "1927", "Transportation", "Mexico City", "the Chagos", "cat", "Bosnia and Herzegovina", "Christopher Columbus", "a ton", "the inquisition", "scrapple", "hyoid horns", "Joan of Arc", "Maternity", "Mulan", "Namibia", "Abraham Lincoln", "Alaska", "President Bush", "Jim Thorpe", "wombat", "the phi phenomenon", "Pushkin", "dessert sized", "the Norman-French army of William, the Duke of Normandy", "An Officer and a Gentleman", "Roger Bannister", "Ronald Reagan", "Monopoly", "Volkswagen", "Lulu", "Miami Heat of the National Basketball Association ( NBA )", "Jurchen Aisin Gioro clan", "a black Ferrari", "the Netherlands", "Harvard University", "1978", "1866", "276,170 inhabitants", "New Haven, Connecticut, firefighter", "stand down.", "Wednesday."], "metric_results": {"EM": 0.59375, "QA-F1": 0.6272569444444445}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1151", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-3112", "mrqa_searchqa-validation-584", "mrqa_searchqa-validation-9916", "mrqa_searchqa-validation-5826", "mrqa_searchqa-validation-16238", "mrqa_searchqa-validation-6742", "mrqa_searchqa-validation-7661", "mrqa_searchqa-validation-5494", "mrqa_searchqa-validation-4802", "mrqa_searchqa-validation-16635", "mrqa_searchqa-validation-15728", "mrqa_searchqa-validation-14499", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-14020", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-3840", "mrqa_searchqa-validation-10902", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-10857", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-9639", "mrqa_triviaqa-validation-7650"], "SR": 0.59375, "CSR": 0.5655381944444444, "EFR": 1.0, "Overall": 0.7442013888888889}, {"timecode": 72, "before_eval_results": {"predictions": ["structure and substance of his questions and answers concerning baptism in the Small Catechism", "three", "Montpelier", "City of Hope National Medical Center", "New Orleans", "a router", "a dollar", "Herbert Hoover", "crawdads", "Doric", "Emil Von Behring", "Senator", "a yeast", "Lewis and Clark", "Mount Everest", "Symphony No. 9", "Etch A Sketch", "Peter Paul Rubens", "grapefruit", "Enrico Fermi", "the caterpillar", "Pilate", "the Juilliard School", "Ralph Lauren", "Mary Poppins", "air", "Kansas City", "Sergeant Philip K. Fish", "a steak", "Franklin Pierce", "a tabulator", "magnesium", "four", "occipital", "Mao Zedong", "Coretta Scott", "the uterine wall", "Men\\'s Basketball", "a Champagne", "thick", "a hog's thigh", "Spanish American War", "George Rogers Clark", "Mitch Robbins", "a dose", "Columbus Day", "Zimbabwe", "the Democratic Republic of Congo", "a cucumber", "a calves", "a person that dawdle", "Prince William", "Michigan State Spartans", "Todd Bridges", "1998", "feet", "Dave", "Berlin", "Tybalt", "evangelical Christian", "Windermere", "58 minutes.", "Kaka", "just over a year ago."], "metric_results": {"EM": 0.390625, "QA-F1": 0.5209821428571428}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, true, false, false, true, true], "QA-F1": [0.14285714285714288, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2448", "mrqa_searchqa-validation-2318", "mrqa_searchqa-validation-16859", "mrqa_searchqa-validation-6176", "mrqa_searchqa-validation-1119", "mrqa_searchqa-validation-3445", "mrqa_searchqa-validation-4928", "mrqa_searchqa-validation-16619", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-16434", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-12215", "mrqa_searchqa-validation-15701", "mrqa_searchqa-validation-1277", "mrqa_searchqa-validation-14222", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-9441", "mrqa_searchqa-validation-11217", "mrqa_searchqa-validation-11175", "mrqa_searchqa-validation-15456", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-13183", "mrqa_searchqa-validation-12319", "mrqa_searchqa-validation-16903", "mrqa_searchqa-validation-2531", "mrqa_searchqa-validation-14131", "mrqa_searchqa-validation-3608", "mrqa_searchqa-validation-13169", "mrqa_searchqa-validation-3267", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-15708", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-7516", "mrqa_triviaqa-validation-4521", "mrqa_triviaqa-validation-449", "mrqa_hotpotqa-validation-1680", "mrqa_newsqa-validation-802"], "SR": 0.390625, "CSR": 0.5631421232876712, "retrieved_ids": ["mrqa_squad-train-7735", "mrqa_squad-train-54908", "mrqa_squad-train-14857", "mrqa_squad-train-3471", "mrqa_squad-train-44858", "mrqa_squad-train-50118", "mrqa_squad-train-18241", "mrqa_squad-train-59572", "mrqa_squad-train-28784", "mrqa_squad-train-10928", "mrqa_squad-train-49738", "mrqa_squad-train-82588", "mrqa_squad-train-30307", "mrqa_squad-train-43265", "mrqa_squad-train-22540", "mrqa_squad-train-37393", "mrqa_searchqa-validation-4904", "mrqa_triviaqa-validation-7047", "mrqa_searchqa-validation-1642", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-1058", "mrqa_naturalquestions-validation-3522", "mrqa_searchqa-validation-3739", "mrqa_naturalquestions-validation-9684", "mrqa_triviaqa-validation-6807", "mrqa_naturalquestions-validation-870", "mrqa_searchqa-validation-9729", "mrqa_hotpotqa-validation-339", "mrqa_hotpotqa-validation-3395", "mrqa_searchqa-validation-2218", "mrqa_triviaqa-validation-5529", "mrqa_hotpotqa-validation-5125"], "EFR": 1.0, "Overall": 0.7437221746575343}, {"timecode": 73, "before_eval_results": {"predictions": ["IgG", "green chloroplast", "Ted", "Belle", "Discworld", "prometheus", "Hulk Hogan", "Thermopylae", "Rodgers & Hammerstein", "Jamaica", "Wawrinka", "Marillion", "\"Rocester.\"", "Roy Keane", "second", "Sarah Vaughan", "sheep, goats, and horses", "Sven Goran Eriksson", "Monaco", "2001: A Space Odyssey", "Irene Dunne", "Mozart", "Mary Quant", "Tina Turner", "a flower", "Abraham", "Fairey Swordfish", "Argentina", "the Porteous Riots", "Parma", "Julian Fellowes", "the cornea", "motocross", "John Nash", "Catwoman", "milk", "Lauren Bacall", "Edward Yorke", "Hokkaido", "\"Alice and Jerry\"", "bologna", "Some Like It Hot", "The Life and Opinions of Tristram Shandy", "Mahatma Gandhi", "Rosalind Leigh", "Erik Thorvaldson", "Churchill Downs", "Honda", "multi-user dungeon", "the narwhal", "Prophet Joseph Smith, Jr.", "Adidas", "Benzodiazepines", "in southern Anatolia", "the Delhi Sultanate", "Andes", "Fade Out: The Calamitous Final Days of MGM", "Fountains of Wayne", "debate preparation.", "four decades", "8 to 10 inches of snow", "wintergreen", "Grasshopper Gulch", "dachshunds"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6390625}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8645", "mrqa_triviaqa-validation-3651", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-4021", "mrqa_triviaqa-validation-5526", "mrqa_triviaqa-validation-7475", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-4491", "mrqa_triviaqa-validation-7124", "mrqa_triviaqa-validation-2197", "mrqa_triviaqa-validation-5147", "mrqa_triviaqa-validation-4323", "mrqa_triviaqa-validation-4342", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3518", "mrqa_triviaqa-validation-2728", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1384", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-4252", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-6119", "mrqa_naturalquestions-validation-6482", "mrqa_newsqa-validation-1019", "mrqa_searchqa-validation-9755"], "SR": 0.578125, "CSR": 0.5633445945945945, "EFR": 0.8888888888888888, "Overall": 0.7215404466966966}, {"timecode": 74, "before_eval_results": {"predictions": ["toys and models", "$10\u201320 million", "London", "1944", "Alfonso Cuar\u00f3n", "FC Bayern Munich", "DeskMate", "J. K. Rowling", "1967", "Anandji Virji Shah", "jena Malone", "\"Le Divorce\"", "Conservatorio Verdi", "The Bye Bye Man", "Esteban Ocon", "pinball", "Ben Elton", "400 MW", "\"personal earnings\" (such as salary and wages)", "1957", "Patricia Arquette", "Robins Air Force Base", "1993", "manny williams", "Newcastle upon Tyne, England", "the Recording Industry Association of America", "Zero Mostel", "Donald Duck", "2 March 1972", "The Vaudevillains", "Taylor Swift", "Ian Boothby", "Jean Acker", "1938", "Norse mythology", "National Hockey League", "Vincent Landay", "political activist", "Syracuse", "dachshund", "bullfighting and cockfighting", "Venus", "Detroit, Michigan", "largest country", "\"We'll Burn That Bridge\"", "Summerlin, Nevada", "gamecock", "1887", "\"The Ones Who Walk Away from Omelas\"", "Philip K. Dick", "Clinchfield Railroad", "Have I Told You Lately", "159", "an undisclosed location", "morocco", "port moresby", "john thompson", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "his boyhood experience in a World War II internment camp", "Lillo Brancato Jr.", "an eye", "bugsy siegel", "Penny Lane", "8"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6869428650340028}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7368421052631579, 0.0, 1.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.23076923076923078, 0.9411764705882353, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3223", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-517", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-5209", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-625", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-4640", "mrqa_hotpotqa-validation-1213", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-2420", "mrqa_triviaqa-validation-1597", "mrqa_triviaqa-validation-6614", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-3277", "mrqa_searchqa-validation-9976", "mrqa_triviaqa-validation-1193"], "SR": 0.609375, "CSR": 0.5639583333333333, "EFR": 1.0, "Overall": 0.7438854166666667}, {"timecode": 75, "before_eval_results": {"predictions": ["counterflow", "Colonel Robert E. Lee", "Donald", "for the red - bed country of its watershed", "best - of - seven", "eleven", "the 4th century", "Macon Blair", "Carolyn Sue Jones", "President pro tempore", "Geoffrey Dyson Palmer", "John Smith", "July 2010", "economic recession", "requiring all non-U.S. ('foreign') financial institutions ( JJIs ) to search their records for customers", "the breast or lower chest of beef or veal", "Edward V", "Presley Smith", "flour and water", "the Gupta Empire", "the dimension sign", "Dr. Rajendra Prasad", "April 20, 1983", "1900", "Tandi", "France", "Richard Parker", "Ryan Pinkston", "Fulton, Arkansas", "differential erosion", "Bumper Robinson and Terrence Howard", "2018", "lead", "a reference to a fictional character", "Katharine Hepburn -- Ethel Thayer", "D\u00e1in", "relieves the driving motor from the load of holding the elevator cab", "Deputy Speaker of the Lok Sabha", "the 1970s", "the `` 0 '' trunk code", "Siddharth Arora / Vibhav Roy", "epidermis", "in the books of Exodus and Deuteronomy", "Archduke Franz Ferdinand of Austria", "blighted ovum or anembryonic gestation", "The Mandate of Heaven", "85 %", "Tony Orlando and Dawn", "Lou Rawls", "William", "a crust of mashed potato", "bridges", "\"Wooden Heart (Muss I Denn)", "Billy", "Golden Globe", "Laurence Olivier Award", "port of Baltimore west to Sandy Hook", "bribing", "Mohamed Anwar al-Sadat", "Another high tide", "Labor Day", "bruce alexander", "a hammer", "Sarajevo"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6322821453142661}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, true, false, false, true, false, false, false, false, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4210526315789473, 0.6666666666666666, 0.3333333333333333, 1.0, 0.35294117647058826, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.1111111111111111, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.16666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 0.8333333333333333, 1.0, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.5714285714285715, 0.6666666666666666, 0.8, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-8114", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-7059", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-8064", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-3118", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10616", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-6266", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-1727", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-914", "mrqa_searchqa-validation-8818"], "SR": 0.46875, "CSR": 0.5627055921052632, "retrieved_ids": ["mrqa_squad-train-55169", "mrqa_squad-train-10252", "mrqa_squad-train-44333", "mrqa_squad-train-65542", "mrqa_squad-train-13800", "mrqa_squad-train-50310", "mrqa_squad-train-42759", "mrqa_squad-train-62030", "mrqa_squad-train-24597", "mrqa_squad-train-29446", "mrqa_squad-train-76560", "mrqa_squad-train-77671", "mrqa_squad-train-39147", "mrqa_squad-train-17780", "mrqa_squad-train-17747", "mrqa_squad-train-62427", "mrqa_newsqa-validation-4062", "mrqa_squad-validation-10333", "mrqa_naturalquestions-validation-8669", "mrqa_hotpotqa-validation-10", "mrqa_naturalquestions-validation-1704", "mrqa_newsqa-validation-1994", "mrqa_hotpotqa-validation-413", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-2732", "mrqa_searchqa-validation-12341", "mrqa_hotpotqa-validation-3481", "mrqa_searchqa-validation-777", "mrqa_hotpotqa-validation-4418", "mrqa_searchqa-validation-1924", "mrqa_squad-validation-8325", "mrqa_squad-validation-3688"], "EFR": 0.9411764705882353, "Overall": 0.7318701625386996}, {"timecode": 76, "before_eval_results": {"predictions": ["New Orleans", "2010", "1901", "1992", "the Election Commission of India", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "a convergent plate boundary", "Debbie Gibson", "Polly Walker", "the President", "employment in which a person works a minimum number of hours defined as such by his / her employer", "Castleford", "James Chadwick", "Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Mark Lowry", "Portuguese and Spanish - French origins", "Andrew Lloyd Webber", "2007 and 2008", "to capitalize on her publicity", "Within two weeks of the second devaluation", "Holden Nowell", "a graded basis, consisting of pass grades 1 ( threshold 85 %, a distinction ), 2 ( 70 -- 84 % ), 3 ( 55 -- 69 % ) & 4 ( 40 -- 54 % )", "The courts", "actions taken by employers or unions that violate the National Labor Relations Act of 1935 ( 49 Stat. 449 ) 29 U.S.C. \u00a7 151 -- 169 ( also known as the NLRA and the Wagner Act", "October 27, 1904", "The Parlement de Bretagne", "Vijaya Mulay", "Mark Humphrey as Frank Hogan", "Thomas Jefferson", "Ren\u00e9 Verdon", "Mario Puzo", "whether they wish to collect a jackpot prize in cash or annuity", "a young girl", "July 21, 1861", "Lead and lead dioxide", "Notts County ( 1894 )", "Mary Chapin Carpenter", "in an interview with Alternative Press in November 2016", "Hellenism", "ended Russia's participation in World War I", "February 7 in Los Angeles, California", "Torah", "Ye Hai Mohabbatein ( lit. This is love ) is an Indian soap opera which first aired on Star Plus on 3 December 2013", "save, rescue, savior", "In the 1920s", "Edward Kenway ( Matt Ryan ), a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "hijab", "the government - owned Panama Canal Authority", "tectonic", "Middle Eastern alchemy", "the investment bank Friedman Billings Ramsey", "the Hooded Claw", "Battle of Culloden", "the Forum", "McLaren-Honda", "Alexander Gorsky", "Turgay Sabit \u015eeren", "August 4, 2000", "Turkey", "the Southeast,", "biological", "king of Spain", "high jump", "Jane Addams"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6603287091743248}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, false, true, true, true, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.22222222222222224, 0.0, 1.0, 0.1904761904761905, 1.0, 0.8656716417910448, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7999999999999999, 0.0, 0.13333333333333333, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.3846153846153846, 0.4, 1.0, 0.7499999999999999, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-1656", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-5444", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-3546", "mrqa_naturalquestions-validation-9450", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-2011", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-3328", "mrqa_hotpotqa-validation-1257", "mrqa_searchqa-validation-1499"], "SR": 0.546875, "CSR": 0.5625, "EFR": 0.9310344827586207, "Overall": 0.7298006465517242}, {"timecode": 77, "before_eval_results": {"predictions": ["lay servants", "Randy Newman", "New Delhi", "Macon Blair", "Ann Gillespie", "A turlough, or turlach", "Gavie Chahal", "the 1830s", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "the 2nd century", "John Young", "Wilt Chamberlain and LeBron James", "March 18, 2005", "Sir Rowland Hill", "Spain", "one", "the Isthmus of Corinth", "Chris Rea", "Jodie Foster", "honey bees", "Cress", "Renhe Sports Management Ltd", "his brother", "A 30 - something man ( XXXX )", "Christina Giles", "scrolls dating back to the 12th century", "1546", "Emily Perkins", "Atlantic ocean", "Joseph M. Scriven", "David Seaman and ex-England cricketer Phil Tufnell", "c. 1000 AD", "if the player busts, the player loses, regardless of whether the dealer subsequently busts", "in the original Star Wars film in 1977", "the Peace of Westphalia of 1648, a stepping stone in the development of the modern state system", "MacFarlane", "Marie Van Brittan Brown", "an evaluation by an individual and can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "Richie Cunningham, played by Ron Howard", "16,801", "Kepner", "The Beatles", "the final episode of the series", "the summer of 1979", "10 May 1940", "December 14, 2017", "2018 and 2019", "May 2010", "the pyloric valve", "Rome, Italy", "Aegisthus", "the portrait of the monarch", "peter ackroyd", "seals", "political correctness", "professional wrestler, actor, and hip hop musician", "1944", "the City of Los Angeles", "Tuesday.", "the parliament within 15 days.", "Java", "ulna", "the Sahara", "Carisa Cunningham,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7048795122887865}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.5, 0.0, 1.0, 0.7741935483870968, 0.8, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.16666666666666669, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-540", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-6763", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-8491", "mrqa_triviaqa-validation-4659", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-2674", "mrqa_searchqa-validation-7968", "mrqa_searchqa-validation-9180"], "SR": 0.5625, "CSR": 0.5625, "EFR": 0.8928571428571429, "Overall": 0.7221651785714286}, {"timecode": 78, "before_eval_results": {"predictions": ["Ernest Gimson, Edward William Godwin, Charles Voysey, Adolf Loos and Otto Wagner", "Emily Perkins", "in the Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Frank Oz", "Mickey Rourke", "Thomas Jefferson, John Adams and Thomas Paine", "foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "Kryptonite", "in the management of balance of payments difficulties and international financial crises", "Hundreds or even thousands", "Iowa", "the Ramones", "The Massachusetts Compromise", "Gabrielle - Suzanne Barbot de Villeneuve", "Johannes Gutenberg", "Cress", "in rocks and minerals", "Devastator", "Lord's, on 15 July 2004 between Middlesex and Surrey, attracted a crowd of 27,509, the highest attendance for any county cricket game at the ground -- other than a one - day final -- since 1953", "April 1979", "Buddhism", "2014", "1974", "gravitation", "Supplemental oxygen", "Nancy Jean Cartwright", "the heart", "the coffee shop Monk's", "increased productivity, trade, and secular economic trends", "1986", "2018", "1960", "Garbi\u00f1e Muguruza", "Jurriaen Aernoutsz", "Sunni Muslim family", "3", "Ali Daei", "the intersection of Del Monte Blvd and Esplanade Street", "the primal rib", "Abigail Hawk", "Bed and breakfast", "J.P. Zenger High", "September 24", "in Super Bowl LII", "Nickelback", "a donor molecule", "Massachusetts", "Behavioral Analysis Unit", "October 29 - 30, 2012", "In Britain followed the rest of the world in decimalising its currency, the Mint moved from London to a new 38 acres ( 15 ha ) plant in Llantrisant, Wales", "British citizens", "Carpathia", "Scharnhorst", "Tasmania", "Prussia", "Gregg Harper", "Daphnis et Chlo\u00e9", "Mogadishu", "Linda Hogan,", "Hakeemullah Mehsud", "Joplin", "credit card debt", "Tolkien", "crimes committed against individuals"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6932220073849432}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false], "QA-F1": [0.25, 1.0, 0.9523809523809523, 1.0, 1.0, 0.06451612903225806, 0.9375, 1.0, 0.14285714285714288, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_squad-validation-5599", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-4021", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-3881", "mrqa_newsqa-validation-2391", "mrqa_searchqa-validation-12140", "mrqa_searchqa-validation-6610"], "SR": 0.609375, "CSR": 0.5630933544303798, "retrieved_ids": ["mrqa_squad-train-81415", "mrqa_squad-train-42128", "mrqa_squad-train-75285", "mrqa_squad-train-23736", "mrqa_squad-train-48764", "mrqa_squad-train-86053", "mrqa_squad-train-30747", "mrqa_squad-train-75390", "mrqa_squad-train-59484", "mrqa_squad-train-69868", "mrqa_squad-train-20249", "mrqa_squad-train-37250", "mrqa_squad-train-39838", "mrqa_squad-train-42121", "mrqa_squad-train-44176", "mrqa_squad-train-10711", "mrqa_squad-validation-8598", "mrqa_hotpotqa-validation-4696", "mrqa_triviaqa-validation-776", "mrqa_naturalquestions-validation-7264", "mrqa_searchqa-validation-13224", "mrqa_squad-validation-341", "mrqa_hotpotqa-validation-5723", "mrqa_squad-validation-8470", "mrqa_searchqa-validation-4548", "mrqa_naturalquestions-validation-4414", "mrqa_hotpotqa-validation-965", "mrqa_newsqa-validation-1225", "mrqa_searchqa-validation-4619", "mrqa_newsqa-validation-1495", "mrqa_hotpotqa-validation-220", "mrqa_searchqa-validation-7657"], "EFR": 0.92, "Overall": 0.727712420886076}, {"timecode": 79, "before_eval_results": {"predictions": ["in collenchyma tissue", "Dollywood", "Chicken Run", "a Stratocaster", "Portugal", "Roger Williams", "Amherst", "Annika Sorenstam", "a croissant", "Christmas", "John Keats", "Ford", "Tiger Woods", "Vanessa Hudgens", "50 million", "Starsky and Hutch", "John Locke", "Canterbury", "the Ottoman Empire", "Phil of the Future", "21", "Nacho Libre", "India", "Mork & Mindy", "867-5309", "Twin-lens reflex camera", "a backfire", "the Rhine & the Main", "Mentor", "Virgin", "Angel Gabriel", "Indiana", "Danny Elfman", "complementary", "the West Africa", "Captain", "driving Miss Daisy", "the hippo", "milk", "Pope John Paul II", "Viggo Mortensen", "Fugu", "Charles Dickens", "chervil", "Aston Martin", "the House of Commons", "Edward R. Murrow", "Mohandas Karamchand Gandhi", "Houston Rockets", "fief", "codemonkey13981", "Mad - Eye Moody and Hedwig", "American country music artist Toby Keith", "advisory speed signs", "Ireland", "Walt Whitman", "otto von Bismarck", "Dire Straits", "Hannaford Brothers Company", "motor ships", "Barack Obama", "Cannes Film Festival,", "Tuesday", "Help!"], "metric_results": {"EM": 0.625, "QA-F1": 0.7208705357142857}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8288", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-2001", "mrqa_searchqa-validation-5773", "mrqa_searchqa-validation-4550", "mrqa_searchqa-validation-15298", "mrqa_searchqa-validation-10313", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-10982", "mrqa_searchqa-validation-766", "mrqa_searchqa-validation-2126", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-8932", "mrqa_searchqa-validation-16557", "mrqa_searchqa-validation-9466", "mrqa_searchqa-validation-1686", "mrqa_searchqa-validation-4209", "mrqa_searchqa-validation-15443", "mrqa_searchqa-validation-1559", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-10271", "mrqa_triviaqa-validation-5181", "mrqa_hotpotqa-validation-5724"], "SR": 0.625, "CSR": 0.5638671875, "EFR": 1.0, "Overall": 0.7438671875}, {"timecode": 80, "UKR": 0.765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1012", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-1126", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1266", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1399", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1748", "mrqa_hotpotqa-validation-1790", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1892", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2138", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2703", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-289", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3396", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-3581", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3671", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-4128", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4276", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4349", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4410", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4486", "mrqa_hotpotqa-validation-4537", "mrqa_hotpotqa-validation-4704", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-4942", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-556", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-5657", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5820", "mrqa_hotpotqa-validation-5836", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-846", "mrqa_hotpotqa-validation-904", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-133", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2005", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2349", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4597", "mrqa_naturalquestions-validation-475", "mrqa_naturalquestions-validation-4816", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5200", "mrqa_naturalquestions-validation-5230", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-6634", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6951", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7461", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7564", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8928", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9874", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-9979", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1074", "mrqa_newsqa-validation-1088", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-1949", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-2035", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2863", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3014", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-337", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3570", "mrqa_newsqa-validation-381", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-636", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-892", "mrqa_searchqa-validation-10377", "mrqa_searchqa-validation-10784", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-10900", "mrqa_searchqa-validation-11241", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-1154", "mrqa_searchqa-validation-11562", "mrqa_searchqa-validation-11618", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-12446", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12757", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13183", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-1363", "mrqa_searchqa-validation-14131", "mrqa_searchqa-validation-14332", "mrqa_searchqa-validation-14354", "mrqa_searchqa-validation-14679", "mrqa_searchqa-validation-15078", "mrqa_searchqa-validation-15188", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15832", "mrqa_searchqa-validation-15958", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-1686", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-190", "mrqa_searchqa-validation-1977", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2073", "mrqa_searchqa-validation-2273", "mrqa_searchqa-validation-2307", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-2513", "mrqa_searchqa-validation-2560", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-3324", "mrqa_searchqa-validation-3400", "mrqa_searchqa-validation-364", "mrqa_searchqa-validation-3695", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-4409", "mrqa_searchqa-validation-4550", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6784", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7165", "mrqa_searchqa-validation-7168", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-7720", "mrqa_searchqa-validation-8431", "mrqa_searchqa-validation-8518", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-8932", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-959", "mrqa_searchqa-validation-9626", "mrqa_searchqa-validation-9894", "mrqa_searchqa-validation-9916", "mrqa_searchqa-validation-9984", "mrqa_squad-validation-10064", "mrqa_squad-validation-10168", "mrqa_squad-validation-10203", "mrqa_squad-validation-10309", "mrqa_squad-validation-10333", "mrqa_squad-validation-10370", "mrqa_squad-validation-10416", "mrqa_squad-validation-1042", "mrqa_squad-validation-1182", "mrqa_squad-validation-1247", "mrqa_squad-validation-1349", "mrqa_squad-validation-1404", "mrqa_squad-validation-157", "mrqa_squad-validation-1577", "mrqa_squad-validation-1769", "mrqa_squad-validation-1956", "mrqa_squad-validation-1977", "mrqa_squad-validation-2118", "mrqa_squad-validation-2140", "mrqa_squad-validation-225", "mrqa_squad-validation-237", "mrqa_squad-validation-2390", "mrqa_squad-validation-2468", "mrqa_squad-validation-2521", "mrqa_squad-validation-256", "mrqa_squad-validation-2573", "mrqa_squad-validation-258", "mrqa_squad-validation-2599", "mrqa_squad-validation-2628", "mrqa_squad-validation-2668", "mrqa_squad-validation-2684", "mrqa_squad-validation-2719", "mrqa_squad-validation-2778", "mrqa_squad-validation-3063", "mrqa_squad-validation-3138", "mrqa_squad-validation-3141", "mrqa_squad-validation-3165", "mrqa_squad-validation-3204", "mrqa_squad-validation-3460", "mrqa_squad-validation-3522", "mrqa_squad-validation-3554", "mrqa_squad-validation-3599", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3925", "mrqa_squad-validation-3958", "mrqa_squad-validation-4072", "mrqa_squad-validation-4122", "mrqa_squad-validation-4157", "mrqa_squad-validation-4162", "mrqa_squad-validation-4164", "mrqa_squad-validation-4286", "mrqa_squad-validation-4304", "mrqa_squad-validation-4311", "mrqa_squad-validation-4421", "mrqa_squad-validation-4429", "mrqa_squad-validation-4490", "mrqa_squad-validation-4579", "mrqa_squad-validation-4755", "mrqa_squad-validation-4795", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4875", "mrqa_squad-validation-5010", "mrqa_squad-validation-5081", "mrqa_squad-validation-5156", "mrqa_squad-validation-519", "mrqa_squad-validation-5373", "mrqa_squad-validation-5472", "mrqa_squad-validation-5555", "mrqa_squad-validation-5754", "mrqa_squad-validation-5839", "mrqa_squad-validation-5891", "mrqa_squad-validation-6019", "mrqa_squad-validation-6166", "mrqa_squad-validation-6214", "mrqa_squad-validation-6227", "mrqa_squad-validation-6266", "mrqa_squad-validation-6524", "mrqa_squad-validation-6611", "mrqa_squad-validation-6613", "mrqa_squad-validation-6854", "mrqa_squad-validation-692", "mrqa_squad-validation-708", "mrqa_squad-validation-7111", "mrqa_squad-validation-7162", "mrqa_squad-validation-7211", "mrqa_squad-validation-751", "mrqa_squad-validation-7559", "mrqa_squad-validation-7588", "mrqa_squad-validation-7639", "mrqa_squad-validation-7683", "mrqa_squad-validation-7752", "mrqa_squad-validation-7793", "mrqa_squad-validation-7831", "mrqa_squad-validation-7864", "mrqa_squad-validation-7902", "mrqa_squad-validation-7934", "mrqa_squad-validation-8245", "mrqa_squad-validation-8476", "mrqa_squad-validation-8527", "mrqa_squad-validation-855", "mrqa_squad-validation-8664", "mrqa_squad-validation-867", "mrqa_squad-validation-9082", "mrqa_squad-validation-9145", "mrqa_squad-validation-9247", "mrqa_squad-validation-9317", "mrqa_squad-validation-9362", "mrqa_squad-validation-9569", "mrqa_squad-validation-9587", "mrqa_squad-validation-9643", "mrqa_squad-validation-9653", "mrqa_squad-validation-9944", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-1625", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1681", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-1871", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-3081", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-3917", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-424", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4343", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-4718", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5431", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5885", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6516", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7362", "mrqa_triviaqa-validation-7475", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-828", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-977"], "OKR": 0.833984375, "KG": 0.490625, "before_eval_results": {"predictions": ["the wedding banquet", "David Copperfield", "Stalin", "Indiana Jones", "Sweeney Todd", "Nirvana", "Cosmopolitan", "changelings", "right", "Erin Brockovich", "Sisera", "Biggie Smalls", "a loop", "Sherlock Holmes", "Ring", "green", "Hanoi", "Star Trek", "the white cliffs", "giant", "the west", "the Sahara", "Puget Sound", "the Golden Age of Murder", "the Erie Canal", "American", "Tibet", "(J. H. Ingraham)", "one", "the Vicious Circle", "Passover", "Roo", "Austin Powers", "offensive", "the manatee", "a cleanser", "the X-Files", "(VUS.7d)", "West Virginia", "La Salle", "November 24, 1963", "John of England", "China", "a pigeon", "Vyacheslav Molotov", "the Beatles", "(3)", "Jack Roosevelt", "hurt Tammy Wynette", "an unpaired electron", "Zbigniew Brzezinski", "1986", "Catherine Tramell", "2018", "(color) leather", "baulk", "mongoose", "by the United States Food and Drug Administration (FDA)", "John Rockwell", "the \"Cisleithanian\" half of Austria-Hungary", "Mexico", "flavorful foods", "a dozen", "a white robe"], "metric_results": {"EM": 0.5, "QA-F1": 0.5487847222222222}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, false, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, true, true, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.22222222222222224, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-16217", "mrqa_searchqa-validation-14303", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-5808", "mrqa_searchqa-validation-519", "mrqa_searchqa-validation-15471", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-6747", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-2389", "mrqa_searchqa-validation-4805", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8613", "mrqa_searchqa-validation-2689", "mrqa_searchqa-validation-5237", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-5858", "mrqa_searchqa-validation-2796", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-2502", "mrqa_searchqa-validation-12464", "mrqa_searchqa-validation-6188", "mrqa_triviaqa-validation-1379", "mrqa_triviaqa-validation-6859", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-5324", "mrqa_hotpotqa-validation-4669", "mrqa_newsqa-validation-3124", "mrqa_triviaqa-validation-2123"], "SR": 0.5, "CSR": 0.5630787037037037, "EFR": 0.96875, "Overall": 0.7244126157407408}, {"timecode": 81, "before_eval_results": {"predictions": ["Rembrandt", "South Africa", "Germany", "the Red Sea", "a pingpong ball", "Living in America", "a howitzer", "the Teenitans", "Orwell", "New Zealand", "the Blue Meanies", "\"Take It\"", "E", "the \"Fisherman's ring\"", "Iris Murdoch", "Samuel Taylor Coleridge", "(Attab)", "St. Francis of Assisi", "porter", "(Barcella)", "a pilsner malt", "Joe Louis", "The Flying Dutchman", "Kentucky", "Iraq", "Newfoundland", "a tail", "Honolulu", "E", "bAFTA", "the Dolphins", "King Solomon", "A House Divided", "a mausoleum", "Virginia Woolf", "high jump", "a title", "Poseidon", "Lucrezia Borgia", "Urdu", "lamb", "man", "Sulfur", "Advil", "Guy Arcizet", "Schoenberg", "a tumbler", "Joel Osteen", "Ambrose Bierce", "Eris", "Azores", "Bill Hayes", "1936", "Upon closure at birth", "Blackfriars", "Sedbergh", "Renzo Piano", "Sydney", "34 days", "\"Si Da Ming Bu\"", "250,000", "A 22-year-old college student in Boston, Massachusetts,", "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "Hungary"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6178285256410256}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.30769230769230765, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-671", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6223", "mrqa_searchqa-validation-8458", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-10497", "mrqa_searchqa-validation-16502", "mrqa_searchqa-validation-3375", "mrqa_searchqa-validation-13243", "mrqa_searchqa-validation-1809", "mrqa_searchqa-validation-2979", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-3172", "mrqa_searchqa-validation-14657", "mrqa_searchqa-validation-11906", "mrqa_searchqa-validation-4659", "mrqa_searchqa-validation-16622", "mrqa_searchqa-validation-14773", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-5951", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-9799", "mrqa_triviaqa-validation-5511", "mrqa_hotpotqa-validation-5146", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-4004"], "SR": 0.578125, "CSR": 0.5632621951219512, "retrieved_ids": ["mrqa_squad-train-46122", "mrqa_squad-train-46066", "mrqa_squad-train-46046", "mrqa_squad-train-15352", "mrqa_squad-train-47763", "mrqa_squad-train-50538", "mrqa_squad-train-14673", "mrqa_squad-train-53091", "mrqa_squad-train-72659", "mrqa_squad-train-57178", "mrqa_squad-train-82201", "mrqa_squad-train-47632", "mrqa_squad-train-71747", "mrqa_squad-train-54658", "mrqa_squad-train-1548", "mrqa_squad-train-17379", "mrqa_searchqa-validation-11241", "mrqa_squad-validation-9649", "mrqa_hotpotqa-validation-3003", "mrqa_hotpotqa-validation-5873", "mrqa_squad-validation-2886", "mrqa_triviaqa-validation-799", "mrqa_squad-validation-6207", "mrqa_hotpotqa-validation-3314", "mrqa_naturalquestions-validation-3427", "mrqa_searchqa-validation-14508", "mrqa_newsqa-validation-3246", "mrqa_triviaqa-validation-7475", "mrqa_triviaqa-validation-7375", "mrqa_searchqa-validation-959", "mrqa_triviaqa-validation-4892", "mrqa_searchqa-validation-7715"], "EFR": 1.0, "Overall": 0.7306993140243903}, {"timecode": 82, "before_eval_results": {"predictions": ["between June and September", "six", "September 29, 1910", "Liguria", "Spanish", "100 million", "Walcha", "Eleanor of Aquitaine", "DJ Scotch Egg", "122,067", "1971", "Ian Rush", "May 4, 2004", "Jan Kazimierz", "1844", "Theodore Robert Bundy", "381.6 days", "Buffalo", "21 July 2015", "25", "128", "16", "E Street Band", "Hindi", "an organ", "Innviertel region of western Upper Austria", "to steal the plans for the Death Star", "god", "Trilochanapala", "F\u00fchrer", "James Victor Chesnutt", "Tainy Sledstviya", "constant support from propaganda campaigns", "Westchester", "\"The Worm\"", "New York", "Northampton, England", "Suspiria", "Russian film industry", "Laurel, Mississippi", "India Today", "VIMN Russia", "McLaren-Honda", "January 28, 2016", "Debbie Reynolds", "steamy pictorials of celebrities", "Stage Stores", "two", "between the 8th and 16th centuries", "Jerry Michael Glanville (born October 14, 1941) is a former American football player and coach, former NASCAR driver and owner, and sportscaster", "Hindi", "beneath the liver", "Golden Gate Bridge", "The Intolerable Acts", "Dik Browne", "oldpatricktoe-end", "Transformers: Age of Extinction", "Afghan security forces", "Libreville, Gabon.", "planning processes are urgently needed", "profundo", "a loop", "Kentucky", "December 1, 2009"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6537946428571428}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.4, 0.0, 0.5, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1766", "mrqa_hotpotqa-validation-4216", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-726", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-5288", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-5213", "mrqa_hotpotqa-validation-4562", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-5717", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-3429", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-43", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-6333", "mrqa_newsqa-validation-2178", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-4061", "mrqa_naturalquestions-validation-2169"], "SR": 0.5625, "CSR": 0.5632530120481928, "EFR": 1.0, "Overall": 0.7306974774096385}, {"timecode": 83, "before_eval_results": {"predictions": ["Imperial Secretariat", "concentration camps", "1985", "1851", "novelty songs", "Bernard King", "Lowell", "1930s and 1940s", "Captain", "\"Mrs. Eastwood & Company\"", "November 23, 2011", "CBS", "Roland R-8", "Joulupukki", "Clive Staples Lewis", "United Holy Salvation Army and Uganda Christian Army/Movement", "\"Nebo Zovyot\"", "Heineken International", "Barcelona", "100 countries", "Colin Blakely", "2004 Nokia Sugar Bowl", "Jon Hamm", "Gian Carlo Menotti", "1883", "Dougray Scott", "January 15, 1975", "Tim Whelan", "Montreal", "Long Island", "Kentucky Music Hall of Fame", "Oklahoma Sooners men's basketball", "trans-Pacific flight", "The United States House of Representatives", "Anthony Lynn", "Alfred Graf von Schlieffen", "graffiti artists", "Justin Adler", "Empire of Japan", "Lieutenant Colonel Iceal E. \"Gene\" Hambleton", "VfL Wolfsburg", "27 November 1956", "2015", "ESPN", "actor", "May", "Hillary Clinton", "1981", "Oakdale", "Juergen M. Geissinger", "Igor Stravinsky", "Inequality of opportunity", "If These Dolls Could Talk", "to bring", "john steinbeck", "in God we trust", "Kopassus", "\"A Whiter Shade of Pale\"", "Matthew Fisher", "$250,000", "Olivetti", "Frisbee", "a typewriter", "May 29, 2018"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7768229166666667}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.25, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8075", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-1105", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-414", "mrqa_hotpotqa-validation-3880", "mrqa_hotpotqa-validation-572", "mrqa_hotpotqa-validation-3809", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-2929", "mrqa_hotpotqa-validation-4691", "mrqa_hotpotqa-validation-4225", "mrqa_naturalquestions-validation-10512"], "SR": 0.703125, "CSR": 0.5649181547619048, "EFR": 1.0, "Overall": 0.731030505952381}, {"timecode": 84, "before_eval_results": {"predictions": ["Apollo 17", "psychological horror", "Julian Dana William McMahon", "Ericsson (\"Telefonaktiebolaget L. M. Ericsson\")", "Casey Bond", "The S6 and S6 Edge+", "John Ford", "Happy Valley", "June", "Washington Huskie", "Hong Kong", "The Legendary A&M Sessions", "Sir Frank P. Lowy", "Lynwood", "Southbank", "De La Soul", "Guangzhou", "J.R. R. Tolkien", "World War II", "31 October 1783", "Japan", "largest Mission Revival Style building in the United States", "Skipton", "1965", "The Clash of Triton", "Amon Leopold G\u00f6th", "Aqua", "Colonial colleges", "The current holder is Dele Alli", "Linda Maria Ronstadt", "Wilhelmus Simon Petrus Fortuijn", "Steven selling", "Tottenham ( ) or Spurs", "Vilyam \"Willie\" Genrikhovich Fisher", "Juan Francisco Antonio Hilari\u00f3n Zea D\u00edaz", "France", "Anne Elizabeth Alice Louise", "The Bruce Springsteen Band", "King Duncan", "Sophie Monk", "On St. Patrick's Day in 1988", "1985", "40 million", "On March 27, 1977, two Boeing 747 passenger jets, KLM Flight 4805 and Pan Am Flight 1736, collided on the runway at Los Rodeos Airport (now Tenerife North Airport)", "General Motors", "singer", "Miami Gardens", "Marvel", "Jango Fett", "The series, consisting of seven episodes, was created and presented by the comedian and sociologist Harald Eia", "Austral L\u00edneas A\u00e9reas", "November 27, 2017", "The epidermis", "May 18, 2010", "The Passenger Pigeon", "parlophone", "Caviar", "The Casalesi Camorra clan", "Jason Chaffetz", "Amsterdam, in the Netherlands, to Ankara, Turkey", "Anne Rice", "The short answer", "Neptune", "538"], "metric_results": {"EM": 0.515625, "QA-F1": 0.677397082084582}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, true, false, false, false, false, true, true, true, false, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, false, false, false, false, false, false, true, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.8, 0.15384615384615385, 0.3333333333333333, 0.3333333333333333, 1.0, 0.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-4368", "mrqa_hotpotqa-validation-4991", "mrqa_hotpotqa-validation-3620", "mrqa_hotpotqa-validation-1555", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-5341", "mrqa_hotpotqa-validation-1221", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-4866", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-5391", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-2329", "mrqa_hotpotqa-validation-1592", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-5579", "mrqa_naturalquestions-validation-7513", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-5602"], "SR": 0.515625, "CSR": 0.5643382352941176, "retrieved_ids": ["mrqa_squad-train-12027", "mrqa_squad-train-21132", "mrqa_squad-train-61678", "mrqa_squad-train-3090", "mrqa_squad-train-5049", "mrqa_squad-train-85681", "mrqa_squad-train-12034", "mrqa_squad-train-60612", "mrqa_squad-train-80578", "mrqa_squad-train-58448", "mrqa_squad-train-20661", "mrqa_squad-train-11246", "mrqa_squad-train-29692", "mrqa_squad-train-21175", "mrqa_squad-train-41095", "mrqa_squad-train-28973", "mrqa_naturalquestions-validation-9753", "mrqa_squad-validation-7835", "mrqa_triviaqa-validation-2930", "mrqa_newsqa-validation-158", "mrqa_squad-validation-6933", "mrqa_naturalquestions-validation-954", "mrqa_squad-validation-8244", "mrqa_squad-validation-7792", "mrqa_newsqa-validation-2013", "mrqa_hotpotqa-validation-4584", "mrqa_triviaqa-validation-7662", "mrqa_triviaqa-validation-2408", "mrqa_searchqa-validation-16699", "mrqa_triviaqa-validation-4408", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-9582"], "EFR": 0.9032258064516129, "Overall": 0.7115596833491462}, {"timecode": 85, "before_eval_results": {"predictions": ["Solovay-Strassen tests", "liberalisation", "Warren Harding", "Roger Chaffee", "Larry Bird", "Stan", "\"Drogon is Coming\"", "a priest or religious leader", "Bob", "Spin", "La Gioconda", "a candy store", "Salam Es Selamu", "The Devil Wears Prada", "heresy", "Cleveland", "Chile", "Iridill", "Black pudding", "Newton", "Union Square", "reflection", "The Sleeping Beauty", "California", "Janice Rees-Jones", "1066", "the stick", "WD-40 L lubricant", "Bonobos", "the sound barrier", "Hillary Clinton", "Adam", "a metronome", "Pupils", "The Firebird", "Elizabeth Taylor", "the leader of a pack of orphan pickpockets", "3", "Guantnamo Bay Naval Station", "is the 44th President of the United States", "Yond Cassius", "Turkish", "Buried Child", "the liver", "Anja Prson", "John Paul Jones", "chalkboard", "a poncho", "\"Pig Crap\"", "Billy Budd", "Utah", "Richie Cunningham", "April 1917", "1799", "polecat", "beards", "fractal geometry", "Tariq Khan", "five times", "The Grandmaster", "The National Infrastructure Program,", "he and Armento, 51, were drinking at a strip club when they decided to go hunt for valium.", "Employee Free Choice act", "1960"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5609375}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9025", "mrqa_searchqa-validation-14669", "mrqa_searchqa-validation-5299", "mrqa_searchqa-validation-11783", "mrqa_searchqa-validation-15600", "mrqa_searchqa-validation-5370", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-9850", "mrqa_searchqa-validation-13659", "mrqa_searchqa-validation-1215", "mrqa_searchqa-validation-9247", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-6908", "mrqa_searchqa-validation-589", "mrqa_searchqa-validation-1507", "mrqa_searchqa-validation-3351", "mrqa_searchqa-validation-854", "mrqa_searchqa-validation-10218", "mrqa_searchqa-validation-11956", "mrqa_searchqa-validation-3371", "mrqa_searchqa-validation-12376", "mrqa_searchqa-validation-2715", "mrqa_searchqa-validation-6776", "mrqa_searchqa-validation-14066", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-5830", "mrqa_triviaqa-validation-899", "mrqa_hotpotqa-validation-5675", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-2839"], "SR": 0.53125, "CSR": 0.563953488372093, "EFR": 1.0, "Overall": 0.7308375726744186}, {"timecode": 86, "before_eval_results": {"predictions": ["over 200 awards", "aphids", "the Cherokee Nation", "Danny Elfman", "White space", "Vladivostok", "Brian Cowen", "is a spiritual being superior to humans in power and intelligence", "Linus", "Seurat", "Pompeii", "Jason", "a tuba", "megaton", "Michigan", "webbing", "September", "Madrid", "Roger Williams", "Washington", "the retina", "statistic", "dance", "\"The Life Times of John Barrymore\"", "The region of Bessarabia", "Andrew Johnson", "the age of liberation", "the black hole", "Uranus XIII", "gluttony", "a catalog", "From each", "German", "words", "Baghdad", "Merlin", "t.S. Eliot", "Oomf", "the Bells", "Dr. George Washington Carver", "Tommy Franks", "dreams", "Tigger", "Songs of Innocence", "Sarai", "Naomi Judd", "The David Bowie Song Lyric Cut-Up Game", "Saudi Arabia", "dustbin lids", "Kufic", "focal length", "mitochondria", "Lady Gaga", "Mars Hill", "Portugal", "The Opera 101", "comic cuts", "Harry Booth", "Vilnius Old Town", "over 20,950 staff", "more than 700", "Zimbabwe,", "someone with a compatible organ", "Orson Welles"], "metric_results": {"EM": 0.5, "QA-F1": 0.5790178571428571}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true], "QA-F1": [0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8000", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-838", "mrqa_searchqa-validation-1912", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-12321", "mrqa_searchqa-validation-2671", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-16202", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-3026", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-12862", "mrqa_searchqa-validation-1985", "mrqa_searchqa-validation-15649", "mrqa_searchqa-validation-9217", "mrqa_searchqa-validation-11033", "mrqa_searchqa-validation-8437", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-13025", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-15646", "mrqa_searchqa-validation-8327", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14277", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-2538", "mrqa_triviaqa-validation-765", "mrqa_hotpotqa-validation-2895", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-1276"], "SR": 0.5, "CSR": 0.5632183908045977, "EFR": 0.96875, "Overall": 0.7244405531609195}, {"timecode": 87, "before_eval_results": {"predictions": ["fear of transferring power to the ethnic Chinese", "peat moss", "linguistic analysis", "Oliver Parker", "Walt Disney", "French mathematician and physicist", "India", "novelty songs", "Barbados", "The Captain Matchbox Whoopee Band", "Guillermo del Toro", "Herb Brooks", "antisemitic", "Commissioner", "itty Hawk", "Scandinavian design", "Linda Ronstadt", "\"Naked Killer\"", "Techwood Drive", "Imperial War Museums", "\"\".", "Olympic Team", "Scottish national team", "William Bradford", "200,167", "Boston, Providence, Hartford", "Rose Theatre", "Columbine", "Gangsta's Paradise", "Sir William Collins", "G\u00e9rard Depardieu", "Lionsgate", "the 13th century", "his son's death", "Dickins", "Louisiana Tech University", "Clark County, Nevada", "around 8000 BC", "Rochdale", "Andrew Lloyd Webber", "12", "1955", "8", "Henry Lau", "American Wrestler", "\"Teenage Dream\"", "2015", "Federal Bureau of Prisons", "Mick Jackson", "Mayor Ed Lee", "two", "Matt Monro", "on the lateral side", "Jason Flemyng", "Elizabeth I", "demolition", "(Michael Gambon)", "homicide", "intention to set up headquarters in Dublin.", "the release of the four men", "Bolivia", "NOTORIOUS", "Spain", "vomiting"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6285657051282051}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, false, true, true, false, false, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, false, true, true], "QA-F1": [0.0, 0.3076923076923077, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.8, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8571428571428571, 0.0, 0.8, 0.8, 1.0, 1.0, 0.6666666666666666, 0.5, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615385, 0.9333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8413", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-4226", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-4727", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-4821", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4736", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-2344", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-5438", "mrqa_naturalquestions-validation-9218", "mrqa_triviaqa-validation-459", "mrqa_triviaqa-validation-4077", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-3806", "mrqa_searchqa-validation-2770"], "SR": 0.453125, "CSR": 0.5619673295454546, "retrieved_ids": ["mrqa_squad-train-49348", "mrqa_squad-train-5665", "mrqa_squad-train-62678", "mrqa_squad-train-11448", "mrqa_squad-train-12072", "mrqa_squad-train-47902", "mrqa_squad-train-52356", "mrqa_squad-train-80198", "mrqa_squad-train-11727", "mrqa_squad-train-37036", "mrqa_squad-train-28033", "mrqa_squad-train-27595", "mrqa_squad-train-33499", "mrqa_squad-train-62162", "mrqa_squad-train-68874", "mrqa_squad-train-27603", "mrqa_squad-validation-9247", "mrqa_newsqa-validation-1301", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-8275", "mrqa_naturalquestions-validation-5997", "mrqa_searchqa-validation-4966", "mrqa_naturalquestions-validation-6091", "mrqa_hotpotqa-validation-1765", "mrqa_searchqa-validation-1993", "mrqa_triviaqa-validation-3839", "mrqa_hotpotqa-validation-5288", "mrqa_squad-validation-89", "mrqa_hotpotqa-validation-5440", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-7280", "mrqa_naturalquestions-validation-9999"], "EFR": 1.0, "Overall": 0.7304403409090909}, {"timecode": 88, "before_eval_results": {"predictions": ["Scottish rivers", "Heracles", "New Kids on the Block", "Van Allen", "Camilla", "Sybil", "a scorpion", "a bill", "souvlaki", "Charles Dana Gibson", "Philip Seymour Hoffman", "ACTIVE", "chicken pox", "Japan", "chickens", "St. Patrick", "Lebanon", "George Sand", "Over the hifls", "The Atchison Family YMCA", "the Holy Grail", "shalom", "the Cumberland Gap", "Belgium", "Hollaback", "Michigan", "Poor Richard's Almanack", "Red", "Al Jolson", "a violin", "Hestia", "Transformers", "glory", "Rand McNally", "Scrabble", "Martin Luther King III", "Duct tape", "Henry Cavendish", "confer", "the gap", "Bill & George Clinton", "Condoleezza Rice", "John Glenn", "the bluest eye", "Spinal Tap", "Janet Reno", "(Casey) Stengel", "Monticello", "Eric Clapton", "a saguaro", "Louisiana", "Beijing", "Charlotte Hornets", "supreme courts", "Sweden", "Lucas McCain", "Christchurch", "hunt", "1449", "No. 17", "billboards with an image of the burning World Trade Center", "At least 14 bodies", "Fullerton, California", "1936"], "metric_results": {"EM": 0.765625, "QA-F1": 0.834671160130719}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9411764705882353, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-862", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-3953", "mrqa_searchqa-validation-5433", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-11022", "mrqa_searchqa-validation-13166", "mrqa_searchqa-validation-6632", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-10409", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-10380", "mrqa_hotpotqa-validation-405", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-798"], "SR": 0.765625, "CSR": 0.5642556179775281, "EFR": 0.9333333333333333, "Overall": 0.7175646652621723}, {"timecode": 89, "before_eval_results": {"predictions": ["green light", "H. LaGuardia High School of Music & Art and Performing Arts", "bresaola", "I Remember It Well", "the Yellowfin", "St. John", "bamboo", "the Stratosphere", "Harry Potter and the Goblet of Fire", "The Carolinas Gardener's Guide", "guru", "Mercury and Venus", "Hallmark Cards", "Confirmation", "Peter Pan", "magnesium", "the Space Shuttle Columbia", "chicken", "The Romanov dynasty", "Jeannette Rankin", "(Stephen) Escoffier", "(George) Orwell", "Billie", "the displacement", "George Harrison", "the foot", "Superbad", "Emily Post", "a drum", "Morris West", "New Zealand", "the Ural Mountains", "Sacco and Vanzetti", "The Church of Jesus Christ of Latter-day Saints", "Copenhagen", "Tennessee", "the (AS) list of all 197 countries of the world", "a spare tire", "President Obama", "(Henry) Ford", "chicken breasts", "language", "Hialeah", "Necessity", "a car which covers the engine", "the Oakland Raiders", "an owl", "Cicero", "cipher", "Elvis Presley", "William E. Grady High School", "the study of the interstellar medium ( ISM )", "Thomas Edison", "Terrell Suggs", "Benjamin Franklin", "(James) Bolam", "fluorine", "held and expressed an irreligious world view which was met with controversy, but in spite of it, he is regarded as one of the greatest classical Arabic poets.", "University of California", "11 November 1918", "the Ku Klux Klan", "Dr. Maria Siemionow, the head of plastic surgery research at the Cleveland, Ohio, hospital,", "Buckhorn Mountain,", "Father of Liberalism"], "metric_results": {"EM": 0.5, "QA-F1": 0.5849317528735631}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.20000000000000004, 1.0, 0.0, 1.0, 1.0, 1.0, 0.06896551724137931, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8897", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-1068", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-12675", "mrqa_searchqa-validation-1036", "mrqa_searchqa-validation-15046", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-13001", "mrqa_searchqa-validation-10002", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-4115", "mrqa_searchqa-validation-8542", "mrqa_searchqa-validation-16904", "mrqa_searchqa-validation-12099", "mrqa_searchqa-validation-13523", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-8714", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-8388", "mrqa_searchqa-validation-16902", "mrqa_searchqa-validation-12427", "mrqa_searchqa-validation-9117", "mrqa_searchqa-validation-6407", "mrqa_searchqa-validation-14302", "mrqa_searchqa-validation-16659", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-3474", "mrqa_hotpotqa-validation-5637", "mrqa_newsqa-validation-1679", "mrqa_newsqa-validation-3677"], "SR": 0.5, "CSR": 0.5635416666666666, "EFR": 1.0, "Overall": 0.7307552083333333}, {"timecode": 90, "UKR": 0.765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1012", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-1126", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1266", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1748", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1768", "mrqa_hotpotqa-validation-1790", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1892", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2138", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2703", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-2895", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2929", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3024", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3396", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-3581", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3660", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3671", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4128", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4276", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4368", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4410", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4486", "mrqa_hotpotqa-validation-4562", "mrqa_hotpotqa-validation-4704", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-4942", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5391", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-556", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-5657", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-5717", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-5820", "mrqa_hotpotqa-validation-5836", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-846", "mrqa_hotpotqa-validation-904", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-133", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4597", "mrqa_naturalquestions-validation-475", "mrqa_naturalquestions-validation-4816", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5200", "mrqa_naturalquestions-validation-5230", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6951", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8928", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9874", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-1074", "mrqa_newsqa-validation-1088", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-1949", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-2035", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2863", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3014", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-337", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3570", "mrqa_newsqa-validation-381", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-892", "mrqa_searchqa-validation-10784", "mrqa_searchqa-validation-1084", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-1154", "mrqa_searchqa-validation-11562", "mrqa_searchqa-validation-11600", "mrqa_searchqa-validation-11618", "mrqa_searchqa-validation-11715", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-12321", "mrqa_searchqa-validation-12446", "mrqa_searchqa-validation-12479", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12757", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13183", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-1363", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-14082", "mrqa_searchqa-validation-14332", "mrqa_searchqa-validation-14354", "mrqa_searchqa-validation-14657", "mrqa_searchqa-validation-14669", "mrqa_searchqa-validation-14679", "mrqa_searchqa-validation-14773", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15078", "mrqa_searchqa-validation-15188", "mrqa_searchqa-validation-15220", "mrqa_searchqa-validation-15583", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15832", "mrqa_searchqa-validation-15958", "mrqa_searchqa-validation-162", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-1686", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-1809", "mrqa_searchqa-validation-190", "mrqa_searchqa-validation-1977", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2073", "mrqa_searchqa-validation-2307", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-2513", "mrqa_searchqa-validation-2560", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-2671", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-3375", "mrqa_searchqa-validation-3400", "mrqa_searchqa-validation-3695", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-4115", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-4409", "mrqa_searchqa-validation-4550", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6632", "mrqa_searchqa-validation-6784", "mrqa_searchqa-validation-6908", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7168", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8431", "mrqa_searchqa-validation-8518", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-8830", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-8932", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-9562", "mrqa_searchqa-validation-959", "mrqa_searchqa-validation-9599", "mrqa_searchqa-validation-9894", "mrqa_searchqa-validation-9916", "mrqa_searchqa-validation-9984", "mrqa_squad-validation-10064", "mrqa_squad-validation-10168", "mrqa_squad-validation-10203", "mrqa_squad-validation-10309", "mrqa_squad-validation-10333", "mrqa_squad-validation-10370", "mrqa_squad-validation-10416", "mrqa_squad-validation-1042", "mrqa_squad-validation-1182", "mrqa_squad-validation-1247", "mrqa_squad-validation-1349", "mrqa_squad-validation-1404", "mrqa_squad-validation-157", "mrqa_squad-validation-1577", "mrqa_squad-validation-1956", "mrqa_squad-validation-1977", "mrqa_squad-validation-2118", "mrqa_squad-validation-2140", "mrqa_squad-validation-225", "mrqa_squad-validation-2390", "mrqa_squad-validation-2468", "mrqa_squad-validation-2521", "mrqa_squad-validation-256", "mrqa_squad-validation-2573", "mrqa_squad-validation-258", "mrqa_squad-validation-2599", "mrqa_squad-validation-2628", "mrqa_squad-validation-2668", "mrqa_squad-validation-2684", "mrqa_squad-validation-2719", "mrqa_squad-validation-2778", "mrqa_squad-validation-3063", "mrqa_squad-validation-3138", "mrqa_squad-validation-3141", "mrqa_squad-validation-3165", "mrqa_squad-validation-3204", "mrqa_squad-validation-3460", "mrqa_squad-validation-3522", "mrqa_squad-validation-3599", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3898", "mrqa_squad-validation-3925", "mrqa_squad-validation-3958", "mrqa_squad-validation-4072", "mrqa_squad-validation-4122", "mrqa_squad-validation-4157", "mrqa_squad-validation-4162", "mrqa_squad-validation-4164", "mrqa_squad-validation-4304", "mrqa_squad-validation-4311", "mrqa_squad-validation-4421", "mrqa_squad-validation-4429", "mrqa_squad-validation-4490", "mrqa_squad-validation-4579", "mrqa_squad-validation-4795", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4875", "mrqa_squad-validation-5010", "mrqa_squad-validation-5081", "mrqa_squad-validation-5156", "mrqa_squad-validation-519", "mrqa_squad-validation-5373", "mrqa_squad-validation-5472", "mrqa_squad-validation-5520", "mrqa_squad-validation-5555", "mrqa_squad-validation-5754", "mrqa_squad-validation-5839", "mrqa_squad-validation-5891", "mrqa_squad-validation-6019", "mrqa_squad-validation-6166", "mrqa_squad-validation-6214", "mrqa_squad-validation-6227", "mrqa_squad-validation-6524", "mrqa_squad-validation-6611", "mrqa_squad-validation-6613", "mrqa_squad-validation-6854", "mrqa_squad-validation-692", "mrqa_squad-validation-708", "mrqa_squad-validation-7111", "mrqa_squad-validation-7162", "mrqa_squad-validation-7211", "mrqa_squad-validation-751", "mrqa_squad-validation-7559", "mrqa_squad-validation-7588", "mrqa_squad-validation-7639", "mrqa_squad-validation-7683", "mrqa_squad-validation-7752", "mrqa_squad-validation-7831", "mrqa_squad-validation-7864", "mrqa_squad-validation-7902", "mrqa_squad-validation-7934", "mrqa_squad-validation-8245", "mrqa_squad-validation-8476", "mrqa_squad-validation-8527", "mrqa_squad-validation-855", "mrqa_squad-validation-867", "mrqa_squad-validation-9082", "mrqa_squad-validation-9145", "mrqa_squad-validation-9317", "mrqa_squad-validation-9362", "mrqa_squad-validation-9569", "mrqa_squad-validation-9587", "mrqa_squad-validation-9643", "mrqa_squad-validation-9653", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-2537", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-3081", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-424", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4343", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4718", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4888", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5885", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6516", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7362", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-977"], "OKR": 0.87109375, "KG": 0.51484375, "before_eval_results": {"predictions": ["nonviolent", "propeller", "Joseph", "Sideways", "Clothes", "Thomas Wolfe", "Ford", "Cinderella", "Missouri", "pyramids", "Subway", "Tila Tequila", "protons", "veins", "Bloomingdale\\'s", "Richard Nixon", "Marie Osmond", "bumblebee", "loyal", "French", "canter", "Barney Miller", "the Constitution", "the bullseye", "Thomas Wolfe", "Lynette \"Squeaky\" Fromme", "the fourth Thursday", "robe", "Matthew", "Fenway Park", "Pennsylvania", "the Liberty Bell", "Cardinal Richelieu", "Guinevere", "James Jeffords", "The Wachowski brothers", "a clef", "Brazil", "Montgomery", "Sindbad", "dictate", "John Brown", "a piranha", "26 miles", "the pupil", "Amish", "Anthony Michael Hall", "Lord Baden-Powell", "judo", "Don Knotts", "messenger", "Spencer Treat Clark", "beloved", "northwest Washington", "Charlie Harper", "Wayne\\'s World", "Sweden", "WB Television Network", "Senior Service", "Eric Edward Whitacre", "martial arts", "Pope Benedict XVI refused Wednesday to soften the Vatican's ban on condom use", "Eleven", "Jackson Storm"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7494791666666667}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-92", "mrqa_searchqa-validation-16965", "mrqa_searchqa-validation-5716", "mrqa_searchqa-validation-8002", "mrqa_searchqa-validation-5462", "mrqa_searchqa-validation-10414", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-8459", "mrqa_searchqa-validation-6649", "mrqa_searchqa-validation-1004", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-13537", "mrqa_searchqa-validation-5754", "mrqa_searchqa-validation-11600", "mrqa_naturalquestions-validation-10610", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3947", "mrqa_hotpotqa-validation-565", "mrqa_hotpotqa-validation-5559", "mrqa_newsqa-validation-1663"], "SR": 0.6875, "CSR": 0.5649038461538461, "retrieved_ids": ["mrqa_squad-train-43018", "mrqa_squad-train-13195", "mrqa_squad-train-23054", "mrqa_squad-train-74433", "mrqa_squad-train-63983", "mrqa_squad-train-63240", "mrqa_squad-train-20812", "mrqa_squad-train-23925", "mrqa_squad-train-73378", "mrqa_squad-train-12090", "mrqa_squad-train-43490", "mrqa_squad-train-58107", "mrqa_squad-train-36556", "mrqa_squad-train-57545", "mrqa_squad-train-5024", "mrqa_squad-train-82912", "mrqa_triviaqa-validation-892", "mrqa_hotpotqa-validation-499", "mrqa_squad-validation-3063", "mrqa_newsqa-validation-1449", "mrqa_triviaqa-validation-5598", "mrqa_searchqa-validation-16502", "mrqa_naturalquestions-validation-9675", "mrqa_newsqa-validation-409", "mrqa_searchqa-validation-11217", "mrqa_searchqa-validation-4623", "mrqa_naturalquestions-validation-7948", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-6900", "mrqa_squad-validation-5473", "mrqa_hotpotqa-validation-745"], "EFR": 1.0, "Overall": 0.7432932692307693}, {"timecode": 91, "before_eval_results": {"predictions": ["1330 Avenue of the Americas", "25", "G gossip Girl", "a pool of blood beneath his head.", "nearly $162 billion", "elections", "military trials for some Guantanamo Bay detainees.", "Missouri", "A Colorado prosecutor", "Camp Lejeune, North Carolina", "Authorities in Fayetteville, North Carolina,", "hundreds", "Isabella, Emma, Olivia, Sophia, Ava, Emily, Madison, Abigail, Chloe and Mia.", "Illlinois.", "Blacks and Hispanics", "al-Moayad", "Wednesday night", "33-year-old", "3 to 17", "\"peregruzka\"", "Australia", "two", "smile softly at the lifeless bodies they hold.", "the equator,", "police", "2004.", "Inter Milan", "the Airbus A330-200", "serious problems with the way children with disabilities are being treated in public schools, including cases of children being held face-down on the ground.", "love the trip route, which winds through the Rockies and climb to 9,000 feet.", "The Washington Post", "hanged in 1979", "that a U.S. helicopter crashed in northeastern Baghdad as a result of clashes betweenU.S.-backed Iraqi forces and gunmen.", "her boyfriend", "Argentine", "poems telling of the pain and suffering of children", "three empty vodka bottles,", "\"The Lost Symbol\"", "gas prices go back up.", "intricate Flemish tapestries", "Pakistan", "additional information", "federal officers' bodies", "WBO welterweight title", "a violent government crackdown seeped out.", "The Tinkler", "a new model is simply out of their reach.", "costs $50 less,", "part", "some dental work done, including removal of his diamond-studded braces.", "wings", "By functions", "one season", "Lady Gaga", "the Quran", "Enid Blyton", "Chile", "Club Deportivo Castell\u00f3n", "Bishop's Stortford", "2000", "New Orleans", "(Katharine) Hepburn", "Lady Sings the Blues", "St Moritz"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6256384587634587}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, true, true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false], "QA-F1": [0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.10256410256410256, 0.0, 1.0, 0.0, 0.888888888888889, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8799999999999999, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-5972", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-3946", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-3073", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-4044", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-223", "mrqa_naturalquestions-validation-6706", "mrqa_searchqa-validation-10962", "mrqa_triviaqa-validation-1028"], "SR": 0.53125, "CSR": 0.5645380434782609, "EFR": 0.8666666666666667, "Overall": 0.7165534420289855}, {"timecode": 92, "before_eval_results": {"predictions": ["the ability to be very influential in the young students life", "Saturn", "a nuclear weapon", "150", "we seek a new way forward, based on mutual interest and mutual respect.", "Tim Clark, Matt Kuchar and Bubba Watson", "\"She was focused so much on learning that she didn't notice,\"", "\"security issues\" and not their faith,", "outlet mall", "Somalia's piracy problem was fueled by environmental and political events", "cars", "skyscrapers", "Kurdish militant group", "Two pages -- usually high school juniors who serve Congress as messengers", "in October of last year, another possible paramilitary group surfaced in Michoacan state.", "Sunday", "Intensifying violence, food shortages and widespread drought", "put a lid on the marking of Ashura", "the Bronx", "Congress", "sculptures", "eight", "Kerstin and two of her brothers,", "Zimbabwe", "in a battle over the rebels' remaining territory in northern Sri Lanka's Vanni region.", "Stratfor,", "potential revenues from oil and gas", "the couple's surrogate", "The woman involved -- Mandi Hamlin", "\"The people kill him with the blocks,", "her mother", "in his 60s, is incarcerated at the Supermax prison in Florence, Colorado,", "will reduce the clean-up time by years,", "Haiti", "four months", "Jennifer Aniston, Marta Kauffman, co-creator of the series \"Friends\" and Kristin Hahn, who was the executive producer of \"The Departed.\"", "it has not intercepted any", "acid", "Utah Valley Regional Medical Center,", "\"We don't believe in standing silent when that happens,\"", "five victims by helicopter, one who died, two in critical condition and two in serious condition.", "22", "38", "India in Mumbai", "iCloud service", "Bob Bogle,", "Jason Chaffetz", "eight", "Blagojevich", "Sunday.", "101", "charbagh", "boxing", "Florida black wolf or Mississippi Valley wolf", "poterson", "the Last Post", "Chillicothe", "Indian", "Hugh Michael Horace Dancy", "Operation Julin", "Kevin Sorbo", "Weeds", "a neuron", "Carthaginians'Phoenician ancestry"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5341511786824287}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, false], "QA-F1": [0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.3076923076923077, 0.0, 0.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.13333333333333333, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 1.0, 0.06666666666666667, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.13333333333333336]}}, "before_error_ids": ["mrqa_squad-validation-2180", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-3928", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-435", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-1274", "mrqa_naturalquestions-validation-9459", "mrqa_naturalquestions-validation-7133", "mrqa_triviaqa-validation-2429", "mrqa_triviaqa-validation-6947", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-5647", "mrqa_searchqa-validation-10289", "mrqa_naturalquestions-validation-2042"], "SR": 0.421875, "CSR": 0.5630040322580645, "EFR": 0.9459459459459459, "Overall": 0.7321024956408021}, {"timecode": 93, "before_eval_results": {"predictions": ["Uighurs", "World Trade Center", "Olivia Olson", "cytosine ( C )", "Wales", "Michael Schumacher", "Sajjad Delafrooz", "one season", "Malloy as Pierre, Phillipa Soo as Natasha, Lucas Steele as Anatole, Amber Gray as H\u00e9l\u00e8ne, Brittain Ashford as Dolokhov, Shaina Taub as Mary", "Thespis", "European Central Bank ( ECB )", "Keith Timberwolvesodeaux", "2014 Winter Olympics in Sochi, Russia", "Arunachal Pradesh", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "the inner core", "Tatsumi", "Gaget, Gauthier & Co.", "sixth season", "During World War II in Berlin", "Queenstown ( now Cobh ) in Ireland", "Sheev Palpatine", "April 2001", "79", "the ulnar collateral ligament of elbow joint", "the medulla oblongata", "Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "if the player busts, the player loses, regardless of whether the dealer subsequently busts", "Hellenic polytheist", "three", "Spanish explorers", "statutory law", "Menorca", "over a 20 - year period", "Lori McKenna", "infection, irritation, or allergies", "between the Eastern Ghats and the Bay of Bengal", "Mary Elizabeth ( Margaret Hoard )", "Captaincy General of Guatemala", "International Orange", "in the duodenum", "October 6, 2017", "upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "the five states which the UN Charter of 1945 grants a permanent seat on the UN Security Council ( UNSC )", "Pebe Sebert and Hugh Moffatt", "Arthur Chung", "the right ventricle", "Lana Del Rey", "Sergeant - Major James Hewson", "November 27, 2013", "Charlton Heston", "faggot", "Parchman Farm", "Spongebob", "1997", "848 km", "2004", "racially-tinged remark", "unemployed or underemployed,", "positive signal", "lizard", "Gone With the Wind", "Chaplin", "Papua"], "metric_results": {"EM": 0.5, "QA-F1": 0.6286250034687535}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, true, false, true, true, true, false, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16, 1.0, 0.0, 0.5, 0.2857142857142857, 0.0, 0.4615384615384615, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.4444444444444445, 0.0, 1.0, 0.25, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9142857142857143, 0.13333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-1886", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-10469", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-7253", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-7233", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-7457", "mrqa_hotpotqa-validation-455", "mrqa_hotpotqa-validation-1448", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-3446", "mrqa_searchqa-validation-8023", "mrqa_searchqa-validation-16519", "mrqa_searchqa-validation-6187"], "SR": 0.5, "CSR": 0.5623337765957447, "retrieved_ids": ["mrqa_squad-train-468", "mrqa_squad-train-17198", "mrqa_squad-train-84327", "mrqa_squad-train-4979", "mrqa_squad-train-79766", "mrqa_squad-train-49726", "mrqa_squad-train-76992", "mrqa_squad-train-78712", "mrqa_squad-train-63002", "mrqa_squad-train-70491", "mrqa_squad-train-11101", "mrqa_squad-train-27007", "mrqa_squad-train-11270", "mrqa_squad-train-39374", "mrqa_squad-train-49594", "mrqa_squad-train-5619", "mrqa_naturalquestions-validation-2326", "mrqa_triviaqa-validation-3203", "mrqa_searchqa-validation-14773", "mrqa_squad-validation-8075", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-4727", "mrqa_naturalquestions-validation-1223", "mrqa_hotpotqa-validation-3620", "mrqa_triviaqa-validation-2142", "mrqa_searchqa-validation-4513", "mrqa_squad-validation-7778", "mrqa_triviaqa-validation-1818", "mrqa_naturalquestions-validation-9753", "mrqa_hotpotqa-validation-565", "mrqa_naturalquestions-validation-6052", "mrqa_hotpotqa-validation-1059"], "EFR": 0.9375, "Overall": 0.730279255319149}, {"timecode": 94, "before_eval_results": {"predictions": ["8", "Mickey Gilley", "1952", "Long Island", "The Keeping Hours", "John \"John\" Alexander Florence", "Queensland", "1995 to 2012", "Los Angeles", "1909 Cuban-American Major League Clubs Series", "1.23 million", "The interview", "Pan Am Railways", "102,984", "Lancashire", "321,520", "Mary Harron", "Newcastle upon Tyne, England", "Sunyani", "a card (or cards) during a card game", "University of Vienna", "1998", "capitol building", "Ericsson", "main east-west road", "William Bradford", "Free and Sovereign State of Tamaulipas", "highland regions of Scotland", "2015", "Richard Arthur", "Elizabeth River", "Sheen Michaels Entertainment", "Nikita Sergeyevich Khrushchev", "Charlyn Marie \" Chan\" Marshall", "Europe", "Pound Puppies and the Legend of Big Paw", "2016 U.S. Senate election", "War Is the Answer", "drawings", "Philip K. Dick", "Northern Ireland", "the junction with Interstate 95", "Cambridge University", "Mineola", "Ministry of European Integration", "\"Fatman\"", "The R-8 Human Rhythm Composer", "London", "North Carolina", "Steve Carell", "The Entity", "Have I Told You Lately", "when an individual noticing that the person in the photograph is attractive, well groomed, and properly attired", "1,350", "arm\u0101ta", "confidentiality", "cartoons", "North Korea", "The Devil Went Down to Georgia.", "1960s song \"A Whiter Shade of Pale\"", "a macadamia", "Travertine", "Final Cut Pro", "dragon"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6816592261904761}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.2857142857142857, 0.4, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5833333333333334, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-2132", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-4743", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-4259", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-5177", "mrqa_hotpotqa-validation-5791", "mrqa_naturalquestions-validation-4740", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-871", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-2151", "mrqa_searchqa-validation-15161", "mrqa_searchqa-validation-5324", "mrqa_triviaqa-validation-6256"], "SR": 0.578125, "CSR": 0.5625, "EFR": 0.9259259259259259, "Overall": 0.7279976851851853}, {"timecode": 95, "before_eval_results": {"predictions": ["YouTube", "soccer", "Rockbridge County", "Rawhide", "John Sullivan", "John Monash", "Ice Princess", "Antonio Lucio Vivaldi", "Terry Malloy", "Alfred Preis", "professional wrestler, actor, and hip hop musician", "2009", "Twitch Interactive, a subsidiary of Amazon.com", "(500) Days of Summer", "Accokeek, Maryland", "Romas Kalanta", "Brian A. Miller", "Love", "Lewis lamp", "The Daily Stormer", "Taipei", "Adam Levine", "the Merrimack people", "Michael Patrick Smith, (born 19 January 1942), known as Michael Crawford,", "\"The Five\"", "The Lykan", "Bundesliga", "Pease Air National Guard Base", "Honolulu", "from 1993 to 1996", "Hannaford", "John Lennon", "The BFG", "Sophie Winkleman", "In 2017, Pachulia (Georgian) won his first NBA Championship", "1954", "Tie Domi", "Harold Lipshitz, March 20, 1931", "American", "Target Corporation", "George A. Romero", "Nassau County Executive", "number five", "British racing driver", "26 September 1961", "Stapleton Cotton", "Ny-\u00c5lesund", "1983", "Linux Format", "French Canadians (also referred to as Franco-Canadians or Canadiens; French: \"Canadien(ne)s fran\u00e7ais(es)\"", "Kew", "twice", "Meghalaya", "December 15, 2017", "Il Trovatore", "Love Is All Around", "There were twelve London newspapers and 24 provincial papers", "12-1", "to try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "The police car sits outside the Westroads Mall in Omaha, Nebraska,", "parallel", "apples", "Jurassic", "Greenland"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6935155122655123}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 0.42857142857142855, 1.0, 0.6666666666666666, 0.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-1128", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-119", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-1609", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-4603", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-111", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-179", "mrqa_hotpotqa-validation-1886", "mrqa_triviaqa-validation-3039", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-982", "mrqa_searchqa-validation-4407", "mrqa_searchqa-validation-8536"], "SR": 0.609375, "CSR": 0.56298828125, "EFR": 1.0, "Overall": 0.74291015625}, {"timecode": 96, "before_eval_results": {"predictions": ["The terrestrial biosphere", "won", "Thomas Jefferson", "Adam", "Jesus'birth", "Paul Rudd", "John F. Kennedy", "the end", "S - shaped", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "the North Cascades range of, Washington", "Garfield Sobers", "stability, security, and predictability of British law and government", "Hal Derwin", "Madison, Wisconsin, United States", "Ireland", "at the hour of death or in the presence of the dying", "a sweet alcoholic drink made with rum, fruit juice, and syrup or grenadine", "13 February", "comprehend and formulate language", "T.J. Miller", "Charles Crozat Converse", "Kiss", "China ( formerly the Republic of China ), Russia (formerly the Soviet Union ), France, the United Kingdom, and the United States", "commercial at", "Pakistan", "Danny Veltri", "Jimmy Matthews", "Norway", "Per Gessle", "Amitabh Bachchan", "Twin Pines / Lone Pine Mall", "Del and Rodney", "Speaker of the House of Representatives", "538", "Robin", "March 2016", "1924", "Clarence Williams", "Siddharth Arora / Vibhav Roy", "Sreejita De", "4 January 2011", "U2", "the 1967 film Cool Hand Luke", "an optional message body", "An empty line", "Kida", "2005", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "1961 during the Cold War", "The person who has existence in two parallel worlds", "\u201cMy Favorite Martian,\u201d", "Perseus", "piscina", "Justin Adler", "Adelaide", "G\u00f6tene in Sweden", "Ronald Cummings,", "Egypt.", "Stuart Gaffney,", "an an abbreviation", "a brandy", "Arthur Miller", "Renzo Piano"], "metric_results": {"EM": 0.625, "QA-F1": 0.7104683761961722}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5263157894736842, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7368421052631579, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.12500000000000003, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.1818181818181818, 0.0, 0.5, 1.0, 1.0, 1.0, 0.4, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-3714", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-7224", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-2512", "mrqa_naturalquestions-validation-7152", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-1133", "mrqa_naturalquestions-validation-7953", "mrqa_naturalquestions-validation-5293", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-3898", "mrqa_triviaqa-validation-6227", "mrqa_newsqa-validation-260", "mrqa_searchqa-validation-5290"], "SR": 0.625, "CSR": 0.5636275773195876, "retrieved_ids": ["mrqa_squad-train-81731", "mrqa_squad-train-60151", "mrqa_squad-train-69207", "mrqa_squad-train-81771", "mrqa_squad-train-70697", "mrqa_squad-train-32728", "mrqa_squad-train-27687", "mrqa_squad-train-16981", "mrqa_squad-train-20149", "mrqa_squad-train-13621", "mrqa_squad-train-14653", "mrqa_squad-train-4570", "mrqa_squad-train-59511", "mrqa_squad-train-69088", "mrqa_squad-train-55001", "mrqa_squad-train-74308", "mrqa_searchqa-validation-1985", "mrqa_searchqa-validation-9008", "mrqa_triviaqa-validation-3131", "mrqa_searchqa-validation-3173", "mrqa_naturalquestions-validation-642", "mrqa_triviaqa-validation-1597", "mrqa_hotpotqa-validation-1169", "mrqa_naturalquestions-validation-7457", "mrqa_searchqa-validation-1912", "mrqa_hotpotqa-validation-4950", "mrqa_searchqa-validation-13081", "mrqa_newsqa-validation-1970", "mrqa_naturalquestions-validation-3319", "mrqa_searchqa-validation-3737", "mrqa_hotpotqa-validation-745", "mrqa_newsqa-validation-3659"], "EFR": 1.0, "Overall": 0.7430380154639176}, {"timecode": 97, "before_eval_results": {"predictions": ["23 November 1946", "Afghanistan", "C. J. Cherryh", "1938", "German", "1978", "Trey Parker and Matt Stone", "Robert \"Bobby\" Bunda", "eighteenth", "Father of Liberalism", "Grave Digger", "1854", "Thriller", "various names", "March 23, 2017", "Vladimir Valentinovich Menshov", "Shery", "American novelist, playwright, and screenwriter", "Miami Marlins", "at least 96", "the Bears", "EN World web site", "John Andr\u00e9", "Brian Bosworth", "Lynwood", "Hawaii County, Hawaii", "Mel Blanc", "January 2016", "Philadelphia", "nine", "\" Finding Nemo\"", "1957", "House of Commons", "Terry the Tomboy", "\"Kids\".", "Switzerland", "Polk County, Georgia", "Charles L. Clifford", "Taoiseach", "2018\u201319 UEFA Europa League group stage", "island of Spitsbergen", "Ronald Ryan", "2017", "10 October 2010", "Che Guevara", "Marigold Newey", "three", "Warsaw, Poland", "horror film", "Virginia", "Vi\u1ec7t B\u1eafc (\"Northern Vietnam\")", "upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "Strabo", "1922", "Bono", "a mosaic", "Indonesia", "\"procedure on her heart,\"", "British", "Mikkel Kessler", "Kosovo", "Oh Henry", "James Earl Ray", "john le Carr\u00e9"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7830721361971362}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.33333333333333337, 0.5, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.33333333333333337, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.5, 0.8, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.9142857142857143, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-2037", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-5149", "mrqa_hotpotqa-validation-4154", "mrqa_hotpotqa-validation-2442", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-2482", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-4552", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-840", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-1911", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-302"], "SR": 0.640625, "CSR": 0.5644132653061225, "EFR": 0.9130434782608695, "Overall": 0.7258038487133984}, {"timecode": 98, "before_eval_results": {"predictions": ["Marathon", "a clove hitch", "birds", "John Knox", "Sheryl Crow", "Rich Girl", "circus wagons", "an endive", "a midwife", "Patricia Arquette", "E", "tuberculosis", "Judges", "Milan", "Abu Musab al-Zarqawi", "Hendrix", "repent", "Sonora", "( Henri) Matisse", "1849", "Queensland", "a Monarch", "Pluto", "Quisp Cereal", "1803", "The Prose Works of John Milton", "a flower", "a catalog", "a Hydra", "the petroleum sector", "Tin", "Irish Coffee", "Rome", "(George) Belushi & the 3 kids", "Jack Dempsey", "Vladivostok", "an earthquake", "Bizarro", "Summer", "sancire", "a GPA", "New Kids on the Block", "Louis (or Ludwig) the German", "Cut", "an apple", "Princeton University", "Stephen Collins", "Plato The lunar", "the Sahara Desert", "a centipede", "the Chemical Element", "Amartya Sen ( 1998, Economics )", "A chromosome", "1883", "drake", "(George) Conan Doyle", "Newfoundland and Labrador", "Kentucky Wildcats", "MGM Resorts International", "eastern Cheshire", "Sporting Lisbon", "Vicente Carrillo Leyva,", "Bodyguard Trevor Rees, formerly known as Trevor Ree-Jones,", "Allison Janney"], "metric_results": {"EM": 0.53125, "QA-F1": 0.596875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14224", "mrqa_searchqa-validation-11657", "mrqa_searchqa-validation-12806", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-7151", "mrqa_searchqa-validation-8848", "mrqa_searchqa-validation-2401", "mrqa_searchqa-validation-4343", "mrqa_searchqa-validation-14595", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-6005", "mrqa_searchqa-validation-1262", "mrqa_searchqa-validation-1069", "mrqa_searchqa-validation-10077", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-12932", "mrqa_searchqa-validation-16210", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-9456", "mrqa_searchqa-validation-4943", "mrqa_searchqa-validation-2890", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-3985", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-730", "mrqa_hotpotqa-validation-5799", "mrqa_hotpotqa-validation-1900", "mrqa_newsqa-validation-2960"], "SR": 0.53125, "CSR": 0.5640782828282829, "EFR": 0.9666666666666667, "Overall": 0.7364614898989899}, {"timecode": 99, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-1126", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1266", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1748", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1768", "mrqa_hotpotqa-validation-1790", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-184", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2138", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2657", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-2895", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2929", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3396", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-3660", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3671", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4274", "mrqa_hotpotqa-validation-4276", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4368", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4486", "mrqa_hotpotqa-validation-4562", "mrqa_hotpotqa-validation-4603", "mrqa_hotpotqa-validation-4654", "mrqa_hotpotqa-validation-4704", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-489", "mrqa_hotpotqa-validation-4942", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-556", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-5657", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-5717", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-5820", "mrqa_hotpotqa-validation-5836", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-846", "mrqa_hotpotqa-validation-904", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-133", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1613", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4597", "mrqa_naturalquestions-validation-475", "mrqa_naturalquestions-validation-4816", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5200", "mrqa_naturalquestions-validation-5230", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6951", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7741", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8928", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9874", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-1074", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-1949", "mrqa_newsqa-validation-2035", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-223", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-337", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3570", "mrqa_newsqa-validation-381", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-892", "mrqa_searchqa-validation-1069", "mrqa_searchqa-validation-10772", "mrqa_searchqa-validation-1084", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-1154", "mrqa_searchqa-validation-11562", "mrqa_searchqa-validation-11600", "mrqa_searchqa-validation-11618", "mrqa_searchqa-validation-11715", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-12321", "mrqa_searchqa-validation-12446", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12757", "mrqa_searchqa-validation-12806", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-1363", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-14082", "mrqa_searchqa-validation-14332", "mrqa_searchqa-validation-14354", "mrqa_searchqa-validation-14657", "mrqa_searchqa-validation-14669", "mrqa_searchqa-validation-14679", "mrqa_searchqa-validation-14773", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15078", "mrqa_searchqa-validation-15188", "mrqa_searchqa-validation-15220", "mrqa_searchqa-validation-15583", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-15832", "mrqa_searchqa-validation-162", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-1686", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-1809", "mrqa_searchqa-validation-190", "mrqa_searchqa-validation-1977", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2073", "mrqa_searchqa-validation-2307", "mrqa_searchqa-validation-2372", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-2513", "mrqa_searchqa-validation-2560", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-2671", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-3375", "mrqa_searchqa-validation-3400", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-4115", "mrqa_searchqa-validation-4228", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-4409", "mrqa_searchqa-validation-4550", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6632", "mrqa_searchqa-validation-6784", "mrqa_searchqa-validation-6908", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7168", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7362", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8431", "mrqa_searchqa-validation-8518", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-8830", "mrqa_searchqa-validation-8932", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-9562", "mrqa_searchqa-validation-959", "mrqa_searchqa-validation-9599", "mrqa_searchqa-validation-9894", "mrqa_searchqa-validation-9916", "mrqa_searchqa-validation-9984", "mrqa_squad-validation-10064", "mrqa_squad-validation-10168", "mrqa_squad-validation-10203", "mrqa_squad-validation-10309", "mrqa_squad-validation-10333", "mrqa_squad-validation-10370", "mrqa_squad-validation-1042", "mrqa_squad-validation-1182", "mrqa_squad-validation-1247", "mrqa_squad-validation-1349", "mrqa_squad-validation-1404", "mrqa_squad-validation-157", "mrqa_squad-validation-1956", "mrqa_squad-validation-1977", "mrqa_squad-validation-2118", "mrqa_squad-validation-2140", "mrqa_squad-validation-225", "mrqa_squad-validation-2390", "mrqa_squad-validation-2521", "mrqa_squad-validation-256", "mrqa_squad-validation-2573", "mrqa_squad-validation-258", "mrqa_squad-validation-2599", "mrqa_squad-validation-2628", "mrqa_squad-validation-2668", "mrqa_squad-validation-2684", "mrqa_squad-validation-2719", "mrqa_squad-validation-2778", "mrqa_squad-validation-3063", "mrqa_squad-validation-3138", "mrqa_squad-validation-3141", "mrqa_squad-validation-3165", "mrqa_squad-validation-3204", "mrqa_squad-validation-3460", "mrqa_squad-validation-3522", "mrqa_squad-validation-3599", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3898", "mrqa_squad-validation-3958", "mrqa_squad-validation-4072", "mrqa_squad-validation-4122", "mrqa_squad-validation-4157", "mrqa_squad-validation-4162", "mrqa_squad-validation-4164", "mrqa_squad-validation-4304", "mrqa_squad-validation-4311", "mrqa_squad-validation-4421", "mrqa_squad-validation-4429", "mrqa_squad-validation-4490", "mrqa_squad-validation-4579", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4875", "mrqa_squad-validation-5010", "mrqa_squad-validation-5081", "mrqa_squad-validation-5156", "mrqa_squad-validation-5373", "mrqa_squad-validation-5520", "mrqa_squad-validation-5555", "mrqa_squad-validation-5754", "mrqa_squad-validation-5839", "mrqa_squad-validation-5891", "mrqa_squad-validation-6019", "mrqa_squad-validation-6166", "mrqa_squad-validation-6214", "mrqa_squad-validation-6227", "mrqa_squad-validation-6524", "mrqa_squad-validation-6611", "mrqa_squad-validation-6613", "mrqa_squad-validation-6854", "mrqa_squad-validation-692", "mrqa_squad-validation-708", "mrqa_squad-validation-7111", "mrqa_squad-validation-7162", "mrqa_squad-validation-7211", "mrqa_squad-validation-751", "mrqa_squad-validation-7559", "mrqa_squad-validation-7588", "mrqa_squad-validation-7639", "mrqa_squad-validation-7683", "mrqa_squad-validation-7752", "mrqa_squad-validation-7831", "mrqa_squad-validation-7864", "mrqa_squad-validation-7902", "mrqa_squad-validation-7934", "mrqa_squad-validation-8245", "mrqa_squad-validation-8476", "mrqa_squad-validation-8527", "mrqa_squad-validation-867", "mrqa_squad-validation-9082", "mrqa_squad-validation-9145", "mrqa_squad-validation-9317", "mrqa_squad-validation-9362", "mrqa_squad-validation-9587", "mrqa_squad-validation-9643", "mrqa_squad-validation-9653", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-1532", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-2537", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3057", "mrqa_triviaqa-validation-3081", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-3947", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-424", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4343", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4888", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5885", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6516", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7343", "mrqa_triviaqa-validation-7362", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-977"], "OKR": 0.82421875, "KG": 0.4953125, "before_eval_results": {"predictions": ["Albert Square", "Steely Dan", "improve", "Toronto", "Goat Island", "Mase", "the people", "Richard Walter Jenkins", "pizzeria de Pietro", "leander club", "Tripoli", "a window", "Sir Winston Churchill", "Tom Hanks", "Jamaica", "The Perils of Penelope Pitstop", "Strangeways", "Paul Gauguin", "Just Hitting the Floor or Sill", "bohemian doyle", "secret state police", "Shania Twain", "o'Sullivan", "The Sea of Azov", "J. M. W. Turner", "ear", "wagner", "Taco Bell", "Indonesia", "Ford Motor Company", "man", "paper sales company", "The Republic of Elbonia", "a horizon", "Erik Aunapuu", "David Bowie", "24", "Ibrox Stadium", "brilliant", "The Cheshire Cat", "Kent", "East of Eden", "manganese", "The 25th of October 2015", "Egypt", "josee bowie", "The Pritzker Prize", "Richard Rodgers and Oscar Hammerstein", "Dr John Sentamu", "a dice game", "green", "Jenny Slate", "an unknown recipient", "Michelle Stafford", "February 9, 1994", "Hexachrome", "13 October 1958", "Revolutionary Armed Forces of Colombia,", "Somali President Sheikh Sharif Sheikh Ahmed", "allegations that a dorm parent mistreated students at the school.", "Thurgood Marshall", "Inigo Montoya", "King of the Hill", "prevent any contaminants in the sink from flowing into the potable water system by siphonage"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5802083333333333}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1664", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-2560", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-1447", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-1876", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-7633", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-7776", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-3808", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-6274", "mrqa_triviaqa-validation-1598", "mrqa_triviaqa-validation-5961", "mrqa_triviaqa-validation-4339", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-2308", "mrqa_naturalquestions-validation-10691", "mrqa_hotpotqa-validation-4558", "mrqa_newsqa-validation-3177", "mrqa_searchqa-validation-11101", "mrqa_naturalquestions-validation-5297"], "SR": 0.515625, "CSR": 0.56359375, "retrieved_ids": ["mrqa_squad-train-64363", "mrqa_squad-train-32604", "mrqa_squad-train-46320", "mrqa_squad-train-48472", "mrqa_squad-train-80058", "mrqa_squad-train-68331", "mrqa_squad-train-66248", "mrqa_squad-train-84192", "mrqa_squad-train-58285", "mrqa_squad-train-46126", "mrqa_squad-train-82761", "mrqa_squad-train-77433", "mrqa_squad-train-44097", "mrqa_squad-train-57823", "mrqa_squad-train-16492", "mrqa_squad-train-53546", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-2951", "mrqa_searchqa-validation-4409", "mrqa_hotpotqa-validation-4552", "mrqa_newsqa-validation-824", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-7239", "mrqa_hotpotqa-validation-2862", "mrqa_searchqa-validation-1041", "mrqa_newsqa-validation-4196", "mrqa_searchqa-validation-1215", "mrqa_naturalquestions-validation-8669", "mrqa_newsqa-validation-2621", "mrqa_naturalquestions-validation-8383"], "EFR": 0.967741935483871, "Overall": 0.7190015120967741}]}