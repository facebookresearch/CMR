{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.5', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.5_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.5.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4160, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["1978", "587,000 square kilometres", "itinerant farmers", "Gamal Abdul Nasser", "39", "Philo of Byzantium", "cnidarians", "the dukes", "Christopher Eccleston", "Ed Lee", "Keck and Mithouard", "Word and Image Department", "Water (H2O) and carbon dioxide (CO2)", "BBC Radio 5 Live", "Baptism", "achievement-oriented motivations (\"pull\")", "2 million", "inferior", "until 1796", "dummy upper stages filled with water", "Variable lymphocyte receptors", "progressive folk-rock", "$32 billion", "Derek Wolfe", "Basel", "1937", "tourism", "white", "Midsummer\u2019s Night", "installed electrical arc light based illumination systems", "2016", "all large cases of the problem are hard", "photooxidative damage", "five", "iteratively", "the Sun", "Climate fluctuations during the last 34 million years", "Bible translation", "Hayri Abaza", "a better understanding of the Mau Mau command structure", "Turnagain Lane", "monophyletic", "adaptive immune system", "The Dornbirner Ach", "water flow through the body cavity", "CBSE", "a cubic interpolation formula", "Writers Guild of America", "education", "five", "14th to the 19th century", "extended structure", "the Lisbon Treaty", "the Romantic Rhine", "2012", "No Child Left Behind", "The Deadly Assassin and Mawdryn undead", "higher than normal O2 exposure for a fee", "three to five", "TFEU article 294", "The Northern Chinese were ranked higher", "Tracy Wolfson", "local building authority regulations and codes of practice", "The WB"], "metric_results": {"EM": 0.859375, "QA-F1": 0.8807426948051948}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4257", "mrqa_squad-validation-486", "mrqa_squad-validation-5362", "mrqa_squad-validation-291", "mrqa_squad-validation-1862", "mrqa_squad-validation-9520", "mrqa_squad-validation-3104", "mrqa_squad-validation-3609", "mrqa_squad-validation-8256"], "SR": 0.859375, "CSR": 0.859375, "EFR": 1.0, "Overall": 0.9296875}, {"timecode": 1, "before_eval_results": {"predictions": ["recreational", "Khorasan", "higher", "bigamy", "pathogens", "the geographical area it covers", "ships", "12 January", "7.5%", "Ren\u00e9 Lalique", "The Hoppings", "the Scots", "double or triple non-French linguistic origins", "wage or salary", "Each packet is labeled with a destination address, source address, and port numbers", "Maria Fold and thrust Belt", "Yinchuan", "1542", "Chivas", "26", "Amazonia: Man and Culture in a Counterfeit Paradise", "1550", "two", "1850", "Greg Brady", "one's superiority, domination and influence upon a person or group of people", "22 October 2006", "10,000", "Foreign Protestants Naturalization Act", "art posters", "Americans", "its unpaired electrons", "Los Angeles", "over 100,000", "wealth", "chromoplasts", "1,548", "The upper Rhine and upper Danube are easily crossed", "Peter Pratt and Geoffrey Beevers", "2010", "Germany and Austria", "Immunoproteomics", "solid economic growth", "ditch digger", "1754\u20131763", "Danny Trevathan", "Jochi", "1999", "rubisco", "August 1914", "public", "affordable housing", "seven", "John Mearsheimer and Robert Pape", "Business Connect", "1550 to 1900", "rules that conflict with morality", "Bauhaus", "2015", "French", "EBSCO", "The official vegetable of Washington State is a sweet onion", "diary", "a place where monks or nuns live"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8047041933760684}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3610", "mrqa_squad-validation-4677", "mrqa_squad-validation-4305", "mrqa_squad-validation-360", "mrqa_squad-validation-9802", "mrqa_squad-validation-9372", "mrqa_squad-validation-7701", "mrqa_squad-validation-6891", "mrqa_hotpotqa-validation-4259", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5763", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-206"], "SR": 0.78125, "CSR": 0.8203125, "EFR": 1.0, "Overall": 0.91015625}, {"timecode": 2, "before_eval_results": {"predictions": ["2016", "expansion", "Shirley and Johnson", "Miller", "1892 to 1894", "26", "99.4", "St Thomas Becket", "they are homebound", "two", "Moscone Center", "Germany", "January", "about thirty", "10", "Greg Olsen", "Prince of P\u0142ock", "via electron microscopy", "computability theory", "1275", "Fred Pierce", "liquid nitrogen", "The Quasiturbine", "UK", "Daniel 8:9\u201312, 23\u201325", "Ex post facto laws", "cameras and microphones", "time and space hierarchy theorems", "$105 billion", "six", "55 mph", "The Bachelor", "malaria, HIV/AIDS, pneumonia, diarrhoea and malnutrition", "hogs and cattle being shipped from Florida to aid the Confederate cause", "extinction of the dinosaurs", "almost a month", "George B. Storer", "CD40", "94", "their dispersed population and distance from the Scottish Parliament in Edinburgh", "the difference in potential energy", "George Westinghouse", "Geneva", "Court of Justice", "Lake \u00dcberlingen", "Budapest", "Jani Beg", "the \"simple people\"", "Stanford University", "formal language", "systematic economic inequalities", "stealing", "the lunar new year holiday", "Steve McQueen", "orange juice", "Sedgefield", "Sirhan Sirhan", "a wooden comb", "native to Asia", "People!  and The Carnabeats", "a Yemeni cleric and his personal assistant", "the 3rd Sun", "Xherdan Shaqiri", "Potomac River"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7732886904761904}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.14285714285714288, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6337", "mrqa_squad-validation-7165", "mrqa_squad-validation-1703", "mrqa_squad-validation-3540", "mrqa_squad-validation-4096", "mrqa_squad-validation-599", "mrqa_squad-validation-8534", "mrqa_squad-validation-7096", "mrqa_squad-validation-4206", "mrqa_squad-validation-3945", "mrqa_squad-validation-1509", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4028", "mrqa_naturalquestions-validation-5687", "mrqa_newsqa-validation-817", "mrqa_searchqa-validation-13857", "mrqa_hotpotqa-validation-1902"], "SR": 0.71875, "CSR": 0.7864583333333334, "EFR": 1.0, "Overall": 0.8932291666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["Religious Coalition for Reproductive Choice", "Dordtse Kil", "malaria parasite", "three", "over $40 million", "SyFy", "Dick Clark", "five", "younger", "nearly three hundred years", "1 July 1851", "the world's economy", "nine", "the property owner", "1916", "twice", "its unpaired electrons", "Golovin", "General relativity", "Matt Smith", "Baltimore", "noble", "carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "CBS and NBC", "mediaeval music", "California", "26", "Masovian Primeval Forest", "Apollo 1 backup crew", "2008", "a hemicycle", "Connectional Table", "waldzither", "Deacons", "performance", "destruction of the forest", "10.0%", "Outlaws", "Africa", "an occupancy permit", "one", "Dignity Health", "Battle of Jumonville Glen", "Kony Ealy", "a legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship", "Napoleon", "2016", "kennes", "kennedy", "Bones", "The History Book Club", "kennedy", "kennedy", "yerevan", "kennedy", "The Solar System is located within the disk, about 26,000 light - years from the galaxy", "Hugh S. Johnson", "The Five Stages of Sleep: Characteristics of non-REM & REM", "tet", "various", "1896", "Carrousel du Louvre", "last year's Gaza campaign and rebutting the so-called \"Goldstone Report\" as biased", "Odense"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7529775238948626}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8387096774193548, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5185185185185186, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-9178", "mrqa_squad-validation-438", "mrqa_squad-validation-6118", "mrqa_squad-validation-4360", "mrqa_squad-validation-6426", "mrqa_searchqa-validation-9423", "mrqa_searchqa-validation-14645", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-6097", "mrqa_searchqa-validation-4289", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-1429", "mrqa_naturalquestions-validation-808", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-7616", "mrqa_hotpotqa-validation-3780", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-1749", "mrqa_hotpotqa-validation-5584"], "SR": 0.703125, "CSR": 0.765625, "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 4, "before_eval_results": {"predictions": ["13", "lasting damage", "178", "1st century BC", "The Five Doctors", "Greater Los Angeles Area at 17,786,419, and San Diego\u2013Tijuana at 5,105,768", "Bryan Davies", "1060s", "Fresno", "they finally captured Ticonderoga", "Utopia", "Ron Grainer", "BBC Dead Ringers", "July 11, 1962", "it require exposure to bright white light to complete division", "49\u201315", "pancake-shaped circular disks", "energy crisis", "nine", "very low tuition fees", "reduce its volume and increase its density", "paying Tesla $125 per month as well as paying his rent at the Hotel New Yorker, expenses the Company would pay for the rest of Tesla's life", "it would do more harm than good", "Ron Grainer", "symbiotic", "Larry Roberts", "CBS", "leftist/communist/nationalist insurgents/opposition", "RSA", "the wisdom and prudence of certain decisions of procurement", "France's claim to the region was superior to that of the British", "immunosuppressive", "very weak", "private citizen", "Florida", "August 1992", "71%", "Samuel Phillips", "1962", "Newcastle Diamonds", "five", "demographics and economic ties", "UNICEF", "Morgan Tsvangirai", "civilians", "maintain an \"aesthetic environment\" and ensure public safety", "Pakistani territory", "August 11, 12 and 13", "right-wing extremist groups", "Argentina lays claim not just to the islands, but to any resources that could be found there.", "it the largest and perhaps most sophisticated ring of its kind in U.S. history", "Don Draper", "it was beautiful. The rain held off wherever Muhammad Ali went.", "it is with Christ", "1979", "the animal", "John Denver", "1587", "House of Commons", "\"You're out.", "to give the victory to the right", "plants", "white", "Montezuma"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6807501039750338}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7142857142857143, 0.45161290322580644, 0.9333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 0.08695652173913043, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7792", "mrqa_squad-validation-2717", "mrqa_squad-validation-10269", "mrqa_squad-validation-7715", "mrqa_squad-validation-8862", "mrqa_squad-validation-236", "mrqa_squad-validation-8811", "mrqa_squad-validation-4451", "mrqa_squad-validation-1462", "mrqa_squad-validation-6874", "mrqa_squad-validation-7713", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3319", "mrqa_naturalquestions-validation-7901", "mrqa_triviaqa-validation-6976", "mrqa_hotpotqa-validation-2418", "mrqa_searchqa-validation-8027", "mrqa_searchqa-validation-13839", "mrqa_triviaqa-validation-1088"], "SR": 0.640625, "CSR": 0.740625, "EFR": 0.9565217391304348, "Overall": 0.8485733695652173}, {"timecode": 5, "before_eval_results": {"predictions": ["Scottish Parliament Building, in the Holyrood area of Edinburgh", "missile projects", "The Entertainment Channel", "home viewers who made tape recordings of the show", "cattle and citrus", "quantum electrodynamics", "confirmation and sometimes the profession of faith", "75th birthday", "a computational problem where a single output (of a total function) is expected for every input", "Plasmodium falciparum", "refusing to stand for its policies", "Masovian gothic style", "more wealth and income", "a polynomial-time reduction", "2012", "European Parliament and the Council of the European Union", "antithetical", "a polynomial time algorithm", "Fred Singer", "William of Volpiano and John of Ravenna", "water in equilibrium with air", "Arizona Cardinals", "91%", "co-chair of TAR WGI", "July 24", "nearly three hundred years", "Encoded Archival description (EAD)", "the Privy Council", "a citizen's relation to the state and its laws", "Golden Gate Bridge", "2011", "respiration", "environmental degradation", "Fred Silverman", "2016", "390", "More than 1 million", "in the chloroplasts of C4 plants, though it has also been found in some C3 angiosperms, and even some gymnosperms", "When the reaction occurs in a liquid solution, the solid formed is called the'precipitate '", "Jim Capaldi, Paul Carrack, and Peter Vale,", "Hanna's best friend, Mona Vanderwaal, informs the girls that she has also received texts from A, drawing the five closer together in trying to figure out who their tormentor is", "in order to halt it following brake failure", "1972", "boy", "first novel in the Harry Potter series and Rowling's debut novel, first published in 1997 by Bloomsbury", "Geophysicists", "CFB Trenton to the coroner's office in Toronto", "April 25 -- 30 in Park Avenue, just outside the Waldorf - Astoria Hotel", "fold their wings completely when they are resting so that one wing rests directly on top of the other over their abdomens", "December 1, 2009, introducing the game's protagonist", "heart sounds, often described as a lub and a dub ( or dup ), that occur in sequence with each heartbeat", "British POWs build a vital railway bridge in enemy-occupied Burma, Allied commandos are assigned to destroy it", "Richard Seddon", "Switzerland", "the \"Black Abbots\"", "1919", "Subha", "Roger Federer", "Bobby Jindal", "July 4", "\"Personal Jesus\"", "Megha Cleary", "cuttlefish", "South Australia"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6197919496081261}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false, false, true, false, false, false, false], "QA-F1": [0.5454545454545454, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5925925925925926, 0.7058823529411764, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.27272727272727276, 0.0, 1.0, 0.0, 0.07999999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-2705", "mrqa_squad-validation-1600", "mrqa_squad-validation-6670", "mrqa_squad-validation-988", "mrqa_squad-validation-2160", "mrqa_squad-validation-3556", "mrqa_squad-validation-8189", "mrqa_squad-validation-7629", "mrqa_squad-validation-5921", "mrqa_squad-validation-8671", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-5510", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2555", "mrqa_triviaqa-validation-7172", "mrqa_hotpotqa-validation-3223", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-2328", "mrqa_searchqa-validation-15727", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-10685", "mrqa_hotpotqa-validation-5834"], "SR": 0.515625, "CSR": 0.703125, "EFR": 0.967741935483871, "Overall": 0.8354334677419355}, {"timecode": 6, "before_eval_results": {"predictions": ["2007", "1973", "Siegfried", "June", "27 September 2001", "teachers who are friendly and supportive", "500", "northern China", "18 February 1546", "carbon related emissions", "Karl von Miltitz", "a supervisory church body", "via the ballast tanks of ships", "1279", "carbon cycle from satellites on a global scale", "the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague", "(at the opposite end from the mouth)", "models", "river Deabolis", "a third group of pigments found in cyanobacteria, and glaucophyte, red algal, and cryptophyte chloroplasts", "October", "three", "a \"chameleon circuit\"", "from sea level", "by using net wealth (adding up assets and subtracting debts), the Oxfam report, for instance, finds that there are more poor people in the United States and Western Europe than in China", "geographic scholars under colonizing empires", "the Lippe", "colonialism", "10,000", "layered basaltic lava flows", "political parties", "$5,000,000", "a second Gleichschaltung or similar event in the future", "a transient or ongoing role", "1986", "Sets heart in mediastinum and limits its motion", "a nominating committee composed of rock and roll historians selects names for the `` Performers '' category ( singers, vocal groups, bands, and instrumentalists of all kinds )", "the country was known as Santo Domingo -- the name of its present capital and patron saint, Saint Dominic -- and continued to be commonly known as such in English until the early 20th century", "O'Meara", "China ( formerly the Republic of China ), Russia (formerly the Soviet Union ), France, the United Kingdom, and the United States", "first Sunday after Easter", "When all the numbers required to win a prize have been marked off", "9.0 -- 9.1 ( M )", "English", "Roanoke", "David Joseph Madden", "Washington Redskins", "minor key symphonies", "in Christianity", "London", "Belfast", "St Paul's Cathedral", "Sydney", "Australian", "Battle of Britain and the Battle of Malta", "a \"stressed and tired force\" made vulnerable by multiple deployments", "Casalesi Camorra", "Microsoft", "colonel", "Queen Wilhelmina", "The Mole", "Paolo di Dono", "Crete", "Selfie"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6824091094771242}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.05714285714285715, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.4, 1.0, 1.0, 0.1714285714285714, 0.23529411764705882, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.4, 1.0, 0.0, 0.4, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2095", "mrqa_squad-validation-2468", "mrqa_squad-validation-3620", "mrqa_squad-validation-9408", "mrqa_squad-validation-7554", "mrqa_squad-validation-9865", "mrqa_squad-validation-6962", "mrqa_squad-validation-1866", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-4942", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-1058", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-3918", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-4084", "mrqa_triviaqa-validation-5713"], "SR": 0.59375, "CSR": 0.6875, "EFR": 1.0, "Overall": 0.84375}, {"timecode": 7, "before_eval_results": {"predictions": ["annual NFL Experience", "over three days", "second and third run movies, along with classic films", "radiography", "the government and the National Assembly and the Senate", "lipophilic alkaloid toxins", "the Compromise of 1850 enabled California to be admitted to the Union as a free state, preventing southern California from becoming its own separate slave state", "presidential representative democratic republic", "phagosomal", "1969", "the preparation and approval process", "Community law", "indirectly, transmitted as gluons, which form part of the virtual pi and rho mesons", "Downtown San Diego", "1954", "Bermuda 419 turf", "The Judiciary", "they are judged \" wrong\" by an individual conscience, or as part of an effort to render certain laws ineffective, to cause their repeal, or to exert pressure to get one's political wishes on some other issue", "\"The Space Museum (albeit frozen and as an exhibit)", "linear", "The Hoppings funfair", "Decision problems", "seven", "Pickawillany", "denying having committed the crime, or by fleeing the jurisdiction", "56.2%", "Financial crisis of 2007\u201308", "Peter Davison", "Percy Shelley", "ctenophores", "American Broadcasting-Paramount Theatres, Inc.", "Ray Henderson", "rises 735 feet ( 224 m )", "B.R. Ambedkar, the chairman of the Drafting Committee, is widely considered to be its chief architect", "Kim Basinger", "currently Secretary of Homeland Security is Kirstjen Nielsen following the appointment of the then - incumbent secretary, John F. Kelly, to the post of White House Chief of Staff by President Donald Trump", "2001 -- 2002 season", ", the novel explores the theme of hope and dreams through Junior's struggles to find a path to break free of his seemingly doomed fate on the reservation", "two", "In 1973, the University of California, Berkeley recognized the need to provide quality library materials to support the Chicano studies programs", "Bachendri Pal", "line the cavities and surfaces of blood vessels and organs throughout the body", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "the number of characters in Ed, Edd n Eddy is fixed at twelve ( thirteen if Plank, a board of wood who acts as one character's imaginary friend, is included )", "Ron Harper", "Dmitri Mendeleev, who built upon earlier discoveries by scientists such as Antoine - Laurent de Lavoisier and John Newlands, but who is nevertheless generally given sole credit for its development", "sacroiliac joint", "patron", "South African political leaders Mangosuthu Buthelezi and Harry Schwarz", "John Chilcot", "peterwindsor.com", "Kind Hearts and Coronets", "Khilona", "Unibet Premier League Darts", "January 2004", "London and Buenos Aires", "Evan Bayh", "after Wood went missing off Catalina Island, near the California coast", "Juarez drug cartel", "c)", "ummi", "Colorado is called the Centennial State", "Luck of the draw", "between the three towns of Doncaster, Scunthorpe and Gainsborough"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5829358525645283}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23076923076923078, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.8, 1.0, 0.34146341463414637, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.12903225806451613, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9523809523809523, 1.0, 0.07142857142857142, 0.0, 0.12903225806451613, 1.0, 0.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.4, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182]}}, "before_error_ids": ["mrqa_squad-validation-2810", "mrqa_squad-validation-8496", "mrqa_squad-validation-4589", "mrqa_squad-validation-10444", "mrqa_squad-validation-436", "mrqa_squad-validation-6787", "mrqa_squad-validation-7774", "mrqa_squad-validation-6776", "mrqa_squad-validation-4730", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-3538", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-710", "mrqa_naturalquestions-validation-9056", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-7182", "mrqa_triviaqa-validation-6484", "mrqa_triviaqa-validation-1140", "mrqa_triviaqa-validation-3602", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-1051", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-493", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-9419", "mrqa_searchqa-validation-12091", "mrqa_searchqa-validation-6517", "mrqa_hotpotqa-validation-1533"], "SR": 0.484375, "CSR": 0.662109375, "EFR": 1.0, "Overall": 0.8310546875}, {"timecode": 8, "before_eval_results": {"predictions": ["Episcopal Areas", "\"vanguard of change and Islamic reform\"", "scoil phr\u00edobh\u00e1ideach", "1993", "Chinatown", "state, relative cost of living, and grade taught", "it stimulated his brain cells", "made tape recordings", "the chosen machine model", "captive import policy", "the 1855 colonial constitution", "Rhine Gorge", "savanna or desert", "1967", "Vince Lombardi Trophy", "2003", "Kurt Vonnegut", "10", "to become more integral within the health care system", "ten", "12", "1995\u201396", "San Diego International Airport", "hydrogen and helium", "two", "Golden Gate Bridge", "Elders", "1331", "it results from rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "Casey Beane", "Spain had more than compensated by recovering Menorca", "Eurasian Plate", "1947", "John Dalton", "they have also won the competition the most times in a row", "it was on this day in 1930", "was a naval battle fought between an alliance of Greek city - states under Themistocles and the Persian Empire", "the performance marker", "Pradyumna", "Phillipa Soo", "May 18, 2010", "Newfoundland and Labrador", "Mangal Pandey", "Asuka", "electron shells", "the pressure is assumed to be 1 atm ( 101.325 kPa )", "an alternate meaning", "a true wireworm", "Malawi", "27", "an Anglo-Saxon saint", "Jena Malone", "American", "1993", "civilians", "if huge hunks of ice -- such as parts of Greenland and the western shelf of Antarctica", "requires police to question people if there's reason to suspect they're in the United States illegally", "Piedad Cordoba", "the Seven Years' War", "a fire-engine hue", "Robert Frost", "congruentem", "The Dark Tower", "Ministry of European Integration"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6246623168498169}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 0.3888888888888889, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.07692307692307691, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9578", "mrqa_squad-validation-2236", "mrqa_squad-validation-1445", "mrqa_squad-validation-7643", "mrqa_squad-validation-8984", "mrqa_squad-validation-6404", "mrqa_squad-validation-3667", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-1119", "mrqa_triviaqa-validation-6160", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-672", "mrqa_hotpotqa-validation-1086", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-3581", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-1996"], "SR": 0.546875, "CSR": 0.6493055555555556, "EFR": 0.9655172413793104, "Overall": 0.8074113984674329}, {"timecode": 9, "before_eval_results": {"predictions": ["Sports Night", "c1750", "Newton", "Ford", "Political Islam", "51.6%", "public policy goals", "the superior and the norm", "1985", "standard", "Theory of the Earth", "New Jersey, Rhode Island and Delaware", "2,249", "AS-205 would have been devoted to space experiments and contribute no new engineering knowledge about the spacecraft", "Earth", "prime elements", "T. J. Ward", "5", "often a trusted friend, who may hold any office, from Elder to Bishop, or no office at all", "primes", "1995", "if the head of government of a country were to refuse to enforce a decision of that country's highest court", "applied mathematics to the construction of calendars", "extensive spy network and Yam route systems", "Elizabeth", "1861\u20131865", "1968", "Daimler-Benz", "25", "Baldwin", "May 1801", "John Schlesinger", "hamburgers", "Ellie Kemper", "Ezeiza International Airport", "it would be the season where Sam Hinkie's goal of \"The Process\" came into full fruition since they'd later earn the #1 selection in the 2016 NBA draft", "insurance agent Ritu Nanda", "bioelectromagnetics", "The State of Franklin", "Austria", "Andes", "Vishal Bhardwaj", "Lincoln Memorial University", "five aerial victories", "actress", "Manuel `` Manny '' Heffley is Greg and Rodrick's younger brother", "Justin Bieber", "Nitty Gritty Dirt Band", "April", "Lucky the Leprechaun", "Leander", "1", "Newport", "Dean Martin, Katharine Hepburn and Spencer Tracy", "between South America and Africa", "\"Quiet Nights\"", "Crandon, Wisconsin", "the Lone Ranger", "z", "Morris worm", "opal", "In 1922, after the Irish War of Independence and the Anglo - Irish Treaty, most of Ireland seceded from the United Kingdom to become the independent Irish Free State", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "John F. Kennedy"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5948243239603535}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, true, false, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.38095238095238093, 1.0, 0.6, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615383, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.15384615384615385, 0.47058823529411764, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-361", "mrqa_squad-validation-9608", "mrqa_squad-validation-7017", "mrqa_squad-validation-10506", "mrqa_squad-validation-3954", "mrqa_squad-validation-2315", "mrqa_squad-validation-6806", "mrqa_squad-validation-6154", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-4617", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-3750", "mrqa_naturalquestions-validation-5355", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-1567", "mrqa_triviaqa-validation-3786", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-4272", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2324", "mrqa_searchqa-validation-9038", "mrqa_searchqa-validation-5125", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5876"], "SR": 0.515625, "CSR": 0.6359375, "EFR": 0.967741935483871, "Overall": 0.8018397177419355}, {"timecode": 10, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1051", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-1327", "mrqa_hotpotqa-validation-1533", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-1902", "mrqa_hotpotqa-validation-1966", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-2251", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2676", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3837", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4617", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-5376", "mrqa_hotpotqa-validation-5584", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5763", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-763", "mrqa_hotpotqa-validation-861", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3536", "mrqa_naturalquestions-validation-3538", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5355", "mrqa_naturalquestions-validation-5510", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-5687", "mrqa_naturalquestions-validation-5876", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-710", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7754", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9842", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-9963", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-3103", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-4114", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-787", "mrqa_searchqa-validation-10685", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-13484", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-13857", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-1429", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15727", "mrqa_searchqa-validation-15900", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-206", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-2892", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-4746", "mrqa_searchqa-validation-5125", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8027", "mrqa_searchqa-validation-9038", "mrqa_searchqa-validation-9812", "mrqa_squad-validation-10004", "mrqa_squad-validation-10010", "mrqa_squad-validation-10024", "mrqa_squad-validation-10038", "mrqa_squad-validation-10059", "mrqa_squad-validation-10068", "mrqa_squad-validation-10072", "mrqa_squad-validation-10097", "mrqa_squad-validation-10112", "mrqa_squad-validation-10115", "mrqa_squad-validation-10124", "mrqa_squad-validation-10140", "mrqa_squad-validation-10232", "mrqa_squad-validation-10340", "mrqa_squad-validation-10340", "mrqa_squad-validation-10395", "mrqa_squad-validation-10412", "mrqa_squad-validation-10427", "mrqa_squad-validation-10433", "mrqa_squad-validation-10444", "mrqa_squad-validation-10471", "mrqa_squad-validation-10493", "mrqa_squad-validation-10506", "mrqa_squad-validation-1078", "mrqa_squad-validation-1138", "mrqa_squad-validation-1172", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1304", "mrqa_squad-validation-1311", "mrqa_squad-validation-1409", "mrqa_squad-validation-1462", "mrqa_squad-validation-1509", "mrqa_squad-validation-1541", "mrqa_squad-validation-1570", "mrqa_squad-validation-158", "mrqa_squad-validation-1600", "mrqa_squad-validation-1634", "mrqa_squad-validation-1637", "mrqa_squad-validation-1651", "mrqa_squad-validation-1703", "mrqa_squad-validation-1762", "mrqa_squad-validation-1817", "mrqa_squad-validation-1862", "mrqa_squad-validation-1866", "mrqa_squad-validation-1975", "mrqa_squad-validation-199", "mrqa_squad-validation-2095", "mrqa_squad-validation-2108", "mrqa_squad-validation-2160", "mrqa_squad-validation-2236", "mrqa_squad-validation-2247", "mrqa_squad-validation-2315", "mrqa_squad-validation-2325", "mrqa_squad-validation-236", "mrqa_squad-validation-2376", "mrqa_squad-validation-2403", "mrqa_squad-validation-2461", "mrqa_squad-validation-2468", "mrqa_squad-validation-2545", "mrqa_squad-validation-2576", "mrqa_squad-validation-2591", "mrqa_squad-validation-2602", "mrqa_squad-validation-2612", "mrqa_squad-validation-2678", "mrqa_squad-validation-2711", "mrqa_squad-validation-2717", "mrqa_squad-validation-2752", "mrqa_squad-validation-276", "mrqa_squad-validation-2810", "mrqa_squad-validation-2861", "mrqa_squad-validation-2869", "mrqa_squad-validation-2902", "mrqa_squad-validation-291", "mrqa_squad-validation-2916", "mrqa_squad-validation-2934", "mrqa_squad-validation-2952", "mrqa_squad-validation-2985", "mrqa_squad-validation-3049", "mrqa_squad-validation-3104", "mrqa_squad-validation-3190", "mrqa_squad-validation-3194", "mrqa_squad-validation-322", "mrqa_squad-validation-3222", "mrqa_squad-validation-3223", "mrqa_squad-validation-3302", "mrqa_squad-validation-3309", "mrqa_squad-validation-334", "mrqa_squad-validation-3347", "mrqa_squad-validation-3416", "mrqa_squad-validation-343", "mrqa_squad-validation-3440", "mrqa_squad-validation-3524", "mrqa_squad-validation-3540", "mrqa_squad-validation-3556", "mrqa_squad-validation-3577", "mrqa_squad-validation-358", "mrqa_squad-validation-3581", "mrqa_squad-validation-360", "mrqa_squad-validation-3609", "mrqa_squad-validation-361", "mrqa_squad-validation-3610", "mrqa_squad-validation-3611", "mrqa_squad-validation-3620", "mrqa_squad-validation-3660", "mrqa_squad-validation-3678", "mrqa_squad-validation-3693", "mrqa_squad-validation-3713", "mrqa_squad-validation-3745", "mrqa_squad-validation-3751", "mrqa_squad-validation-3752", "mrqa_squad-validation-3820", "mrqa_squad-validation-3851", "mrqa_squad-validation-3866", "mrqa_squad-validation-3871", "mrqa_squad-validation-3873", "mrqa_squad-validation-3954", "mrqa_squad-validation-3957", "mrqa_squad-validation-3962", "mrqa_squad-validation-3986", "mrqa_squad-validation-4026", "mrqa_squad-validation-4096", "mrqa_squad-validation-4179", "mrqa_squad-validation-418", "mrqa_squad-validation-4186", "mrqa_squad-validation-419", "mrqa_squad-validation-4206", "mrqa_squad-validation-4242", "mrqa_squad-validation-4246", "mrqa_squad-validation-4257", "mrqa_squad-validation-4260", "mrqa_squad-validation-4305", "mrqa_squad-validation-436", "mrqa_squad-validation-4360", "mrqa_squad-validation-4376", "mrqa_squad-validation-438", "mrqa_squad-validation-4403", "mrqa_squad-validation-4421", "mrqa_squad-validation-4447", "mrqa_squad-validation-4451", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-45", "mrqa_squad-validation-453", "mrqa_squad-validation-4533", "mrqa_squad-validation-4547", "mrqa_squad-validation-4575", "mrqa_squad-validation-4589", "mrqa_squad-validation-4630", "mrqa_squad-validation-466", "mrqa_squad-validation-4677", "mrqa_squad-validation-47", "mrqa_squad-validation-4707", "mrqa_squad-validation-4730", "mrqa_squad-validation-4771", "mrqa_squad-validation-4775", "mrqa_squad-validation-4832", "mrqa_squad-validation-486", "mrqa_squad-validation-487", "mrqa_squad-validation-4927", "mrqa_squad-validation-4935", "mrqa_squad-validation-4980", "mrqa_squad-validation-500", "mrqa_squad-validation-5052", "mrqa_squad-validation-5099", "mrqa_squad-validation-510", "mrqa_squad-validation-516", "mrqa_squad-validation-5172", "mrqa_squad-validation-519", "mrqa_squad-validation-5230", "mrqa_squad-validation-524", "mrqa_squad-validation-5250", "mrqa_squad-validation-5329", "mrqa_squad-validation-5334", "mrqa_squad-validation-5362", "mrqa_squad-validation-5362", "mrqa_squad-validation-5364", "mrqa_squad-validation-539", "mrqa_squad-validation-5434", "mrqa_squad-validation-5440", "mrqa_squad-validation-5455", "mrqa_squad-validation-5502", "mrqa_squad-validation-5558", "mrqa_squad-validation-5562", "mrqa_squad-validation-5597", "mrqa_squad-validation-5650", "mrqa_squad-validation-5671", "mrqa_squad-validation-5693", "mrqa_squad-validation-57", "mrqa_squad-validation-5753", "mrqa_squad-validation-5772", "mrqa_squad-validation-5783", "mrqa_squad-validation-5791", "mrqa_squad-validation-5881", "mrqa_squad-validation-5921", "mrqa_squad-validation-5921", "mrqa_squad-validation-5951", "mrqa_squad-validation-5980", "mrqa_squad-validation-599", "mrqa_squad-validation-5999", "mrqa_squad-validation-6013", "mrqa_squad-validation-6042", "mrqa_squad-validation-6118", "mrqa_squad-validation-6154", "mrqa_squad-validation-6193", "mrqa_squad-validation-6217", "mrqa_squad-validation-6238", "mrqa_squad-validation-6288", "mrqa_squad-validation-6291", "mrqa_squad-validation-6421", "mrqa_squad-validation-6426", "mrqa_squad-validation-6491", "mrqa_squad-validation-6552", "mrqa_squad-validation-6595", "mrqa_squad-validation-6653", "mrqa_squad-validation-6670", "mrqa_squad-validation-6676", "mrqa_squad-validation-6677", "mrqa_squad-validation-6776", "mrqa_squad-validation-6787", "mrqa_squad-validation-6801", "mrqa_squad-validation-6805", "mrqa_squad-validation-6806", "mrqa_squad-validation-6852", "mrqa_squad-validation-6861", "mrqa_squad-validation-6874", "mrqa_squad-validation-6891", "mrqa_squad-validation-6948", "mrqa_squad-validation-6958", "mrqa_squad-validation-6962", "mrqa_squad-validation-6996", "mrqa_squad-validation-7017", "mrqa_squad-validation-7026", "mrqa_squad-validation-7030", "mrqa_squad-validation-7035", "mrqa_squad-validation-71", "mrqa_squad-validation-7105", "mrqa_squad-validation-7137", "mrqa_squad-validation-7165", "mrqa_squad-validation-7173", "mrqa_squad-validation-7328", "mrqa_squad-validation-7331", "mrqa_squad-validation-734", "mrqa_squad-validation-7347", "mrqa_squad-validation-7372", "mrqa_squad-validation-7380", "mrqa_squad-validation-7384", "mrqa_squad-validation-7395", "mrqa_squad-validation-742", "mrqa_squad-validation-7458", "mrqa_squad-validation-7554", "mrqa_squad-validation-7575", "mrqa_squad-validation-758", "mrqa_squad-validation-7628", "mrqa_squad-validation-7629", "mrqa_squad-validation-764", "mrqa_squad-validation-7647", "mrqa_squad-validation-7653", "mrqa_squad-validation-7713", "mrqa_squad-validation-7715", "mrqa_squad-validation-7723", "mrqa_squad-validation-7747", "mrqa_squad-validation-7774", "mrqa_squad-validation-7792", "mrqa_squad-validation-7793", "mrqa_squad-validation-786", "mrqa_squad-validation-7956", "mrqa_squad-validation-7976", "mrqa_squad-validation-7993", "mrqa_squad-validation-8002", "mrqa_squad-validation-8134", "mrqa_squad-validation-816", "mrqa_squad-validation-817", "mrqa_squad-validation-8189", "mrqa_squad-validation-82", "mrqa_squad-validation-8232", "mrqa_squad-validation-8256", "mrqa_squad-validation-828", "mrqa_squad-validation-8319", "mrqa_squad-validation-8320", "mrqa_squad-validation-8338", "mrqa_squad-validation-8374", "mrqa_squad-validation-8416", "mrqa_squad-validation-847", "mrqa_squad-validation-8496", "mrqa_squad-validation-8534", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8613", "mrqa_squad-validation-8657", "mrqa_squad-validation-8667", "mrqa_squad-validation-8671", "mrqa_squad-validation-8679", "mrqa_squad-validation-8687", "mrqa_squad-validation-8699", "mrqa_squad-validation-8723", "mrqa_squad-validation-8728", "mrqa_squad-validation-8732", "mrqa_squad-validation-8796", "mrqa_squad-validation-8811", "mrqa_squad-validation-8839", "mrqa_squad-validation-8862", "mrqa_squad-validation-8872", "mrqa_squad-validation-8920", "mrqa_squad-validation-893", "mrqa_squad-validation-8930", "mrqa_squad-validation-8939", "mrqa_squad-validation-8984", "mrqa_squad-validation-8987", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_squad-validation-916", "mrqa_squad-validation-9178", "mrqa_squad-validation-9240", "mrqa_squad-validation-9245", "mrqa_squad-validation-9285", "mrqa_squad-validation-9304", "mrqa_squad-validation-9311", "mrqa_squad-validation-9331", "mrqa_squad-validation-9351", "mrqa_squad-validation-9408", "mrqa_squad-validation-9413", "mrqa_squad-validation-9470", "mrqa_squad-validation-9520", "mrqa_squad-validation-9532", "mrqa_squad-validation-959", "mrqa_squad-validation-96", "mrqa_squad-validation-9608", "mrqa_squad-validation-9647", "mrqa_squad-validation-9777", "mrqa_squad-validation-9802", "mrqa_squad-validation-9845", "mrqa_squad-validation-9849", "mrqa_squad-validation-9865", "mrqa_squad-validation-988", "mrqa_squad-validation-9984", "mrqa_squad-validation-999", "mrqa_squad-validation-9994", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-1140", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1644", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3602", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4272", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4542", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-5871", "mrqa_triviaqa-validation-6115", "mrqa_triviaqa-validation-6160", "mrqa_triviaqa-validation-6338", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6484", "mrqa_triviaqa-validation-6513", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-6903", "mrqa_triviaqa-validation-6976", "mrqa_triviaqa-validation-7172", "mrqa_triviaqa-validation-7616"], "OKR": 0.904296875, "KG": 0.41171875, "before_eval_results": {"predictions": ["November 1979", "Pittsburgh", "William Rainey Harper", "classical element fire", "the Mughal state", "photosynthesis", "first half of the 10th century", "53,000", "their animosity toward each other", "giving her brother Polynices a proper burial", "lion, leopard, buffalo", "1998", "Widener Library", "a fee per unit of information transmitted", "Nobel Prize", "Bart Starr", "UK", "Australia", "7.8%", "Mars", "2005", "27", "Louis King", "Boston", "Venancio Flores", "Coronation Street", "Prescription Drug User Fee Act", "Art Deco-style skyscraper", "Lindsey Islands", "a male-dominated industry", "Brig Gen Augustine Warner Robins", "other areas of special sovereignty", "Russian film industry", "Thriller", "Restoration Hardware", "Russell Humphreys", "Iron Man 3", "Andrzej Go\u0142ota", "Waylon Smithers", "Ordos City China Science Flying Universe Science and Technology Co. Ltd.", "The The Vanishing", "Wendell Erdman Berry", "Che Guevara", "Indians", "gastrocnemius", "1,228 km / h", "cells", "a compiler can derive machine code", "typhoid fever", "William Boyd", "Octopussy", "A4", "a day", "77-year-old Oscar winner", "The supplemental spending bill", "genocide", "off the coast of Dubai", "murder", "Stalin", "The Industrial Workers of the World", "The chemical element", "Sam Bellamy", "Kellogg's", "Tintin"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7322754502442003}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.3076923076923077, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.7499999999999999, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1054", "mrqa_squad-validation-8279", "mrqa_squad-validation-7724", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-201", "mrqa_hotpotqa-validation-173", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-6061", "mrqa_naturalquestions-validation-9885", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-5525", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-163", "mrqa_searchqa-validation-1856", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-16856"], "SR": 0.65625, "CSR": 0.6377840909090908, "EFR": 1.0, "Overall": 0.7470099431818181}, {"timecode": 11, "before_eval_results": {"predictions": ["the difference in potential energy", "canny", "accessory pigments that override the chlorophylls' green colors", "62", "$20,000", "Cybermen", "1080i HD", "income inequality", "104 \u00b0F (40 \u00b0C)", "conspiracy against Islam by the Western governments.", "road engines", "religious", "Larry Ellison", "consultant", "Derek Wolfe and Malik Jackson", "luxurious parks and royal gardens", "Establishment Clause of the First Amendment", "Centrum", "20%", "win an acquittal and avoid imprisonment or a fine", "day of the 5th month of the traditional Chinese calendar", "Melissa Disney", "Theodore Roosevelt", "Andy Cole", "13 February", "420 mg ( dry weight )", "1807", "sewn - on at the wearers preference", "Monk's Caf\u00e9", "Director of National Intelligence", "Francisco Pizarro", "Czech word, robota", "Kyla Pratt", "1945", "Michael Crawford", "a young husband and wife and how they deal with the challenge of buying secret Christmas gifts for each other with very little money", "2017 / 18 Divisional Round", "New Croton Reservoir in Westchester and Putnam counties", "550 quadrillion Imperial gallons", "Kepner", "herd maintenance", "Annette Strean", "Anwar Sadat", "Alberich", "The Battle of Kasserine Pass", "Richard Wagner", "Muriel Spark", "Islamic philosophy", "Minnesota", "After School", "Scottish", "a large stein mug", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "nearly $162 billion in war funding", "The number of deaths attributed directly to the fires", "Juri Kibuishi", "last weekend at Old Trafford", "The Truman Show", "Jack McCall", "diamond", "90%", "Pakistan Rupee", "\"evidence of the cozy relationship between some elements of [the agency] and the oil and gas industry.\"", "a paragraph about the king and crown prince that makes it illegal to defame, insult or threaten the crown"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6030424783549784}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.36363636363636365, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.08, 0.5454545454545454]}}, "before_error_ids": ["mrqa_squad-validation-7577", "mrqa_squad-validation-7162", "mrqa_squad-validation-9632", "mrqa_squad-validation-7087", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-1018", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-5154", "mrqa_triviaqa-validation-5538", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4664", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-1265", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-2716", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-702"], "SR": 0.515625, "CSR": 0.6276041666666667, "EFR": 0.967741935483871, "Overall": 0.7385223454301075}, {"timecode": 12, "before_eval_results": {"predictions": ["San Joaquin Light & Power Building", "2003", "British", "Doritos", "live", "23.9%", "Daewoo", "1777", "34\u201319", "John D. Rockefeller", "Pole Mokotowskie", "Newcastle College", "63%", "Boulton", "can produce both eggs and sperm at the same time", "laws of physics", "buoyancy", "international drug suppliers, rather than consumers", "Oregon", "tuberculosis", "Young Men's Christian Association", "the Chancery lawyer \"Conversation\" Kenge", "Charles Springall", "The Iron Duke", "John Glenn", "28", "The Golden Child", "Scarborough", "Il Trovatore", "tunisia", "Kentucky Derby", "Calvin Coolidge", "Hindi", "The Bridge on the River Kwai", "Nowhere Boy", "The Stereophonics", "Lancashire", "the recorder", "Michael J. Fox", "Japanese", "Poland", "October 30, 2017", "By 1912", "Ravi Shastri", "Bohrium", "Anthony Hopkins", "Ella Jane Fitzgerald", "Bill Cosby", "Anna Clyne", "aluminum foil", "2014", "Hong Kong's Victoria Harbor", "school", "Herman Thomas", "Sunday", "sedative", "conifers", "funerals", "Afrikanerdom", "The dachshund", "the Washington-Franklin Issues", "tunisia", "parts per million", "rice"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6854166666666668}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4645", "mrqa_squad-validation-3492", "mrqa_triviaqa-validation-6317", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-6840", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-4", "mrqa_triviaqa-validation-4591", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-validation-444", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-134", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3613", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-15193", "mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-10875", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-13542"], "SR": 0.640625, "CSR": 0.6286057692307692, "EFR": 0.9565217391304348, "Overall": 0.7364786266722407}, {"timecode": 13, "before_eval_results": {"predictions": ["Elders", "WatchESPN", "pulmonary fibrosis", "the third and fourth series respectively", "February 1, 2016", "Antigone", "1972", "over fifty", "Norman named Oursel", "\u00a341,004", "1806-07", "Executive Vice President of Football Operations and General Manager", "British", "1920s", "poet", "the number of social services that people can access wherever they move", "john Dryden", "21", "Fresh Fields", "cyanoguttatus", "Ecuador", "Brussels", "sally kells", "Egypt", "sally McCormack", "photographer and filmmaker", "john johnson", "Uganda", "dogs", "coffee", "Leicestershire", "Billaley & His johnsons", "Pepi Simpson", "gold", "Istanbul", "wale", "diana johnson", "Los Angeles", "L. Pasteur", "Al Pacino", "White Christmas", "the nucleus", "12 November 2010", "November 1975", "Afonso IV of Portugal", "southern Turkey", "\"Barney Miller\"", "Yellow fever", "Nye County", "\"Traumnovelle\" (\"Dream Story\")", "the \"Pour le M\u00e9rite\"", "San Francisco, California", "rebecca Barnett", "1-0", "not for sale", "June 6, 1944", "19", "backbreaking labor", "Earl Louis \"Curly\" Lambeau", "the Oxford English Dictionary", "Jimmy Carter", "john davis", "Jezebel", "cheese"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6454175420168067}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, false, false, false, true, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.5, 0.25, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.23529411764705882, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7762", "mrqa_squad-validation-1135", "mrqa_triviaqa-validation-4193", "mrqa_triviaqa-validation-5352", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-49", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-3272", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-1349", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-6920", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-9939", "mrqa_naturalquestions-validation-9672", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-2852", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-4067", "mrqa_searchqa-validation-7204", "mrqa_searchqa-validation-7749"], "SR": 0.546875, "CSR": 0.6227678571428572, "EFR": 0.9655172413793104, "Overall": 0.7371101447044335}, {"timecode": 14, "before_eval_results": {"predictions": ["Xbox One", "tyrosinase", "increased trade with poor countries and the fragmentation of the means of production, resulting in low skilled jobs becoming more tradeable", "2005", "Art Deco style in painting and art", "The Prospect Studios", "Advanced Steam movement", "three", "the Moscone Center in San Francisco", "since 2001", "identified change orders or project changes that increased costs", "comedies and family-oriented series", "case law by the Court of Justice, international law and general principles of European Union law", "oxide compounds such as silicon dioxide, making up almost half of the crust's mass", "278", "aureus", "quietly", "Melvil Dewey", "saxophone", "Thaddeus or Thaddaeus", "7 wives", "Russia", "feet", "the classic building toy, Lincoln Logs", "March 19", "Diptera", "krakatoa", "fucus vesiculosus", "fruit", "telephones", "Erik Thorvaldson", "tungsten", "louis paolucci", "Black September", "warblers", "comets", "geodetics", "comets", "osmic acid", "Niveditha, Diwakar, Shruti", "Tagalog or English", "2013", "1861", "James Madison", "Lou Rawls", "the University of Missouri's women's basketball team", "Marco Hietala", "New Orleans, Louisiana", "Worcester", "North Carolina", "July 23, 1971", "March 22", "August 19, 2007", "state senators", "Ike", "95", "J. Crew", "john Harvard", "Catalonia (from Latin Mons Serratus Saw-Toothed Mountain)", "(Hodgkin's lymphoma)", "johnson city", "five thieves enter the Riviera Casino in Las Vegas with guitar cases full of guns, and stage a daring robbery.", "norman Lionfish", "Vladimirovich \"Val\" Bure"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5785714285714285}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444444, 1.0, 0.5, 0.6666666666666666, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.8571428571428571]}}, "before_error_ids": ["mrqa_squad-validation-7534", "mrqa_squad-validation-512", "mrqa_squad-validation-3670", "mrqa_squad-validation-1546", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-992", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-6032", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-3008", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-5801", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-2189", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-5483", "mrqa_hotpotqa-validation-3366", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-4545", "mrqa_newsqa-validation-3783", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-14099", "mrqa_searchqa-validation-10963", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-12049", "mrqa_hotpotqa-validation-4862"], "SR": 0.46875, "CSR": 0.6125, "EFR": 1.0, "Overall": 0.741953125}, {"timecode": 15, "before_eval_results": {"predictions": ["the Treaties establishing the European Union", "Isaac Newton", "draftsman", "mesoglea", "Associate Membership", "his own men", "Baltimore Ravens", "through sponsors", "61.1%", "Baden-W\u00fcrttemberg", "Most Western countries, and some others,", "lion, leopard, buffalo, rhinoceros, and elephant", "25-minute", "Romeo and Juliet", "Terrence Malick", "Ceefax", "Ireland", "William David Charles Carling", "Istanbul", "red deer", "prophet", "hair", "otter", "chiang Kai-shek", "mountain", "philly", "crenallated bridge replete with mock towers", "charing Cross/St Pancras International", "Spice Girls", "accounting", "feet", "South Dakota", "17", "Moldova", "AFC Wimbledon", "electric chair", "Northern Ireland", "chuckleberry Finn", "Rocketship X-M (1950)", "the country", "Thomas Jefferson", "Ben Willis", "Ra\u00fal Eduardo Esparza", "The Lykan Hypersport", "a hyper - active kinase, that confers an aberrant, ligand - independent, non-regulated growth stimulus to the active site", "commercial", "44", "model, actress and television host", "Wendie Jo Sperber", "Jeff Meldrum", "Canadian", "Lisa Brown", "Afghanistan", "the Kurdish area of northern Iraq.", "Saturday", "Jaime Andrade", "Bill Haas", "tritonic", "the infield", "labor day", "eagles", "Delaware", "crude oil", "Anil Kapoor"], "metric_results": {"EM": 0.5, "QA-F1": 0.5485491071428572}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6128", "mrqa_squad-validation-980", "mrqa_squad-validation-2086", "mrqa_squad-validation-8278", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-2434", "mrqa_triviaqa-validation-1709", "mrqa_triviaqa-validation-533", "mrqa_triviaqa-validation-3956", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2967", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-487", "mrqa_triviaqa-validation-1998", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-9271", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-5036", "mrqa_hotpotqa-validation-2555", "mrqa_hotpotqa-validation-1116", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-1507", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-1443", "mrqa_searchqa-validation-9664", "mrqa_searchqa-validation-3979"], "SR": 0.5, "CSR": 0.60546875, "EFR": 0.9375, "Overall": 0.728046875}, {"timecode": 16, "before_eval_results": {"predictions": ["12 December 1964", "sea water", "\"trading rules\" that are \"enacted by quasi-government bodies\" which could hinder trade \"directly or indirectly, actually or potentially\" would be caught by article 34.", "around 5 million", "Van de Graaff generator", "bounding", "between 1835 and 1842", "Vivienne Westwood", "\"every Christian is a confessor.\"", "France", "megaprojects", "demand for higher quality housing increased", "1960s to the mid-1970s", "W. Edwards Deming", "Irsay", "1947, 1956, 1975, 2015 and 2017", "travis", "Kevin Kline", "a computer maintenance utility included in Microsoft Windows designed to free up disk space on a computer's hard drive", "Saint Alphonsa", "President of the United States", "Ed Roland", "Kristy Swanson", "a compact layout to combine keys which are usually kept separate", "Miobrara", "American football", "Lex Luger and Rick Rude", "Blue laws", "Earth", "1923", "United Kingdom", "The U.S. state of Georgia", "Allison Janney", "Sir Hugh Beaver", "23 September 1889", "October 22, 2017", "Los Angeles", "1980", "\"two all beef patties, special sauce, lettuce, cheese, pickles, onions, on a sesame-seed bun.", "Missouri", "Australia and Ireland", "hitler", "Llandudno", "albinism", "round five of the 2017 season", "John Schlesinger", "Clitheroe Football Club", "1992", "Neighbours", "$7.3 billion", "France's famous Louvre museum", "Kenneth Cole", "The Hutus were considered inferior, prompting resentment that was passed on through the generations. The first major assault on Tutsis occurred in 1959, killing thousands and prompting more attacks over the years.", "10", "members of the lower house of parliament, which will be tasked with drafting a new constitution after three decades of Mubarak's rule.", "her decades-long portrayal of Alice Horton", "Arabic", "saprophytes", "traven", "Mihammad al-Wahhab", "denis mogen David", "copper", "The Ansonia", "Chiltern Hills"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6783989474436989}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, false, false, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, true], "QA-F1": [0.0, 1.0, 0.06896551724137931, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.9142857142857143, 1.0, 1.0, 1.0, 1.0, 0.058823529411764705, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.923076923076923, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8369", "mrqa_squad-validation-4332", "mrqa_squad-validation-2297", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-7635", "mrqa_triviaqa-validation-7696", "mrqa_triviaqa-validation-6799", "mrqa_hotpotqa-validation-2793", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2112", "mrqa_searchqa-validation-15859", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-7481", "mrqa_searchqa-validation-14243", "mrqa_hotpotqa-validation-3279"], "SR": 0.609375, "CSR": 0.6056985294117647, "EFR": 0.92, "Overall": 0.7245928308823529}, {"timecode": 17, "before_eval_results": {"predictions": ["Henry Plitt", "charging their students tuition", "lysozyme and phospholipase A2", "tensions over slavery and the power of bishops in the denomination", "The Fugitive", "Thomas Edison and Nikola Tesla", "reformers", "minimal loss over any terrestrial distance", "greenhouse gas", "the \"scariest TV show of all time\"", "The Scottish Parliament", "the Raven Special Lager", "Arabic", "lunar module", "Mexico Lottery", "the sidecar", "the Eiffel Tower", "Mexico", "Richard III", "hair", "yellow fever", "geometria", "Michael Caine", "a brief message appended to the end of a letter (following the signature) or other text", "Season 1", "the Lewis and Clark Bicentennial", "Maria Full of Grace", "Pro-Jig Clamp Set", "Arby's", "Stevenson", "the pupil", "Albright", "Union Pacific & the Central Pacific", "J.R. Tolkien", "tarantulas", "Marvell", "Mars", "29", "Sean O' Neal", "Muhammad", "Baker, California, USA", "April 1979", "the International Border", "a Horse With No Name", "Missouri", "anahuac", "British Overseas Airways Corporation", "Tahrir Square", "Hercules", "14 December 1990", "Gianna", "first freshman to finish as the runner-up", "Switzerland\u2013European Union relations", "Gesellschaft mit beschr\u00e4nkter Haftung", "1868", "The Da Vinci Code", "Paul McCartney", "a multibillion dollar arms deal in the country,", "Chesley \"Sully\" Sullenberger", "Abbey Road", "tennis", "In Time", "Milira", "Pangaea"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6148313492063492}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, true, true, false, true, false, true, false, true, false, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.4, 1.0, 0.8, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444444, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6024", "mrqa_squad-validation-2437", "mrqa_squad-validation-1496", "mrqa_searchqa-validation-2979", "mrqa_searchqa-validation-2179", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-9651", "mrqa_searchqa-validation-16347", "mrqa_searchqa-validation-7518", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-15636", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-2953", "mrqa_searchqa-validation-10931", "mrqa_searchqa-validation-12632", "mrqa_searchqa-validation-3951", "mrqa_searchqa-validation-9998", "mrqa_searchqa-validation-16905", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-1169", "mrqa_triviaqa-validation-5676", "mrqa_triviaqa-validation-3031", "mrqa_triviaqa-validation-4902", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-5401", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-1811"], "SR": 0.53125, "CSR": 0.6015625, "EFR": 1.0, "Overall": 0.739765625}, {"timecode": 18, "before_eval_results": {"predictions": ["early 1960", "the Saudi monarchy", "Tomingaj, near Gra\u010dac", "Muqali", "2016", "by technique", "2008", "41", "Stockton and Darlington Railway", "increasing access to education", "penance", "Intermezzo", "white", "Bangladesh", "Warren Gamaliel Harding", "Joy Division", "Athol Fugard", "a ska or reggae fan", "the Constitution", "beibei", "glaciers", "Charles Earl Bowles", "a nonmetrical hymn with words taken from a biblical text other than from the Book of Psalms", "sanguine", "Maine", "the Atlantic", "Pilgrim's Progress", "the Holy Grail", "Macbeth", "Chocolate Factory", "Mountain Dew", "Engelbert Humperdinck", "Mike McCready", "paddle sport", "Smokey Robinson", "Lhasa", "Heart rhythm", "statute or the Constitution itself", "Kerry Shale as Tadheus `` Tad '' Stone", "2028", "Patrick Walshe", "Massachusetts", "Madison, Wisconsin", "Marc Brunel", "Czech Republic", "Donegal", "Sir Edwin Landseer", "Albert Einstein", "hot springs", "Academy Award for Best Animated Feature", "An aircraft", "Minnesota", "Lisburn Distillery Football Club", "Wilton Mall", "2002\u201303", "9 a.m.", "a civil disturbance call", "May 2008", "Ralph Cifaretto", "Lance Cpl. Maria Lauterbach", "on the Ohio River near Warsaw, Kentucky,", "Percy Sledge", "Dublin", "Herald of Free Enterprise"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6987980769230768}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.923076923076923, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9516", "mrqa_squad-validation-1227", "mrqa_squad-validation-3179", "mrqa_searchqa-validation-15758", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-7430", "mrqa_searchqa-validation-9612", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-3814", "mrqa_searchqa-validation-6780", "mrqa_searchqa-validation-2642", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9637", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-9975", "mrqa_naturalquestions-validation-10653", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-3955", "mrqa_hotpotqa-validation-218", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-541", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2573"], "SR": 0.609375, "CSR": 0.6019736842105263, "EFR": 1.0, "Overall": 0.7398478618421053}, {"timecode": 19, "before_eval_results": {"predictions": ["Ice Ages", "Oligocene", "Gosforth Park", "Fridays", "refuse to sign bail until certain demands are met, such as favorable bail conditions, or the release of all the activists.", "to encourage investment", "*R\u012bnaz,", "Beyonc\u00e9 and Bruno Mars", "\"zip\" the mouth shut when the animal is not feeding, by forming intercellular connections with the opposite adhesive strip", "J\u00f3zsef Pulitzer", "\"Spitting Image\"", "left fielder", "Ellesmere Port", "Punjabi/Pashtun", "KXII", "Madonna Louise Ciccone", "Manor of More", "Europe", "Adelaide", "Woodsy owl", "\"The Snowman\"", "1994\u201395", "Satchmo, Satch or Pops", "The Livingston family", "hamburgers", "C. J. Cherryh", "Holston River", "1939", "Tabasco", "Fox", "Lieutenant Colonel Iceal E. \"Gene\" Hambleton", "Houston Rockets", "\"Catch Me If You Can\"", "striker", "a vegetarian dish called Buddha's delight", "The Washington Post", "\"Secrets and Lies\"", "Bobb McKittrick", "the endocrine ( hormonal ) systems", "a vanishing point", "Dorothy Gale", "an investor couple in Austin, Texas", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "Cape Town", "the French Revolution", "a branch of mathematics where variables ( verbs) represent numbers", "Rajasthan", "almond", "Diogenes", "Pete Docter", "Dr. Jennifer Arnold and husband Bill Klein,", "American", "Gary Player", "\"full civil equality,\"", "made 109 as Sri Lanka, seeking a win to level the series at 1-1, closed on top again in the final session with a 74 stand", "a American politician and businessman who was the 46th Vice President", "Publius Ovidius Naso", "Martin Van Buren", "Herodotus", "Pentagon", "a Beanie Baby", "Wigan Athletic", "Iowa", "social networking sites"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5549843741723608}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.6206896551724138, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.4, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.19354838709677416, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5456", "mrqa_squad-validation-6931", "mrqa_squad-validation-114", "mrqa_squad-validation-4497", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-5249", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-5163", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-4864", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-3491", "mrqa_triviaqa-validation-2413", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-2898", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-15956", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1182"], "SR": 0.46875, "CSR": 0.5953125, "EFR": 0.9705882352941176, "Overall": 0.7326332720588236}, {"timecode": 20, "UKR": 0.7265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1051", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-173", "mrqa_hotpotqa-validation-1902", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-2555", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2676", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3279", "mrqa_hotpotqa-validation-3302", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5036", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5249", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-5376", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-5763", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-861", "mrqa_hotpotqa-validation-926", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-1418", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-5876", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-6199", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-9039", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9175", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9672", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-9975", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1329", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3103", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-4114", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-560", "mrqa_searchqa-validation-10685", "mrqa_searchqa-validation-10875", "mrqa_searchqa-validation-10931", "mrqa_searchqa-validation-10963", "mrqa_searchqa-validation-1164", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12632", "mrqa_searchqa-validation-13542", "mrqa_searchqa-validation-13604", "mrqa_searchqa-validation-13784", "mrqa_searchqa-validation-13857", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15193", "mrqa_searchqa-validation-15636", "mrqa_searchqa-validation-15676", "mrqa_searchqa-validation-15727", "mrqa_searchqa-validation-15900", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-16905", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2639", "mrqa_searchqa-validation-2642", "mrqa_searchqa-validation-2892", "mrqa_searchqa-validation-301", "mrqa_searchqa-validation-3131", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-3894", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-4410", "mrqa_searchqa-validation-4879", "mrqa_searchqa-validation-5125", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-6517", "mrqa_searchqa-validation-6780", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-7481", "mrqa_searchqa-validation-7749", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-8865", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-9419", "mrqa_searchqa-validation-9612", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9637", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-9998", "mrqa_squad-validation-10004", "mrqa_squad-validation-10013", "mrqa_squad-validation-10024", "mrqa_squad-validation-1006", "mrqa_squad-validation-10078", "mrqa_squad-validation-10097", "mrqa_squad-validation-10112", "mrqa_squad-validation-10199", "mrqa_squad-validation-10395", "mrqa_squad-validation-10412", "mrqa_squad-validation-1042", "mrqa_squad-validation-10427", "mrqa_squad-validation-10433", "mrqa_squad-validation-10444", "mrqa_squad-validation-10493", "mrqa_squad-validation-10506", "mrqa_squad-validation-1052", "mrqa_squad-validation-1078", "mrqa_squad-validation-1138", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1304", "mrqa_squad-validation-1445", "mrqa_squad-validation-1462", "mrqa_squad-validation-1496", "mrqa_squad-validation-1512", "mrqa_squad-validation-1527", "mrqa_squad-validation-1541", "mrqa_squad-validation-1546", "mrqa_squad-validation-1570", "mrqa_squad-validation-158", "mrqa_squad-validation-1600", "mrqa_squad-validation-1637", "mrqa_squad-validation-1684", "mrqa_squad-validation-1762", "mrqa_squad-validation-1850", "mrqa_squad-validation-1862", "mrqa_squad-validation-1866", "mrqa_squad-validation-199", "mrqa_squad-validation-2236", "mrqa_squad-validation-2247", "mrqa_squad-validation-2297", "mrqa_squad-validation-236", "mrqa_squad-validation-2376", "mrqa_squad-validation-2468", "mrqa_squad-validation-2545", "mrqa_squad-validation-2576", "mrqa_squad-validation-2591", "mrqa_squad-validation-2602", "mrqa_squad-validation-2705", "mrqa_squad-validation-2723", "mrqa_squad-validation-276", "mrqa_squad-validation-2834", "mrqa_squad-validation-2869", "mrqa_squad-validation-2952", "mrqa_squad-validation-3004", "mrqa_squad-validation-302", "mrqa_squad-validation-3049", "mrqa_squad-validation-3063", "mrqa_squad-validation-3092", "mrqa_squad-validation-3190", "mrqa_squad-validation-3194", "mrqa_squad-validation-3302", "mrqa_squad-validation-3309", "mrqa_squad-validation-332", "mrqa_squad-validation-3372", "mrqa_squad-validation-3398", "mrqa_squad-validation-3416", "mrqa_squad-validation-3436", "mrqa_squad-validation-3524", "mrqa_squad-validation-3525", "mrqa_squad-validation-3540", "mrqa_squad-validation-3577", "mrqa_squad-validation-358", "mrqa_squad-validation-3610", "mrqa_squad-validation-3616", "mrqa_squad-validation-3620", "mrqa_squad-validation-3640", "mrqa_squad-validation-3660", "mrqa_squad-validation-3667", "mrqa_squad-validation-3670", "mrqa_squad-validation-3715", "mrqa_squad-validation-3820", "mrqa_squad-validation-3851", "mrqa_squad-validation-3865", "mrqa_squad-validation-3871", "mrqa_squad-validation-3925", "mrqa_squad-validation-3950", "mrqa_squad-validation-3986", "mrqa_squad-validation-402", "mrqa_squad-validation-4044", "mrqa_squad-validation-4127", "mrqa_squad-validation-4179", "mrqa_squad-validation-4186", "mrqa_squad-validation-419", "mrqa_squad-validation-4194", "mrqa_squad-validation-4201", "mrqa_squad-validation-4246", "mrqa_squad-validation-436", "mrqa_squad-validation-4360", "mrqa_squad-validation-4376", "mrqa_squad-validation-438", "mrqa_squad-validation-4403", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4497", "mrqa_squad-validation-4506", "mrqa_squad-validation-4533", "mrqa_squad-validation-4649", "mrqa_squad-validation-466", "mrqa_squad-validation-4677", "mrqa_squad-validation-4707", "mrqa_squad-validation-487", "mrqa_squad-validation-4927", "mrqa_squad-validation-4935", "mrqa_squad-validation-494", "mrqa_squad-validation-4980", "mrqa_squad-validation-500", "mrqa_squad-validation-510", "mrqa_squad-validation-516", "mrqa_squad-validation-5172", "mrqa_squad-validation-5173", "mrqa_squad-validation-5185", "mrqa_squad-validation-5193", "mrqa_squad-validation-5230", "mrqa_squad-validation-5334", "mrqa_squad-validation-5362", "mrqa_squad-validation-5366", "mrqa_squad-validation-5434", "mrqa_squad-validation-5448", "mrqa_squad-validation-5455", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5562", "mrqa_squad-validation-5581", "mrqa_squad-validation-5650", "mrqa_squad-validation-5791", "mrqa_squad-validation-5809", "mrqa_squad-validation-585", "mrqa_squad-validation-5866", "mrqa_squad-validation-5921", "mrqa_squad-validation-5951", "mrqa_squad-validation-5980", "mrqa_squad-validation-599", "mrqa_squad-validation-6013", "mrqa_squad-validation-6015", "mrqa_squad-validation-6024", "mrqa_squad-validation-6154", "mrqa_squad-validation-6193", "mrqa_squad-validation-6217", "mrqa_squad-validation-6238", "mrqa_squad-validation-6337", "mrqa_squad-validation-6382", "mrqa_squad-validation-641", "mrqa_squad-validation-6595", "mrqa_squad-validation-6653", "mrqa_squad-validation-6670", "mrqa_squad-validation-6676", "mrqa_squad-validation-6677", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-6805", "mrqa_squad-validation-6833", "mrqa_squad-validation-6874", "mrqa_squad-validation-6891", "mrqa_squad-validation-6891", "mrqa_squad-validation-6942", "mrqa_squad-validation-6996", "mrqa_squad-validation-7096", "mrqa_squad-validation-7105", "mrqa_squad-validation-7137", "mrqa_squad-validation-715", "mrqa_squad-validation-7162", "mrqa_squad-validation-7165", "mrqa_squad-validation-7347", "mrqa_squad-validation-737", "mrqa_squad-validation-7380", "mrqa_squad-validation-7534", "mrqa_squad-validation-7554", "mrqa_squad-validation-7575", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-7653", "mrqa_squad-validation-7670", "mrqa_squad-validation-7701", "mrqa_squad-validation-7708", "mrqa_squad-validation-7715", "mrqa_squad-validation-7724", "mrqa_squad-validation-7747", "mrqa_squad-validation-7792", "mrqa_squad-validation-7850", "mrqa_squad-validation-7956", "mrqa_squad-validation-8068", "mrqa_squad-validation-816", "mrqa_squad-validation-817", "mrqa_squad-validation-8189", "mrqa_squad-validation-8196", "mrqa_squad-validation-8231", "mrqa_squad-validation-8287", "mrqa_squad-validation-8362", "mrqa_squad-validation-8374", "mrqa_squad-validation-8416", "mrqa_squad-validation-8496", "mrqa_squad-validation-8534", "mrqa_squad-validation-8566", "mrqa_squad-validation-8613", "mrqa_squad-validation-8657", "mrqa_squad-validation-8667", "mrqa_squad-validation-8687", "mrqa_squad-validation-8699", "mrqa_squad-validation-8732", "mrqa_squad-validation-878", "mrqa_squad-validation-879", "mrqa_squad-validation-8839", "mrqa_squad-validation-8939", "mrqa_squad-validation-8984", "mrqa_squad-validation-9040", "mrqa_squad-validation-9074", "mrqa_squad-validation-9249", "mrqa_squad-validation-9265", "mrqa_squad-validation-9331", "mrqa_squad-validation-9578", "mrqa_squad-validation-96", "mrqa_squad-validation-9606", "mrqa_squad-validation-9608", "mrqa_squad-validation-9632", "mrqa_squad-validation-9783", "mrqa_squad-validation-9798", "mrqa_squad-validation-980", "mrqa_squad-validation-9802", "mrqa_squad-validation-9845", "mrqa_squad-validation-9849", "mrqa_squad-validation-988", "mrqa_squad-validation-9966", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-1080", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1201", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-1510", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1719", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-1998", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2377", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-2731", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2967", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3786", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-3955", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4135", "mrqa_triviaqa-validation-4193", "mrqa_triviaqa-validation-4272", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-4469", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-4542", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4586", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-4801", "mrqa_triviaqa-validation-4916", "mrqa_triviaqa-validation-5075", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5203", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5525", "mrqa_triviaqa-validation-5538", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-5676", "mrqa_triviaqa-validation-5777", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6484", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-6661", "mrqa_triviaqa-validation-6700", "mrqa_triviaqa-validation-6799", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6840", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-7080", "mrqa_triviaqa-validation-7132", "mrqa_triviaqa-validation-7172", "mrqa_triviaqa-validation-7187", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-7696", "mrqa_triviaqa-validation-804"], "OKR": 0.8828125, "KG": 0.45625, "before_eval_results": {"predictions": ["America's Funniest Home Videos", "the Romantic Rhine", "philanthropy", "1986", "Republic of Kenya", "PNU and ODM camps", "pseudorandom number generators", "\u20ac5,000", "a baffle plate", "a pound", "Spain", "Havana", "Harriet Beecher Stowe", "MIT Blackjack Team", "a broken woman out on a", "a fibber McGee", "the Bladder", "Pulsed Laser", "Admiral Richard E. Byrd", "Resident Evil", "Lake Mead", "James Earl Ray", "President Lyndon B. Johnson", "the Madding Crowd", "trumpet", "the Boer War", "Nancy Reagan", "Agatha Christie", "Cleveland", "Oscar Wilde", "thesaurus", "the Latin for \"destruction\"", "Lake lake", "Starland Vocal Band", "kerosene", "a squash blossom necklace", "The Tomb of the Unknown Soldier", "Carol Ann Susi", "Upstate New York", "Las Vegas", "Missouri, during the Kirtland period of Latter Day Saint history, circa 1834", "Dirk Benedict", "Priyadarshan", "high-elevation lakes", "Japan", "Paul Allen", "Nigeria", "The Star Spangled Banner", "The Truman Show", "George Gently", "Leslie Knope", "1978", "University of Missouri", "Nicolas Winding Refn", "Osaka International Airport", "$26 billion", "2013\u201314 Premier League", "Pixar's", "Inter Milan", "the fact that the teens were charged as adults.", "if your ex's loved ones ask why you broke up", "Samir Kuntar", "two years ago", "April 21, 1947"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6765252976190477}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true, false, true, false, false, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.14285714285714288, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9161", "mrqa_searchqa-validation-12798", "mrqa_searchqa-validation-14614", "mrqa_searchqa-validation-10745", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-11583", "mrqa_searchqa-validation-4655", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10638", "mrqa_searchqa-validation-4475", "mrqa_searchqa-validation-8444", "mrqa_searchqa-validation-11905", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-1537", "mrqa_triviaqa-validation-173", "mrqa_triviaqa-validation-7392", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1034"], "SR": 0.59375, "CSR": 0.5952380952380952, "EFR": 1.0, "Overall": 0.732172619047619}, {"timecode": 21, "before_eval_results": {"predictions": ["punts", "arrested", "Confucian propriety and ancestor veneration", "5,000 years", "Wells Fargo Center", "immunomodulators", "the RSA algorithm", "the Black Ahab's weapon", "Francis Scott Key", "the 1906 San Francisco earthquake", "piccolo", "how queer everything is today", "Sri Lanka", "Rolling Stone", "Marlon Brando", "Fred Foy", "tears", "The Old Curiosity Shop", "pearl", "Australia", "work work", "Europa", "Pope John XXIII", "the sun", "Mercury", "the 3 Musketeers", "the ball handling infraction", "John Edwards", "Fatah", "turkey-human relationships", "Congo", "the publication of resolutions against Boris Pasternak in the Literaturnaya", "Bombay", "Rome", "lymphatic", "Bed and breakfast", "In 1038", "19 June 2018", "Mahatma Gandhi", "Virginia Dare", "Lionel Hardcastle", "iOS, watchOS, and tvOS", "The Shard", "The Blue Boy", "Allende", "Sue Ryder", "Adrian Edmondson", "sense of taste", "Tony Curtis", "the lead roles", "good", "The Prodigy", "Miss Mulatto, Mani, and Nova", "New Journalism", "Boyd Gaming", "Barbara Niven", "2000", "103", "April 13", "Juan Martin Del Potro.", "the Gaslight Theater", "Anil Kapoor", "killing up to 280,000 people and displacing up to 2 million,", "1964"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6710317460317461}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7219", "mrqa_searchqa-validation-12130", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-5408", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-12504", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-14559", "mrqa_searchqa-validation-14313", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-14613", "mrqa_searchqa-validation-16149", "mrqa_searchqa-validation-6247", "mrqa_searchqa-validation-16301", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2748", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-7058", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-2583", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-639", "mrqa_hotpotqa-validation-325", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-926", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-3501"], "SR": 0.5625, "CSR": 0.59375, "EFR": 1.0, "Overall": 0.7318749999999999}, {"timecode": 22, "before_eval_results": {"predictions": ["Mike Carey", "Rankine cycle", "May 21, 2013", "iger team", "11:28", "1321 to 1323", "AFC", "the Drama Desk Award for Outstanding Lighting Design for \"Cats\", \"Miss Saigon\", and \" Equus\"", "Rafael Palmeiro", "10", "Central Park", "Enkare Nairobi", "PewDiepie", "Thrushcross Grange", "Linda Louise McCartney", "10 June 1921", "A Bug's Life", "Michelle Anne Sinclair", "Angel Parrish", "OSRIC", "Bundesliga", "Alleyne v. United States", "cleaning services, support services, property services, catering services, security services and facility management", "4,972", "Commodore", "Mount Everest", "119", "British", "Red Dead Redemption", "Denver Broncos, Berlin Thunder, Las Vegas Outlaws and Ottawa Renegades", "a neo-v\u00f6lkisch movement within black metal", "Sugar Ray Robinson", "rotund bodies with arching rostrums, V-shaped blowholes and dark gray or black skin", "Jeffrey Adam", "A Rush of Blood to the Head", "a political ideology", "Mendel", "John Adams", "Toto", "Pandit Jawaharlal Nehru", "2014", "James Arthur", "14", "Vanity Fair", "Route sixty-six", "The Welcome Stranger", "Pour Moi", "aardvark", "Jeremy Bates", "a police patrol car", "Sixteen", "Mexico", "Antonie van Leeuwenhoek", "a story", "a broken pelvis", "Brown and her family", "Rocco", "love", "Armistice Day", "a double-headed eagle", "Roger Federer", "The Silence of the Lambs", "Venezuela", "Marie Fredriksson"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6172008030185758}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, true, false, true, false, false, false, false, true, false, true, false, true, true, false, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.375, 0.8, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.47058823529411764, 1.0, 0.25, 1.0, 0.6666666666666666, 1.0, 1.0, 0.10526315789473685, 0.33333333333333337, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-136", "mrqa_squad-validation-3926", "mrqa_hotpotqa-validation-1727", "mrqa_hotpotqa-validation-3629", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-2313", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-1669", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-3964", "mrqa_hotpotqa-validation-1386", "mrqa_hotpotqa-validation-3237", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-2918", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-7692", "mrqa_triviaqa-validation-3951", "mrqa_newsqa-validation-120", "mrqa_newsqa-validation-254", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-1496", "mrqa_searchqa-validation-9888", "mrqa_searchqa-validation-7840", "mrqa_searchqa-validation-6337", "mrqa_searchqa-validation-45", "mrqa_naturalquestions-validation-9195"], "SR": 0.515625, "CSR": 0.5903532608695652, "EFR": 1.0, "Overall": 0.731195652173913}, {"timecode": 23, "before_eval_results": {"predictions": ["Wardenclyffe", "\"a day of rote learning and often wearying spiritual exercises.\"", "Pitt", "the meeting of the Church's General Assembly", "vocational subjects", "CBS", "the Greek Goddess of Revenge", "smell", "Tokyo Prefecture, Japan", "a \u201cbitter almond\u201d smell", "a graffito at Herculaneum", "Sid Ritchie", "8.8/10", "a large collection of stars held together by mutual gravity", "high jump", "milk lactose", "Derbyshire, England", "James Cameron", "Emma Chambers", "Sherlock Holmes", "Spain", "secretary/procurer of drugs", "Sherlock Holmes", "\"Shrine of Democracy,\"", "oxygen", "Alberto juantorena", "William Randolph Hearst", "Henry III of England", "Cream", "Canada", "The Rocketeer", "Paul Maskey", "ambergris", "The Time Machine", "The Lion King", "5 September 1666", "pneumonoultramicroscopicsilicovolcanoconiosis", "William Strauss and Neil Howe", "florida", "Thorleif Haug", "at the fictional elite conservative Vermont boarding school Welton Academy", "1978", "July", "Ghana Technology University College", "Charles Quinton Murphy", "12", "Afghanistan", "Washington, D.C.", "Michael Jordan", "Arnoldo Rueda Medina", "\" waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.\"", "cowardly lion", "a bank", "Eintracht Frankfurt", "Mohamed Anwar al-Sadat", "pattern matching", "\"jazz's\"", "the black hole's extreme tidal forces into a long, thin,", "Canada", "Olivia Newton-John", "soup", "Rene Lacoste", "plaque", "the Vampire Intelligences"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4574997384011076}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.9411764705882353, 0.6666666666666666, 0.0, 1.0, 0.4, 0.2222222222222222, 0.0, 1.0, 0.0, 0.0, 0.06896551724137931, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.2, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-545", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-226", "mrqa_triviaqa-validation-1155", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-181", "mrqa_triviaqa-validation-7071", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-6597", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-4695", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5569", "mrqa_triviaqa-validation-6533", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-4852", "mrqa_naturalquestions-validation-590", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-2509", "mrqa_naturalquestions-validation-5791", "mrqa_naturalquestions-validation-10442", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-2055", "mrqa_hotpotqa-validation-2807", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-914", "mrqa_newsqa-validation-3302", "mrqa_searchqa-validation-8992", "mrqa_searchqa-validation-16390", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-9742", "mrqa_hotpotqa-validation-1516"], "SR": 0.359375, "CSR": 0.5807291666666667, "EFR": 0.975609756097561, "Overall": 0.7243927845528455}, {"timecode": 24, "before_eval_results": {"predictions": ["Department for Culture, Media and Sport", "40,000", "Geordie", "left foot", "there was nothing which would constitute a hazard in unfriendly hands", "Virgil", "Queen Elizabeth I", "Tokyo, Japan", "February", "Loki", "Samuel Johnson", "12.0 litres", "West Point", "33", "Caernarfon", "curling", "Manchester", "boxing", "in 1987", "Joe Willie Kirk", "protects and holds the lungs, heart, trachea, esophagus, endocrine glands, thoracic aorta and the pulmonary artery", "\"Ain't No Mountain High Enough\"", "ambulance driver", "a Tale of Two Cities", "Hampshire", "New Zealand", "U2", "the right ventricle", "Christian Wulff", "tofu Dragon", "a reddish-purple berry that is related to the blueberry and cranberry", "The Jacaranda", "Jesse Garon Presley", "Bedingfield", "epernay", "its population", "gentry Buddhism", "the Mongol Yuan Dynasty", "65,507 bytes", "A turlough", "Health or vitality is an attribute assigned to entities, such as the player character, enemies and objects within a role - playing or video game, that indicates its state in combat", "George Warren Barnes", "The Grandmaster", "Nan Britton", "The Nick Cannon Show", "Marvel Comics", "Taylor Swift", "West Africa", "September 8, 2017", "CNN's Max Foster", "seven", "the Russian air force", "At least 38 people", "about 3,000 kilometers", "Paul McCartney and Ringo Starr", "lightning strikes", "Clifford Odets", "degaussing", "Fargo", "it wasn't meaty enough", "Mahayana", "Washington Irving", "a grizzly bear", "Carrie Underwood"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6097926812770562}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.13333333333333333, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 0.1818181818181818, 0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.125, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1598", "mrqa_triviaqa-validation-588", "mrqa_triviaqa-validation-5958", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-3363", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-2261", "mrqa_triviaqa-validation-3844", "mrqa_triviaqa-validation-328", "mrqa_triviaqa-validation-2092", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-183", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-5809", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-1787", "mrqa_naturalquestions-validation-2205", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-3026", "mrqa_newsqa-validation-3355", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-7", "mrqa_searchqa-validation-14881", "mrqa_searchqa-validation-9185"], "SR": 0.515625, "CSR": 0.578125, "EFR": 1.0, "Overall": 0.72875}, {"timecode": 25, "before_eval_results": {"predictions": ["Gaelic", "to destroy the antichrist", "The Mask of Anarchy", "Japanese", "it is neither zero nor a unit", "jesse pinto da cunha", "7", "azzurri", "rounders", "al bundy", "The Mirror", "south", "aeschylus", "otello", "Jordan", "quiver", "Downton Abbey", "groucho", "raclette", "a lie detector", "Daedalus", "Una Stubbs", "muscle", "a sea otter", "Fringillidae", "All Quiet", "May", "charles bundy", "Paul C\u00e9zanne", "puffer", "A-ha", "a head", "Percy thrower", "All Star", "jesse", "Noel Kahn", "Mitch Murray", "Rajendra Prasad", "an upright triangle", "Alamodome and city of San Antonio", "Chris Rea", "retinal ganglion cell axons and glial cells", "Restoration Hardware", "Khalifa International Stadium", "James Harrison", "Canada's first train robbery", "just off the northwest tip of Canisteo Peninsula in Amundsen Sea", "Bardot", "E22", "booch", "1998.", "Umar Farouk AbdulMutallab", "Lebanese", "the two remaining crew members", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "Tehran,", "China", "pashaluk", "Fritos", "vodka", "j bundy", "jalaneno peppers", "Chernobyl", "cherubim"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5366844654528478}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, false, false, false, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, true, false, true, true, false, true, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.3333333333333333, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2564", "mrqa_squad-validation-6864", "mrqa_triviaqa-validation-2763", "mrqa_triviaqa-validation-948", "mrqa_triviaqa-validation-1097", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-5383", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-5526", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-4377", "mrqa_triviaqa-validation-3506", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-3757", "mrqa_triviaqa-validation-5483", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-3316", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3008", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-1204", "mrqa_newsqa-validation-3097", "mrqa_searchqa-validation-16661", "mrqa_searchqa-validation-14891", "mrqa_searchqa-validation-5387", "mrqa_searchqa-validation-4640", "mrqa_searchqa-validation-7780"], "SR": 0.484375, "CSR": 0.5745192307692308, "EFR": 1.0, "Overall": 0.7280288461538461}, {"timecode": 26, "before_eval_results": {"predictions": ["defensins", "the Earth must be much older than had previously been supposed in order to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea", "5 to 15 years", "pre-game and halftime coverage", "sternum", "new Zealand", "carbon", "ethelbald", "one of the Clarks\u2019 cats", "air masses", "Rajasthan", "ha", "Exile", "Argentina", "yachts", "kia", "power station", "the troposphere", "the Battle of Goose Green", "Ellen Morgan", "charles drake", "Spain", "Richard Noble", "Sweet", "tintoretto", "Earth", "120 beats per minute", "Hans Lippershey", "richmond", "richmond", "Wilkie Collins", "yunte Huang", "Canada", "Baton Rouge", "Laura Jane Haddock", "a man called Lysander", "The Inn at Newport Ranch", "Woodrow Wilson", "neeraj Goswani", "Rumplestiltskin", "Max Martin", "Sean Yseult", "Mount Everest", "18.7 miles", "\"eternal outsider, the sardonic drifter,\" someone who rebels against the social structure", "a coaxial cable with RCA connectors or a fibre optic cable with TOSLINK connectors", "Tony Stewart", "Stage Stores", "at least $20 million to $30 million", "it is arguably working harder than most industries to address the issue of climate change.", "40 militants and six Pakistan soldiers", "Gen. Stanley McChrystal,", "Natalie Cole", "$627,", "CNN affiliate WFTV", "a mot", "The Man Without A Country", "a moon", "Benazir Bhutto", "Dracula", "Ipanema", "a sitar", "the band's logo in gold lettering over black sleeve", "J. Presper Eckert and John William Mauchly's ENIAC"], "metric_results": {"EM": 0.5, "QA-F1": 0.6056210741872508}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.47619047619047616, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.38095238095238093, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0909090909090909, 0.0, 1.0, 0.4444444444444445, 0.09523809523809523, 1.0, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.823529411764706, 0.18181818181818182]}}, "before_error_ids": ["mrqa_squad-validation-5054", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-3022", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-6873", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-7148", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-6840", "mrqa_triviaqa-validation-4242", "mrqa_triviaqa-validation-2360", "mrqa_triviaqa-validation-5472", "mrqa_triviaqa-validation-2870", "mrqa_triviaqa-validation-6043", "mrqa_triviaqa-validation-4663", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-2250", "mrqa_naturalquestions-validation-2839", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-5513", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-469", "mrqa_searchqa-validation-2806", "mrqa_searchqa-validation-2896", "mrqa_searchqa-validation-3888", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3750"], "SR": 0.5, "CSR": 0.5717592592592593, "EFR": 0.9375, "Overall": 0.7149768518518519}, {"timecode": 27, "before_eval_results": {"predictions": ["three hours", "Cosgrove Hall", "American Civil Rights Movement", "the role of backbenchers in their scrutiny of the government and partly to compensate for the fact that there is no revising chamber", "Armin Meiwes", "Katherine Harris", "\"Vera Cruz\" (1954)", "WAMC", "Nikhil Banerjee", "Croatian retired professional basketball player who is currently Special advisor to Jerry Reinsdorf, the owner of the Chicago Bulls.", "Food and Agriculture Organization", "in Kolkata", "2010 to 2012", "Dorothy", "bonobo", "near North Chicago, in Lake County, Illinois", "Harris Museum, Harris Institute or Art School, Harris Technical School and the Harris Orphanage", "Mark Dayton", "Black Panther Party", "Chiba, Japan", "Denmark", "The Sun", "North America, Australia, and India", "October 3, 2017", "Walt Disney and Ub Iwerks", "River Clyde", "Chicago", "\"Red Rock West\"", "the provisional government of Carlos Manuel de C\u00e9spedes y Quesada", "Seventeen", "Kim Sung-su,", "ten years of probation and subsequently ordered him to therapy at a long-term in-patient facility, after his attorneys argued that the teen had affluenza and needed rehabilitation instead of prison", "Outside", "Paul Teutul Jr.", "bypasses, to cross major bridges, and to provide direct intercity connections", "Governor Al Smith", "Charles Carson", "Chris Sarandon", "Beijing", "fourth season", "Schadenfreude", "Poland", "John Mortimer", "norry", "Japanese silvergrass", "daily newspaper", "richmond", "macroscopic, multicellular, marine algae", "the privileged ethnicity,", "Saturday", "\"The Book\"", "Iowa's critical presidential caucuses on January 3.", "partying", "12.3 million people worldwide are \"laboring in bondage.\"", "\"ancient hatreds\" or irrational leaders.", "Doom 3", "Benjamin Franklin", "global village", "Forrest Gump", "leachman", "WD-40 L lubricant", "beef", "hunting", "lemon"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5166636315073815}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6, 0.0, 1.0, 0.4, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7272727272727273, 1.0, 0.4, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5285", "mrqa_squad-validation-9479", "mrqa_hotpotqa-validation-1214", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-1140", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-5227", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8404", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-225", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-4019", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-2723", "mrqa_searchqa-validation-10221", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-6504", "mrqa_searchqa-validation-1507", "mrqa_searchqa-validation-13505", "mrqa_triviaqa-validation-647"], "SR": 0.421875, "CSR": 0.56640625, "EFR": 0.972972972972973, "Overall": 0.7210008445945946}, {"timecode": 28, "before_eval_results": {"predictions": ["Lucas Horenbout", "Barnett Center", "UNESCO's World Heritage list", "punk rock", "Texas", "Argentine cuisine", "Valley Falls", "1942", "Estelle Sylvia Pankhurst", "Rhode Island School of Design", "Battleship", "Arthur Miller", "1916", "Abigail", "Fortean", "Dame Eleen June Atkins", "Toshi Ichiyanagi", "Africa", "jerseys", "Catwoman", "Nassau County", "Duncan Kenworthy", "Mr. Church", "Edward Robert Martin Jr.", "23", "Innviertel", "World War II", "Minette Walters", "Le Divorce", "Princeton University", "Three's Company", "Rungrado 1st", "Linux Format", "1881", "August 17, 1945", "Guy Carawan", "presidential representative democratic republic", "Sir Henry Cole", "Peloponnese", "Mel Gibson", "Barack and Michelle Obama", "Austria", "edward Delius", "iron", "324", "the solar system", "The Word", "James Richter knows the rabbit-ear antennas on his old- fashioned television will listen for a signal and hear nothing.", "Felipe Calderon", "Ronnie White", "Manmohan Singh's", "The woman who received the first-ever near-total face transplant in the United States", "Fernando Caceres", "Veracruz, Mexico", "Gettysburg", "Vermont", "sheep", "Compost", "Abbey Theatre", "Nile", "out-of-body", "United States, NATO member states, Russia and India", "Russian air company Vertikal-T", "Judge Ricardo Urbina"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7134435876623376}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.3, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-4902", "mrqa_hotpotqa-validation-4642", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-1987", "mrqa_triviaqa-validation-167", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-1159", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-2797", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-16562", "mrqa_searchqa-validation-6054", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1563"], "SR": 0.59375, "CSR": 0.5673491379310345, "EFR": 1.0, "Overall": 0.7265948275862069}, {"timecode": 29, "before_eval_results": {"predictions": ["over the winter of 1973\u201374", "smartphones", "three hundred sixty", "Franois Truffaut", "Final Cut", "a good trip", "a film that has a three-dimensional, stereoscopic form or appearance", "a American Tragedy", "a broody hen", "Dairy Queen", "Roger Bacon", "Gene Autry", "a new modifier", "swoven polypropylene", "Sydney", "offbeat", "Alexander Graham Bell", "the Gulf War", "Colorado River", "Sing Sing", "the South Beach diet", "the integument of the upper part of the head, usually including the... a part of this integument with the accompanying hair", "the Intihuatana pyramid", "Phnom Penh", "Fairbanks", "Jay-Z", "Star Trek", "wives and concubines", "Kevin Costner", "Rhode Island", "a mysterious old mansion and its equally fascinating master", "Gunsmoke", "Holiday Inn", "spacewar", "innermost in the eye", "a normally inaccessible mini-game in the 2004 video game Grand Theft Auto : San Andreas, developed by Rockstar North", "Massachusetts", "flawed democracy", "31", "The Lightning thief", "geometer", "Dead Poets", "Diana Ross", "9", "Taiwan", "Sinclair Lewis", "Elton John", "Fort Bragg", "September 23, 1935", "$10.5 million", "\"Dinotopia\"", "Robert Jenrick", "Alien Resurrection", "the arrival of the first Spanish conquistadors in the region of North America now known as Texas in 1519", "new Touch", "543 elected members", "Deutschneudorf", "unknown,", "seven", "Jeffrey Jamaleldine", "30,000", "four", "the titular `` fool '', a solitary figure who is not understood by others, but is actually wise", "November 1961"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6616701007326007}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-526", "mrqa_searchqa-validation-9525", "mrqa_searchqa-validation-1424", "mrqa_searchqa-validation-16759", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-9324", "mrqa_searchqa-validation-4657", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-9950", "mrqa_searchqa-validation-13829", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-5692", "mrqa_searchqa-validation-11919", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-9142", "mrqa_naturalquestions-validation-3332", "mrqa_triviaqa-validation-4803", "mrqa_triviaqa-validation-1650", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-7195", "mrqa_hotpotqa-validation-2786", "mrqa_hotpotqa-validation-4754", "mrqa_newsqa-validation-4050", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-3119"], "SR": 0.578125, "CSR": 0.5677083333333333, "EFR": 1.0, "Overall": 0.7266666666666666}, {"timecode": 30, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-1727", "mrqa_hotpotqa-validation-173", "mrqa_hotpotqa-validation-1740", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1913", "mrqa_hotpotqa-validation-1964", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2313", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-2563", "mrqa_hotpotqa-validation-2583", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2691", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2786", "mrqa_hotpotqa-validation-281", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2927", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-443", "mrqa_hotpotqa-validation-4465", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-468", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4874", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-5015", "mrqa_hotpotqa-validation-5036", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5157", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5760", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-861", "mrqa_hotpotqa-validation-926", "mrqa_hotpotqa-validation-951", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3536", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6199", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-9039", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9142", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-9997", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2093", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2536", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3103", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-4114", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-926", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10275", "mrqa_searchqa-validation-10638", "mrqa_searchqa-validation-10875", "mrqa_searchqa-validation-10931", "mrqa_searchqa-validation-11583", "mrqa_searchqa-validation-1164", "mrqa_searchqa-validation-11905", "mrqa_searchqa-validation-11919", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12504", "mrqa_searchqa-validation-12528", "mrqa_searchqa-validation-12632", "mrqa_searchqa-validation-12798", "mrqa_searchqa-validation-13542", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15193", "mrqa_searchqa-validation-1584", "mrqa_searchqa-validation-15976", "mrqa_searchqa-validation-16301", "mrqa_searchqa-validation-16562", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-1958", "mrqa_searchqa-validation-2179", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-2892", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3159", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-3376", "mrqa_searchqa-validation-3413", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-3607", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-4410", "mrqa_searchqa-validation-45", "mrqa_searchqa-validation-472", "mrqa_searchqa-validation-4755", "mrqa_searchqa-validation-4879", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-5578", "mrqa_searchqa-validation-5787", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-6247", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6489", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-7481", "mrqa_searchqa-validation-7749", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8444", "mrqa_searchqa-validation-8865", "mrqa_searchqa-validation-8992", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-9324", "mrqa_searchqa-validation-9612", "mrqa_searchqa-validation-9637", "mrqa_searchqa-validation-9651", "mrqa_searchqa-validation-9742", "mrqa_searchqa-validation-9812", "mrqa_squad-validation-10024", "mrqa_squad-validation-10078", "mrqa_squad-validation-10097", "mrqa_squad-validation-10395", "mrqa_squad-validation-1042", "mrqa_squad-validation-10427", "mrqa_squad-validation-10433", "mrqa_squad-validation-10444", "mrqa_squad-validation-10471", "mrqa_squad-validation-10493", "mrqa_squad-validation-1052", "mrqa_squad-validation-1078", "mrqa_squad-validation-1138", "mrqa_squad-validation-1304", "mrqa_squad-validation-1445", "mrqa_squad-validation-1496", "mrqa_squad-validation-1541", "mrqa_squad-validation-1598", "mrqa_squad-validation-1637", "mrqa_squad-validation-1833", "mrqa_squad-validation-1850", "mrqa_squad-validation-1862", "mrqa_squad-validation-1975", "mrqa_squad-validation-199", "mrqa_squad-validation-2108", "mrqa_squad-validation-2236", "mrqa_squad-validation-2247", "mrqa_squad-validation-2297", "mrqa_squad-validation-2376", "mrqa_squad-validation-2545", "mrqa_squad-validation-2576", "mrqa_squad-validation-276", "mrqa_squad-validation-2810", "mrqa_squad-validation-2952", "mrqa_squad-validation-3004", "mrqa_squad-validation-3190", "mrqa_squad-validation-3194", "mrqa_squad-validation-3302", "mrqa_squad-validation-3309", "mrqa_squad-validation-332", "mrqa_squad-validation-3398", "mrqa_squad-validation-3436", "mrqa_squad-validation-3524", "mrqa_squad-validation-3525", "mrqa_squad-validation-3577", "mrqa_squad-validation-358", "mrqa_squad-validation-360", "mrqa_squad-validation-3616", "mrqa_squad-validation-3620", "mrqa_squad-validation-3640", "mrqa_squad-validation-3660", "mrqa_squad-validation-3670", "mrqa_squad-validation-3715", "mrqa_squad-validation-3800", "mrqa_squad-validation-3820", "mrqa_squad-validation-3851", "mrqa_squad-validation-3865", "mrqa_squad-validation-387", "mrqa_squad-validation-3871", "mrqa_squad-validation-3926", "mrqa_squad-validation-3957", "mrqa_squad-validation-402", "mrqa_squad-validation-4044", "mrqa_squad-validation-4186", "mrqa_squad-validation-4194", "mrqa_squad-validation-4201", "mrqa_squad-validation-424", "mrqa_squad-validation-4332", "mrqa_squad-validation-4360", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4506", "mrqa_squad-validation-4547", "mrqa_squad-validation-4649", "mrqa_squad-validation-4677", "mrqa_squad-validation-4775", "mrqa_squad-validation-487", "mrqa_squad-validation-4927", "mrqa_squad-validation-4935", "mrqa_squad-validation-494", "mrqa_squad-validation-5054", "mrqa_squad-validation-510", "mrqa_squad-validation-5172", "mrqa_squad-validation-5173", "mrqa_squad-validation-5185", "mrqa_squad-validation-5334", "mrqa_squad-validation-5348", "mrqa_squad-validation-5366", "mrqa_squad-validation-5434", "mrqa_squad-validation-5448", "mrqa_squad-validation-5455", "mrqa_squad-validation-5581", "mrqa_squad-validation-5650", "mrqa_squad-validation-5791", "mrqa_squad-validation-5809", "mrqa_squad-validation-585", "mrqa_squad-validation-5951", "mrqa_squad-validation-5980", "mrqa_squad-validation-6013", "mrqa_squad-validation-6015", "mrqa_squad-validation-6024", "mrqa_squad-validation-6118", "mrqa_squad-validation-6193", "mrqa_squad-validation-6217", "mrqa_squad-validation-6238", "mrqa_squad-validation-629", "mrqa_squad-validation-6337", "mrqa_squad-validation-6382", "mrqa_squad-validation-6638", "mrqa_squad-validation-6677", "mrqa_squad-validation-6698", "mrqa_squad-validation-6703", "mrqa_squad-validation-6787", "mrqa_squad-validation-6805", "mrqa_squad-validation-6833", "mrqa_squad-validation-6874", "mrqa_squad-validation-6891", "mrqa_squad-validation-6996", "mrqa_squad-validation-703", "mrqa_squad-validation-7162", "mrqa_squad-validation-7165", "mrqa_squad-validation-7347", "mrqa_squad-validation-737", "mrqa_squad-validation-7575", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-7647", "mrqa_squad-validation-7653", "mrqa_squad-validation-7670", "mrqa_squad-validation-7715", "mrqa_squad-validation-7724", "mrqa_squad-validation-7747", "mrqa_squad-validation-7850", "mrqa_squad-validation-8002", "mrqa_squad-validation-8068", "mrqa_squad-validation-816", "mrqa_squad-validation-817", "mrqa_squad-validation-8189", "mrqa_squad-validation-8196", "mrqa_squad-validation-824", "mrqa_squad-validation-8374", "mrqa_squad-validation-8416", "mrqa_squad-validation-8534", "mrqa_squad-validation-8687", "mrqa_squad-validation-8732", "mrqa_squad-validation-879", "mrqa_squad-validation-8839", "mrqa_squad-validation-8939", "mrqa_squad-validation-90", "mrqa_squad-validation-9040", "mrqa_squad-validation-9074", "mrqa_squad-validation-9249", "mrqa_squad-validation-9265", "mrqa_squad-validation-9413", "mrqa_squad-validation-9451", "mrqa_squad-validation-9783", "mrqa_squad-validation-9798", "mrqa_squad-validation-9802", "mrqa_squad-validation-9849", "mrqa_squad-validation-9994", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-1210", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1998", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2261", "mrqa_triviaqa-validation-2387", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-247", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-3001", "mrqa_triviaqa-validation-3155", "mrqa_triviaqa-validation-328", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-362", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3786", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4192", "mrqa_triviaqa-validation-4193", "mrqa_triviaqa-validation-4222", "mrqa_triviaqa-validation-4272", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-4542", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4607", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4910", "mrqa_triviaqa-validation-4916", "mrqa_triviaqa-validation-5017", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-5203", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-5614", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6115", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-636", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-647", "mrqa_triviaqa-validation-6688", "mrqa_triviaqa-validation-6700", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6804", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6976", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7080", "mrqa_triviaqa-validation-7087", "mrqa_triviaqa-validation-7187", "mrqa_triviaqa-validation-7207", "mrqa_triviaqa-validation-7223", "mrqa_triviaqa-validation-746", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-804", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-922"], "OKR": 0.890625, "KG": 0.4796875, "before_eval_results": {"predictions": ["increasing access to education", "Arab", "North Carolina", "boats", "safety issues in the company's cars", "High Court Judge Justice Davis", "Roland S. Martin", "the recent theft in Switzerland of two paintings by Pablo Picasso, Bjoern Quellenberg, a spokesman for the Kunsthaus,", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry", "The U.S.-Mexico border", "a music video on his land.", "Monterrey", "striker", "The government late Tuesday announced it would file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Long troop deployments", "60 euros -- $89 -- for shoes that are also worn by dogs who walk on ice in Alaska.", "the estate with its 18th-century sights, sounds, and scents", "27", "Amanda Knox's aunt", "\"falling space debris,\"", "Chinese tourists", "Cameron-Ritchie", "The Transportation Security Administration", "the Gulf", "Booches Billiard Hall", "a collapsed apartment building in Cologne, Germany", "skull", "40", "Democratic VP candidate", "1831", "Daniel Radcliffe", "Conway", "Nick Adenhart", "a ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight", "the nucleus", "the Constitution of India came into effect on 26 January 1950 replacing the Government of India Act ( 1935 ) as the governing document of India", "the cavities and surfaces of blood vessels and organs throughout the body", "The mixing of sea water and fresh water", "a nearly - identical `` non- driver identification card '' to identify persons who are unable or don't want to drive", "Gina Tognoni / to\u028an\u02c8jo\u028ani / ( born November 28, 1973 )", "Dilbert", "dpurves Member", "chui chuan", "George III", "chICAGO", "algebra", "Rosslyn Chapel", "acid house", "Oahu", "at Thrushcross Grange", "Rigoletto", "Mudvayne", "The Hertz Corporation", "Elliot Fletcher", "Frankenstein", "Berkeley", "Robert Devereux", "the Wessex", "Wyoming", "Mall of America", "Mayo", "Joseph Cheshire Cotten", "Bangkok", "Jimmy Ellis"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5871594691151143}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.4, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.9393939393939393, 0.0, 0.5714285714285715, 0.25, 1.0, 0.0, 0.7499999999999999, 0.1111111111111111, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6451612903225806, 1.0, 0.4444444444444445, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-689", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-247", "mrqa_newsqa-validation-2271", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-1440", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-419", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3729", "mrqa_newsqa-validation-2042", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-2556", "mrqa_triviaqa-validation-268", "mrqa_triviaqa-validation-2395", "mrqa_triviaqa-validation-285", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-5157", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-4972", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-7634", "mrqa_searchqa-validation-9692", "mrqa_hotpotqa-validation-5460"], "SR": 0.46875, "CSR": 0.564516129032258, "EFR": 0.9705882352941176, "Overall": 0.7213177478652751}, {"timecode": 31, "before_eval_results": {"predictions": ["1969", "Highly combustible materials", "the monsoons", "2002", "Joanne Wheatley", "Clarence Williams", "invertebrates", "the southernmost tip of the South American mainland", "Stephen Graham", "warplanes", "The Drew Las Vegas", "1957", "Kentucky", "arthropods", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "1648 - 51 war", "the breast or lower chest of beef or veal", "1933", "Jason Lee", "southern Anatolia", "halogenated paraffin hydrocarbons that contain only carbon, chlorine, and fluorine", "Professor Eobard Thawne", "Dr. Lexie Grey", "Number 4, Privet Drive, Little Whinging in Surrey, England", "1939", "Alex Skuby", "the University of Oxford", "10,000 BC", "Bacon", "November 27, 2013", "In the television series's fourth season", "a single underlying concept", "Action Jackson", "giraffe", "1883", "Dyfed powys", "Elizabeth", "Indonesia", "crow", "johnny", "Cannes Film Festival", "Conservative Party", "1967", "Hjernevask", "corn", "4,000", "1966", "Our daughter", "\"The most affecting thing about this whole wheelchair for children is when the parents realize the gift that is being given to their children and they reach out to hug you.\"", "\"We need to know if they have been abused or neglected.\"", "bipartisan rhetoric", "early detection and helping other women cope with the disease.\"", "rising disposable income and an increasing interest in leisure pursuits,", "16", "Stephen Dedalus", "Omaha", "Communist Party", "ice hockey", "johnny tester", "Cock Robin", "toothbrush", "trans-Pacific flight", "Davos Seaworth", "Phil Collins"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5421382751665259}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, true, false, true, true, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.07999999999999999, 1.0, 1.0, 1.0, 0.0, 0.9, 0.8, 0.6666666666666666, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 0.0, 1.0, 1.0, 0.7272727272727272, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.10526315789473685, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3491", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-1133", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-9323", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-6374", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5473", "mrqa_hotpotqa-validation-5", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-491", "mrqa_searchqa-validation-1451", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-6195", "mrqa_searchqa-validation-4784", "mrqa_searchqa-validation-6554", "mrqa_hotpotqa-validation-3464"], "SR": 0.4375, "CSR": 0.560546875, "EFR": 0.8888888888888888, "Overall": 0.7041840277777778}, {"timecode": 32, "before_eval_results": {"predictions": ["best, worst and average case complexity", "via pores in the epidermis", "Charles Lebrun", "Ajay Tyagi", "the Earth", "28 %", "West Norse sailors", "a single childbirthidymal tubule ( luminal diameter. 15 -. 25 mm ) to the lumen", "plant", "Andrew Lloyd Webber", "the Kansas City Chiefs", "John J. Flanagan", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "Ra\u00fal Eduardo Esparza", "In 1945", "Milira", "1989", "Ledger", "New England Patriots XX, XXXI, XXXVI, XXXVIII", "USS Chesapeake", "Christopher Lloyd", "near the Afghan - Pakistan border, from central Afghanistan to northern Pakistan", "Pakistan", "a combination of genetics and the male hormone dihydrotestosterone", "Nazi Germany and Fascist Italy", "the remaining surface of the enamel", "Rocinante", "the words spoken to Adam and Eve after their sin, reminds worshippers of their sinfulness and mortality and thus, implicitly, of their need to repent in time", "Alastair Cook", "Brad Dourif", "Robert Irsay", "a federal republic in which the president, Congress, and federal courts share powers reserved to the national government according to its Constitution", "Phil Simms", "a tittle", "Fred", "Haute Qualit\u00e9 Environnementale", "Otto von Bismarck", "a moustache", "(Vince) Lombardi", "Sid Vicious", "\"Kids\"", "Toby Kennish", "Free Range Films", "Gambaga", "a label used to indicate that a product was made in Switzerland", "in their home country", "Hermione Youlanda Ruby Clinton-Baddeley", "can I", "his comments while Saudi authorities discuss whether he should be charged with a crime, according to local media.", "peanuts, nuts, shellfish, peanuts, tree nuts, wheat and soy", "Sunday", "barter -- trading goods and services without exchanging money", "the area was sealed off, so they did not know casualty figures.", "Bob Bogle,", "(Count von) Zeppelin", "(Henry) Tudor,", "Tsingtao Lager", "a false name", "a bats", "Galileo", "Life Among the Lowly", "Charlotte Gainsbourg and Willem Dafoe", "2,000 euros ($2,963)", "Seasons of My Heart"], "metric_results": {"EM": 0.5, "QA-F1": 0.5942843614718615}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, true], "QA-F1": [0.25, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1904761904761905, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.3333333333333333, 1.0, 0.08333333333333333, 0.19999999999999998, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1700", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-251", "mrqa_naturalquestions-validation-832", "mrqa_triviaqa-validation-2415", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-599", "mrqa_hotpotqa-validation-2482", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-461", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-1778", "mrqa_searchqa-validation-2860", "mrqa_searchqa-validation-4471", "mrqa_searchqa-validation-13843", "mrqa_searchqa-validation-9195", "mrqa_searchqa-validation-16579"], "SR": 0.5, "CSR": 0.5587121212121212, "EFR": 1.0, "Overall": 0.7260392992424242}, {"timecode": 33, "before_eval_results": {"predictions": ["Paul Nash", "ten", "Charles Perrault", "2", "Arousal regulation", "ten", "Bachendri Pal", "7 July", "usually in May", "Arthur Chung", "The Outback", "Andy Serkis", "France", "Sean O'Neal", "Himadri Station", "257,083", "to ensure party discipline in a legislature", "Empiricism", "beneath the liver", "George II", "1920s", "the cardiac pacemaker", "Ye Hai Mohabbatein", "May 26, 2017", "as bridesmaids to the Marriage of Mercury and Philology", "Mason Alan Dinehart", "federal law intended to check the president's power to commit the United States to an armed conflict without the consent of the U.S. Congress", "Buffalo Lookout", "Lykan", "Geoffrey Dyson Palmer", "ice giants", "March 26, 1973", "1991", "Pete Seeger", "switzerland", "1915", "bankside power station", "onion", "trombone", "12", "T. R. M. Howard", "Route 37 East", "Robert John Day", "Do Kyung-soo", "Teotihuacan", "Travis County", "1872", "no other designer has had a greater impact,", "Arsene Wenger", "Derek Mears", "Bob Bogle", "Miguel Cotto", "give peace to my nation.", "Pakistani officials,", "the Nile", "Pushing Daisies", "Dirty Diana", "shiner", "Sir Isaac Newton", "Nefertiti", "nonfat", "400 years", "The show which looks at how children as young as eight would cope without their parents for two weeks.", "A receptionist"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6498295454545454}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7999999999999999, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.08, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5520", "mrqa_naturalquestions-validation-5909", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-7312", "mrqa_naturalquestions-validation-8937", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-9450", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-6579", "mrqa_triviaqa-validation-7403", "mrqa_triviaqa-validation-7748", "mrqa_hotpotqa-validation-3294", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-1042", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-3076", "mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-15769", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-2000"], "SR": 0.578125, "CSR": 0.5592830882352942, "EFR": 0.9259259259259259, "Overall": 0.7113386778322439}, {"timecode": 34, "before_eval_results": {"predictions": ["Warfare and the long occupation", "extremely high", "1986", "the name used by Union forces", "Walter Mondale", "Vienna", "humid subtropical climate, with hot summers and mild winters", "A footling breech", "Irish", "John McConnell", "the times sign or the dimension sign", "the liver and kidneys", "If These Dolls Could Talk", "1961 to 1989", "the lateral side of the tibia, with which it is connected above and below", "the university's science club", "her relationship with Bob Dylan", "Malware", "Konakuppakatil Gopinathan Balakrishnan", "May 19, 2008", "the IB Diploma Program and the IB Career - related Program for students aged 15 to 18", "Jack Barry", "Catherine Tramell", "In the year 2026", "more than a million", "March 23, 2018", "subduction zone", "Jacques Cousteau", "1960", "`` Fix You ''", "minced meat ( commonly beef when named cottage pie or lamb when named shepherd's pie )", "men use violence within relationships to exercise power and control", "the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "Imola", "Wigan", "a person trained for travelling in space.", "Dante Alighieri", "David Hockney", "\"Li'l Abner,\"", "Tokyo", "the Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide", "Irish Chekhov", "Cody Miller", "Mary Elizabeth Hartman", "ten", "the Tallahassee City Commission", "Roosevelt Island", "air support.", "about 1 percent of children ages 3 to 17 have PSP or a related disorder, an increase over previous estimates.", "Sabina Guzzanti", "two Israeli soldiers,", "large accumulations of ice in places such as the north Georgia mountains,", "The American Civil Liberties Union", "the Zimbabwean government", "the brig Jolly Roger", "baldness", "parliaments", "Archer-Daniels-Midland Co", "the potato", "The History of the World", "the Leyden jar", "14", "Jeanne Tripplehorn's", "Festival Foods in Kansas City, Missouri,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6284756042568542}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444444, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.5454545454545454, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.5333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-9387", "mrqa_triviaqa-validation-2357", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-3689", "mrqa_hotpotqa-validation-3633", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1018", "mrqa_newsqa-validation-664", "mrqa_searchqa-validation-14874", "mrqa_searchqa-validation-9057", "mrqa_searchqa-validation-8834", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-4177"], "SR": 0.546875, "CSR": 0.5589285714285714, "EFR": 0.9655172413793104, "Overall": 0.7191860375615764}, {"timecode": 35, "before_eval_results": {"predictions": ["algorithms have been written that solve the problem in reasonable times in most cases", "1964", "a growing percentage of the Somali population has become dependent on humanitarian aid.", "the Internet", "further reconciliation among Sunnis, Shiites and tribes of different sects and bring some former members of Saddam Hussein's Baath party into the political fold.", "more than 4,000", "Nasser Medical Institute", "an annual road trip,", "a different picture from the one described by former CIA officer John Kiriakou.", "the shoreline of the city of Quebradillas.", "Wednesday night", "his acute and visionary observation of contemporary life was distilled into a number of brilliant, powerful novels, which have been published all over the world.\"", "iCloud service", "gun charges,", "UNICEF", "dismissed all charges", "Peterson had his personal.40-caliber pistol,", "Kris Allen,", "Janet and La Toya,", "co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "\"fusion teams,\"", "They're big, strong, and fierce -- and they wear little blue booties.", "move out of her rental house", "one", "NATO fighters", "an estimated 750", "one", "black, red or white,", "Cambodia.", "October 29 and November 5", "more than 9,500 energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals.", "two and a half hours", "the tourist industry,", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "his temporary departure from the Beatles in January 1969", "Tokyo", "Madison's proposed Virginia Plan", "Judith Cynthia Aline Keppel", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "Convention's first act, on 10 August 1792, was to establish the French First Republic and officially strip the king of all political powers", "Fu\u00dfball-Bundes", "Elvis Presley", "a new generation of infant transport", "Indy", "Nadia Comaneci", "Florida", "an older man about 50 and a young fellow about 24", "1862", "World War I", "Meghan Markle", "the mid-1980s by DJs from Chicago", "Air Refueling Squadron", "South African", "1754", "a lemon chiffon", "Artemis", "rodeo", "a canticle", "the Sun Also Rises", "a beaver", "Tequila Sunrise", "Hawley Harvey Crippen", "Vladivostok", "cussler"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5102234355421618}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.7499999999999999, 1.0, 0.06451612903225806, 0.0, 1.0, 0.07692307692307691, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.35294117647058826, 0.8, 0.0, 0.3636363636363636, 0.23529411764705882, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.8571428571428571, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-540", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-856", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-755", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-2980", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-10311", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-4688", "mrqa_triviaqa-validation-68", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-7594", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-3028", "mrqa_hotpotqa-validation-4603", "mrqa_hotpotqa-validation-1034", "mrqa_searchqa-validation-4930", "mrqa_searchqa-validation-6852", "mrqa_searchqa-validation-9366", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-4322"], "SR": 0.390625, "CSR": 0.5542534722222222, "EFR": 0.9230769230769231, "Overall": 0.709762954059829}, {"timecode": 36, "before_eval_results": {"predictions": ["harassment and, at least to the bystander, somewhat inane", "2014 Winter Olympics in Sochi, Russia", "Sleeping with the Past", "Smyrna", "John Barry", "2018", "2020 National Football League ( NFL ) season", "a Norwegian town circa 1879", "in the 18th century", "before the first year begins", "Vicente Fox", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "the Speaker or, in his absence, by the Deputy Speaker", "Supplemental oxygen", "1961", "approximately 11 %", "his cousin D\u00e1in", "1933", "Earle Hyman", "alpha efferent neurons", "water on the ground surface enters the soil", "chain elongation", "1", "the team that lost the pre-game coin toss", "Thomas Edison", "In the television series's fourth season", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "the Senate at large", "September 4, 2000 to February 25, 2003", "somatic cell nuclear transfer", "a leonine contract, a take - it - or - leave - it contract, or a boilerplate contract", "Eddie Murphy", "Payson, Lauren, and Kaylie", "lisping Violet- Elizabeth Bott", "Johnnie L. Cochran, Jr.", "a pieman", "Singapore", "California", "bambi II", "potash", "Rob Reiner", "Byron De La Beckwith, Sr.", "Lombardy", "Yasiin Bey", "FIFA Women's World Cup", "University of Texas Longhorns football", "Carol Ann Duffy", "2000,", "left hundreds of messages in languages ranging from French and Spanish to Japanese and Hebrew.", "how health care can affect families.", "the family's blog", "Port-au-Prince, Haiti,", "more than 200.", "1959.", "Luxor Las Vegas Hotel", "Tennessee", "Vatican City", "Thomas Edison", "Gulf of Tonkin", "amor", "Saint Telemachus", "Fareed Zakaria", "three", "engaging with the Taliban in Pakistan and Afghanistan."], "metric_results": {"EM": 0.34375, "QA-F1": 0.528342205624015}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, true, false, true, false, true, true, false, false, true, true, false], "QA-F1": [0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.5, 0.4444444444444445, 0.6666666666666666, 0.8, 0.0, 1.0, 0.7878787878787877, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.7272727272727272, 0.33333333333333337, 0.0, 0.6, 1.0, 0.3137254901960785, 1.0, 0.4, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 0.6666666666666666, 0.0, 0.4444444444444444, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.17391304347826086]}}, "before_error_ids": ["mrqa_squad-validation-7014", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-9371", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-5006", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3187", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-1420", "mrqa_triviaqa-validation-7394", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-3547", "mrqa_hotpotqa-validation-257", "mrqa_hotpotqa-validation-2390", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-2855", "mrqa_searchqa-validation-15743", "mrqa_searchqa-validation-15986", "mrqa_searchqa-validation-9778", "mrqa_searchqa-validation-10986", "mrqa_newsqa-validation-4175"], "SR": 0.34375, "CSR": 0.5485641891891893, "EFR": 0.8809523809523809, "Overall": 0.7002001890283139}, {"timecode": 37, "before_eval_results": {"predictions": ["warmer climate", "Lauren Taylor", "Chris Martin", "Gavin DeGraw", "the field is limited to drivers who meet more exclusive criteria", "James `` Jamie '' Dornan", "2012", "Games played", "2013", "Sunday night", "President pro tempore of the Senate", "Jonathan Breck", "118", "landowner", "Madison, Wisconsin, United States", "England and Wales", "1994 season", "Kingsford, Michigan", "a sweet alcoholic drink made with rum, fruit juice, and syrup or grenadine", "head - up", "Austria - Hungary", "Lana Del Rey", "Paracelsus", "George Warren Barnes", "ESPN", "If the car is slowed initially by manual use of the automatic gear box", "American", "1996", "the Social Security Act of 1935", "Andr\u00e9 3000", "Celtic", "Latitude", "Soviet Russia defaulted on all of Imperial Russia's commitments to the Triple Entente alliance", "the Lake", "the nine of diamonds", "Poland", "Appaloosa", "enzootic", "Lichfield Cathedral", "charlie lloyd", "Bocelli became completely blind at the age of 12", "Pandosia", "Spain", "Ronald Joseph Ryan", "2,099", "1969 until 1974", "the fourth President of Pakistan from 1971 to 1973", "Melbourne", "five", "Trevor Rees", "three out of four", "1998.", "international search team", "Opry Mills,", "Marion", "Frankfort", "Doc Holliday", "a mosquito repellent", "the Backstreet Boys", "the Muscular Dystrophy Association", "Easter Seals", "Cilla black", "the daily mirror", "yellow"], "metric_results": {"EM": 0.5, "QA-F1": 0.6203955081833371}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, false, true, false, true, true, true, false, false, true, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.25, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 0.0, 0.0, 1.0, 0.5, 0.8, 0.0, 0.7368421052631579, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.09523809523809525, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3595", "mrqa_naturalquestions-validation-8308", "mrqa_naturalquestions-validation-8272", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-2229", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-750", "mrqa_naturalquestions-validation-3022", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-7138", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-5444", "mrqa_triviaqa-validation-2011", "mrqa_triviaqa-validation-2275", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-2818", "mrqa_hotpotqa-validation-1513", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-4940", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-1301", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-10393", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-3357"], "SR": 0.5, "CSR": 0.5472861842105263, "EFR": 0.96875, "Overall": 0.7175041118421053}, {"timecode": 38, "before_eval_results": {"predictions": ["support from China for a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda, as well as a nearly $1.8 billion dam\"", "lemur", "Man Ray", "Billy Bob Thornton", "Oprah Winfrey", "1957", "Italy", "calcium", "Alpha waves", "savings account", "Meyer Lansky", "Iraq", "(15) Sir Laurence de Warren", "Live Free or Die Hard", "balmer", "Alexander Solzhenitsyn", "The Fugitive (1993)", "Sisyphus", "INXS", "A Few Good Men", "farewell", "Fyodor Dostoevsky", "Elizabeth Barrett Browning (1806-1861)", "Quiz Show", "sculpere", "a Viking warship", "(William) Inge", "Jezebel", "Barbara Walters", "Qualcomm", "Roz Doyle (Gilpin)", "The Brady Bunch", "1789", "Norman victory", "1984", "on the shore of Lake Erie in downtown", "in the Rocky Mountains in southwestern Colorado and northwestern New Mexico", "the golden age of India", "13", "April 12, 2017", "Atlantic Ocean", "Dylan Thomas", "Aberdeen", "Mnemosyne", "Dakar, Senegal", "Monopoly", "gilda", "Kentucky Wildcats", "William Shakespeare", "\"Secrets and Lies\"", "26,000", "drawings and approximately 1 million old master prints", "A Bug's Life", "SpongeBob SquarePants 4-D", "10 below in Chicago, Illlinois.", "federal officers' bodies", "reducing\" greenhouse emissions.", "UNICEF", "28", "state senators who will decide whether to remove him from office heard him loud and clear on FBI recordings of his phone calls.", "24 Rezai", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "at the age of seven", "2 September 1990"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6583751407369622}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [0.8292682926829268, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.16, 0.0, 0.967741935483871, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8452", "mrqa_searchqa-validation-7060", "mrqa_searchqa-validation-1385", "mrqa_searchqa-validation-9483", "mrqa_searchqa-validation-13127", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-8357", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-15072", "mrqa_searchqa-validation-170", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-2098", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-720", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-2931", "mrqa_triviaqa-validation-1375", "mrqa_hotpotqa-validation-5799", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-156", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-3118", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-3283", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-6641", "mrqa_naturalquestions-validation-4462"], "SR": 0.53125, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.723671875}, {"timecode": 39, "before_eval_results": {"predictions": ["the Bible", "Raimond Gaita", "Fife", "Isabella Hedgeland", "Daniel Andre Sturridge", "The 2016 Oklahoma Sooners football team", "Overijssel, Netherlands", "20th Century Fox", "1972", "epic verse", "Continental AG", "1976", "Mick Jackson", "Bishop's Stortford", "The Fault in Our Stars", "Bhushan Patel", "Kassie DePaiva", "Harriet Tubman", "Vernon Kay", "the Royal Air Force (RAF)", "Saturday Night Live", "The Dayton Memorial Hall", "\"51 Heroes of Aviation\"", "1976", "\u00c6thelstan", "three", "KBS2", "The Indianapolis Motor Speedway", "Houston Rockets", "three", "Henry II", "63 mph", "Maria Szraiber", "collect a jackpot prize in cash or annuity", "Inequality of opportunity", "Marley & Me", "1981", "a limited period of time", "1946", "Oregon and Washington", "Cambridge", "squash", "Sotheby\u2019s", "scoop", "Bart\u00f3k", "Niger", "joey", "Mohamed Alanssi,", "more than 1.2 million people.", "33-year-old", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "propofol", "Atlanta's Hartsfield-Jackson International Airport", "Judge Herman Thomas", "Dr. Quinn", "Reine de Saba", "Johnny Weissmuller", "PLAYS & PLAYWRIGHTS", "a jazz saxophonist", "the Hudson", "Twin-lens reflex", "Irsay", "1962", "Dwight Clark"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5233096764346764}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, true, false, true, false, false, true, false, false, true, false, true, false, false, false, true, false, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.4, 0.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.7777777777777778, 1.0, 1.0, 1.0, 0.30769230769230765, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-4423", "mrqa_hotpotqa-validation-4300", "mrqa_hotpotqa-validation-859", "mrqa_hotpotqa-validation-4691", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-487", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-3745", "mrqa_hotpotqa-validation-3053", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-8465", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-6370", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-3593", "mrqa_searchqa-validation-10066", "mrqa_searchqa-validation-13694", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-8329", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-4169"], "SR": 0.40625, "CSR": 0.543359375, "EFR": 0.9736842105263158, "Overall": 0.7177055921052631}, {"timecode": 40, "UKR": 0.70703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1214", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2583", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3026", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3279", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3472", "mrqa_hotpotqa-validation-3610", "mrqa_hotpotqa-validation-3837", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-402", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4664", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5015", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5157", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-5367", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5584", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-763", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-963", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1418", "mrqa_naturalquestions-validation-1437", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3666", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-6093", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-6641", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8868", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9553", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9975", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1240", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1409", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-2661", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4123", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-787", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10143", "mrqa_searchqa-validation-10963", "mrqa_searchqa-validation-10986", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-11544", "mrqa_searchqa-validation-11578", "mrqa_searchqa-validation-11583", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12154", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12528", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13484", "mrqa_searchqa-validation-13505", "mrqa_searchqa-validation-13843", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-1419", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14243", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-1484", "mrqa_searchqa-validation-14874", "mrqa_searchqa-validation-14891", "mrqa_searchqa-validation-15743", "mrqa_searchqa-validation-15769", "mrqa_searchqa-validation-16390", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-16850", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2860", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-301", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-3607", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-4159", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-45", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4746", "mrqa_searchqa-validation-4784", "mrqa_searchqa-validation-5030", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-5578", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6430", "mrqa_searchqa-validation-6852", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-7634", "mrqa_searchqa-validation-7652", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8964", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9057", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-9324", "mrqa_squad-validation-10004", "mrqa_squad-validation-10059", "mrqa_squad-validation-1006", "mrqa_squad-validation-10112", "mrqa_squad-validation-10140", "mrqa_squad-validation-1016", "mrqa_squad-validation-10232", "mrqa_squad-validation-10433", "mrqa_squad-validation-10503", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1312", "mrqa_squad-validation-1462", "mrqa_squad-validation-1509", "mrqa_squad-validation-1570", "mrqa_squad-validation-158", "mrqa_squad-validation-1634", "mrqa_squad-validation-1703", "mrqa_squad-validation-1965", "mrqa_squad-validation-199", "mrqa_squad-validation-2086", "mrqa_squad-validation-2160", "mrqa_squad-validation-2251", "mrqa_squad-validation-2315", "mrqa_squad-validation-2376", "mrqa_squad-validation-2591", "mrqa_squad-validation-2752", "mrqa_squad-validation-2916", "mrqa_squad-validation-2985", "mrqa_squad-validation-3223", "mrqa_squad-validation-3230", "mrqa_squad-validation-3269", "mrqa_squad-validation-34", "mrqa_squad-validation-3416", "mrqa_squad-validation-3492", "mrqa_squad-validation-3556", "mrqa_squad-validation-3581", "mrqa_squad-validation-360", "mrqa_squad-validation-3610", "mrqa_squad-validation-3611", "mrqa_squad-validation-366", "mrqa_squad-validation-3660", "mrqa_squad-validation-3667", "mrqa_squad-validation-3670", "mrqa_squad-validation-3678", "mrqa_squad-validation-3693", "mrqa_squad-validation-3711", "mrqa_squad-validation-3851", "mrqa_squad-validation-3957", "mrqa_squad-validation-3986", "mrqa_squad-validation-4044", "mrqa_squad-validation-4179", "mrqa_squad-validation-4360", "mrqa_squad-validation-4403", "mrqa_squad-validation-4421", "mrqa_squad-validation-4750", "mrqa_squad-validation-494", "mrqa_squad-validation-509", "mrqa_squad-validation-5147", "mrqa_squad-validation-5275", "mrqa_squad-validation-5375", "mrqa_squad-validation-545", "mrqa_squad-validation-5455", "mrqa_squad-validation-5456", "mrqa_squad-validation-5502", "mrqa_squad-validation-5581", "mrqa_squad-validation-5753", "mrqa_squad-validation-6024", "mrqa_squad-validation-6034", "mrqa_squad-validation-6382", "mrqa_squad-validation-6565", "mrqa_squad-validation-6653", "mrqa_squad-validation-6703", "mrqa_squad-validation-6787", "mrqa_squad-validation-6852", "mrqa_squad-validation-703", "mrqa_squad-validation-7037", "mrqa_squad-validation-7047", "mrqa_squad-validation-7096", "mrqa_squad-validation-7125", "mrqa_squad-validation-7137", "mrqa_squad-validation-7252", "mrqa_squad-validation-7276", "mrqa_squad-validation-7347", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-758", "mrqa_squad-validation-764", "mrqa_squad-validation-7683", "mrqa_squad-validation-7701", "mrqa_squad-validation-7715", "mrqa_squad-validation-7850", "mrqa_squad-validation-7976", "mrqa_squad-validation-8002", "mrqa_squad-validation-8033", "mrqa_squad-validation-8068", "mrqa_squad-validation-8134", "mrqa_squad-validation-816", "mrqa_squad-validation-82", "mrqa_squad-validation-8231", "mrqa_squad-validation-8278", "mrqa_squad-validation-8319", "mrqa_squad-validation-8332", "mrqa_squad-validation-8338", "mrqa_squad-validation-8370", "mrqa_squad-validation-8374", "mrqa_squad-validation-8452", "mrqa_squad-validation-8476", "mrqa_squad-validation-8699", "mrqa_squad-validation-8723", "mrqa_squad-validation-878", "mrqa_squad-validation-8796", "mrqa_squad-validation-8872", "mrqa_squad-validation-8984", "mrqa_squad-validation-8987", "mrqa_squad-validation-9074", "mrqa_squad-validation-9304", "mrqa_squad-validation-9311", "mrqa_squad-validation-9372", "mrqa_squad-validation-9516", "mrqa_squad-validation-9606", "mrqa_squad-validation-9798", "mrqa_triviaqa-validation-10", "mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-1124", "mrqa_triviaqa-validation-1210", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1510", "mrqa_triviaqa-validation-1528", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1753", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1848", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2288", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2763", "mrqa_triviaqa-validation-2772", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2931", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3031", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-3757", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3786", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4542", "mrqa_triviaqa-validation-4560", "mrqa_triviaqa-validation-4607", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-4937", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-5383", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5554", "mrqa_triviaqa-validation-5812", "mrqa_triviaqa-validation-5849", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-647", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-7172", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-7748", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-92"], "OKR": 0.81640625, "KG": 0.48046875, "before_eval_results": {"predictions": ["St. George's United Methodist Church", "in San Francisco", "Kirstjen Nielsen", "6 - 7 % average GDP growth annually", "source code", "English law", "2013", "Fleetwood Mac", "Ric Flair", "the French firm R2E Micral", "October 27, 2017", "31 December 1600", "1608", "1260 cubic centimeters ( cm )", "the internal reproductive anatomy", "December 24, 1836", "the environment", "Franklin and Wake counties", "seven", "byte - level", "the nerves and ganglia", "Danish", "Hermann Ebbinghaus", "Sam Waterston", "on the southeastern coast of the Commonwealth of Virginia in the United States", "March 11, 2018", "Stephen A. Douglas", "HTTP / 1.1", "Puerto Rico", "the computer", "a gestational age of approximately 7 weeks and 5 days", "the Tin Woodman", "Schadenfreude", "tahrir Square", "Purple Rain", "Phaethon", "Northumberland", "Eric Blair", "Cubism", "the Metropolitan Borough of Oldham", "philosopher professor and writer", "Londonderry", "St. Louis, Missouri", "Chelmsford", "national parliaments", "Mary Harron", "The Hotpoint Electric Heating Company", "staff sergeant", "Dennis Davern", "ordered the makers of certain antibiotics to add a \"black box\" label warning", "Zimbabwe", "Islamic militants", "boatlift", "he was diagnosed with skin cancer.", "The Brave Little Toaster", "John Paul Jones", "a halogen", "Edinburgh", "Mrs. Doubtfire", "arsenic", "Detroit Rock City", "2004", "Katherine Harris", "Liberty Enlightening the World"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7219681054239878}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.5, 0.11764705882352941, 1.0, 0.0, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.07142857142857142, 1.0, 0.6666666666666666, 0.0, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10030", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-7458", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-8116", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-6822", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-37", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-654", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3655", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-9233"], "SR": 0.609375, "CSR": 0.5449695121951219, "EFR": 1.0, "Overall": 0.7097751524390243}, {"timecode": 41, "before_eval_results": {"predictions": ["manually suppress the fire", "John Spinks", "Kida", "Tom Bower", "Thomas Jefferson", "Krypton", "duodenum", "left - sided heart failure", "July -- October 2012", "Abbot Suger", "Filipino American History Month", "1997", "9 February 2018", "re-education", "season ten", "out of RAF Coningsby in Lincolnshire", "Louis XV", "20 year - old Kyla Coleman", "Carthaginians'Phoenician ancestry", "Kennedy Space Center ( KSC ) in Florida", "a legal case in certain legal systems written by one or more judges expressing disagreement with the majority opinion of the court which gives rise to its judgment", "painting", "Nicholas Sparks", "Road / Track", "IBM", "1997", "Effy ended up flatting with Naomi", "its common name derived from its empirical formula, P O )", "ex-lover who did the protagonist wrong", "spiritual ideas, virtues and the essence of scriptures", "Geraldine Margaret Agnew", "Sir Hugh Beaver", "November 1999", "st pauls", "Norman Hartnell", "becoming bald or fear of being around bald people.", "bitter liqueurs", "2 1/2", "cactus", "gin", "Chrysler Automobile N.V.", "Kentwood, Louisiana", "King James II of England (James VII of Scotland)", "Anah\u00ed Giovanna Puente Portilla de Velasco", "June 9, 2015", "Revolver", "The Dragon School", "bankruptcies", "President Pervez Musharraf", "Rihanna", "climatecare, one of Europe's most experienced providers of carbon offset,", "Leo Frank,", "2006", "Indian Ocean", "Patrick Dempsey", "\"On Monsters: Or, Nine or More (Monstrous) Not Cannies\"", "caine", "a lake located about 15 miles (24 km) east of Lake Wales,", "FDR", "Anaheim", "a place name", "\" tests\" of President Obama that Joe Biden warned about during the campaign.", "for security reasons and not because of their faith.", "a house party in Crandon, Wisconsin,"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5361183805418719}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5714285714285715, 0.13333333333333336, 1.0, 0.2758620689655173, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.7142857142857143, 0.2857142857142857, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.9, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-8267", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-3632", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-8314", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-2596", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-7393", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-4880", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-236", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2075", "mrqa_searchqa-validation-8838", "mrqa_searchqa-validation-9112", "mrqa_searchqa-validation-6175", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-2315"], "SR": 0.421875, "CSR": 0.5420386904761905, "EFR": 0.918918918918919, "Overall": 0.692972771879022}, {"timecode": 42, "before_eval_results": {"predictions": ["Magnetic stratigraphers", "other individuals, teams, or entire organizations", "1974", "an explosion and a fire", "Centre of Excellence", "Dominican", "Ministry of European Integration", "philosopher, historian and playwright", "Patricia Jude Francis Kensit", "1933", "Kim Jong-hyun", "in Japan", "musician", "Hans Rosenfeldt", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "Danish", "Groundhog Day", "Barnoldswick", "Stephen Lee", "Johnny Bright", "2007", "16,116", "8 February 1405", "Flavivirus", "the USC Marshall School of Business", "over 1 million acre", "the Flatbush section of Brooklyn, New York City", "Louis \"Louie\" Zamperini", "quantum mechanics", "October 20, 2017", "Tennessee", "322,421", "Saint Michael, Barbados", "`` Real Girl ''", "in the duodenum by enterocytes of the duodsenal lining", "28 July 1914", "Italy", "Kevin Kline", "Bob Dylan", "the anterolateral corner of the spinal cord", "dijon", "Nottingham", "the forest of Fontainebleau", "Phil Redmond", "pheidias", "James Mason", "The Man with One Red Shoe", "Inter Milan", "2009", "the motherless cub defended by Elphaba in \"Wicked.\"", "a plaque at the home of his great-grandfather", "al-Maqdessi's", "soldiers,", "Haeftling", "in Heaven", "the Grateful Dead", "canterbury", "\"Livin\"", "Endymion", "the Pittsburgh Steelers", "cricket", "Solidarity", "Mexico", "business"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7236979166666667}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 0.4, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.8571428571428571, 0.6, 1.0, 1.0, 1.0, 0.25000000000000006, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-2358", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-1778", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3024", "mrqa_hotpotqa-validation-5220", "mrqa_hotpotqa-validation-3719", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-7511", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-66", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-2733", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-6621", "mrqa_searchqa-validation-5984", "mrqa_searchqa-validation-4679", "mrqa_searchqa-validation-11491"], "SR": 0.578125, "CSR": 0.5428779069767442, "EFR": 1.0, "Overall": 0.7093568313953489}, {"timecode": 43, "before_eval_results": {"predictions": ["a six membraned chloroplast", "the third season of the television series How I Met Your Mother", "Massillon, Ohio", "based on her castle alone, and even more so after informing her that Aslan had come to Narnia", "2006 -- 03 and 2006 -- 06 )", "Arnold Schoenberg", "based on superior measurables such as size, speed, and strength, has increased his `` draft stock '' despite having a possibly average or subpar college career", "a synonym for ummat al - Islamiyah", "Chelsea", "Akshay Kumar", "based by the stomach acid into its active form, pepsinogen", "based on the Earth's axial tilt, which fluctuates within a margin of 2 \u00b0 over a 40,000 - year period, due to tidal forces resulting from the orbit of the Moon", "the victory of good over evil, the arrival of spring, end of winter, and for many a festive day to meet others, play and laugh, forget and forgive, and repair broken relationships", "Rafael Nadal", "June 1991", "The Epistle of Paul to the Philippians", "2018", "1999 to 2001", "Isabela Moner", "12 November 2010", "Theodosius I died", "Richard Stallman", "Dalveer Bhandari", "after their last show", "based on the image plane of a perspective drawing where the two - dimensional perspective projections ( or drawings ) of mutually parallel lines in three - dimensional space appear to converge", "Covington, Kentucky", "The Han ( 202 BC -- 220 AD )", "the foreign exchange market (FX )", "the European economy had collapsed", "northwest of Bemis Heights", "an Indian government ministry", "Peter Andrew Beardsley MBE", "May 17, 2018", "William Boyd", "raven", "Florentius", "Norman Mailer", "\"Funny Bones\"", "Snickers candy bars", "Daily Herald", "Diamond Rio", "15,000 people for basketball matches and 15,500 for concerts (with standing public ramp)", "Kristian Eivind Espedal", "Erich Schmidt-Leichner", "Canterbury in England", "Delaware River", "Estero, Florida", "43 percent", "Nineteen", "July", "At least 40 former U.S. Marines or sons of Marines who lived at Camp Lejeune", "Brett Cummins,", "held in a trust fund", "a kidney that was a biological match.", "ice cream", "Walla Walla", "A Million Little pieces", "Linus", "plebeians", "a butterfly effect", "William Bradford", "the letter Z", "a negligent act or omission", "Constantinople"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5894998498542662}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.26666666666666666, 1.0, 0.19999999999999998, 0.28571428571428575, 1.0, 0.06896551724137931, 0.7499999999999999, 0.5, 1.0, 0.0, 0.12121212121212122, 0.983050847457627, 1.0, 1.0, 0.33333333333333337, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.5333333333333333, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-10419", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-6113", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-9987", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-4860", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-10148", "mrqa_triviaqa-validation-2593", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-7778", "mrqa_hotpotqa-validation-5310", "mrqa_hotpotqa-validation-4522", "mrqa_hotpotqa-validation-3678", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1279", "mrqa_searchqa-validation-2166", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-11574"], "SR": 0.46875, "CSR": 0.5411931818181819, "EFR": 1.0, "Overall": 0.7090198863636363}, {"timecode": 44, "before_eval_results": {"predictions": ["Times Square Studios", "Kittie", "the George Washington Bridge", "Craig William Macneill", "Oracle Corporation", "Annette Bening", "\"War & Peace\"", "Thomas Mawson", "British", "Jean Erdman", "Rabies", "Electress of Hanover", "national and international media", "New Jersey", "1995", "Francis Albert Sinatra", "Black pudding", "Esteban Ocon", "Kurt Vonnegut Jr.", "mixed martial arts", "1st Baron Dowding", "9", "16\u201321", "University of Nevada, Las Vegas", "6,396.", "Division of Cook", "Radcliffe College", "13 people", "George Harrison", "The Shins", "The Captain Matchbox Whoopee Band", "Tom Coburn", "20th", "the National League ( NL ) champion Cleveland Indians", "Staci Keanan", "the life of the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "March 2003", "Harrys", "fermenting dietary fiber into short - chain fatty acids ( SCFAs ), such as acetic acid and butyric acid, which are then absorbed by the host", "14 November 2001", "turnip", "the very first F1 car Ayrton Senna ever drove", "Falkland Islands", "gleneagles", "Dublin", "Neighbours", "Margate", "a long-range missile", "Daniel Wozniak,", "the HMS Beagle", "Iran's President Mahmoud Ahmadinejad", "Raymond Soeoth of Indonesia", "Naples home.", "Jaime Andrade", "trenchcoat", "Eragon", "Matthew Perry", "Ivan the Terrible", "Kingston", "Lisa Gherardini", "Cleopatra", "LSD", "the Island", "four"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7291150525525526}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.9189189189189189, 1.0, 0.0, 0.9777777777777777, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-167", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3092", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-2581", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-7393", "mrqa_triviaqa-validation-4732", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1418", "mrqa_newsqa-validation-472", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-134", "mrqa_searchqa-validation-5564", "mrqa_triviaqa-validation-2578"], "SR": 0.65625, "CSR": 0.54375, "EFR": 0.9090909090909091, "Overall": 0.6913494318181818}, {"timecode": 45, "before_eval_results": {"predictions": ["the emergence of the new state of Turkey in the Ottoman Anatolian heartland, as well as the creation of modern Balkan and Middle Eastern states", "May 1, 2011", "Bob Day", "English", "23 December 1893", "James Franco", "31", "Sun Valley, Idaho", "Marsamxett Harbour", "Lockhart", "girls aged 11 to 18", "Duval County", "American burlesque", "Captain Cook's Landing Place", "alt-right", "Kentucky", "Belladonna", "16th season", "Louis Mountbatten", "Mario Lemieux", "The O2 Arena", "Channel 4", "British racing driver", "1921", "Get Him to the Greek", "Interstate 22", "Ronald Lyle \" Ron\" Goldman", "RAF Mount Pleasant", "Azusa Pacific University", "Mel Blanc", "five", "performed under the mononym Charice until his gender transition to male", "\"Section.80\"", "Kida", "four distinct levels", "commemorating fealty and filial piety", "fifty small, white, five - pointed stars arranged in nine offset horizontal rows, where rows of six stars ( top and bottom ) alternate with rows of five stars", "the Roman Empire", "Theodore Roosevelt", "1987", "sweden", "swooned", "300", "The Quatermass Experiment", "Honeybees", "sweden", "George Washington", "British Prime Minister Gordon Brown's", "about 62,000 U.S. troops in the country,", "11", "$273 million", "Iran", "diplomatic relations", "the state's first lady,", "Jakarta", "Alexander Calder", "Nautilus", "sweden", "jabberwocky", "metamorphic", "ping-pong", "California, Texas and Florida,", "Bright Automotive,", "an open window"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6097592555466452}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true], "QA-F1": [0.5517241379310345, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 0.13953488372093026, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9846", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-3332", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-2410", "mrqa_hotpotqa-validation-4900", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-3452", "mrqa_naturalquestions-validation-5943", "mrqa_naturalquestions-validation-3918", "mrqa_triviaqa-validation-4413", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-6393", "mrqa_triviaqa-validation-154", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-3375", "mrqa_searchqa-validation-2701", "mrqa_searchqa-validation-10688", "mrqa_searchqa-validation-7787", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-16616", "mrqa_newsqa-validation-2338"], "SR": 0.53125, "CSR": 0.5434782608695652, "EFR": 1.0, "Overall": 0.7094769021739131}, {"timecode": 46, "before_eval_results": {"predictions": ["matching white pants", "Nina Stibbe", "December", "Matt Groening", "in the series \"Runaways\"", "fourth-largest media group", "July 1, 1916", "ten", "more than 40 million", "Peter G. Kelly", "Wes Archer", "26,000", "Anne of Green Gables", "She was a Democratic-Farmer-Labor Party nominee for United States Congress in 2010, unsuccessfully challenging incumbent Republican Michele Bachmann.", "teachers teacher", "first wife Anna", "MARC Penn Line", "the Louisiana Superdome", "Univision", "\"The Light in the Piazza\"", "400 MW", "Fort Hood", "green and yellow", "\"Seducing Mr. Perfect\"", "Juilliard School", "The interview", "Hilo", "\" Invader (Invasor)", "Joseph J. Pulitzer", "Lazio region", "the free travel Schengen Area", "Norman Lear", "2,627.", "March 31, 2013", "mid-1980s", "in the terrestrial biosphere", "Dr. Derek Shepherd ( Patrick Dempsey )", "December 19, 2014", "Waylon Jennings", "1992", "fluorspar", "Nunavut", "Antoine Henri Becquerel", "ledger", "axe", "\\'The Comedy of Errors", "Malaysia", "Zahi Hawass,", "\"Wolfman,\"", "increased foot patrols", "fractured pelvis and sacrum", "1.2 million", "three", "maintain an \"aesthetic environment\" and ensure public safety", "Baltimore", "Zelda", "Natalie Jane Imbruglia", "\\'", "\\'", "the Air Force", "\\'Jeopardy", "Scotland", "the Bulletin", "\\'Home and Away"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5630208333333333}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false, false, false, false, true, true, true, true, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.5, 0.5, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 0.2, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-499", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-2412", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-3485", "mrqa_hotpotqa-validation-1094", "mrqa_hotpotqa-validation-5369", "mrqa_hotpotqa-validation-2770", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-2079", "mrqa_hotpotqa-validation-5053", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-2267", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-2172", "mrqa_triviaqa-validation-6705", "mrqa_triviaqa-validation-7126", "mrqa_triviaqa-validation-5859", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-3268", "mrqa_newsqa-validation-3165", "mrqa_searchqa-validation-4361", "mrqa_searchqa-validation-5529", "mrqa_searchqa-validation-3237", "mrqa_searchqa-validation-430", "mrqa_triviaqa-validation-6837"], "SR": 0.453125, "CSR": 0.5415558510638299, "EFR": 1.0, "Overall": 0.709092420212766}, {"timecode": 47, "before_eval_results": {"predictions": ["meningitis", "talc", "the Florida Panthers", "John Hersey", "singapore", "Lysistrata", "tulips", "an Old Manse", "\\'N \\'E", "Ross Perot", "Turkey", "Antarctica", "Spider-Man", "Ezra Cornell", "Arthur Miller", "(John Wilkes) Booth", "Florida", "Johns Hopkins", "Lurch", "Mississippi River", "London", "Millie", "(Scott) Peterson", "\\' Daddy", "John Henry", "Broadway", "Guernsey", "Fort Sumter", "Orion", "Notre-Dame de Paris", "\\'Duan De Marco", "Troilus", "Puente Hills Mall", "Spanish soldiers under conquistador Francisco Pizarro, his brothers, and their native allies", "elected", "beta decay", "the St. Louis Cardinals, respectively", "Victor Dhar", "parthenogenesis", "Kim Basinger", "bison", "the Grail", "in the Tower", "San Francisco", "Frank McCourt", "Daewoo", "Rihanna", "12", "Ribosomes", "political correctness", "The Royal Family", "Food and Agriculture Organization", "two or three acts", "Friends", "1983", "First Balkan War", "Britain.", "\"your President Bush doesn't like us Muslims.\"", "Tuesday", "Rin Tin Tin", "the Bronx.\"", "an \"unnamed international terror group\"", "the 11th anniversary of the September 11, 2001, terror attacks.", "Iran's parliament speaker"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6628434065934066}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.42857142857142855, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3076923076923077, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14657", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-458", "mrqa_searchqa-validation-16038", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-166", "mrqa_searchqa-validation-3872", "mrqa_searchqa-validation-11180", "mrqa_searchqa-validation-7867", "mrqa_searchqa-validation-2713", "mrqa_searchqa-validation-1780", "mrqa_searchqa-validation-8979", "mrqa_searchqa-validation-12962", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-10257", "mrqa_triviaqa-validation-2889", "mrqa_triviaqa-validation-1213", "mrqa_triviaqa-validation-3207", "mrqa_hotpotqa-validation-5807", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-2215"], "SR": 0.59375, "CSR": 0.5426432291666667, "EFR": 1.0, "Overall": 0.7093098958333334}, {"timecode": 48, "before_eval_results": {"predictions": ["Stella Nina McCartney", "Little Women", "the Violin", "Mark Twain", "the Rubik's Cube", "Newport News Point", "Cur de Lion", "the salivary glands", "Chinese", "Solidarity", "the Thamkrabok monastery", "Aristotle", "(Diane) Arbus", "(George) Adams", "the Manhattan island", "Martina Navratilova", "Sweden", "molten", "Margaret Spellings", "City of Hope", "Frasier Crane", "Like Water for Chocolate", "a catalog", "Lewis Carroll", "a high school football team in the fictional town of Dillon, Texas.", "Signs", "Henry Cisneros", "Kilimanjaro", "Nguyen", "William Shakespeare", "Scotchgard", "Prince", "Thomas Lennon", "Peter Finch", "plant anatomy", "Strabo", "the fourth C key from left on a standard 88 - key piano keyboard", "Julia Roberts", "Havana, Kingston, and Nassau, which reside under Spanish, British, and pirate influence, respectively", "The Soviet Union, which hosted the 1980 Summer Olympics,", "rabbit", "Ogaden", "london", "Hera", "David Bowie", "Timothy Carroll", "Aquaman", "Percy thrower", "the Female Socceroos", "about 5320 km", "She made her film debut in the 1995 teen drama \"Kids\".", "Byeon Seung-wook", "People v. Turner", "between the three towns of Doncaster, Scunthorpe and Gainsborough", "Sulla", "Jenny Bae", "the man ran out of bullets and blew himself up.", "overturned about 5:15 p.m.", "Islamic", "throwing three punching but said only one connected.", "overturned terrorism convictions for a Yemeni cleric and his personal assistant, saying they did not receive a fair trial.", "in cities throughout Canada.", "several weeks,", "Madonna"], "metric_results": {"EM": 0.5, "QA-F1": 0.6210783102766798}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, false, true, false, true, true, false, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, false, false, true, true, false, false, false, true, true, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, false, false, true, true], "QA-F1": [0.8, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.19999999999999998, 0.4, 1.0, 0.36363636363636365, 0.5217391304347826, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2571", "mrqa_searchqa-validation-8312", "mrqa_searchqa-validation-13564", "mrqa_searchqa-validation-7073", "mrqa_searchqa-validation-13528", "mrqa_searchqa-validation-13895", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-15210", "mrqa_searchqa-validation-14796", "mrqa_searchqa-validation-12851", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-6550", "mrqa_searchqa-validation-1226", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-12278", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-9085", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-3947", "mrqa_naturalquestions-validation-75", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-452", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-5670", "mrqa_hotpotqa-validation-1533", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-816", "mrqa_newsqa-validation-3362"], "SR": 0.5, "CSR": 0.5417729591836735, "EFR": 1.0, "Overall": 0.7091358418367347}, {"timecode": 49, "before_eval_results": {"predictions": ["Children of Men", "Fernando Rey", "\"The Occupants Of Interplanetary Craft\"", "chutney", "America", "Islam", "Zionism", "Gaelic", "an alien", "the Robert F. Kennedy Bridge", "the Grimm brothers", "(Jose de) San Martin", "Elizabeth Edward", "Richard Cory", "Franklin D. Roosevelt", "Pulps", "Italy", "Borneo", "Brazil", "a surrogate", "American Bandstand", "Sanrio", "Brazil", "Missouri University of Science and Technology", "Jude Law", "Dick Cheney", "vitamin D", "Diversions", "an internal combustion engine", "General McClellan", "messenger", "Homeland Security", "the people of the United States", "Thomas Jefferson's", "it has a core population in Michigan and surrounding states and provinces", "2018", "the Missouri River", "antimeridian", "Teri Garr", "a vertebral column ( spine )", "teflip", "teflon", "Great Victoria Desert", "pelagia", "stomach cramps, fever, and muscle pain", "an ASIN", "Lake Placid", "Albert Reynolds", "Clive Staples Lewis", "XL", "Benedict of Nursia", "The Spiderwick Chronicles", "1967", "1945", "You Can Be a Star", "electronic gaming machines", "Dean Martin, Katharine Hepburn and Spencer Tracy", "$5.5 billion", "forced drugging", "heavy turbulence", "April 22.", "the couple's surrogate", "Friday,", "Afghanistan"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5426339285714286}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.14285714285714285, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-16374", "mrqa_searchqa-validation-8686", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-12097", "mrqa_searchqa-validation-9677", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7097", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-9261", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-9737", "mrqa_searchqa-validation-2941", "mrqa_searchqa-validation-4456", "mrqa_searchqa-validation-9947", "mrqa_searchqa-validation-12956", "mrqa_searchqa-validation-9069", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-1673", "mrqa_naturalquestions-validation-6789", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-188", "mrqa_triviaqa-validation-2394", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-3139", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-3420", "mrqa_newsqa-validation-4109", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-1054"], "SR": 0.46875, "CSR": 0.5403125, "EFR": 0.9705882352941176, "Overall": 0.7029613970588235}, {"timecode": 50, "UKR": 0.728515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1003", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-156", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1655", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-236", "mrqa_hotpotqa-validation-2482", "mrqa_hotpotqa-validation-2548", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3026", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-3279", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3610", "mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-3837", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-4009", "mrqa_hotpotqa-validation-4180", "mrqa_hotpotqa-validation-4300", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4900", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5004", "mrqa_hotpotqa-validation-5015", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5310", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-5367", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-963", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-10419", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1437", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3666", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4316", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-6083", "mrqa_naturalquestions-validation-6093", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7639", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8759", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9553", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9975", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1240", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1409", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2661", "mrqa_newsqa-validation-2678", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3268", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3874", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-726", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10143", "mrqa_searchqa-validation-10688", "mrqa_searchqa-validation-10986", "mrqa_searchqa-validation-10987", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-11489", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-11544", "mrqa_searchqa-validation-11578", "mrqa_searchqa-validation-11583", "mrqa_searchqa-validation-12097", "mrqa_searchqa-validation-12154", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12278", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12528", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13484", "mrqa_searchqa-validation-13505", "mrqa_searchqa-validation-13895", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-1419", "mrqa_searchqa-validation-14243", "mrqa_searchqa-validation-14642", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-1484", "mrqa_searchqa-validation-14854", "mrqa_searchqa-validation-14891", "mrqa_searchqa-validation-15769", "mrqa_searchqa-validation-15815", "mrqa_searchqa-validation-16038", "mrqa_searchqa-validation-16297", "mrqa_searchqa-validation-16390", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-166", "mrqa_searchqa-validation-16616", "mrqa_searchqa-validation-16850", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-2079", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-2627", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3041", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-3410", "mrqa_searchqa-validation-3607", "mrqa_searchqa-validation-3637", "mrqa_searchqa-validation-4159", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4456", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-45", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-4746", "mrqa_searchqa-validation-4784", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5030", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-6275", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-6430", "mrqa_searchqa-validation-6852", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-7596", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8312", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8838", "mrqa_searchqa-validation-8964", "mrqa_searchqa-validation-9069", "mrqa_searchqa-validation-9233", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-9324", "mrqa_searchqa-validation-9366", "mrqa_squad-validation-1006", "mrqa_squad-validation-10140", "mrqa_squad-validation-1016", "mrqa_squad-validation-10232", "mrqa_squad-validation-10433", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1312", "mrqa_squad-validation-1462", "mrqa_squad-validation-1509", "mrqa_squad-validation-1634", "mrqa_squad-validation-1965", "mrqa_squad-validation-199", "mrqa_squad-validation-2086", "mrqa_squad-validation-2160", "mrqa_squad-validation-2251", "mrqa_squad-validation-2315", "mrqa_squad-validation-2376", "mrqa_squad-validation-2591", "mrqa_squad-validation-2752", "mrqa_squad-validation-2916", "mrqa_squad-validation-3223", "mrqa_squad-validation-3230", "mrqa_squad-validation-34", "mrqa_squad-validation-3416", "mrqa_squad-validation-3492", "mrqa_squad-validation-3581", "mrqa_squad-validation-360", "mrqa_squad-validation-3610", "mrqa_squad-validation-366", "mrqa_squad-validation-3670", "mrqa_squad-validation-3678", "mrqa_squad-validation-3693", "mrqa_squad-validation-3711", "mrqa_squad-validation-3851", "mrqa_squad-validation-3957", "mrqa_squad-validation-3986", "mrqa_squad-validation-4179", "mrqa_squad-validation-4360", "mrqa_squad-validation-4403", "mrqa_squad-validation-4750", "mrqa_squad-validation-494", "mrqa_squad-validation-5035", "mrqa_squad-validation-509", "mrqa_squad-validation-5375", "mrqa_squad-validation-545", "mrqa_squad-validation-5455", "mrqa_squad-validation-5502", "mrqa_squad-validation-5581", "mrqa_squad-validation-5753", "mrqa_squad-validation-6034", "mrqa_squad-validation-6382", "mrqa_squad-validation-6565", "mrqa_squad-validation-6653", "mrqa_squad-validation-6703", "mrqa_squad-validation-6787", "mrqa_squad-validation-6852", "mrqa_squad-validation-703", "mrqa_squad-validation-7037", "mrqa_squad-validation-7047", "mrqa_squad-validation-7096", "mrqa_squad-validation-7125", "mrqa_squad-validation-7137", "mrqa_squad-validation-7252", "mrqa_squad-validation-7276", "mrqa_squad-validation-7347", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-758", "mrqa_squad-validation-764", "mrqa_squad-validation-7683", "mrqa_squad-validation-7701", "mrqa_squad-validation-7715", "mrqa_squad-validation-7850", "mrqa_squad-validation-7976", "mrqa_squad-validation-8002", "mrqa_squad-validation-8068", "mrqa_squad-validation-8134", "mrqa_squad-validation-8231", "mrqa_squad-validation-8278", "mrqa_squad-validation-8332", "mrqa_squad-validation-8338", "mrqa_squad-validation-8476", "mrqa_squad-validation-8699", "mrqa_squad-validation-878", "mrqa_squad-validation-8796", "mrqa_squad-validation-8987", "mrqa_squad-validation-9074", "mrqa_squad-validation-9304", "mrqa_squad-validation-9372", "mrqa_squad-validation-9516", "mrqa_squad-validation-9606", "mrqa_squad-validation-9798", "mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-1124", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1510", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1753", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2288", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-2596", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2763", "mrqa_triviaqa-validation-2830", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2931", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3031", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-3757", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4607", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-4937", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-5812", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-6705", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7172", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-7748", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-92"], "OKR": 0.828125, "KG": 0.48671875, "before_eval_results": {"predictions": ["741 weeks from 1973 to 1988", "Baaghi", "the fourth C key from left on a standard 88 - key piano keyboard", "in Rome in 336", "Captaincy General of Guatemala", "1998 ( XXXIII )", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "Yuzuru Hanyu", "four", "2001", "Eddie Van Halen", "Ozzie Smith", "the fourth quarter of the preceding year", "in London's West End in 1986", "Archduke Franz Ferdinand of Austria", "Jacob Ludlow", "William Wyler", "a sociological perspective", "is a major river in the southern United States of America", "31", "By 1770 BC", "President of the United States", "Welch, West Virginia", "the heart", "( last appearance after beating New England Patriots in Super Bowl LII )", "President of the United States", "2017", "The Statue of Freedom", "September 6, 2019", "Norway", "Mike Higham, who had previously worked with Sondheim on Sweeney Todd : The Demon Barber of Fleet Street", "Marley & Me", "puff the Magic Dragon", "The.44 Calibre Killer", "Edinburgh", "John Nash", "lancaster", "reims", "St Helens", "Gorky", "Trey Parker and Matt Stone", "The Life of Charlotte", "The Future", "Lionel Boyce", "the superhero Birdman", "Corendon Airlines", "From Here to Eternity", "the full fascist conspiracy theory of supernatural Jewish power", "Anil Kapoor", "Sotomayor,", "one of the shocks of the year", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "March 3,", "1918-1919.", "two remaining crew members from the helicopter,", "in rural Tennessee.", "a 5% beer", "Mikhail Saakashvili", "Barnard College", "the lion", "the OSS", "Chen Lu", "Johnson", "The Young and the Restless"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5951450892857142}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, true], "QA-F1": [0.0, 0.5, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.6666666666666666, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-2479", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-9773", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-2063", "mrqa_triviaqa-validation-6342", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4995", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-1084", "mrqa_searchqa-validation-8101", "mrqa_searchqa-validation-11815", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-10525", "mrqa_searchqa-validation-8489", "mrqa_searchqa-validation-2997"], "SR": 0.484375, "CSR": 0.5392156862745099, "EFR": 0.9393939393939394, "Overall": 0.7043938001336898}, {"timecode": 51, "before_eval_results": {"predictions": ["gravity", "Charles Oscar Waters", "1996", "Rashidun Caliphs", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "Earle Hyman", "1998", "free floating", "Hasmukh Adhia", "the United States Congress declared war ( Public Law 77 - 328, 55 STAT 795 ) on the Empire of Japan", "its absolute temperature", "Joseph M. Scriven", "Strabo", "pit road speed", "5.7 million", "the Magnavox Odyssey", "Mike Alstott", "to the left of the dinner plate", "1", "2010", "present - day southeastern Texas", "Shenzi", "1871", "the American punk rock band the Ramones", "fourteen - year - old Georgia Nicholson ( Groome )", "Beorn", "8,850 km ( 5,500 mi )", "in the United States by Melvil Dewey", "Joseph Sherrard Kearns", "one person", "members of the gay ( LGBT ) community", "two classes of organic compounds", "Wat Tyler", "Tyburn", "stockonium and radium", "Russia", "Tennessee", "bond", "Prague", "Renoir", "four", "London, United Kingdom", "Jenson Button", "1974", "1975", "Carrefour", "Florida and Oklahoma", "Nelson County", "Pakistan", "onto the college campus.\"", "orders immigrants to carry their alien registration documents at all times", "Saturday", "breast cancer.\"", "Wednesday", "Will Smith.", "accusations of improper or criminal conduct.", "beta", "Answer Who is.", "Consumers Union Reports", "Amsterdam", "Auguste Escoffier", "Shirley Schmidt", "a double curve", "Anne Rice"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6231770833333333}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, true, false, false, true, true, true, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.1, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.5, 1.0, 0.3333333333333333, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.8, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-6481", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-607", "mrqa_hotpotqa-validation-5727", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-76", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-2369", "mrqa_newsqa-validation-4114", "mrqa_newsqa-validation-2843", "mrqa_searchqa-validation-401", "mrqa_searchqa-validation-8345", "mrqa_searchqa-validation-13488", "mrqa_searchqa-validation-14938", "mrqa_searchqa-validation-11102"], "SR": 0.515625, "CSR": 0.5387620192307692, "EFR": 1.0, "Overall": 0.7164242788461539}, {"timecode": 52, "before_eval_results": {"predictions": ["Martha Coolidge", "bioelectromagnetics", "Serial (Bad) Weddings", "Dizzy Dean", "George Raft", "Josh", "Javed Miandad", "the Shriners", "DJ Scotch Egg", "126,202", "Target Corporation", "Gatwick", "15", "North America", "25 June 1971", "810", "Wal-Mart Canada Corp.", "heaviest album of all", "Borwick railway station", "1834", "5.3 million", "1943", "250 million copies worldwide", "7 June 1985", "1983", "200,000 passengers", "Gavril Yudin orchestrated the first movement piano sketch", "Danny Glover", "Sam Kinison", "Indianapolis Motor Speedway", "Sun Belt Conference", "Cinema of Russia", "erosion", "1,000", "`` Nelson's Sparrow ''", "nearby", "coercivity", "Jimmy Flynn", "Horace Lawson Hunley", "Sally Field", "Homo sapiens", "tartar", "Ithaca", "ernie", "ernie Holzman", "caryatid", "Ceylon", "paddington", "London and Buenos Aires", "Yusuf Saad Kamel", "St. Louis,", "he won two Emmys for work on the 'Columbo' series starring Peter Falk.", "Brown-Waite", "re-impose order", "a 33-year-old man after his wife and five children were found dead in their Naples home.", "Mashhad", "synapses", "Hungarian", "Antwerp", "Tim Burton", "S. (1) Civil War", "The Sound of Music", "a broom", "Cabinda"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5422558922558922}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.4, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 1.0, 1.0, 0.07407407407407407, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-3864", "mrqa_hotpotqa-validation-726", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-4768", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-3475", "mrqa_hotpotqa-validation-74", "mrqa_hotpotqa-validation-3641", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-1837", "mrqa_naturalquestions-validation-6169", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-3679", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-2870", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-3756", "mrqa_triviaqa-validation-1516", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-1775", "mrqa_searchqa-validation-1383", "mrqa_searchqa-validation-11862", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-10896", "mrqa_searchqa-validation-5254", "mrqa_searchqa-validation-5297"], "SR": 0.453125, "CSR": 0.5371462264150944, "EFR": 0.9714285714285714, "Overall": 0.7103868345687332}, {"timecode": 53, "before_eval_results": {"predictions": ["25 October 1921", "Park Seo-joon", "Marty Ingels", "\"Complex\" magazine", "Margarida", "Norbertine", "Richard II", "Coahuila, Mexico", "Province of Canterbury", "born 2 May 2015", "Eenasul Fateh", "pretty pretty", "all U.S. territories", "Jung Yun-ho", "Debbie Isitt", "Clive Staples Lewis", "2017", "Adam Karpel", "evangelical Christian periodical", "the Battelle Energy Alliance", "2012", "British television network ITV", "Bardot", "Steve Carell", "Jesus", "2,615", "Mermaids", "publicly", "General Manager", "Bellagio and The Mirage", "Aksel Sandemose", "January 30, 1930", "prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "90 \u00b0 N 0 \u00b0 W", "1830", "Authority", "1984", "Elizabeth Dean Lail", "Times Square in New York City", "1898", "Nowhere Boy", "sama", "John Napier", "watt hours", "South Africa", "at", "British Airways", "cubed", "al Fayed's", "preparing to test-fire a long-range missile under the guise of a satellite launch.", "police dogs", "February 2008", "Sadr City,", "she was a young skater and desperately wanted to make her mother proud.", "NATO's Membership Action Plan,", "Three", "Dame Christie", "the union", "Converse", "Trinidad and Tobago", "\"I think, therefore I am\"", "the Two Gentlemen of Verona", "\"Laugh, and the world Laugh\"", "\"The Vision\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.5651270604395604}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, false, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.15384615384615383, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.21428571428571427, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.6, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2784", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-2190", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2831", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-3031", "mrqa_hotpotqa-validation-4446", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-4844", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-64", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-811", "mrqa_triviaqa-validation-2698", "mrqa_triviaqa-validation-2441", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-414", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-852", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-12630"], "SR": 0.453125, "CSR": 0.5355902777777778, "EFR": 0.9428571428571428, "Overall": 0.7043613591269842}, {"timecode": 54, "before_eval_results": {"predictions": ["Egypt", "Jean Baptiste Le Roy", "South Africa", "A Dangerous Man: Lawrence After Arabia", "John J. Pershing", "Barbra Streisand", "Australia and England", "Numb3rs", "Ken Purdy", "switzerland", "a sense of loyalty and dedication to a specific person or persons", "ethiopian", "a mathematician, astronomer, physician, classical scholar, translator, Catholic cleric, jurist, governor, military leader, diplomat and economist", "Cyclops", "La Toya", "Adolf Hitler", "Brits", "tiger", "four years", "oakum", "1912", "actress earhart", "13", "B\u00e9la Bart\u00f3k", "Jack Nicklaus", "Benghazi", "Paris", "Norwegian", "radishes", "ethiopian", "Amsterdam", "an amanuensis", "Lew Brown", "Paracelsus", "a section of the Torah ( Five Books of Moses ) used in Jewish liturgy during a single week", "2014", "Times Square in New York City west to Lincoln Park in San Francisco", "Luther Ingram", "The Constitution of India", "Middle Eastern alchemy", "seven members", "Russian Ark", "Harry F. Sinclair", "diving duck", "ABC", "perjury and obstruction of justice", "Benj Pasek and Justin Paul", "1974", "Alexandre Caizergues,", "Buenos Aires.", "Vicente Carrillo Leyva,", "Prague", "your environmental efforts make even more impact than Harrison Ford's chest.", "President Obama,", "269,000", "Marxist guerrillas", "Lech Walesa", "outside", "Sweden", "Hiroshima", "Paraguay", "The \"NFL HQ\" crew", "Roger Williams", "James Joyce"], "metric_results": {"EM": 0.5, "QA-F1": 0.5986705043859649}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, true, false, false, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.631578947368421, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-6411", "mrqa_triviaqa-validation-4934", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-3334", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-360", "mrqa_triviaqa-validation-4677", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-50", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1394", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-6490", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-1058", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-2530", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-1239"], "SR": 0.5, "CSR": 0.5349431818181818, "EFR": 1.0, "Overall": 0.7156605113636364}, {"timecode": 55, "before_eval_results": {"predictions": ["severe flooding", "Iranian consulate in Peshawar", "Australia and New Zealand", "suicide bombing", "Kurdish group that has been attacking Turkey from inside northern Iraq.", "District of Columbia National Guard,", "the simple puzzle video game,", "the flooding was so fast that the thing flipped over,\"", "regulators in the agency's Colorado office received improper gifts from energy industry representatives and engaged in illegal drug use and inappropriate sexual relations with them.", "Dubai", "to admit he cheated on you,", "42 years old", "Elizabeth Taylor is having a \"procedure on her heart,\"", "to fritter his cash away on fast cars, drink and celebrity parties.\"", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.\"", "Six", "Brown-Waite", "tennis", "Alinghi", "Michael Schumacher", "Coleman", "Hong Kong,", "Roy Foster", "I'm certainly not nearly as good of a speaker as he is.\"", "Liverpool", "romantic", "Mandi Hamlin", "to put a lid on the marking of Ashura this year.", "10 municipal police", "Herman Cain", "Two", "dissuading and its tactics in doing so.", "Fall 1998", "October 27, 1904", "February 9, 2018", "June 25", "12.9 - kilometre ( 8 mi )", "must be at least 18 or 21 years old", "interstate communications by radio, television, wire, satellite, and cable", "starch", "Eva Cassidy", "2007", "patient ben", "Bahrain", "triathlon", "kya", "Kevin Spacey", "noises off", "black nationalism", "Great Lakes and Midwestern", "Jarome Iginla", "1980", "MG", "Boeing EA-18G Growler", "the Salzburg Festival", "Fiat Chrysler Automobiles NV", "Washington Irving", "Jamaica Inn", "ernest", "ex post facto", "drums", "Schmidt", "birds", "Big Momma"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6421999007936509}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-1499", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-1180", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-2466", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-2818", "mrqa_newsqa-validation-2758", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-595", "mrqa_hotpotqa-validation-2215", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5233", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-7031"], "SR": 0.546875, "CSR": 0.53515625, "EFR": 1.0, "Overall": 0.715703125}, {"timecode": 56, "before_eval_results": {"predictions": ["Suez Canal", "Basil Feldman,", "antelope", "Caroline Garcia", "Roger Casement", "Czech Republic", "Oliver Twist", "testicles", "driving Miss Daisy", "Oklahoma", "QM2", "Djibouti and Yemen", "pigs", "Bruce Wayne", "playoff basketball", "geyser", "\"Best Late Night Comedy\"", "rum cocktails", "wigan", "tennis", "beetles", "Dan Dare", "Josh Brolin", "Pearl Slaghoople", "home", "Flybe", "Egypt", "Francesca Annis", "Brat Pack", "jujitsu", "weasel", "Prince Edward", "Gary Player", "May 2017", "Finding work at a seafood restaurant", "Columbia River Gorge", "Doug Pruzan", "2019", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "9 February 2018", "Liga MX", "Attorney General and as Lord Chancellor of England", "The Cosmopolitan of Las Vegas", "Mel Blanc", "Democratic", "Tak and the Power of Juju", "a jet-powered tailless delta wing high-altitude strategic bomber", "Phil Spector", "30-minute", "12 million", "\"it should stay that way.\"", "11,", "1,073", "Manchester United", "\"The Da Vinci Code\"", "You're The One That I Want", "home", "a warning", "the Fountain of Youth", "fontanels", "Rouen", "Bangkok", "Typhoid Mary", "a cure or solution for any illness or problem"], "metric_results": {"EM": 0.5, "QA-F1": 0.5705431547619048}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, false, true, false, true, false, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.9600000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-5969", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2837", "mrqa_triviaqa-validation-1424", "mrqa_triviaqa-validation-131", "mrqa_triviaqa-validation-23", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-5067", "mrqa_triviaqa-validation-942", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-5736", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-1904", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-8542", "mrqa_naturalquestions-validation-3697", "mrqa_hotpotqa-validation-3608", "mrqa_hotpotqa-validation-510", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-2913", "mrqa_searchqa-validation-7030", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-3538", "mrqa_searchqa-validation-7493"], "SR": 0.5, "CSR": 0.5345394736842105, "EFR": 1.0, "Overall": 0.7155797697368421}, {"timecode": 57, "before_eval_results": {"predictions": ["kinshasa", "lips", "two of the foremost sites for public baseball analytics and projections,", "piscinae", "Richard Marx", "zero", "kenyan", "kenyan", "Vim Tonic", "Saddam Hussein", "avro", "anthropocene", "shabbat", "Cosmos: A spacetime Odyssey", "Matlock", "Persuasion", "marty", "old Trafford", "eye", "8", "Frank Warner", "lung", "wake", "\"Sugar Baby Love\"", "fifteen", "canisius College", "george v", "significant achievement", "ge Derek Jacobi", "points based scoring", "l8r", "basil", "4 January 2011", "A hairpin turn", "1986", "A lymphocytes", "as a maritime signal, indicating that the vessel flying it is about to leave,", "James Bolam", "1850", "22 \u00b0 00 \u2032 N 80 \u00b0 00\u2032 W", "comedy", "hiphop", "Juventus", "Kennebec County", "Magnus Carlsen", "george jon stewart", "\"The Walking Dead\"", "Pylos and Thebes", "\" Raw Power,\"", "the mine", "an impromptu memorial for the late singer at the \"Stone Circle,\" a neolithic monument in the grounds of the venue.", "North Korea", "at the bottom of the hill", "201-262-2800.", "the legitimacy of that race.", "Brian Smith.", "New Balance", "dehydration in the Sahara", "Mystery Science", "\"Baby Got Back In The U.S.S.\"", "Wales", "Deep Purple", "James II", "Mexico"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5616972969107551}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, false, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9565217391304348, 0.0, 1.0, 0.8421052631578948, 1.0, 0.0, 1.0, 0.8, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.75, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 0.8000000000000002, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6831", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-4237", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-6622", "mrqa_triviaqa-validation-2362", "mrqa_triviaqa-validation-4022", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-1998", "mrqa_triviaqa-validation-7584", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-1293", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-4483", "mrqa_naturalquestions-validation-9979", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-5451", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5299", "mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-5412", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1352", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-4095", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-15364", "mrqa_searchqa-validation-15959"], "SR": 0.453125, "CSR": 0.533135775862069, "EFR": 0.9428571428571428, "Overall": 0.7038704587438424}, {"timecode": 58, "before_eval_results": {"predictions": ["Payaya Indians", "March 11, 2018", "Wisconsin", "1535", "Erica Rivera", "International Baccalaureate", "Tommy James", "alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Paul Lynde", "Western Bloc ( the United States, its NATO allies and others )", "James Rodr\u00edguez", "Hugh S. Johnson", "The Miracles", "spontaneously", "the lower back", "Beijing", "a Cadillac", "Imperial Japan", "Salman Khan", "`` Deadman's Gun ''", "development of electronic computers in the 1950s", "the defendant owed a duty to the deceased to take care", "Sri Lanka Podujana Peramuna", "Hold On", "alternative rock", "November 1999", "Hal Derwin", "the government - owned Panama Canal Authority", "Zoe McLellan as Meredith Brody, NCIS Special Agent", "the `` round '', the rear leg of the cow", "William Wyler", "Woody Paige", "William Blake", "Miranda v. Arizona", "multi-user dungeon", "Buddha", "new German U-Stadtbahns", "climate", "Much Ado About Nothing", "Lancashire", "1926 Paris", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom", "Lithuania", "Walcha", "King James I of England", "Best Actor in 2013", "Charles Guiteau", "nearly 80 years", "Kurt Cobain,", "the outdoors, particularly if they have a garden to eat from,", "London's Heathrow airport", "on Anjuna beach in Goa", "56,", "the results by a chaplain about 1:45 p.m.", "\"The Lost Symbol,\"", "Iranian city of Mashhad", "hangman", "out-of-body experience", "Athol Fugard", "Jonah", "Treasure Island", "Death Watch", "ink", "Boston"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6285379117410368}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, false, true, true, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3846153846153846, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.1904761904761905, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.5, 0.125, 0.0, 0.33333333333333337, 1.0, 0.25, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-6821", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-6949", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-5678", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-5403", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3727", "mrqa_searchqa-validation-8279", "mrqa_searchqa-validation-1484", "mrqa_searchqa-validation-16773", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-9721", "mrqa_searchqa-validation-9151"], "SR": 0.546875, "CSR": 0.5333686440677966, "EFR": 0.9655172413793104, "Overall": 0.7084490520894214}, {"timecode": 59, "before_eval_results": {"predictions": ["to step down as majority leader.", "Long Island convenience store", "Kenneth Cole", "Dennis Davern,", "Harry Nicolaides,", "Russia and China", "Pakistani territory", "free fixes for the consumer.", "heavy turbulence", "to the southern city of Naples", "outside influences in next month's run-off election,", "Miss USA Rima Fakih", "Teen Patti", "is looking at designating the sedative as a \"scheduled\" drug,", "President Obama", "$55.7 million", "ensure that all prescription drugs on the market are FDA approved,", "$3,200 per week", "autonomy", "\"How to Shoot Friends and Interrogate People.\"", "Jenny Sanford,", "Lebanon's Mediterranean coastline", "Animal Planet", "\"an ongoing investigation into 24 illnesses in multiple states,\"", "Monday", "new-car market", "an army major assigned to a guard unit protecting Mexican President Felipe Calderon.", "fast cars, drink and celebrity parties.", "Dr. Jennifer Arnold and husband Bill Klein,", "Sabina Guzzanti", "a bag", "general secretary", "British Columbia, Canada", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "warplanes", "Arunachal Pradesh", "chain elongation", "1926", "March 14", "Sreejita De", "Il Divo", "honey", "perfumery", "november", "Albert Einstein", "Nancy Astor", "MS Herald of Free Enterprise", "in the Counter Terrorism Intranet Referral Unit", "1,462", "Ben Ainslie", "Dalton Brothers", "HackThis Site.org", "Shut Up", "the 2016 U.S. Senate election", "Mark Anthony \"Baz\" Luhrmann", "Two Pi\u00f1a Coladas", "Martin Luther", "Penn State", "Joe Hill", "a turkey", "the National Archives", "Hera", "nag", "the East River"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6716671118233618}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, false, true, false, true, false, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, false, true, true, false, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, true, true], "QA-F1": [0.923076923076923, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.25, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.14814814814814817, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3076923076923077, 0.07999999999999999, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666665, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-689", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-1826", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-961", "mrqa_naturalquestions-validation-388", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-4997", "mrqa_triviaqa-validation-5109", "mrqa_triviaqa-validation-1501", "mrqa_hotpotqa-validation-2684", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-2989", "mrqa_searchqa-validation-5556", "mrqa_searchqa-validation-8933"], "SR": 0.53125, "CSR": 0.5333333333333333, "EFR": 1.0, "Overall": 0.7153385416666667}, {"timecode": 60, "UKR": 0.71484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-156", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1655", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-1837", "mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-205", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-236", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-2407", "mrqa_hotpotqa-validation-2410", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2684", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3167", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3837", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-4300", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4656", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4900", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5004", "mrqa_hotpotqa-validation-5015", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-5367", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5670", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-5774", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-870", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-963", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1437", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1959", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3666", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3947", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4316", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4646", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5300", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-6083", "mrqa_naturalquestions-validation-6093", "mrqa_naturalquestions-validation-613", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8759", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9553", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9979", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1240", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1409", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2416", "mrqa_newsqa-validation-2466", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2661", "mrqa_newsqa-validation-2678", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-515", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-726", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10143", "mrqa_searchqa-validation-10525", "mrqa_searchqa-validation-10688", "mrqa_searchqa-validation-10986", "mrqa_searchqa-validation-10987", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-11489", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-11544", "mrqa_searchqa-validation-11578", "mrqa_searchqa-validation-11815", "mrqa_searchqa-validation-11869", "mrqa_searchqa-validation-12154", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-1226", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13505", "mrqa_searchqa-validation-13895", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-1419", "mrqa_searchqa-validation-14243", "mrqa_searchqa-validation-14642", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-1484", "mrqa_searchqa-validation-14891", "mrqa_searchqa-validation-15538", "mrqa_searchqa-validation-15815", "mrqa_searchqa-validation-16038", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16616", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-2079", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-2381", "mrqa_searchqa-validation-2582", "mrqa_searchqa-validation-2627", "mrqa_searchqa-validation-289", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3041", "mrqa_searchqa-validation-3110", "mrqa_searchqa-validation-3153", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-3607", "mrqa_searchqa-validation-3637", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4456", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-45", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-4746", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5030", "mrqa_searchqa-validation-6430", "mrqa_searchqa-validation-6852", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-7596", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7764", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8737", "mrqa_searchqa-validation-8838", "mrqa_searchqa-validation-8964", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9233", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-9324", "mrqa_searchqa-validation-9366", "mrqa_squad-validation-1006", "mrqa_squad-validation-10140", "mrqa_squad-validation-1016", "mrqa_squad-validation-10433", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1312", "mrqa_squad-validation-1462", "mrqa_squad-validation-1509", "mrqa_squad-validation-1634", "mrqa_squad-validation-1965", "mrqa_squad-validation-199", "mrqa_squad-validation-2086", "mrqa_squad-validation-2160", "mrqa_squad-validation-2251", "mrqa_squad-validation-2376", "mrqa_squad-validation-2752", "mrqa_squad-validation-2916", "mrqa_squad-validation-3223", "mrqa_squad-validation-3230", "mrqa_squad-validation-34", "mrqa_squad-validation-3416", "mrqa_squad-validation-3492", "mrqa_squad-validation-3581", "mrqa_squad-validation-360", "mrqa_squad-validation-3610", "mrqa_squad-validation-366", "mrqa_squad-validation-3670", "mrqa_squad-validation-3678", "mrqa_squad-validation-3693", "mrqa_squad-validation-3711", "mrqa_squad-validation-3851", "mrqa_squad-validation-3957", "mrqa_squad-validation-3986", "mrqa_squad-validation-4750", "mrqa_squad-validation-494", "mrqa_squad-validation-5035", "mrqa_squad-validation-5375", "mrqa_squad-validation-545", "mrqa_squad-validation-5455", "mrqa_squad-validation-5502", "mrqa_squad-validation-5581", "mrqa_squad-validation-5753", "mrqa_squad-validation-6034", "mrqa_squad-validation-6382", "mrqa_squad-validation-6565", "mrqa_squad-validation-6653", "mrqa_squad-validation-6703", "mrqa_squad-validation-6787", "mrqa_squad-validation-6852", "mrqa_squad-validation-703", "mrqa_squad-validation-7037", "mrqa_squad-validation-7096", "mrqa_squad-validation-7125", "mrqa_squad-validation-7137", "mrqa_squad-validation-7252", "mrqa_squad-validation-7276", "mrqa_squad-validation-7347", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-758", "mrqa_squad-validation-764", "mrqa_squad-validation-7701", "mrqa_squad-validation-7715", "mrqa_squad-validation-7850", "mrqa_squad-validation-7976", "mrqa_squad-validation-8002", "mrqa_squad-validation-8068", "mrqa_squad-validation-8134", "mrqa_squad-validation-8231", "mrqa_squad-validation-8332", "mrqa_squad-validation-8338", "mrqa_squad-validation-8699", "mrqa_squad-validation-878", "mrqa_squad-validation-8987", "mrqa_squad-validation-9074", "mrqa_squad-validation-9304", "mrqa_squad-validation-9372", "mrqa_squad-validation-9516", "mrqa_squad-validation-9606", "mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1510", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1753", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-183", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2288", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-2596", "mrqa_triviaqa-validation-2763", "mrqa_triviaqa-validation-2830", "mrqa_triviaqa-validation-2837", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2931", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3031", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-3757", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3784", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4179", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-4237", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4607", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-4937", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6098", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-6342", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-6499", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-6705", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936"], "OKR": 0.765625, "KG": 0.47265625, "before_eval_results": {"predictions": ["Sheppard", "Bonnie Lipton", "Benzodiazepines", "19 June 2018", "in the central plains", "giant planet", "ase", "During Hanna's recovery masquerade celebration", "Gibraltar", "1928", "Coordinated Universal Time", "Bob Peterson", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "Beijing", "2017 - 18", "summer of 1979", "Johnny Cash & Willie Nelson", "$2 million", "Mel Gibson", "The virion must assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "on the Fox Ranch in Malibu Creek State Park, northwest of Los Angeles", "rum, fruit juice, and syrup or grenadine", "a Scandinavian patronymic surname, meaning son of Hans", "port of Nueva Espa\u00f1a", "Kelly Reno", "cytosine ( C )", "Jon", "to encourage rebellion against the British authorities", "near the city of Cairo, Illinois", "ice giants", "Christopher Columbus", "bageecha", "\"Wainwrights\u2019", "trees", "Gremlins", "audi", "tartare", "Babylonian Empire", "black", "newbury", "the Taliban's Islamic Emirate of Afghanistan", "2009", "2009", "Humberside Airport", "\"King of Cool\"", "Debbie Reynolds", "Patricia Jude Francis Kensit", "neuro-orthopaedic Irish veterinary surgeon", "Arthur E. Morgan III,", "\"procedure on her heart,\"", "back at work", "He called it the largest and perhaps most sophisticated ring of its kind in U.S. history.", "former CEO of an engineering and construction company with a vast personal fortune.", "Russian concerns that the defensive shield could be used for offensive aims.", "the militants are suspected of launching attacks inside Pakistan and in neighboring Afghanistan from their haven in the mountainous tribal region along the northwestern border.", "to see my kids graduate from this school district.", "Yellowstone National Park", "\"Crazy Foot\"", "uniform", "John Knox", "Roman Catholicism", "the pelvis", "Emiliano Zapata", "Anderson Cooper"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6225520764135694}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.7272727272727272, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 0.72, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.4, 0.8, 1.0, 0.10714285714285712, 0.23529411764705885, 1.0, 0.08695652173913045, 0.4, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-6763", "mrqa_naturalquestions-validation-7953", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-800", "mrqa_triviaqa-validation-4525", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-4394", "mrqa_hotpotqa-validation-3690", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3885", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-1219", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-2547", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-2695", "mrqa_searchqa-validation-9270", "mrqa_searchqa-validation-8946", "mrqa_searchqa-validation-12686", "mrqa_searchqa-validation-14359"], "SR": 0.484375, "CSR": 0.532530737704918, "EFR": 0.9393939393939394, "Overall": 0.6850099354197715}, {"timecode": 61, "before_eval_results": {"predictions": ["StubHub Center in Carson, California", "Haiti", "1038", "3.5 mya", "Arkansas", "on the lateral side of the tibia, with which it is connected above and below", "David Motl", "Andrew Lloyd Webber", "Gene MacLellan", "much of the European industrial infrastructure had been destroyed", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "60", "Everybody on board", "Nicklaus", "Tim Allen", "Won", "the song was used as the theme song for the Michael Douglas film, The Jewel of the Nile", "Thomas Jefferson", "the symbol \u00d7", "Bobby Darin", "Buffalo Bill", "Puerto Rico", "late January or early February", "the inverted - drop - shaped icon that marks locations in Google Maps", "Nalini Negi", "Samaria", "Andreas Vesalius", "Anakin Luke", "Southampton ( 1902 )", "Wisconsin", "the New Testament", "~ 3.5 million years old", "Cambodian", "8", "the Welcome Stranger", "black Swan", "brixham", "Alison Moyet", "Tacoma", "jaws", "Manchester", "black nationalism", "American college football coach", "January 28, 2016", "Buckingham Palace", "2013", "dance", "Isabella", "\"Teen Patti\" (\"Card Game\")", "more than 78,000 parents", "Jaime Andrade", "Facebook", "750", "Scarlett Keeling", "they don't feelMisty Cummings has told them everything she knows.", "April 28,", "Doc Holliday", "Ezra", "\"Nashville Star\"", "a cardioverter", "the CCP", "reasoning", "\"Johnny B. Goode\"", "rubles"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5632270507270507}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false, false], "QA-F1": [0.7142857142857143, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 1.0, 0.46153846153846156, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5000000000000001, 0.33333333333333337, 1.0, 0.2222222222222222, 1.0, 0.5, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.7, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-7027", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-833", "mrqa_triviaqa-validation-797", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-2462", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-7755", "mrqa_hotpotqa-validation-5285", "mrqa_hotpotqa-validation-278", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-1453", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-879", "mrqa_searchqa-validation-15666", "mrqa_searchqa-validation-8515", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5900", "mrqa_searchqa-validation-3147"], "SR": 0.46875, "CSR": 0.5315020161290323, "EFR": 0.9705882352941176, "Overall": 0.69104305028463}, {"timecode": 62, "before_eval_results": {"predictions": ["Tuesday night,", "the legitimacy of that race.", "16 in 33 internationals.", "Majid Movahedi,", "women and breast cancer.", "Kris Allen,", "a tanker that sailed under a Saudi flag,", "the only goal of the game to ensure Hamburg remain in touch with the top three as", "took an anti-doping test after a Serie A game at Roma", "the Little Rock Nine,", "5,600 people every year,", "Brad Blauser,", "Jacob Zuma,", "farmer Alan Graham outside Bangor,", "an internal inquiry was launched based on a claim of misconduct involving a dormitory parent.", "Paul Ryan (R-WI)", "be silent.", "standing by to provide security as needed.", "peanuts, nuts, shellfish and fish", "her experience and became a passionate advocate for early detection and helping other women cope with the disease.", "56,", "English", "suicides", "the shelling of the compound", "Stephen Worgu", "40-years-and-growing career.", "in Buenos Aires.", "Turkey,", "a bank", "not know the girl had been victimized.", "40 militants and six Pakistan soldiers dead,", "Sunday", "Massachusetts", "Gugu Mbatha - Raw", "an extension of the Hypertext Transfer Protocol ( HTTP ) for secure communication over a computer network, and is widely used on the Internet", "the mid-1980s", "in pilgrimages to Jerusalem", "the Sun Harvester", "Rick Rude", "James Martin Lafferty", "Purple Rain", "a\u00e7ai berry", "Lundy", "insulin", "Jerusalem", "Sarek", "johnny", "a karst cave", "Nathan Bedford Forrest", "Mark Neveldine and Brian Taylor", "the 10-metre platform event", "KXII", "1623", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services", "National Basketball Development League", "the Boston Red Sox", "the London Bridge", "Passover", "Hormel Foods", "the penny", "a dowry", "1849", "beryl", "Bastille Day"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6074195705610179}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true], "QA-F1": [0.4, 1.0, 0.0, 0.0, 0.6, 1.0, 0.2857142857142857, 0.4210526315789473, 0.0, 0.42857142857142855, 0.4, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.8333333333333333, 0.33333333333333337, 0.72, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.15384615384615383, 0.923076923076923, 0.0, 1.0, 0.0, 0.56, 0.0, 0.46153846153846156, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4210526315789474, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-2556", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2758", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-1002", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-3875", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-1199", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-3010", "mrqa_triviaqa-validation-7232", "mrqa_hotpotqa-validation-2486", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-5503", "mrqa_searchqa-validation-11056"], "SR": 0.453125, "CSR": 0.5302579365079365, "EFR": 1.0, "Overall": 0.6966765873015873}, {"timecode": 63, "before_eval_results": {"predictions": ["Mandi Hamlin", "Dan Parris, 25, and Rob Lehr, 26,", "publicly criticized his father's parenting skills.", "Cameroon,", "martial arts,", "telling CNN his comments had been taken out of context.", "$24.1 million,", "Jacob Zuma,", "peace with Israel", "Daniel Wozniak,", "His replacement, African National Congress Deputy President Kgalema Motlanthe,", "Senate Democrats", "Hundreds of women protest child trafficking and shout anti-French slogans", "September, Bianchi's death during childbirth", "March 22,", "Amanda Knox's", "The woman", "Hong Kong's Victoria Harbor", "hot and humid and it rains almost every day of the year.", "Garth Brooks", "the two-state solution", "more than 1.2 million", "Michelle Obama", "around 3.5 percent of global greenhouse emissions.", "off the coast of Dubai", "death of cardiac arrest", "Tuesday night's short program", "the \" Michoacan Family,\"", "The forward's lawyer", "New York high school.", "four county GOP chairmen", "stand down.", "a bow bridge with 16 arches shielded by ice guards", "Andy Serkis", "Nicolas Anelka", "the chest", "1987", "helps scientists better understand the spread of pollution around the globe", "Made a searching and moral inventory of ourselves", "Hanna Alstr\u00f6m and Bj\u00f8rn Floberg", "puckett", "a corvids", "Saturday Night Live", "Racing Cars", "Mujib", "Peru", "dogs", "pomegranate", "Cymbeline", "Indiana University", "University of Texas Longhorns", "Galo (], \"Rooster\")", "Vernier, Switzerland", "Patrick Dempsey and Amanda Peterson", "Indian", "A play-by-post role-playing game", "Eleanor Rigby", "auctions", "Giacomo Puccini", "Maude", "Prince Edward Island", "Chief Justice of the Supreme Court", "beef", "Bach"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7023602291295306}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.8235294117647058, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8750000000000001, 0.0, 0.5714285714285715, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-3566", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-3224", "mrqa_newsqa-validation-2526", "mrqa_newsqa-validation-4055", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-4382", "mrqa_triviaqa-validation-4353", "mrqa_triviaqa-validation-1909", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-1229", "mrqa_searchqa-validation-186", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-7088", "mrqa_searchqa-validation-13505"], "SR": 0.578125, "CSR": 0.531005859375, "EFR": 1.0, "Overall": 0.696826171875}, {"timecode": 64, "before_eval_results": {"predictions": ["July 18, 1994,", "had the surgery December 13", "\"E! News\"", "1,073", "Security officer Stephen Johns", "acid", "would take states in the wrong direction.", "have", "Jewish civil rights activists", "July 4.", "different women coping with breast cancer", "\"A Whiter Shade of Pale\"", "concentration camps,", "Atlantic Ocean.", "heavy flannel or wool", "The ship", "police", "not seem \"unusually prejudicial.\"", "Jacob,", "Former detainees", "Republican", "Darrel Mohler", "United States, NATO member states, Russia", "The sole survivor of the crash", "12-hour-plus", "CEO of an engineering and construction company", "Madonna", "in the head", "Sen. Barack Obama", "Tuesday,", "\"Artromptu memorial for the late singer at the \"Stone Circle,\"", "The Screening Room", "Cadillac", "Speaker of the House of Representatives", "In of 2011, with an estimated population of 1.2 billion, India is the world's second most populous country after the People's Republic of China", "St. Augustine renders it as clara notitia cum laude, `` brilliant celebrity with praise ''", "in elocution teaching to demonstrate rounded vowel sounds", "Eddie Murphy", "non-ferrous", "The pia mater", "Phil Lynott", "Here Comes the bride", "Neptune", "a jumper", "Department of Justice", "Uganda", "the large valley", "South Dakota", "National Collegiate Athletic Association", "Christopher McCulloch", "Argentino Americanos", "\"Waiting for Guffman\"", "villanelle", "2010", "\"I Am Furious (Yellow)\"", "Liam Cunningham", "\"Hey, I heard you missed us, we're back\"", "Donkey Kong", "oxygen", "the Thirty Years", "Massachusetts", "shared", "Aramaic", "Uvula"], "metric_results": {"EM": 0.5, "QA-F1": 0.5996350276221599}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, true, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, true, false, false, false, true, false, false, true, false, true, false, true, true], "QA-F1": [0.0, 0.2, 1.0, 1.0, 0.5, 0.6666666666666666, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.11764705882352941, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2962962962962963, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-67", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2958", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1351", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-3605", "mrqa_triviaqa-validation-1914", "mrqa_triviaqa-validation-5386", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-3551", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3824", "mrqa_hotpotqa-validation-1848", "mrqa_searchqa-validation-6072", "mrqa_searchqa-validation-8827", "mrqa_searchqa-validation-14353", "mrqa_searchqa-validation-16606"], "SR": 0.5, "CSR": 0.5305288461538462, "EFR": 0.96875, "Overall": 0.6904807692307692}, {"timecode": 65, "before_eval_results": {"predictions": ["three", "four Impressionist paintings worth about $163 million (180 million Swiss francs) Sunday in a heist police characterized as \"spectacular.\"", "Spc. Megan Lynn Touma,", "The ruling Justicialist Party, or PJ by its Spanish acronym,", "authorizing killings and kidnappings by paramilitary death squads.", "no evidence as to the cause of death,", "She is the Magneto to my Wolverine, the Saruman to my Frodo, the Dr. Octopus to my Spiderman.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Israel's vice prime minister Silvan Shalom", "Mexico", "$250,000 for Rivers' charity: God's Love We Deliver.", "pirates on a lifeboat off the coast of Somalia,", "Muslim", "\"persistent pain.\"", "capital murder and three counts of attempted murder in the shootings at the University of Alabama in Huntsville last week.", "800,000", "Two other boys ages 13 and 15", "Nineteen", "Paul Schlesselman of West Helena, Arkansas,", "Tim Masters, center,", "Cash for Clunkers", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Deputy Treasury Secretary", "The federal officers' bodies", "the content of the speech, not just the delivery.", "My family comes from a Muslim background, and we're not defined by religion,\"", "volatile zone along the equator between South America and Africa.", "Anil Kapoor's", "former general secretary of the Communist Party,", "Dr. Albert Reiter,", "some", "Robert Barnett,", "near major hotels and in the parking areas of major Chinese supermarkets", "Marcus Aurelius", "2018", "23 September 1889", "November 17, 1800", "nearby objects show a larger parallax than farther objects when observed from different positions,", "March 26, 1973", "100,000", "Jesus our Lord", "the Zulu warriors", "ad nausea", "Something In The Air", "Calvors Brewery", "gold", "Charlie Chan", "woodentops", "2002", "British Bristol Olympus turbojet", "Indian club ATK.", "Republican", "paracyclist", "pubs, bars and restaurants", "Venice", "Albert Park", "a tort", "Willy Ronis", "Maine", "Toronto", "President Abraham Lincoln", "lexis", "Anne", "the pinna"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5796281322843823}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, false, true, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, true, true, false, true, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.09999999999999999, 1.0, 1.0, 0.36363636363636365, 0.4444444444444445, 0.0, 0.2222222222222222, 0.5, 1.0, 1.0, 0.25, 1.0, 0.0, 0.6153846153846153, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.07999999999999999, 0.4, 0.0, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-3747", "mrqa_newsqa-validation-2424", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2717", "mrqa_newsqa-validation-3265", "mrqa_newsqa-validation-55", "mrqa_newsqa-validation-2330", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-2414", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-9101", "mrqa_naturalquestions-validation-2146", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-2983", "mrqa_triviaqa-validation-6304", "mrqa_triviaqa-validation-5378", "mrqa_triviaqa-validation-4778", "mrqa_hotpotqa-validation-1458", "mrqa_hotpotqa-validation-4977", "mrqa_searchqa-validation-11525", "mrqa_searchqa-validation-365", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4774", "mrqa_searchqa-validation-11877", "mrqa_searchqa-validation-3776"], "SR": 0.453125, "CSR": 0.5293560606060606, "EFR": 1.0, "Overall": 0.6964962121212122}, {"timecode": 66, "before_eval_results": {"predictions": ["William Harold \"Bill\" Ponsford", "The British traders' name for the route was derived from combining its name among the northeastern Algonquian tribes, \"Mishimayagat\" or \"Great Trail\", with that of the Shawnee and Delaware,", "The southernmost large city in Europe,", "a poll of policymakers by the Bangor Daily News ranked Millett as the ninth most influential person in Maine politics.", "theScanian War", "\u00c6thelwald Moll", "Glam metal", "Mitsubishi", "\"Lend a hand \u2014 care for the land!\"", "Marc Bolan", "Mineola", "lo Stivale", "Arizona State University.", "Jack Thomas Chick", "Milk Barn Animation", "Green Chair", "The iPod Classic", "Claudius", "1692", "London", "duck", "Jay Chou", "July 14, 2009", "1828", "2009", "October Sky", "Tianhe Stadium", "Louis Silvie \"Louie\" Zamperini", "$26 billion", "five", "Alain Robbe-Grillet", "Winter Haven", "2007", "Felicity Huffman", "an American musical group founded by Marcus Bowens and Jermaine Fuller, with the later addition of J.J. O'Neal and Dougy Williams", "the churches of Galatia", "a cell, DNA replication begins at specific locations, or origins of replication, in the genome.", "Orwell", "Angola", "Austria - Hungary", "prince edward island", "(Larry) Lewis", "york", "(John) Dryden", "clambroth", "Philippines", "king george vi", "pabbingdon bear", "1981", "claimed since her October indictment that the child might still be alive,", "Asashoryu", "July for A Country Christmas", "Ronaldinho", "Anne Frank,", "a review of state government practices completed in 100 days.", "the abduction of minors.", "(Jose de) San Martin", "pantaloons", "(Henry) Murger", "a plus word", "Cervantes", "the Palace of Versailles", "Oriole Park", "a plus clue"], "metric_results": {"EM": 0.5, "QA-F1": 0.6232867826617827}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, true, true, true, false, false, true, false, false, false], "QA-F1": [0.6666666666666666, 0.0, 0.0, 0.19047619047619047, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5555555555555556, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.15384615384615385, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-1634", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-3893", "mrqa_hotpotqa-validation-1605", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-897", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-9670", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-5110", "mrqa_triviaqa-validation-2310", "mrqa_triviaqa-validation-7416", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-1941", "mrqa_searchqa-validation-3284", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-13439", "mrqa_searchqa-validation-11120", "mrqa_searchqa-validation-8137"], "SR": 0.5, "CSR": 0.5289179104477613, "EFR": 0.9375, "Overall": 0.6839085820895523}, {"timecode": 67, "before_eval_results": {"predictions": ["Florio", "Diego Maradona", "Mariah Carey", "a Rear-Admiral of the Navy", "Bangladesh", "Mayflower", "Union of Post Office Workers", "tennyson", "cavall", "River Yare", "Ottawa", "taiwan", "a pitcher", "rhinos", "Evelyn Glennie", "New Zealand", "carousel", "Algiers", "land between two rivers", "narcolepsy", "Carl Smith", "Bapu Nadkarni", "jump jump", "Cubism", "Pink Floyd", "Brian Deane", "Zaire", "Dublin", "Battle of Agincourt", "usir", "Good Life", "tennin throw", "volcanic activity and then gradually moves away from the ridge", "a maritime signal, indicating that the vessel flying it is about to leave, and Reed chose the name to represent'a voyage of adventure'on which the programme would set out", "2003", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "Agra Cantonment - H. Nizamuddin Gatimaan Express", "Stephen Curry of Davidson", "the bank's own funds and signed by a cashier", "Kody and his first wife Meri", "Delphine Software International", "Taeko Ikeda", "The Walt Disney Company", "1874", "2006", "11", "sarod", "National Hockey League", "Jaime Andrade", "to kill members of the Zetas cartel", "Patrick McGoohan,", "more than 30 Latin American and Caribbean nations", "her father's stepmother,", "Friday", "images of the small girl being sexually assaulted.", "the Nazi war crimes suspect who had been ordered deported to Germany,", "Johns Hopkins", "Birmingham", "Norah Jones", "a stallion", "a seaport", "The X-Files", "Rita Mae Brown", "Civil War"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5242965367965368}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, false, true, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.6111111111111112, 1.0, 0.0, 0.1818181818181818, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.2222222222222222, 0.0, 1.0, 0.0, 0.5333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4837", "mrqa_triviaqa-validation-4641", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-3319", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-7152", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-2358", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3421", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-2644", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-237", "mrqa_naturalquestions-validation-3287", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-5787", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-5587", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-829", "mrqa_newsqa-validation-2117", "mrqa_searchqa-validation-16186", "mrqa_searchqa-validation-1152", "mrqa_searchqa-validation-9901"], "SR": 0.453125, "CSR": 0.5278033088235294, "EFR": 0.9714285714285714, "Overall": 0.6904713760504202}, {"timecode": 68, "before_eval_results": {"predictions": ["Martin \"Al\" Culhane,", "sincerity", "Indonesian", "Mammoths", "She had a smile on her face, like she always does when she comes in here,\"", "President Obama embarked on a two-day, two-city charm offensive in Turkey,", "anarchist", "Mercedes,", "former Procol Harum bandmate", "Dog patch Labs", "San Diego,", "Technological Institute of Higher Learning of Monterrey,", "10", "\"Dancing With the Stars.\"", "a one-shot victory in the Bob Hope Classic on the final hole", "3-2", "78,000 parents of children ages 3 to 17.iReport.com:", "machine guns and two silencers", "The Screening Room", "the death of a pregnant soldier", "16", "Anil Kapoor.", "2-1", "CNN", "Joan Rivers", "on the bench", "actress", "one", "college campus.", "Elisabeth", "1979", "Phillip A. Myers.", "pulmonary heart disease", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "205 lb ( 93 kg ) and under", "The Natya Shastra", "military units from their parent countries of Great Britain and France, as well as by American Indian allies", "Bulgaria", "Daryl Sabara", "the Seton Hall Pirates men's basketball team", "mind's", "john jarndyce", "Utah", "leeds", "Southampton", "vote", "salmon", "Gryffindor", "StubHub Center", "Jaguar Land Rover Limited", "Sutton Hoo", "Luis Resto", "Price", "Buffalo", "FAI Junior Cup", "2013", "walrus", "inamona", "rapier", "Newton", "a salaam", "the hip", "Japan", "Alive"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6724419881588999}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0909090909090909, 1.0, 0.0, 0.5882352941176471, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3020", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-2155", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-1580", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-2906", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-805", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-1628", "mrqa_triviaqa-validation-436", "mrqa_hotpotqa-validation-3348", "mrqa_hotpotqa-validation-4036", "mrqa_hotpotqa-validation-1997", "mrqa_hotpotqa-validation-2043", "mrqa_searchqa-validation-16356", "mrqa_searchqa-validation-12454"], "SR": 0.578125, "CSR": 0.5285326086956521, "EFR": 0.9629629629629629, "Overall": 0.6889241143317231}, {"timecode": 69, "before_eval_results": {"predictions": ["10 Afghan police officers", "Molotov cocktails, rocks and glass.", "rwanda", "more than 100", "out in the woods", "Sri Lanka", "Hanin Zoabi,", "Six", "Immigration Minister Eric Besson", "deciding the duties of the new prime minister has been a sticking point in the negotiations.", "Iran", "education", "he was going to get his money,\"", "review their emergency plans", "a female soldier,", "Burhanuddin Rabbani,", "African-Americans", "a place for another non-European Union player in Frank Rijkaard's squad.", "1979", "Congress", "Venus Williams", "four months ago,", "Bowe Bergdahl", "5-0,", "hundreds", "they are angry and scared,", "a Royal Air Force helicopter", "$1.5", "600 square miles of south-central Washington,", "Brian David Mitchell,", "Dean Martin, Katharine Hepburn and Spencer Tracy", "allegedly faking a doctor's note", "November 2, 2016", "Nepal", "mind your manners", "is a new continent that Amerigo Vespucci had discovered on his voyage", "in late September", "a substance that fully activates the receptor that it binds to )", "for the 1994 season", "Tbilisi, Georgia", "a leaf", "New Zealand", "john seddon", "the fallopian tube", "basildon-born Perry", "france", "\"Holiday Inn\"", "preston marley", "University of Texas at Austin", "Robbie Gould", "Patrick Swayze", "Roy Spencer", "North Sudan", "Indian classical", "32", "Jocelyn Moorhouse", "Hawaii", "beethoven", "aardwolf", "ceviche", "Peter Bogdanovich", "Machiavelli", "Michael Phelps", "Milton Glaser"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6597627632783882}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, false, false, true, false, false, true, true, false, false, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true, true, true, true, false, false, true], "QA-F1": [0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6153846153846153, 0.0, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.6666666666666666, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.375, 0.3076923076923077, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3270", "mrqa_newsqa-validation-1725", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-985", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-2540", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-1121", "mrqa_naturalquestions-validation-4109", "mrqa_naturalquestions-validation-8928", "mrqa_naturalquestions-validation-10258", "mrqa_triviaqa-validation-6338", "mrqa_triviaqa-validation-4557", "mrqa_triviaqa-validation-4913", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-2550", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-3761", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-2060", "mrqa_searchqa-validation-8460"], "SR": 0.5625, "CSR": 0.5290178571428572, "EFR": 0.9642857142857143, "Overall": 0.6892857142857143}, {"timecode": 70, "UKR": 0.6875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1533", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-1911", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2215", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3610", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-379", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-487", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4902", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5097", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-58", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-816", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2205", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3978", "mrqa_naturalquestions-validation-4033", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-9842", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1284", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-2539", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3921", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3995", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-575", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10192", "mrqa_searchqa-validation-10280", "mrqa_searchqa-validation-10780", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-11046", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11237", "mrqa_searchqa-validation-11464", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-12126", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-12726", "mrqa_searchqa-validation-13694", "mrqa_searchqa-validation-13829", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-15311", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15769", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16077", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-166", "mrqa_searchqa-validation-16705", "mrqa_searchqa-validation-16759", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2610", "mrqa_searchqa-validation-2701", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-3147", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-3413", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4167", "mrqa_searchqa-validation-4277", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5692", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6944", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7060", "mrqa_searchqa-validation-7073", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-716", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8095", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-9721", "mrqa_searchqa-validation-9742", "mrqa_searchqa-validation-9812", "mrqa_squad-validation-10024", "mrqa_squad-validation-10068", "mrqa_squad-validation-10340", "mrqa_squad-validation-1052", "mrqa_squad-validation-1316", "mrqa_squad-validation-1703", "mrqa_squad-validation-199", "mrqa_squad-validation-2564", "mrqa_squad-validation-2591", "mrqa_squad-validation-291", "mrqa_squad-validation-2934", "mrqa_squad-validation-2985", "mrqa_squad-validation-3045", "mrqa_squad-validation-3269", "mrqa_squad-validation-3302", "mrqa_squad-validation-332", "mrqa_squad-validation-3372", "mrqa_squad-validation-3416", "mrqa_squad-validation-3491", "mrqa_squad-validation-3577", "mrqa_squad-validation-3609", "mrqa_squad-validation-3611", "mrqa_squad-validation-3667", "mrqa_squad-validation-3723", "mrqa_squad-validation-3745", "mrqa_squad-validation-375", "mrqa_squad-validation-3954", "mrqa_squad-validation-4127", "mrqa_squad-validation-4186", "mrqa_squad-validation-4257", "mrqa_squad-validation-436", "mrqa_squad-validation-4630", "mrqa_squad-validation-486", "mrqa_squad-validation-512", "mrqa_squad-validation-5185", "mrqa_squad-validation-5230", "mrqa_squad-validation-5348", "mrqa_squad-validation-5362", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5999", "mrqa_squad-validation-6595", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-689", "mrqa_squad-validation-6958", "mrqa_squad-validation-7047", "mrqa_squad-validation-7137", "mrqa_squad-validation-7165", "mrqa_squad-validation-7252", "mrqa_squad-validation-7394", "mrqa_squad-validation-7458", "mrqa_squad-validation-7554", "mrqa_squad-validation-7653", "mrqa_squad-validation-7701", "mrqa_squad-validation-7850", "mrqa_squad-validation-786", "mrqa_squad-validation-7956", "mrqa_squad-validation-816", "mrqa_squad-validation-8278", "mrqa_squad-validation-828", "mrqa_squad-validation-8316", "mrqa_squad-validation-8332", "mrqa_squad-validation-8496", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_squad-validation-9245", "mrqa_squad-validation-9285", "mrqa_squad-validation-9408", "mrqa_squad-validation-96", "mrqa_squad-validation-9845", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1293", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2358", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2870", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3053", "mrqa_triviaqa-validation-3089", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3146", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-3367", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4803", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-5007", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5075", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5584", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6022", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6461", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7217", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7616", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936"], "OKR": 0.791015625, "KG": 0.44375, "before_eval_results": {"predictions": ["September 13, 2012", "Hank Williams", "September 2, 1945", "Lord Banquo", "King Louie", "Waylon Jennings", "( Robert Irsay )", "18", "Kate '' Mulgrew", "Castleford", "October 2012", "Sammi Smith", "2018", "Lake Powell", "the namesake town of Manchester - by - the - Sea, Massachusetts", "South Dakota ( 30.3 % )", "five times", "`` Rockstar '' is the fifth U.S. single by the Canadian rock band Nickelback from their fifth album, All the Right Reasons ( 2005 )", "William Shakespeare's As You Like It, spoken by the melancholy Jaques in Act II Scene VII", "2 total", "the southeastern United States", "the sperm and one from the egg", "committed suicide", "2002", "the problems", "one of The Canterbury Tales by Geoffrey Chaucer", "1956", "the west coast of Central America", "the largest part of the brain", "semi-automatic, but not fully automatic, firearms by Swiss citizens and foreigners with permanent residence", "Parashara ( c. 400 -- c. 500 AD ), the author of V\u1e5bksayurveda ( the science of life of trees )", "1990", "john hurt", "Cyprus", "George W. Bush", "southerly", "snakes", "weetabix", "Hague", "Nigeria", "Harry F. Sinclair", "Russian", "1903", "motor vehicles", "2018", "pastels", "the Saint Petersburg Conservatory", "middleweight division", "her children \"have no problems about the school, they are happy about everything.\"", "CNN", "FBI's Baltimore field office", "dismissed all charges Wednesday night and ordered the release of the four men", "Apple", "228", "22-10.", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "(Peter) Rubens", "Neptune", "(Robert) Siegel", "(Albert) Brooks", "Wang Chung", "(Mark) Calder", "France", "the pound sterling"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6205018939393939}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7272727272727273, 0.0, 0.6666666666666666, 0.3636363636363636, 0.4545454545454545, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.16666666666666666, 0.6, 0.13333333333333333, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.42857142857142855, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-7651", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-10421", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-9791", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-6050", "mrqa_triviaqa-validation-7782", "mrqa_triviaqa-validation-5334", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-5809", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-680", "mrqa_hotpotqa-validation-3988", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-2700", "mrqa_searchqa-validation-14226", "mrqa_searchqa-validation-6853", "mrqa_searchqa-validation-9619", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-12794"], "SR": 0.46875, "CSR": 0.528169014084507, "EFR": 0.9411764705882353, "Overall": 0.6783222219345485}, {"timecode": 71, "before_eval_results": {"predictions": ["Norway", "Morocco", "Casablanca", "Geneva", "Classics", "New Jersey", "Baikal", "rock salt", "badger", "Kuiper Belt", "a cat", "Mimi Bobeck", "winter", "nag", "Ned", "Boston", "sport", "Ohio", "pantomime", "halfpipe", "Samuel", "Apple", "Moscow", "a bus", "shiatsu", "Columbo", "William McKinley", "Hannibal", "Ankara", "The Deep", "orangutan", "\"Death, be not\"", "Anatomy", "16", "8 bytes", "Western Australia", "more than 1,000", "ex-England football goalkeeper David Seaman", "October 29 - 30, 2012", "Andreas Vesalius", "Hawaii", "bird", "neurons", "worked", "minder", "potatoes", "shellbark", "antelope", "281", "44", "Rockbridge County", "Freeform", "1965", "200,167", "Harvard", "Martin Ingerman", "a body", "Sonia Sotomayor", "\"the most beautiful woman in the world.\"", "A staff sergeant in the U.S. Air Force", "April 26, 1913,", "Kim Il Sung", "Ricardo Valles de la Rosa,", "Tuesday"], "metric_results": {"EM": 0.6875, "QA-F1": 0.765625}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4164", "mrqa_searchqa-validation-6061", "mrqa_searchqa-validation-14509", "mrqa_searchqa-validation-7372", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-7819", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-4240", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-6671", "mrqa_triviaqa-validation-2132", "mrqa_triviaqa-validation-1374", "mrqa_triviaqa-validation-7120", "mrqa_hotpotqa-validation-4321", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-2678"], "SR": 0.6875, "CSR": 0.5303819444444444, "EFR": 1.0, "Overall": 0.690529513888889}, {"timecode": 72, "before_eval_results": {"predictions": ["alloys", "Defending Your Life", "Amadeus", "\"Spanish Ladies\"", "Charley", "Microsoft", "Beethoven", "Latin", "high jump", "Nicole Kidman", "Diamond DLL-1", "Fort Sumter", "Bucharest", "the Sahara", "Happy Days", "Mentor", "Vanna White", "Morris West", "trod", "Frank Sinatra", "Green Lantern", "infrared", "barbecue", "the Great Gondorff", "\"J Jungle Jim\"", "Old North Church", "Jerry Reed", "Jean-Paul Sartre", "Michelle Kwan", "Crispy Flautas", "Baal", "Westinghouse Electric", "gastrocnemius muscle", "prophets", "Blue laws", "capital and financial markets", "Anna Faris", "charbagh", "Shirley Partridge", "Barry Bonds", "Colonel Tom Parker", "george v", "three", "the Mad Hatter", "Barbarella", "Pentecost", "Nissan", "Brazil", "nuclear weapons", "Atlas ICBM", "William Bradford", "Tim Allen", "Rochdale, North West England", "postmodern schools", "Netherlands", "Romeo Montague", "Fullerton, California,", "misdemeanor assault charges", "education", "12-1", "completed flood preparations", "Kris Allen", "north-south highway", "a one-shot victory in the Bob Hope Classic"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6864583333333334}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-7720", "mrqa_searchqa-validation-2904", "mrqa_searchqa-validation-9180", "mrqa_searchqa-validation-12784", "mrqa_searchqa-validation-3016", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-16954", "mrqa_searchqa-validation-5649", "mrqa_searchqa-validation-15982", "mrqa_searchqa-validation-16034", "mrqa_searchqa-validation-613", "mrqa_searchqa-validation-13204", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-5481", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-3640", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-212", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-2858"], "SR": 0.609375, "CSR": 0.5314640410958904, "EFR": 1.0, "Overall": 0.6907459332191781}, {"timecode": 73, "before_eval_results": {"predictions": ["Debbie Wassermanultz", "the armadillo", "the Wilderness Road", "Bigfoot", "the Rupee", "Cold Mountain", "Frisbee", "Bewitched", "the pommel horse", "Queen Victoria", "a basement", "Isaac Newton", "the \"Ramayana\"", "Lumire", "a marshmallow", "Hillary's America", "a Whiskey Cocktail", "Blondes", "C-E", "the Old Manse", "the Caribbean", "tadpoles", "Fermium", "an accomplice-witness", "Aerie", "Woody Guthrie", "the Arawak Indians", "the Clydesdales", "Fontanelle", "Winston Churchill", "defense", "Herman Melville", "upon a military service member's retirement, separation, or discharge from active duty", "1924", "1926", "the Veterans Committee", "Dumont d'Urville Station, where the Institut polaire fran\u00e7ais Paul - \u00c9mile Victor is based", "Gwendoline Christie", "Cambridge May Ball scene, set in 1963", "British colonial government", "SE coast of China", "\"His Holiness.\"", "east coast", "indiana feldman", "benjamin mccartney", "Bodhidharma", "Lincolnshire", "\u00c9dith Piaf", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "English", "the villanelle poetic form", "7 November 1435", "1966", "Paper", "New York Shakespeare Festival", "Ribhu Dasgupta", "12 hours in jail.", "Islamic republic's alleged efforts to acquire nuclear weapons", "potential revenues from oil and gas", "Stephen Johns", "around 3.5 percent of global greenhouse emissions.", "Elin Nordegren,", "Tuesday", "Ben Roethlisberger"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5702986551298778}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, true, true, true, true, false, false, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6896551724137931, 1.0, 1.0, 0.3076923076923077, 0.4, 1.0, 0.2105263157894737, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.45454545454545453, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.11764705882352941, 0.2857142857142857, 0.8, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-7951", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-9349", "mrqa_searchqa-validation-16348", "mrqa_searchqa-validation-15440", "mrqa_searchqa-validation-14992", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-16416", "mrqa_searchqa-validation-136", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-3770", "mrqa_searchqa-validation-12722", "mrqa_searchqa-validation-1486", "mrqa_searchqa-validation-3732", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-3524", "mrqa_naturalquestions-validation-923", "mrqa_triviaqa-validation-3177", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-6231", "mrqa_triviaqa-validation-3438", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2137", "mrqa_hotpotqa-validation-95", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-2944"], "SR": 0.484375, "CSR": 0.5308277027027026, "EFR": 0.9393939393939394, "Overall": 0.6784974534193284}, {"timecode": 74, "before_eval_results": {"predictions": ["Highway 68 ( Holman Highway / Sunset Drive )", "Phillip Paley", "the next episode, `` Seeing Red ''", "classical architecture", "Darlene Cates", "Michael Madhusudan Dutta", "an active supporter of the League of Nations", "16 November 2001", "September 19 - 22, 2017", "November 1999", "on the inner wall of the pedestal", "mining", "April 1, 2016", "after World War II", "changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "Spanish missionaries, ranchers and troops", "St Pancras International, the other British calling points being Ebbsfleet International and Ashford International in Kent", "John Travolta", "1960", "Interstate 20, while the real town is nowhere near any interstate", "Butter Island", "December 1886", "18 February 2000", "a synonym for the content component of communication", "Nalini Negi", "for operations, personnel, equipment, and activities", "Lesley Gore", "depicting multiple alternative realities rather than a novel", "September 8, 2017", "the Coercive Acts", "American production duo The Chainsmokers", "Procol Harum", "Tahrir Square", "team GB", "china", "Slim Whitman", "ely", "Castor", "castor canadensis", "gasoline", "Hanford Nuclear Reservation", "Eddie Collins", "over 1.6 million", "Paul W. S. Anderson", "Roscoe Lee Browne", "dice", "Clara Petacci", "Charice", "Citizens", "that Iran could be secretly working on a nuclear weapon", "Somali's coast.", "Daniel Radcliffe", "Pakistan", "a motel,", "Joe Jackson's", "to set up headquarters in Dublin.", "Winston Churchill", "Ricky Martin", "Mark Twain", "San Salvador", "Amelia Earhart", "ethanol", "Knott's Berry Farm", "the pituitary"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6980919884010917}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.33333333333333337, 1.0, 1.0, 0.08695652173913043, 0.3636363636363636, 1.0, 0.0, 0.0, 1.0, 0.08, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8750000000000001, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-138", "mrqa_naturalquestions-validation-9253", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-9986", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-8787", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-283", "mrqa_triviaqa-validation-3294", "mrqa_hotpotqa-validation-1559", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-5521", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-1950", "mrqa_newsqa-validation-192", "mrqa_searchqa-validation-6444"], "SR": 0.5625, "CSR": 0.53125, "EFR": 1.0, "Overall": 0.690703125}, {"timecode": 75, "before_eval_results": {"predictions": ["North Atlantic Ocean", "Dmitri Mendeleev", "Tagore", "charbagh", "Amerigo Vespucci", "2018", "at the head of Lituya Bay in Alaska", "0.30 in ( 7.6 mm )", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "the breast or lower chest of beef or veal", "December 14, 2017", "Brian Steele", "around 2011", "19 June 2018", "Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s", "Harishchandra", "pickup trucks", "If a vehicle towing a trailer skids", "Todd Griffin", "Julie Adams", "FIGG Bridge Engineers, a Tallahassee - based firm", "Turducken", "synovial joint", "Hallertau in Germany", "Billy Idol", "2017", "2020", "August Darnell", "International System of Units ( SI )", "Rufus and Chaka Khan", "a political ideology", "New York City", "Trimdon, County Durham", "zelle", "green", "49", "oscar clendenning Hammerstein II", "eros", "Robinson", "argentina", "1875", "Brendan O'Brien", "33 of the 100 seats", "Manchester", "Mitsubishi Motors", "John McClane", "Adelaide", "Exit 82", "attracted some U.S. senators who couldn't resist taking the vehicles for a spin.\"", "380,000 pounds", "Spc. Megan Lynn Touma,", "operates 52 nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States.", "Lindsey Vonn", "to do more to stop the Afghan opium trade", "Bill", "comments he made last night at the Annual Caddy Awards dinner in Shanghai,\"", "Byron", "a pythons", "Odin", "Pop art", "Wall Street", "a blubber", "Kinsey Millhone", "Romanov"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6242294405800598}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 0.3333333333333333, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.16666666666666669, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 0.0, 1.0, 0.2105263157894737, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-8183", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-8845", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-1497", "mrqa_triviaqa-validation-6857", "mrqa_triviaqa-validation-6288", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-2197", "mrqa_hotpotqa-validation-4352", "mrqa_hotpotqa-validation-404", "mrqa_hotpotqa-validation-620", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-2813", "mrqa_searchqa-validation-4241", "mrqa_searchqa-validation-4802", "mrqa_searchqa-validation-2728", "mrqa_searchqa-validation-15046"], "SR": 0.53125, "CSR": 0.53125, "EFR": 0.9333333333333333, "Overall": 0.6773697916666668}, {"timecode": 76, "before_eval_results": {"predictions": ["several weeks,", "involvement during World War II in killings at a Nazi German death camp in Poland.", "the Klan", "Filippo Inzaghi", "pesos", "Silicon Valley.", "Booches Billiard Hall,", "Liverpool", "Expedia", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "The Ministry of Defense said the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "Afghan National Security Forces", "Two United Arab Emirates based companies", "planned attacks", "$273 million", "\"disagreements\" with the Port Authority of New York and New Jersey,", "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "actress", "Brian Smith", "Brazil", "she supports the two-state solution to the Mideast conflict,", "$3 billion,", "The Stooges comedic farce entitled \"Three Little Beers,\"", "he was \"greatly moved\" by meeting victims of abuse in Valletta, Malta.", "Dan Brown", "the Indians were gathering information about the rebels to give to the Colombian military.", "mental health", "Annie Duke", "570 billion pesos ($42 billion)", "India", "response to a civil disturbance call,", "November 17, 1800", "legislation", "concerned with all legal affairs", "Giancarlo Stanton", "A status line", "David Ben - Gurion", "18", "TC and Paul", "antelopes", "Mikhail S. Gorbachev", "14", "Ravi Shankar", "Baton Rouge", "Peter Falk", "gymnastics", "Mussolini", "the Hebrides", "August 21, 1995", "1966", "Algernod Lanier Washington", "Highwayman", "Michael Rispoli", "Middlesbrough", "Brenton Thwaites", "adhesion", "a crumpets", "the Potomac River", "dinoflagellates", "Double Indemnity", "Glengarry Glenross", "The Piano", "$40,000"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7180506974213436}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.23529411764705882, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23076923076923078, 0.8947368421052632, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.6, 0.1111111111111111, 0.5, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2114", "mrqa_newsqa-validation-109", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1711", "mrqa_naturalquestions-validation-2618", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-1452", "mrqa_triviaqa-validation-3804", "mrqa_triviaqa-validation-4623", "mrqa_hotpotqa-validation-3491", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-2003", "mrqa_searchqa-validation-13091", "mrqa_searchqa-validation-12186", "mrqa_searchqa-validation-12900"], "SR": 0.609375, "CSR": 0.5322646103896104, "EFR": 0.88, "Overall": 0.6669060470779221}, {"timecode": 77, "before_eval_results": {"predictions": ["Dr. Jennifer Arnold and husband Bill Klein,", "David Beckham", "Alexandre Caizergues,", "jobs", "Saturn", "Adam Yahiye Gadahn,", "debris", "1959,", "lower house of parliament,", "Six members", "into a residential area in East Java early Wednesday,", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "psychotropic drugs", "3 to 17", "NASCAR,", "former president, Alberto Fujimori,", "jazz", "Mark Fields of Ford", "Aniston, Demi Moore and Alicia Keys", "FBI Special Agent Daniel Cain,", "the commissions are OK, \" provided that they are properly structured and administered.\"", "northwest Pakistan", "Daytime Emmy Lifetime Achievement Award.", "Jewish", "President Obama's", "Woosuk Ken Choi,", "forcibly injecting them with psychotropic drugs", "Almost all British troops", "Filippo Inzaghi", "Greeley, Colorado,", "building bombs,", "\"E! News\"", "Sophia Akuffo", "North Carolina", "Los Lonely Boys", "LED illuminated display", "pagan custom, namely, the winter solstice", "1966", "1985", "sedimentary rock", "sedge", "vertebrae", "Polish", "To Kill a Mockingbird", "the Passover", "Samuel Johnson", "makes", "just two years", "the Knight Company", "Ferdinand Magellan", "23 March 1991", "the 34th President of the United States", "154 days", "three members", "\"I, (Annoyed Grunt)- Bot\"", "Scottish", "Muhammad Ali", "Captain Nemo", "Man Ray", "\"Hans & Franz\" \"SNL\" skit", "James Watt", "Mercury and Venus", "snakes", "the Scripps National Spelling Bee"], "metric_results": {"EM": 0.5625, "QA-F1": 0.685463169642857}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.6875000000000001, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.5, 0.33333333333333337, 0.5714285714285715, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-4042", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-3749", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-6242", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-2469", "mrqa_triviaqa-validation-5116", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2281", "mrqa_triviaqa-validation-1043", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-826", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-3504", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12187"], "SR": 0.5625, "CSR": 0.5326522435897436, "EFR": 0.9642857142857143, "Overall": 0.6838407165750916}, {"timecode": 78, "before_eval_results": {"predictions": ["a micronutrient-rich diet", "Jamie Fraser (Sam Heughan)", "August 14, 1848", "American Horror Story", "Daniel Craig", "Adelaide City Football Club", "Captain Marvel", "\"Nebo Zovyot\"", "Gainsborough Trinity F.C.", "Carlos Santana", "DreamWorks Animation", "40 Acres and a Mule Filmworks", "Fort Bragg", "Division I Football Bowl Subdivision", "Robert A. Iger", "It's Always Sunny in Philadelphia", "A play-by-post role-playing game (or sim)", "Newcastle upon Tyne, England", "Stephen James Ireland", "Rooster", "27 January 1974", "second cousin once removed", "James Dean", "Chelsea Melini", "his bomber crash landed in the ocean during World War II", "Illinois", "Twitch Interactive, a subsidiary of Amazon.com.", "1853", "Coalwood", "Sarajevo", "Gareth Barry", "Ryukyuan people", "the east coast of Queensland, thereby including the Great Barrier Reef, in the east by Vanuatu ( formerly the New Hebrides ) and by New Caledonia", "Samantha Jo `` Mandy '' Moore", "a patronymic surname, which arose separately in England and Wales", "Duisburg", "in the middle of the 15th century, in Yemen's Sufi monasteries", "the 4th century", "between 1765 and 1783", "President", "Another Day in Paradise", "Ronnie Carroll", "North by Northwest", "an umbrella", "Paris", "stomach", "sir Laurence kern", "the 114.5 metre sculpture", "Afghanistan", "82 passengers,", "appealed against the punishment for the player", "South Africa", "then-presidential candidate Barack Obama,", "what caused the collapse of the building which contained the city's historical archives,", "in the Bronx and grew up in a public housing project, not too far from the stadium of her favorite team -- the New York Yankees.", "eco videos", "human skin", "The Legend of Sleepy Hollow", "the firebrand Shi'ite cleric became a major", "Louisville, Kentucky", "Tigger", "Giovanni Bertati", "Ariel Sharon", "\"What a joy to breathe the balmy air of Grosvenor Square\""], "metric_results": {"EM": 0.5, "QA-F1": 0.635352408008658}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false], "QA-F1": [0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.26666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.8, 1.0, 0.3636363636363636, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.3333333333333333, 0.5714285714285715, 0.0909090909090909, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5873", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4909", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-3914", "mrqa_hotpotqa-validation-4704", "mrqa_hotpotqa-validation-1423", "mrqa_hotpotqa-validation-4791", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-1965", "mrqa_triviaqa-validation-106", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-2138", "mrqa_triviaqa-validation-5468", "mrqa_newsqa-validation-794", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-3258", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-6", "mrqa_searchqa-validation-9651", "mrqa_searchqa-validation-10111", "mrqa_searchqa-validation-6928", "mrqa_searchqa-validation-9708", "mrqa_searchqa-validation-8670", "mrqa_searchqa-validation-13464"], "SR": 0.5, "CSR": 0.5322389240506329, "EFR": 0.96875, "Overall": 0.6846509098101266}, {"timecode": 79, "before_eval_results": {"predictions": ["St. Louis", "Warrington, Florida", "Pope Sergius III", "Muskogean", "Lt. Col. Masahiko tofuhita", "England national team", "Carlos Santana", "Dana Fox", "7 January 1936", "The Onion", "Portsea", "Lapland", "1698", "Cuban", "2010", "Virginia", "she is a well-known historical figure in 16th-century Irish history.", "Ouse and Foss", "Thrushcross Grange", "Marijus Adomaitis", "My Boss, My Teacher", "York County", "Grave Digger", "a mermaid", "Europop", "Humberside", "Viaport Rotterdam", "Yasir Hussain", "566", "Cookstown", "the Institute for Jewish Research", "Kairi", "Network - Protocol driver ( Middleware driver )", "head coach", "Phoebe ( MacKenzie Mauzy )", "Malayalam", "October 19, 1961 -- January 19, 1963", "Lyndon B. Johnson", "a maritime signal, indicating that the vessel flying it is about to leave", "the quadrennial rugby union world championship", "alligators", "otto", "polo", "gogglebox", "white", "12", "castle", "Octavian", "38", "hanged in 1979", "Karen Floyd", "Canada.", "Michael Arrington,", "South Carolina Republican Party Chairwoman Karen Floyd", "usion teams,", "The remaining 240 patients will be taken to hospitals in other provinces", "an out-of-body experience", "air dominance", "Wyoming", "an acting truthfully related to... like Abe Lincoln", "the Hudson", "Catherine the Great", "the Taliban", "James Naismith"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5838451479076479}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.0, 0.14285714285714285, 1.0, 1.0, 0.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.5454545454545454, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-2988", "mrqa_hotpotqa-validation-946", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-5700", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-5608", "mrqa_hotpotqa-validation-2012", "mrqa_hotpotqa-validation-4847", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-983", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8603", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6426", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-5498", "mrqa_triviaqa-validation-3825", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-1210", "mrqa_searchqa-validation-1484", "mrqa_searchqa-validation-13952", "mrqa_searchqa-validation-1570", "mrqa_searchqa-validation-4615", "mrqa_searchqa-validation-10199"], "SR": 0.46875, "CSR": 0.5314453125, "EFR": 0.9705882352941176, "Overall": 0.6848598345588235}, {"timecode": 80, "UKR": 0.658203125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1520", "mrqa_hotpotqa-validation-1533", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1885", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-1911", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2520", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-379", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-5097", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-58", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-834", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3910", "mrqa_naturalquestions-validation-3978", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-6854", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9842", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1284", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1476", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2210", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2539", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-3921", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3995", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-575", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-794", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10087", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10192", "mrqa_searchqa-validation-10280", "mrqa_searchqa-validation-10780", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-11046", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11237", "mrqa_searchqa-validation-11464", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11616", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-12726", "mrqa_searchqa-validation-13829", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14807", "mrqa_searchqa-validation-15311", "mrqa_searchqa-validation-15405", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16077", "mrqa_searchqa-validation-16403", "mrqa_searchqa-validation-16416", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16705", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2610", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-3147", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-3382", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4167", "mrqa_searchqa-validation-4277", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-6239", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6944", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7060", "mrqa_searchqa-validation-7073", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-716", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8095", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9073", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-9721", "mrqa_squad-validation-10024", "mrqa_squad-validation-10340", "mrqa_squad-validation-1052", "mrqa_squad-validation-1316", "mrqa_squad-validation-1703", "mrqa_squad-validation-199", "mrqa_squad-validation-2564", "mrqa_squad-validation-291", "mrqa_squad-validation-2934", "mrqa_squad-validation-2985", "mrqa_squad-validation-3045", "mrqa_squad-validation-3269", "mrqa_squad-validation-3302", "mrqa_squad-validation-332", "mrqa_squad-validation-3416", "mrqa_squad-validation-3491", "mrqa_squad-validation-3577", "mrqa_squad-validation-3609", "mrqa_squad-validation-3611", "mrqa_squad-validation-3667", "mrqa_squad-validation-375", "mrqa_squad-validation-3954", "mrqa_squad-validation-4127", "mrqa_squad-validation-4186", "mrqa_squad-validation-4257", "mrqa_squad-validation-436", "mrqa_squad-validation-4630", "mrqa_squad-validation-5185", "mrqa_squad-validation-5230", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5999", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-6958", "mrqa_squad-validation-7047", "mrqa_squad-validation-7165", "mrqa_squad-validation-7252", "mrqa_squad-validation-7394", "mrqa_squad-validation-7458", "mrqa_squad-validation-7554", "mrqa_squad-validation-7653", "mrqa_squad-validation-7701", "mrqa_squad-validation-7850", "mrqa_squad-validation-786", "mrqa_squad-validation-8278", "mrqa_squad-validation-828", "mrqa_squad-validation-8316", "mrqa_squad-validation-8332", "mrqa_squad-validation-8496", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_squad-validation-9245", "mrqa_squad-validation-9285", "mrqa_squad-validation-96", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2358", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3053", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3146", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-3317", "mrqa_triviaqa-validation-3367", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4695", "mrqa_triviaqa-validation-4803", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-5007", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5075", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-5584", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-6022", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6461", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7616", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-7782", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936"], "OKR": 0.77734375, "KG": 0.46015625, "before_eval_results": {"predictions": ["Parthenon", "Dennis Potter", "6", "Botticelli", "sapodilla", "Venice", "architect", "mktgpartners Canada", "Chief Inspector of Prisons", "Some Like It Hot", "havrevrevre", "the Gunpowder Plot", "Prussia", "New York City", "Wee Jimmy Krankie", "Norway", "mrslin man", "1812", "William Shatner", "phil mcckelson", "mrs v.", "lenny Henry", "mrs prism", "Camellia sinensis", "pale horse, named Binky", "geomorphology", "Bolivia", "gold", "solids", "Denver", "heat and boil water for tea", "reasoning horses", "Peter Brooks", "Mike Nesmith", "Gloria ( Lisa Stelly )", "first developed and popularized by the Israeli company Mirabilis in 1996", "two", "pigs", "Janie Crawford", "Dragon Ball GT chief character designer Katsuyoshi Nakatsuru", "$60 million", "Alan David Sokal", "Crystal Dynamics", "1955", "Alexandre Dumas", "Peter & Gordon", "Little Dixie", "Learjet", "$17,000", "there is not a process", "schedule a screening appointment for the early part of their menstrual cycle.", "forgery and flying without a valid license,", "700", "April.", "eight", "nine newly-purchased bicycles", "ethanol", "Florida", "the Pequots", "a flying saucer", "Voltaire", "the Jordan River", "a Digestive Drinks", "Prince William and Kate Middleton"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5621685606060607}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, true, false, false, true, false, true, false, false, true, true, false, true, false, false, false, false, false, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, false, true, false, false, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.09523809523809523, 0.2857142857142857, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_triviaqa-validation-3537", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-5545", "mrqa_triviaqa-validation-259", "mrqa_triviaqa-validation-2530", "mrqa_triviaqa-validation-7411", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-1303", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3174", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-2083", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-5014", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-1801", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-34", "mrqa_hotpotqa-validation-1142", "mrqa_newsqa-validation-2358", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1611", "mrqa_searchqa-validation-6294", "mrqa_searchqa-validation-4617", "mrqa_searchqa-validation-11144", "mrqa_searchqa-validation-15708"], "SR": 0.46875, "CSR": 0.5306712962962963, "EFR": 0.9705882352941176, "Overall": 0.6793925313180827}, {"timecode": 81, "before_eval_results": {"predictions": ["catalyst", "coco chiffon", "Clara Wieck", "philenophon", "carpathia", "Portugal", "LBJ forms commission", "perfume", "Brighton", "a righteous man", "throw", "Wars of the Roses", "Norman Brookes", "albert Reynolds", "al jazeera", "m69", "Bayern M\u00fcnchen", "superficial thrombophlebitis", "Lorelei", "mrs lancer", "sir Walter Scott", "Shayne Ward", "French Guiana", "Vienna", "Amsterdam", "pickled peppers", "air pressure", "Paris", "Spain", "Arizona Diamondbacks", "little jack horner", "marc", "Freedom Day", "Have I Told You Lately", "in the New Testament", "Woody Paige", "1661", "2015", "amino acids glycine and arginine", "the right", "Jacksonville Jacksonville Jacksonville", "January 2004", "The More", "Adelaide Lightning", "Idaho", "Donald Sterling", "Richard Arthur", "Retina display", "giant mega-yacht 'Wally Island'", "an \"unnamed international terror group\"", "her decades-long portrayal of Alice Horton on", "the death of a pregnant soldier", "apps", "nearly $162 billion in war funding", "\"Mad Men\"", "$273 million", "artillery", "coronary heart", "Brownsville", "kings", "Frank Sinatra", "Tasmania", "the Bodleian", "Ivy Dickens"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6924242424242424}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, false, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.33333333333333337, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-402", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-1438", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-3302", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2348", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7501", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-7319", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-686", "mrqa_hotpotqa-validation-1220", "mrqa_hotpotqa-validation-370", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-162", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-6131", "mrqa_searchqa-validation-8274", "mrqa_searchqa-validation-9043"], "SR": 0.609375, "CSR": 0.5316310975609756, "EFR": 1.0, "Overall": 0.6854668445121951}, {"timecode": 82, "before_eval_results": {"predictions": ["buxton", "mikado", "Yellowstone", "hanks", "london", "almond", "Salvation Army", "bali", "0", "vignale", "John of Gaunt", "mountain tunes", "the Benedictine Order", "Trinity College, Cambridge", "teaching evolution", "china", "1", "phosphorus", "1927", "Tennessee", "architecture", "Hinduism", "Howard Keel", "australia", "USS Missouri", "o", "Lithium", "st Anne's", "Colleen McCullough", "\u00ef\u00bf\u00bd", "australian chiles", "mike", "1439", "A monocot related to lilies and grasses", "Amanda Leighton", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "the coccygeal nerve", "Nigeria is the largest oil and gas producer in Africa", "36 months", "Lori Rom", "Charles Otto Puth Jr.", "Alfred Joel Horford Reynoso", "Wonder Woman", "Hellenism", "Castle on the Hill", "2013 Cannes Film Festival", "Dominican", "World War I", "\"What she's doing is putting a personal and human face on the issue... there's nothing more crucial,\"", "resources", "The Stooges", "Kandi Burruss,", "\"We must find ways to relieve some of this stress,\"", "Somali-based", "power-sharing talks", "Stanford University", "the blimps", "the Lisa", "the South Beach diet", "the root", "the Southern Cross", "Indonesia", "the Homo erectus", "Athol Fugard"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5686801046176047}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, true, false, true, false, true, false, false, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4444444444444445, 0.0, 0.19999999999999998, 0.3636363636363636, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3636363636363636, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-5145", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-5933", "mrqa_triviaqa-validation-5460", "mrqa_triviaqa-validation-6658", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-3401", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-5081", "mrqa_triviaqa-validation-4668", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-2255", "mrqa_naturalquestions-validation-1090", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-190", "mrqa_hotpotqa-validation-4649", "mrqa_hotpotqa-validation-5077", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-1031", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-2236", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-10249"], "SR": 0.484375, "CSR": 0.5310617469879518, "EFR": 0.9696969696969697, "Overall": 0.6792923683369843}, {"timecode": 83, "before_eval_results": {"predictions": ["Luigi Pirandello", "benjamin Samson", "boll weevil", "Taenarum", "Liechtenstein", "tartan", "Mark Darcy", "z", "mulhac\u00e9n", "nasdaq", "gatsby", "polo", "James Garfield", "Operation", "Rarely", "mike Gatting", "australia", "Massachusetts", "eldorado", "willward p Powell", "Armageddon", "purple rain", "Sinclair Lewis", "Herman Wouk", "Silverstone", "Keswick", "a toad", "Runic", "blue", "John Nash", "Robert Plant", "Pocahontas", "during the 1890s Klondike Gold Rush", "5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "Washington, D.C.", "Massillon, Ohio", "Robin", "All My Sons by Arthur Miller", "1975", "frequency f", "June 2, 2008", "Ronnie Schell", "Estadio de L\u00f3pez Cort\u00e1zar", "George Adamski", "Bangor International Airport", "Paul Bettany and Jeff Goldblum", "The Swatch Group", "Campbellsville", "British oil companies' efforts to drill off the northern coast of the islands.", "\"El Viceroy\"", "Charles Darwin", "1-1", "Philippines", "three out of four", "the surgical anesthetic propofol", "murder in connection with the death of a woman", "hogs", "Aquitaine", "Niels Bohr", "Sicilian pizza", "the funny bone", "family trees", "King of Hill", "the Tigris River"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6272294207317073}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.2926829268292683, 0.0, 0.08333333333333334, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5749", "mrqa_triviaqa-validation-706", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-1767", "mrqa_triviaqa-validation-866", "mrqa_triviaqa-validation-5704", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-1714", "mrqa_triviaqa-validation-4943", "mrqa_triviaqa-validation-2603", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-5798", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-662", "mrqa_newsqa-validation-1903", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-837", "mrqa_searchqa-validation-4860", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-15607", "mrqa_searchqa-validation-8422", "mrqa_searchqa-validation-9720"], "SR": 0.53125, "CSR": 0.5310639880952381, "EFR": 1.0, "Overall": 0.6853534226190476}, {"timecode": 84, "before_eval_results": {"predictions": ["Tony Blair", "brown", "sam Mendes", "Operation Dynamo", "outlaw series", "john martin", "spain", "spouse", "Messenger", "Hungary", "granada", "rugby", "oliver Twist", "whiskey", "alien Invader Films", "Big Dipper", "ape", "Luigi Pirandello", "h Hillsborough", "horseshoes", "Annie Leibovitz", "atomic kitten", "thalia", "Tommy Roe", "cuticle", "peter", "rings", "peter Sampras", "spain house", "Sherlock Holmes", "maxilla", "the Suez Canal", "Presley Smith", "in the blood to the liver", "The Impalas", "in southern Turkey, dividing the Mediterranean coastal region of southern Turkey from the central Anatolian Plateau", "Henry Selick", "Patricia Field", "Tulsa, Oklahoma", "-- `` full '' sexual intercourse", "Linda Ronstadt", "romantic comedy", "Bonkyll Castle", "American", "Outstanding Lighting Design", "North Atlantic Conference", "12", "George M. Cohan", "\"We thought we'd be really big in Liverpool.\"", "Taliban", "\"I sort of had a fascination with John Dillinger when I was about 10, 11 years old, for some reason,\"", "Tim Clark, Matt Kuchar and Bubba Watson", "Dubai", "early detection and helping other women cope with the disease.", "Michelle Rounds", "28", "the 400th anniversary", "London", "a hump", "Big Brother", "Tom Cruise", "a short circuit", "Katherine Heigl", "\"being behind the eight ball\""], "metric_results": {"EM": 0.65625, "QA-F1": 0.7497237137862137}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3076923076923077, 1.0, 0.15384615384615385, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_triviaqa-validation-5562", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4639", "mrqa_triviaqa-validation-6077", "mrqa_triviaqa-validation-5074", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-1998", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-5458", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-8950", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-4123", "mrqa_newsqa-validation-2121", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-2915", "mrqa_searchqa-validation-15106", "mrqa_searchqa-validation-15145"], "SR": 0.65625, "CSR": 0.5325367647058823, "EFR": 0.9090909090909091, "Overall": 0.6674661597593582}, {"timecode": 85, "before_eval_results": {"predictions": ["Los Ticos", "\"a potential hazard may occur due to re-entry of satellite debris into the earth's atmosphere.\"", "\"something's wrong with this lady.\"", "Chevron", "Monday night", "citizenship", "The Bronx County District Attorneys Office", "1950s,", "eventual closure of Guanta Bay prison and CIA \"black site\" prisons,", "flooding and debris", "unprecedented rise in American politics.", "attempted car-jacking", "45 minutes, five days a week.", "Niger Delta.", "Los Alamitos Joint Forces Training Base", "10 below", "\"Mad Men\"", "Fargo, North Dakota,", "Asashoryu's", "she was held hostage by a still unidentified group of bandits.", "at checkposts and military camps in the Mohmand agency,", "China", "President Obama", "Mashhad,", "Washington State's decommissioned Hanford nuclear site,", "Bollywood superstar", "16", "prisoners at the South Dakota State Penitentiary", "Thursday and Friday", "the iPods", "3-0", "know what is important in life,", "armored fighting vehicle", "International System of Units ( SI ), the modern form of the metric system", "Nashville", "Chelsea ( 2009 -- 10 )", "her castle", "BC Jean", "Brenda", "China", "Emily Davison", "priam", "new york history", "streptococcus", "lions of Islam", "business cycle", "Spanish", "stop motion effects", "international football competition", "gamecock", "Alexandre Dimitri Song Billong", "40 million", "26,000", "1911", "Singha (Thai: \u0e2a\u0e34\u0e07\u0e2b\u0e4c ) is a 5% abv pale lager produced by Boon Rawd Brewery.", "Dirk Werner Nowitzki", "Mike Myers", "Louisiana", "lily", "actress", "Bob Dylan", "Princess Diana", "credit", "Uncle Tom's Cabin"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5399169090713208}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, true, false, false, false, false, true, false, false, true, false, true, true], "QA-F1": [0.0, 0.07692307692307693, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5882352941176471, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.08, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2666666666666667, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-341", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2395", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-5820", "mrqa_naturalquestions-validation-2600", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-9436", "mrqa_triviaqa-validation-6440", "mrqa_triviaqa-validation-2820", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-2511", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-4680", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-5239", "mrqa_hotpotqa-validation-1467", "mrqa_searchqa-validation-10741", "mrqa_searchqa-validation-3250", "mrqa_searchqa-validation-5024", "mrqa_searchqa-validation-14366"], "SR": 0.46875, "CSR": 0.5317950581395349, "EFR": 1.0, "Overall": 0.6854996366279069}, {"timecode": 86, "before_eval_results": {"predictions": ["australia", "sternum", "pink", "clark", "sam Cooke", "bette davis", "faversham", "coke", "spain", "australia", "Mediterranean", "afro", "golf", "will carling", "Jonathan Swift", "mikef breuer", "nyasaland", "ken Russell", "crows", "Enrico Caruso", "tara", "Morgan Spurlock", "spain", "spain", "Saturn", "mayflower", "e", "1879", "lizards", "cosmos", "time", "The West Wing", "Kimberlin Brown", "one", "activates a relay which will handle the higher current load", "Andy Warhol", "Mel Tillis", "Organisms in the domains of Archaea and Bacteria", "silk, hair / fur ( including wool ) and feathers", "gold ( Au )", "Thomas Perez", "1937", "bioelectromagnetics", "Len Wiseman", "Jewish woman", "Nathan Bedford Forrest", "tower house", "Bruce Almighty", "Six of the 15", "Mashhad,", "issued his first military orders as leader of North Korea", "200.", "test scores and graduation rates", "Nothing", "several weeks,", "digging", "The Beverly Hillbillies", "Airbus", "Kenya", "frank", "David Beckham", "The Mousetrap", "a guardian angel", "The Clouds"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5926846590909091}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.18181818181818182, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2920", "mrqa_triviaqa-validation-6109", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-5514", "mrqa_triviaqa-validation-504", "mrqa_triviaqa-validation-960", "mrqa_triviaqa-validation-3736", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-1587", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-2548", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-5531", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-2670", "mrqa_hotpotqa-validation-3506", "mrqa_newsqa-validation-2336", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-1614", "mrqa_newsqa-validation-77", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-12949"], "SR": 0.5625, "CSR": 0.5321479885057472, "EFR": 1.0, "Overall": 0.6855702227011494}, {"timecode": 87, "before_eval_results": {"predictions": ["CBS", "The Los Angeles Dance Theater", "Northern Ireland", "Richard Allen Street", "1983 Summer Universiade", "Missouri Tigers", "fourth-largest", "Buddha's delight", "Dame Helen Mirren and Dame Judi Dench", "Toshi Ichiyanagi", "1988", "Reinhard Heydrich", "Bass", "2004 Paris Motor Show", "Jeffrey Chiang", "Hanford Site", "December 19, 1998", "Mickey's PhilharMagic", "Arizona Health Care Cost Containment System", "Donald Sterling", "Sada Carolyn Thompson", "Matt Groening", "Nick Cannon Show", "glee", "Danish", "Portal", "Enemy", "July 25 to August 4", "2015", "810", "San Diego County Fair", "The Seasiders", "via redox ( both reduction and oxidation occurring simultaneously ) reactions", "Iowa", "SI joint", "Stefanie Scott", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W", "erosion", "Kerris Lilla Dorsey", "Joseph Sherrard Kearns", "spark-ignition", "Jack Lemmon", "mEXICO", "Suez Canal", "mmorpg", "jape", "oliver", "Tina Turner", "Roger Federer", "five", "public toilets and playgrounds.", "Haiti's minister of culture and information.", "Ferrari", "mark", "Les Bleus", "228", "Frederick II", "burping", "Vietnam", "centigrade", "Boston", "New Orleans", "Wilbur Clark", "The Curse of the Black Pearl"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6903273809523809}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, false, true, true, false, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3545", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-4584", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-8417", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-1673", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-761", "mrqa_searchqa-validation-1818", "mrqa_searchqa-validation-256", "mrqa_searchqa-validation-280", "mrqa_searchqa-validation-11596", "mrqa_searchqa-validation-10796"], "SR": 0.640625, "CSR": 0.5333806818181819, "EFR": 0.9565217391304348, "Overall": 0.6771211091897233}, {"timecode": 88, "before_eval_results": {"predictions": ["Sunday,", "severe flooding", "southern city of Naples", "Hundreds of contraband cell phones", "he had tried to activate a \"kill switch\" that would cut off the well before abandoning the structure.", "be silent.", "on the roadway", "Chinese", "Taliban", "served in the military,", "Prague is a city of romance, of incredible architecture", "suicide car", "The Intertropical Convergence", "Gordon Brown", "\"Quiet Nights,\"", "17-month", "Asashoryu", "managing", "Hungary", "Bush administration", "London", "Ron Howard", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "former World Trade Center's \"archaeological heart,\"", "Dodi Fayed,", "four", "Operation Crank Call,\"", "District Attorney Larry Abrahamson", "her child's piano lessons.", "the equator,", "Michael Partain,", "Nigeria,", "Abid Ali Neemuchwala", "Indian Civil Service", "Buddhism", "Waylon Jennings", "Bart Cummings", "You are a puzzle", "Lulu", "interstitial and intravascular", "jones", "March 1939", "centaurs", "Aesir", "Truro", "swindon town", "sisyphus", "calcium sulfate", "Andrea Ch\u00e9nier", "La Familia Michoacana", "nausea, vomiting, diarrhea, jaundice, fever, and abdominal pain", "Central Avenue", "and attends a college of magic in New York", "Lewis and Clark Expedition", "Japan", "band Green Day", "William", "John Donne", "tusks", "Joan of Arc", "lilac", "the Library of Congress", "snacks", "R.E.M."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6005801893519918}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.5714285714285715, 0.046511627906976744, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.11111111111111112, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.3636363636363636, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-695", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-234", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2214", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3978", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-5125", "mrqa_triviaqa-validation-3411", "mrqa_triviaqa-validation-3218", "mrqa_triviaqa-validation-191", "mrqa_hotpotqa-validation-5355", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-3984", "mrqa_searchqa-validation-12880"], "SR": 0.53125, "CSR": 0.5333567415730337, "EFR": 0.9333333333333333, "Overall": 0.6724786399812734}, {"timecode": 89, "before_eval_results": {"predictions": ["38,", "have a smile on her face when her kids were around.", "Pakistan's combustible Swat Valley,", "education about rainforests.", "The Senate has joined the House in passing an additional $2 billion for Cash for Clunkers, allowing the program to move forward. This will bring additional buyers to the showroom.", "\"extremely weak\" and said he weighs barely 100 pounds in a court document filed this week, but he walked on his own during the 45 minutes he was at the ceremony.", "Africa", "Bollywood superstar", "NATO", "Wigan Athletic", "technology experts Michael Arrington,", "Africa", "Osama", "between Pyongyang and Seoul", "Saturday's Hungarian Grand Prix.", "\"Empire of the Sun,\"", "Schalke", "Hu Jintao", "cars", "more than 4,000", "1 million", "Tomas Olsson,", "\"Great Charter\" in Latin.", "64,", "jazz", "last surviving British soldier from World War I", "Pixar's", "Miami Beach, Florida,", "some of the best stunt ever pulled off", "near Pakistan's border with Afghanistan", "his business dealings for possible securities violations", "five", "12.9 - kilometre ( 8 mi )", "Pakistan", "The early modern period", "Wisconsin", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "the `` 0 '' trunk code", "Vancouver, British Columbia", "between Division Street and East Broadway", "mike woodforde", "president", "hippety hopper", "tony", "mike crawford", "potatoes", "annette Crosbie", "Group IIB Elements", "Knoxville, Tennessee", "Coalwood", "Sun Records founder Sam Phillips", "Westminster system", "Charles Bronson", "Europop", "blood sausage", "northeastern part", "T. S. Eliot", "Elie Wiesel", "Juliana", "Deep brain stimulation", "shalom", "Nixon", "Genghis Khan", "a packer"], "metric_results": {"EM": 0.46875, "QA-F1": 0.655316738647346}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 0.10526315789473685, 0.1702127659574468, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.8571428571428571, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.5714285714285715, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.5, 1.0, 0.923076923076923, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 1.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-3398", "mrqa_newsqa-validation-3304", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-3358", "mrqa_newsqa-validation-2682", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-6949", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-5000", "mrqa_triviaqa-validation-6970", "mrqa_triviaqa-validation-5601", "mrqa_triviaqa-validation-3605", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-1662", "mrqa_hotpotqa-validation-973", "mrqa_searchqa-validation-9368", "mrqa_searchqa-validation-8966", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13619", "mrqa_searchqa-validation-16575"], "SR": 0.46875, "CSR": 0.5326388888888889, "EFR": 0.9117647058823529, "Overall": 0.6680213439542484}, {"timecode": 90, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1520", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1779", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1885", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-190", "mrqa_hotpotqa-validation-1911", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2520", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-379", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-58", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-824", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-951", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10682", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-1959", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3612", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3910", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-5820", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-6854", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1284", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1476", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2210", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-234", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2539", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-3921", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3995", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-575", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10192", "mrqa_searchqa-validation-10280", "mrqa_searchqa-validation-1075", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-11046", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11144", "mrqa_searchqa-validation-11237", "mrqa_searchqa-validation-11464", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11616", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-12633", "mrqa_searchqa-validation-12726", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14807", "mrqa_searchqa-validation-15311", "mrqa_searchqa-validation-15405", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16077", "mrqa_searchqa-validation-16403", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16705", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2610", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-3147", "mrqa_searchqa-validation-317", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-3382", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4277", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-6239", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6944", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7073", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-716", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8095", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-9721", "mrqa_squad-validation-10024", "mrqa_squad-validation-10340", "mrqa_squad-validation-1052", "mrqa_squad-validation-1316", "mrqa_squad-validation-1703", "mrqa_squad-validation-199", "mrqa_squad-validation-2564", "mrqa_squad-validation-291", "mrqa_squad-validation-2934", "mrqa_squad-validation-2985", "mrqa_squad-validation-3045", "mrqa_squad-validation-3269", "mrqa_squad-validation-3302", "mrqa_squad-validation-3416", "mrqa_squad-validation-3491", "mrqa_squad-validation-3609", "mrqa_squad-validation-3611", "mrqa_squad-validation-3667", "mrqa_squad-validation-375", "mrqa_squad-validation-4127", "mrqa_squad-validation-4257", "mrqa_squad-validation-436", "mrqa_squad-validation-4630", "mrqa_squad-validation-5185", "mrqa_squad-validation-5230", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5999", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-6958", "mrqa_squad-validation-7165", "mrqa_squad-validation-7252", "mrqa_squad-validation-7458", "mrqa_squad-validation-7554", "mrqa_squad-validation-7701", "mrqa_squad-validation-7850", "mrqa_squad-validation-786", "mrqa_squad-validation-8278", "mrqa_squad-validation-828", "mrqa_squad-validation-8316", "mrqa_squad-validation-8332", "mrqa_squad-validation-8496", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_squad-validation-9285", "mrqa_squad-validation-96", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2348", "mrqa_triviaqa-validation-2358", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2548", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-281", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3146", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-3736", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4695", "mrqa_triviaqa-validation-4731", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-5007", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5074", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5520", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-6022", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6217", "mrqa_triviaqa-validation-6461", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6658", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7012", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-7772", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-960"], "OKR": 0.78125, "KG": 0.48828125, "before_eval_results": {"predictions": ["Apple Inc.", "second", "R-rated horror remake \"Wolfman,\" starring Benicio del Toro, grossed an estimated $30.6 million for a per-theater average of $9,497.", "using recreational drugs", "annual White House Correspondents' Association dinner", "CNN affiliate WFTV.", "Rwandan", "Another high tide", "scored a hat-trick", "Apple employees", "80,", "at least seven", "OneLegacy,", "Kenneth Cole", "in the area where the single-engine Cessna 206 went down,", "psychotropic drugs", "He pleaded with a judge in March to stop Noriko Savoie from being able to travel to Japan for summer vacation.", "strangulation and asphyxiation and had two broken bones in his neck,", "they can learn in safer surroundings.", "southern city of Naples", "July in the Philippines", "abducting each other for ransoms or retribution.", "Kim", "Colorado prosecutor", "Egyptians", "Georgia Aquarium", "Saturday", "The pun-loving fashion designer", "Republicans", "dismissed all charges", "NATO fighters", "Airbus A330-200", "14 \u00b0 41 \u2032 34 '' N 17 \u00b0 26 \u2032 48 '' W", "The Crescent City", "two", "the government", "Lenin", "eight", "Kenny Anderson", "Akhenaten", "paddington bear", "france", "north west", "The Hague", "Tombstone", "dromedary", "the Grail", "Kim Smith", "T. R. M. Howard", "Orchard County", "The Bye Bye Man", "University of Oregon", "35,124", "Phil Collins", "Valhalla Highlands Historic District", "Can't Be Tamed", "koma", "Howard Hughes", "the tapir", "fred", "Meyer Lansky", "Russians", "Brave New World", "Nathaniel Hurd"], "metric_results": {"EM": 0.59375, "QA-F1": 0.671991292510594}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, true, true, true, false, false, true, false, true, false, false, false, true, false, true, true, true, false, false, true, false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.1111111111111111, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.7692307692307693, 1.0, 0.8235294117647058, 0.42857142857142855, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-411", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-1119", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-7300", "mrqa_naturalquestions-validation-4500", "mrqa_naturalquestions-validation-6896", "mrqa_triviaqa-validation-3386", "mrqa_triviaqa-validation-4041", "mrqa_triviaqa-validation-827", "mrqa_hotpotqa-validation-3080", "mrqa_searchqa-validation-11572", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-9772"], "SR": 0.59375, "CSR": 0.5333104395604396, "EFR": 0.9615384615384616, "Overall": 0.6931104052197802}, {"timecode": 91, "before_eval_results": {"predictions": ["the francs", "tulle", "Count Basie", "English", "a snake", "The Naked Gun", "the Clean Air Act", "Mercury & Venus", "Einstein on the Beach", "The Girl Who Loved Tom Gordon", "Dunkin' Donuts", "the Vikings", "royal", "Raven Symone", "the Clark bar", "the \"wonderful lamp\"", "The Devil\\'s Advocate", "The Big Red One", "Charles Dickens", "Harry S. Truman", "echinacea", "peripheral", "The Police", "Halley's comet", "Just say no", "Namibia", "Kilimanjaro", "(re)\"ut)\"", "Dresden", "Magnolia grandiflora", "the bass", "Big Ben", "Michael Edwards", "any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "the Naturalization Act of 1790", "Tim McGraw and Kenny Chesney", "seawater pearls", "May 2010", "an LCoS chip from Himax ), field - sequential color system, LED illuminated display", "the Mayor's son", "Kentucky Derby", "spain", "La Boh\u00e8me", "surtsey", "switzerland", "billiard", "multi-user dungeon", "Herman G\u00f6ring", "Robert Digges Wimberly Connor", "1,288,500", "1901", "member of the Executive Council of the General Anthroposophical Society at the Goetheanum in Dornach, Switzerland", "Dunlop Tyres", "Charles Hastings Judd", "1963", "Washington", "Wigan Athletic", "Democrats", "did not", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "this week,\"", "President Thabo Mbeki", "Secretary of State Hillary Clinton", "Lt. Holley Wimunc."], "metric_results": {"EM": 0.515625, "QA-F1": 0.652088149612537}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, false, false, false, true, true, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.1111111111111111, 0.28571428571428575, 0.5714285714285715, 0.0, 1.0, 0.7586206896551725, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.761904761904762, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4210526315789474, 1.0, 0.0, 0.33333333333333337, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1061", "mrqa_searchqa-validation-4487", "mrqa_searchqa-validation-13279", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-6474", "mrqa_searchqa-validation-10727", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-13135", "mrqa_searchqa-validation-1991", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-5935", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-1745", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-9604", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-7641", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-4204", "mrqa_hotpotqa-validation-4593", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-169", "mrqa_newsqa-validation-2548", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1392"], "SR": 0.515625, "CSR": 0.5331182065217391, "EFR": 0.9354838709677419, "Overall": 0.6878610404978962}, {"timecode": 92, "before_eval_results": {"predictions": ["brinley", "bedding", "Apollo 11", "the queen", "Les Invalides", "White House", "prince harry", "September", "him", "Sir John Barbirolli", "Antonia Margaret Caroline Fraser", "george carlin", "testicles", "dumbo", "chromium", "staple singers", "Baffin Island", "antilles", "Coronation Street", "kia", "Hard Times", "Israel", "aikido", "the Library of Congress", "four", "Operation", "member of the third generation of the acting family", "kings cross", "pyrotechnic", "I'm Sorry, I'll Read That Again", "france", "Q", "Afghanistan", "more of one good could be produced only by diverting resources from the other good, resulting in less production of it", "August 15, 1971", "December 24, 1836", "October 3, 2013", "a vertebral column ( spine )", "Rachel Kelly Tucker", "21 February", "\"boundary river\".", "Piper", "Southland", "Dennis Potter", "Ben Savage", "The Tempest", "Southern State Parkway", "Ben Daughtrey", "40", "give detainees greater latitude in selecting legal representation", "$24,000-30,000", "the BBC's central London offices", "in a motel,", "Eintracht Frankfurt", "nearly all", "March 24,", "the whistling sound", "the piano", "the bigwig", "anthrax", "table tennis", "jade", "Tunisia", "Don Quixote"], "metric_results": {"EM": 0.625, "QA-F1": 0.6697916666666666}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-7661", "mrqa_triviaqa-validation-5934", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-3385", "mrqa_triviaqa-validation-5112", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-9553", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-1710", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1255", "mrqa_searchqa-validation-9400", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-1003", "mrqa_searchqa-validation-802"], "SR": 0.625, "CSR": 0.534106182795699, "EFR": 0.9583333333333334, "Overall": 0.6926285282258065}, {"timecode": 93, "before_eval_results": {"predictions": ["Colombia", "bobbyians", "Ty Hardin", "dakota", "Thames", "Nadia Comaneci", "John Masefield", "\u00c9dith Piaf", "Mercury", "sesame street", "groundhogs", "joule", "gerry adams", "m65", "britishtennis.com", "paul anka", "keeper of the Longstone (Fame Islands) lighthouse", "romania", "Thomas de quincey", "romania", "thwaites", "cauliflower", "Ambassador Bridge", "harrogate", "seven wonders", "Sony Interactive Entertainment", "beetle", "bauxite", "Jupiter", "Jezebel", "plum", "sandstone Trail", "Session Initiation Protocol", "eusebeia", "Part 2", "2013", "Tom Waits", "Etienne de Mestre", "in the 1935 court case Schechter Poultry Corp. v. United States ( 295 U.S. 495 )", "Florida, where new arrival Roy makes two oddball friends and a bad enemy, and joins an effort to stop construction of a pancake house", "19th-century", "Blue", "Johnny D. Bright", "Justice of the Peace", "5320 km", "Hordaland", "Barnoldswick", "Flushed Away", "women.", "18", "American Civil Liberties Union", "Michelle Obama", "Egypt", "People Against Switching Sides", "$8.8 million", "Manmohan Singh's", "the Etruscans", "Judi Dench", "Westminster Abbey", "a final contest", "the wolverine", "Richard Nixon", "Mexico", "Nebraska"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6848169191919191}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0909090909090909, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4808", "mrqa_triviaqa-validation-3773", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-432", "mrqa_triviaqa-validation-5097", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-1718", "mrqa_triviaqa-validation-4200", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2624", "mrqa_naturalquestions-validation-1312", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-1211", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-1319", "mrqa_searchqa-validation-5374"], "SR": 0.640625, "CSR": 0.5352393617021276, "EFR": 1.0, "Overall": 0.7011884973404255}, {"timecode": 94, "before_eval_results": {"predictions": ["gulf of Mexico", "personal computer", "korky the cat", "tintoretto", "Vitcos", "hawkley", "blofeld", "light", "152", "Sir Hardy Amies", "copper line", "hector bERLIOZ", "edward", "jony", "Jacopo tintoretto", "9", "Stephen Potter", "0", "mony treaty", "Femoral", "dragonheart", "Pete Sampras", "barbarian", "cosmos", "little women", "king's college", "Richard Curtis", "south Carolina", "croquet", "John Donne", "espresso", "Boston pops Orchestra", "for the red - bed country of its watershed", "September 4, 2000", "the northern bluegrass band the Greenbriar Boys", "CBS Television City", "Australia", "Abigail Hawk", "Alice", "Asia", "1937", "The conversation", "five", "26,000", "London, United Kingdom", "Democratic", "Michelle Anne Sinclair", "Matt Groening", "citizenship", "Cash for Clunkers", "\"A Lion Among Men,\"", "Lee Myung-Bak", "prisoners at the South Dakota State Penitentiary", "1,500 Marines", "Pakistani officials,", "Steven Green", "Chevy Chase", "the Statue of Liberty", "House", "fog", "Prado Museum", "William M. Tweed", "a repo man", "Graceland"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6228365384615384}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-5072", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-164", "mrqa_triviaqa-validation-947", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-4910", "mrqa_triviaqa-validation-2269", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-2784", "mrqa_triviaqa-validation-4359", "mrqa_triviaqa-validation-6498", "mrqa_triviaqa-validation-1541", "mrqa_triviaqa-validation-3442", "mrqa_triviaqa-validation-1389", "mrqa_naturalquestions-validation-1988", "mrqa_naturalquestions-validation-4007", "mrqa_hotpotqa-validation-987", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-5727", "mrqa_hotpotqa-validation-963", "mrqa_newsqa-validation-3681", "mrqa_searchqa-validation-10824", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-14018"], "SR": 0.5625, "CSR": 0.5355263157894736, "EFR": 0.9285714285714286, "Overall": 0.6869601738721804}, {"timecode": 95, "before_eval_results": {"predictions": ["Hollywood", "18th", "last week,", "\"Rin Tin Tin: The Life and the Legend\"", "Pakistani official Tuesday bristled at the accusation, and we don't blame our failures on others,\"", "the two \"The Ethiopian army's answer to the rebels has been to viciously attack civilians in the Ogaden,\"", "2,000 euros ($2,963)", "at Afghanistan's restive provinces to support Marines and soldiers fighting a dug-in Taliban force.", "prostate cancer,", "Karthik Rajaram,", "to kill then-Sen. Obama on October 23, 2008, shortly before the presidential election.", "Robert Barnett,", "finance", "Samoa", "Jacob Zuma,", "severe famine", "WTA Tour titles", "off the coast of Dubai with a celebrity-studded gala and a three-day party.", "an \"unnamed international terror group\"", "a U.S. soldier", "southern Bangladesh,", "38,", "U.S. State Department and British Foreign Office", "Jaipur", "$40 billion during the operations phase.", "$55.7 million", "11:30 p.m. Tuesday,", "the student", "9 a.m.", "40", "Afghanistan and India", "Frank Ricci,", "1976", "Lisa Stelly", "the British rock band Procol Harum", "a remittance", "Mahatma Gandhi", "twice", "at Merrimen and his gang", "former president Mahinda Rajapaksa", "adoine de caunes", "Carson City", "mazovia", "gunners", "copper", "poland", "technetium", "acetone", "Histoires ou contes du temps pass\u00e9", "Lord Chancellor of England", "Mathew Sacks", "Easy", "1932", "Centennial Olympic Stadium", "February 14, 1859", "Protestant Christian", "the Dust", "black", "gold", "a Contender", "tuna", "a lampoon", "Tommy Franks", "Liechtenstein"], "metric_results": {"EM": 0.625, "QA-F1": 0.6857514880952381}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1217", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-880", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-10510", "mrqa_naturalquestions-validation-1368", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-4806", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-216", "mrqa_triviaqa-validation-2079", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-4007", "mrqa_searchqa-validation-14056", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-12584"], "SR": 0.625, "CSR": 0.5364583333333333, "EFR": 1.0, "Overall": 0.7014322916666667}, {"timecode": 96, "before_eval_results": {"predictions": ["the surge,", "immediate release", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "her doctors and a family lawyer", "genocide", "Rodong Sinmun", "satellites", "near the Somali coast", "Her husband and attorney, James Whitehouse,", "Hundreds", "Eleven people", "near his home in Peshawar", "12-1", "Aung San Suu Kyi", "July", "\"wider relationship\"", "bartering", "civilians,", "41,280", "The Tinkler.", "suicides", "Charlotte Gainsbourg and Willem Dafoe", "hopes the journalists and the flight crew will be freed,", "1940's Japan.", "\"a striking blow to due process and the rule of law.\"", "September 23,", "first five Potter films have been held in a trust fund", "five female pastors", "out of necessity,", "Vice's broadband television network.", "AbdulMutallab", "71 percent of Americans consider China an economic threat to the United States,", "American rock band R.E.M.", "Achal Kumar Jyoti", "Kenny Anderson", "7000301604928199000", "September 2, 1945", "President", "Washington metropolitan area", "Divyanka Tripathi and Karan Patel", "tiffany and co", "heath Ledger", "Elizabeth Taylor", "jesse", "dodo", "saitaima", "edusa", "pomona", "Nicolas Vanier", "two", "Karen O", "North African Arab", "Rage Against the Machine", "MGM Resorts International", "4,530", "Krypto Report", "Hawaii", "Washington", "Danube", "Fiddler", "the Intrigue", "Brisbane", "Grand Central Station", "shrewd"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7312770562770563}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true], "QA-F1": [0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 0.0, 1.0, 0.0, 0.5714285714285715, 0.5, 0.36363636363636365, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-3998", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-769", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-4063", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-2182", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-7343", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-3428", "mrqa_searchqa-validation-4231", "mrqa_searchqa-validation-8774", "mrqa_searchqa-validation-16255"], "SR": 0.640625, "CSR": 0.5375322164948453, "EFR": 1.0, "Overall": 0.7016470682989691}, {"timecode": 97, "before_eval_results": {"predictions": ["Kindle Fire", "President Obama and Britain's Prince Charles", "students often know ahead of time when and where violence will flare up on campus.", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "women.", "the FDA is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.", "the iconic Hollywood headquarters of Capitol Records,", "Trevor Rees,", "next year", "her landlord defaulted on the mortgage and the house fell into foreclosure.", "Filippo Inzaghi", "there is not a process to ensure that", "Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,", "\"a potential hazard may occur due to re-entry of satellite debris into the earth's atmosphere.\"", "Illness", "apartment near Fort Bragg in North Carolina.", "a laundry facility", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "18th", "the government.", "15,000", "in Japan: the IV cafe.", "Top Gun", "the son decided to leave al Qaeda.", "at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.\"", "he \"persisted in meddling in Zimbabwe's electoral process,\"", "a delegation of American Muslim and Christian leaders", "Barack Obama", "autonomy", "Jeddah, Saudi Arabia,", "1831", "a \"procedure on her heart,\"", "September 1980", "in 1905", "Amerigo Vespucci", "the forex market", "restricted a single ethnic group by specifically limiting further Chinese immigration", "a `` house edge ''", "in the 2011 World Series", "Rory McIlroy", "Lago de Nicaragua", "(Macbeth) Soliloquy", "war and peace", "perse", "y Yahoo!", "1929", "rounders", "ukraine", "University of Southern California", "Lucille Ball", "14", "Cheshire County", "Atlantic Ocean", "tomato", "an odd-eyed cat", "Magnate", "a robin", "Qwerty", "Austria", "garlic", "Argentina", "Repent ye", "C Daryl Chessman", "Paul"], "metric_results": {"EM": 0.5, "QA-F1": 0.6622266225038402}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 0.967741935483871, 0.8, 1.0, 0.9714285714285714, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.9600000000000001, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.13333333333333333, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-2358", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-3052", "mrqa_newsqa-validation-131", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-3943", "mrqa_newsqa-validation-2546", "mrqa_naturalquestions-validation-3122", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5000", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-1415", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-1144", "mrqa_triviaqa-validation-6942", "mrqa_hotpotqa-validation-4152", "mrqa_hotpotqa-validation-1273", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-2609", "mrqa_searchqa-validation-2778", "mrqa_searchqa-validation-15672"], "SR": 0.5, "CSR": 0.5371492346938775, "EFR": 1.0, "Overall": 0.7015704719387755}, {"timecode": 98, "before_eval_results": {"predictions": ["downslope", "James Blunt", "katherine Parr", "air", "Lancashire", "red", "Dame Nellie Melba", "hannibal heyes and kid curry", "bubba", "lighthouse keeper", "al yganeh", "Violin", "Parliament-Funkadelic", "ediopia", "marriage", "Adrian Cronauer", "Thomas Jefferson", "macau", "Eric Coates", "Dublin", "Welcome Stranger", "brown", "Violin", "nippon Sangyo", "north raleigh", "Time Machine", "jack Nicholson", "romania", "new hampshire", "anniepowell", "arrowhead", "gun", "harm - joy", "Kelly Reno", "9 September 2018", "Ed Sheeran", "Lexie", "a local organization of businesses whose goal is to further the interests of businesses", "Andrea Brooks", "The Impalas", "Cleveland Browns", "edith cavell", "Rick and Morty", "Steve Carell", "Jena Malone", "Red and Assiniboine Rivers", "Illinois", "Field of Dreams", "Steven Green", "Iraq", "the surgical anesthetic propofol", "70,000 or so", "Port-au-Prince harbor where his fleet of trucks used to pick up cargo.", "$250,000 for Rivers' charity: God's Love We Deliver.", "an antihistamine and an epinephrine auto-injector for emergencies,", "John and Elizabeth Calvert", "War of 1812", "Valium", "Wilbur and Orville Wright", "pie", "Battle of Waterloo", "Pan", "Qantas", "Anna and the King of Siam"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6346750452898551}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, true, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3, 0.2608695652173913, 1.0, 0.8, 0.0, 1.0, 1.0, 0.3333333333333333, 0.5, 0.5, 1.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_triviaqa-validation-6214", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-2990", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-7748", "mrqa_triviaqa-validation-3614", "mrqa_triviaqa-validation-6041", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-1364", "mrqa_triviaqa-validation-6862", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-6807", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-5509", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-1700", "mrqa_hotpotqa-validation-1434", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-4208", "mrqa_searchqa-validation-7820", "mrqa_searchqa-validation-15766", "mrqa_searchqa-validation-14171", "mrqa_searchqa-validation-3710"], "SR": 0.5625, "CSR": 0.537405303030303, "EFR": 0.9642857142857143, "Overall": 0.6944788284632034}, {"timecode": 99, "UKR": 0.69921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1520", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-1779", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1885", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-190", "mrqa_hotpotqa-validation-1911", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2453", "mrqa_hotpotqa-validation-2520", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3209", "mrqa_hotpotqa-validation-3351", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-379", "mrqa_hotpotqa-validation-38", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4593", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-58", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-824", "mrqa_hotpotqa-validation-951", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1959", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-3612", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3910", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-5235", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-5820", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-6854", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7499", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1284", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-1476", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2210", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-234", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2539", "mrqa_newsqa-validation-2610", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-3921", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-3995", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-575", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10192", "mrqa_searchqa-validation-10280", "mrqa_searchqa-validation-10727", "mrqa_searchqa-validation-1075", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11144", "mrqa_searchqa-validation-11237", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11616", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-12726", "mrqa_searchqa-validation-13135", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13632", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14056", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14807", "mrqa_searchqa-validation-15311", "mrqa_searchqa-validation-15405", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16077", "mrqa_searchqa-validation-16403", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16705", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2610", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-2816", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-3147", "mrqa_searchqa-validation-317", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-3382", "mrqa_searchqa-validation-4055", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4277", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-6239", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6944", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-716", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8095", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-9721", "mrqa_searchqa-validation-9772", "mrqa_squad-validation-10024", "mrqa_squad-validation-10340", "mrqa_squad-validation-1052", "mrqa_squad-validation-1316", "mrqa_squad-validation-1703", "mrqa_squad-validation-199", "mrqa_squad-validation-2564", "mrqa_squad-validation-291", "mrqa_squad-validation-2934", "mrqa_squad-validation-2985", "mrqa_squad-validation-3045", "mrqa_squad-validation-3269", "mrqa_squad-validation-3302", "mrqa_squad-validation-3416", "mrqa_squad-validation-3491", "mrqa_squad-validation-3609", "mrqa_squad-validation-3611", "mrqa_squad-validation-3667", "mrqa_squad-validation-375", "mrqa_squad-validation-4127", "mrqa_squad-validation-4257", "mrqa_squad-validation-436", "mrqa_squad-validation-4630", "mrqa_squad-validation-5185", "mrqa_squad-validation-5230", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5999", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-6958", "mrqa_squad-validation-7165", "mrqa_squad-validation-7252", "mrqa_squad-validation-7458", "mrqa_squad-validation-7701", "mrqa_squad-validation-7850", "mrqa_squad-validation-786", "mrqa_squad-validation-8278", "mrqa_squad-validation-828", "mrqa_squad-validation-8316", "mrqa_squad-validation-8332", "mrqa_squad-validation-8496", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1144", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2269", "mrqa_triviaqa-validation-2348", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2548", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-281", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-3442", "mrqa_triviaqa-validation-3736", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4041", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4179", "mrqa_triviaqa-validation-4204", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4695", "mrqa_triviaqa-validation-4731", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5072", "mrqa_triviaqa-validation-5074", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5520", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-6022", "mrqa_triviaqa-validation-6041", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6217", "mrqa_triviaqa-validation-6359", "mrqa_triviaqa-validation-6461", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6942", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-960"], "OKR": 0.841796875, "KG": 0.4984375, "before_eval_results": {"predictions": ["caracas", "Richard Marx", "cat", "lodges", "indiana jones", "moon river", "Malcolm Turnbull", "fish", "charlie cairoli", "Lone Ranger", "Addis Ababa", "jupiter", "Donald Sutherland", "alligator", "\"Holiday Inn\"", "equatorial Guinea", "mEXICO", "south", "piccadilly", "dravidian", "Paul Gauguin", "mary caine", "mammals", "martin dryden", "QPR", "south", "Reel Life:", "yellow", "cupressaceae", "cycling", "24", "Helen Clark", "India", "more than a million members ( including 195,000 youth members )", "in 2010", "the foreign exchange market ( FX )", "July 23, 2016", "drizzle", "31 January 1934", "Jack Wilton", "Clarence Nash", "English", "September 21, 2014", "\"Stu\" Roosa", "Kew Gardens", "Jack Posobiec", "\"Si Da Ming Bu\"", "James William McCutcheon", "1975", "Hamas,", "Nazi war crimes suspect", "Dean Martin, Katharine Hepburn and Spencer Tracy", "MEND", "Swiss art heist", "South Africa", "Oxbow,", "BoomerBaby", "Auguste Rodin", "Red Button", "the College of William", "Walt Whitman", "a good kiss", "a kiwi", "a quark"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5986979166666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, false, false, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, false, true, false, false, false, true, false, false, false, true, true, true, false, true, false, true, true, false, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1946", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-5221", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-1045", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-4056", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-4464", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-5564", "mrqa_hotpotqa-validation-1900", "mrqa_hotpotqa-validation-4911", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-4722", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-1083", "mrqa_newsqa-validation-4109", "mrqa_newsqa-validation-4030", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-3358", "mrqa_searchqa-validation-14625", "mrqa_searchqa-validation-659", "mrqa_searchqa-validation-12683"], "SR": 0.53125, "CSR": 0.53734375, "EFR": 0.9666666666666667, "Overall": 0.7086927083333333}]}