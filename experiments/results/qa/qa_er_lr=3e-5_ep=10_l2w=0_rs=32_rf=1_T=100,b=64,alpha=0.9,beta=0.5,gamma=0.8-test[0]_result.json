{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[0]', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[0]/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=1, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 7930, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["67.9", "Mike Carey", "rapidly raising population and traffic in cities along SR 99, as well as the desirability of Federal funding", "their greatest common divisor is one", "the Official Report", "immunoinformatics", "lupus erythematosus", "Kublai Khan", "New Testament", "1926", "other ctenophores", "60%", "he was illiterate in Czech", "architect or engineer", "British", "Gateshead Council", "Book of Genesis", "Shing-Tung Yau", "complexity classes", "Mexico", "cabinet", "after its 1977 merger with Radcliffe College", "The Master", "chastity", "Mark Woods", "one another", "100% oxygen", "Steam engines", "the wedding banquet", "the Ohio Company of Virginia", "CBS and NBC", "aircraft manufacturing", "a school or other place of formal education", "Times Square Studios", "Normans and Norman", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "decreases", "Duke Richard II of Normandy, and King Ethelred II of England", "Royal Shakespeare Company", "books and articles", "between 1835 and 1842", "Edmonton, Canada", "rises in sea levels", "it is neither zero nor a unit", "University of Chicago College Bowl Team", "algorithms have been written that solve the problem in reasonable times in most cases", "13.34%", "FMCG manufacturing, metal processing, steel and electronic manufacturing and food processing", "49\u201315", "The Mongols' extensive West Asian and European contacts", "east-west", "the kip", "Croatia", "railway locomotives", "the law is no longer to be taught to Christians but belonged only to city hall", "Josh Norman", "1964 and 1968", "expelled Jews", "Elton Rule", "gravel", "11:28", "Super Bowl XLVII", "a fee per unit of information transmitted", "seven"], "metric_results": {"EM": 0.875, "QA-F1": 0.9101325145442792}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3846153846153846, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.5, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1187", "mrqa_squad-validation-2853", "mrqa_squad-validation-6986", "mrqa_squad-validation-2704", "mrqa_squad-validation-1891", "mrqa_squad-validation-1161", "mrqa_squad-validation-1089", "mrqa_squad-validation-2473"], "SR": 0.875, "CSR": 0.875, "EFR": 1.0, "Overall": 0.9375}, {"timecode": 1, "before_eval_results": {"predictions": ["\"Fourth\"", "The input string", "European Parliament", "the Meuse", "overseas colonies", "Kings Canyon Avenue and Clovis Avenue", "entertainment", "Astra 2A", "poison", "atoon", "Orange", "the BBC", "Dolby Digital", "shocked", "Executive Vice President of Football Operations", "Thomas Edison", "British colonists would not be safe as long as the French were present.", "least prejudiced", "inferior", "Johann Tetzel", "planktonic", "main porch", "the courts of member states", "The Shah's army was split by diverse internecine feuds and by the Shah's decision to divide his army into small groups concentrated in various cities.", "1st century BC", "Iberia", "Silas B. Cobb", "Aristotle", "The Swahili", "the sheepshanks Gallery", "1968", "heavy/highway, heavy civil or heavy engineering", "type III secretion system", "Commission v Italy", "Gary Kubiak", "The European Court of Justice", "silicates", "1933", "Endosymbiotic gene transfer", "proplastids", "Jurassic Period", "Conrad of Montferrat", "sacramental union", "reciprocating Diesel", "the Arabs and much of the rest of the Third World", "mineral deposits", "Grand Canal d'Alsace", "Vince Lombardi Trophy", "\u00dcberseering BV v Nordic Construction GmbH", "first 15 years", "Neoclassical economics", "Variable lymphocyte receptors", "exploration is still continuing to determine if there are more reserves.", "Elder", "internal migration and urbanisation", "high risk preparations and some other compounding functions", "water level", "two", "James `` Jamie '' Dornan ( born 1 May 1982 ) is an Irish actor, model, and musician. He played Axel von Fersen in Sofia Coppola's Marie Antoinette ( 2006 )", "A driver's license is an official document permitting a specific individual to operate one or more types of motorized vehicles", "radioisotope heater unit provides heat from radioactive decay of a material and can potentially produce heat for decades", "Mickey Mantle", "A lymphocytes is one of the subtypes of white blood cell in a vertebrate's immune system", "concentration camps"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7201914098972922}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.5, 0.2857142857142857, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7689", "mrqa_squad-validation-1767", "mrqa_squad-validation-3922", "mrqa_squad-validation-6029", "mrqa_squad-validation-2672", "mrqa_squad-validation-6211", "mrqa_squad-validation-1902", "mrqa_squad-validation-375", "mrqa_squad-validation-10186", "mrqa_squad-validation-4636", "mrqa_squad-validation-6163", "mrqa_squad-validation-8927", "mrqa_squad-validation-3370", "mrqa_squad-validation-7635", "mrqa_squad-validation-8273", "mrqa_squad-validation-2315", "mrqa_naturalquestions-validation-5378", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-9342"], "SR": 0.6875, "CSR": 0.78125, "retrieved_ids": ["mrqa_squad-train-70513", "mrqa_squad-train-16760", "mrqa_squad-train-21251", "mrqa_squad-train-4861", "mrqa_squad-train-57912", "mrqa_squad-train-17251", "mrqa_squad-train-78244", "mrqa_squad-train-17003", "mrqa_squad-train-37013", "mrqa_squad-train-27074", "mrqa_squad-train-10704", "mrqa_squad-train-50610", "mrqa_squad-train-40332", "mrqa_squad-train-75771", "mrqa_squad-train-37318", "mrqa_squad-train-43178", "mrqa_squad-validation-2704", "mrqa_squad-validation-2853", "mrqa_squad-validation-1187", "mrqa_squad-validation-1891", "mrqa_squad-validation-1161", "mrqa_squad-validation-6986", "mrqa_squad-validation-2473", "mrqa_squad-validation-1089"], "EFR": 1.0, "Overall": 0.890625}, {"timecode": 2, "before_eval_results": {"predictions": ["six quadrangles", "1883", "1870 to 1939", "2010", "unit-dose, or a single doses of medicine", "2003", "Budapest Telephone Exchange", "induction motor", "markets", "to avoid being targeted by the boycott", "socialist realism", "Ten", "education, sanitation, and traffic control", "4000", "St. Bartholomew's Day massacre", "early twentieth century homes", "Thesis 86", "five or more seats", "in a glass case", "1,230 kilometres (764 miles)", "force-free magnetic fields", "Tem\u00fcjin-\u00fcge", "The Three Doctors", "San Francisco Bay Area's Levi's Stadium", "prime", "St. Lawrence and Mississippi watersheds", "2010", "cholera", "168,637", "four", "faith", "25", "river Deabolis", "ten", "to spearhead the regeneration of the North-East", "late 1920s", "2007", "1936", "1968", "the European Parliament and the Council of the European Union", "typhus, smallpox and respiratory infections", "California", "Spanish", "muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins", "the sheepshanks Gallery", "Four thousand", "Prime ideals", "Stairs", "lack of understanding of the legal ramifications, or due to a fear of seeming rude", "MHC class I molecules", "Great Fire of London", "in his lab and elsewhere", "Rudy Clark", "Art Carney", "Ren\u00e9 Verdon 1961 -- 1965   Henry Haller 1966 -- 1987   Jon Hill 1987", "honey bees may be the state's most valuable export", "July 4, 1776", "May 5, 1904", "The Lykan Hypersport is a Lebanese limited production supercar built by W Motors, a United Arab Emirates based company", "E \u00d7 12", "digitization of social systems", "Trace Adkins", "Iggy Pop invented punk rock", "Around 204,000 people have fled their homes in the Somali capital of Mogadishu as a result of a militant offensive against government forces"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7795138888888888}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.8333333333333334, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.2222222222222222, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9855", "mrqa_squad-validation-6526", "mrqa_squad-validation-1277", "mrqa_squad-validation-7246", "mrqa_squad-validation-5751", "mrqa_squad-validation-9302", "mrqa_squad-validation-133", "mrqa_squad-validation-9012", "mrqa_squad-validation-4546", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-1173", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-3164"], "SR": 0.734375, "CSR": 0.765625, "retrieved_ids": ["mrqa_squad-train-16170", "mrqa_squad-train-1085", "mrqa_squad-train-58005", "mrqa_squad-train-43842", "mrqa_squad-train-19408", "mrqa_squad-train-24155", "mrqa_squad-train-57023", "mrqa_squad-train-821", "mrqa_squad-train-41932", "mrqa_squad-train-58466", "mrqa_squad-train-71732", "mrqa_squad-train-58088", "mrqa_squad-train-47828", "mrqa_squad-train-25677", "mrqa_squad-train-77723", "mrqa_squad-train-71988", "mrqa_squad-validation-1161", "mrqa_squad-validation-2853", "mrqa_squad-validation-2473", "mrqa_naturalquestions-validation-158", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-3828", "mrqa_squad-validation-3922", "mrqa_squad-validation-6986", "mrqa_squad-validation-2672", "mrqa_squad-validation-8273", "mrqa_squad-validation-1891", "mrqa_naturalquestions-validation-5378", "mrqa_squad-validation-4636", "mrqa_naturalquestions-validation-9342", "mrqa_squad-validation-6163", "mrqa_squad-validation-1187"], "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 3, "before_eval_results": {"predictions": ["the Hamiltonian path problem", "Thomas Edison", "systematic economic inequalities", "July 31, 1995", "vitamin D", "Thomas Edison", "1987", "terra nullius", "Leukocytes", "research, exhibitions and other shows", "Erg\u00e4nzungsschulen", "along the coast", "a \"principal hostile country\"", "17", "Book of Discipline", "67.9", "Turkey", "over 100%", "Saffir-Simpson Scale", "New Orleans", "3,000", "Earth", "Bruno Mars", "an archipelago-like estuary", "Antigone", "Soviet", "Dodge D-50", "18", "the ozone generated in contact with the skin", "polynomial-time", "the WMO", "several years", "Danny Trevathan", "Albert of Mainz", "patient care rounds drug product selection", "Thomas Coke", "Colony of Victoria Act 1855", "Anglo-Saxon populations", "John Debney", "Geordie", "Ancient Greeks", "Budget cuts", "Systemic acquired resistance", "one darkened lens", "Legislative Assembly", "the desire to prevent things that are indisputably bad", "Any member", "Tyrion", "Manchuria", "Kenneth Cook ( 2018 )   Samuel F. Herd", "to capitalize on her publicity", "1937", "Wah - Wah", "James W. Marshall", "1979", "January 12, 2017", "a loop", "Kyrie Irving", "Charles Carson", "Morgan Freeman", "James Duchovny", "The Ward House", "is a desktop microcomputer launched in 1977 and sold by Tandy Corporation through their Radio Shack stores.", "fibula l pin"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7916782111395282}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, false], "QA-F1": [0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.6666666666666666, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.11764705882352941, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-1760", "mrqa_squad-validation-9764", "mrqa_squad-validation-3770", "mrqa_squad-validation-3096", "mrqa_squad-validation-100", "mrqa_squad-validation-1436", "mrqa_squad-validation-1764", "mrqa_squad-validation-7713", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-849", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-6338"], "SR": 0.734375, "CSR": 0.7578125, "retrieved_ids": ["mrqa_squad-train-7545", "mrqa_squad-train-65571", "mrqa_squad-train-38283", "mrqa_squad-train-64010", "mrqa_squad-train-25892", "mrqa_squad-train-13538", "mrqa_squad-train-74638", "mrqa_squad-train-4319", "mrqa_squad-train-72574", "mrqa_squad-train-58073", "mrqa_squad-train-61137", "mrqa_squad-train-77504", "mrqa_squad-train-55589", "mrqa_squad-train-72092", "mrqa_squad-train-4934", "mrqa_squad-train-23830", "mrqa_squad-validation-8273", "mrqa_naturalquestions-validation-1173", "mrqa_squad-validation-6163", "mrqa_squad-validation-4546", "mrqa_squad-validation-1277", "mrqa_squad-validation-3922", "mrqa_squad-validation-1902", "mrqa_squad-validation-6029", "mrqa_squad-validation-6986", "mrqa_squad-validation-1161", "mrqa_squad-validation-1187", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-10273", "mrqa_squad-validation-2315", "mrqa_squad-validation-9855", "mrqa_squad-validation-8927"], "EFR": 1.0, "Overall": 0.87890625}, {"timecode": 4, "before_eval_results": {"predictions": ["immune surveillance", "Arizona Cardinals", "10,000", "3 January 1521", "The Scotland Act 1998", "AC", "Chuck Howley", "1\u20133 \u03bcm thick", "Air Force", "Meiji Restoration", "peroxides, chlorates, nitrates, perchlorates, and dichromates", "842", "June 4, 2014", "recast as decision problems", "Gateshead Council", "more than 70", "Gamal Abdul Nasser", "the Bible", "with a rolling circle mechanism", "mantle", "Hangzhou", "etic branches", "12 December 1963", "UNESCO World Heritage Site", "January 27, 1967", "cortisol and catecholamines", "constituency seats", "drummes", "Tower Theatre", "Min system", "coach", "North American Aviation", "banded iron", "Arizona Cardinals", "Super Bowl 50", "major business districts", "HO", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d", "two", "destruction of the forest", "the Commission", "the net mechanical energy", "Mark Humphrey", "Saul", "Stopping by Woods on a Snowy Evening", "southwestern Colorado and northwestern New Mexico", "Paul to the Philippians", "Jack Worthington", "1997", "accomplish the objectives of the organization", "3 October 1990", "The federal government received only those powers which the colonies had recognized as belonging to king and parliament", "Jerry Leiber", "Del and Rodney are invited to a fancy dress party", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia", "displacement", "June 12, 2018", "Tim McGraw and Kenny Chesney", "N 80 \u00b0 00 \u2032 W \ufeff", "James Martin Lafferty", "La traviata", "Cadillac Stingray", "Marco Polo", "the theory that at birth the (human) mind is a \"blank slate\" without rules for"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7092715992647058}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.5, 1.0, 1.0, 0.2, 0.0, 0.5714285714285715, 0.16666666666666666, 0.6875000000000001, 1.0, 1.0, 0.5714285714285715, 0.7058823529411764, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-635", "mrqa_squad-validation-8750", "mrqa_squad-validation-6327", "mrqa_squad-validation-3811", "mrqa_squad-validation-1649", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-73", "mrqa_squad-validation-186", "mrqa_squad-validation-2632", "mrqa_squad-validation-6614", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-1748", "mrqa_naturalquestions-validation-5451", "mrqa_hotpotqa-validation-5838", "mrqa_newsqa-validation-1003", "mrqa_searchqa-validation-5338"], "SR": 0.578125, "CSR": 0.721875, "retrieved_ids": ["mrqa_squad-train-46308", "mrqa_squad-train-35099", "mrqa_squad-train-24149", "mrqa_squad-train-35658", "mrqa_squad-train-36675", "mrqa_squad-train-5489", "mrqa_squad-train-27538", "mrqa_squad-train-42082", "mrqa_squad-train-55650", "mrqa_squad-train-15842", "mrqa_squad-train-72770", "mrqa_squad-train-81139", "mrqa_squad-train-59247", "mrqa_squad-train-85736", "mrqa_squad-train-60224", "mrqa_squad-train-36383", "mrqa_squad-validation-6526", "mrqa_naturalquestions-validation-158", "mrqa_squad-validation-3096", "mrqa_naturalquestions-validation-5378", "mrqa_naturalquestions-validation-4586", "mrqa_squad-validation-1161", "mrqa_naturalquestions-validation-8657", "mrqa_squad-validation-1764", "mrqa_squad-validation-8273", "mrqa_squad-validation-2672", "mrqa_squad-validation-6986", "mrqa_squad-validation-1089", "mrqa_searchqa-validation-8172", "mrqa_squad-validation-6163", "mrqa_squad-validation-7246", "mrqa_squad-validation-7689"], "EFR": 0.9259259259259259, "Overall": 0.8239004629629629}, {"timecode": 5, "before_eval_results": {"predictions": ["orientalism", "Director", "east", "NL and NC", "1534", "April 1887", "\u00d6gedei Khan", "beaches", "on the West Side", "John Pap, Lord of Pelham Manor", "Dai Setsen", "821,784", "salvaging a country usually seen as one of the most stable and prosperous in Africa", "Air", "ancestors", "divergence problem", "Islamist", "satellite television", "capturing Fort Beaus\u00e9jour in June 1755, cutting the French fortress at Louisbourg off from land-based reinforcements", "accepting Jesus as your personal Lord and Savior", "131", "investigated the other alleged mistakes, which were \"generally unfounded and also marginal to the assessment", "Maria Sk\u0142odowska-Curie Institute of Oncology", "winter of 1973\u201374", "P", "BAFTA", "Off-Off Campus", "the General Conference", "Super Bowl XLIV", "Sam Chisholm", "the mother", "LDS Church", "the most cost efficient bidder", "non-governmental", "non-Catholics", "southern and central parts of France", "Aveo", "Luther's education", "1998", "administrative supervision over all courts and the personnel thereof ''", "XXXX", "Thunder Road", "Sylvester Stallone", "Los Angeles", "Mohammad Reza Pahlavi", "Thomas Mundy Peterson", "Hebrew", "butter and guns", "season seven", "TLC", "Mexico", "1987", "Asuka", "4 September 1936", "Amy Wong", "2002", "Pasek and Paul", "U.S. service members who have died without their remains being identified", "Western cultures", "Ovid", "William Bradford", "Bryant Purvis", "Tennis scoring system", "40,400"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7130933302808302}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.761904761904762, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2586", "mrqa_squad-validation-3319", "mrqa_squad-validation-8386", "mrqa_squad-validation-10273", "mrqa_squad-validation-9989", "mrqa_squad-validation-9416", "mrqa_squad-validation-8579", "mrqa_squad-validation-1759", "mrqa_squad-validation-7819", "mrqa_squad-validation-6752", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-10093", "mrqa_triviaqa-validation-471", "mrqa_searchqa-validation-13515", "mrqa_hotpotqa-validation-1989"], "SR": 0.671875, "CSR": 0.7135416666666667, "retrieved_ids": ["mrqa_squad-train-46228", "mrqa_squad-train-2582", "mrqa_squad-train-13392", "mrqa_squad-train-63290", "mrqa_squad-train-17178", "mrqa_squad-train-85346", "mrqa_squad-train-8857", "mrqa_squad-train-32078", "mrqa_squad-train-80959", "mrqa_squad-train-50498", "mrqa_squad-train-10636", "mrqa_squad-train-20570", "mrqa_squad-train-54191", "mrqa_squad-train-64829", "mrqa_squad-train-31288", "mrqa_squad-train-34727", "mrqa_squad-validation-1767", "mrqa_squad-validation-3922", "mrqa_naturalquestions-validation-6483", "mrqa_newsqa-validation-1003", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-10546", "mrqa_squad-validation-2315", "mrqa_hotpotqa-validation-5838", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-5378", "mrqa_naturalquestions-validation-10273", "mrqa_squad-validation-7246", "mrqa_naturalquestions-validation-8186", "mrqa_searchqa-validation-5697", "mrqa_squad-validation-2473", "mrqa_newsqa-validation-3164"], "EFR": 1.0, "Overall": 0.8567708333333334}, {"timecode": 6, "before_eval_results": {"predictions": ["monophyletic", "computational power", "1971", "rapid combustion", "Stanford", "pressure swing adsorption", "red algal chloroplast lineage", "capturing three traders and killing 14 people of the Miami nation, including Old Briton", "Citadel Media", "lands west of the Appalachian Mountains", "two", "ten", "clerical", "average workers", "Catholic", "late 1886", "potentially dangerous", "Golovin", "the characteristics of the contact between the surface and the object", "within the chloroplast's stroma", "the last 7000 years", "3.6%", "God", "C. J. Anderson", "Robert R. Gilruth", "1469", "arid and semi-arid areas with near-desert landscapes", "estimated $200,000", "slightly more than atmospheric pressure", "temperature and sea level change with observations", "6800", "after their second year", "consumes ATP and oxygen, releases CO2, and produces no sugar", "Giuliano da Sangallo", "two", "Chuck Connors", "Eva Cassidy", "Lincoln Logs", "tin star", "muscle tissue", "large African antelope", "Jerusalem", "Hamlet", "the Mayflower", "President Abraham Lincoln", "Martina Hingis", "tuna", "TESLAR Satellite", "a kind of superman", "Gentlemen Prefer Blondes", "Decoupage", "Cesium", "Bouvier", "Paris", "Titanic", "temperature", "New Zealand", "george Stevens", "In April 2010, City Council adopted an ordinance that bans smoking in all areas frequented by the public, with limited exceptions, including unenclosed areas at certain drinking establishments", "Mark Neveldine and Brian Taylor", "four", "Crescent Pizza Pockets", "Edward Kenway", "By 1770 BC"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7374412593984963}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.25, 0.2105263157894737, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3676", "mrqa_squad-validation-2754", "mrqa_squad-validation-10316", "mrqa_squad-validation-8399", "mrqa_squad-validation-3479", "mrqa_squad-validation-8529", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-1800", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-1643", "mrqa_naturalquestions-validation-8909", "mrqa_searchqa-validation-9828", "mrqa_naturalquestions-validation-3938"], "SR": 0.703125, "CSR": 0.7120535714285714, "retrieved_ids": ["mrqa_squad-train-66007", "mrqa_squad-train-82334", "mrqa_squad-train-24237", "mrqa_squad-train-7221", "mrqa_squad-train-73854", "mrqa_squad-train-75122", "mrqa_squad-train-41708", "mrqa_squad-train-74236", "mrqa_squad-train-20666", "mrqa_squad-train-32186", "mrqa_squad-train-36258", "mrqa_squad-train-83056", "mrqa_squad-train-78486", "mrqa_squad-train-50415", "mrqa_squad-train-66239", "mrqa_squad-train-5254", "mrqa_squad-validation-7635", "mrqa_squad-validation-1161", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-4586", "mrqa_squad-validation-3922", "mrqa_searchqa-validation-6338", "mrqa_newsqa-validation-1003", "mrqa_naturalquestions-validation-10680", "mrqa_newsqa-validation-1030", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-1407", "mrqa_hotpotqa-validation-1989", "mrqa_squad-validation-6986", "mrqa_squad-validation-4546", "mrqa_hotpotqa-validation-5838"], "EFR": 1.0, "Overall": 0.8560267857142857}, {"timecode": 7, "before_eval_results": {"predictions": ["a free state", "Oireachtas funds", "May 21, 2013", "Warsaw", "the communications between Yuan dynasty and its ally and subordinate in Persia, the Ilkhanate", "n > 3", "Spanish moss", "frequency and severity of micrometeorite impacts", "Charles Avison", "colonel", "Pedro Men\u00e9ndez de Avil\u00e9s", "a mainline Protestant Methodist denomination", "Capital Cities Communications", "up to 40 km wide", "meritocracy", "reactive allotrope of oxygen", "a string over an alphabet", "markets", "women not taking jobs due to marriage or pregnancy", "May 21, 2013", "By 9000 BP", "Neoclassical economics", "iteratively", "home viewers who made tape recordings of the show", "the Kenyan coastal town of Kilifi", "first set of endosymbiotic events", "Abe Silverstein", "embroidery", "three", "1823", "British Sky Broadcasting Group plc", "Pylos and Thebes", "Supreme Court Judge", "the north wing of the State Capitol", "Syracuse", "artist and graffiti writer", "attorney, politician, and the principle founder of the Miami Dolphins.", "Bergen", "John Lennon", "David Irving", "album", "Croatan, Nantahala, and Uwharrie", "15,000", "2016 World Indoor Championships", "a German World War I fighter ace credited with 35 victories.", "psilocybin", "Washington, D.C.", "Hero", "Saint-Domingue", "Jena Malone", "1925", "National Hockey League", "one", "2009", "Walldorf", "a Douglas-Long Beach built B-17G-95-DL", "nursery rhyme", "Necator americanus and Ancylostoma duodenale", "Hamelin", "lizards", "the Blue or Silver Lines to the Morgan Boulevard Metro Station.", "a neurological disorder causing an enticing urge to move the body while relaxing or trying to get to sleep", "C. S. Forester", "six verses"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6453454142284444}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.10526315789473685, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.588235294117647, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.4, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8066", "mrqa_squad-validation-5351", "mrqa_squad-validation-10174", "mrqa_squad-validation-9195", "mrqa_squad-validation-3497", "mrqa_squad-validation-1678", "mrqa_squad-validation-150", "mrqa_squad-validation-9436", "mrqa_squad-validation-8453", "mrqa_hotpotqa-validation-5591", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-4565", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-3672", "mrqa_hotpotqa-validation-2925", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-60", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-5346", "mrqa_naturalquestions-validation-6200", "mrqa_newsqa-validation-4024", "mrqa_searchqa-validation-7219", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-1246", "mrqa_triviaqa-validation-221"], "SR": 0.546875, "CSR": 0.69140625, "retrieved_ids": ["mrqa_squad-train-51738", "mrqa_squad-train-28857", "mrqa_squad-train-54605", "mrqa_squad-train-25393", "mrqa_squad-train-60538", "mrqa_squad-train-69703", "mrqa_squad-train-10866", "mrqa_squad-train-1284", "mrqa_squad-train-45089", "mrqa_squad-train-69181", "mrqa_squad-train-12151", "mrqa_squad-train-80667", "mrqa_squad-train-18125", "mrqa_squad-train-39712", "mrqa_squad-train-17619", "mrqa_squad-train-64070", "mrqa_naturalquestions-validation-5451", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-5928", "mrqa_squad-validation-1902", "mrqa_triviaqa-validation-3032", "mrqa_naturalquestions-validation-849", "mrqa_squad-validation-133", "mrqa_squad-validation-2632", "mrqa_squad-validation-10186", "mrqa_searchqa-validation-5697", "mrqa_naturalquestions-validation-10546", "mrqa_searchqa-validation-6338", "mrqa_squad-validation-6211", "mrqa_squad-validation-8750", "mrqa_newsqa-validation-1030", "mrqa_squad-validation-9989"], "EFR": 1.0, "Overall": 0.845703125}, {"timecode": 8, "before_eval_results": {"predictions": ["a rock concert", "50-yard line", "decrease in the price of skilled labor", "12 December 1964", "speed-up theorem", "Schr\u00f6dinger equation", "Guinness World Records", "nominate speakers", "Louis Agassiz", "Annual Status of Education Report", "locomotion", "geophysical surveys", "The later accidental introduction of Beroe", "socially", "one of the most common forms of school discipline", "the Romantic Rhine", "British colonists", "adjustable spring-loaded valve", "complicated", "Arabic numerals", "7\u20134\u20132\u20133 system", "much higher school fees", "WatchESPN", "Economist", "Richard Lindzen", "prohibited emigration", "Islamism", "between 25-minute episodes", "Figaro", "Mazda", "US Naval Submarine Base New London submarine school", "Secretary of Defense", "Minyue", "Vyd\u016bnas", "1989", "American", "Anne of Green Gables", "Fainaru Fantaj\u012b Tuerubu", "Reverend Lovejoy", "composer, music teacher, and conductor", "Walldorf", "Kings Point, New York", "Bill Miner", "Agent 99", "Outside", "Christopher Nolan", "Umina Beach", "Chicago", "Thriller", "1992", "Let's Make Sure We Kiss Goodbye", "Andrzej Go\u0142ota", "Alonso L\u00f3pez", "post\u2013World War II", "Waylon Albright", "The New Yorker", "Type 10", "need to repent in time", "nirvana", "Jason Chaffetz", "a dull and invisible gas", "Speaker of the House of Representatives", "1947, 1956, 1975, 2015 and 2017", "49"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6622858044733044}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.45454545454545453, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7436", "mrqa_squad-validation-8369", "mrqa_squad-validation-10386", "mrqa_squad-validation-4327", "mrqa_squad-validation-2085", "mrqa_squad-validation-7131", "mrqa_squad-validation-7707", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-2481", "mrqa_naturalquestions-validation-4556", "mrqa_searchqa-validation-14617", "mrqa_naturalquestions-validation-5865"], "SR": 0.59375, "CSR": 0.6805555555555556, "retrieved_ids": ["mrqa_squad-train-16597", "mrqa_squad-train-47011", "mrqa_squad-train-12233", "mrqa_squad-train-49953", "mrqa_squad-train-41327", "mrqa_squad-train-64973", "mrqa_squad-train-23475", "mrqa_squad-train-59463", "mrqa_squad-train-79799", "mrqa_squad-train-45803", "mrqa_squad-train-82793", "mrqa_squad-train-32911", "mrqa_squad-train-2310", "mrqa_squad-train-84619", "mrqa_squad-train-3200", "mrqa_squad-train-49312", "mrqa_squad-validation-4546", "mrqa_newsqa-validation-4024", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-7468", "mrqa_squad-validation-9416", "mrqa_squad-validation-3811", "mrqa_squad-validation-8273", "mrqa_hotpotqa-validation-60", "mrqa_squad-validation-6526", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-1590", "mrqa_hotpotqa-validation-3672", "mrqa_squad-validation-6029", "mrqa_squad-validation-8066", "mrqa_triviaqa-validation-7615"], "EFR": 1.0, "Overall": 0.8402777777777778}, {"timecode": 9, "before_eval_results": {"predictions": ["Konwiktorska Street", "glass", "39", "1760", "CALIPSO", "the state", "The European Court of Justice", "if the head of government of a country were to refuse to enforce a decision of that country's highest court", "lymphokines", "solar power", "specific terminology has no more (or no less) meaning than the individual orator intends it to have", "Finsteraarhorn", "1015 kelvins", "Aaron Spelling", "1770", "833,500", "1851", "Canada", "Northern", "between 1859 and 1865", "a forum to inform the jury and the public of the political circumstances surrounding the case and their reasons for breaking the law via civil disobedience", "Gap", "\"missile gap\"", "Kensington", "Port of Long Beach", "saloon-keeper", "made into a TV series", "close to 50 million", "George Clooney", "Matt Groening", "Hern\u00e1n Crespo", "William Finn", "Kenny Young", "Alistair Grant", "poetry, theater, art, music, the media, and books", "The Rebirth", "sulfur mustard H", "Hearts", "Christian Maelen", "a scholar during the Joseon Dynasty who begins to write erotic novels, and becomes the lover of the King's favorite concubine", "The Terminator", "Saint Petersburg Conservatory", "Michael Phelps", "Bolton, England", "Quasimodo", "Cuban", "Cleveland Browns", "Maldives", "Paris", "the Kentucky Music Hall of Fame", "American 3D computer-animated comedy", "Agra", "Polka", "Esteban Ocon", "film actress", "Kassie DePaiva", "Jaydev Shah", "Stephen Lang", "Doris Lessing", "April", "voluntary manslaughter", "Sodra nongovernmental organization", "Tutankhamun", "kana"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7550033861155185}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.25, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.4, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3173", "mrqa_squad-validation-6814", "mrqa_squad-validation-6837", "mrqa_squad-validation-5294", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-1980", "mrqa_newsqa-validation-534", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-9926"], "SR": 0.703125, "CSR": 0.6828125, "retrieved_ids": ["mrqa_squad-train-84565", "mrqa_squad-train-31249", "mrqa_squad-train-20609", "mrqa_squad-train-75571", "mrqa_squad-train-186", "mrqa_squad-train-19453", "mrqa_squad-train-50648", "mrqa_squad-train-74092", "mrqa_squad-train-44053", "mrqa_squad-train-57220", "mrqa_squad-train-4260", "mrqa_squad-train-39234", "mrqa_squad-train-4503", "mrqa_squad-train-21240", "mrqa_squad-train-83154", "mrqa_squad-train-14105", "mrqa_searchqa-validation-7219", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-8186", "mrqa_triviaqa-validation-6381", "mrqa_naturalquestions-validation-1382", "mrqa_hotpotqa-validation-2986", "mrqa_squad-validation-8369", "mrqa_naturalquestions-validation-6573", "mrqa_squad-validation-7819", "mrqa_hotpotqa-validation-2925", "mrqa_hotpotqa-validation-60", "mrqa_hotpotqa-validation-32", "mrqa_squad-validation-4949", "mrqa_squad-validation-3479", "mrqa_naturalquestions-validation-8657"], "EFR": 1.0, "Overall": 0.84140625}, {"timecode": 10, "UKR": 0.77734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-2362", "mrqa_hotpotqa-validation-2481", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-2925", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3398", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3672", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3875", "mrqa_hotpotqa-validation-3898", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-476", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4903", "mrqa_hotpotqa-validation-4944", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5162", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-5366", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5512", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-5591", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5701", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-60", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-965", "mrqa_hotpotqa-validation-982", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1748", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-3330", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-476", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-6604", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-9536", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9842", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-534", "mrqa_searchqa-validation-13515", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-5338", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-9926", "mrqa_squad-validation-10001", "mrqa_squad-validation-10059", "mrqa_squad-validation-10063", "mrqa_squad-validation-1008", "mrqa_squad-validation-10123", "mrqa_squad-validation-10148", "mrqa_squad-validation-10174", "mrqa_squad-validation-10181", "mrqa_squad-validation-10186", "mrqa_squad-validation-10273", "mrqa_squad-validation-10316", "mrqa_squad-validation-10386", "mrqa_squad-validation-10457", "mrqa_squad-validation-10463", "mrqa_squad-validation-10470", "mrqa_squad-validation-1078", "mrqa_squad-validation-1089", "mrqa_squad-validation-1161", "mrqa_squad-validation-1177", "mrqa_squad-validation-1177", "mrqa_squad-validation-1182", "mrqa_squad-validation-1187", "mrqa_squad-validation-1219", "mrqa_squad-validation-1254", "mrqa_squad-validation-1277", "mrqa_squad-validation-133", "mrqa_squad-validation-134", "mrqa_squad-validation-1356", "mrqa_squad-validation-1423", "mrqa_squad-validation-1432", "mrqa_squad-validation-1436", "mrqa_squad-validation-1498", "mrqa_squad-validation-150", "mrqa_squad-validation-151", "mrqa_squad-validation-1593", "mrqa_squad-validation-1613", "mrqa_squad-validation-1614", "mrqa_squad-validation-1640", "mrqa_squad-validation-1649", "mrqa_squad-validation-1665", "mrqa_squad-validation-1678", "mrqa_squad-validation-168", "mrqa_squad-validation-1681", "mrqa_squad-validation-175", "mrqa_squad-validation-1759", "mrqa_squad-validation-1767", "mrqa_squad-validation-1779", "mrqa_squad-validation-1815", "mrqa_squad-validation-185", "mrqa_squad-validation-1859", "mrqa_squad-validation-1891", "mrqa_squad-validation-1898", "mrqa_squad-validation-1902", "mrqa_squad-validation-1920", "mrqa_squad-validation-194", "mrqa_squad-validation-1980", "mrqa_squad-validation-2079", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2144", "mrqa_squad-validation-215", "mrqa_squad-validation-2186", "mrqa_squad-validation-2197", "mrqa_squad-validation-2200", "mrqa_squad-validation-2214", "mrqa_squad-validation-2248", "mrqa_squad-validation-2272", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-242", "mrqa_squad-validation-2473", "mrqa_squad-validation-2490", "mrqa_squad-validation-2568", "mrqa_squad-validation-2586", "mrqa_squad-validation-2612", "mrqa_squad-validation-2632", "mrqa_squad-validation-2666", "mrqa_squad-validation-2672", "mrqa_squad-validation-270", "mrqa_squad-validation-270", "mrqa_squad-validation-2704", "mrqa_squad-validation-271", "mrqa_squad-validation-2725", "mrqa_squad-validation-2765", "mrqa_squad-validation-2775", "mrqa_squad-validation-2807", "mrqa_squad-validation-2811", "mrqa_squad-validation-2853", "mrqa_squad-validation-2873", "mrqa_squad-validation-2893", "mrqa_squad-validation-2950", "mrqa_squad-validation-2975", "mrqa_squad-validation-2986", "mrqa_squad-validation-3007", "mrqa_squad-validation-3076", "mrqa_squad-validation-3096", "mrqa_squad-validation-3124", "mrqa_squad-validation-3125", "mrqa_squad-validation-313", "mrqa_squad-validation-3140", "mrqa_squad-validation-3143", "mrqa_squad-validation-3173", "mrqa_squad-validation-3275", "mrqa_squad-validation-3319", "mrqa_squad-validation-3327", "mrqa_squad-validation-3370", "mrqa_squad-validation-3407", "mrqa_squad-validation-3464", "mrqa_squad-validation-3479", "mrqa_squad-validation-3492", "mrqa_squad-validation-3497", "mrqa_squad-validation-3511", "mrqa_squad-validation-3537", "mrqa_squad-validation-3550", "mrqa_squad-validation-3581", "mrqa_squad-validation-3676", "mrqa_squad-validation-3723", "mrqa_squad-validation-3740", "mrqa_squad-validation-375", "mrqa_squad-validation-3750", "mrqa_squad-validation-3770", "mrqa_squad-validation-3774", "mrqa_squad-validation-3788", "mrqa_squad-validation-38", "mrqa_squad-validation-3802", "mrqa_squad-validation-3811", "mrqa_squad-validation-3813", "mrqa_squad-validation-3842", "mrqa_squad-validation-3852", "mrqa_squad-validation-3871", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3923", "mrqa_squad-validation-3937", "mrqa_squad-validation-3939", "mrqa_squad-validation-3945", "mrqa_squad-validation-402", "mrqa_squad-validation-4034", "mrqa_squad-validation-4179", "mrqa_squad-validation-420", "mrqa_squad-validation-4227", "mrqa_squad-validation-4246", "mrqa_squad-validation-4292", "mrqa_squad-validation-4327", "mrqa_squad-validation-4430", "mrqa_squad-validation-4437", "mrqa_squad-validation-4473", "mrqa_squad-validation-4484", "mrqa_squad-validation-4607", "mrqa_squad-validation-4612", "mrqa_squad-validation-4636", "mrqa_squad-validation-4660", "mrqa_squad-validation-4737", "mrqa_squad-validation-4750", "mrqa_squad-validation-476", "mrqa_squad-validation-4882", "mrqa_squad-validation-4898", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-4981", "mrqa_squad-validation-5007", "mrqa_squad-validation-5019", "mrqa_squad-validation-5030", "mrqa_squad-validation-5042", "mrqa_squad-validation-5085", "mrqa_squad-validation-5135", "mrqa_squad-validation-5147", "mrqa_squad-validation-5196", "mrqa_squad-validation-5198", "mrqa_squad-validation-5276", "mrqa_squad-validation-5294", "mrqa_squad-validation-5325", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5351", "mrqa_squad-validation-5389", "mrqa_squad-validation-5434", "mrqa_squad-validation-5531", "mrqa_squad-validation-5621", "mrqa_squad-validation-5634", "mrqa_squad-validation-5671", "mrqa_squad-validation-5699", "mrqa_squad-validation-5724", "mrqa_squad-validation-5736", "mrqa_squad-validation-5751", "mrqa_squad-validation-5788", "mrqa_squad-validation-5797", "mrqa_squad-validation-5869", "mrqa_squad-validation-5875", "mrqa_squad-validation-5947", "mrqa_squad-validation-5961", "mrqa_squad-validation-6029", "mrqa_squad-validation-6089", "mrqa_squad-validation-611", "mrqa_squad-validation-6159", "mrqa_squad-validation-6163", "mrqa_squad-validation-624", "mrqa_squad-validation-6318", "mrqa_squad-validation-6341", "mrqa_squad-validation-635", "mrqa_squad-validation-6351", "mrqa_squad-validation-6381", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6546", "mrqa_squad-validation-6555", "mrqa_squad-validation-6569", "mrqa_squad-validation-6589", "mrqa_squad-validation-6594", "mrqa_squad-validation-6628", "mrqa_squad-validation-6636", "mrqa_squad-validation-6648", "mrqa_squad-validation-6676", "mrqa_squad-validation-6700", "mrqa_squad-validation-6716", "mrqa_squad-validation-6752", "mrqa_squad-validation-679", "mrqa_squad-validation-6814", "mrqa_squad-validation-682", "mrqa_squad-validation-6837", "mrqa_squad-validation-6838", "mrqa_squad-validation-6873", "mrqa_squad-validation-6877", "mrqa_squad-validation-6924", "mrqa_squad-validation-6960", "mrqa_squad-validation-6978", "mrqa_squad-validation-6981", "mrqa_squad-validation-6986", "mrqa_squad-validation-7126", "mrqa_squad-validation-7131", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7168", "mrqa_squad-validation-7194", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7246", "mrqa_squad-validation-7277", "mrqa_squad-validation-729", "mrqa_squad-validation-73", "mrqa_squad-validation-735", "mrqa_squad-validation-7354", "mrqa_squad-validation-7354", "mrqa_squad-validation-7367", "mrqa_squad-validation-7384", "mrqa_squad-validation-7436", "mrqa_squad-validation-7447", "mrqa_squad-validation-7476", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-7570", "mrqa_squad-validation-7585", "mrqa_squad-validation-76", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7652", "mrqa_squad-validation-7656", "mrqa_squad-validation-7689", "mrqa_squad-validation-7707", "mrqa_squad-validation-7713", "mrqa_squad-validation-773", "mrqa_squad-validation-7782", "mrqa_squad-validation-7788", "mrqa_squad-validation-7819", "mrqa_squad-validation-7838", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-8028", "mrqa_squad-validation-8043", "mrqa_squad-validation-8049", "mrqa_squad-validation-8066", "mrqa_squad-validation-8118", "mrqa_squad-validation-8139", "mrqa_squad-validation-816", "mrqa_squad-validation-824", "mrqa_squad-validation-8253", "mrqa_squad-validation-8273", "mrqa_squad-validation-8283", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8386", "mrqa_squad-validation-8399", "mrqa_squad-validation-8453", "mrqa_squad-validation-850", "mrqa_squad-validation-8500", "mrqa_squad-validation-8502", "mrqa_squad-validation-8505", "mrqa_squad-validation-8523", "mrqa_squad-validation-8529", "mrqa_squad-validation-8555", "mrqa_squad-validation-8579", "mrqa_squad-validation-8601", "mrqa_squad-validation-8603", "mrqa_squad-validation-8643", "mrqa_squad-validation-8680", "mrqa_squad-validation-8683", "mrqa_squad-validation-8750", "mrqa_squad-validation-8801", "mrqa_squad-validation-889", "mrqa_squad-validation-8900", "mrqa_squad-validation-8901", "mrqa_squad-validation-8924", "mrqa_squad-validation-8927", "mrqa_squad-validation-8948", "mrqa_squad-validation-8966", "mrqa_squad-validation-8969", "mrqa_squad-validation-8987", "mrqa_squad-validation-9048", "mrqa_squad-validation-9097", "mrqa_squad-validation-9135", "mrqa_squad-validation-9157", "mrqa_squad-validation-9165", "mrqa_squad-validation-9254", "mrqa_squad-validation-9269", "mrqa_squad-validation-9271", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9325", "mrqa_squad-validation-9328", "mrqa_squad-validation-9367", "mrqa_squad-validation-9416", "mrqa_squad-validation-9436", "mrqa_squad-validation-9459", "mrqa_squad-validation-9470", "mrqa_squad-validation-9531", "mrqa_squad-validation-9543", "mrqa_squad-validation-9553", "mrqa_squad-validation-9559", "mrqa_squad-validation-9608", "mrqa_squad-validation-9764", "mrqa_squad-validation-9787", "mrqa_squad-validation-9819", "mrqa_squad-validation-9868", "mrqa_squad-validation-9900", "mrqa_squad-validation-9901", "mrqa_squad-validation-9943", "mrqa_squad-validation-9989", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-1670", "mrqa_triviaqa-validation-1800", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-471", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-5627", "mrqa_triviaqa-validation-5960", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6577", "mrqa_triviaqa-validation-6734", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-71", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7601", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7683"], "OKR": 0.8984375, "KG": 0.37421875, "before_eval_results": {"predictions": ["hydrogen and helium", "supervisory church body", "AC", "Jerricho Cotchery", "Isaac Komnenos", "a certain number of teacher's salaries are paid", "cylinders and valve gear", "ABC1", "ATP energy", "Transform", "Hulu", "two", "social unrest and violence", "Toyota Corona, the Toyota Corolla, the Datsun B210", "Metro Light Rail system", "veteran", "A Turing machine", "Luther", "antibodies", "1978", "Mark Twain", "1804", "seven", "a skirt with a narrow enough hem", "David Hilbert", "India", "Patsy Stone", "a jingle", "anaerobe", "Kiss Me", "lincoln", "Mumbai", "avant-garde", "Jordan", "Nang Klao Rama III", "A.N. Whitehead", "A-ha", "N Africa", "William Holden", "Charlie Brooker", "racing", "gold", "an N", "Egypt", "tennis", "\"Pale Rider\"", "an inspector of police", "Norns", "steel", "Old Trafford", "the people you know and love", "rabbit", "The King and I", "Lisbeth Salander", "Standard Oil Company", "Miranda warning", "Janis Joplin", "1983", "over 1.6 million", "Mark Neveldine and Brian Taylor", "Alina Cho", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves", "a talking stuffed bear", "paul mcc McCartney"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5620349702380953}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, true, true, false, false, true, false, false, false, false, false, false, true, true, false, false, false], "QA-F1": [0.42857142857142855, 0.0, 0.6666666666666666, 0.0, 1.0, 0.8750000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.4, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3667", "mrqa_squad-validation-2468", "mrqa_squad-validation-1402", "mrqa_squad-validation-780", "mrqa_squad-validation-7036", "mrqa_squad-validation-3708", "mrqa_squad-validation-336", "mrqa_triviaqa-validation-921", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-1160", "mrqa_triviaqa-validation-3657", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-1291", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-2937", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-6821", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-4137", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-2372", "mrqa_triviaqa-validation-3159", "mrqa_triviaqa-validation-4982", "mrqa_triviaqa-validation-4005", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-1461", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-7580", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9135", "mrqa_hotpotqa-validation-1526", "mrqa_newsqa-validation-2422", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-10099"], "SR": 0.453125, "CSR": 0.6619318181818181, "retrieved_ids": ["mrqa_squad-train-61511", "mrqa_squad-train-53843", "mrqa_squad-train-40429", "mrqa_squad-train-70521", "mrqa_squad-train-21467", "mrqa_squad-train-3360", "mrqa_squad-train-42664", "mrqa_squad-train-35587", "mrqa_squad-train-78267", "mrqa_squad-train-2288", "mrqa_squad-train-16225", "mrqa_squad-train-54897", "mrqa_squad-train-47042", "mrqa_squad-train-36923", "mrqa_squad-train-86151", "mrqa_squad-train-45046", "mrqa_squad-validation-1649", "mrqa_squad-validation-1089", "mrqa_hotpotqa-validation-2673", "mrqa_naturalquestions-validation-661", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-4596", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5378", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-2130", "mrqa_squad-validation-2315", "mrqa_searchqa-validation-13515", "mrqa_squad-validation-6814", "mrqa_hotpotqa-validation-3564"], "EFR": 0.9714285714285714, "Overall": 0.7366720779220779}, {"timecode": 11, "before_eval_results": {"predictions": ["high than normal O2 exposure", "center of the curving path", "polynomial", "1206", "Dutch East India Company", "Newton", "after the Franco-German War", "chloroplasts", "June 4, 2014", "energy", "December 2014", "18 February 1546", "19", "meritocracy", "photooxidative damage", "tangential force", "St John the Baptist", "more or less rounded, sometimes nearly spherical and other times more cylindrical or egg-shaped", "adenosine triphosphate", "Johann Eck", "Marne", "the American Civil War", "cigarettes", "egypt", "Charles Dickens's Bleak House", "butterfly", "John Flamsteed", "bison", "Anita Roddick", "the S bank of the river", "Tamar", "Nizhny Novgorod", "The Word", "The Left Book Club", "tchaikovsky", "Tony Blackburn", "desert", "spa town", "butterfly", "Tarzan", "James Hanratty", "Middlesbrough", "karl marx", "London Pride", "thaw", "jumper", "Milton Keynes", "November", "kofta", "Coventry to Leicester Motorway", "Bon Jovi", "Little Arrows", "legion", "Saint Vitus", "Syriza", "longer from shore to shore", "apes", "Channel 4", "Cartoon Network Too", "pattern matching", "women to cover their bodies and heads from view", "desert tortoise", "Big Ben", "egypt"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5702256944444445}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3610", "mrqa_squad-validation-8229", "mrqa_squad-validation-361", "mrqa_squad-validation-8931", "mrqa_squad-validation-4469", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2889", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-3045", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-7124", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-4400", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-6665", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-18", "mrqa_searchqa-validation-12699", "mrqa_searchqa-validation-5935", "mrqa_newsqa-validation-3918"], "SR": 0.515625, "CSR": 0.6497395833333333, "retrieved_ids": ["mrqa_squad-train-47277", "mrqa_squad-train-68377", "mrqa_squad-train-22409", "mrqa_squad-train-57855", "mrqa_squad-train-38970", "mrqa_squad-train-24803", "mrqa_squad-train-37360", "mrqa_squad-train-58182", "mrqa_squad-train-62924", "mrqa_squad-train-48175", "mrqa_squad-train-892", "mrqa_squad-train-10987", "mrqa_squad-train-33439", "mrqa_squad-train-43025", "mrqa_squad-train-215", "mrqa_squad-train-46715", "mrqa_squad-validation-9012", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-7219", "mrqa_squad-validation-7713", "mrqa_triviaqa-validation-6514", "mrqa_squad-validation-9195", "mrqa_squad-validation-4961", "mrqa_squad-validation-1902", "mrqa_naturalquestions-validation-3938", "mrqa_hotpotqa-validation-4422", "mrqa_triviaqa-validation-1590", "mrqa_squad-validation-4636", "mrqa_hotpotqa-validation-5587", "mrqa_squad-validation-9989", "mrqa_triviaqa-validation-3032", "mrqa_squad-validation-3370"], "EFR": 1.0, "Overall": 0.7399479166666666}, {"timecode": 12, "before_eval_results": {"predictions": ["the Marburg Colloquy", "a negative long-term impact", "with known magnitudes of force", "Southern California Megaregion, one of the 11 megaregions of the United States", "Ed Whitfield", "Basel", "Levi's Stadium", "Miasma theory", "a freshwater lake", "Monte Gargano", "50-yard line", "National Galleries of Scotland", "Arts & Entertainment Television (A&E)", "gravity", "Paul Samuelson, the first American to win the Nobel Memorial Prize in Economic Sciences", "fans", "pathogen attack", "a not-for-profit United States computer networking consortium", "1850", "Don Valley Parkway / Highway404 Junction in Toronto", "Spanish explorers", "eight", "Columbia University", "Waylon Jennings", "black and yellow", "Melissa Disney", "five", "1990", "Jeffery Lewis", "In 1987", "Tyler, Ali, and Lydia", "May 18, 2018", "2015", "Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "$75,000", "332", "Hudson Bay", "an expression of unknown origin", "accomplish the objectives of the organization", "the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "1775", "seam insignia", "a normally inaccessible mini-game", "Charles Darwin", "heat transfer", "bactrian", "2016", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Roger Nichols and Paul Williams", "T.S. Eliot", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Joe Spano", "at the 1964 Republican National Convention in San Francisco, California", "north end", "Paul Lynde", "a sweet-and-sour, dark-brown vinegar", "Ross Bagdasarian", "Gracie Mansion", "Peter Seamus O'Toole", "Arthur E. Morgan III", "his wife, Cabinet members, governors and other public and private officials", "Wayne's World", "peter", "Arkansas"], "metric_results": {"EM": 0.53125, "QA-F1": 0.689738938875452}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.8, 0.2857142857142857, 0.4615384615384615, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545455, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.7894736842105263, 0.0, 0.0, 0.4, 1.0, 0.4, 0.25, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.8, 0.4, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5213", "mrqa_squad-validation-10321", "mrqa_squad-validation-2553", "mrqa_squad-validation-4877", "mrqa_squad-validation-10352", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1613", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-9142", "mrqa_naturalquestions-validation-627", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-1864", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-1575", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-11", "mrqa_hotpotqa-validation-3299", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-3444", "mrqa_searchqa-validation-15790"], "SR": 0.53125, "CSR": 0.640625, "retrieved_ids": ["mrqa_squad-train-5930", "mrqa_squad-train-66897", "mrqa_squad-train-75960", "mrqa_squad-train-68351", "mrqa_squad-train-78449", "mrqa_squad-train-53528", "mrqa_squad-train-67408", "mrqa_squad-train-78912", "mrqa_squad-train-18109", "mrqa_squad-train-68307", "mrqa_squad-train-24915", "mrqa_squad-train-1293", "mrqa_squad-train-70451", "mrqa_squad-train-16469", "mrqa_squad-train-18586", "mrqa_squad-train-72576", "mrqa_squad-validation-1767", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-7590", "mrqa_hotpotqa-validation-2986", "mrqa_squad-validation-9855", "mrqa_hotpotqa-validation-4080", "mrqa_triviaqa-validation-2937", "mrqa_squad-validation-6814", "mrqa_hotpotqa-validation-2244", "mrqa_triviaqa-validation-6514", "mrqa_hotpotqa-validation-2925", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-7574", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-3045"], "EFR": 1.0, "Overall": 0.7381249999999999}, {"timecode": 13, "before_eval_results": {"predictions": ["phlogiston theory of combustion and corrosion", "1974", "Andrew Alper", "shaping ideas about the free market", "learning", "The Prospect Studios", "Milton Friedman Institute", "the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "1972", "polytechnics became new universities", "conservation of momentum", "October 1973", "Frontex", "a dispute over control of the confluence of the Allegheny and Monongahela rivers", "1080i HD", "1924", "Purple drank", "a multi-control USB mouse manufactured by Mitsumi Electric", "Raden Panji Nugroho Notosusanto", "juice", "October 21, 2016", "Donald Duck", "February 12, 2014", "Black Mountain College", "An aircraft", "University of Vienna", "Sunday, November 2, 2003", "Golden Calf", "2012 Summer Olympics", "February 13, 1946", "Leslie James \"Les\" Clark", "stopwatch feature", "Nathan Rothschild", "The Grandmaster", "Montana State University", "37", "Shakespeare Wallah", "a Rugby Sevens competition for the twelve Aviva Premiership clubs that will play the following season", "Hank Azaria", "Supergirl", "Michelle", "22 November 17615 July 1816", "Mauritian", "mixed martial arts", "Cape Cod", "Captain B.J. Hunnicutt", "Humberside Airport", "the most influential private citizen in the America of his day", "energy trading company", "the new king", "Connie", "Book of Judges", "12", "35", "span", "the seven stages of a man's life", "HMS Thunderbolt", "vinegar Joe", "Ralph Lauren", "the Pakistani Taliban's chief in Punjab", "Detroit", "Walt Disney Night", "John Lennon and George Harrison", "terminal brain cancer"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6007421398046398}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.15384615384615385, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7763", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-5055", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-4646", "mrqa_hotpotqa-validation-5239", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-405", "mrqa_naturalquestions-validation-2844", "mrqa_triviaqa-validation-4212", "mrqa_newsqa-validation-1095", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-15877", "mrqa_newsqa-validation-2128"], "SR": 0.5625, "CSR": 0.6350446428571428, "retrieved_ids": ["mrqa_squad-train-81254", "mrqa_squad-train-13791", "mrqa_squad-train-81694", "mrqa_squad-train-17169", "mrqa_squad-train-57536", "mrqa_squad-train-38727", "mrqa_squad-train-57399", "mrqa_squad-train-9554", "mrqa_squad-train-67003", "mrqa_squad-train-337", "mrqa_squad-train-50853", "mrqa_squad-train-29759", "mrqa_squad-train-78371", "mrqa_squad-train-23173", "mrqa_squad-train-25426", "mrqa_squad-train-53311", "mrqa_squad-validation-7713", "mrqa_naturalquestions-validation-1613", "mrqa_squad-validation-361", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-7580", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-1575", "mrqa_squad-validation-8453", "mrqa_squad-validation-8066", "mrqa_naturalquestions-validation-8186", "mrqa_squad-validation-9012", "mrqa_hotpotqa-validation-2244", "mrqa_squad-validation-8399", "mrqa_squad-validation-2853"], "EFR": 0.9285714285714286, "Overall": 0.7227232142857142}, {"timecode": 14, "before_eval_results": {"predictions": ["Chu'Tsai", "second-largest", "17", "Ed Mangan", "suspended", "send aid", "almost 6.4 billion litres", "a dam turbine", "SAP Center in San Jose", "Niels Jerne", "Central Bridge", "Vistula River", "an enzyme called rubisco", "Spanish moss", "Louis Pasteur", "\"The Tales of Hoffmann\"", "Lake Placid, New York", "four", "Elton John", "1978", "Victorian England", "Indian", "polyphonic", "1970", "Tom Kitt", "Outside", "Forbes", "acidic bogs", "Tampa", "Saoirse Ronan", "Anne Cox Chambers  Anne Beau Cox Chambers", "Kansas State", "Prescription Drug User Fee Act", "San Jose Sharks", "Foxborough", "Cyclic Defrost", "Stalybridge Celtic Football Club", "John Kevin Delaney", "Sister, Sister", "1955", "2016", "2 March 1972", "Derry", "11 November 1869", "Sam Bettley", "Larry Drake", "Wilhelmus Simon Petrus Fortuijn", "Hockey Club Davos", "Sex Drive", "Toxics Release Inventory", "St Augustine's Abbey", "a fictional character in the \"Star Wars\" franchise", "Fred Claus", "La Scala, Milan", "its genome", "1997", "nicotinic acid", "Israel", "Kim Clijsters", "Rev. Alberto Cutie", "Brassa", "William Rehnquist", "cubbard", "milk chocolate"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6118303571428572}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9695", "mrqa_squad-validation-2939", "mrqa_squad-validation-457", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-414", "mrqa_hotpotqa-validation-2574", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-187", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-2725", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-4780", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-866", "mrqa_naturalquestions-validation-9368", "mrqa_triviaqa-validation-5714", "mrqa_newsqa-validation-1150", "mrqa_searchqa-validation-13156", "mrqa_searchqa-validation-16559", "mrqa_searchqa-validation-13152", "mrqa_searchqa-validation-6300"], "SR": 0.515625, "CSR": 0.6270833333333333, "retrieved_ids": ["mrqa_squad-train-35434", "mrqa_squad-train-13335", "mrqa_squad-train-15519", "mrqa_squad-train-2815", "mrqa_squad-train-31588", "mrqa_squad-train-32719", "mrqa_squad-train-43721", "mrqa_squad-train-54213", "mrqa_squad-train-58456", "mrqa_squad-train-51972", "mrqa_squad-train-68732", "mrqa_squad-train-15860", "mrqa_squad-train-18843", "mrqa_squad-train-12188", "mrqa_squad-train-72116", "mrqa_squad-train-74374", "mrqa_hotpotqa-validation-5591", "mrqa_squad-validation-6163", "mrqa_triviaqa-validation-2674", "mrqa_newsqa-validation-1095", "mrqa_hotpotqa-validation-405", "mrqa_squad-validation-3676", "mrqa_squad-validation-10273", "mrqa_naturalquestions-validation-10615", "mrqa_searchqa-validation-15877", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-422", "mrqa_naturalquestions-validation-3257", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-7511"], "EFR": 1.0, "Overall": 0.7354166666666666}, {"timecode": 15, "before_eval_results": {"predictions": ["11", "1968", "relationship between teachers and children", "Westinghouse Electric", "nine months", "more than 70", "Inherited wealth", "Super Bowl XXXIII", "Alan Turing", "North America", "Conservative", "contrasts", "Clair Cameron Patterson", "Esp\u00edrito Santo Financial Group (ESFG)", "Europe", "Scotiabank Saddledome", "Carter", "S Pictures' \"Veyyil\"", "PewDie Pie", "Seoul, South Korea", "Pittsburgh", "2012", "Chinese Coffee", "John Gotti", "$7.3 billion", "a French natural philosopher, mathematician, physicist", "Umina Beach, New South Wales", "Port Macquarie", "\"Naked\"", "Edinburgh", "Anthony Lynn", "Levi Weeks", "the Mayor of the City of New York", "soccer", "Memphis Minnie", "The Five", "James Mitchum", "Candice Susan Swanepoel", "Bhaktivedanta Manor", "Thomas Christopher Ince", "Texas Tech Red Raiders", "U.S.", "south", "historic buildings, arts, and published works", "June 24, 1935", "James K. Polk", "March 31, 1944", "Linux Format", "Na Na", "Koch Industries", "Rymill Park", "Eliot Cutler", "Tchaikovsky's \"Swan Lake\" ballet", "Thomas Jefferson", "Marie Van Brittan Brown", "Billy Idol", "Full Metal jacket", "Bombay Stock Exchange", "eight or nine", "Saddam Hussein's Revolutionary Command Council", "Mastino", "Anthony Fokker", "Jean Baptiste Say", "chloropsia"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7637896825396826}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3402", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-2786", "mrqa_hotpotqa-validation-5319", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-5125", "mrqa_hotpotqa-validation-1192", "mrqa_triviaqa-validation-5715", "mrqa_newsqa-validation-1843", "mrqa_searchqa-validation-8148", "mrqa_searchqa-validation-1728", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-581"], "SR": 0.6875, "CSR": 0.630859375, "retrieved_ids": ["mrqa_squad-train-20910", "mrqa_squad-train-36557", "mrqa_squad-train-6062", "mrqa_squad-train-15069", "mrqa_squad-train-85973", "mrqa_squad-train-17802", "mrqa_squad-train-9493", "mrqa_squad-train-34495", "mrqa_squad-train-55430", "mrqa_squad-train-27162", "mrqa_squad-train-9898", "mrqa_squad-train-19621", "mrqa_squad-train-44361", "mrqa_squad-train-55477", "mrqa_squad-train-65816", "mrqa_squad-train-17620", "mrqa_squad-validation-4949", "mrqa_triviaqa-validation-4438", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-4080", "mrqa_triviaqa-validation-2437", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-9926", "mrqa_squad-validation-1277", "mrqa_triviaqa-validation-1921", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-4556", "mrqa_squad-validation-7246", "mrqa_hotpotqa-validation-1527", "mrqa_squad-validation-7819", "mrqa_searchqa-validation-15877", "mrqa_hotpotqa-validation-2764"], "EFR": 1.0, "Overall": 0.7361718749999999}, {"timecode": 16, "before_eval_results": {"predictions": ["Aston Webb", "Jin", "in his lab and elsewhere", "1253", "provide direct patient care services", "RNA silencing", "50", "lectured on the Psalms, the books of Hebrews, Romans, and Galatians", "Yes\u00fcgei", "a renewed version of the Russian imperialism and colonialism", "the American Philosophical Society", "Albert Einstein", "Bristol", "Javier Bardem", "the world's fastest steam engine", "Ben Whishaw", "2", "fullers", "Richard Noble", "Skylab", "Philistine", "Spain", "milk", "boron", "rue", "an unauthorized military expedition", "law", "bats", "Styal", "huffing", "sergeant", "Margaret Beckett", "Brad Pitt", "an hair follicle", "Canada", "a tax was made to Scots", "a marble campanile", "ancient name for calamine", "Sesame Street", "Big Brother", "Avro Lancaster", "Leeds", "Tim Brooke- Taylor", "Joseph Priestley", "white", "the Euro-Atlantic community", "Denbighshire", "Fernando Torres", "ADNAMS", "pakistan", "aircraft carrier", "Leicestershire", "Wyoming", "Xenophon", "O'Meara", "3 total", "his sixth", "2013", "Mexico", "Newcastle retained fourth place", "Hugh Grant", "pink", "dance partner", "1"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6151041666666668}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6316", "mrqa_squad-validation-6547", "mrqa_squad-validation-2254", "mrqa_squad-validation-10128", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-5830", "mrqa_triviaqa-validation-5856", "mrqa_triviaqa-validation-5844", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-6246", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-6996", "mrqa_triviaqa-validation-7155", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-5226", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3685", "mrqa_naturalquestions-validation-8087", "mrqa_hotpotqa-validation-1953", "mrqa_newsqa-validation-2467", "mrqa_hotpotqa-validation-3468"], "SR": 0.5625, "CSR": 0.6268382352941176, "retrieved_ids": ["mrqa_squad-train-84856", "mrqa_squad-train-74672", "mrqa_squad-train-49720", "mrqa_squad-train-4734", "mrqa_squad-train-6805", "mrqa_squad-train-45840", "mrqa_squad-train-62847", "mrqa_squad-train-25691", "mrqa_squad-train-77924", "mrqa_squad-train-45678", "mrqa_squad-train-37573", "mrqa_squad-train-74553", "mrqa_squad-train-37356", "mrqa_squad-train-61584", "mrqa_squad-train-55893", "mrqa_squad-train-48225", "mrqa_naturalquestions-validation-10684", "mrqa_searchqa-validation-5856", "mrqa_triviaqa-validation-6821", "mrqa_squad-validation-1767", "mrqa_hotpotqa-validation-5838", "mrqa_squad-validation-1649", "mrqa_hotpotqa-validation-3638", "mrqa_squad-validation-1402", "mrqa_triviaqa-validation-7045", "mrqa_naturalquestions-validation-6573", "mrqa_triviaqa-validation-921", "mrqa_searchqa-validation-14617", "mrqa_naturalquestions-validation-5378", "mrqa_searchqa-validation-5697", "mrqa_hotpotqa-validation-2786", "mrqa_squad-validation-10273"], "EFR": 1.0, "Overall": 0.7353676470588235}, {"timecode": 17, "before_eval_results": {"predictions": ["pattern recognition receptors", "in the northern Mokot\u00f3w", "in the courtyard adjoining the Assembly Hall", "several", "1527", "Zhongdu", "British culture", "Chicago Bears", "Bill Clinton", "differences in value added by labor, capital and land", "geochemical evolution of rock units", "an assemblage", "eagle", "Red", "Pink Floyd", "Constantinople", "Zeus", "10th anniversary", "Mesozoic", "Rodney King", "ethanethiol", "coral", "1849", "Clyde", "New Orleans", "an indelible pattern", "the liver", "right-to-left languages", "nonfiction", "he founded the colony of Rhode Island", "ring-tailed lemurs", "token lands on or passes overGO", "Mediolanum", "Martin Luther's prayer", "rupture of membranes", "emperor Claudius Caesar", "white blood cells", "Green Lantern", "Zeus", "New York Giants", "prince bernhard", "Yves Saint Laurent", "the Trucial States", "mccain", "in Ljubljana", "Carmen", "mammals", "Diamond Jim Brady", "south coast", "fruit", "the Crimean War", "a Swedish glass factory", "pesto", "providing telecommunication services to enterprises and offices", "1834", "Pantagruel", "the Savoy", "Leon Marcus Uris", "Galaxy S6 and S6 Edge", "Nazi Party members", "open heart surgery", "Tommy James and the Shondells", "the tax rate paid by a small business", "along the Californian coast"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5300347222222221}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.5, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3]}}, "before_error_ids": ["mrqa_squad-validation-1042", "mrqa_squad-validation-9400", "mrqa_squad-validation-5429", "mrqa_searchqa-validation-10868", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2329", "mrqa_searchqa-validation-7938", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-8485", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-6670", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-16258", "mrqa_searchqa-validation-4065", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-4154", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-15504", "mrqa_searchqa-validation-14923", "mrqa_searchqa-validation-2095", "mrqa_searchqa-validation-8417", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-11410", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-1387", "mrqa_hotpotqa-validation-2978", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-2546", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-2250"], "SR": 0.46875, "CSR": 0.6180555555555556, "retrieved_ids": ["mrqa_squad-train-62386", "mrqa_squad-train-6552", "mrqa_squad-train-71271", "mrqa_squad-train-83431", "mrqa_squad-train-20770", "mrqa_squad-train-57926", "mrqa_squad-train-78828", "mrqa_squad-train-42030", "mrqa_squad-train-77621", "mrqa_squad-train-19077", "mrqa_squad-train-51774", "mrqa_squad-train-68795", "mrqa_squad-train-38133", "mrqa_squad-train-49553", "mrqa_squad-train-37614", "mrqa_squad-train-50744", "mrqa_squad-validation-3610", "mrqa_triviaqa-validation-11", "mrqa_naturalquestions-validation-9071", "mrqa_triviaqa-validation-737", "mrqa_squad-validation-9012", "mrqa_squad-validation-2315", "mrqa_searchqa-validation-14617", "mrqa_squad-validation-635", "mrqa_searchqa-validation-13156", "mrqa_triviaqa-validation-1590", "mrqa_naturalquestions-validation-2130", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2372", "mrqa_searchqa-validation-15790", "mrqa_naturalquestions-validation-222", "mrqa_newsqa-validation-3444"], "EFR": 1.0, "Overall": 0.733611111111111}, {"timecode": 18, "before_eval_results": {"predictions": ["Apollo Extension Series", "5.3%", "seizures", "Mercury", "a setup phase in each involved node", "infrastructure and industrial", "MHC class I molecules", "European Court of Human Rights", "it is neither zero nor a unit", "neuronal dendrites", "The Lost Symbol", "President Robert Mugabe", "November 26", "Al Nisr Al Saudi", "two paintings", "Marcus Schrenker", "glass shards", "Chancellor Angela Merkel", "Tibet's independence", "drug trafficking is a transnational threat", "steamboat", "Osama bin Laden", "2002", "Yusuf Saad Kamel", "general secretary", "byproducts", "September", "Transport Workers Union", "urging more help", "Eintracht Frankfurt", "Jacob", "5 1/2-year-old", "canyon", "eco-horror", "Bob Dole", "Kenyan and Somali governments", "unwanted baggage", "U.S. senators", "April", "Chesley \"Sully\" Sullenberger", "Nigeria, Africa's largest producer", "Thursday", "housing, business and infrastructure repairs", "A third beluga whale", "Martin Aloysius Culhane", "Washington", "a painting and body tech operation", "to Dubai", "India in Mumbai", "snowstorm", "38", "Juan Martin Del Potro.", "he did cheat on you", "E-1, since 1996, don't have an insignia to wear", "two easily observed features", "The Comedy of Errors", "Xenophon", "devotional literature", "14th Street", "Lord Byron", "curve Ball", "jupiter", "the rams horns", "lithography"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5681134521116138}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false, true, false, false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, true, false, false, false, false, false], "QA-F1": [0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.0, 1.0, 0.5882352941176471, 0.0, 0.6666666666666666, 0.2857142857142857, 0.0, 0.5, 0.4, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.08000000000000002, 0.6666666666666666, 0.0, 1.0, 0.4, 1.0, 0.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4008", "mrqa_squad-validation-6717", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-961", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-456", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1184", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-2901", "mrqa_hotpotqa-validation-4925", "mrqa_searchqa-validation-15244", "mrqa_searchqa-validation-9803", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-8188", "mrqa_searchqa-validation-15495"], "SR": 0.421875, "CSR": 0.6077302631578947, "retrieved_ids": ["mrqa_squad-train-16811", "mrqa_squad-train-29170", "mrqa_squad-train-43834", "mrqa_squad-train-27219", "mrqa_squad-train-63457", "mrqa_squad-train-9733", "mrqa_squad-train-77332", "mrqa_squad-train-15370", "mrqa_squad-train-79533", "mrqa_squad-train-22991", "mrqa_squad-train-78255", "mrqa_squad-train-44186", "mrqa_squad-train-78627", "mrqa_squad-train-9177", "mrqa_squad-train-11415", "mrqa_squad-train-20888", "mrqa_triviaqa-validation-4663", "mrqa_hotpotqa-validation-5226", "mrqa_squad-validation-8750", "mrqa_hotpotqa-validation-4477", "mrqa_naturalquestions-validation-5865", "mrqa_squad-validation-8579", "mrqa_squad-validation-8273", "mrqa_hotpotqa-validation-3638", "mrqa_naturalquestions-validation-9650", "mrqa_squad-validation-2553", "mrqa_triviaqa-validation-221", "mrqa_hotpotqa-validation-422", "mrqa_searchqa-validation-6300", "mrqa_triviaqa-validation-1643", "mrqa_hotpotqa-validation-982", "mrqa_triviaqa-validation-4445"], "EFR": 1.0, "Overall": 0.7315460526315789}, {"timecode": 19, "before_eval_results": {"predictions": ["one of the most common", "April 1, 1963", "Islamism", "he did not want disloyal men in his army.", "conservation of momentum", "west", "154", "17 February 1546", "1996", "1962", "Friday", "Anil Kapoor.", "2005", "2008", "Ben Roethlisberger", "President Obama", "Matthew Chance", "11th year in a row", "suspended", "romantic", "the Nazi war crimes suspect", "that the deadly attack on India's financial capital last month was planned inside Pakistan", "of the Movement for Democratic Change", "Steven Chu", "Wednesday at the age of 95", "Zed", "island stronghold of the Islamic militant group Abu Sayyaf", "stepped into the museum with a rifle and began firing", "2,000 euros", "buckling under pressure from the ruling party.", "Vertikal-T", "The Everglades, known as the River of Grass,", "\"a striking blow to due process and the rule of law\"", "federal officers' bodies", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "in his car", "mother's gratitude for his mother", "South African police have opened a criminal investigation into allegations that a dorm parent mistreated students at the school.", "hundreds", "girls", "blind Majid Movahedi, the man who blinded her.", "Biden", "suicide car bombing", "Christina Romete,", "the 2007 semi defeat by the English on home soil with a 19-12 victory in Auckland", "\"Wolfman,\"", "Vernon Forrest", "The man ran out of bullets and blew himself up", "Jason Chaffetz", "Windsor, Ontario", "Alicia Keys", "Oxbow", "Zhanar Tokhtabayeba", "In 1038", "Johannes Gutenberg", "husbands", "a crystal ball", "senior men's Lithuanian national team", "two years", "Mark Hillman", "Craig Ferguson", "The spectacled bear", "sour cream", "Constellation family"], "metric_results": {"EM": 0.5, "QA-F1": 0.5768353174603175}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.8, 1.0, 0.4, 0.2857142857142857, 1.0, 1.0, 0.2222222222222222, 0.0, 0.33333333333333337, 0.2, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.14285714285714288, 1.0, 1.0, 0.19999999999999998, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-440", "mrqa_naturalquestions-validation-10551", "mrqa_triviaqa-validation-1427", "mrqa_hotpotqa-validation-4927", "mrqa_searchqa-validation-12135", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-10395", "mrqa_searchqa-validation-9391"], "SR": 0.5, "CSR": 0.60234375, "retrieved_ids": ["mrqa_squad-train-76493", "mrqa_squad-train-59683", "mrqa_squad-train-61794", "mrqa_squad-train-22373", "mrqa_squad-train-59403", "mrqa_squad-train-31626", "mrqa_squad-train-36937", "mrqa_squad-train-66876", "mrqa_squad-train-81950", "mrqa_squad-train-11671", "mrqa_squad-train-69208", "mrqa_squad-train-42370", "mrqa_squad-train-74738", "mrqa_squad-train-53349", "mrqa_squad-train-77435", "mrqa_squad-train-12874", "mrqa_squad-validation-8066", "mrqa_squad-validation-8529", "mrqa_squad-validation-7713", "mrqa_hotpotqa-validation-3468", "mrqa_squad-validation-6717", "mrqa_naturalquestions-validation-5781", "mrqa_squad-validation-635", "mrqa_triviaqa-validation-6413", "mrqa_naturalquestions-validation-8961", "mrqa_squad-validation-10273", "mrqa_newsqa-validation-2422", "mrqa_naturalquestions-validation-7728", "mrqa_hotpotqa-validation-1767", "mrqa_naturalquestions-validation-627", "mrqa_hotpotqa-validation-2978", "mrqa_triviaqa-validation-7590"], "EFR": 1.0, "Overall": 0.73046875}, {"timecode": 20, "UKR": 0.802734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1334", "mrqa_hotpotqa-validation-1419", "mrqa_hotpotqa-validation-1484", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1696", "mrqa_hotpotqa-validation-1750", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1840", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-1912", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2456", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2574", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3057", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3299", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4182", "mrqa_hotpotqa-validation-4189", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4565", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4717", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4780", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-5034", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5319", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-5366", "mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5838", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-748", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-982", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-476", "mrqa_naturalquestions-validation-4876", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9545", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-3297", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3365", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-3931", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-456", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-872", "mrqa_searchqa-validation-11130", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-12104", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-13156", "mrqa_searchqa-validation-13321", "mrqa_searchqa-validation-13515", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-14923", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-16258", "mrqa_searchqa-validation-16559", "mrqa_searchqa-validation-1926", "mrqa_searchqa-validation-2095", "mrqa_searchqa-validation-2329", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-5338", "mrqa_searchqa-validation-5531", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-7219", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7938", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-9391", "mrqa_searchqa-validation-9683", "mrqa_squad-validation-100", "mrqa_squad-validation-10001", "mrqa_squad-validation-10059", "mrqa_squad-validation-10063", "mrqa_squad-validation-1008", "mrqa_squad-validation-10186", "mrqa_squad-validation-10256", "mrqa_squad-validation-10273", "mrqa_squad-validation-10316", "mrqa_squad-validation-10346", "mrqa_squad-validation-10352", "mrqa_squad-validation-10370", "mrqa_squad-validation-10386", "mrqa_squad-validation-10399", "mrqa_squad-validation-1042", "mrqa_squad-validation-10457", "mrqa_squad-validation-10463", "mrqa_squad-validation-10470", "mrqa_squad-validation-10475", "mrqa_squad-validation-10484", "mrqa_squad-validation-1078", "mrqa_squad-validation-1177", "mrqa_squad-validation-12", "mrqa_squad-validation-1207", "mrqa_squad-validation-1219", "mrqa_squad-validation-1254", "mrqa_squad-validation-1263", "mrqa_squad-validation-133", "mrqa_squad-validation-1330", "mrqa_squad-validation-134", "mrqa_squad-validation-1402", "mrqa_squad-validation-1432", "mrqa_squad-validation-1498", "mrqa_squad-validation-150", "mrqa_squad-validation-151", "mrqa_squad-validation-1565", "mrqa_squad-validation-1612", "mrqa_squad-validation-1640", "mrqa_squad-validation-168", "mrqa_squad-validation-1764", "mrqa_squad-validation-1813", "mrqa_squad-validation-185", "mrqa_squad-validation-185", "mrqa_squad-validation-1891", "mrqa_squad-validation-1980", "mrqa_squad-validation-2018", "mrqa_squad-validation-2085", "mrqa_squad-validation-2085", "mrqa_squad-validation-2109", "mrqa_squad-validation-215", "mrqa_squad-validation-2167", "mrqa_squad-validation-2214", "mrqa_squad-validation-2314", "mrqa_squad-validation-2370", "mrqa_squad-validation-246", "mrqa_squad-validation-2500", "mrqa_squad-validation-2559", "mrqa_squad-validation-2586", "mrqa_squad-validation-2666", "mrqa_squad-validation-2672", "mrqa_squad-validation-269", "mrqa_squad-validation-270", "mrqa_squad-validation-2704", "mrqa_squad-validation-2754", "mrqa_squad-validation-2853", "mrqa_squad-validation-2913", "mrqa_squad-validation-2939", "mrqa_squad-validation-2964", "mrqa_squad-validation-2975", "mrqa_squad-validation-3096", "mrqa_squad-validation-3124", "mrqa_squad-validation-3125", "mrqa_squad-validation-313", "mrqa_squad-validation-3140", "mrqa_squad-validation-3143", "mrqa_squad-validation-3275", "mrqa_squad-validation-3319", "mrqa_squad-validation-336", "mrqa_squad-validation-3370", "mrqa_squad-validation-3407", "mrqa_squad-validation-3492", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-350", "mrqa_squad-validation-3535", "mrqa_squad-validation-3550", "mrqa_squad-validation-36", "mrqa_squad-validation-3667", "mrqa_squad-validation-3740", "mrqa_squad-validation-375", "mrqa_squad-validation-3750", "mrqa_squad-validation-3770", "mrqa_squad-validation-3774", "mrqa_squad-validation-3802", "mrqa_squad-validation-3811", "mrqa_squad-validation-3813", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3904", "mrqa_squad-validation-3937", "mrqa_squad-validation-3939", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-4227", "mrqa_squad-validation-4246", "mrqa_squad-validation-4469", "mrqa_squad-validation-4473", "mrqa_squad-validation-4546", "mrqa_squad-validation-4591", "mrqa_squad-validation-4636", "mrqa_squad-validation-4669", "mrqa_squad-validation-468", "mrqa_squad-validation-4737", "mrqa_squad-validation-4754", "mrqa_squad-validation-4799", "mrqa_squad-validation-4860", "mrqa_squad-validation-4877", "mrqa_squad-validation-4898", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5007", "mrqa_squad-validation-5019", "mrqa_squad-validation-5135", "mrqa_squad-validation-5294", "mrqa_squad-validation-5325", "mrqa_squad-validation-5351", "mrqa_squad-validation-5613", "mrqa_squad-validation-5699", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5716", "mrqa_squad-validation-5721", "mrqa_squad-validation-5724", "mrqa_squad-validation-5797", "mrqa_squad-validation-5875", "mrqa_squad-validation-5933", "mrqa_squad-validation-5961", "mrqa_squad-validation-6025", "mrqa_squad-validation-6089", "mrqa_squad-validation-6116", "mrqa_squad-validation-6157", "mrqa_squad-validation-6159", "mrqa_squad-validation-6163", "mrqa_squad-validation-6341", "mrqa_squad-validation-635", "mrqa_squad-validation-6502", "mrqa_squad-validation-6526", "mrqa_squad-validation-6579", "mrqa_squad-validation-6614", "mrqa_squad-validation-6628", "mrqa_squad-validation-6669", "mrqa_squad-validation-6676", "mrqa_squad-validation-6700", "mrqa_squad-validation-6705", "mrqa_squad-validation-6803", "mrqa_squad-validation-6873", "mrqa_squad-validation-6986", "mrqa_squad-validation-7036", "mrqa_squad-validation-7168", "mrqa_squad-validation-719", "mrqa_squad-validation-7233", "mrqa_squad-validation-7246", "mrqa_squad-validation-7277", "mrqa_squad-validation-7294", "mrqa_squad-validation-735", "mrqa_squad-validation-7354", "mrqa_squad-validation-7367", "mrqa_squad-validation-7384", "mrqa_squad-validation-7428", "mrqa_squad-validation-7456", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-7531", "mrqa_squad-validation-7570", "mrqa_squad-validation-7585", "mrqa_squad-validation-76", "mrqa_squad-validation-7652", "mrqa_squad-validation-7671", "mrqa_squad-validation-7689", "mrqa_squad-validation-7707", "mrqa_squad-validation-7782", "mrqa_squad-validation-7788", "mrqa_squad-validation-780", "mrqa_squad-validation-7805", "mrqa_squad-validation-7838", "mrqa_squad-validation-7909", "mrqa_squad-validation-791", "mrqa_squad-validation-8028", "mrqa_squad-validation-8043", "mrqa_squad-validation-8045", "mrqa_squad-validation-8073", "mrqa_squad-validation-8283", "mrqa_squad-validation-8386", "mrqa_squad-validation-8500", "mrqa_squad-validation-8502", "mrqa_squad-validation-8529", "mrqa_squad-validation-8555", "mrqa_squad-validation-8603", "mrqa_squad-validation-8680", "mrqa_squad-validation-8754", "mrqa_squad-validation-8830", "mrqa_squad-validation-8834", "mrqa_squad-validation-8900", "mrqa_squad-validation-8901", "mrqa_squad-validation-8905", "mrqa_squad-validation-891", "mrqa_squad-validation-8927", "mrqa_squad-validation-8931", "mrqa_squad-validation-8939", "mrqa_squad-validation-8948", "mrqa_squad-validation-8966", "mrqa_squad-validation-8987", "mrqa_squad-validation-9200", "mrqa_squad-validation-9254", "mrqa_squad-validation-9269", "mrqa_squad-validation-9271", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9302", "mrqa_squad-validation-9322", "mrqa_squad-validation-9328", "mrqa_squad-validation-9436", "mrqa_squad-validation-95", "mrqa_squad-validation-9531", "mrqa_squad-validation-9532", "mrqa_squad-validation-9543", "mrqa_squad-validation-9695", "mrqa_squad-validation-9768", "mrqa_squad-validation-9819", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_squad-validation-9931", "mrqa_squad-validation-9989", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1246", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-1670", "mrqa_triviaqa-validation-1800", "mrqa_triviaqa-validation-1873", "mrqa_triviaqa-validation-1976", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-247", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2937", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3278", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-3955", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-4137", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4850", "mrqa_triviaqa-validation-486", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5025", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-5638", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-581", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-5960", "mrqa_triviaqa-validation-5965", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6246", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6734", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-7124", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-828"], "OKR": 0.892578125, "KG": 0.48203125, "before_eval_results": {"predictions": ["2002", "1,548", "American Institute of Electrical Engineers", "a deficit", "18 and 19", "Virgin Media", "in a number of stages", "light energy", "128", "voting directly or elect representatives from among themselves to form a governing body, such as a parliament.", "Mary Ellen Mark", "Australian Defence Force", "naval aviator, test pilot, and businessman", "Algirdas", "1970", "Vilnius Old Town", "Rounders", "music of pre-Hispanic and contemporary music of the Andes", "Robert John Day", "Rio Gavin Ferdinand", "National Lottery", "Battle of Prome", "M2M", "Czech Republic", "Citizens for a Sound Economy", "right-hand", "Gatwick", "Australian", "Darkroom", "House of Commons", "La Familia Michoacana", "five", "1983", "James Douglas Packer", "Ireland", "Erich Maria Remarque", "Best Musical", "Floyd Mutrux and Colin Escott", "Fred &quot;Sonic&quot", "the Mikoyan design bureau", "American Indian", "A Little Princess", "1943", "TD Garden", "the interior of Titania named after \"Bertram, count of Rousillon\"", "Vixen", "5", "Ry\u016bky\u016b minzoku", "Chicago", "The General Electric/Allison J35", "Axl Rose", "Prudential Center", "Homer Hickam, Jr.", "CBS All Access", "Diary of a Wimpy Kid : The Long Haul", "vision of colours,", "July 28, 1948.", "Department of Homeland Security Secretary Janet Napolitano", "\"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "Beatles", "a key", "Operation Cast Lead", "future relations with Washington", "650"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6487511837121213}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, false, false, false, false, true, false, true, false, true, true, true, false, false, false, false, true, false, true, true, true, false, true, false, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.09090909090909091, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.6666666666666666, 0.5, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0625, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4010", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-5692", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1812", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-2488", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-2486", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-2430", "mrqa_naturalquestions-validation-6298", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-3423", "mrqa_newsqa-validation-3865", "mrqa_searchqa-validation-9122", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-571"], "SR": 0.53125, "CSR": 0.5989583333333333, "retrieved_ids": ["mrqa_squad-train-66947", "mrqa_squad-train-64720", "mrqa_squad-train-15442", "mrqa_squad-train-82228", "mrqa_squad-train-44899", "mrqa_squad-train-60615", "mrqa_squad-train-59666", "mrqa_squad-train-1117", "mrqa_squad-train-57069", "mrqa_squad-train-74183", "mrqa_squad-train-11542", "mrqa_squad-train-18003", "mrqa_squad-train-13667", "mrqa_squad-train-48674", "mrqa_squad-train-17179", "mrqa_squad-train-82144", "mrqa_naturalquestions-validation-6665", "mrqa_squad-validation-133", "mrqa_hotpotqa-validation-3343", "mrqa_squad-validation-8386", "mrqa_squad-validation-150", "mrqa_triviaqa-validation-1643", "mrqa_naturalquestions-validation-9142", "mrqa_naturalquestions-validation-9368", "mrqa_newsqa-validation-3501", "mrqa_naturalquestions-validation-1160", "mrqa_newsqa-validation-3164", "mrqa_searchqa-validation-4432", "mrqa_naturalquestions-validation-5370", "mrqa_hotpotqa-validation-5591", "mrqa_naturalquestions-validation-6483", "mrqa_squad-validation-2853"], "EFR": 1.0, "Overall": 0.7552604166666665}, {"timecode": 21, "before_eval_results": {"predictions": ["1622", "wars", "all age groups", "cabin depressurization", "one of the largest gold rushes the world has ever seen", "1985", "whether the organelle carries out the last leg of the pathway or if it happens in the cytosol", "Sultan Selim II", "Anthony Bellew", "A Hard Day's Night", "Lord Chancellor of England", "Ben Elton", "West Cheshire Association Football League", "KB", "1,467", "a pioneering New Zealand food writer", "Old Executive Office Building", "Bad Reputation", "John R. Leonetti", "The Legend of Sleepy Hollow", "25 August 1949", "Harmony Korine", "137th", "Texas Tech University", "Pisgah National Forest (6,214 ft)", "703", "King George IV and the Duke of Wellington", "Rochdale", "Tudor music and English folk-song", "Northern Ireland", "Pylos and Thebes", "Larry Gatlin & the Gatlin Brothers Band", "Beatles", "Lady Frederick Windsor", "Monica Seles", "2 November 1902", "best time in space (381.6 days)", "the NYPD's 83rd Precinct", "Patterns of Sexual Behavior", "Peel Holdings", "about 560", "wrestler, actor, and hip hop musician", "Theodor W. Adorno", "Gianna", "City of Newcastle", "James Edward Kelly", "gender queer", "Kiss", "Rick Scott", "Minette Walters", "2004", "Tamil", "67,038", "73", "Barbara Windsor", "Stephenie Meyer", "Jane Austen", "Pakistan's", "Michelle Rounds", "A Doll's House", "Florida", "horse", "auction houses with watch departments and specialist watch-only auctioneers", "Mickey Mouse"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6383941579254079}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.75, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.923076923076923, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2987", "mrqa_hotpotqa-validation-1312", "mrqa_hotpotqa-validation-2016", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-5880", "mrqa_hotpotqa-validation-3232", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-3339", "mrqa_hotpotqa-validation-334", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-984", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-5295", "mrqa_hotpotqa-validation-693", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-2217", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-3707", "mrqa_hotpotqa-validation-5052", "mrqa_naturalquestions-validation-2159", "mrqa_triviaqa-validation-6256", "mrqa_triviaqa-validation-2227", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-7724"], "SR": 0.546875, "CSR": 0.5965909090909092, "retrieved_ids": ["mrqa_squad-train-75402", "mrqa_squad-train-85679", "mrqa_squad-train-2448", "mrqa_squad-train-83652", "mrqa_squad-train-7348", "mrqa_squad-train-76303", "mrqa_squad-train-51989", "mrqa_squad-train-67458", "mrqa_squad-train-83054", "mrqa_squad-train-59331", "mrqa_squad-train-58560", "mrqa_squad-train-80798", "mrqa_squad-train-76488", "mrqa_squad-train-58553", "mrqa_squad-train-45700", "mrqa_squad-train-63938", "mrqa_squad-validation-186", "mrqa_squad-validation-4877", "mrqa_naturalquestions-validation-661", "mrqa_squad-validation-361", "mrqa_hotpotqa-validation-3387", "mrqa_newsqa-validation-4030", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-7036", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-3727", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-14767", "mrqa_triviaqa-validation-1160", "mrqa_newsqa-validation-1887", "mrqa_hotpotqa-validation-2503"], "EFR": 1.0, "Overall": 0.7547869318181818}, {"timecode": 22, "before_eval_results": {"predictions": ["$960 billion", "European Court of Human Rights", "kidney and bladder stones, and arthritis, and an ear infection ruptured an ear drum", "NP", "Zwickau prophets", "1948", "New Orleans", "The Corleones", "david perdue's Charles Dickens Page - Mrs Gamp", "cowpox", "1984", "ravens", "Eros", "insulin", "bullfight", "17 pink \"double-word\" squares", "12", "The Pennine Way", "Muriel Spark", "basil", "La Mancha", "Martin Van Buren", "Bonnie and Clyde", "Tears for Fears combines weighty lyrics of self-exploration with a compelling and sensual pop sound.", "Hillary Clinton", "Gettysburg", "Tom Hanks", "six dots", "sound and light", "Panama", "mushrooms", "Harrods", "Usain Bolt", "Mead", "To Kill a Mockingbird", "Solo", "Sudan", "Hyperbole", "Russia", "Doctor Doom", "Steve Jobs", "moorish", "Rajasthan", "in love with a young man called Jenik", "50 Outdoor Games", "David Hockney", "a compact bone that sits between the calcaneus (heel bone) and the tibia and fibula (bones of the lower leg)", "Barnaby Rudge", "Surficial", "1861", "Thomas Jefferson", "Ceredigion", "10 : 30am", "2014", "Seattle, Washington", "Capture of the Five Boroughs", "79 AD", "Wilmette", "Dodi Fayed", "last week", "Robert Park", "Sounder Ri", "Syracuse", "4"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6091006216006216}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, false, true, false, true, true, false, false, true, false, true, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2505", "mrqa_triviaqa-validation-3577", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-2366", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-1286", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-3065", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-6681", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-5116", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-6130", "mrqa_triviaqa-validation-6659", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-6192", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-6458", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-7443", "mrqa_hotpotqa-validation-1874", "mrqa_newsqa-validation-2959", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-14932", "mrqa_searchqa-validation-2394"], "SR": 0.53125, "CSR": 0.59375, "retrieved_ids": ["mrqa_squad-train-2826", "mrqa_squad-train-78605", "mrqa_squad-train-55232", "mrqa_squad-train-39579", "mrqa_squad-train-32349", "mrqa_squad-train-76474", "mrqa_squad-train-36982", "mrqa_squad-train-36285", "mrqa_squad-train-60663", "mrqa_squad-train-19447", "mrqa_squad-train-54553", "mrqa_squad-train-65280", "mrqa_squad-train-70904", "mrqa_squad-train-74982", "mrqa_squad-train-8507", "mrqa_squad-train-76485", "mrqa_hotpotqa-validation-1028", "mrqa_triviaqa-validation-5714", "mrqa_naturalquestions-validation-4556", "mrqa_hotpotqa-validation-1473", "mrqa_naturalquestions-validation-2562", "mrqa_newsqa-validation-3084", "mrqa_naturalquestions-validation-10680", "mrqa_newsqa-validation-2467", "mrqa_squad-validation-1891", "mrqa_triviaqa-validation-3727", "mrqa_searchqa-validation-9828", "mrqa_newsqa-validation-2791", "mrqa_searchqa-validation-14767", "mrqa_triviaqa-validation-1156", "mrqa_newsqa-validation-3444", "mrqa_hotpotqa-validation-1989"], "EFR": 0.9666666666666667, "Overall": 0.7475520833333333}, {"timecode": 23, "before_eval_results": {"predictions": ["1759-60", "tyrosinase", "Keraites", "Israelis", "Konstantin Mereschkowski", "disease", "gounod and Reyer", "Vienna", "March 10, 1997", "m\u00e4rs\u014d\u00b4p\u0113\u0259l", "meat", "spain", "police detective drama", "1985", "whitsun", "John Steinbeck", "drew Carey", "beta", "pakistan", "John Peel", "an Italian luthier", "Cheshire", "Rebecca Adlington", "silks", "a redesigned power station", "Notts County", "Tigris", "John Fitzgerald Kennedy", "spainning the Palazzo Rio, or Palace River,", "cows", "steel", "Dominican Republic", "Hydra", "whitsun", "Frank Harris", "Charles Atlas", "han pecks", "harrow", "Isle of Wight", "violin", "Roberto Cammarelle", "elephant", "whitsun", "Dick Turpin", "cynthia", "restless leg syndrome", "spleen", "Olympics", "Jimmy Knapp", "pennado Tuerto, Argentina", "gargantua", "spanned a longer period of time", "Pebe Sebert and Hugh Moffatt", "October 29, 2015", "in the season - five premiere episode `` Second Opinion ''", "Leslie Knope", "First Balkan War", "Get Him to the Greek", "next year", "heavy turbulence", "\"These guys need to take a look around and see that we're facing 10 percent unemployment and an economy on the brink of collapse,\"", "Patrick Bouvier Kennedy", "Schumer", "the mouth"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5670272435897435}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.3333333333333333, 0.923076923076923, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8488", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-3634", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-7057", "mrqa_triviaqa-validation-821", "mrqa_triviaqa-validation-3350", "mrqa_triviaqa-validation-7404", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-7333", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-6876", "mrqa_naturalquestions-validation-3440", "mrqa_naturalquestions-validation-2818", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-5271", "mrqa_newsqa-validation-1893", "mrqa_searchqa-validation-7791", "mrqa_searchqa-validation-14752", "mrqa_searchqa-validation-7996"], "SR": 0.484375, "CSR": 0.5891927083333333, "retrieved_ids": ["mrqa_squad-train-36955", "mrqa_squad-train-14328", "mrqa_squad-train-61911", "mrqa_squad-train-28897", "mrqa_squad-train-41839", "mrqa_squad-train-528", "mrqa_squad-train-58654", "mrqa_squad-train-943", "mrqa_squad-train-79751", "mrqa_squad-train-80629", "mrqa_squad-train-84373", "mrqa_squad-train-56448", "mrqa_squad-train-61367", "mrqa_squad-train-32423", "mrqa_squad-train-38946", "mrqa_squad-train-35372", "mrqa_squad-validation-8037", "mrqa_squad-validation-1402", "mrqa_squad-validation-2085", "mrqa_triviaqa-validation-5474", "mrqa_squad-validation-8399", "mrqa_squad-validation-10386", "mrqa_squad-validation-7713", "mrqa_naturalquestions-validation-222", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-1312", "mrqa_newsqa-validation-3501", "mrqa_naturalquestions-validation-9135", "mrqa_hotpotqa-validation-4969", "mrqa_newsqa-validation-3444", "mrqa_naturalquestions-validation-8046", "mrqa_triviaqa-validation-4663"], "EFR": 1.0, "Overall": 0.7533072916666665}, {"timecode": 24, "before_eval_results": {"predictions": ["a comb jelly", "cicadas", "gaseous oxygen", "There is no known case of any U.S. citizens buying Canadian drugs for personal use with a prescription", "9th century", "Teha'amana", "Gerald R. Ford", "peasant Bruegel", "Lautrec", "I Don't Want to Miss a Thing", "Sunday, November 6", "beautiful", "saddle oxfords", "NAFTA", "Axis", "Mrs. Clarissa Dalloway", "the Agreement on Ending the War and Restoring Peace in Vietnam", "venial sin", "Toronto", "ancroiait", "the beaver", "benedict", "menudo-mdo", "Alfred Nobel", "Smith & Wesson", "550 nm", "Emma Watson", "Pan Am", "Cardinal Richelieu", "Robert E. Lee", "mansard", "OK", "Nixon", "koolsla", "July 17, 1889", "diamonds", "Sonny", "morphine", "Antarctica", "Ice Age", "hand", "lyndon johnson", "lorin Farr", "An American Tail", "pelican", "Private First Class", "Mount Kenya", "habsburg", "canuck", "bathsheba", "valley", "Johnny Depp", "Wembley Stadium", "Mike Higham", "MGM Resorts International", "Olympic Games", "bat", "halogen", "Ferdinand Magellan", "January 24, 2012", "\"Famous Ghost Stories\"", "a man who said he had found it in the desert five months before.", "haitians", "Leaders of more than 30 Latin American and Caribbean nations"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4778053977272727}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, false, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.45454545454545453, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.125]}}, "before_error_ids": ["mrqa_squad-validation-6383", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-5385", "mrqa_searchqa-validation-1375", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-16821", "mrqa_searchqa-validation-12291", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-11213", "mrqa_searchqa-validation-11414", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-1004", "mrqa_searchqa-validation-16836", "mrqa_searchqa-validation-14524", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-7094", "mrqa_searchqa-validation-9261", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-9698", "mrqa_searchqa-validation-8944", "mrqa_searchqa-validation-263", "mrqa_searchqa-validation-3135", "mrqa_searchqa-validation-16246", "mrqa_searchqa-validation-10911", "mrqa_naturalquestions-validation-9773", "mrqa_triviaqa-validation-2047", "mrqa_hotpotqa-validation-4468", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-2225"], "SR": 0.421875, "CSR": 0.5825, "retrieved_ids": ["mrqa_squad-train-1202", "mrqa_squad-train-51362", "mrqa_squad-train-74829", "mrqa_squad-train-39958", "mrqa_squad-train-38614", "mrqa_squad-train-70619", "mrqa_squad-train-29434", "mrqa_squad-train-46117", "mrqa_squad-train-70344", "mrqa_squad-train-8070", "mrqa_squad-train-9404", "mrqa_squad-train-43247", "mrqa_squad-train-78657", "mrqa_squad-train-32581", "mrqa_squad-train-27577", "mrqa_squad-train-46102", "mrqa_hotpotqa-validation-4565", "mrqa_squad-validation-6717", "mrqa_searchqa-validation-9122", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1173", "mrqa_squad-validation-6837", "mrqa_squad-validation-1187", "mrqa_searchqa-validation-10395", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-5489", "mrqa_squad-validation-10174", "mrqa_newsqa-validation-3444", "mrqa_hotpotqa-validation-984", "mrqa_triviaqa-validation-2372", "mrqa_hotpotqa-validation-391", "mrqa_searchqa-validation-6670"], "EFR": 1.0, "Overall": 0.75196875}, {"timecode": 25, "before_eval_results": {"predictions": ["Khitan rulers", "over 100%", "vaccination", "natural grass stadiums", "three", "President Thabo Mbeki", "full health-care coverage", "a lump in Henry's nether regions was a cancerous tumor.", "a bag", "photos", "gun charges", "the area where the single-engine Cessna 206 went down", "she sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "\"falling space debris,\"", "the man was dead", "three different videos", "fifth season September 21.", "stand down.", "hot and humid", "KBR", "Mother Nature has proven to be a challenge.", "\"Twilight\"", "the firm's \"private client\" list", "Venus Williams", "Booches Billiard Hall", "Peshawar", "23-year-old", "three", "creating and distributing affordable, durable and solar-powered laptops to the world's poorest children.", "\"There is no exclusively military solution to the issues we and our partners confront in Afghanistan.", "Sovereign Wealth Funds", "Immigration Minister Eric Besson", "American Legion National Commander David Rehbein", "Inmates", "a kidney that was a biological match.", "death squad killings", "hours", "over 1000 square meters", "Cal Ripken Jr.", "Hillary Marcus", "Caylee", "iTunes", "\"Hell\"", "45 minutes, five days a week.", "to secure more funds from the region.", "an upper respiratory infection", "\"It hurts my heart to see him in pain,", "Brown-Waite", "Shenzhen", "he knew the owner of the home, a Vietnam veteran who had given him permission to enter the house and take painkillers or other pills whenever he wanted.", "the Italian Serie A title", "Zimbabwe's main opposition party said Sunday.", "the year 2026", "Mitch Murray", "The Ranch is an American comedy web television series starring Ashton Kutcher, Danny Masterson, Debra Winger, Elisha Cuthbert, and Sam Elliott", "Captain Mark Phillips", "Asuka Watts", "Bruce Alexander", "President of the United States", "Pittsburgh Steelers", "Edward Montgomery \"Monty\" Clift", "The Man Trap", "Chris Matthews", "books"], "metric_results": {"EM": 0.4375, "QA-F1": 0.576804424255082}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, true, true, false, false, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 0.2, 0.0, 0.4, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 0.13333333333333333, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.14814814814814814, 0.09523809523809522, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.42857142857142855, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.5714285714285715, 0.8571428571428571, 1.0, 0.4736842105263158, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.05555555555555555, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-448", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-1137", "mrqa_naturalquestions-validation-7896", "mrqa_triviaqa-validation-1508", "mrqa_triviaqa-validation-4087", "mrqa_hotpotqa-validation-3375", "mrqa_searchqa-validation-16581"], "SR": 0.4375, "CSR": 0.5769230769230769, "retrieved_ids": ["mrqa_squad-train-1003", "mrqa_squad-train-53644", "mrqa_squad-train-17841", "mrqa_squad-train-83246", "mrqa_squad-train-12149", "mrqa_squad-train-11196", "mrqa_squad-train-85229", "mrqa_squad-train-85327", "mrqa_squad-train-31399", "mrqa_squad-train-862", "mrqa_squad-train-63935", "mrqa_squad-train-85958", "mrqa_squad-train-74772", "mrqa_squad-train-63297", "mrqa_squad-train-66200", "mrqa_squad-train-2663", "mrqa_naturalquestions-validation-849", "mrqa_hotpotqa-validation-4289", "mrqa_searchqa-validation-2329", "mrqa_triviaqa-validation-2437", "mrqa_naturalquestions-validation-2250", "mrqa_newsqa-validation-3621", "mrqa_hotpotqa-validation-5319", "mrqa_squad-validation-6814", "mrqa_hotpotqa-validation-5052", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-4365", "mrqa_searchqa-validation-15877", "mrqa_naturalquestions-validation-1864", "mrqa_squad-validation-3173", "mrqa_squad-validation-6526", "mrqa_naturalquestions-validation-1382"], "EFR": 1.0, "Overall": 0.7508533653846153}, {"timecode": 26, "before_eval_results": {"predictions": ["David G. Booth", "pr\u00e9tendus r\u00e9form\u00e9s", "true Islamic system", "$20,000 ($472,500 in today's dollars)", "intention to set up headquarters in Dublin.", "ordered the makers of certain antibiotics to add a \"black box\" label warning", "Mississippi school district and high school", "baby daughter Jada,", "Elisabeth", "Tuesday", "it is provocative action,\"", "John Kiriakou.", "Ricardo Urbina", "a full dozen armed policemen squeals to a halt.", "Christopher Savoie", "near Garacad, Somalia", "allergies in general -- both food and inhalant -- are on the rise,", "pilots with mild to moderate depression will be allowed to fly while taking antidepressants", "How I Met Your Mother", "200 human bodies at various life stages -- from conception to old age,", "teenager", "inmates", "as many as 50,000 members", "U.S. State Department and British Foreign Office", "the two remaining crew members from the helicopter,", "at a Little Rock military recruiting center was angry over the treatment of Muslims,", "up to $50,000", "Arthur E. Morgan III", "Caster Semenya", "Ewan McGregor", "baseball bat", "President Obama and his wife, Michelle,", "$60 million", "Iran test-launched a rocket capable of carrying a satellite,", "three", "Gov. Mark Sanford", "gun", "$1.5 million", "off the coast", "Davidson college students", "Diego Milito", "an upper respiratory infection", "the Transportation Security Administration", "McDonald's", "collapsed apartment building", "gang rape", "leftist Workers' Party.", "protest child trafficking and shout anti-French slogans Wednesday in Abeche, Chad.", "full garden and pool, a tennis court,", "October 3,", "The Impeccable", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "the Imperial Family", "1998", "November 2016", "sewing machines", "exploits on the Island", "Duncan", "estimated half a million acres", "hiphop", "four sections", "Stanford", "a rough, rugged rock", "Lawrencium"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5520798849842967}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, true, false, false, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.33333333333333337, 0.9333333333333333, 0.07142857142857142, 0.2857142857142857, 0.0, 0.0, 0.5, 0.0, 0.09090909090909091, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.5882352941176471, 0.0, 1.0, 0.33333333333333337, 1.0, 0.8, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 0.5, 0.13333333333333333, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1489", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-4182", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-9119", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-7585", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-550", "mrqa_searchqa-validation-679"], "SR": 0.390625, "CSR": 0.5700231481481481, "retrieved_ids": ["mrqa_squad-train-51868", "mrqa_squad-train-9811", "mrqa_squad-train-72252", "mrqa_squad-train-43778", "mrqa_squad-train-69853", "mrqa_squad-train-73927", "mrqa_squad-train-17507", "mrqa_squad-train-83678", "mrqa_squad-train-79637", "mrqa_squad-train-84597", "mrqa_squad-train-60316", "mrqa_squad-train-13461", "mrqa_squad-train-77458", "mrqa_squad-train-50801", "mrqa_squad-train-25489", "mrqa_squad-train-4620", "mrqa_searchqa-validation-8172", "mrqa_hotpotqa-validation-3039", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-3865", "mrqa_triviaqa-validation-1590", "mrqa_newsqa-validation-463", "mrqa_triviaqa-validation-1461", "mrqa_newsqa-validation-961", "mrqa_newsqa-validation-574", "mrqa_squad-validation-3676", "mrqa_squad-validation-1042", "mrqa_squad-validation-375", "mrqa_triviaqa-validation-6659", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-2422", "mrqa_naturalquestions-validation-10273"], "EFR": 1.0, "Overall": 0.7494733796296296}, {"timecode": 27, "before_eval_results": {"predictions": ["computability theory", "economically", "Labor", "Children of Earth", "the triangular bone within the pelvis", "Crap E-mail", "133", "one count of attempted murder in the second degree in the October 12 attack in Deerfield Beach, Florida.", "Sri Lanka", "Muslim", "a man walked through an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening,", "200", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "innovative, exciting skyscrapers", "pro-democracy activists clashed Friday with Egyptian security forces", "1831", "Glasgow, Scotland", "10 to 15 percent", "Kearny, New Jersey", "19-12", "four university students and a safety officer", "a number of calls", "Mark Obama Ndesandjo", "New York City Mayor Michael Bloomberg", "The Delta Queen", "At least 15", "opposition parties", "July 23.", "for a full facial transplant since 2004.", "citizenship", "\"The most affecting thing about this whole wheelchair for children is when the parents realize the gift that is being given to their children and they reach out to hug you.\"", "Gustav's top winds weakened to 110 mph", "the 11th anniversary of the September 11, 2001, terror attacks.", "a curfew", "Omar bin Laden's sons", "Tim Masters", "anesthetic propofol", "Peshawar", "Pixar", "The Da Vinci Code", "martial arts", "Bahrami", "producing rock music with a country influence.", "Ace Basaran and a group of her friends started up a Turkish-American friendship club at her university.\"", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "$8.8 million", "near his home in Peshawar", "the refusal or inability to \"turn it off\"", "fighting charges of Nazi war crimes for well over two decades.", "Minerals Management Service Director Elizabeth Birnbaum", "$420,000", "U.S.-flagged", "`` planted '' into the bracket in a manner that is typically intended so that the best don't meet until later in the competition", "1994", "disputes between two or more states", "shorthand", "Carmen Miranda", "Lesley Garrett", "Denmark", "the tissues of the outer third of the vagina", "Mark Neveldine and Brian Taylor", "Ulysses S. Grant", "Kevin Bacon", "Yellow"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5481639394782777}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false], "QA-F1": [0.4, 1.0, 1.0, 1.0, 0.25, 0.0, 0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.4444444444444445, 0.3076923076923077, 1.0, 0.5714285714285715, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.07407407407407407, 0.0, 1.0, 1.0, 1.0, 0.7058823529411764, 0.5, 0.0, 0.0, 0.0392156862745098, 0.5, 0.8, 1.0, 1.0, 1.0, 0.5, 0.1818181818181818, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1704", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2215", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-1028", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-8092", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-4194", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420"], "SR": 0.40625, "CSR": 0.5641741071428572, "retrieved_ids": ["mrqa_squad-train-49807", "mrqa_squad-train-85183", "mrqa_squad-train-74899", "mrqa_squad-train-23458", "mrqa_squad-train-59142", "mrqa_squad-train-60483", "mrqa_squad-train-50415", "mrqa_squad-train-2240", "mrqa_squad-train-72702", "mrqa_squad-train-43094", "mrqa_squad-train-1534", "mrqa_squad-train-55415", "mrqa_squad-train-6659", "mrqa_squad-train-4315", "mrqa_squad-train-20299", "mrqa_squad-train-24032", "mrqa_searchqa-validation-6338", "mrqa_hotpotqa-validation-2574", "mrqa_naturalquestions-validation-10680", "mrqa_triviaqa-validation-6175", "mrqa_naturalquestions-validation-2818", "mrqa_naturalquestions-validation-1407", "mrqa_squad-validation-2939", "mrqa_searchqa-validation-8188", "mrqa_triviaqa-validation-6130", "mrqa_hotpotqa-validation-4080", "mrqa_triviaqa-validation-2889", "mrqa_triviaqa-validation-7511", "mrqa_naturalquestions-validation-7443", "mrqa_triviaqa-validation-4382", "mrqa_newsqa-validation-2730", "mrqa_squad-validation-6837"], "EFR": 0.9736842105263158, "Overall": 0.7430404135338347}, {"timecode": 28, "before_eval_results": {"predictions": ["Ex post facto laws,", "temperature and light", "on comparing these particles with the bits of metal projected by his \"electric gun,\"", "He won it with a clear strategy that was stuck to with remarkably little internal drama.", "Hillary Clinton", "volatile", "older generation", "Ken Plunkett,", "800,000 people", "Monday and Tuesday", "a full garden and pool, a tennis court, or several heli-pads.", "surrogate", "all civilians and laying down arms,\"", "two-hour finale.", "\"The biology professor is charged with capital murder and three counts of attempted murder in the shootings at the University of Alabama in Huntsville last week.", "prisoners", "Ashley \"A.J.\" Jewell", "environmental and political events", "July", "Iran of trying to build nuclear bombs,", "The U.S. State Department and British Foreign Office", "former U.S. secretary of state.", "misdemeanor assault charges", "The Tupolev Tu-160 strategic bombers", "Argentine", "Jezebel.com's Crap E-mail", "Piers Morgan Tonight", "$50,000", "Leo Frank", "Omar bin Laden", "some of the most gigantic pumpkins in the world,", "U.S. President-elect Barack Obama", "former World Trade Center's \"archaeological heart,\"", "children's hospital in St. Louis, Missouri.", "Marc by Marc Jacobs and Rag & Bone.", "free drug treatment.", "Pope Benedict XVI", "The National Telecommunications and Information Administration offered a program to help people buy converter boxes that make old TVs work in the new era.", "three searches", "Basel", "more than 700 guests each year", "first grand Slam,", "suppress the memories and to live as normal a life as possible; the culture of his time said that he should get on with his life,\"", "$530 million", "Jennifer Arnold and husband Bill Klein", "murder", "role as a bride in the 2007 movie \"License to Wed\"", "no reason", "2009", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "David Mohler and Jared Mohler", "Robert Barnett", "Yuzuru Hanyu", "7.6 mm", "Scopes Trial in the United States", "Patrick Chukwuemeka Okogwu", "Petula Clark", "Lord Marmaduke", "Roman Kostomarov", "August 1973", "ten episodes", "\"L'Etranger\"", "Los Angeles Times", "George Orwell"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5628069175691458}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 0.08333333333333334, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5384615384615384, 0.4, 0.8, 0.5333333333333333, 0.4444444444444445, 0.0, 0.0, 0.08695652173913042, 0.0, 1.0, 0.2857142857142857, 0.8571428571428571, 0.08, 0.0, 0.923076923076923, 1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4096", "mrqa_squad-validation-1389", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2291", "mrqa_newsqa-validation-636", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-1765", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-2214", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1663", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2030", "mrqa_naturalquestions-validation-6832", "mrqa_triviaqa-validation-6701", "mrqa_hotpotqa-validation-1824", "mrqa_searchqa-validation-1869"], "SR": 0.421875, "CSR": 0.5592672413793103, "retrieved_ids": ["mrqa_squad-train-66121", "mrqa_squad-train-76252", "mrqa_squad-train-73665", "mrqa_squad-train-52542", "mrqa_squad-train-8730", "mrqa_squad-train-31490", "mrqa_squad-train-81289", "mrqa_squad-train-23160", "mrqa_squad-train-79671", "mrqa_squad-train-12210", "mrqa_squad-train-10970", "mrqa_squad-train-32201", "mrqa_squad-train-46339", "mrqa_squad-train-6838", "mrqa_squad-train-85216", "mrqa_squad-train-40981", "mrqa_triviaqa-validation-6083", "mrqa_hotpotqa-validation-2244", "mrqa_naturalquestions-validation-5378", "mrqa_triviaqa-validation-4646", "mrqa_naturalquestions-validation-10546", "mrqa_hotpotqa-validation-5180", "mrqa_newsqa-validation-1181", "mrqa_hotpotqa-validation-5055", "mrqa_hotpotqa-validation-2217", "mrqa_searchqa-validation-3135", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-2287", "mrqa_searchqa-validation-2394", "mrqa_newsqa-validation-571", "mrqa_newsqa-validation-3731"], "EFR": 0.972972972972973, "Overall": 0.7419167928704566}, {"timecode": 29, "before_eval_results": {"predictions": ["fully consistent with the conceptual definition of force offered by Newtonian mechanics", "peer tuitions", "six", "Located", "Tennyson", "1028", "the Golden Hind", "Bennington", "Candice", "aluminum", "a crushed rock or gravel,", "Moscow", "a garter and milk snakes", "South America", "Idi Amin", "Whitney Houston's cover of Dolly Parton's \"I Will Always Love", "pink", "Tom", "anemia", "Frank Sinatra", "fishes", "Robert", "Vienna", "the Russian fleet", "Woody Allen", "a second", "Citizen Kane", "\"The Stag\"", "Jolly Roger", "bears", "lili johnson", "Jet Blue", "a beret", "a shot glass", "Cusco", "the Phantom of the Opera", "an eye", "Holy Roman Empire", "Hawaii", "Bob Newhart", "shorthand", "cologne", "carrots", "Utah", "a megrim", "a black and white tuxedo cat", "Sam Shepard", "petroleum", "August Wilson", "the survivors of Oceanic Flight 815", "bay leaf", "the Blue Nile", "China ( formerly the Republic of China ), France, the United Kingdom, and the United States", "Roman Reigns", "The Mexican Seismic Alert System", "violin", "(Caballero)", "Simeon Williamson", "11 November 1869", "Ten Walls", "Nia Kay", "new kidney", "the surgical anesthetic propofol", "helicopters and unmanned aerial vehicles from the White House to patrol the border region with Mexico."], "metric_results": {"EM": 0.4375, "QA-F1": 0.4901948380566801}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.846153846153846, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5263157894736842]}}, "before_error_ids": ["mrqa_squad-validation-10284", "mrqa_searchqa-validation-13463", "mrqa_searchqa-validation-1839", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-41", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-14158", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14060", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-12860", "mrqa_searchqa-validation-9533", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-1265", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-16829", "mrqa_searchqa-validation-13176", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-8784", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-16060", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-975", "mrqa_triviaqa-validation-2394", "mrqa_triviaqa-validation-6791", "mrqa_hotpotqa-validation-4819", "mrqa_newsqa-validation-1445"], "SR": 0.4375, "CSR": 0.5552083333333333, "retrieved_ids": ["mrqa_squad-train-55411", "mrqa_squad-train-50926", "mrqa_squad-train-86530", "mrqa_squad-train-74251", "mrqa_squad-train-34056", "mrqa_squad-train-50067", "mrqa_squad-train-71857", "mrqa_squad-train-66171", "mrqa_squad-train-60076", "mrqa_squad-train-13148", "mrqa_squad-train-4946", "mrqa_squad-train-51827", "mrqa_squad-train-12058", "mrqa_squad-train-22990", "mrqa_squad-train-37560", "mrqa_squad-train-14290", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-2757", "mrqa_hotpotqa-validation-2673", "mrqa_searchqa-validation-12941", "mrqa_naturalquestions-validation-1382", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-6821", "mrqa_searchqa-validation-16205", "mrqa_hotpotqa-validation-693", "mrqa_squad-validation-6614", "mrqa_squad-validation-7763", "mrqa_newsqa-validation-352", "mrqa_squad-validation-9695", "mrqa_naturalquestions-validation-10615", "mrqa_searchqa-validation-15872", "mrqa_triviaqa-validation-2121"], "EFR": 1.0, "Overall": 0.7465104166666666}, {"timecode": 30, "UKR": 0.755859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-1484", "mrqa_hotpotqa-validation-1574", "mrqa_hotpotqa-validation-1750", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1912", "mrqa_hotpotqa-validation-1914", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-2217", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2281", "mrqa_hotpotqa-validation-2310", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2440", "mrqa_hotpotqa-validation-2456", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2605", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3299", "mrqa_hotpotqa-validation-3310", "mrqa_hotpotqa-validation-3339", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4182", "mrqa_hotpotqa-validation-4189", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-5002", "mrqa_hotpotqa-validation-5034", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-5366", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5880", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-748", "mrqa_hotpotqa-validation-872", "mrqa_hotpotqa-validation-918", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2250", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-476", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4876", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-5390", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-975", "mrqa_naturalquestions-validation-9773", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2016", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-3083", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3154", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-456", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-86", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10395", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-11414", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-12104", "mrqa_searchqa-validation-12242", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-1265", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12897", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-13321", "mrqa_searchqa-validation-14752", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14932", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-15244", "mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-1595", "mrqa_searchqa-validation-16258", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1696", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1773", "mrqa_searchqa-validation-1926", "mrqa_searchqa-validation-2095", "mrqa_searchqa-validation-2329", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-5204", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5338", "mrqa_searchqa-validation-5478", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-6770", "mrqa_searchqa-validation-6958", "mrqa_searchqa-validation-7938", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8791", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-9391", "mrqa_searchqa-validation-9683", "mrqa_squad-validation-100", "mrqa_squad-validation-10001", "mrqa_squad-validation-1002", "mrqa_squad-validation-10063", "mrqa_squad-validation-10174", "mrqa_squad-validation-10186", "mrqa_squad-validation-10256", "mrqa_squad-validation-10316", "mrqa_squad-validation-10370", "mrqa_squad-validation-10386", "mrqa_squad-validation-10457", "mrqa_squad-validation-10463", "mrqa_squad-validation-10470", "mrqa_squad-validation-1078", "mrqa_squad-validation-1177", "mrqa_squad-validation-1219", "mrqa_squad-validation-1263", "mrqa_squad-validation-133", "mrqa_squad-validation-1330", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1498", "mrqa_squad-validation-150", "mrqa_squad-validation-151", "mrqa_squad-validation-1612", "mrqa_squad-validation-1613", "mrqa_squad-validation-168", "mrqa_squad-validation-175", "mrqa_squad-validation-1759", "mrqa_squad-validation-1779", "mrqa_squad-validation-1813", "mrqa_squad-validation-1891", "mrqa_squad-validation-1980", "mrqa_squad-validation-2018", "mrqa_squad-validation-2085", "mrqa_squad-validation-2085", "mrqa_squad-validation-2109", "mrqa_squad-validation-2129", "mrqa_squad-validation-2167", "mrqa_squad-validation-2214", "mrqa_squad-validation-2302", "mrqa_squad-validation-246", "mrqa_squad-validation-2586", "mrqa_squad-validation-2666", "mrqa_squad-validation-270", "mrqa_squad-validation-2704", "mrqa_squad-validation-2754", "mrqa_squad-validation-2913", "mrqa_squad-validation-2939", "mrqa_squad-validation-2975", "mrqa_squad-validation-3037", "mrqa_squad-validation-313", "mrqa_squad-validation-3213", "mrqa_squad-validation-3370", "mrqa_squad-validation-3479", "mrqa_squad-validation-350", "mrqa_squad-validation-3535", "mrqa_squad-validation-3581", "mrqa_squad-validation-3667", "mrqa_squad-validation-3740", "mrqa_squad-validation-375", "mrqa_squad-validation-3750", "mrqa_squad-validation-3811", "mrqa_squad-validation-3842", "mrqa_squad-validation-385", "mrqa_squad-validation-3922", "mrqa_squad-validation-3937", "mrqa_squad-validation-3939", "mrqa_squad-validation-4096", "mrqa_squad-validation-4179", "mrqa_squad-validation-4246", "mrqa_squad-validation-4292", "mrqa_squad-validation-4469", "mrqa_squad-validation-4591", "mrqa_squad-validation-4669", "mrqa_squad-validation-468", "mrqa_squad-validation-4860", "mrqa_squad-validation-4877", "mrqa_squad-validation-4898", "mrqa_squad-validation-5007", "mrqa_squad-validation-5030", "mrqa_squad-validation-5042", "mrqa_squad-validation-5135", "mrqa_squad-validation-5325", "mrqa_squad-validation-5351", "mrqa_squad-validation-5613", "mrqa_squad-validation-5699", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5716", "mrqa_squad-validation-5721", "mrqa_squad-validation-5809", "mrqa_squad-validation-5875", "mrqa_squad-validation-5933", "mrqa_squad-validation-6025", "mrqa_squad-validation-6116", "mrqa_squad-validation-6157", "mrqa_squad-validation-6163", "mrqa_squad-validation-6274", "mrqa_squad-validation-6341", "mrqa_squad-validation-6383", "mrqa_squad-validation-6579", "mrqa_squad-validation-6676", "mrqa_squad-validation-6700", "mrqa_squad-validation-6705", "mrqa_squad-validation-6803", "mrqa_squad-validation-7036", "mrqa_squad-validation-7168", "mrqa_squad-validation-7233", "mrqa_squad-validation-7233", "mrqa_squad-validation-7246", "mrqa_squad-validation-7294", "mrqa_squad-validation-735", "mrqa_squad-validation-7354", "mrqa_squad-validation-7367", "mrqa_squad-validation-7428", "mrqa_squad-validation-7491", "mrqa_squad-validation-7516", "mrqa_squad-validation-7531", "mrqa_squad-validation-7531", "mrqa_squad-validation-7570", "mrqa_squad-validation-7707", "mrqa_squad-validation-7782", "mrqa_squad-validation-7788", "mrqa_squad-validation-780", "mrqa_squad-validation-7805", "mrqa_squad-validation-7838", "mrqa_squad-validation-7909", "mrqa_squad-validation-8386", "mrqa_squad-validation-850", "mrqa_squad-validation-8500", "mrqa_squad-validation-8502", "mrqa_squad-validation-8523", "mrqa_squad-validation-8555", "mrqa_squad-validation-8601", "mrqa_squad-validation-8603", "mrqa_squad-validation-8680", "mrqa_squad-validation-8861", "mrqa_squad-validation-8900", "mrqa_squad-validation-8901", "mrqa_squad-validation-8905", "mrqa_squad-validation-8927", "mrqa_squad-validation-8931", "mrqa_squad-validation-8948", "mrqa_squad-validation-8966", "mrqa_squad-validation-9254", "mrqa_squad-validation-9269", "mrqa_squad-validation-9271", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9302", "mrqa_squad-validation-9322", "mrqa_squad-validation-9328", "mrqa_squad-validation-9412", "mrqa_squad-validation-9436", "mrqa_squad-validation-95", "mrqa_squad-validation-9590", "mrqa_squad-validation-9695", "mrqa_squad-validation-9901", "mrqa_squad-validation-9989", "mrqa_triviaqa-validation-1052", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1427", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1840", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-247", "mrqa_triviaqa-validation-2937", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3045", "mrqa_triviaqa-validation-3065", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3577", "mrqa_triviaqa-validation-3657", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3803", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4137", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4430", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-486", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-5226", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-5965", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6659", "mrqa_triviaqa-validation-6698", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6784", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7404", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7613"], "OKR": 0.890625, "KG": 0.45546875, "before_eval_results": {"predictions": ["The IPCC process on climate change", "coronary thrombosis", "a Qutb", "ANNA", "the Bicentennial Symphony", "the Statue of Saint Peter", "(Ivanhoe)", "kerosene", "England", "Shirley Temple Black", "flagellare", "President Lyndon B. Johnson", "Bart Simpson", "(SACEUR)", "Frasier", "Hispanic heritage", "fish", "(2)", "a motor neuron", "Bill Clinton's", "george david thoreau", "Mexico", "cosmology", "a trace of copper", "Welsh and English", "a decoupage", "The Great American Novel", "Arethusa", "(ALA)", "Bucharest", "heart defects", "manager", "Wheat", "Aboriginal children", "Dusty Old Dust", "the pancreas", "a Ninja", "World War II", "a pastry crust", "Bill Murray", "(Deneb IV)", "The Awl", "the (Testudo nigra)", "a Knesset", "Secretary's Day 2016", "Life", "Dante's Inferio", "a calves", "Lulu", "the Great Hunger", "eulogy", "Roald Dahl", "Massachusetts", "third", "the leaves of the plant species", "Amy Dorrit", "araneidan", "Michael Sheen", "St. Patrick's Day in 1988", "Kealakekua Bay", "Santiago Herrera", "three", "Secretary of State", "Fernando Torres was both hero and villain as third-placed Chelsea beat promoted Swansea 4-1."], "metric_results": {"EM": 0.359375, "QA-F1": 0.4810639880952381}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, true, true, false, false, true, true, false], "QA-F1": [0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25]}}, "before_error_ids": ["mrqa_squad-validation-8719", "mrqa_searchqa-validation-14778", "mrqa_searchqa-validation-15141", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-15830", "mrqa_searchqa-validation-3606", "mrqa_searchqa-validation-4718", "mrqa_searchqa-validation-12276", "mrqa_searchqa-validation-1744", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-875", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-4055", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-12346", "mrqa_searchqa-validation-8084", "mrqa_searchqa-validation-9336", "mrqa_searchqa-validation-2416", "mrqa_searchqa-validation-16391", "mrqa_searchqa-validation-12944", "mrqa_searchqa-validation-1913", "mrqa_searchqa-validation-349", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-3032", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-2083", "mrqa_searchqa-validation-15784", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-16220", "mrqa_searchqa-validation-3267", "mrqa_searchqa-validation-4325", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-9172", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-4167", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5627", "mrqa_newsqa-validation-2470"], "SR": 0.359375, "CSR": 0.548891129032258, "retrieved_ids": ["mrqa_squad-train-39468", "mrqa_squad-train-14945", "mrqa_squad-train-58911", "mrqa_squad-train-65920", "mrqa_squad-train-83101", "mrqa_squad-train-6376", "mrqa_squad-train-35327", "mrqa_squad-train-80351", "mrqa_squad-train-16376", "mrqa_squad-train-30805", "mrqa_squad-train-82614", "mrqa_squad-train-15065", "mrqa_squad-train-83521", "mrqa_squad-train-9927", "mrqa_squad-train-36865", "mrqa_squad-train-82265", "mrqa_searchqa-validation-10395", "mrqa_newsqa-validation-2422", "mrqa_triviaqa-validation-5474", "mrqa_newsqa-validation-3678", "mrqa_searchqa-validation-14767", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-3106", "mrqa_naturalquestions-validation-9071", "mrqa_newsqa-validation-2108", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1872", "mrqa_newsqa-validation-2291", "mrqa_searchqa-validation-2536"], "EFR": 1.0, "Overall": 0.7301688508064517}, {"timecode": 31, "before_eval_results": {"predictions": ["corrosion", "steamboats", "imperfect", "Henry Chibret", "Manhattan Project", "Maryland", "the Scotch Egg", "\"carnaval\"", "Dale Earnhardt", "Paul", "Ezra Pound", "Cuyahoga", "\"The Mathematical Movie Database", "Apollo 13", "a coyote", "cholera", "the sun", "satin pie", "the Air Force Academy", "Harry Lime", "shrewd", "a nucleus", "Don Juan De Marco", "Texas", "IBM", "Chuck Yeager", "the CIA", "Indira Gandhi International Airport", "elephants", "Saudi Arabia", "Abnormal Psychology", "goat milk", "Billy Idol", "anaphylaxis", "copper", "Jenny Ballinger", "Bank of America", "the National Lampoon", "Terry Bradshaw", "Florence", "farce", "parasites", "Columbia University", "Hillary Clinton", "Don Quixote", "Medium", "Seattle", "insulin", "dilettante", "Pilcro", "Ricky Martin", "Mizzou", "1989", "sea water and fresh water", "Canada and the United States", "Napoleon Bonaparte", "the wattage", "Frederick Forsyth", "Daniil Shafran", "67,575", "various names", "Alan Graham", "At least 13", "The Everglades,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6125744047619047}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-757", "mrqa_searchqa-validation-9215", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-2914", "mrqa_searchqa-validation-9740", "mrqa_searchqa-validation-15434", "mrqa_searchqa-validation-16718", "mrqa_searchqa-validation-2267", "mrqa_searchqa-validation-12607", "mrqa_searchqa-validation-16219", "mrqa_searchqa-validation-4764", "mrqa_searchqa-validation-9617", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-13909", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-3155", "mrqa_searchqa-validation-12978", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-10838", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7582", "mrqa_searchqa-validation-2405", "mrqa_searchqa-validation-11232", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-8628", "mrqa_triviaqa-validation-5038", "mrqa_newsqa-validation-3911"], "SR": 0.53125, "CSR": 0.54833984375, "retrieved_ids": ["mrqa_squad-train-68149", "mrqa_squad-train-43505", "mrqa_squad-train-10991", "mrqa_squad-train-41157", "mrqa_squad-train-64488", "mrqa_squad-train-11664", "mrqa_squad-train-3494", "mrqa_squad-train-52126", "mrqa_squad-train-57528", "mrqa_squad-train-969", "mrqa_squad-train-42188", "mrqa_squad-train-66654", "mrqa_squad-train-11378", "mrqa_squad-train-83102", "mrqa_squad-train-27473", "mrqa_squad-train-31742", "mrqa_searchqa-validation-398", "mrqa_newsqa-validation-949", "mrqa_searchqa-validation-10614", "mrqa_hotpotqa-validation-4330", "mrqa_squad-validation-6029", "mrqa_triviaqa-validation-3159", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-3970", "mrqa_hotpotqa-validation-1874", "mrqa_squad-validation-6752", "mrqa_searchqa-validation-10199", "mrqa_squad-validation-448", "mrqa_triviaqa-validation-3880", "mrqa_squad-validation-2315", "mrqa_searchqa-validation-14979"], "EFR": 1.0, "Overall": 0.73005859375}, {"timecode": 32, "before_eval_results": {"predictions": ["Eric Roberts", "Department for Culture, Media and Sport", "Locks of Love", "Tennessee Cherokee", "Wayne Gretzky", "Boris Godunov", "Williamsburg", "Pitcairn", "air superiority", "Halloween", "a port-wine stain", "hurricane", "Broadway", "one", "AAA", "Jutland", "mutual fund", "the Two Sicilies", "1773", "Prometheus", "a thick cream soup", "4", "Minnesota", "Violeta Barrios de Chamorro", "Horatio Nelson", "Pillsbury", "oxygen", "develop special talents", "jeans", "Jenna Bush", "Jonathan Demme", "California", "Asia-Pacific", "the ear", "Moulin Rouge", "Cape Cod", "the chondrocra- nium", "febreze", "the 1989 wreck", "ice ages", "an oak leaf cluster", "Slavic", "Walter Payton", "Orleans", "Mars", "the pilots' compartment", "copper", "Tom Ridge", "George Babbitt", "Meg Ryan, Julia Roberts, Kate", "Jeopardy", "imperative", "18", "George Halas", "Long Island", "the Dormouse", "Salema", "Kent", "Gareth Jones", "Prince Sung-won", "Newcastle upon Tyne, England", "Illness", "Dan Parris, 25, and Rob Lehr, 26,", "2,000 euros ($2,963)"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6088541666666667}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, true, false, true, false, true, false, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16822", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-13952", "mrqa_searchqa-validation-5519", "mrqa_searchqa-validation-8665", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-13050", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-15587", "mrqa_searchqa-validation-2878", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-7358", "mrqa_searchqa-validation-11313", "mrqa_searchqa-validation-14545", "mrqa_searchqa-validation-2960", "mrqa_searchqa-validation-13215", "mrqa_searchqa-validation-8969", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-12784", "mrqa_naturalquestions-validation-1223", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-1605", "mrqa_hotpotqa-validation-3971", "mrqa_newsqa-validation-2296"], "SR": 0.53125, "CSR": 0.5478219696969697, "retrieved_ids": ["mrqa_squad-train-22902", "mrqa_squad-train-29991", "mrqa_squad-train-43172", "mrqa_squad-train-63870", "mrqa_squad-train-84701", "mrqa_squad-train-32274", "mrqa_squad-train-11472", "mrqa_squad-train-40252", "mrqa_squad-train-58044", "mrqa_squad-train-80711", "mrqa_squad-train-84040", "mrqa_squad-train-33482", "mrqa_squad-train-21746", "mrqa_squad-train-66385", "mrqa_squad-train-52366", "mrqa_squad-train-18787", "mrqa_triviaqa-validation-2674", "mrqa_squad-validation-7635", "mrqa_newsqa-validation-820", "mrqa_naturalquestions-validation-975", "mrqa_squad-validation-2939", "mrqa_searchqa-validation-12333", "mrqa_squad-validation-2754", "mrqa_searchqa-validation-15872", "mrqa_newsqa-validation-500", "mrqa_squad-validation-4469", "mrqa_hotpotqa-validation-4753", "mrqa_squad-validation-3708", "mrqa_squad-validation-9855", "mrqa_newsqa-validation-3534", "mrqa_searchqa-validation-3571", "mrqa_triviaqa-validation-1156"], "EFR": 1.0, "Overall": 0.729955018939394}, {"timecode": 33, "before_eval_results": {"predictions": ["2100", "Apollo 5", "Benazir Bhutto", "the Democratic VP candidate", "The space agency says the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "i report form", "Muqtada al-Sadr", "@", "\"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\"", "The new Touch", "role as a bride in the 2007 movie \"License to Wed\"", "Janet Napolitano", "sodium dichromate", "citizenship", "10,000", "4,000", "CNN/Opinion Research Corporation", "California", "Michael Arrington", "Ralph Cifaretto", "Tom Baer", "Two", "LulzSec.", "The Transportation Security Administration", "Maude", "was killed", "Nineteen", "dance show", "1918-1919.", "Liza Murphy", "Turkey", "the Cowardly Lion", "\"It took everything I had to move out of her rental house because it is facing foreclosure", "Gary Player", "capital murder and three counts of attempted murder", "Nicole", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "a \"prostitute\"", "Mississippi", "40", "April 22", "barter -- trading goods and services without exchanging money", "14", "funded by a German company and affiliated with the group Bread for the World.", "the war years", "Silicon Valley", "between 1917 and 1924", "June 6, 1944", "demolishing American third seed Venus Williams in the final of the Sony Ericsson Open in Miami on Saturday.", "Seoul", "free", "\"The driver of that train, who was among the dead,", "Charlene Holt", "gastrocnemius", "Miami Heat", "2", "Finch", "architect", "the Secret Intelligence Service", "Ready Player One", "alcoholic drinks", "Mars", "antonyms", "Tartarus"], "metric_results": {"EM": 0.5, "QA-F1": 0.5542169831979614}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 0.4799999999999999, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.08695652173913045, 1.0, 1.0, 1.0, 0.19047619047619047, 0.2857142857142857, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.0, 0.16666666666666669, 0.15384615384615385, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-850", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-3020", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-535", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-1288", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-4032", "mrqa_triviaqa-validation-3514", "mrqa_hotpotqa-validation-1791", "mrqa_searchqa-validation-201"], "SR": 0.5, "CSR": 0.5464154411764706, "retrieved_ids": ["mrqa_squad-train-22715", "mrqa_squad-train-78973", "mrqa_squad-train-29330", "mrqa_squad-train-48449", "mrqa_squad-train-45633", "mrqa_squad-train-76197", "mrqa_squad-train-52009", "mrqa_squad-train-40814", "mrqa_squad-train-41439", "mrqa_squad-train-12398", "mrqa_squad-train-57285", "mrqa_squad-train-68662", "mrqa_squad-train-50041", "mrqa_squad-train-4175", "mrqa_squad-train-70290", "mrqa_squad-train-66799", "mrqa_triviaqa-validation-821", "mrqa_triviaqa-validation-7724", "mrqa_hotpotqa-validation-4780", "mrqa_newsqa-validation-2608", "mrqa_squad-validation-2754", "mrqa_squad-validation-8719", "mrqa_newsqa-validation-3961", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-82", "mrqa_squad-validation-150", "mrqa_searchqa-validation-6300", "mrqa_hotpotqa-validation-2741", "mrqa_triviaqa-validation-1921", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-7728", "mrqa_newsqa-validation-1589"], "EFR": 1.0, "Overall": 0.7296737132352942}, {"timecode": 34, "before_eval_results": {"predictions": ["for complicity and to Odinga declaring himself the \"people's president\"", "KGPE", "Lerotholi Polytechnic Football Club", "John Delaney", "1979", "The Catholic Church in Ireland", "Trey Parker and Matt Stone", "Province of Canterbury", "310", "Bohemia", "Czech", "people working in film and the performing arts", "McLean, Virginia,", "Sophie Winkleman", "Portsmouth's Christian Oxlade-Chamberlain", "Domingo \"Sam\" Samudio", "coaxial", "Bruce McLaren", "Marika Nicolette Green", "Katherine Harris", "Frank Thomas' Big Hurt", "World War I", "John Richard Schlesinger, CBE", "Nikolai Trubetzkoy", "3,500,000", "Rabies", "Big Bad Wolf", "Shakespeare's play of the same name", "2016", "George Orwell", "July 8, 2014", "Prabh Gill", "the Mediterranean", "Gabriel Iglesias", "Kolkata", "two", "Roscoe Lee Browne", "23", "video game", "a Peach", "U.S.", "Jimmy Ellis", "July 11, 2016", "Srinagar", "\"Drunken Master II", "San Francisco, California", "\"The Brothers\"", "Daniel Espinosa", "Adam Levine, Blake Shelton, and Pharrell Williams", "Benjamin Andrew \" Ben\" Stokes", "Durham, North Carolina at Cameron Indoor Stadium", "Royal Albert Hall and The Kennedy Center", "Louis XV", "Venezuela and the remainder in Colombia", "`` king ''", "Munich", "Admiral Van Galen Evertsen,", "USA Today serving as its megaphone.", "Myanmar", "Laurean", "pipelines and hostage-taking", "a Rolling Stone", "Nick is", "General McClellan"], "metric_results": {"EM": 0.515625, "QA-F1": 0.60343793767507}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, false, true, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false], "QA-F1": [0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6, 0.5, 1.0, 0.25, 1.0, 1.0, 0.3333333333333333, 0.2857142857142857, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8422", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-4729", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5039", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-1199", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-3889", "mrqa_hotpotqa-validation-3576", "mrqa_naturalquestions-validation-10354", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-4748", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-3428", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-2753", "mrqa_searchqa-validation-15973"], "SR": 0.515625, "CSR": 0.5455357142857142, "retrieved_ids": ["mrqa_squad-train-45978", "mrqa_squad-train-333", "mrqa_squad-train-26606", "mrqa_squad-train-17549", "mrqa_squad-train-78751", "mrqa_squad-train-23547", "mrqa_squad-train-35690", "mrqa_squad-train-19329", "mrqa_squad-train-71488", "mrqa_squad-train-11013", "mrqa_squad-train-54865", "mrqa_squad-train-72227", "mrqa_squad-train-21831", "mrqa_squad-train-75065", "mrqa_squad-train-78564", "mrqa_squad-train-75200", "mrqa_searchqa-validation-5320", "mrqa_triviaqa-validation-3634", "mrqa_newsqa-validation-1598", "mrqa_squad-validation-8399", "mrqa_squad-validation-1161", "mrqa_naturalquestions-validation-10273", "mrqa_squad-validation-5429", "mrqa_searchqa-validation-15495", "mrqa_triviaqa-validation-3692", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-5552", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4005", "mrqa_searchqa-validation-14778", "mrqa_hotpotqa-validation-2262", "mrqa_newsqa-validation-3970"], "EFR": 1.0, "Overall": 0.7294977678571428}, {"timecode": 35, "before_eval_results": {"predictions": ["downward pressure", "ABC Television Center", "Cincinnati", "North Sea", "\"The Next Step\"", "\"Gliding Dance of the Maidens\"", "Patti Smith", "\"Darconville\u2019s Cat\"", "\"Kitty Hawk\"", "Augustus", "Netherlands", "Eva Ibbotson", "the 70 m and 90 m events", "Wiz Khalifa", "\"Danger Mouse\"", "Ernest Hemingway", "the Newell Highway", "comedy", "in the 2013\u20132014 television season", "University of Southern California", "New York City", "extreme nationalist, and nativist", "1st Marquess of Westminster", "Christopher McCulloch", "Marigold Newey", "an album", "The Wachowskis", "Arena of Khazan", "WikiLeaks", "top division", "Hennepin County, Minnesota", "a fantasy role-playing game", "New Jersey", "east", "Giacomo Puccini", "Thomas Jefferson", "Winchester", "his virtuoso playing techniques and compositions in orchestral fusion", "1912", "Eisenhower Executive Office Building", "\"The Original Sound Track from Five Summer Stories\"", "\"American Chopper\"", "The women's experiences in each vignette are designed to demonstrate the popular views of society on the issue in each of the given decades.", "in his time a Latitudinarian", "Hindi", "Netherlands", "soccer", "Kings Point, New York", "a Ballon d'Or", "Westminster system", "Bruce Grobbelaar", "was less dangerous", "seven years earlier", "`` Mirror Image ''", "79", "Alaska", "football", "\"People\u2019s Dispensary for sick Animals\"", "in an effort to make the animals' lives as natural as possible.", "Kurt Cobain", "\"I remember growing up in the Middle East, influenced, enjoying his music, waiting for his albums,\"", "Our Sea", "a sedimentary rock", "Tchekoff"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6303354978354978}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.08, 0.5, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7182", "mrqa_hotpotqa-validation-3216", "mrqa_hotpotqa-validation-4486", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-455", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-5596", "mrqa_hotpotqa-validation-3767", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-2257", "mrqa_naturalquestions-validation-4338", "mrqa_triviaqa-validation-6520", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1352", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-282"], "SR": 0.546875, "CSR": 0.5455729166666667, "retrieved_ids": ["mrqa_squad-train-36166", "mrqa_squad-train-40138", "mrqa_squad-train-77348", "mrqa_squad-train-29755", "mrqa_squad-train-76572", "mrqa_squad-train-85527", "mrqa_squad-train-3881", "mrqa_squad-train-37807", "mrqa_squad-train-80769", "mrqa_squad-train-14467", "mrqa_squad-train-47475", "mrqa_squad-train-26651", "mrqa_squad-train-29665", "mrqa_squad-train-64599", "mrqa_squad-train-12820", "mrqa_squad-train-57107", "mrqa_hotpotqa-validation-3343", "mrqa_triviaqa-validation-2225", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-2076", "mrqa_newsqa-validation-1384", "mrqa_searchqa-validation-9261", "mrqa_hotpotqa-validation-3576", "mrqa_newsqa-validation-0", "mrqa_searchqa-validation-16258", "mrqa_triviaqa-validation-7585", "mrqa_hotpotqa-validation-3084", "mrqa_triviaqa-validation-645", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-4028", "mrqa_hotpotqa-validation-4323", "mrqa_squad-validation-6029"], "EFR": 0.9655172413793104, "Overall": 0.7226086566091954}, {"timecode": 36, "before_eval_results": {"predictions": ["a phylum of animals that live in marine waters worldwide", "Luger P08", "Commanding General", "the southern (Dolomitic) Alps", "various deities, beings, and heroes", "a fictional world", "Wandsworth, London", "1967", "Washington, D.C.", "capital crimes", "around 8000 BC", "Smoothie King Center", "Nan Britton", "1977", "9\u201310 March 1945", "Currer Bell", "Jim Davis", "Ted Bundy", "Crawley Town", "Parlophone", "McDowell County, West Virginia", "Life Is a Minestrone", "Louis Silvie \"Louie\" Zamperini", "an English professional footballer who plays as a striker for Premier League club Stoke City.", "Shakespeare in the Park", "peat moss", "South Australia", "Apple Lisa", "George Gordon Byron, 6th Baron Byron, FRS (22 January 1788 \u2013 19 April 1824)", "Singapore", "1853", "Oktoberfest", "1:00 a.m. Eastern Time Zone", "Mickey Mouse Cup", "Manchester\u2013Boston Regional Airport", "Frank Ocean", "Gangsta's Paradise", "Ryan Guno Babel", "casinos", "331", "Kristina Ceyton and Kristian Moliere", "1835", "Helsinki, Finland", "Francisco P. Felix", "Carrefour", "2014", "Empire Falls", "Universal's Volcano Bay", "North Queensland", "Tim Cook", "203 people", "the Caucasus region", "on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "Charlotte Hornets", "a burrow", "Billie Holiday", "Cumberland", "low-calorie", "American Civil Liberties Union", "three", "Three Little Pigs", "Spider-Man", "Carrie Bradshaw", "French parliamentary commission recommended a partial ban on any veils that cover the face -- including the burqa, the full-body covering worn by some Muslim women."], "metric_results": {"EM": 0.453125, "QA-F1": 0.5509076427045176}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, true, false, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.28571428571428575, 0.3076923076923077, 0.6666666666666666, 1.0, 0.125, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.26666666666666666, 0.5, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4534", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-1580", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5563", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-1606", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-2865", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-5562", "mrqa_hotpotqa-validation-1753", "mrqa_hotpotqa-validation-614", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-4653", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-423", "mrqa_newsqa-validation-4076", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-4804", "mrqa_newsqa-validation-297"], "SR": 0.453125, "CSR": 0.5430743243243243, "retrieved_ids": ["mrqa_squad-train-68205", "mrqa_squad-train-23624", "mrqa_squad-train-70433", "mrqa_squad-train-20899", "mrqa_squad-train-15552", "mrqa_squad-train-72868", "mrqa_squad-train-10168", "mrqa_squad-train-77479", "mrqa_squad-train-3038", "mrqa_squad-train-34985", "mrqa_squad-train-50677", "mrqa_squad-train-36003", "mrqa_squad-train-59212", "mrqa_squad-train-1102", "mrqa_squad-train-56971", "mrqa_squad-train-57172", "mrqa_searchqa-validation-5385", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-2269", "mrqa_searchqa-validation-6708", "mrqa_newsqa-validation-2905", "mrqa_searchqa-validation-349", "mrqa_newsqa-validation-2232", "mrqa_naturalquestions-validation-158", "mrqa_hotpotqa-validation-1602", "mrqa_triviaqa-validation-3159", "mrqa_searchqa-validation-1839", "mrqa_newsqa-validation-2970", "mrqa_squad-validation-8453", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-1301"], "EFR": 1.0, "Overall": 0.7290054898648649}, {"timecode": 37, "before_eval_results": {"predictions": ["many castles and vineyards", "SSM Cardinal Glennon Children's Medical Center in St. Louis.", "2000.", "lack of a cause of death", "will American go bankrupt?", "The Spanish flight crew is innocent and should be released,", "Hundreds scraped together his last salary, some money he made from trading sugar bought at a discount from the supermarket where he worked, and funds borrowed from friends to secure a visitor's visa and bus ticket to Johannesburg.", "Pope Benedict XVI", "Herman Cain", "Zulfikar Ali Bhutto,", "money or other discreet aid", "News of the World tabloid.", "sovereignty over them.", "prostate cancer", "July 23.", "the killers had plotted the attack with Al Gamaa al-Islamiyya,", "the single-engine Cessna 206 went down,", "Bhola", "software magnate", "150 passengers", "Sharon Bialek", "for using recreational drugs in September,", "did not speak to those who had gathered but shadow-boxed to spectators and cameras before meeting his distant relatives.\"", "1800s and the era of Mark Twain,", "the state's attorney", "photos", "\"Twilight\"", "male veterans struggling with homelessness and addiction.", "cities throughout Canada.", "\"whole ethos is one of violence\" and that it had \"made a brutal choice to step up attacks against innocent civilians.\"", "opening of its new restaurant next to the home of Mona Lisa as something completely normal.", "writing and starring in 'The Prisoner' about a former spy locked away in an isolated village who tries to escape each episode.", "this month of three people with ties to the U.S. Consulate in Ciudad Juarez, Mexico", "in a muddy barley field owned by farmer Alan Graham outside Bangor,", "Swat Valley", "Former Beatles", "the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "Dolgorsuren Dagvadorj", "the southern city of Naples", "last summer.", "Kevin Kuranyi", "revelry", "London Heathrow's Terminal 5.", "18", "last April.", "Casablanca, Morocco,", "criticized his father's parenting skills.", "$250,000", "at a construction site in the heart of Los Angeles.", "$3 billion", "Birmingham", "7 July", "Bob Dylan", "St Pancras International", "a double-hung window", "Denver", "Brazil", "Phelan Beale", "sandstone", "1967", "Babe Ruth", "Penn Station", "steaks", "David Lodge"], "metric_results": {"EM": 0.5, "QA-F1": 0.5883321388756171}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true], "QA-F1": [0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.05714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.1904761904761905, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4615384615384615, 0.08695652173913045, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 0.9333333333333333, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8990", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-925", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-1776", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2938", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-1516", "mrqa_newsqa-validation-316", "mrqa_naturalquestions-validation-5457", "mrqa_triviaqa-validation-2027", "mrqa_hotpotqa-validation-3250", "mrqa_searchqa-validation-15555"], "SR": 0.5, "CSR": 0.5419407894736843, "retrieved_ids": ["mrqa_squad-train-75260", "mrqa_squad-train-25620", "mrqa_squad-train-45947", "mrqa_squad-train-81711", "mrqa_squad-train-11686", "mrqa_squad-train-40149", "mrqa_squad-train-45691", "mrqa_squad-train-28417", "mrqa_squad-train-33161", "mrqa_squad-train-24457", "mrqa_squad-train-34720", "mrqa_squad-train-59662", "mrqa_squad-train-21243", "mrqa_squad-train-32567", "mrqa_squad-train-35370", "mrqa_squad-train-65320", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-3865", "mrqa_searchqa-validation-15587", "mrqa_hotpotqa-validation-3060", "mrqa_squad-validation-1389", "mrqa_triviaqa-validation-2578", "mrqa_searchqa-validation-12135", "mrqa_hotpotqa-validation-812", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-15790", "mrqa_hotpotqa-validation-1580", "mrqa_hotpotqa-validation-3638", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-1164", "mrqa_hotpotqa-validation-550", "mrqa_newsqa-validation-1181"], "EFR": 1.0, "Overall": 0.7287787828947369}, {"timecode": 38, "before_eval_results": {"predictions": ["death of a heretic.\"", "Lars von Trier", "37", "cancerous tumor.", "in her home", "not be the worst thing in the world to wait and watch how this plays out in Iran.", "evokes childhood memories in this four-line ode to Mom.", "84-year-old", "free laundry service.", "The Bahamas", "opium", "the situation of America wielding a big stick for the last eight years.\"", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Galveston,", "Six", "Russell,", "581 points", "try to make life a little easier", "northwestern Montana", "building bombs,", "forgery and flying without a valid license,", "2050,", "Eintracht Frankfurt", "Juan Martin Del Potro.", "50", "because its facilities are full.", "stealing the personal credit information of thousands of unsuspecting American and European consumers,", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Akshay Kumar", "133", "more than 100", "The Ski Train", "a man's lifeless, naked body", "Flint, Michigan.", "London.", "Philippines", "$500,000", "work rule issues.", "not be allowed", "The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza,", "Hundreds of women", "second time since the 1990s", "Black History Month", "Congress", "Joe Lieberman,", "Bastian Schweinsteiger", "Bobby Jindal", "two suicide bombers,", "the stores around Lilla Torg.", "\"17 Again\"", "United Arab Emirates", "By petition for a writ of certiorari", "New Mexico", "The Soviet Union, too, had been heavily affected", "jewelled Easter eggs", "a supercontinent", "the A38", "ethereal", "youngest TV director ever", "Citizens for a Sound Economy", "bees", "Zenda", "Jean Lafitte", "a penalty kick"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5618483025200661}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, true, false, false, false, true, true, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.08695652173913043, 0.06060606060606061, 1.0, 0.4, 0.0, 1.0, 0.0, 0.1212121212121212, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 0.2857142857142857, 0.4, 1.0, 1.0, 1.0, 0.0, 0.21276595744680848, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.35294117647058826, 0.25, 0.25, 1.0, 1.0, 0.8, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.26666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3052", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-3006", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-4860", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3713", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-14613"], "SR": 0.421875, "CSR": 0.5388621794871795, "retrieved_ids": ["mrqa_squad-train-30255", "mrqa_squad-train-75919", "mrqa_squad-train-66333", "mrqa_squad-train-33708", "mrqa_squad-train-32204", "mrqa_squad-train-51257", "mrqa_squad-train-10240", "mrqa_squad-train-69822", "mrqa_squad-train-44490", "mrqa_squad-train-33658", "mrqa_squad-train-81609", "mrqa_squad-train-9066", "mrqa_squad-train-18789", "mrqa_squad-train-74171", "mrqa_squad-train-31152", "mrqa_squad-train-51066", "mrqa_hotpotqa-validation-3299", "mrqa_newsqa-validation-2232", "mrqa_searchqa-validation-12941", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-10354", "mrqa_newsqa-validation-848", "mrqa_hotpotqa-validation-2342", "mrqa_naturalquestions-validation-975", "mrqa_triviaqa-validation-2287", "mrqa_searchqa-validation-15434", "mrqa_newsqa-validation-3960", "mrqa_hotpotqa-validation-60", "mrqa_hotpotqa-validation-2488", "mrqa_newsqa-validation-1184", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-5294"], "EFR": 0.972972972972973, "Overall": 0.7227576554920305}, {"timecode": 39, "before_eval_results": {"predictions": ["radicalize the Islamist movement", "Western New York and Central New York", "in people and animals", "Andy Serkis", "Aldis Hodge", "Malloy as Pierre, Phillipa Soo as Viktor, Lucas Steele as Anatole, Amber Gray as H\u00e9l\u00e8ne, Brittain Ashford as Sonya, Nick Choksi as Dolokhov", "sedimentary", "all land - living organisms, both alive and dead, as well as carbon stored in soils", "John Young", "semi-autonomous organisational units", "W. Edwards Deming", "MacFarlane", "Cyndi Grecco", "any untuitous medical occurrence in a patient or clinical investigation subject administered a pharmaceutical product", "orbit", "`` Everywhere ''", "governor of West Virginia", "December 12, 2017", "in South Africa", "1832", "Curtis Armstrong", "fibrous tissue", "St Pancras International", "Jerry Leiber and Mike Stoller", "De Wayne Warren", "Charles Woodson", "Jesse Wesley Williams", "Peter Cetera", "291 episodes", "the squadron encountered the Baltic Fleet of 41 sail under convoy of the HMS Serapis", "Book of Exodus", "April 25 -- 30 in Park Avenue,", "Kristy Swanson", "2001", "1994", "Washington", "Triple Alliance of Germany, Austria - Hungary, and Italy", "The Australian colonies became profitable exporters of wool and gold, mainly because of gold rushes in the colony of Victoria,", "Wisconsin", "The standing rib roast", "Donald Trump", "David Tennant", "Ben Findon, Mike Myers and Bob Puzey", "Charles Path\u00e9", "Barbara Windsor", "1854", "1997", "eleven", "Mel Gibson", "boy", "works in a bridal shop", "Venice", "The Wrestling Classic", "Piper Aircraft light planes", "Mike Fiers", "Rockland", "Delilah Rene", "three", "\"The Hutus were considered inferior,", "Frank Ricci,", "Zanzibar", "Mao Zedong", "Guglielmo Marconi", "Drew Kesse,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5921918883961567}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.16, 0.6666666666666666, 0.2222222222222222, 1.0, 0.5, 1.0, 1.0, 1.0, 0.14634146341463414, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 0.0, 1.0, 0.4, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.7272727272727273, 0.0, 0.0, 0.0, 0.4, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-2365", "mrqa_hotpotqa-validation-2210", "mrqa_hotpotqa-validation-1952", "mrqa_newsqa-validation-3661", "mrqa_searchqa-validation-12993", "mrqa_newsqa-validation-3331"], "SR": 0.484375, "CSR": 0.5375, "retrieved_ids": ["mrqa_squad-train-64342", "mrqa_squad-train-1952", "mrqa_squad-train-74962", "mrqa_squad-train-71385", "mrqa_squad-train-37447", "mrqa_squad-train-71121", "mrqa_squad-train-51416", "mrqa_squad-train-4104", "mrqa_squad-train-17455", "mrqa_squad-train-55613", "mrqa_squad-train-47451", "mrqa_squad-train-59470", "mrqa_squad-train-17453", "mrqa_squad-train-64023", "mrqa_squad-train-25798", "mrqa_squad-train-77185", "mrqa_newsqa-validation-1948", "mrqa_triviaqa-validation-6256", "mrqa_hotpotqa-validation-2092", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-456", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-922", "mrqa_hotpotqa-validation-187", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-984", "mrqa_newsqa-validation-185", "mrqa_squad-validation-2704", "mrqa_triviaqa-validation-4005", "mrqa_searchqa-validation-8969", "mrqa_hotpotqa-validation-3216"], "EFR": 0.9393939393939394, "Overall": 0.7157694128787879}, {"timecode": 40, "UKR": 0.791015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1604", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-1840", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-2225", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3310", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4443", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4729", "mrqa_hotpotqa-validation-4744", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4944", "mrqa_hotpotqa-validation-5034", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5317", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5424", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5590", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-854", "mrqa_hotpotqa-validation-866", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3525", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-370", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4658", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1610", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1818", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2470", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3286", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3521", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-973", "mrqa_searchqa-validation-10210", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-11232", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-1177", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-12959", "mrqa_searchqa-validation-13156", "mrqa_searchqa-validation-13489", "mrqa_searchqa-validation-13740", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-14978", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15004", "mrqa_searchqa-validation-15504", "mrqa_searchqa-validation-15505", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-15839", "mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16246", "mrqa_searchqa-validation-16353", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-3098", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-41", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-4567", "mrqa_searchqa-validation-4764", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5521", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-5698", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6659", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8176", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-8366", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8791", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-8995", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9683", "mrqa_searchqa-validation-9698", "mrqa_searchqa-validation-9803", "mrqa_squad-validation-100", "mrqa_squad-validation-10370", "mrqa_squad-validation-10457", "mrqa_squad-validation-12", "mrqa_squad-validation-1254", "mrqa_squad-validation-1330", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1498", "mrqa_squad-validation-1540", "mrqa_squad-validation-1586", "mrqa_squad-validation-1613", "mrqa_squad-validation-175", "mrqa_squad-validation-185", "mrqa_squad-validation-1902", "mrqa_squad-validation-1920", "mrqa_squad-validation-194", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2167", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-2473", "mrqa_squad-validation-2640", "mrqa_squad-validation-2672", "mrqa_squad-validation-270", "mrqa_squad-validation-2757", "mrqa_squad-validation-2762", "mrqa_squad-validation-2916", "mrqa_squad-validation-2986", "mrqa_squad-validation-3037", "mrqa_squad-validation-3125", "mrqa_squad-validation-3319", "mrqa_squad-validation-3407", "mrqa_squad-validation-3511", "mrqa_squad-validation-3545", "mrqa_squad-validation-3708", "mrqa_squad-validation-3770", "mrqa_squad-validation-3774", "mrqa_squad-validation-3786", "mrqa_squad-validation-3811", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3934", "mrqa_squad-validation-3939", "mrqa_squad-validation-4010", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-448", "mrqa_squad-validation-4484", "mrqa_squad-validation-4546", "mrqa_squad-validation-4671", "mrqa_squad-validation-4750", "mrqa_squad-validation-4799", "mrqa_squad-validation-4860", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5019", "mrqa_squad-validation-5050", "mrqa_squad-validation-5135", "mrqa_squad-validation-5276", "mrqa_squad-validation-5311", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5474", "mrqa_squad-validation-5634", "mrqa_squad-validation-5669", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5736", "mrqa_squad-validation-5751", "mrqa_squad-validation-6029", "mrqa_squad-validation-6318", "mrqa_squad-validation-6351", "mrqa_squad-validation-6481", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6594", "mrqa_squad-validation-6630", "mrqa_squad-validation-6981", "mrqa_squad-validation-7023", "mrqa_squad-validation-7168", "mrqa_squad-validation-7194", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7294", "mrqa_squad-validation-7466", "mrqa_squad-validation-7531", "mrqa_squad-validation-76", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7689", "mrqa_squad-validation-7763", "mrqa_squad-validation-7907", "mrqa_squad-validation-7932", "mrqa_squad-validation-8066", "mrqa_squad-validation-8066", "mrqa_squad-validation-8127", "mrqa_squad-validation-8209", "mrqa_squad-validation-8253", "mrqa_squad-validation-8386", "mrqa_squad-validation-8399", "mrqa_squad-validation-8488", "mrqa_squad-validation-8501", "mrqa_squad-validation-8754", "mrqa_squad-validation-8772", "mrqa_squad-validation-8901", "mrqa_squad-validation-8931", "mrqa_squad-validation-8948", "mrqa_squad-validation-9012", "mrqa_squad-validation-9322", "mrqa_squad-validation-9495", "mrqa_squad-validation-9506", "mrqa_squad-validation-955", "mrqa_squad-validation-9590", "mrqa_squad-validation-9768", "mrqa_squad-validation-9819", "mrqa_squad-validation-9855", "mrqa_squad-validation-9868", "mrqa_squad-validation-9901", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-1052", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1291", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1427", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-2889", "mrqa_triviaqa-validation-30", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4357", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6418", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6681", "mrqa_triviaqa-validation-6704", "mrqa_triviaqa-validation-6724", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-7601", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-895"], "OKR": 0.8515625, "KG": 0.5078125, "before_eval_results": {"predictions": ["Turkana", "Action Jackson", "Ben Willis", "Phillip Schofield and Christine Bleakley", "Fort Kent, Maine, at the Canada -- US border, south to Key West, Florida", "The Third Five - year Plan", "Hermann Ebbinghaus", "Ronald Reagan", "He was born in Baltimore, Maryland, in 1950, the son of Emmet William Ryan ( 1922 -- 1974 ), a Baltimore City Police Department homicide lieutenant, and World War II veteran", "collect menstrual flow", "noon of April 1st", "late 1980s", "Lesley Gore", "Andrew Lincoln", "Joe Spano", "the object is placed further away from the mirror / lens than the focal point", "a political ideology", "Matt Monro", "Wednesday, 5 September 1666", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "adrenal medulla", "The United States presidential line of succession", "multinational retail corporation", "July 2, 1776", "a balance sheet", "within an English - language book", "2018", "Fred Ott", "Timvin Campbell", "October 6, 2017", "early Christians of Mesopotamia", "Anna Murphy", "a mountainous, peninsular mainland jutting out into the Mediterranean Sea", "`` new version '' of Rent", "Santo Domingo", "1976", "Brad Johnson", "in Egypt", "Montreal", "2003", "May 30, 2017", "A turlough", "King Louie", "thirteen British colonies that declared independence from the Kingdom of Great Britain", "The euro", "local authorities", "`` One Son ''", "2017", "Nick Kroll", "Ron Harper", "1,350", "Zach Johnson", "The Jetsons", "Trinidad", "Point", "Keelung", "England", "Sunday,", "Elizabeth Birnbaum", "Karen Floyd", "Michelob", "burritos", "John Ritter", "Australian actor and film producer."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6187547521922521}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, true, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.9600000000000001, 1.0, 1.0, 0.0, 0.14285714285714288, 1.0, 0.6666666666666666, 0.3076923076923077, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1818181818181818, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7857142857142858, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-6670", "mrqa_naturalquestions-validation-5876", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-7464", "mrqa_triviaqa-validation-336", "mrqa_triviaqa-validation-783", "mrqa_hotpotqa-validation-1328", "mrqa_searchqa-validation-15726"], "SR": 0.546875, "CSR": 0.5377286585365854, "retrieved_ids": ["mrqa_squad-train-53738", "mrqa_squad-train-80428", "mrqa_squad-train-63360", "mrqa_squad-train-37575", "mrqa_squad-train-7548", "mrqa_squad-train-80296", "mrqa_squad-train-31210", "mrqa_squad-train-50907", "mrqa_squad-train-84000", "mrqa_squad-train-66413", "mrqa_squad-train-43831", "mrqa_squad-train-16829", "mrqa_squad-train-11145", "mrqa_squad-train-71423", "mrqa_squad-train-16280", "mrqa_squad-train-55540", "mrqa_triviaqa-validation-3516", "mrqa_squad-validation-1764", "mrqa_hotpotqa-validation-3446", "mrqa_searchqa-validation-201", "mrqa_newsqa-validation-5", "mrqa_hotpotqa-validation-3521", "mrqa_searchqa-validation-2183", "mrqa_squad-validation-4469", "mrqa_newsqa-validation-1388", "mrqa_searchqa-validation-947", "mrqa_hotpotqa-validation-1011", "mrqa_triviaqa-validation-519", "mrqa_naturalquestions-validation-2582", "mrqa_newsqa-validation-801", "mrqa_naturalquestions-validation-5241", "mrqa_searchqa-validation-16265"], "EFR": 1.0, "Overall": 0.737623856707317}, {"timecode": 41, "before_eval_results": {"predictions": ["many middle eastern scientists", "23 September 1889", "countries that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "ummat al - Islamiyah", "2018", "Saint Alphonsa", "Vincent Price", "in the basic curriculum", "air moisture", "the right of the dinner plate", "Yondu Udonta", "Terry Kath", "1830s", "December 19, 2016", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "4th", "one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "supervillains who pose catastrophic challenges to the world", "Atlanta", "21 June 2007", "Australia", "Bed and breakfast", "Masha Skorobogatov", "An empty line", "Jack Gleeson", "Coriolis force", "October 6, 2017", "Joudeh Al - Goudia family", "160km / hour between Delhi to Agra", "Escherichia coli", "Roman Reigns", "Andrea Brooks", "Audrey II", "Acid rain ''", "Kaley Christine Cuoco", "Quantitative psychological research", "Jules Shear", "Scott Schwartz", "1952", "Randy VanWarmer", "April 3, 1973", "manta rays and Scorpion fish", "Natural - language processing", "Detroit Tigers", "President Friedrich Ebert", "9.1 %", "Bruno Mars", "between the Mediterranean Sea to the north and the Red Sea in the south", "Lord's", "Kingsford, Michigan", "Honor\u00e9 Mirabeau", "Daniel Defoe", "1961", "venezuela", "First Street", "I write What I Like", "Princes Park", "cancerous tumor.", "Kyra and Violet,", "mpire of the Sun,\"", "\" Bulldog\"", "PetsHotels", "Carrie Underwood", "17-day odyssey"], "metric_results": {"EM": 0.5, "QA-F1": 0.6215665491216962}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.8648648648648648, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666665, 0.0, 1.0, 1.0, 0.0, 0.35294117647058826, 0.0, 0.56, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.28571428571428575, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.1818181818181818, 0.8571428571428571, 0.0, 1.0, 0.0, 0.4444444444444445, 0.9, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-3795", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-1552", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2229", "mrqa_naturalquestions-validation-1455", "mrqa_triviaqa-validation-7151", "mrqa_newsqa-validation-3380", "mrqa_newsqa-validation-3276", "mrqa_searchqa-validation-3943", "mrqa_triviaqa-validation-560"], "SR": 0.5, "CSR": 0.5368303571428572, "retrieved_ids": ["mrqa_squad-train-46458", "mrqa_squad-train-38540", "mrqa_squad-train-61243", "mrqa_squad-train-13974", "mrqa_squad-train-4881", "mrqa_squad-train-23874", "mrqa_squad-train-73710", "mrqa_squad-train-38342", "mrqa_squad-train-48436", "mrqa_squad-train-38715", "mrqa_squad-train-29994", "mrqa_squad-train-34713", "mrqa_squad-train-527", "mrqa_squad-train-64903", "mrqa_squad-train-30975", "mrqa_squad-train-64445", "mrqa_naturalquestions-validation-5538", "mrqa_searchqa-validation-7358", "mrqa_naturalquestions-validation-4387", "mrqa_squad-validation-448", "mrqa_triviaqa-validation-7226", "mrqa_searchqa-validation-14617", "mrqa_newsqa-validation-1809", "mrqa_naturalquestions-validation-5865", "mrqa_newsqa-validation-661", "mrqa_searchqa-validation-8084", "mrqa_hotpotqa-validation-5880", "mrqa_searchqa-validation-5008", "mrqa_hotpotqa-validation-2206", "mrqa_searchqa-validation-16205", "mrqa_squad-validation-7436", "mrqa_newsqa-validation-1519"], "EFR": 0.90625, "Overall": 0.7186941964285715}, {"timecode": 42, "before_eval_results": {"predictions": ["at least partly the product of a declining state of mind", "1861\u20131865", "private", "A player who can switch between playing shooting guard and small forward is known as a swingman", "703", "December 1974", "Sufism", "Capellini", "1614", "international association football competitions such as the FIFA World Cup, AFC Asian Cup and East Asian Football Championship", "German and American", "thirteen", "McComb, Mississippi", "Santa Fe", "Donald Duck", "Kew Gardens", "the northeastern part", "American Horror Story", "Chicago", "Free Range Films", "\"Cymbeline\"", "Vernon Smith", "Australia", "Frank Fertitta, Jr.", "October 16, 2015", "the Battelle Energy Alliance", "Kaep", "I write What I Like", "October 2016", "Sid Vicious", "51,271", "Sun Valley", "\"Guardians of the Galaxy\"", "burlesque", "Pulitzer Prize", "Scott Carson", "IFFHS World's Best Goalkeeper", "Suspiria", "singer, songwriter, actress", "Belladonna", "Russell T Davies", "\"Creed\"", "for his involvement in spot-fixing.", "Erich Schmidt-Leichner", "\"as-Sindib\u0101du al- Ba\u1e25riyy\"", "He has also performed in a number of operatic roles, including Alfred in \"Die Fledermaus\" by Johann Strauss, Sellem in Igor Stravinsky's \"The Rake's Progress\"", "Movie Masters", "6,241", "Hennepin County", "the Australian coast, primary products, consumer cargoes and extensive passenger services", "Adelaide Botanic Garden, Hutt Street, and Victoria Park", "Paul Newman", "Michael Buffer", "Presley Smith", "AFC Wimbledon", "Higgs", "Teppanyaki", "a share in the royalties for the tune.", "sumo wrestling", "\"The Real Housewives of Atlanta\"", "Antnio Guterres", "ratify the Constitution of the United States", "a letter", "Dairy Queen"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6554747596153846}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, false, true, true, false, false, false, true], "QA-F1": [0.6153846153846153, 0.0, 1.0, 0.125, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.16, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2523", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-3352", "mrqa_hotpotqa-validation-4614", "mrqa_hotpotqa-validation-4296", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-973", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5385", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-780", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-4201", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-2239", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-5245", "mrqa_naturalquestions-validation-5293", "mrqa_triviaqa-validation-3569", "mrqa_triviaqa-validation-748", "mrqa_newsqa-validation-2151", "mrqa_searchqa-validation-3012", "mrqa_searchqa-validation-732", "mrqa_searchqa-validation-11496"], "SR": 0.53125, "CSR": 0.5367005813953488, "retrieved_ids": ["mrqa_squad-train-20325", "mrqa_squad-train-32057", "mrqa_squad-train-19549", "mrqa_squad-train-40941", "mrqa_squad-train-66301", "mrqa_squad-train-41863", "mrqa_squad-train-28438", "mrqa_squad-train-15578", "mrqa_squad-train-21078", "mrqa_squad-train-71943", "mrqa_squad-train-30928", "mrqa_squad-train-66163", "mrqa_squad-train-68998", "mrqa_squad-train-16653", "mrqa_squad-train-37121", "mrqa_squad-train-27006", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-3876", "mrqa_newsqa-validation-434", "mrqa_naturalquestions-validation-2159", "mrqa_newsqa-validation-2755", "mrqa_searchqa-validation-10614", "mrqa_hotpotqa-validation-5547", "mrqa_naturalquestions-validation-2844", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-174", "mrqa_triviaqa-validation-6996", "mrqa_hotpotqa-validation-3576", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-3834", "mrqa_triviaqa-validation-1310"], "EFR": 1.0, "Overall": 0.7374182412790697}, {"timecode": 43, "before_eval_results": {"predictions": ["25 percent", "Evey", "1848 to 1852", "Adelaide", "the George Washington Bridge", "1996", "Kansas Cityizards", "Jacking", "February 5, 2015", "The Worm", "MGM Resorts International", "Sam Raimi", "StubHub Center", "1912", "balloons Street, Manchester", "Liverpool Bay", "St. Louis, Missouri", "Stephen King", "\"Seducing Mr. Perfect\"", "British Labour Party", "The Five", "Ang Lee", "Taylor Swift", "Objectivism", "\"Traumnovelle\" (\"Dream Story\")", "Mandarin", "lead female role of London Tipton", "KlingStubbins", "the Goddess of Pop", "Chevron Corporation", "Black Panther Party", "Baldwin", "John \"John\" Alexander Florence", "through YouTube", "Kohlberg K Travis Roberts", "Salzkammergut", "Rain Man", "feats of exploration", "video game", "Father Dougal McGuire", "1 million", "Mulberry", "London", "Subway restaurants", "cancer", "Campbellsville", "Mark Helfrich", "three", "Rickie Lee Skaggs", "Field Marshal Lord Gort", "Owsley Stanley", "Madeline Reeves", "16 seasons", "gas exchange", "the underground organization of the Irish Republican Brotherhood", "West Ham boss Allardyce", "gin", "almost 9 million", "vitamin injections that promise to improve health and beauty.", "to lose bouts,", "to gain exposure to a stock on which you are", "Oxford University Dramatic Society", "The Many Names of the Quintessential Balkan Dish", "Haiti"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6690352182539683}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.25, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 0.0, 0.3333333333333333, 1.0, 1.0, 0.4444444444444444, 1.0, 0.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-5487", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-5136", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-2770", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4257", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4239", "mrqa_hotpotqa-validation-3856", "mrqa_hotpotqa-validation-4501", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-992", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-8767", "mrqa_triviaqa-validation-2810", "mrqa_triviaqa-validation-5974", "mrqa_newsqa-validation-3325", "mrqa_searchqa-validation-5457", "mrqa_searchqa-validation-10176", "mrqa_searchqa-validation-14771"], "SR": 0.53125, "CSR": 0.5365767045454546, "retrieved_ids": ["mrqa_squad-train-22276", "mrqa_squad-train-37455", "mrqa_squad-train-73996", "mrqa_squad-train-12879", "mrqa_squad-train-3321", "mrqa_squad-train-32051", "mrqa_squad-train-39561", "mrqa_squad-train-70516", "mrqa_squad-train-65829", "mrqa_squad-train-69703", "mrqa_squad-train-52960", "mrqa_squad-train-36339", "mrqa_squad-train-2940", "mrqa_squad-train-57602", "mrqa_squad-train-42299", "mrqa_squad-train-5934", "mrqa_searchqa-validation-2183", "mrqa_hotpotqa-validation-5346", "mrqa_triviaqa-validation-2047", "mrqa_searchqa-validation-8363", "mrqa_newsqa-validation-3838", "mrqa_searchqa-validation-9803", "mrqa_searchqa-validation-15830", "mrqa_searchqa-validation-16220", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-1301", "mrqa_naturalquestions-validation-2844", "mrqa_triviaqa-validation-336", "mrqa_searchqa-validation-3606", "mrqa_newsqa-validation-1123", "mrqa_searchqa-validation-16829", "mrqa_hotpotqa-validation-671"], "EFR": 1.0, "Overall": 0.7373934659090909}, {"timecode": 44, "before_eval_results": {"predictions": ["broken arm", "Mexico", "Wyoming", "Franklin D. Roosevelt", "zero", "Copenhagen", "Dinner with Friends", "the 1904 Summer Olympics", "New Zealand", "Follies", "The Honeymooners", "pumpkin seeds", "enamel", "Macy's", "method acting", "Sam Kinison", "If the president dies", "Roman Empire", "Alaska", "Matt Leinart", "Kublai Khan", "Jeremy Bentham", "Republic of Korea", "the Mekong", "King Neptune", "the 1984 Summer Olympics", "R Is For", "Fiddler on the Roof", "gas", "the Danforth Foundation", "Ivory Coast", "The Lord of the Rings: The Return of the King", "Birch", "Alanis Morissette", "a tie", "Simple Syrup", "Daedalus", "Emma Peel", "Schindler", "King Henry VIII", "Stephen Crane", "Mississippi", "Mail", "he leaves Oz in a balloon", "the Madding Crowd", "Steely Dan", "Linda Tripp", "the Sierra Nevada", "Ali", "adios", "the Gadsden Treaty", "New Zealand to New Guinea", "Alexa Bliss", "the empire begins to enter decline and instability", "the Marshall Plan", "Dick Turpin", "black", "15", "The Deep Blue Sea", "Mary-Kay Wilmers", "the shipping industry -- responsible for 5% of global greenhouse gas emissions, according to the United Nations -- embraces this technology the same way the public has.\"", "Hundreds", "2008,", "three"], "metric_results": {"EM": 0.515625, "QA-F1": 0.590625}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7525", "mrqa_searchqa-validation-3120", "mrqa_searchqa-validation-16746", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-14186", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-508", "mrqa_searchqa-validation-12098", "mrqa_searchqa-validation-15476", "mrqa_searchqa-validation-8837", "mrqa_searchqa-validation-15203", "mrqa_searchqa-validation-3792", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-2669", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-16207", "mrqa_searchqa-validation-540", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-13328", "mrqa_searchqa-validation-14848", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-1399", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-11005", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-8568", "mrqa_naturalquestions-validation-6157", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-5093", "mrqa_newsqa-validation-3979"], "SR": 0.515625, "CSR": 0.5361111111111111, "retrieved_ids": ["mrqa_squad-train-62476", "mrqa_squad-train-78027", "mrqa_squad-train-20185", "mrqa_squad-train-24274", "mrqa_squad-train-53017", "mrqa_squad-train-16684", "mrqa_squad-train-69744", "mrqa_squad-train-69983", "mrqa_squad-train-61720", "mrqa_squad-train-59819", "mrqa_squad-train-85903", "mrqa_squad-train-17406", "mrqa_squad-train-26480", "mrqa_squad-train-52207", "mrqa_squad-train-2851", "mrqa_squad-train-55951", "mrqa_naturalquestions-validation-6849", "mrqa_squad-validation-9436", "mrqa_hotpotqa-validation-4855", "mrqa_newsqa-validation-3020", "mrqa_naturalquestions-validation-5537", "mrqa_searchqa-validation-3032", "mrqa_hotpotqa-validation-5523", "mrqa_newsqa-validation-3940", "mrqa_squad-validation-10128", "mrqa_newsqa-validation-3006", "mrqa_searchqa-validation-11410", "mrqa_triviaqa-validation-6381", "mrqa_newsqa-validation-499", "mrqa_hotpotqa-validation-3971", "mrqa_naturalquestions-validation-886", "mrqa_squad-validation-10386"], "EFR": 1.0, "Overall": 0.7373003472222222}, {"timecode": 45, "before_eval_results": {"predictions": ["the Lunar Roving Vehicle (LRV)", "the leader of a drug cartel that set off two grenades during a public celebration in September,", "Robert", "Too many glass shards", "Sunday.", "\"including taking any and all appropriate personnel actions including termination, discipline and referral of any wrongdoing for criminal prosecution.\"", "Graham,", "Islamabad", "Jackson was battling a potentially fatal disease that required a life-saving lung transplant,", "President Thabo Mbeki", "\"project work\"", "Siemionow", "12 hours in jail.", "the United States, NATO member states, Russia and India", "Obama chief of staff and the Obama people", "$273 million", "Wigan Athletic in northern England.", "\"The most visible, most exciting family in America is this beautiful black family and so people are ready and looking for those kinds of images,\"", "43,000", "former U.S. secretary of state.", "helped finance the insurgency against U.S. troops in Iraq with Iraqi funds he transferred to Syria before Hussein's government collapsed in April 2002.", "Brad Blauser,", "U.S. President-elect Barack Obama", "more than 1.2 million people.", "North Korea", "a bookish intellectual who's cool in a crisis and quick on his feet, like Ken Jennings with a shot of adrenaline.", "Angela Merkel", "within a few months,\"", "Passers-by", "British troops in Iraq are being pulled out because the agreement that allows them to be there expires on Friday,", "Lance Cpl. Maria Lauterbach", "Picasso's muse and mistress, Marie-Therese Walter.", "80,", "Polo", "bartering", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.\"", "fluoroquinolones", "Uzbekistan.", "The Ethiopian army's answer to the rebels has been to viciously attack civilians in the Ogaden,\"", "his comments", "in the northwestern province of Antioquia,", "a student who admitted to hanging a noose in a campus library,", "to make space for two ocean wind farms -- taking up 2 percent of the state's waters -- without angering fishing industries, killing whales or harming ecosystems.", "Asian qualifying Group 2", "247", "Six", "flooding and debris", "then-Sen. Obama", "Unseeded Frenchwoman Aravane Rezai", "Al-Shabaab,", "201", "IBM", "Gorakhpur", "HM Chief Inspector of Prisons", "Charlie Cairoli", "Sweden", "2017", "Edmund Ironside", "Comme des Gar\u00e7ons", "Roosevelt", "Romeo", "the Shang dynasty", "Germany"], "metric_results": {"EM": 0.3125, "QA-F1": 0.46361864408739406}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, true, true, true, false, true, false, false, false, true], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.06666666666666667, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.36363636363636365, 1.0, 0.5714285714285715, 0.2222222222222222, 0.0, 1.0, 0.3076923076923077, 0.0, 0.4, 0.5714285714285715, 1.0, 0.0, 1.0, 0.4, 1.0, 0.1, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.0, 0.5, 0.07407407407407407, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.25, 0.6666666666666666, 0.5, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4027", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-182", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-1848", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-1257", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-3285", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-1863", "mrqa_triviaqa-validation-4496", "mrqa_hotpotqa-validation-3844", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-11067", "mrqa_searchqa-validation-4478"], "SR": 0.3125, "CSR": 0.53125, "retrieved_ids": ["mrqa_squad-train-21551", "mrqa_squad-train-83079", "mrqa_squad-train-82854", "mrqa_squad-train-38252", "mrqa_squad-train-5908", "mrqa_squad-train-13447", "mrqa_squad-train-50554", "mrqa_squad-train-61596", "mrqa_squad-train-72726", "mrqa_squad-train-5030", "mrqa_squad-train-36805", "mrqa_squad-train-58998", "mrqa_squad-train-54054", "mrqa_squad-train-69196", "mrqa_squad-train-35279", "mrqa_squad-train-22930", "mrqa_squad-validation-8386", "mrqa_hotpotqa-validation-2210", "mrqa_triviaqa-validation-1387", "mrqa_squad-validation-8066", "mrqa_searchqa-validation-3943", "mrqa_naturalquestions-validation-2201", "mrqa_searchqa-validation-5038", "mrqa_hotpotqa-validation-1580", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-3678", "mrqa_searchqa-validation-8417", "mrqa_newsqa-validation-2244", "mrqa_searchqa-validation-13463", "mrqa_hotpotqa-validation-3672", "mrqa_naturalquestions-validation-9172", "mrqa_triviaqa-validation-7737"], "EFR": 1.0, "Overall": 0.736328125}, {"timecode": 46, "before_eval_results": {"predictions": ["Thames River", "an independent homeland for the country's ethnic", "Turkish President Abdullah Gul", "Facebook", "received no reports", "in Fayetteville, North Carolina,", "\"surge\" strategy he implemented last year.\"", "Christopher Savoie", "work together to stabilize Somalia and cooperate in security and military operations.", "not guilty", "not guilty by reason of insanity", "\"You can go from rags to riches there.", "United States", "Manny Pacquiao", "Garth Brooks", "The son of Gabon's former president", "super-yacht designers Wally", "a body", "Prince George's County Correctional Center", "The train in front had stopped", "Karen Floyd", "welcomed President Obama with huge excitement Monday.", "1983.", "$20 million to $30 million,", "Daryeel Bulasho Guud", "Zulfikar Ali Bhutto,", "the insurgency,", "Stoke City.", "Raymond Soeoth,", "three different videos", "a national telephone survey of more than 78,000 parents", "is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.\"", "his past and his future", "Omar", "travel in cars with tinted windows", "mild to moderate depression", "recite her poetry", "economic growth", "Mitt Romney", "\"The Sopranos,\"", "The Louvre", "Jason Chaffetz", "a delegation of American Muslim and Christian leaders", "the legitimacy of that race.", "Australian officials", "misdemeanor", "$10 billion", "\"procedure on her heart,\"", "safety issues in the company's cars", "Newcastle", "54", "1 mile", "Abanindranath Tagore CIE", "1996", "Kiri Te Kanawa", "Submarine Sunk", "1960's", "the onset and progression of Alzheimer's disease", "The Future", "1770", "Jacob and Wilhelm Grimm", "Seasons In The Sun", "Geraldine Farrar", "Michael Harney"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7028164543789545}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 0.05, 0.9333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-851", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-2547", "mrqa_newsqa-validation-247", "mrqa_naturalquestions-validation-8995", "mrqa_triviaqa-validation-4212", "mrqa_hotpotqa-validation-5485", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-13873", "mrqa_naturalquestions-validation-3802"], "SR": 0.59375, "CSR": 0.5325797872340425, "retrieved_ids": ["mrqa_squad-train-19766", "mrqa_squad-train-73285", "mrqa_squad-train-17587", "mrqa_squad-train-66539", "mrqa_squad-train-11873", "mrqa_squad-train-28609", "mrqa_squad-train-83924", "mrqa_squad-train-36546", "mrqa_squad-train-30341", "mrqa_squad-train-64329", "mrqa_squad-train-10030", "mrqa_squad-train-21517", "mrqa_squad-train-76690", "mrqa_squad-train-28968", "mrqa_squad-train-21980", "mrqa_squad-train-19161", "mrqa_newsqa-validation-297", "mrqa_triviaqa-validation-5116", "mrqa_triviaqa-validation-5974", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-9568", "mrqa_searchqa-validation-12184", "mrqa_hotpotqa-validation-2741", "mrqa_triviaqa-validation-4982", "mrqa_hotpotqa-validation-2239", "mrqa_newsqa-validation-3973", "mrqa_squad-validation-2704", "mrqa_hotpotqa-validation-4937", "mrqa_squad-validation-3479", "mrqa_squad-validation-3319", "mrqa_newsqa-validation-1918"], "EFR": 1.0, "Overall": 0.7365940824468085}, {"timecode": 47, "before_eval_results": {"predictions": ["a waffles", "Clinton", "Harry Potter", "a stagecoach", "Cyrillic", "a fruitcake", "the Charleston", "shiatsu", "Siegfried", "taxonomy", "Anne Rice", "Martin Landau", "mantle", "second marriage", "Lady Jane Grey", "Santera", "the Duggar family", "the Lincoln penny", "Rookwood", "optimistic", "Belarus", "Airplane", "French toast", "the gingerbread", "milk", "Sex Pistols", "Samuel Johnson", "Agatha Christie", "Jack Dempsey", "conglomerare", "a broody hen", "Edison", "Mount Everest", "black-eyed pea dip", "Giacomo Puccini", "Battlestar Galactica", "Yugoslavia", "the Surgeon General", "a place bet", "Voyna i mir", "Frank Lloyd Wright", "Falcon Crest", "William the Conqueror", "Grant and Sherman", "Adam Smith", "a yeast", "Pearl S. Buck", "(Victor) Hugo", "Buckhead", "Mona Lisa", "Sisyphus", "2017 season", "Nathan Hale", "the town of Carcassonne in Aude, France", "John F. Kennedy", "bacteria", "appett\\'s Charge", "Samuel M \"Sam\" Raimi", "1940s and 1950s", "Sleepy Brown", "because the Indians were gathering information about the rebels to give to the Colombian military.", "a judge to order the pop star's estate to pay him a monthly allowance,", "ALS6", "Stevie Wonder"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6338158700980392}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.125, 0.7058823529411764, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12474", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-4988", "mrqa_searchqa-validation-7903", "mrqa_searchqa-validation-6879", "mrqa_searchqa-validation-482", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-9457", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-10067", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-1056", "mrqa_searchqa-validation-16434", "mrqa_searchqa-validation-11677", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-9587", "mrqa_searchqa-validation-15591", "mrqa_searchqa-validation-2263", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-14969", "mrqa_searchqa-validation-10394", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-1015", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-6789", "mrqa_hotpotqa-validation-491", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1952"], "SR": 0.53125, "CSR": 0.5325520833333333, "retrieved_ids": ["mrqa_squad-train-28027", "mrqa_squad-train-19680", "mrqa_squad-train-24058", "mrqa_squad-train-81194", "mrqa_squad-train-57053", "mrqa_squad-train-10176", "mrqa_squad-train-67837", "mrqa_squad-train-43115", "mrqa_squad-train-57780", "mrqa_squad-train-3495", "mrqa_squad-train-85679", "mrqa_squad-train-46920", "mrqa_squad-train-77432", "mrqa_squad-train-65230", "mrqa_squad-train-79712", "mrqa_squad-train-70098", "mrqa_newsqa-validation-1384", "mrqa_triviaqa-validation-1310", "mrqa_naturalquestions-validation-6200", "mrqa_hotpotqa-validation-866", "mrqa_searchqa-validation-4804", "mrqa_hotpotqa-validation-5180", "mrqa_searchqa-validation-14771", "mrqa_hotpotqa-validation-405", "mrqa_newsqa-validation-3022", "mrqa_hotpotqa-validation-3587", "mrqa_newsqa-validation-1765", "mrqa_squad-validation-3610", "mrqa_searchqa-validation-14767", "mrqa_naturalquestions-validation-1953", "mrqa_triviaqa-validation-4957", "mrqa_squad-validation-8990"], "EFR": 1.0, "Overall": 0.7365885416666667}, {"timecode": 48, "before_eval_results": {"predictions": ["Argentine composer Lalo Schifrin", "September 19, 1977", "American", "late January or early February", "1995 Mitsubishi Eclipse - Shot at by Johnny Tran", "John Adams of Massachusetts, Benjamin Franklin of Pennsylvania, Thomas Jefferson of Virginia, Robert R. Livingston of New York, and Roger Sherman of Connecticut", "the Ute name for them, k\u0268mantsi ( enemy )", "the sixth series", "January 2004", "season two", "in the 1970s", "above ground", "Phosphorus pentoxide", "the Student League for Industrial Democracy ( SLID )", "Donald Fauntleroy Duck", "the New Testament", "The Geography of Oklahoma", "an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner", "Michael Phelps", "British", "22 \u00b0 00 \u2032 N 80 \u00b0 00\u2032 W", "Martin Lawrence", "1986", "1932", "Sophocles", "BBC One on Saturday evenings", "the American Revolutionary War", "The Abbott and Costello Show", "2009 model year", "Mohammad Reza Pahlavi", "Saint Alphonsa", "Billie Jean King", "the Battle of Antietam and Lincoln's Emancipation Proclamation", "C\u03bc and C\u03b4 )", "Daya", "the four - letter suffix", "the Jews rejected it", "Andy", "the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "pools campaign contributions from members and donates those funds to campaign for or against candidates, ballot initiatives, or legislation", "Article 368", "eight episode series", "Julie Deborah Kavner", "September 19, 2017", "May 2002", "Chelsea", "San Antonio, Texas", "Homer Banks, Carl Hampton and Raymond Jackson", "Omar Khayyam", "1984", "Joseph Nye Welch", "the B section", "Puck", "Harrods", "Adolfo Rodr\u00edguez Sa\u00e1", "Tool", "seven years", "a satellite.", "Ali Bongo", "motorbike accident.", "The Sisters Rosensweig", "Cops", "Vatican", "Timothy Dalton"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5727483303429027}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.5000000000000001, 0.5454545454545454, 0.16, 0.8, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.125, 0.6666666666666666, 0.0, 0.8421052631578948, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-156", "mrqa_naturalquestions-validation-9098", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-74", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-3122", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-9281", "mrqa_naturalquestions-validation-3436", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-1449", "mrqa_naturalquestions-validation-4796", "mrqa_triviaqa-validation-6730", "mrqa_hotpotqa-validation-2441", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-2167", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4956"], "SR": 0.40625, "CSR": 0.5299744897959184, "retrieved_ids": ["mrqa_squad-train-75318", "mrqa_squad-train-50642", "mrqa_squad-train-73507", "mrqa_squad-train-80661", "mrqa_squad-train-30971", "mrqa_squad-train-19097", "mrqa_squad-train-25609", "mrqa_squad-train-50503", "mrqa_squad-train-998", "mrqa_squad-train-44764", "mrqa_squad-train-73445", "mrqa_squad-train-53079", "mrqa_squad-train-82373", "mrqa_squad-train-4923", "mrqa_squad-train-50600", "mrqa_squad-train-19349", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-3862", "mrqa_squad-validation-9195", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-1744", "mrqa_hotpotqa-validation-5245", "mrqa_naturalquestions-validation-5293", "mrqa_hotpotqa-validation-4296", "mrqa_newsqa-validation-2031", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-2486", "mrqa_searchqa-validation-2914", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2206", "mrqa_triviaqa-validation-1291", "mrqa_newsqa-validation-1990"], "EFR": 0.9473684210526315, "Overall": 0.72554670716971}, {"timecode": 49, "before_eval_results": {"predictions": ["Louisa May Alcott", "Davy Crockett", "Sugar Ray", "Suspicious Minds", "Jupiter", "Newspaper", "the gift of the Magi", "LPGA", "Henry VIII", "Corpulent", "Kate Chopin", "Copacabana", "Krakow", "Daredevil", "Tunis", "beryl", "DOP", "Mauna Loa", "John Lennon", "Macy\\'s", "\"Sonny\" Corleone", "Fred Claus", "the Philosopher's Stone", "the Kremlin", "the raven", "the New York Cosmos", "Richard Phillips Feynman", "John Grisham", "PET", "Catherine the Great", "Nixon", "Eleanor Roosevelt", "arthritis", "the Hermitage Museum", "Crispix", "Sabatini", "St. Louis", "yesterday Henry loved me", "the Caspian tern", "the Beagle", "the Autry Museum of the American West", "Wing-T", "Mindanao", "Henry Hudson", "a diamond", "Roger Brooke Taney", "the United Nations", "stalking", "The Unbearable Lightness of Being", "the Appian Way", "Joseph", "`` Nearer, My God, to Thee ''", "marriage officiant", "1 October 2006", "a financial trader turned Internet entrepreneur", "Kenny Everett", "Eminem", "Jack White", "Isabella II", "villanelle", "Revolutionary Armed Forces of Colombia,", "Oaxaca, Mexico", "$5.5 billion", "will \"apologize for his behavior\" Friday when he makes a statement at PGA headquarters in Ponte Vedra Beach, Florida,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6318824404761905}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, false, true, false, true, false, true, false, true, true, false, false, true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 0.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.25, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6521", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-9168", "mrqa_searchqa-validation-14716", "mrqa_searchqa-validation-8091", "mrqa_searchqa-validation-1620", "mrqa_searchqa-validation-14204", "mrqa_searchqa-validation-4798", "mrqa_searchqa-validation-2632", "mrqa_searchqa-validation-1000", "mrqa_searchqa-validation-14052", "mrqa_searchqa-validation-14906", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-12629", "mrqa_searchqa-validation-944", "mrqa_searchqa-validation-15908", "mrqa_searchqa-validation-16894", "mrqa_searchqa-validation-1953", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-16642", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-683", "mrqa_searchqa-validation-15730", "mrqa_searchqa-validation-2231", "mrqa_naturalquestions-validation-8217", "mrqa_naturalquestions-validation-1285", "mrqa_triviaqa-validation-2643", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3761"], "SR": 0.515625, "CSR": 0.5296875, "retrieved_ids": ["mrqa_squad-train-64693", "mrqa_squad-train-35731", "mrqa_squad-train-46687", "mrqa_squad-train-32122", "mrqa_squad-train-58762", "mrqa_squad-train-76539", "mrqa_squad-train-19525", "mrqa_squad-train-83076", "mrqa_squad-train-83134", "mrqa_squad-train-70783", "mrqa_squad-train-73747", "mrqa_squad-train-27847", "mrqa_squad-train-77114", "mrqa_squad-train-55115", "mrqa_squad-train-10140", "mrqa_squad-train-50334", "mrqa_newsqa-validation-544", "mrqa_triviaqa-validation-5856", "mrqa_triviaqa-validation-3634", "mrqa_newsqa-validation-3429", "mrqa_hotpotqa-validation-3452", "mrqa_searchqa-validation-7996", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-5928", "mrqa_searchqa-validation-10199", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-2467", "mrqa_hotpotqa-validation-4257", "mrqa_squad-validation-6717", "mrqa_searchqa-validation-15830", "mrqa_newsqa-validation-247", "mrqa_squad-validation-8386"], "EFR": 0.967741935483871, "Overall": 0.7295640120967742}, {"timecode": 50, "UKR": 0.759765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1003", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1417", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1604", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-2225", "mrqa_hotpotqa-validation-2302", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2458", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-2739", "mrqa_hotpotqa-validation-2766", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-310", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3767", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4744", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4999", "mrqa_hotpotqa-validation-5038", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5317", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-805", "mrqa_hotpotqa-validation-870", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3062", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3757", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3795", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9098", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9557", "mrqa_naturalquestions-validation-9778", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1610", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1818", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2470", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3286", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3521", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3676", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-973", "mrqa_searchqa-validation-1000", "mrqa_searchqa-validation-10210", "mrqa_searchqa-validation-10394", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-10752", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-10955", "mrqa_searchqa-validation-10981", "mrqa_searchqa-validation-11232", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-11973", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-12474", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12698", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-12959", "mrqa_searchqa-validation-13328", "mrqa_searchqa-validation-13740", "mrqa_searchqa-validation-1399", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15203", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-15839", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-16894", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-3098", "mrqa_searchqa-validation-3295", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-4567", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5521", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-5698", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-6659", "mrqa_searchqa-validation-6765", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7287", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-7903", "mrqa_searchqa-validation-7989", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8176", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-8995", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9683", "mrqa_searchqa-validation-9698", "mrqa_searchqa-validation-9803", "mrqa_squad-validation-100", "mrqa_squad-validation-10370", "mrqa_squad-validation-10457", "mrqa_squad-validation-12", "mrqa_squad-validation-1254", "mrqa_squad-validation-1330", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1540", "mrqa_squad-validation-1586", "mrqa_squad-validation-1613", "mrqa_squad-validation-175", "mrqa_squad-validation-185", "mrqa_squad-validation-1902", "mrqa_squad-validation-1920", "mrqa_squad-validation-194", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-2523", "mrqa_squad-validation-2640", "mrqa_squad-validation-270", "mrqa_squad-validation-2762", "mrqa_squad-validation-2916", "mrqa_squad-validation-2986", "mrqa_squad-validation-3037", "mrqa_squad-validation-3125", "mrqa_squad-validation-3319", "mrqa_squad-validation-3511", "mrqa_squad-validation-3545", "mrqa_squad-validation-3708", "mrqa_squad-validation-3770", "mrqa_squad-validation-3811", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3934", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-448", "mrqa_squad-validation-4546", "mrqa_squad-validation-4671", "mrqa_squad-validation-4750", "mrqa_squad-validation-4860", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5050", "mrqa_squad-validation-5135", "mrqa_squad-validation-5276", "mrqa_squad-validation-5311", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5474", "mrqa_squad-validation-5669", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5736", "mrqa_squad-validation-5751", "mrqa_squad-validation-6029", "mrqa_squad-validation-6351", "mrqa_squad-validation-6481", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6630", "mrqa_squad-validation-7023", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7531", "mrqa_squad-validation-76", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7689", "mrqa_squad-validation-7763", "mrqa_squad-validation-7932", "mrqa_squad-validation-8066", "mrqa_squad-validation-8209", "mrqa_squad-validation-8253", "mrqa_squad-validation-8386", "mrqa_squad-validation-8488", "mrqa_squad-validation-8754", "mrqa_squad-validation-8772", "mrqa_squad-validation-8931", "mrqa_squad-validation-8948", "mrqa_squad-validation-9012", "mrqa_squad-validation-9322", "mrqa_squad-validation-9495", "mrqa_squad-validation-9506", "mrqa_squad-validation-955", "mrqa_squad-validation-9590", "mrqa_squad-validation-9819", "mrqa_squad-validation-9868", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1291", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-30", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-4949", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6579", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6724", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-7601", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-895"], "OKR": 0.853515625, "KG": 0.47421875, "before_eval_results": {"predictions": ["collusion between the colossus of the North [the United States] and the col Colossus of the South [Brazil]\"", "recall", "American", "helicopters and unmanned aerial vehicles", "2-1", "President Obama", "Adam Lambert and Kris Allen", "Arabic, French and English", "Saturday", "Python", "a skilled hacker could disrupt the system and cause a blackout.", "a Starbucks", "a violent government crackdown seeped out.\"", "the former Massachusetts governor in an ad Sunday in Iowa's The Des Moines Register newspaper.", "Knox's parents", "was criticized for saying Chaudhary's death was warning to management.", "her fianc\u00e9,", "\"The techniques they used were all authorized, but the manner in which they applied them was overly aggressive and too persistent,\"", "a woman", "Haiti.", "228", "to Nazi Germany", "say these planning processes are urgently needed and have been a long time in coming.", "J.Crew outfits", "India", "2009", "President Obama", "At least 38 people", "for death squad killings carried out during his rule in the 1990s.", "Rod Blagojevich", "10", "Dr. Maria Siemionow,", "Tom Hanks", "the front-seat passenger in the Mercedes that carried Diana, her boyfriend, Dodi Fayed, and their driver, Henri Paul.", "forgery and flying without a valid license,", "14", "Britain.", "Dog patch Labs", "two", "is pretty much resolved,\"", "16", "\"bystander effect\":", "Tom Hanks", "received no reports from pilots in the air of any sightings", "\"Empire of the Sun,\"", "the United States", "2002.", "Mubarak,", "15", "three", "London's 20,000-capacity O2 Arena.\"", "Taiwan", "the inner core and growing bud", "Turducken", "Blanche Barrow", "'reactive to far ambush'", "latte", "wineries", "Belgian", "Guthred", "soothsayer", "Campbell's soup", "Lotte New York Palace", "Fringillidae"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5236583409280777}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false, true, false, false, true, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.16666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.2105263157894737, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.2857142857142857, 1.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0909090909090909, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3870", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-2561", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-3629", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-2958", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-914", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3651", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-5242", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-1389", "mrqa_hotpotqa-validation-471", "mrqa_searchqa-validation-15063", "mrqa_triviaqa-validation-4377"], "SR": 0.453125, "CSR": 0.5281862745098039, "retrieved_ids": ["mrqa_squad-train-17967", "mrqa_squad-train-17535", "mrqa_squad-train-60339", "mrqa_squad-train-83024", "mrqa_squad-train-70559", "mrqa_squad-train-79495", "mrqa_squad-train-21548", "mrqa_squad-train-51467", "mrqa_squad-train-51596", "mrqa_squad-train-42889", "mrqa_squad-train-66727", "mrqa_squad-train-49991", "mrqa_squad-train-61625", "mrqa_squad-train-5928", "mrqa_squad-train-12632", "mrqa_squad-train-58774", "mrqa_searchqa-validation-7996", "mrqa_squad-validation-2468", "mrqa_triviaqa-validation-5474", "mrqa_newsqa-validation-1947", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-7017", "mrqa_triviaqa-validation-4212", "mrqa_hotpotqa-validation-3971", "mrqa_newsqa-validation-1671", "mrqa_hotpotqa-validation-4642", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-6789", "mrqa_hotpotqa-validation-1980", "mrqa_triviaqa-validation-6520", "mrqa_naturalquestions-validation-9494"], "EFR": 1.0, "Overall": 0.7231372549019608}, {"timecode": 51, "before_eval_results": {"predictions": ["Thursday night,", "a judge to order the pop star's estate", "Red Lines,\"", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military.", "Herman Thomas", "\"bleaching\" in which algae living in the coral die and leave behind whitened skeletons.", "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,", "2,000 euros ($2,963)", "Christopher Savoie", "learn in safer surroundings.", "eight-week", "\"Oprah is an angel, she is God-sent,\"", "D.J. Knight of Pearlman, Texas,", "Grayback Forestry in Medford, Oregon,", "January", "Goa", "A member of the group dubbed the \"Jena 6\"", "the IV cafe.", "Venus Williams", "two", "Monday", "Seoul", "Bikini Atoll", "collaborating with the Colombian government,", "trying to cover up the truth behind her daughter's murder,", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "in a basement,", "Pakistan", "Charles Lock", "\"This is not something that anybody can reasonably anticipate,\"", "Somali-based", "gun charges", "\"17 Again,\"", "eight", "\"We need a president who understands the world today, the future we seek and the change we need.", "helping on the sandbags", "to sniff out cell phones.", "28", "41,", "a \" happy ending\" to the case.", "Dharamsala, India.", "Liza Murphy,", "Arsene Wenger", "four months ago,", "India", "that Birnbaum had resigned \"on her own terms and own volition.\"", "hundreds", "photos", "Arizona", "raping and killing a 14-year-old Iraqi girl.", "1620", "Season 4", "Louis Mountbatten", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "dromedary", "faggot", "chocolate", "a creek", "2007", "Aldosterone", "built", "presidential nomination", "Animal Crackers", "eastern Ethiopia"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5151099768287268}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0909090909090909, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7692307692307693, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.125, 0.6250000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3076923076923077, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3224", "mrqa_newsqa-validation-1545", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-1188", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-2985", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-784", "mrqa_newsqa-validation-475", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-5236", "mrqa_naturalquestions-validation-6874", "mrqa_triviaqa-validation-827", "mrqa_triviaqa-validation-1488", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-397", "mrqa_searchqa-validation-6635", "mrqa_searchqa-validation-7781", "mrqa_triviaqa-validation-4912"], "SR": 0.4375, "CSR": 0.5264423076923077, "retrieved_ids": ["mrqa_squad-train-38135", "mrqa_squad-train-50757", "mrqa_squad-train-47677", "mrqa_squad-train-68256", "mrqa_squad-train-40509", "mrqa_squad-train-25412", "mrqa_squad-train-29638", "mrqa_squad-train-8460", "mrqa_squad-train-5862", "mrqa_squad-train-53543", "mrqa_squad-train-18434", "mrqa_squad-train-74222", "mrqa_squad-train-43616", "mrqa_squad-train-24403", "mrqa_squad-train-76922", "mrqa_squad-train-65648", "mrqa_hotpotqa-validation-1874", "mrqa_newsqa-validation-3022", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-1379", "mrqa_searchqa-validation-16246", "mrqa_newsqa-validation-2001", "mrqa_squad-validation-2523", "mrqa_hotpotqa-validation-4508", "mrqa_searchqa-validation-14186", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-6670", "mrqa_squad-validation-133", "mrqa_searchqa-validation-7094", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-5856", "mrqa_newsqa-validation-2695"], "EFR": 1.0, "Overall": 0.7227884615384614}, {"timecode": 52, "before_eval_results": {"predictions": ["Pope Paul Biya", "mild to moderate depression", "Summer", "\"I reject this course because it sets goals that are beyond what we can achieve at a reasonable cost,", "VBS.TV", "children of street cleaners and firefighters.", "Switzerland", "2009", "Tim Clark, Matt Kuchar and Bubba Watson", "Araceli Valencia,", "400 farmers", "Friday,", "Lisa Brown", "Zimbabwe's main opposition party", "five", "planned attacks", "\" Teen Patti\"", "Seasons of My Heart", "a pool of blood beneath his head.", "improve the environment", "Jared Polis", "her dancing against a stripper's pole.", "bragging about his sex life on television", "Adam Sandler, Bill Murray, Chevy Chase", "12.3 million", "almost 30 tunnels,", "English", "Arkansas,", "Millvina Dean,", "a pure meritocracy,", "Bill Haas", "\"I'm certainly not nearly as good of a speaker as he is.\"", "Tuesday in Los Angeles.", "after giving birth to baby daughter Jada,", "co-writing credits", "sailing", "Jeffrey Jamaleldine", "2.5 million", "the state's first lady,", "Somali President Sheikh Sharif Sheikh Ahmed", "NATO fighters", "U.S. Vice President Dick Cheney", "5:20 p.m.", "Former Beatles", "The Rosie Show", "southwestern Mexico,", "helicopters and unmanned aerial vehicles", "a \"prostitute\"", "Hanin Zoabi", "two", "April 2010.", "1985", "1599", "`` Seeing Red ''", "western", "PPTH", "different levels of importance of human psychological and physical needs", "the Dutch Empire", "1692", "Cushman", "a bobtail squid", "imperfect", "Whitewater", "Drums"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6890090811965812}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, false, false, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.3076923076923077, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1590", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-2821", "mrqa_naturalquestions-validation-6545", "mrqa_triviaqa-validation-934", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-2536", "mrqa_searchqa-validation-14214", "mrqa_searchqa-validation-9278"], "SR": 0.609375, "CSR": 0.5280070754716981, "retrieved_ids": ["mrqa_squad-train-34070", "mrqa_squad-train-20150", "mrqa_squad-train-55269", "mrqa_squad-train-68097", "mrqa_squad-train-51409", "mrqa_squad-train-51674", "mrqa_squad-train-51120", "mrqa_squad-train-65931", "mrqa_squad-train-72160", "mrqa_squad-train-50905", "mrqa_squad-train-8598", "mrqa_squad-train-6675", "mrqa_squad-train-11813", "mrqa_squad-train-83735", "mrqa_squad-train-59536", "mrqa_squad-train-35213", "mrqa_naturalquestions-validation-2503", "mrqa_searchqa-validation-12784", "mrqa_squad-validation-10174", "mrqa_newsqa-validation-3753", "mrqa_searchqa-validation-14596", "mrqa_triviaqa-validation-6246", "mrqa_newsqa-validation-1598", "mrqa_searchqa-validation-7791", "mrqa_triviaqa-validation-7737", "mrqa_naturalquestions-validation-4387", "mrqa_searchqa-validation-5038", "mrqa_hotpotqa-validation-2728", "mrqa_newsqa-validation-3441", "mrqa_squad-validation-2987", "mrqa_hotpotqa-validation-3304", "mrqa_newsqa-validation-2701"], "EFR": 1.0, "Overall": 0.7231014150943396}, {"timecode": 53, "before_eval_results": {"predictions": ["anvil launching", "Abigail", "Encore", "David Jolly", "9 November 1955", "Ch\u014dfu, Tokyo, Japan", "Adrian Peter McLaren", "Anthony John Herrera", "2008\u201309", "Lev Ivanovich Yashin", "Lu\u00eds Carlos Almeida da Cunha,", "Mot\u00f6rhead", "Solace", "1943", "Vivendi S.A.", "Wings of Desire", "May 27, 2016", "Super Bowl XXIX", "Rikki Farr", "Hudson Bay Mining and Smelting Company", "Bourbon County", "100 metres", "Acela Express", "5249", "Johnnie Ray", "Harry Booth", "John McClane", "James Packer", "black nationalism", "USC Marshall School of Business", "Saint Motel", "You're Next", "eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music", "Christian", "Fife", "Peter 'Drago' Sell", "Marktown", "Hidden America with Jonah Ray", "Bangkok", "Gregg Popovich", "Roy Spencer", "Chicago", "constant support from propaganda campaigns", "2012", "torpedoes", "Saint Paul", "1501", "the Man Booker Prize for Fiction", "Wayne Rooney", "Nicholas John \"Nic\" Cester", "Sharon Sheeley", "two tectonic plates move towards each other at a convergent plate boundary", "2013 -- 14", "1947, 1956, 1975, 2015 and 2017", "the Chief Whip", "Loki", "Manchester", "Sri Lanka", "Felipe Massa.", "security officer Stephen Johns reportedly opened the door", "Admiral Hyman Rickover", "\"simple\"", "Vestal Virgins", "Mike Mushok"], "metric_results": {"EM": 0.625, "QA-F1": 0.7051143483709272}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true, false], "QA-F1": [0.5, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.47619047619047616, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7368421052631579, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-1854", "mrqa_hotpotqa-validation-5670", "mrqa_hotpotqa-validation-2235", "mrqa_hotpotqa-validation-3337", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-3024", "mrqa_naturalquestions-validation-8699", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-5865", "mrqa_triviaqa-validation-2833", "mrqa_newsqa-validation-2941", "mrqa_searchqa-validation-127", "mrqa_searchqa-validation-7762", "mrqa_naturalquestions-validation-1089"], "SR": 0.625, "CSR": 0.5298032407407407, "retrieved_ids": ["mrqa_squad-train-17374", "mrqa_squad-train-8501", "mrqa_squad-train-65504", "mrqa_squad-train-22286", "mrqa_squad-train-72088", "mrqa_squad-train-21290", "mrqa_squad-train-17740", "mrqa_squad-train-49045", "mrqa_squad-train-51280", "mrqa_squad-train-61262", "mrqa_squad-train-36610", "mrqa_squad-train-65967", "mrqa_squad-train-25129", "mrqa_squad-train-35604", "mrqa_squad-train-12343", "mrqa_squad-train-14185", "mrqa_newsqa-validation-499", "mrqa_searchqa-validation-3792", "mrqa_searchqa-validation-1744", "mrqa_searchqa-validation-2914", "mrqa_triviaqa-validation-1389", "mrqa_newsqa-validation-647", "mrqa_naturalquestions-validation-9545", "mrqa_searchqa-validation-482", "mrqa_triviaqa-validation-5715", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-4038", "mrqa_searchqa-validation-4804", "mrqa_hotpotqa-validation-1767", "mrqa_newsqa-validation-638", "mrqa_naturalquestions-validation-1953", "mrqa_newsqa-validation-3833"], "EFR": 0.9583333333333334, "Overall": 0.7151273148148147}, {"timecode": 54, "before_eval_results": {"predictions": ["Betty Crocker", "Frank Sinatra", "khaki", "Frank Sinatra", "the Ross Ice Shelf", "robota", "Easter Island", "the troposphere", "Huntsville, Alabama", "Khrushchev", "Venus", "the pupil", "Jordan", "German", "toga", "George Washington", "Canadian River", "Hairspray", "The Prince and the Pauper", "Bangkok", "nitrogen", "Charles Ponzi", "Jason Voorhees", "Islamic Republic", "Allen Klein", "the Spanish Republic", "The sleeping Tiger", "George Wallace", "sushi", "Hyperion", "the Byzantine Empire", "Fall Out Boy", "freezing", "silver", "beak", "William Jennings Bryan", "The Blues Brothers", "Ford Madox Ford", "the flyer", "Copenhagen", "fig", "double bass", "Flight of the Bumblebee", "M", "the Panama Canal", "Dan Marino", "Crustaceans", "Hestia", "Band of Brothers", "France", "I Have a Dream Speech", "Profit maximization happens when marginal cost is equal to marginal revenue", "Havana Harbor", "December 10, 2017", "Thai", "Heather Stanning", "redhead", "April 1, 1949", "Lantern Waste", "Glam metal", "July in the Philippines", "Abhisit Vejjajiva", "prisoners at the South Dakota State Penitentiary", "1989"], "metric_results": {"EM": 0.625, "QA-F1": 0.7007154304029304}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3076923076923077, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5728", "mrqa_searchqa-validation-6583", "mrqa_searchqa-validation-4968", "mrqa_searchqa-validation-13503", "mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-7142", "mrqa_searchqa-validation-8663", "mrqa_searchqa-validation-1059", "mrqa_searchqa-validation-15081", "mrqa_searchqa-validation-16507", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-8786", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-4922", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-12780", "mrqa_searchqa-validation-7435", "mrqa_searchqa-validation-10730", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-53", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-7321", "mrqa_newsqa-validation-3407"], "SR": 0.625, "CSR": 0.5315340909090909, "retrieved_ids": ["mrqa_squad-train-42736", "mrqa_squad-train-48152", "mrqa_squad-train-4920", "mrqa_squad-train-81254", "mrqa_squad-train-86043", "mrqa_squad-train-44823", "mrqa_squad-train-19795", "mrqa_squad-train-425", "mrqa_squad-train-19744", "mrqa_squad-train-40542", "mrqa_squad-train-61306", "mrqa_squad-train-69246", "mrqa_squad-train-16763", "mrqa_squad-train-33194", "mrqa_squad-train-70896", "mrqa_squad-train-27389", "mrqa_newsqa-validation-2232", "mrqa_searchqa-validation-2753", "mrqa_newsqa-validation-4161", "mrqa_naturalquestions-validation-1223", "mrqa_newsqa-validation-876", "mrqa_triviaqa-validation-6192", "mrqa_naturalquestions-validation-3052", "mrqa_hotpotqa-validation-3304", "mrqa_naturalquestions-validation-1455", "mrqa_triviaqa-validation-6514", "mrqa_searchqa-validation-5320", "mrqa_newsqa-validation-1191", "mrqa_triviaqa-validation-4748", "mrqa_newsqa-validation-2844", "mrqa_naturalquestions-validation-1552", "mrqa_newsqa-validation-3999"], "EFR": 1.0, "Overall": 0.7238068181818181}, {"timecode": 55, "before_eval_results": {"predictions": ["Transportation Security Administration", "an open window", "helping to plan the September 11, 2001, terror attacks,", "Iran", "1960s", "Janet Napolitano", "legislation Wednesday requiring federal oil industry regulators to wait at least two years after leaving government service before going to work for companies they helped regulate.", "Stanford", "\"How I Met Your Mother,\"", "Opryland", "three", "April 2", "Arabic, French and English,", "Fernando Gonzalez", "apologized,", "10 below", "KBR", "L'Aquila earthquake,", "crocodile eggs", "200", "Joe Patane", "Palm", "82", "1975", "Leo Frank", "50", "inspector-general", "Friday,", "two", "France's famous Louvre museum", "Ashley \"A.J.\" Jewell,", "be silent.", "SSM Cardinal Glennon Children's Medical Center", "two weeks after Black History Month", "Two people", "one of Africa's most stable nations.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "federal officers' bodies", "contraband cell phones", "an engineering and construction company", "$50,000", "a national telephone survey", "The oceans are growing crowded,", "Michelle Rounds", "DBG", "a strict interpretation of the law,", "Monday and Tuesday", "\"A good vegan cupcake has the power to transform everything for the better,\"", "sailing", "246", "Patrick McGoohan,", "provides the public with financial information about a nonprofit organization", "President Gerald Ford", "tomato sauce", "windmill", "France", "Mt Kenya", "R&B", "Donald Duck", "English", "the Edo era", "the Grand Canal", "pennies", "merengue and bachata music"], "metric_results": {"EM": 0.625, "QA-F1": 0.7575016534391534}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.4444444444444445, 0.9600000000000001, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.29629629629629634, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-2666", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-975", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-3183", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-2943", "mrqa_triviaqa-validation-5309", "mrqa_hotpotqa-validation-2866", "mrqa_searchqa-validation-2063", "mrqa_naturalquestions-validation-4925"], "SR": 0.625, "CSR": 0.533203125, "retrieved_ids": ["mrqa_squad-train-75296", "mrqa_squad-train-22771", "mrqa_squad-train-13201", "mrqa_squad-train-48895", "mrqa_squad-train-39608", "mrqa_squad-train-38220", "mrqa_squad-train-43853", "mrqa_squad-train-674", "mrqa_squad-train-11206", "mrqa_squad-train-51267", "mrqa_squad-train-69128", "mrqa_squad-train-35292", "mrqa_squad-train-76945", "mrqa_squad-train-20978", "mrqa_squad-train-12644", "mrqa_squad-train-59015", "mrqa_searchqa-validation-10838", "mrqa_hotpotqa-validation-2486", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-2622", "mrqa_searchqa-validation-14128", "mrqa_naturalquestions-validation-6028", "mrqa_newsqa-validation-3629", "mrqa_searchqa-validation-12607", "mrqa_searchqa-validation-8148", "mrqa_triviaqa-validation-4382", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-4486", "mrqa_naturalquestions-validation-9237", "mrqa_squad-validation-4469", "mrqa_searchqa-validation-4764", "mrqa_hotpotqa-validation-3943"], "EFR": 1.0, "Overall": 0.724140625}, {"timecode": 56, "before_eval_results": {"predictions": ["Hundreds of women protest child trafficking and shout anti-French slogans Wednesday in Abeche, Chad.", "$150 billion", "Kellogg Brown", "Thursday", "\"Empire of the Sun,\"", "Buenos Aires.", "a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the difference,\"", "Sonia Sotomayor", "Herman Cain", "There's no chance", "15-year-old", "137", "22-year-old", "\"bystander effect\":", "Bryant Purvis", "570 billion pesos ($42 billion)", "Ralph Lauren", "will not support the Stop Online Piracy Act,", "outstanding performance by a female actor in a drama series for her role as Deputy Chief Brenda Johnson.", "Krishna Rajaram,", "helping to plan the September 11, 2001,", "The pilot,", "$10 billion", "HPV (human papillomavirus)", "Washington State's decommissioned Hanford nuclear site,", "\"It hurts my heart to see him in pain, but it enlightens at the same time to know my son is strong enough to make it through on a daily basis,\"", "Pakistan's", "in a canyon in the path of the blaze Thursday.", "left the medical engineering company", "an antihistamine and an epinephrine auto-injector", "U.S. Holocaust Memorial Museum,", "racial intolerance.", "Pixar's \"Toy Story\"", "Al-Shabaab,", "were accused along with a third teen, Jesus Mendez, 16, of being in a group that poured alcohol over brewer and set him ablaze in a dispute over $40, a video game and a bicycle.", "3-0", "Tuesday", "", "Dubai's shoreline", "would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "Dr. Jennifer Arnold and husband Bill Klein,", "The forehead and chin", "five", "British Prime Minister Gordon Brown's", "in the Pacific Ocean territory of Guam within striking distance,", "Angola", "The drama of the action in-and-around the golf course", "May 4", "a \" happy ending\" to the case.", "Harry Nicolaides,", "ALS6", "dispense summary justice or merely deal with local administrative applications in common law jurisdictions", "comes from the ancient Etruscan root autu - and has within it connotations of the passing of the year", "a hyper - active kinase, that confers an aberrant, ligand - independent, non-regulated growth stimulus to the cancer cells", "hound", "a feisty creature", "Cambodia", "Cold Spring", "February 13, 1946", "Arrowhead Stadium", "Thomas Jefferson", "refrigerator", "X-Files", "1961"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7544693760656437}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true], "QA-F1": [0.08333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8750000000000001, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.7450980392156863, 1.0, 0.16666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.782608695652174, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-933", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-970", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-3347", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-7827", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-1316", "mrqa_hotpotqa-validation-298"], "SR": 0.671875, "CSR": 0.5356359649122807, "retrieved_ids": ["mrqa_squad-train-33457", "mrqa_squad-train-57106", "mrqa_squad-train-49237", "mrqa_squad-train-26487", "mrqa_squad-train-64345", "mrqa_squad-train-54735", "mrqa_squad-train-61301", "mrqa_squad-train-80354", "mrqa_squad-train-15007", "mrqa_squad-train-13453", "mrqa_squad-train-53282", "mrqa_squad-train-52598", "mrqa_squad-train-32852", "mrqa_squad-train-63717", "mrqa_squad-train-75366", "mrqa_squad-train-38086", "mrqa_squad-validation-9195", "mrqa_squad-validation-4469", "mrqa_searchqa-validation-8665", "mrqa_hotpotqa-validation-2634", "mrqa_squad-validation-8990", "mrqa_searchqa-validation-282", "mrqa_searchqa-validation-7353", "mrqa_squad-validation-8453", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-1671", "mrqa_searchqa-validation-5856", "mrqa_triviaqa-validation-1387", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5563", "mrqa_squad-validation-1704", "mrqa_newsqa-validation-1519"], "EFR": 1.0, "Overall": 0.7246271929824561}, {"timecode": 57, "before_eval_results": {"predictions": ["$8.8 million", "led the weekend box office,", "Kurdish militant group in Turkey", "Dr. Maria Siemionow,", "Alwin Landry", "a city of romance, of incredible architecture and history.", "\"He called it the largest and perhaps most sophisticated ring of its kind in U.S. history.", "free services.", "the number of new cases is falling.\"", "Amitabh Bachchan", "\"How I Met Your Mother,\"", "Harlem,", "closure of Guant Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel under the guidelines of the Army Field Manual.", "\"She had a smile on her face, like she always does when she comes in here,\"", "Screen Actors Guild", "11 healthy eggs", "south of Atlanta", "Guinea, Myanmar, Sudan and Venezuela.", "four", "the \" Michoacan Family,\"", "Hutu militias and members of the general population sought out Tutsis and moderate Hutus -- and went on a 100-day killing rampage.", "$60 billion", "laser sights", "Tim Clark, Matt Kuchar and Bubba Watson", "Mark Fields", "three", "the Taliban", "Isabella", "Larry Abrahamson", "near his home in Peshawar", "Dogpatch Labs", "California Highway Patrol.", "cross-country skiers", "no one is sure", "\"The Sopranos,\"", "a one-shot victory in the Bob Hope Classic", "peace with Israel", "Stratfor", "Adam Yahiye Gadahn,", "antihistamine and an epinephrine auto-injector", "The EU naval force", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "Deputy Treasury Secretary", "heavy turbulence", "success as a recording artist", "St. Louis, Missouri.", "Emmy-winning Patrick McGoohan,", "Henrik Stenson", "Three", "Dore Gold,", "for using recreational drugs in September,", "Roger Federer", "between 11000 and 9000 BC", "December 2, 2013", "Hayes", "overprotective", "Morgan Spurlock", "football", "War & Peace", "Eugene Levy", "Floyd Patterson", "Jan & Dean", "Sweeney Todd", "vocal"], "metric_results": {"EM": 0.5625, "QA-F1": 0.655052775043584}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, false, true, true, false, false, false, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.9411764705882353, 0.10714285714285712, 1.0, 0.0, 1.0, 1.0, 0.0, 0.24242424242424243, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.15384615384615383, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-1509", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-2058", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-1121", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-2170", "mrqa_triviaqa-validation-6757", "mrqa_hotpotqa-validation-3321", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-10144"], "SR": 0.5625, "CSR": 0.5360991379310345, "retrieved_ids": ["mrqa_squad-train-71884", "mrqa_squad-train-49871", "mrqa_squad-train-51022", "mrqa_squad-train-72590", "mrqa_squad-train-3348", "mrqa_squad-train-1068", "mrqa_squad-train-31793", "mrqa_squad-train-84672", "mrqa_squad-train-38718", "mrqa_squad-train-56481", "mrqa_squad-train-24248", "mrqa_squad-train-65214", "mrqa_squad-train-61714", "mrqa_squad-train-62083", "mrqa_squad-train-74608", "mrqa_squad-train-63095", "mrqa_hotpotqa-validation-4294", "mrqa_naturalquestions-validation-1372", "mrqa_newsqa-validation-3326", "mrqa_hotpotqa-validation-2728", "mrqa_squad-validation-3708", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-8685", "mrqa_newsqa-validation-1681", "mrqa_hotpotqa-validation-3521", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-5995", "mrqa_searchqa-validation-3032", "mrqa_newsqa-validation-476", "mrqa_searchqa-validation-6338", "mrqa_hotpotqa-validation-2441", "mrqa_hotpotqa-validation-5563"], "EFR": 0.9642857142857143, "Overall": 0.7175769704433497}, {"timecode": 58, "before_eval_results": {"predictions": ["at a depth of about 1,300 meters in the Mediterranean Sea.", "three machine guns and two silencers", "the 3rd District of Utah.", "Kenneth Cole", "\"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "President Bush", "Stephen Tyrone Johns", "Transport Workers Union leaders", "eight Indian army troopers, including one officer, and 17 militants,", "23-year-old", "prisoners at the South Dakota State Penitentiary", "its interpretation of Islamic law, or sharia, on Somalia.", "art fair,", "Workers'", "\"It would satisfy me more. Why he's more American than a German,", "Pope", "boyhood experience in a World War II internment camp", "Secretary of State", "Mother's", "in a canyon in the path of the blaze Thursday.", "poppy production", "1979", "Alexandre Caizergues, of France,", "sportswear,", "military trials for some detainees", "Larry King", "onstage demos.", "iTunes", "Indonesian", "eight-week", "order", "Natalie Cole", "Somali-based", "folding table", "federal ocean planning.", "Ventures", "British", "Long Island", "former Procol Harum bandmate Gary Brooker", "Former Mobile County Circuit Judge Herman Thomas", "Sen. Barack Obama", "July 23.", "suicides", "lack of a cause of death", "Sunday.", "preliminary injunction", "105-year", "helping consumers move beyond these hard times", "a man's lifeless, naked body", "iTunes", "eight", "summer of 1990", "`` Cheitharol Kummaba ''", "Ricky Nelson", "Blind Faith", "Portugal", "reclining or couchant sphinx", "Andrew Johnson", "Charles Nungesser", "The United States of America (USA),", "chocolate", "tides", "bananas", "Leonard Bernstein"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6743786136249371}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7058823529411764, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.15384615384615385, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.0, 0.7000000000000001, 1.0, 0.5, 1.0, 0.6, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.888888888888889, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1448", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-2135", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-2617", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-4798", "mrqa_triviaqa-validation-3070", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-722", "mrqa_searchqa-validation-14467", "mrqa_searchqa-validation-7269"], "SR": 0.546875, "CSR": 0.5362817796610169, "retrieved_ids": ["mrqa_squad-train-10538", "mrqa_squad-train-76178", "mrqa_squad-train-41544", "mrqa_squad-train-41787", "mrqa_squad-train-6842", "mrqa_squad-train-46586", "mrqa_squad-train-42810", "mrqa_squad-train-28619", "mrqa_squad-train-84306", "mrqa_squad-train-72584", "mrqa_squad-train-74619", "mrqa_squad-train-40088", "mrqa_squad-train-8289", "mrqa_squad-train-44438", "mrqa_squad-train-64336", "mrqa_squad-train-70338", "mrqa_hotpotqa-validation-5226", "mrqa_squad-validation-8399", "mrqa_searchqa-validation-9261", "mrqa_hotpotqa-validation-370", "mrqa_squad-validation-7713", "mrqa_naturalquestions-validation-2196", "mrqa_squad-validation-8453", "mrqa_naturalquestions-validation-7468", "mrqa_searchqa-validation-5869", "mrqa_hotpotqa-validation-5562", "mrqa_hotpotqa-validation-1393", "mrqa_newsqa-validation-3923", "mrqa_triviaqa-validation-1389", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-8062", "mrqa_hotpotqa-validation-2574"], "EFR": 1.0, "Overall": 0.7247563559322033}, {"timecode": 59, "before_eval_results": {"predictions": ["oxygen saturations", "John Smith", "McFerrin", "Brahmagupta's Brahmasputha Siddhanta ( 7th century )", "August 31, 2017", "multiple", "Egypt", "Alan Tudyk", "detention camp", "1840s", "1940s", "1985", "helps scientists better understand the spread of pollution around the globe", "electors", "The resulting molecule, now mature insulin, is stored as a hexamer in secretory vesicles", "a EU state ( Italy )", "Carpenter", "Kirstjen Nielsen", "$1.84 billion", "First Epistle of John", "crossbar", "February 7, 2018", "White Sox", "The Star Spangled Banner", "The United States emerged from World War II as a global superpower", "Tara / Ghost of Christmas Past", "Stephen Foster", "1975", "6 -- 14 July", "majority of members present at that time", "off the northeast coast of Australia", "`` Blood is the New Black ''", "continent of Antarctica", "the courts", "the primal rib", "positions Arg15 - Ile16", "Steve Valentine", "a political ideology", "Anakin Skywalker", "Lesley Gore", "Tom Hanks", "The present coat of arms of South Africa", "peninsular", "Terry Kath", "Left Behind", "Beorn", "Johnny Cash", "the time of the arrival of Columbus", "rapid, continuous warming", "The Romantics", "Gorakhpur Junction", "Maarten Tromp", "manubrium", "horseracing", "Tainy Sledstviya", "travel diary", "\"Nebo Zovyot\"", "Caster Semenya", "Berga an der Elster", "A North Korean Foreign Ministry spokesman described U.S. Vice President Dick Cheney as a \"mentally deranged person steeped in the inveterate enmity towards the system\"", "Nellie Bly", "Red Adair", "godliness", "Pourred"], "metric_results": {"EM": 0.5625, "QA-F1": 0.615402913059163}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-9881", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-9683", "mrqa_hotpotqa-validation-3429", "mrqa_newsqa-validation-2405", "mrqa_searchqa-validation-8408", "mrqa_searchqa-validation-3666"], "SR": 0.5625, "CSR": 0.53671875, "retrieved_ids": ["mrqa_squad-train-48408", "mrqa_squad-train-61246", "mrqa_squad-train-36910", "mrqa_squad-train-40648", "mrqa_squad-train-86306", "mrqa_squad-train-38630", "mrqa_squad-train-51387", "mrqa_squad-train-73765", "mrqa_squad-train-55243", "mrqa_squad-train-70589", "mrqa_squad-train-49724", "mrqa_squad-train-3218", "mrqa_squad-train-25023", "mrqa_squad-train-28332", "mrqa_squad-train-64937", "mrqa_squad-train-68951", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-6256", "mrqa_newsqa-validation-1165", "mrqa_searchqa-validation-16507", "mrqa_triviaqa-validation-1286", "mrqa_squad-validation-3497", "mrqa_newsqa-validation-1893", "mrqa_naturalquestions-validation-3828", "mrqa_searchqa-validation-8237", "mrqa_hotpotqa-validation-3250", "mrqa_triviaqa-validation-3657", "mrqa_hotpotqa-validation-1602", "mrqa_squad-validation-9400", "mrqa_triviaqa-validation-6701", "mrqa_newsqa-validation-4107", "mrqa_searchqa-validation-947"], "EFR": 0.9642857142857143, "Overall": 0.7177008928571429}, {"timecode": 60, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1003", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1417", "mrqa_hotpotqa-validation-1604", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-2225", "mrqa_hotpotqa-validation-2302", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2739", "mrqa_hotpotqa-validation-2850", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-310", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3429", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3474", "mrqa_hotpotqa-validation-3485", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3767", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-4233", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4999", "mrqa_hotpotqa-validation-5038", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5317", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-870", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2646", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3062", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3795", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-9281", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9557", "mrqa_naturalquestions-validation-9778", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1044", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1590", "mrqa_newsqa-validation-1610", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1678", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1708", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1818", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2058", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-2335", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-289", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3286", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3521", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3867", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-65", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-983", "mrqa_searchqa-validation-1000", "mrqa_searchqa-validation-10210", "mrqa_searchqa-validation-10394", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-10752", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-10955", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-11973", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12698", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12763", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-12959", "mrqa_searchqa-validation-13328", "mrqa_searchqa-validation-13503", "mrqa_searchqa-validation-1399", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-14969", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15730", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-15839", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16642", "mrqa_searchqa-validation-16746", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-16894", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2791", "mrqa_searchqa-validation-3098", "mrqa_searchqa-validation-3295", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-4219", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-4922", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5521", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-5698", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-6659", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7317", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-7989", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-9176", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9683", "mrqa_searchqa-validation-9698", "mrqa_squad-validation-100", "mrqa_squad-validation-10370", "mrqa_squad-validation-10457", "mrqa_squad-validation-12", "mrqa_squad-validation-1254", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1540", "mrqa_squad-validation-1613", "mrqa_squad-validation-175", "mrqa_squad-validation-185", "mrqa_squad-validation-1902", "mrqa_squad-validation-194", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-2523", "mrqa_squad-validation-270", "mrqa_squad-validation-2762", "mrqa_squad-validation-2916", "mrqa_squad-validation-2986", "mrqa_squad-validation-3037", "mrqa_squad-validation-3125", "mrqa_squad-validation-3319", "mrqa_squad-validation-3511", "mrqa_squad-validation-3545", "mrqa_squad-validation-3708", "mrqa_squad-validation-3811", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3934", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-448", "mrqa_squad-validation-4546", "mrqa_squad-validation-4750", "mrqa_squad-validation-4860", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5050", "mrqa_squad-validation-5135", "mrqa_squad-validation-5276", "mrqa_squad-validation-5311", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5474", "mrqa_squad-validation-5669", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5736", "mrqa_squad-validation-6029", "mrqa_squad-validation-6351", "mrqa_squad-validation-6481", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6630", "mrqa_squad-validation-7023", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7531", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7689", "mrqa_squad-validation-7763", "mrqa_squad-validation-8066", "mrqa_squad-validation-8209", "mrqa_squad-validation-8386", "mrqa_squad-validation-8754", "mrqa_squad-validation-8772", "mrqa_squad-validation-8931", "mrqa_squad-validation-9012", "mrqa_squad-validation-9322", "mrqa_squad-validation-9495", "mrqa_squad-validation-9506", "mrqa_squad-validation-955", "mrqa_squad-validation-9590", "mrqa_squad-validation-9868", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4371", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6579", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6724", "mrqa_triviaqa-validation-6730", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7683"], "OKR": 0.861328125, "KG": 0.50703125, "before_eval_results": {"predictions": ["hydrogen isotopes", "Sax Rohmer", "O.J. Simpson", "Castro", "Crystal Gayle", "carry On Cleo", "St. Ambrose", "health imbalance", "dijon", "Wisconsin", "plum", "having an affair with their maid", "Wonga", "Secretary of State William H. Seward", "\u201cfrozen\u201d", "The World is Not Enough", "Barnaby Rudge", "James Chadwick", "Gary Barlow", "a meteoroid", "Lesley Lawson", "California", "Cesium", "Picasso", "brain production of the abnormal protein beta-amyloid,", "trumpet", "King George I", "silk", "index fingers", "earache", "90%", "muezzin", "moon", "watts", "alan ladd", "soy", "Virginia", "(Andrea) Artz", "PJ Harvey", "Jim Jones", "the grueling decathlon", "Runcorn", "Watch Tower Tract Society", "Amnesty International", "[n]", "naomi watts", "Arthur Miller", "Carousel", "cover story", "potash", "Paris", "headdresses worn by Muslim women", "Janie Crawford", "Danielle Rose Russell", "their unusual behavior", "The Seduction of Hillary Rodham", "Apsley George Benet Cherry-Garrard", "\"Star Wars\"", "86", "allegations that a dorm parent mistreated students at the school.", "daiquiri", "apples", "Morocco", "the nerves and ganglia outside the brain and spinal cord"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6833333333333333}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-3379", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-240", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4497", "mrqa_triviaqa-validation-3854", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-6726", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-2261", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-6285", "mrqa_newsqa-validation-2618"], "SR": 0.65625, "CSR": 0.5386782786885246, "retrieved_ids": ["mrqa_squad-train-68558", "mrqa_squad-train-72499", "mrqa_squad-train-81122", "mrqa_squad-train-7958", "mrqa_squad-train-81064", "mrqa_squad-train-80661", "mrqa_squad-train-81864", "mrqa_squad-train-6890", "mrqa_squad-train-23399", "mrqa_squad-train-12976", "mrqa_squad-train-26671", "mrqa_squad-train-2354", "mrqa_squad-train-21794", "mrqa_squad-train-268", "mrqa_squad-train-41421", "mrqa_squad-train-55234", "mrqa_naturalquestions-validation-6319", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-4501", "mrqa_searchqa-validation-201", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-7614", "mrqa_newsqa-validation-4011", "mrqa_triviaqa-validation-2287", "mrqa_squad-validation-7131", "mrqa_newsqa-validation-850", "mrqa_triviaqa-validation-2227", "mrqa_hotpotqa-validation-3402", "mrqa_hotpotqa-validation-1585", "mrqa_naturalquestions-validation-6028"], "EFR": 1.0, "Overall": 0.7376575307377049}, {"timecode": 61, "before_eval_results": {"predictions": ["a paragraph about the king and crown prince that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown.", "Mugabe and Tsvangirai", "never", "Samuel Herr,", "checkposts and military camps in the Mohmand agency,", "Cannes Film Festival", "customers are lining up for vitamin injections that promise", "Caster Semenya", "Andrew Morris,", "Alberto Espinoza Barron,", "More than 22 million", "41,280", "Casey Anthony,", "18", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "Phoenix,", "Superman brought down the Ku Klux Klan,", "children's", "between 1917 and 1924", "Mississippi", "$3 billion", "Steve Williams", "school in South Africa", "Dr. Jennifer Arnold and husband Bill Klein,", "\"Up,\"", "\"The Real Housewives of Atlanta\"", "Bollywood", "Wigan Athletic", "seven-time Formula One world champion", "President Obama's", "California-based Current TV", "civilians,", "Michelle Obama", "Adriano", "returning combat veterans", "Kit of Elsinore", "\"black rain\" of drilling fluid and a roar of escaping gas erupted from the doomed Deepwater Horizon shortly before the explosion that sank the oil rig,", "Phay Siphan, secretary of the Cambodian Council of Ministers.", "staff sergeant in the U.S. Air Force,", "nine", "Jason Voorhees", "George Washington", "chairman of the House Budget Committee", "Kenyan", "Werder Bremen,", "discusses his roots as he castigates U.S. policies and deplores Israel's offensive in Gaza", "the HSH Nordbank Arena", "Chancellor", "rare thriller writer", "Monday", "the Carrousel du Louvre,", "Prem Lata Agarwal", "Wendi McLendon - Covey", "Terry Kath", "equinox", "William Shakespeare", "Nicolas Sarkozy", "Melanie Owen", "Thor", "Queenston Delta", "George III", "the Taj Mahal", "a colon", "Carol Worthington"], "metric_results": {"EM": 0.5, "QA-F1": 0.5688578869047618}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, false, true, true, true, true], "QA-F1": [0.42857142857142855, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11999999999999998, 0.2, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-702", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-1929", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-1861", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-2678", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-3209", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-3139", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-1704", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-2955", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-400", "mrqa_triviaqa-validation-4714", "mrqa_hotpotqa-validation-4692"], "SR": 0.5, "CSR": 0.538054435483871, "retrieved_ids": ["mrqa_squad-train-25536", "mrqa_squad-train-75467", "mrqa_squad-train-40521", "mrqa_squad-train-32335", "mrqa_squad-train-68142", "mrqa_squad-train-67489", "mrqa_squad-train-33493", "mrqa_squad-train-7085", "mrqa_squad-train-10891", "mrqa_squad-train-30369", "mrqa_squad-train-51274", "mrqa_squad-train-40808", "mrqa_squad-train-50682", "mrqa_squad-train-64068", "mrqa_squad-train-62986", "mrqa_squad-train-33001", "mrqa_newsqa-validation-1277", "mrqa_naturalquestions-validation-4190", "mrqa_triviaqa-validation-11", "mrqa_newsqa-validation-949", "mrqa_triviaqa-validation-5715", "mrqa_naturalquestions-validation-1325", "mrqa_searchqa-validation-15830", "mrqa_hotpotqa-validation-2210", "mrqa_newsqa-validation-2869", "mrqa_searchqa-validation-944", "mrqa_newsqa-validation-3960", "mrqa_triviaqa-validation-2047", "mrqa_newsqa-validation-2235", "mrqa_hotpotqa-validation-455", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-9368"], "EFR": 0.96875, "Overall": 0.7312827620967741}, {"timecode": 62, "before_eval_results": {"predictions": ["500 feet down an embankment", "\"terrorizing the country and attacking civilians.\"", "1960s song \"A Whiter Shade of Pale\"", "in Nuevo Leon,", "Kenneth Cole", "little blue booties.", "David McKenzie", "$8.8 million", "\"I never thought any of this was going to be easy,\"", "Saturday", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "14 years", "planning processes are urgently needed", "Mandi Hamlin", "Malcolm X", "financial gain,", "a face-to-face interview with the president", "Six", "Steve Jobs", "a head injury.", "Daniel Radcliffe", "Tillakaratne Dilshan", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "a construction site in the heart of Los Angeles.", "Jaipur", "Al-Aqsa mosque", "Bahrami", "criminals", "whether to close some entrances, bring in additional officers, and make security more visible,", "would not do it", "Mandi Hamlin", "E. coli", "Six members of Zoe's Ark", "The United States", "100 percent", "Brian Mabry", "150", "\"Watchmen\"", "autonomy.", "\"She was focused so much on learning that she didn't notice,\"", "and renewable energy at home everyday,\"", "two", "responsibility for the abductions.", "Brian David Mitchell,", "One of Osama bin Laden's sons", "as part of its 18-month journey around the world.", "Italian Serie A title", "1983.", "Tutsi ethnic minority and the Hutu majority", "They are co-chairs of the Genocide Prevention Task Force.", "we were petitioned and have been looking into it for the past two years,\"", "William Shakespeare's As You Like It", "DeWayne Warren", "1954", "jackstones", "fool", "The Merchant of Venice", "designated hitter rule", "The 8th Habit", "11 June 1959", "Plutarch", "Japanese", "a cab", "seven nights a week"], "metric_results": {"EM": 0.59375, "QA-F1": 0.674987078893329}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, false, false, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.12121212121212123, 1.0, 1.0, 0.0, 0.6666666666666666, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.14285714285714288, 0.7692307692307692, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-863", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2336", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3617", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-6383", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-6532", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-3266", "mrqa_searchqa-validation-16570"], "SR": 0.59375, "CSR": 0.5389384920634921, "retrieved_ids": ["mrqa_squad-train-60590", "mrqa_squad-train-6850", "mrqa_squad-train-52767", "mrqa_squad-train-82952", "mrqa_squad-train-41118", "mrqa_squad-train-59711", "mrqa_squad-train-41195", "mrqa_squad-train-51503", "mrqa_squad-train-3374", "mrqa_squad-train-7224", "mrqa_squad-train-49023", "mrqa_squad-train-64748", "mrqa_squad-train-35221", "mrqa_squad-train-78259", "mrqa_squad-train-45528", "mrqa_squad-train-72193", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-2020", "mrqa_triviaqa-validation-7404", "mrqa_searchqa-validation-6583", "mrqa_naturalquestions-validation-6849", "mrqa_newsqa-validation-2167", "mrqa_hotpotqa-validation-2865", "mrqa_searchqa-validation-3398", "mrqa_naturalquestions-validation-863", "mrqa_newsqa-validation-2802", "mrqa_searchqa-validation-14865", "mrqa_naturalquestions-validation-2901", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-8285"], "EFR": 0.9615384615384616, "Overall": 0.7300172657203907}, {"timecode": 63, "before_eval_results": {"predictions": ["Jaime Andrade", "\"He knows what happened at the pool that day,\"", "President Obama and Britain's Prince Charles", "the insurgency,", "Reporters Without Borders", "Genocide Prevention Task Force.", "Britain's", "five female pastors", "Bob Bogle,", "Sri Lanka,", "82", "Fullerton, California,", "$249", "Zulfikar Ali Bhutto,", "two African-Americans", "Climatecare,", "Paul Blart: Mall Cop", "President Obama and Britain's Prince Charles", "public opinion in Turkey.\"", "10 municipal police officers", "left his indelible fingerprints on the entertainment industry.", "murder in the beating death of a company boss who fired them.", "\"And even though she's not here anymore, I'm not afraid to say it, sometimes she was a pain in the ass,\"", "he was a practicing Muslim... that he was mad at the U.S. military because of what they had done to Muslims in the past,\"", "Siri.", "Derek Mears", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Heshmatollah Attarzadeh", "the mammoth's skull,", "because the Indians were gathering information about the rebels to give to the Colombian military.", "--the Louvre.", "Mubarak,", "Princess Diana,", "revelry", "Human Rights Watch", "45 minutes, five days a week.", "February 12", "1959,", "London", "\"illegitimate.\"", "BBC's central London offices", "Russia", "industrialized nations", "Zuma", "saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "to sniff out cell phones.", "shock, quickly followed by speculation about what was going to happen next,\"", "U.S. Food and Drug Administration", "$1.5 million.", "former Pakistani Prime Minister Nawaz Sharif", "raping and killing a 14-year-old Iraqi girl.", "1837", "Austin Winkler", "2005", "In Real Life:", "hispanica", "FIFA World Cup", "General Allenby", "turns out to be a terrible date", "Treaty of Gandamak", "United Kingdom", "The School of Athens", "beak", "Candide"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6516483516483517}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.26666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.17142857142857143, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 0.0, 0.125, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.125, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.25, 0.923076923076923, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1462", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3293", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-3401", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-1098", "mrqa_naturalquestions-validation-3482", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-6230", "mrqa_triviaqa-validation-3824", "mrqa_hotpotqa-validation-5720", "mrqa_hotpotqa-validation-4086", "mrqa_searchqa-validation-13103", "mrqa_searchqa-validation-1971"], "SR": 0.578125, "CSR": 0.53955078125, "retrieved_ids": ["mrqa_squad-train-15663", "mrqa_squad-train-38800", "mrqa_squad-train-2059", "mrqa_squad-train-53745", "mrqa_squad-train-25169", "mrqa_squad-train-51081", "mrqa_squad-train-27581", "mrqa_squad-train-42385", "mrqa_squad-train-57903", "mrqa_squad-train-45229", "mrqa_squad-train-38654", "mrqa_squad-train-55829", "mrqa_squad-train-75393", "mrqa_squad-train-45658", "mrqa_squad-train-7563", "mrqa_squad-train-64709", "mrqa_searchqa-validation-5484", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-863", "mrqa_newsqa-validation-393", "mrqa_naturalquestions-validation-8062", "mrqa_newsqa-validation-1496", "mrqa_triviaqa-validation-7724", "mrqa_newsqa-validation-4170", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-15777", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-2925", "mrqa_triviaqa-validation-1242", "mrqa_newsqa-validation-925", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3435"], "EFR": 1.0, "Overall": 0.73783203125}, {"timecode": 64, "before_eval_results": {"predictions": ["Wolfgang Amadeus Mozart", "Denmark\u2013Norway, Brandenburg and Sweden", "John Schlesinger", "philanthropist", "The Suite Life of Zach & Cody", "Two Is Better Than One", "Karl-Anthony Towns", "Omega SA", "9 November 1967", "designated hitter rule", "Jay Park", "Wayne County, Michigan", "Japan and Hong Kong", "Cleopatra VII Philopator", "8,211", "Allies of World War I, or Entente Powers", "August Heckscher", "Orange County, Florida, United States", "Gareth Barry", "capital crimes or capital offences", "Ned Flanders", "Westley Sissel Unseld", "Ken Howard", "Fat Albert", "Thomas Joseph \"T. J.\" Lavin", "15", "Germany", "I-League club Salgaocar", "Fudge", "1995", "Tuesday", "January 2001", "PlayStation 2 (PS2)", "England", "Ryukyuan people", "Major Charles White Whittlesey", "fennec fox", "1993 to 1996", "Yunnan-Fu", "Port Moresby, Papua New Guinea", "Macau Peninsula, Macau", "1993", "The Bangor Daily News is an American newspaper covering a large portion of rural Maine, published six days per week in Bangor, Maine", "The Land of Enchantment", "Territory of Hawaii", "William Finn", "My Backyard", "focuses on homosexuality, gay sex, and the gay bear subculture", "The Tales of Hoffmann", "Shakespeare's reputation", "October 3, 2017", "1984 Summer Olympics in Los Angeles", "Afghanistan", "Michael Crawford", "parallelogram", "Operation Frequent Wind", "Matthew 2:11", "toxic smoke from burn pits", "15,000", "breast cancer.", "mast", "Casablanca", "Vilnius", "Asaph Hall"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6824309371184372}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.8000000000000002, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.4, 0.7692307692307693, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.4, 0.0, 0.08333333333333334, 1.0, 0.5, 1.0, 1.0, 0.13333333333333333, 1.0, 0.0, 1.0, 0.4444444444444444, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-4702", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-589", "mrqa_hotpotqa-validation-5115", "mrqa_hotpotqa-validation-802", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-1776", "mrqa_hotpotqa-validation-2625", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-1394", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-5655", "mrqa_naturalquestions-validation-75", "mrqa_triviaqa-validation-6706", "mrqa_triviaqa-validation-305", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-4147", "mrqa_searchqa-validation-7441"], "SR": 0.5625, "CSR": 0.5399038461538461, "retrieved_ids": ["mrqa_squad-train-20828", "mrqa_squad-train-7673", "mrqa_squad-train-31587", "mrqa_squad-train-19196", "mrqa_squad-train-38215", "mrqa_squad-train-68889", "mrqa_squad-train-78529", "mrqa_squad-train-40717", "mrqa_squad-train-32760", "mrqa_squad-train-40995", "mrqa_squad-train-51773", "mrqa_squad-train-25656", "mrqa_squad-train-41492", "mrqa_squad-train-34600", "mrqa_squad-train-78644", "mrqa_squad-train-21616", "mrqa_naturalquestions-validation-9368", "mrqa_triviaqa-validation-3624", "mrqa_searchqa-validation-16718", "mrqa_squad-validation-9400", "mrqa_searchqa-validation-4325", "mrqa_squad-validation-1161", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-3020", "mrqa_naturalquestions-validation-8439", "mrqa_triviaqa-validation-3200", "mrqa_newsqa-validation-1663", "mrqa_newsqa-validation-3923", "mrqa_hotpotqa-validation-5180", "mrqa_naturalquestions-validation-9150", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-3430"], "EFR": 1.0, "Overall": 0.7379026442307692}, {"timecode": 65, "before_eval_results": {"predictions": ["10 October 2010", "vice-president", "Claudio Javier L\u00f3pez", "the Las Vegas Strip in Paradise", "The Swiss federal popular initiative \"against mass immigration", "Double Crossed", "John McClane", "Dan Brandon Bilzerian", "Philadelphia Naval Shipyard", "The Pentagon", "1958", "our greatest comedienne - Australia's Lucille Ball", "Canadian", "Seventeen", "spot-fixing", "Numb", "Easter Rising of 1916", "Spain, Mexico and France", "Juilliard School", "Mark \"Chopper\" Read", "small forward position", "37,776", "the Ais", "Erreway", "Eric Liddell", "Sam Kinison", "Spring city", "Robert Matthew Hurley", "India Today", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "9,984", "Southern Progress Corporation", "the Wabanaki Confederacy", "1966", "Flamingo Las Vegas", "The Soloist", "1st Earl Grosvenor", "Labour Party", "Walt Disney and Ub Iwerks", "1983", "stolperstein", "rapper", "BMW X6", "Cleveland Celtics", "Larnelle Steward Harris", "May 5, 2015", "Bank of China Tower", "teacher", "Barnoldswick", "New Orleans, Louisiana", "eight", "The International Baccalaureate", "Mark Jackson", "1991", "Antigua and Barbuda", "trout", "Montpelier", "allergen-free", "African National Congress Deputy President Kgalema Motlanthe,", "13.", "Portugal", "Splice", "law firm", "2010"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7240277777777777}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15999999999999998, 1.0, 1.0, 0.0, 1.0, 0.4444444444444444, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3904", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-1452", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-776", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-1123", "mrqa_triviaqa-validation-1138", "mrqa_newsqa-validation-3733", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-16601", "mrqa_naturalquestions-validation-1856"], "SR": 0.640625, "CSR": 0.5414299242424243, "retrieved_ids": ["mrqa_squad-train-70806", "mrqa_squad-train-54652", "mrqa_squad-train-66794", "mrqa_squad-train-30410", "mrqa_squad-train-44575", "mrqa_squad-train-83761", "mrqa_squad-train-62533", "mrqa_squad-train-37002", "mrqa_squad-train-41962", "mrqa_squad-train-33004", "mrqa_squad-train-38235", "mrqa_squad-train-6697", "mrqa_squad-train-48942", "mrqa_squad-train-56805", "mrqa_squad-train-57634", "mrqa_squad-train-65814", "mrqa_triviaqa-validation-7511", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2547", "mrqa_newsqa-validation-4156", "mrqa_naturalquestions-validation-3122", "mrqa_hotpotqa-validation-3084", "mrqa_triviaqa-validation-6707", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-16473", "mrqa_newsqa-validation-1085", "mrqa_searchqa-validation-9347", "mrqa_newsqa-validation-3049", "mrqa_searchqa-validation-947", "mrqa_triviaqa-validation-6701", "mrqa_searchqa-validation-230", "mrqa_newsqa-validation-3994"], "EFR": 1.0, "Overall": 0.7382078598484849}, {"timecode": 66, "before_eval_results": {"predictions": ["October 25, 1881", "Harold Holt", "January 31, 1993", "Despicable Me 3", "Bill Clinton", "1,800", "Frederick I", "Bring Me Sunshine (1994) was originally a three-part retrospective in tribute to Eric Morecambe", "Juventus", "November 6, 2018", "four", "the fifth level", "five", "Macau, China", "American", "from 1345 to 1377", "sandstone", "May 4, 2004", "VIMN Russia", "Brickyard", "Harlem neighborhood", "1999", "Parapsychologist", "the 2014 New Year Honours", "Greg Gorman and Helmut Newton", "Prince Ioann", "2015", "Winecoff Hotel fire", "the B-17 Flying Fortress bomber", "Coinapult", "Hawaii", "Kenji Hatanaka", "the Ruul", "Cameron Diaz", "remixes", "Seattle", "Ghana's Asamoah Gyan", "Scott Paul Carson", "John Snow", "Manchester, England", "\"The Tonight Show\"", "Tamaulipas", "boundary river", "Anomalisa", "technical director", "Victoria", "Las Vegas", "Cleveland, Ohio", "Stu Henderson", "AVN Adult Entertainment Expo", "Boyd Gaming", "Hirschman", "Jimmy Marinos", "the brain, muscles, and liver", "Churchill", "Wimbledon", "the Czech Republic", "at airports", "dual nationality", "Ronnie White,", "rain", "the RMS Titanic", "neon", "customers are lining up for vitamin injections that promise to improve health and beauty."], "metric_results": {"EM": 0.5, "QA-F1": 0.5877490942028986}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 0.8, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.782608695652174]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-3578", "mrqa_hotpotqa-validation-2541", "mrqa_hotpotqa-validation-5002", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-3287", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-5359", "mrqa_hotpotqa-validation-3126", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-5281", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1183", "mrqa_hotpotqa-validation-843", "mrqa_hotpotqa-validation-1932", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4117", "mrqa_hotpotqa-validation-618", "mrqa_hotpotqa-validation-3419", "mrqa_hotpotqa-validation-2306", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-7704", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-95", "mrqa_newsqa-validation-606", "mrqa_searchqa-validation-2698", "mrqa_newsqa-validation-3325"], "SR": 0.5, "CSR": 0.5408115671641791, "retrieved_ids": ["mrqa_squad-train-19280", "mrqa_squad-train-5772", "mrqa_squad-train-3565", "mrqa_squad-train-52382", "mrqa_squad-train-76063", "mrqa_squad-train-52214", "mrqa_squad-train-22361", "mrqa_squad-train-22113", "mrqa_squad-train-12655", "mrqa_squad-train-20617", "mrqa_squad-train-20425", "mrqa_squad-train-64568", "mrqa_squad-train-47598", "mrqa_squad-train-80156", "mrqa_squad-train-70249", "mrqa_squad-train-35595", "mrqa_searchqa-validation-16829", "mrqa_newsqa-validation-3799", "mrqa_hotpotqa-validation-3060", "mrqa_newsqa-validation-3382", "mrqa_triviaqa-validation-4137", "mrqa_newsqa-validation-2984", "mrqa_searchqa-validation-3032", "mrqa_naturalquestions-validation-6200", "mrqa_newsqa-validation-2370", "mrqa_squad-validation-7131", "mrqa_searchqa-validation-10202", "mrqa_hotpotqa-validation-3971", "mrqa_newsqa-validation-18", "mrqa_triviaqa-validation-6413", "mrqa_searchqa-validation-1913", "mrqa_hotpotqa-validation-2108"], "EFR": 0.96875, "Overall": 0.7318341884328359}, {"timecode": 67, "before_eval_results": {"predictions": ["her brother, Brian", "1956", "Portugal. The Man", "North Atlantic Ocean", "eleven", "Anthony Hopkins", "Andrew Gold", "$19.8 trillion", "Binding of a ligand to the extracellular region causes a series of structural rearrangements in the RTK that lead to its enzymatic activation", "January 15, 2007", "April 17, 1982", "XXXIX", "Don McMillan", "31", "Louis XV", "White Sox", "Korean Republic Won", "the book of Acts", "Executive Chef Danny Veltri", "Narendra Modi", "Ossie Schectman", "Max Martin", "Ray Conniff", "Lightning thief", "state legislators of Assam", "using a baby as bait", "the narrator", "Guy Berryman", "`` The person who has existence in two parallel worlds", "Spanish / Basque origin", "Johannes Gutenberg", "Jason Momoa", "Bumper Robinson", "John Lawrence Tone", "Elena Anaya", "Spirit", "2014", "Pangaea", "from Fort Kent, Maine, at the Canada -- US border, south to Key West, Florida", "Robin", "under the veil of the covering", "Peggy Lipton", "Nepal", "certain actions taken by employers or unions that violate the National Labor Relations Act", "December 12, 2017", "March 31 to April 8, 2018", "Manhattan", "1969", "on location", "Neil Young", "provinces along the Yangtze River and in provinces in the south", "Thomas Jefferson", "egypt", "Strictly Come Dancing", "Adam Dawes", "Robbie Gould", "Musicology", "two years,", "Lashkar-e-Tayyiba", "HPV", "Jackie Kennedy", "necropolis", "Spider-Man", "Fidel"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6739759015494309}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, false, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, false, false, false, true, true, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.18181818181818182, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 0.47058823529411764, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-5775", "mrqa_naturalquestions-validation-70", "mrqa_naturalquestions-validation-3275", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-2381", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8796", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-9107", "mrqa_triviaqa-validation-1975", "mrqa_hotpotqa-validation-1629", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1372", "mrqa_searchqa-validation-13420", "mrqa_searchqa-validation-8650"], "SR": 0.5625, "CSR": 0.5411305147058824, "retrieved_ids": ["mrqa_squad-train-86437", "mrqa_squad-train-27031", "mrqa_squad-train-34175", "mrqa_squad-train-60117", "mrqa_squad-train-81379", "mrqa_squad-train-70695", "mrqa_squad-train-70402", "mrqa_squad-train-288", "mrqa_squad-train-52623", "mrqa_squad-train-68567", "mrqa_squad-train-57515", "mrqa_squad-train-11319", "mrqa_squad-train-7260", "mrqa_squad-train-21537", "mrqa_squad-train-82765", "mrqa_squad-train-12235", "mrqa_newsqa-validation-2113", "mrqa_searchqa-validation-1399", "mrqa_naturalquestions-validation-2196", "mrqa_squad-validation-4546", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-13040", "mrqa_hotpotqa-validation-5117", "mrqa_newsqa-validation-2031", "mrqa_searchqa-validation-15830", "mrqa_squad-validation-3479", "mrqa_searchqa-validation-16559", "mrqa_naturalquestions-validation-9494", "mrqa_hotpotqa-validation-1585", "mrqa_triviaqa-validation-1291", "mrqa_searchqa-validation-2083", "mrqa_newsqa-validation-8"], "EFR": 0.9285714285714286, "Overall": 0.7238622636554621}, {"timecode": 68, "before_eval_results": {"predictions": ["Jeff Barry and Andy Kim", "Golde", "2018", "James Madison", "one season", "September 29, 2017", "c. 1000 AD", "Samantha Jo `` Mandy '' Moore", "March 1995", "1957", "Andrew McCarthy", "an investor couple", "Erica Rivera", "epidermis", "the cavities and surfaces of blood vessels and organs throughout the body", "the Norman given name Robert", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "11 : 15 p.m.", "Sylvester Stallone", "Frankel", "Director of National Intelligence", "November 25, 2002", "S", "Marley & Me", "State Bar of Arizona", "19th - century", "wintertime", "The Romantics", "W. Edwards Deming", "Jodie Sweetin", "a Native American nation", "1992", "presidential representative democratic republic", "Eddie Murphy", "Vincent Price", "Andy Serkis", "Jehnna ( Olivia d'Abo )", "O'Meara", "Kimberlin Brown", "writ of certiorari", "financial inflows to developing countries", "Tom Waits", "`` Reveille ''", "The Constitution of India", "StubHub Center", "1439", "plate tectonics", "Himadri Station", "Rajendra Prasad", "1997", "March 10, 2017", "b-Man", "Irish Setter", "Hattie McDaniel", "Tian Tan Buddha", "Bonkyll Castle", "Alf Clausen", "the helicopter crashed in northeastern Baghdad as a result of clashes between U.S.-backed Iraqi forces and gunmen.", "Peter Garrett", "Turkey", "piracy", "lifejackets", "a foot fault", "Josh"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6921995322385948}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, true, true, true, true, false, false, false, true, true, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.3333333333333333, 0.4615384615384615, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.3636363636363636, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9375, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-8950", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-6490", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-1340", "mrqa_triviaqa-validation-5724", "mrqa_hotpotqa-validation-3965", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-101", "mrqa_searchqa-validation-1397"], "SR": 0.609375, "CSR": 0.5421195652173914, "retrieved_ids": ["mrqa_squad-train-76092", "mrqa_squad-train-8926", "mrqa_squad-train-22442", "mrqa_squad-train-52660", "mrqa_squad-train-75426", "mrqa_squad-train-3527", "mrqa_squad-train-65393", "mrqa_squad-train-43070", "mrqa_squad-train-76203", "mrqa_squad-train-79321", "mrqa_squad-train-31452", "mrqa_squad-train-29082", "mrqa_squad-train-46609", "mrqa_squad-train-41287", "mrqa_squad-train-28766", "mrqa_squad-train-23642", "mrqa_searchqa-validation-10144", "mrqa_newsqa-validation-1419", "mrqa_naturalquestions-validation-4338", "mrqa_hotpotqa-validation-2696", "mrqa_triviaqa-validation-240", "mrqa_newsqa-validation-975", "mrqa_naturalquestions-validation-7901", "mrqa_newsqa-validation-828", "mrqa_hotpotqa-validation-2484", "mrqa_squad-validation-150", "mrqa_newsqa-validation-3965", "mrqa_hotpotqa-validation-59", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2985", "mrqa_hotpotqa-validation-2229", "mrqa_searchqa-validation-3032"], "EFR": 0.96, "Overall": 0.7303457880434783}, {"timecode": 69, "before_eval_results": {"predictions": ["Syracuse University", "The Fault in Our Stars", "Stadio Olimpico in Rome, Italy", "traditional music", "water", "May 4, 2004", "Northumbrian", "2001", "American Horror Story", "Salisbury", "University of Nevada, Las Vegas (UNLV)", "The third single from the album, \"Scars to Your Beautiful\"", "November of that year", "Morse Field", "small family car", "Jamel\u00e3o", "Laura Elizabeth \"Laurie\" Metcalf", "Kingdom of Dalmatia", "Russell T Davies", "George Balanchine", "Oliver Parker", "he turned 51, he died of cancer", "Snowball II", "Oakland", "FCI Danbury", "Division of Barton", "Bob Day", "the Appalachian Mountains", "1875", "London", "Shut Up", "James Lofton", "Big Bad Wolf", "Robert Marvin \"B Bobby\" Hull, OC", "books, films", "Iranian-American", "You're Next", "Bay of Fundy", "2009", "Umberto II", "2016 United States elections", "War Is the Answer", "1891", "Supergirl", "DI Humphrey Goodman", "Cinderella", "1,382", "left-hand or right-hand", "Syracuse", "Buckingham Palace", "Rhode Island", "Anglican", "powers in the Eastern Bloc ( the Soviet Union and its satellite states )", "1972 -- 81", "barba", "the Austrian Von Trapp family", "Ramadan", "the Dalai Lama's", "70,000 or so", "1994", "Lord Peter Wimsey", "carbonic acid", "neon", "Claire Goose"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6839590097402597}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9090909090909091, 0.0, 0.4, 0.0, 1.0, 0.0, 0.6666666666666666, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5619", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-3504", "mrqa_hotpotqa-validation-4315", "mrqa_hotpotqa-validation-3703", "mrqa_hotpotqa-validation-4352", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-4473", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-278", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5942", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-4021", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-1720", "mrqa_searchqa-validation-4425", "mrqa_searchqa-validation-10452"], "SR": 0.578125, "CSR": 0.5426339285714286, "retrieved_ids": ["mrqa_squad-train-61608", "mrqa_squad-train-58277", "mrqa_squad-train-52922", "mrqa_squad-train-9702", "mrqa_squad-train-51929", "mrqa_squad-train-79248", "mrqa_squad-train-76839", "mrqa_squad-train-53474", "mrqa_squad-train-54116", "mrqa_squad-train-41766", "mrqa_squad-train-54573", "mrqa_squad-train-42220", "mrqa_squad-train-14176", "mrqa_squad-train-43701", "mrqa_squad-train-84862", "mrqa_squad-train-11215", "mrqa_searchqa-validation-7791", "mrqa_newsqa-validation-2100", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-1702", "mrqa_hotpotqa-validation-106", "mrqa_squad-validation-7689", "mrqa_searchqa-validation-9803", "mrqa_searchqa-validation-10395", "mrqa_hotpotqa-validation-2728", "mrqa_searchqa-validation-12098", "mrqa_squad-validation-4010", "mrqa_hotpotqa-validation-3321", "mrqa_naturalquestions-validation-9107", "mrqa_searchqa-validation-6635", "mrqa_triviaqa-validation-2027", "mrqa_searchqa-validation-16836"], "EFR": 1.0, "Overall": 0.7384486607142857}, {"timecode": 70, "UKR": 0.791015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-139", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4646", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5611", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-875", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3330", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-9056", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1527", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1998", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3067", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3083", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10176", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11005", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13441", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-13909", "mrqa_searchqa-validation-14094", "mrqa_searchqa-validation-14204", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14752", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16534", "mrqa_searchqa-validation-16733", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-5457", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7045", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-7326", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-7903", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8807", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-944", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-10284", "mrqa_squad-validation-10352", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1498", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2123", "mrqa_squad-validation-215", "mrqa_squad-validation-2197", "mrqa_squad-validation-2248", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3464", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3750", "mrqa_squad-validation-3813", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-3904", "mrqa_squad-validation-4096", "mrqa_squad-validation-4469", "mrqa_squad-validation-457", "mrqa_squad-validation-4669", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-4949", "mrqa_squad-validation-5042", "mrqa_squad-validation-5325", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-5869", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-6636", "mrqa_squad-validation-682", "mrqa_squad-validation-6838", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-76", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-7932", "mrqa_squad-validation-8028", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8500", "mrqa_squad-validation-8529", "mrqa_squad-validation-8719", "mrqa_squad-validation-8754", "mrqa_squad-validation-8966", "mrqa_squad-validation-9165", "mrqa_squad-validation-9306", "mrqa_squad-validation-9531", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4953", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5928", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6784", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6791", "mrqa_triviaqa-validation-6879", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-934"], "OKR": 0.865234375, "KG": 0.48125, "before_eval_results": {"predictions": ["November 17, 2017", "Melissa Disney", "the Archies", "writ of certiorari", "Edgar Lungu", "December 25", "Marcus Atilius Regulus", "Ahmad Givens", "Sufi verse Tum Ek Gorakh Dhanda Ho ( You are a puzzle )", "San Francisco", "mash potato", "Gregor Mendel", "October 1, 2015", "New Zealand", "Pittsburgh", "pit road speed", "The Statue of Freedom", "101.325 kPa", "1923", "Xiu Li Dai and Yongge Dai", "Michael Schumacher", "deal with local administrative applications in common law jurisdictions", "John Quincy Adams", "Ray Charles", "NBC's Days of our Lives", "2013", "Internal epithelia", "Upstate New York", "Henry Purcell", "5,534", "Lord Irwin", "three", "Roman Reigns", "Efren Manalang Reyes", "the Persian Empire", "provide bridging funding for existing federal programs at current, reduced, or expanded levels", "2004", "`` Fix You ''", "Ed Roland", "Jos\u00e9 Mart\u00ed", "16 August 1975", "the state in which both reactants and products are present in concentrations which have no further tendency to change with time", "Sumitra", "The management team", "Ravi River", "Sir Rowland Hill", "Pangaea", "Sally Field", "a cylinder of glass or plastic that runs along the fiber's length", "Dr. Sachchidananda Sinha", "Barbara Windsor", "shale", "The Hague", "The World is Not Enough", "He also served as Chairperson of the Organisation of African Unity from 25 May 1963 to 17 July 1964", "Leona Lewis", "Hong Kong International", "\"We want to reset our relationship and so we will do it together.\"", "an average of 25 percent", "Evan Wolfson,", "(thsauros)", "Galileo Galilei", "Aristophanes", "Roy"], "metric_results": {"EM": 0.625, "QA-F1": 0.702872388028638}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4615384615384615, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428572, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-3587", "mrqa_naturalquestions-validation-8470", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-8326", "mrqa_triviaqa-validation-4467", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-5085", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-260", "mrqa_searchqa-validation-14422", "mrqa_searchqa-validation-10531"], "SR": 0.625, "CSR": 0.543794014084507, "retrieved_ids": ["mrqa_squad-train-81325", "mrqa_squad-train-11811", "mrqa_squad-train-13458", "mrqa_squad-train-31943", "mrqa_squad-train-77947", "mrqa_squad-train-55665", "mrqa_squad-train-77856", "mrqa_squad-train-9256", "mrqa_squad-train-76935", "mrqa_squad-train-7410", "mrqa_squad-train-53756", "mrqa_squad-train-11492", "mrqa_squad-train-26751", "mrqa_squad-train-83105", "mrqa_squad-train-15338", "mrqa_squad-train-74031", "mrqa_squad-validation-7763", "mrqa_hotpotqa-validation-5833", "mrqa_triviaqa-validation-4646", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-3621", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-5675", "mrqa_newsqa-validation-3733", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-5116", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-2729", "mrqa_searchqa-validation-12474", "mrqa_naturalquestions-validation-7144"], "EFR": 0.9583333333333334, "Overall": 0.7279254694835682}, {"timecode": 71, "before_eval_results": {"predictions": ["Jackie", "Chester", "superhuman abilities", "76,416", "Rawhide", "Chris Weidman", "Jacksonville Jacksonville Jacksonville", "Orfeo ed Euridice", "Martin Scorsese", "Citgo Petroleum Corporation", "Nihon K\u014dk\u016b Kabushiki-gaisha", "Southern Progress Corporation", "Laura Dern", "Conservative Party", "Mickey's PhilharMagic", "45%", "between 1252 and 1259", "Hamburger Sport-Verein e.V.", "Kolkata", "Liga MX", "Queen City", "Dz\u016bkija", "Texas", "six", "Rob Reiner", "The R-8 Human Rhythm Composer", "WAMC", "67,575", "Claude Mak\u00e9l\u00e9l\u00e9", "Ribosomes", "Sacramento Kings", "1945", "Fairfax County", "Syracuse University.", "John Mills", "Martin \"Marty\" McCann", "Blue Ridge Parkway", "Republic of Maldives", "Kegeyli rayon\u0131", "Ella", "2000", "quarterly", "William Bradley DuVall (born September 6, 1967)", "Coronation Street", "Duke", "Richard Masur", "anabolic steroids", "diving duck", "House of Hohenstaufen", "local South Australian and Australian produced content", "EBSCO Information Services", "Karen Taylor", "as a pH indicator, a color marker, and a dye", "Thomas Edison", "wool", "Runic", "cycling", "Brett Cummins,", "58", "Security officer Stephen Johns reportedly opened the door for the man police say was", "osprey", "Fyodor Dostoevsky", "Turtle Wax", "Cho Won-suk."], "metric_results": {"EM": 0.59375, "QA-F1": 0.6884548611111111}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, true, false, false, true, true, false, true, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.25, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1395", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-1220", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-5766", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-5558", "mrqa_hotpotqa-validation-2938", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-1000", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4357", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-7849", "mrqa_triviaqa-validation-4244", "mrqa_searchqa-validation-14348", "mrqa_newsqa-validation-1048"], "SR": 0.59375, "CSR": 0.5444878472222222, "retrieved_ids": ["mrqa_squad-train-54636", "mrqa_squad-train-25063", "mrqa_squad-train-34790", "mrqa_squad-train-8457", "mrqa_squad-train-59374", "mrqa_squad-train-42237", "mrqa_squad-train-24309", "mrqa_squad-train-51990", "mrqa_squad-train-25236", "mrqa_squad-train-51845", "mrqa_squad-train-48969", "mrqa_squad-train-39637", "mrqa_squad-train-16073", "mrqa_squad-train-77330", "mrqa_squad-train-41798", "mrqa_squad-train-43728", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1718", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-5085", "mrqa_hotpotqa-validation-3591", "mrqa_searchqa-validation-1971", "mrqa_naturalquestions-validation-9773", "mrqa_squad-validation-9195", "mrqa_newsqa-validation-384", "mrqa_hotpotqa-validation-3446", "mrqa_squad-validation-4096", "mrqa_searchqa-validation-4478", "mrqa_triviaqa-validation-4137", "mrqa_naturalquestions-validation-5396", "mrqa_hotpotqa-validation-486"], "EFR": 1.0, "Overall": 0.7363975694444445}, {"timecode": 72, "before_eval_results": {"predictions": ["the theory of direct scattering and inverse scattering", "Doctor of Philosophy (PhD)", "Australian Supercars Championship", "The Jacksonville Jaguars", "\"Crossed: Dead or Alive\"", "Philadelphia Naval Shipyard", "GmbH", "Abbey Road", "Laurie Metcalf", "Nine-card Brag", "\"God Spell\"", "freshman", "Free Range Films", "Mondays", "\"Beauty and the Beast\"", "Jehovah", "fourth President of Pakistan", "Tamara Ecclestone Rutland", "Newport", "Tomorrowland", "1985", "Dave Bautista", "1002", "1", "John Meston", "late 19th and early 20th centuries", "Terrence Jones", "1967", "Casey Bond", "actress", "Christian Kern", "Naomi Elaine Campbell", "The dyers of Lincoln", "The 2013\u201314 Premier League", "Ghostbusters Spooktacular", "an American financier", "Maldives", "Them", "its air-cushioned sole", "4,972", "Brittany Snow", "Perth, Western Australia", "Harry F. Sinclair", "Interstate 22", "William McKinley", "Denmark", "Tunisian", "lieutenant general", "Indian", "Sir Philip Anthony Hopkins", "Raimond Gaita", "13.5 %", "1980", "Isaiah Amir Mustafa", "Michigan", "Jon Stewart", "Australia", "seven", "Congress", "July", "Michelangelo", "airplanes", "the Confederate Memorial", "buying"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6779463158369409}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.1818181818181818, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.8, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-294", "mrqa_hotpotqa-validation-1612", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-723", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-1007", "mrqa_naturalquestions-validation-9824", "mrqa_triviaqa-validation-982", "mrqa_newsqa-validation-4127", "mrqa_newsqa-validation-270", "mrqa_searchqa-validation-16142", "mrqa_searchqa-validation-5340"], "SR": 0.59375, "CSR": 0.5451626712328768, "retrieved_ids": ["mrqa_squad-train-17413", "mrqa_squad-train-51487", "mrqa_squad-train-78682", "mrqa_squad-train-57588", "mrqa_squad-train-2445", "mrqa_squad-train-64083", "mrqa_squad-train-16513", "mrqa_squad-train-42071", "mrqa_squad-train-62377", "mrqa_squad-train-833", "mrqa_squad-train-45168", "mrqa_squad-train-57583", "mrqa_squad-train-77867", "mrqa_squad-train-44586", "mrqa_squad-train-76042", "mrqa_squad-train-83219", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-5821", "mrqa_naturalquestions-validation-6794", "mrqa_newsqa-validation-2743", "mrqa_naturalquestions-validation-2588", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-3468", "mrqa_searchqa-validation-15141", "mrqa_searchqa-validation-2638", "mrqa_hotpotqa-validation-4239", "mrqa_newsqa-validation-1181", "mrqa_hotpotqa-validation-5854", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-2227", "mrqa_naturalquestions-validation-9368"], "EFR": 1.0, "Overall": 0.7365325342465754}, {"timecode": 73, "before_eval_results": {"predictions": ["Scandinavian Airlines System Aktiebolag", "April 18, 1972", "Indianapolis", "a lauded intellectual", "1970", "the 1745 rebellion", "96", "distance runner", "West African descendants", "Sparafucile", "James Stenbeck", "December 19, 1998", "Philip Quast", "November 20, 1942", "Argentine cuisine", "1 January 2007", "Matt Lucas", "Waylon J. Smithers Jr.", "2006", "Scotland", "Lakshmibai", "Umberto II", "England", "beer and soft drinks", "five", "C. H. Greenblatt", "tentacles", "1505\u201344", "There Is Only the Fight... : An Analysis of the Alinsky Model", "England", "Lord Gort", "Laurel, Mississippi", "antelope", "Wayne Conley", "churros", "1802", "Key West", "Pablo Escobar", "David Michael Bautista Jr.", "Iftikhar Ali Khan", "October 25, 1881", "Bob Dylan", "\"Complex\" magazine", "Harrison Ford", "PBS", "Cristiano Ronaldo", "1943", "wineries", "Animorphs", "Jeff Meldrum", "quantum mechanics", "early - to - mid fourth century", "the gated community of Pebble Beach", "$2.187 billion", "Iona", "Jeffrey Archer", "Spearchucker", "Galveston, Texas,", "celebrities", "are concerned that the legislation will foster racial profiling, arguing that most police officers don't have enough training to look past race while investigating a person's legal status.", "the Ohio", "the Aplodintidae family", "W. Somerset Maugham", "bullfights"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6835441468253969}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, false, true, false, true, false, false, true, false, true, true], "QA-F1": [0.0, 0.5, 0.6666666666666666, 0.4444444444444445, 1.0, 0.28571428571428575, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-3153", "mrqa_hotpotqa-validation-5060", "mrqa_hotpotqa-validation-1619", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-1423", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-4446", "mrqa_hotpotqa-validation-5747", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-3363", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-2364", "mrqa_hotpotqa-validation-1116", "mrqa_naturalquestions-validation-3363", "mrqa_triviaqa-validation-535", "mrqa_triviaqa-validation-7264", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-3584", "mrqa_searchqa-validation-11034"], "SR": 0.609375, "CSR": 0.5460304054054055, "retrieved_ids": ["mrqa_squad-train-67309", "mrqa_squad-train-14244", "mrqa_squad-train-44401", "mrqa_squad-train-35607", "mrqa_squad-train-39092", "mrqa_squad-train-18419", "mrqa_squad-train-18325", "mrqa_squad-train-50294", "mrqa_squad-train-86497", "mrqa_squad-train-84126", "mrqa_squad-train-85915", "mrqa_squad-train-34401", "mrqa_squad-train-6709", "mrqa_squad-train-64291", "mrqa_squad-train-35962", "mrqa_squad-train-34153", "mrqa_newsqa-validation-2653", "mrqa_searchqa-validation-16219", "mrqa_hotpotqa-validation-2486", "mrqa_searchqa-validation-14778", "mrqa_naturalquestions-validation-886", "mrqa_triviaqa-validation-4032", "mrqa_squad-validation-6526", "mrqa_newsqa-validation-270", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-4257", "mrqa_hotpotqa-validation-59", "mrqa_newsqa-validation-2213", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-63", "mrqa_searchqa-validation-1073", "mrqa_squad-validation-10321"], "EFR": 0.96, "Overall": 0.728706081081081}, {"timecode": 74, "before_eval_results": {"predictions": ["1947", "Bury, Greater Manchester, England", "Nye County", "December 18, 1916 \u2013 July 2, 1973", "two-time", "a governor", "40 Acres", "The Panther", "al-Qaeda", "Soviet Union", "India", "Skyscraper", "17 October 2006", "April 1, 1949", "Big Machine Records", "Brady John Haran", "Jesus", "Heywood \"Woody\" Allen", "moth", "Elliot Fletcher", "Joe McCoy and Memphis Minnie", "Johnny McDaid", "Christopher McCulloch", "Strange Interlude", "1989 until 1994", "Tallahassee City Commission", "the Royal Navy", "Mika H\u00e4kkinen", "World Music Awards", "1998", "Peter 'Drago' Sell", "sitcom \"Barney Miller\"", "oyote Ugly", "smell odors and aromas from the film via scratch & sniff cards", "A Song of Ice and Fire", "Andy Roddick", "The Dewey Lake Monster", "Kim Jong-hyun", "The game telecast airs every Friday night at 7:45pm ET during the college football regular season", "A Boltzmann machine", "January 1930", "12", "\"Pour le M\u00e9rite\"", "Franklin, Indiana", "zoonotic", "August 6, 1845", "Manchester Airport", "Miracle", "Elizabeth River", "Metrolink", "\"Complex\" magazine", "the oneness of the body, the church, through what Christians have in common, what they have communion in", "Covington, Kentucky", "Elizabeth Dean Lail", "doubles", "Newcastle Falcons", "Rihanna", "British Prime Minister", "order after demonstrators rose up across Greece", "a nuclear weapon", "France, Spain, England", "the City of Bridgeport", "a gag", "iPS cells"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7219428396358544}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.5, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.23529411764705882, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-3848", "mrqa_hotpotqa-validation-2520", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-463", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-3336", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-5475", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-2579", "mrqa_triviaqa-validation-2322", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-122", "mrqa_searchqa-validation-5810", "mrqa_searchqa-validation-14546", "mrqa_searchqa-validation-12635"], "SR": 0.609375, "CSR": 0.546875, "retrieved_ids": ["mrqa_squad-train-78170", "mrqa_squad-train-32452", "mrqa_squad-train-7649", "mrqa_squad-train-14356", "mrqa_squad-train-40302", "mrqa_squad-train-28702", "mrqa_squad-train-11774", "mrqa_squad-train-37681", "mrqa_squad-train-38422", "mrqa_squad-train-63001", "mrqa_squad-train-58153", "mrqa_squad-train-8024", "mrqa_squad-train-74446", "mrqa_squad-train-43462", "mrqa_squad-train-43556", "mrqa_squad-train-21313", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1537", "mrqa_searchqa-validation-15784", "mrqa_hotpotqa-validation-4194", "mrqa_triviaqa-validation-920", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-859", "mrqa_naturalquestions-validation-9284", "mrqa_hotpotqa-validation-294", "mrqa_naturalquestions-validation-6573", "mrqa_hotpotqa-validation-217", "mrqa_searchqa-validation-4192", "mrqa_newsqa-validation-316", "mrqa_triviaqa-validation-4496"], "EFR": 1.0, "Overall": 0.7368750000000001}, {"timecode": 75, "before_eval_results": {"predictions": ["horn", "Shanghai", "Paul Bunyan", "Peter Davison", "Pandora", "Reservoir", "Alaska", "Cowboy Builders", "The DMC-12", "presidential helicopter", "Lundey", "Alamo Rent A Car", "David Hockney", "spark-ignition", "Janis Joplin", "long", "Humphrey Bogart", "pig", "Antoine Lavoisier", "1960", "Argentina", "King County Executive", "Steely Dan", "U.S. ambassador to the United Nations", "Jane Austen", "the Cuban missile crisis", "plaster", "The Female Brain", "algae", "Venus", "Declaration of Independence", "decorate", "hot chocolate", "Jim Peters", "Armageddon", "Kansas", "carry On carry On", "Mitte", "Pangaea", "salmon", "the Wild Bunch", "815", "Project Gutenberg", "Bloodaxe", "California", "Nissan", "Isar", "The Man with the Golden Gun", "Salvador Allende", "The Green Mile", "Poland", "around 1600 BC", "Sean O' Neal", "May 18, 2018", "Dissection", "Baden-W\u00fcrttemberg", "March", "$60 billion on America's infrastructure.", "Mary Phagan Kean", "$500,000", "vitamin A", "Azkaban", "conga drums", "Vibe"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6351648351648351}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.30769230769230765, 0.4, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2230", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-5007", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-444", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-2875", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-191", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-225", "mrqa_triviaqa-validation-4612", "mrqa_triviaqa-validation-85", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3944", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-2638", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-7549", "mrqa_hotpotqa-validation-65", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3850"], "SR": 0.578125, "CSR": 0.5472861842105263, "retrieved_ids": ["mrqa_squad-train-66940", "mrqa_squad-train-55790", "mrqa_squad-train-1318", "mrqa_squad-train-40231", "mrqa_squad-train-52213", "mrqa_squad-train-20828", "mrqa_squad-train-73793", "mrqa_squad-train-37513", "mrqa_squad-train-49034", "mrqa_squad-train-60598", "mrqa_squad-train-40797", "mrqa_squad-train-39650", "mrqa_squad-train-72097", "mrqa_squad-train-11760", "mrqa_squad-train-66129", "mrqa_squad-train-76841", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-2030", "mrqa_triviaqa-validation-7614", "mrqa_searchqa-validation-732", "mrqa_naturalquestions-validation-373", "mrqa_triviaqa-validation-7585", "mrqa_hotpotqa-validation-5307", "mrqa_searchqa-validation-10452", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-220", "mrqa_hotpotqa-validation-2886", "mrqa_newsqa-validation-850", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-3200", "mrqa_newsqa-validation-3306"], "EFR": 0.9629629629629629, "Overall": 0.7295498294346979}, {"timecode": 76, "before_eval_results": {"predictions": ["the murders of his father and brother.", "\" happy ending\" to the case.", "head injury.", "Bryant Purvis", "1983", "Chaffetz", "Bryant Purvis", "jazz", "$30 million,", "1994,", "his album \"Tha Carter III\"", "peanuts, nuts, shellfish and fish", "fusion teams", "April 2010.", "of", "October 19,", "Addis Ababa,", "Barack Obama as the next president of the United States.", "five", "SSM Cardinal Glennon Children's Medical Center in St. Louis.", "Gadhafi's son, Mutassim, and his former defense minister, Abu Baker Yunis.", "off Somalia's coast.", "the Indian army and separatist militants in Indian-administered", "The Charlie Daniels Band,", "France's", "growing crowded,", "Saturday.", "Obama", "\"a whole new treasure trove of fossils\"", "and renewable energy at home everyday,\"", "insect stings,", "Kenneth Cole", "a one-shot victory in the Bob Hope Classic on the final hole", "Natalie Cole's", "\"The people kill him with the blocks,", "Hundreds", "Current TV", "Colorado prosecutor", "100% of its byproducts", "in a stream in shark River Park in Monmouth County", "Galveston, Texas, to Veracruz, Mexico,", "nine-wicket", "70,000", "the Nazi war crimes suspect", "his former caddy,", "three", "fatally shooting a limo driver", "named his company Polo because \"it was the sport of kings. It was glamorous, sexy and international.\"", "overhaul domestic policies", "President Barack Obama,", "Patrick McGoohan,", "Hathi Jr", "to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "Bacon", "raw hides", "Verdi", "fort boyard", "1,521", "Australian", "Edward R. Murrow", "Dizzy Gillespie", "Duncan", "Douglas MacArthur", "neo-Nazi"], "metric_results": {"EM": 0.671875, "QA-F1": 0.727899952470023}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10526315789473685, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.1111111111111111, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8125000000000001, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2382", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-2385", "mrqa_newsqa-validation-2610", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-4012", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2061", "mrqa_naturalquestions-validation-3184", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-6091", "mrqa_hotpotqa-validation-4863", "mrqa_searchqa-validation-7423", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-7657"], "SR": 0.671875, "CSR": 0.5489042207792207, "retrieved_ids": ["mrqa_squad-train-2573", "mrqa_squad-train-46620", "mrqa_squad-train-70314", "mrqa_squad-train-22963", "mrqa_squad-train-40389", "mrqa_squad-train-79", "mrqa_squad-train-816", "mrqa_squad-train-73982", "mrqa_squad-train-77061", "mrqa_squad-train-19313", "mrqa_squad-train-3082", "mrqa_squad-train-30749", "mrqa_squad-train-32566", "mrqa_squad-train-31378", "mrqa_squad-train-41303", "mrqa_squad-train-77396", "mrqa_hotpotqa-validation-5587", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-9141", "mrqa_hotpotqa-validation-4676", "mrqa_newsqa-validation-2415", "mrqa_hotpotqa-validation-3402", "mrqa_searchqa-validation-1869", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-2984", "mrqa_naturalquestions-validation-8439", "mrqa_triviaqa-validation-4714", "mrqa_hotpotqa-validation-5359", "mrqa_hotpotqa-validation-2042", "mrqa_naturalquestions-validation-1295", "mrqa_newsqa-validation-2839", "mrqa_hotpotqa-validation-1394"], "EFR": 1.0, "Overall": 0.7372808441558443}, {"timecode": 77, "before_eval_results": {"predictions": ["\" Terry the Tomboy\"", "Gatwick Airport", "Abdul Razzak Yaqoob", "Robert Matthew Hurley", "William Shakespeare", "Syracuse University", "American", "\"Peshwa\" (Prime Minister)", "eastern Tennessee, United States", "Lowe's Companies, Inc.", "Arthur Schnitzler's 1926 novella \"Traumnovelle\" (\"Dream Story\"),", "Hindi", "\"Big Mamie\"", "first", "Kate Millett", "Sydney, New South Wales, Australia", "Democritus", "Centre of Excellence", "Columbine", "381.6 days", "Richard Strauss", "West Cheshire League Division Two", "1994", "Yorgos Lanthimos", "Roscoe Lee Browne", "the \"Black Abbots\"", "chocolate-colored", "265 million", "\"The Brothers Karamazov\"", "Netrobalane canopus", "Telugu and Tamil", "Australian-American", "Albert Park", "\"Two Pi\u00f1a Coladas\"", "Brenton Thwaites", "Charles and Thomas Guard", "1919", "Elise Marie Stefanik", "Lower Manhattan, New York City", "King of the Polish-Lithuanian Commonwealth", "pop music and popular culture", "Almeda Mall", "Ronald Joseph Ryan", "robot Overlords", "Doctor of Philosophy", "McClelland and Stewart", "Madeleine L' Engle", "February 5, 2015", "Nathan Bedford Forrest", "the Battle of Iwo Jima", "capital crimes or capital offences", "the Tigris and Euphrates rivers", "between 1765 and 1783", "During his first year in Spain, Messi rarely played with the Infantiles due to a transfer conflict with Newell's ; as a foreigner, he could only be fielded in friendlies and the Catalan league", "christopher", "Werner Heisenberg", "the Liberal Democrats", "hundreds", "change course", "in a Johannesburg church that has become a de facto transit camp,", "Poses", "\"Pig in a poke\"", "October", "Mandi Hamlin"], "metric_results": {"EM": 0.59375, "QA-F1": 0.68359375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 0.6, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.25, 1.0, 0.5, 0.8, 1.0, 0.5, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-2340", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-266", "mrqa_hotpotqa-validation-2550", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-1745", "mrqa_hotpotqa-validation-5420", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-5297", "mrqa_hotpotqa-validation-3130", "mrqa_hotpotqa-validation-5569", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-2009", "mrqa_newsqa-validation-3946", "mrqa_newsqa-validation-2651", "mrqa_searchqa-validation-11271", "mrqa_newsqa-validation-391"], "SR": 0.59375, "CSR": 0.5494791666666667, "retrieved_ids": ["mrqa_squad-train-69008", "mrqa_squad-train-40148", "mrqa_squad-train-8832", "mrqa_squad-train-59659", "mrqa_squad-train-75784", "mrqa_squad-train-79879", "mrqa_squad-train-84966", "mrqa_squad-train-66585", "mrqa_squad-train-58010", "mrqa_squad-train-21899", "mrqa_squad-train-64873", "mrqa_squad-train-39299", "mrqa_squad-train-29590", "mrqa_squad-train-1233", "mrqa_squad-train-37773", "mrqa_squad-train-16348", "mrqa_searchqa-validation-12784", "mrqa_newsqa-validation-949", "mrqa_triviaqa-validation-7304", "mrqa_squad-validation-5751", "mrqa_naturalquestions-validation-3783", "mrqa_newsqa-validation-2470", "mrqa_searchqa-validation-13176", "mrqa_searchqa-validation-12135", "mrqa_newsqa-validation-1372", "mrqa_hotpotqa-validation-4357", "mrqa_searchqa-validation-10099", "mrqa_hotpotqa-validation-5854", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2903", "mrqa_triviaqa-validation-2291", "mrqa_hotpotqa-validation-3169"], "EFR": 1.0, "Overall": 0.7373958333333335}, {"timecode": 78, "before_eval_results": {"predictions": ["a albatross", "Gianlorenzo Bernini", "Yerette", "Denzel Washington", "a prologue", "Ben- Hur: A Tale of the Christ", "Al Capone", "a prism schism", "Bucharest", "Tennessee", "Dick Wolf", "a shovel", "Helena Bonham Carter", "Cincinnati", "Friday", "Miss Havisham", "the River Thames", "at the top", "high", "New Jersey", "Tarsus", "the gold rush", "grain", "Eli Whitney", "Breckenridge", "A.D. 1045, gunpowder rockets", "*bishops", "Esperanto", "Hundred Years' War", "Mending Wall", "Twelfth Night", "Christian Slater", "Polyphemus", "Special Boat Teams", "bananas", "Today Show", "Sally Field", "earmarks", "Turin", "antonyms", "a septum", "The Golden Girls", "polo", "Wikipedia", "the Maritimes", "1773", "Rich Daley", "nanocrystal", "Bee", "best man", "Missouri Waltz", "The Romantics", "Danny Veltri", "`` Tip and Ty ''", "sotto voce", "byward", "Wildcats", "Knowlton School", "Ronald Wilson Reagan", "The interview", "U.S. Agency for International Development", "183", "Jacob Zuma", "1987"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7247023809523809}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14500", "mrqa_searchqa-validation-14670", "mrqa_searchqa-validation-1713", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-7541", "mrqa_searchqa-validation-14621", "mrqa_searchqa-validation-8526", "mrqa_searchqa-validation-7331", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-12526", "mrqa_searchqa-validation-7961", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-2894", "mrqa_searchqa-validation-3743", "mrqa_searchqa-validation-5831", "mrqa_naturalquestions-validation-4552", "mrqa_triviaqa-validation-5498", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-5573", "mrqa_newsqa-validation-93"], "SR": 0.65625, "CSR": 0.5508306962025317, "retrieved_ids": ["mrqa_squad-train-44006", "mrqa_squad-train-13733", "mrqa_squad-train-39679", "mrqa_squad-train-81242", "mrqa_squad-train-15163", "mrqa_squad-train-48963", "mrqa_squad-train-15174", "mrqa_squad-train-36110", "mrqa_squad-train-68191", "mrqa_squad-train-53306", "mrqa_squad-train-40381", "mrqa_squad-train-47557", "mrqa_squad-train-64751", "mrqa_squad-train-47789", "mrqa_squad-train-73150", "mrqa_squad-train-29570", "mrqa_newsqa-validation-3172", "mrqa_naturalquestions-validation-2943", "mrqa_squad-validation-3676", "mrqa_naturalquestions-validation-886", "mrqa_newsqa-validation-3277", "mrqa_hotpotqa-validation-5199", "mrqa_searchqa-validation-13909", "mrqa_hotpotqa-validation-5547", "mrqa_searchqa-validation-13152", "mrqa_searchqa-validation-944", "mrqa_naturalquestions-validation-8933", "mrqa_triviaqa-validation-4572", "mrqa_newsqa-validation-3293", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-1312", "mrqa_newsqa-validation-4030"], "EFR": 0.9545454545454546, "Overall": 0.7285752301495972}, {"timecode": 79, "before_eval_results": {"predictions": ["minnie", "pale yellow", "Tuscaloosa", "fish", "six", "unicorns", "david seville", "whiskas", "Becher's Brook", "a spear", "cabinets", "the royal court", "Mujib", "a scarlet tanager", "Jack Lemmon", "long jump", "kipps: simple Soul", "pound", "Venezuela", "abbey", "President George H.W. Bush", "george gooch", "mulligan", "2.2046", "Hugh Hefner", "Florentius", "Kofi Annan", "Marina Piccola", "left", "George Eliot", "Richard II", "meadows", "mount of light", "smartphones", "plymouth rock", "Israelites", "South Africa", "Topeka", "Brazil", "george osborne", "Ever Lannasing Circles", "Florence", "Sicily", "Nirvana and Kiss", "Space Oddity", "the Smiths", "russell", "Jeffery Deaver", "\u00c9dith Piaf", "Cuba", "(ICRC)", "Asuka", "Canada south of the Arctic", "Beldam / Other Mother", "Ballarat Bitter", "Mani", "\"the backside.\"", "110 mph,", "\"We have several hundred people working for us in Indianapolis [alone].\"", "work together to stabilize Somalia and cooperate in security and military operations.", "Mike Rowe", "an Enigma", "Edinburgh", "speech"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5461786477411478}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, true, false, false, true, true, true, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-5320", "mrqa_triviaqa-validation-888", "mrqa_triviaqa-validation-2346", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-1090", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-3033", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-1601", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-1678", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-5972", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-5529", "mrqa_triviaqa-validation-5747", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-492", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-6818", "mrqa_naturalquestions-validation-1872", "mrqa_naturalquestions-validation-2851", "mrqa_hotpotqa-validation-507", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2234", "mrqa_searchqa-validation-328"], "SR": 0.484375, "CSR": 0.55, "retrieved_ids": ["mrqa_squad-train-9474", "mrqa_squad-train-27068", "mrqa_squad-train-86244", "mrqa_squad-train-4495", "mrqa_squad-train-81797", "mrqa_squad-train-47169", "mrqa_squad-train-30820", "mrqa_squad-train-24668", "mrqa_squad-train-34237", "mrqa_squad-train-85230", "mrqa_squad-train-42739", "mrqa_squad-train-86238", "mrqa_squad-train-22060", "mrqa_squad-train-60428", "mrqa_squad-train-73343", "mrqa_squad-train-11552", "mrqa_naturalquestions-validation-74", "mrqa_triviaqa-validation-4625", "mrqa_naturalquestions-validation-2818", "mrqa_hotpotqa-validation-4483", "mrqa_triviaqa-validation-3379", "mrqa_triviaqa-validation-7226", "mrqa_hotpotqa-validation-5655", "mrqa_squad-validation-7819", "mrqa_triviaqa-validation-4032", "mrqa_searchqa-validation-15872", "mrqa_naturalquestions-validation-6285", "mrqa_newsqa-validation-1333", "mrqa_hotpotqa-validation-5115", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-2730", "mrqa_searchqa-validation-16246"], "EFR": 0.9696969696969697, "Overall": 0.731439393939394}, {"timecode": 80, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1619", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2267", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3360", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3967", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4646", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5343", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-681", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3330", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6754", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1527", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1974", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1998", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3083", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-10176", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-10493", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13104", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-14094", "mrqa_searchqa-validation-14204", "mrqa_searchqa-validation-14348", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16534", "mrqa_searchqa-validation-16733", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-16891", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-5457", "mrqa_searchqa-validation-5831", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7045", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8807", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-944", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2248", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3750", "mrqa_squad-validation-3813", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-4469", "mrqa_squad-validation-4669", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-5042", "mrqa_squad-validation-5325", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-5869", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-682", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-7932", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8500", "mrqa_squad-validation-8529", "mrqa_squad-validation-8719", "mrqa_squad-validation-8754", "mrqa_squad-validation-8966", "mrqa_squad-validation-9306", "mrqa_squad-validation-9531", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5167", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5928", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6791", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7717", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-934"], "OKR": 0.869140625, "KG": 0.51328125, "before_eval_results": {"predictions": ["eight", "habsburg monarchy", "Levi", "Lucy", "hedgehog", "liqueur", "Carlos Santana", "Hungary", "Greek", "groucho Marx", "hms Guerriere", "collage", "maycomb", "fish", "cleveland", "1966", "black monks", "Billy Fury", "Tuesday", "gybeer", "Whist", "George Clooney", "scouting and data sides", "A4", "Mussolini", "george osborne", "Margaret Thatcher", "nylvester McCoy", "grasses", "Jacob", "Something In The Air", "Chicago", "Hague", "thailand", "Inigo Montoya", "President Nixon", "Pearl Jam", "Aleister Crowley", "ruf Dwight", "arm\u0101ta", "setts", "batsman", "mecenae", "mountain", "Saddam Hussein", "Tombstone", "liriope", "Indonesia", "Linford Christie", "Swiss", "Daedalus", "twice", "starch", "David Ben - Gurion", "DTM", "Tetrahydrogestrinone (THG)", "7pm", "Ma Khin Khin Leh,", "prisoners at the South Dakota State Penitentiary", "Shanghai", "a dialysis line", "Dick & Jane", "\"think big\"", "on the inner edge of the galaxy"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5427083333333333}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-7274", "mrqa_triviaqa-validation-5921", "mrqa_triviaqa-validation-2957", "mrqa_triviaqa-validation-7182", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-6871", "mrqa_triviaqa-validation-5580", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7475", "mrqa_triviaqa-validation-5378", "mrqa_triviaqa-validation-4563", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-1372", "mrqa_triviaqa-validation-5332", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-2928", "mrqa_triviaqa-validation-5168", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-7616", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-6791", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9726", "mrqa_hotpotqa-validation-3117", "mrqa_searchqa-validation-4839", "mrqa_naturalquestions-validation-808"], "SR": 0.515625, "CSR": 0.5495756172839505, "retrieved_ids": ["mrqa_squad-train-68084", "mrqa_squad-train-30538", "mrqa_squad-train-23371", "mrqa_squad-train-24700", "mrqa_squad-train-23465", "mrqa_squad-train-666", "mrqa_squad-train-52905", "mrqa_squad-train-18847", "mrqa_squad-train-75641", "mrqa_squad-train-47770", "mrqa_squad-train-85583", "mrqa_squad-train-66537", "mrqa_squad-train-9030", "mrqa_squad-train-76513", "mrqa_squad-train-15459", "mrqa_squad-train-13353", "mrqa_searchqa-validation-12860", "mrqa_naturalquestions-validation-8628", "mrqa_searchqa-validation-14923", "mrqa_newsqa-validation-2135", "mrqa_newsqa-validation-2763", "mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-2799", "mrqa_squad-validation-4877", "mrqa_newsqa-validation-3597", "mrqa_squad-validation-4027", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1718", "mrqa_searchqa-validation-10394", "mrqa_newsqa-validation-475", "mrqa_hotpotqa-validation-3638", "mrqa_newsqa-validation-4025"], "EFR": 0.9032258064516129, "Overall": 0.7232946597471127}, {"timecode": 81, "before_eval_results": {"predictions": ["Cleckheaton", "the Haitian Revolution", "pommel horse", "1", "india robin", "philosopher", "people", "cannons", "Columba", "julius", "a power outage", "snapdragons", "weather", "\u201cFor Gallantry\u201d", "Zachary Taylor", "1951", "bachus- polka", "Venice", "Cambridge", "a snake", "Saturn", "Liverpool", "carb", "nixon", "bexhill", "Laos", "Florida", "One afternoon in 1866, John Pemberton, an Atlanta pharmacist, stirred up a fr ``grant, caramel-colored liquid in a three legged kettle", "vanilla", "Patrick Henry", "Apocalypse Now", "nelson", "Let It Snow", "elkie Brooks", "cruisin", "edwina currie", "Pyotr Ilich Tchaikovsky", "turkish", "Edinburgh", "Charlotte Corday", "Algiers", "Queen Mary II", "golf", "yellow", "Norwich", "Trenton", "the hose", "a sea otter", "saffron", "Ghana", "daniel ostroff", "243 days", "the Sunni Muslim family", "Terry Reid", "due to a leg injury", "8 May 1989", "David Yates", "Mohamed Mohamud Qeyre", "15-year-old's", "the News of the World tabloid.", "a Referendum", "a bear", "pemmican", "Reform"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5821428571428571}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, true, false, true, true, true, false, true, true, false, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, true, false, false, false, true, false, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.05714285714285715, 0.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-6689", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-6399", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-2942", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-4943", "mrqa_triviaqa-validation-1944", "mrqa_triviaqa-validation-7747", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-7053", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-262", "mrqa_triviaqa-validation-5145", "mrqa_triviaqa-validation-704", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-522", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-3778", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-1282", "mrqa_searchqa-validation-8180", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-14428"], "SR": 0.53125, "CSR": 0.5493521341463414, "retrieved_ids": ["mrqa_squad-train-7556", "mrqa_squad-train-45748", "mrqa_squad-train-6756", "mrqa_squad-train-50815", "mrqa_squad-train-55354", "mrqa_squad-train-26870", "mrqa_squad-train-48749", "mrqa_squad-train-62108", "mrqa_squad-train-11373", "mrqa_squad-train-86583", "mrqa_squad-train-76986", "mrqa_squad-train-58210", "mrqa_squad-train-60376", "mrqa_squad-train-84143", "mrqa_squad-train-84784", "mrqa_squad-train-79293", "mrqa_searchqa-validation-7657", "mrqa_hotpotqa-validation-3238", "mrqa_triviaqa-validation-7724", "mrqa_newsqa-validation-1333", "mrqa_searchqa-validation-3032", "mrqa_hotpotqa-validation-3022", "mrqa_searchqa-validation-4956", "mrqa_naturalquestions-validation-1786", "mrqa_newsqa-validation-2467", "mrqa_hotpotqa-validation-1423", "mrqa_triviaqa-validation-2643", "mrqa_searchqa-validation-10838", "mrqa_hotpotqa-validation-84", "mrqa_newsqa-validation-1107", "mrqa_squad-validation-457", "mrqa_squad-validation-1489"], "EFR": 0.9, "Overall": 0.7226048018292682}, {"timecode": 82, "before_eval_results": {"predictions": ["from long Sutton to Bury St Edmunds", "the secant", "thaksin", "tissues", "Yardbirds", "Brazil", "1123", "Queen Elizabeth II", "b Bruce Alexander", "niece", "Manhunt 2", "Carson City", "special sauce", "jewellers", "a GUI", "smith", "the wren", "Northwestern University", "Love Never Dies", "lunar new year holiday", "island", "Frank Saul", "steam locomotive", "David Davis", "Eric Coates", "\u00c9dith Piaf", "george i", "anteros", "rugby", "Alabama", "antelope", "Breakfast Club", "draft horse", "baldrick", "John Donne", "Loch lomond", "Salt Lake City", "red", "a toy modeled after a character on the fictional television show Woody's Roundup", "the Battle of Austerlitz", "Venus", "compact pussycat", "tintin", "Mercury", "guitar", "Celsius", "Phil Spector", "germany", "Hans Lippershey", "robin robin", "Yann Martel", "foreign investors", "the Gilbert building", "Panzerkampfwagen VIII Maus ( `` Mouse '' )", "second", "The Sleaford and North Hykeham by-election", "Four Weddings and a Funeral", "his mother, Katherine Jackson, his three children and undisclosed charities.", "The last survivor of the Titanic, 97-year-old Millvina Dean,", "1979", "Iraq", "Phil Mickelson", "the Crimean War", "Larry Ellison,"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6469629329004329}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-709", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-6143", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-2425", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-7334", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-4127", "mrqa_triviaqa-validation-7414", "mrqa_triviaqa-validation-4859", "mrqa_triviaqa-validation-5389", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-7246", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-1594"], "SR": 0.59375, "CSR": 0.5498870481927711, "retrieved_ids": ["mrqa_squad-train-46320", "mrqa_squad-train-71915", "mrqa_squad-train-80065", "mrqa_squad-train-45840", "mrqa_squad-train-65881", "mrqa_squad-train-46590", "mrqa_squad-train-15336", "mrqa_squad-train-85845", "mrqa_squad-train-10223", "mrqa_squad-train-40137", "mrqa_squad-train-71920", "mrqa_squad-train-4919", "mrqa_squad-train-107", "mrqa_squad-train-73637", "mrqa_squad-train-62854", "mrqa_squad-train-37038", "mrqa_hotpotqa-validation-4194", "mrqa_newsqa-validation-2618", "mrqa_triviaqa-validation-6791", "mrqa_newsqa-validation-1468", "mrqa_hotpotqa-validation-5305", "mrqa_naturalquestions-validation-303", "mrqa_squad-validation-5213", "mrqa_hotpotqa-validation-2786", "mrqa_newsqa-validation-959", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-4960", "mrqa_searchqa-validation-2416", "mrqa_naturalquestions-validation-9342", "mrqa_newsqa-validation-2245", "mrqa_hotpotqa-validation-4833", "mrqa_naturalquestions-validation-158"], "EFR": 1.0, "Overall": 0.7427117846385543}, {"timecode": 83, "before_eval_results": {"predictions": ["lymph node", "Swedish", "Toyota", "March 19", "Julie Andrews", "fury", "fungi", "Verona", "Phil Spector", "fidelio", "norway encore", "minder", "Pisces", "peppers", "yMCA", "valley", "macbeth", "crossword puzzle", "Diana Ross", "burt Reinhardt", "nettle leaves", "cartoons", "nautes", "riyal", "Paul Rudd", "bertrand egypt", "one Thousand and one", "george arundel", "sewing machine", "Bristol Aeroplane Company", "piano", "Tutankhamun", "a colony", "fedora", "the Indus valley", "Helen Gurley Brown", "tzonese alexander Heath", "krakatoa", "pinocchio", "Oregon", "aaron alexander", "belshazzar hartman", "head unit", "biluim", "graveolens", "Ken Platt", "handley Page", "julia hargreaves", "Sarah Vaughan", "\"Mr Loophole\"", "1", "Shrek", "Watson and Crick", "in the axial skeleton", "25 November 2015", "Jean Erdman", "the Netherlands", "9 a.m.", "to the U.S. Holocaust Memorial Museum", "following suit,", "Robert Langdon", "Parris Island", "watts", "a sunflower"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6175347222222223}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, true, false, true, false, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5, 0.4, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4062", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-6348", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-2247", "mrqa_triviaqa-validation-987", "mrqa_triviaqa-validation-4471", "mrqa_triviaqa-validation-4917", "mrqa_triviaqa-validation-1167", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-4656", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-7495", "mrqa_triviaqa-validation-6094", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-971", "mrqa_triviaqa-validation-6704", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-667", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-893", "mrqa_hotpotqa-validation-413", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-144", "mrqa_searchqa-validation-9567"], "SR": 0.546875, "CSR": 0.5498511904761905, "retrieved_ids": ["mrqa_squad-train-53055", "mrqa_squad-train-65301", "mrqa_squad-train-13692", "mrqa_squad-train-31328", "mrqa_squad-train-24989", "mrqa_squad-train-37286", "mrqa_squad-train-82461", "mrqa_squad-train-54469", "mrqa_squad-train-35939", "mrqa_squad-train-83954", "mrqa_squad-train-74584", "mrqa_squad-train-28209", "mrqa_squad-train-3010", "mrqa_squad-train-20357", "mrqa_squad-train-32716", "mrqa_squad-train-65291", "mrqa_naturalquestions-validation-7679", "mrqa_hotpotqa-validation-833", "mrqa_newsqa-validation-2423", "mrqa_squad-validation-1902", "mrqa_triviaqa-validation-3878", "mrqa_hotpotqa-validation-3060", "mrqa_triviaqa-validation-6757", "mrqa_naturalquestions-validation-4190", "mrqa_newsqa-validation-649", "mrqa_searchqa-validation-15790", "mrqa_squad-validation-6547", "mrqa_hotpotqa-validation-2579", "mrqa_newsqa-validation-3191", "mrqa_searchqa-validation-2638", "mrqa_newsqa-validation-2025", "mrqa_searchqa-validation-14767"], "EFR": 0.9655172413793104, "Overall": 0.7358080613711001}, {"timecode": 84, "before_eval_results": {"predictions": ["David Ben - Gurion", "Points that lie to the right of the production possibilities curve are said to be unattainable", "Amber Riley", "the public", "the eighth episode of Arrow's second season", "Clarence L. Tinker", "2002", "1993", "reminds worshippers of their sinfulness and mortality and thus, implicitly, of their need to repent in time", "John Roberts", "Kelly Reno", "butane", "Parker's pregnancy", "October 2004", "state or other organizational body", "Ben Willis", "The Enchantress", "the true horrors of human history", "Leon Huff", "the oral mucosa", "Muno, Foofa, Brobee, and Toodee", "President Richard Nixon", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "The management team", "Bill's yacht in Monte Carlo", "The Parlement de Bretagne", "Matt Flinders", "during prenatal development", "Geothermal gradient", "benzodiazepines", "Ferm\u00edn Francisco", "three", "Coriolis effect", "94 by 50", "The enthalpy of fusion of a substance, also known as ( latent ) heat of fusion", "Washington metropolitan area", "Vanessa Ferlito", "Jack", "ice giants", "2010", "1920s", "in different parts of the globe", "the Korean Empire", "cylinder of glass or plastic that runs along the fiber's length", "The Intolerable Acts", "Hanna Alstr\u00f6m", "Internal epithelia", "Bob Dylan", "Latitude", "Indian monks", "`` house edge ''", "Sikh", "ronald reagan", "john Mortimer", "Florida", "niece", "second", "Kitty Kelley", "Strategic Arms Reduction Treaty", "the \"race for the future... and it won't be won with a president who is stuck in the past.\"", "George Calvert", "Pasha", "Ontario", "Medellin"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5820152070152069}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 0.13333333333333333, 0.0, 1.0, 0.7692307692307692, 0.4615384615384615, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.4444444444444445, 1.0, 0.0, 0.4, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.15384615384615385, 1.0, 0.8571428571428572, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2890", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-251", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-2684", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-8937", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-5809", "mrqa_naturalquestions-validation-5555", "mrqa_triviaqa-validation-1031", "mrqa_newsqa-validation-4006", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-430", "mrqa_searchqa-validation-13409"], "SR": 0.453125, "CSR": 0.5487132352941176, "retrieved_ids": ["mrqa_squad-train-64908", "mrqa_squad-train-46604", "mrqa_squad-train-10639", "mrqa_squad-train-13763", "mrqa_squad-train-25830", "mrqa_squad-train-6448", "mrqa_squad-train-38964", "mrqa_squad-train-44663", "mrqa_squad-train-85799", "mrqa_squad-train-66296", "mrqa_squad-train-57382", "mrqa_squad-train-23677", "mrqa_squad-train-6594", "mrqa_squad-train-42560", "mrqa_squad-train-72033", "mrqa_squad-train-63846", "mrqa_hotpotqa-validation-5830", "mrqa_newsqa-validation-646", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-866", "mrqa_triviaqa-validation-6689", "mrqa_naturalquestions-validation-3257", "mrqa_triviaqa-validation-1316", "mrqa_searchqa-validation-13515", "mrqa_searchqa-validation-15063", "mrqa_triviaqa-validation-1975", "mrqa_naturalquestions-validation-5561", "mrqa_searchqa-validation-5869", "mrqa_newsqa-validation-144", "mrqa_searchqa-validation-6338", "mrqa_hotpotqa-validation-2886", "mrqa_newsqa-validation-2884"], "EFR": 0.8571428571428571, "Overall": 0.7139055934873949}, {"timecode": 85, "before_eval_results": {"predictions": ["July 14, 2017,", "27 July and 7 August 2021", "telecommunications, pharmaceuticals, aircraft, heavy machinery and other industries", "North Dakota ( 21.5 % )", "Tom Selleck", "Dimitar Berbatov and Carlos Tevez", "18 - season career", "flour", "Mahatma Gandhi", "J. Presper Eckert and John William Mauchly's ENIAC", "Missouri River", "Fa Ze YouTubers", "Orangeville, Ontario, Canada", "May 2016", "up to 100,000 write / erase cycles", "Kiss", "one", "36 months", "October 2012", "winter", "gesture in which the head is tilted in alternating up and down arcs along the sagittal plane", "July 2, 1928", "push the food down the esophagus", "Jackie Robinson", "The Lightning thief", "TLC - All That Theme Song", "230 million kilometres ( 143,000,000 mi )", "September 8, 2017", "William the Conqueror", "Abid Ali Neemuchwala", "Sri Lanka Podujana Peramuna, led by former president Mahinda Rajapaksa", "in the city of Chicago", "The User State Migration Tool", "1807", "the season - five premiere episode `` Second Opinion ''", "Ming", "Saint Peter", "1773", "September 25, 1987", "20 years from the filing date", "It acts as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "Billie Jean King", "Internal epithelia", "March 2, 2016", "detritus from the settlement of the sedimentation", "Ravi River", "Lorenzo Lamas", "12 to 36 months old", "Alice Cooper", "L'Engle's own Connecticut home, Crosswicks", "on a bronze plaque", "boxelder bug", "acetone", "spouse", "21 August 1986", "Taylor Swift", "2006", "large accumulations of ice in places such as the north Georgia mountains, causing hazardous driving conditions.", "the Zimbabwean government", "South Africa", "a Tory", "Charles Boyer", "Last of the Mohicans", "a michelada"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6402391098484848}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.13333333333333333, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.3636363636363636, 1.0, 0.6666666666666666, 0.125, 0.5, 1.0, 1.0, 1.0, 0.33333333333333337, 0.9090909090909091, 1.0, 1.0, 1.0, 0.5714285714285715, 0.4, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9090909090909091, 0.72, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.5714285714285715, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5647", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-1359", "mrqa_naturalquestions-validation-5413", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-1368", "mrqa_naturalquestions-validation-1789", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-7930", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-4225", "mrqa_naturalquestions-validation-3095", "mrqa_naturalquestions-validation-4633", "mrqa_triviaqa-validation-1705", "mrqa_triviaqa-validation-5074", "mrqa_hotpotqa-validation-3641", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-656", "mrqa_searchqa-validation-13973"], "SR": 0.46875, "CSR": 0.5477834302325582, "retrieved_ids": ["mrqa_squad-train-73714", "mrqa_squad-train-53963", "mrqa_squad-train-43329", "mrqa_squad-train-55504", "mrqa_squad-train-39019", "mrqa_squad-train-70458", "mrqa_squad-train-20276", "mrqa_squad-train-42741", "mrqa_squad-train-73707", "mrqa_squad-train-38234", "mrqa_squad-train-82268", "mrqa_squad-train-20782", "mrqa_squad-train-56120", "mrqa_squad-train-75061", "mrqa_squad-train-11348", "mrqa_squad-train-45443", "mrqa_newsqa-validation-863", "mrqa_squad-validation-9764", "mrqa_triviaqa-validation-645", "mrqa_newsqa-validation-274", "mrqa_hotpotqa-validation-5261", "mrqa_triviaqa-validation-7737", "mrqa_hotpotqa-validation-2673", "mrqa_searchqa-validation-11271", "mrqa_squad-validation-7763", "mrqa_newsqa-validation-859", "mrqa_squad-validation-5351", "mrqa_searchqa-validation-10199", "mrqa_triviaqa-validation-6192", "mrqa_searchqa-validation-3666", "mrqa_triviaqa-validation-5389", "mrqa_naturalquestions-validation-1223"], "EFR": 0.8823529411764706, "Overall": 0.7187616492818057}, {"timecode": 86, "before_eval_results": {"predictions": ["Georgia Groome", "Redenbacher family", "Klaus Baudelaire", "Pure Java driver", "Mark Sanderson", "Russia", "the Mongol Empire", "July 4, 1776", "Egypt", "a blighted ovum or anembryonic gestation", "left hand ring finger", "the Arafura Sea through the Torres Strait", "`` can't wait ''", "inanimate or living", "statute or the Constitution itself", "2012", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "Sebastian Vettel", "Ace", "1998", "Brazil and Paraguay", "to form a higher alkane", "1947", "the Ramones", "Arizona", "Jurchen Aisin Gioro clan", "China", "Jacques Cousteau", "Crist\u00f3bal Baca ( Vaca )", "Nepal", "538", "Seven pillars of Ismailism", "President", "performance marker", "March 8, 2018", "Gary Player", "1997", "Holden Nowell", "pathology", "332 members", "if the occurrence of one does not affect the probability of occurrence of the other", "the toe", "2018", "mitosis", "The Maidstone Studios in Maidstone, Kent", "the Outfield", "International Border ( IB )", "Samara Cook", "photodiode", "1858", "1991", "george III", "Tom Good", "Donington Park", "Kolkata", "The Handmaid's Tale", "Tom Cole", "Takashi Saito,", "Herman Cain", "billboards", "a hormone", "the Conehead", "House of Lords", "Sean Maguire"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5302353896103896}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, false, true], "QA-F1": [0.5714285714285715, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8333333333333333, 1.0, 0.0, 0.0, 0.2222222222222222, 0.4, 0.5, 0.3636363636363636, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-6482", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-7516", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-8623", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-8493", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-5546", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-3150", "mrqa_searchqa-validation-10490", "mrqa_searchqa-validation-15707"], "SR": 0.421875, "CSR": 0.5463362068965517, "retrieved_ids": ["mrqa_squad-train-77629", "mrqa_squad-train-39724", "mrqa_squad-train-2852", "mrqa_squad-train-64055", "mrqa_squad-train-46139", "mrqa_squad-train-56205", "mrqa_squad-train-67856", "mrqa_squad-train-50789", "mrqa_squad-train-54651", "mrqa_squad-train-44106", "mrqa_squad-train-18016", "mrqa_squad-train-48476", "mrqa_squad-train-69100", "mrqa_squad-train-86567", "mrqa_squad-train-12202", "mrqa_squad-train-9398", "mrqa_newsqa-validation-1147", "mrqa_hotpotqa-validation-5475", "mrqa_naturalquestions-validation-1783", "mrqa_triviaqa-validation-737", "mrqa_hotpotqa-validation-4075", "mrqa_squad-validation-2754", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-1000", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-802", "mrqa_triviaqa-validation-5074", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-3130", "mrqa_hotpotqa-validation-5363", "mrqa_newsqa-validation-1639"], "EFR": 0.918918918918919, "Overall": 0.7257854001630941}, {"timecode": 87, "before_eval_results": {"predictions": ["norway", "skunk", "Romeo and Juliet", "sitcom", "Niger", "Artemis", "using parentheses", "Spanish", "driving Miss Daisy", "malta", "Robert Galbraith", "Dubai", "Bristol", "Double Trouble", "argentina", "Barack Obama", "the Observer", "argentina", "timothy laurence", "Brian Clough", "Peter Paul Rubens", "Pembrokeshire Coast National Park", "blood left at crime scenes", "willy", "javelin throw", "port moresby", "carousel", "Maxwell", "synagogues", "Cambridge", "airplane", "Richard Curtis", "Keswick", "Louis Le Vau", "horseshoes", "david Copperfield", "the Union Gap", "Gatcombe Park", "india", "Charlotte", "Michael Phelps", "Vietnam", "Mumbai,", "mumps", "stop motion effects", "Sebastian Flyte", "the Eagle", "seaweed", "Hindi", "Madrid", "Paul Bunyan", "Reverse - Flash", "nose", "summer of 1990 and continued until 1992", "1986", "Chad", "1967", "1,500 Marines", "1,500", "came forward Monday \"for the other women who couldn't or wouldn't.\"", "cholesterol", "volcanic", "a miniature sleigh", "Baldwin is a hamlet and census-designated place (CDP) located in the town of Hempstead"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6919871794871795}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-994", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-7596", "mrqa_triviaqa-validation-7209", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-6967", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-6617", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-1925", "mrqa_triviaqa-validation-4680", "mrqa_triviaqa-validation-7042", "mrqa_triviaqa-validation-225", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-8386", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-1657", "mrqa_hotpotqa-validation-1557"], "SR": 0.65625, "CSR": 0.5475852272727273, "retrieved_ids": ["mrqa_squad-train-59010", "mrqa_squad-train-79280", "mrqa_squad-train-49504", "mrqa_squad-train-20487", "mrqa_squad-train-47688", "mrqa_squad-train-69481", "mrqa_squad-train-9726", "mrqa_squad-train-65143", "mrqa_squad-train-21731", "mrqa_squad-train-12513", "mrqa_squad-train-61888", "mrqa_squad-train-8155", "mrqa_squad-train-28885", "mrqa_squad-train-72076", "mrqa_squad-train-36313", "mrqa_squad-train-36608", "mrqa_newsqa-validation-1019", "mrqa_naturalquestions-validation-8000", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-5655", "mrqa_searchqa-validation-11407", "mrqa_naturalquestions-validation-7574", "mrqa_newsqa-validation-220", "mrqa_searchqa-validation-13454", "mrqa_hotpotqa-validation-2574", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-3297", "mrqa_triviaqa-validation-7384", "mrqa_hotpotqa-validation-4052", "mrqa_newsqa-validation-1011", "mrqa_hotpotqa-validation-866", "mrqa_naturalquestions-validation-3027"], "EFR": 0.9545454545454546, "Overall": 0.7331605113636364}, {"timecode": 88, "before_eval_results": {"predictions": ["Cool Runnings", "five times", "an explosion and a fire", "fathers who categorize themselves as a \"geek.\"", "elderships", "Andrew Davis", "Drowning Pool", "Lombardy region", "7", "\"Realty Bites\"", "2007", "Florida Panthers", "capital crimes or capital offences", "10-metre platform event", "\"The Wonder Years\"", "3,000", "beer and soft drinks", "Shohola Falls", "The Kennedy Center", "Saoirse Ronan", "H. R. Haldeman", "Netherlands", "Miss Universe 2010 Ximena Navarrete", "John R. Leonetti", "Fat Albert", "Oklahoma State", "stoneware", "Prudence Jane Goward", "Revolver", "Walcha", "\"The Double Life of V\u00e9ronique\"", "Los Alamos National Laboratory", "Apatosaurus", "Currer Bell", "Anne Perry", "Bergen County", "Santiago del Estero Province", "Province of Syracuse", "David Irving", "Florida", "219", "Charice", "Harlem", "Francis Schaeffer", "Eric Edward Whitacre", "George Balanchine", "Laura Dern", "Statue of Liberty", "Axl Rose", "second", "notable for being one of the youngest publicly documented people to be identified as transgender", "Bob Parr", "1928", "October 30, 2017", "Spanish-American War", "warblers", "Oregon", "a book.", "South Africa", "Russia", "547", "molar", "Lady Jane Grey", "Yemen"], "metric_results": {"EM": 0.625, "QA-F1": 0.7238219246031746}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.6, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.8, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.2857142857142857, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-1405", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-2211", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-920", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-3796", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-1870", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-1135", "mrqa_triviaqa-validation-3584", "mrqa_newsqa-validation-1389", "mrqa_searchqa-validation-15444"], "SR": 0.625, "CSR": 0.5484550561797753, "retrieved_ids": ["mrqa_squad-train-17555", "mrqa_squad-train-12989", "mrqa_squad-train-22486", "mrqa_squad-train-75429", "mrqa_squad-train-24863", "mrqa_squad-train-44211", "mrqa_squad-train-77304", "mrqa_squad-train-65999", "mrqa_squad-train-17290", "mrqa_squad-train-48285", "mrqa_squad-train-6832", "mrqa_squad-train-36617", "mrqa_squad-train-18745", "mrqa_squad-train-14071", "mrqa_squad-train-39479", "mrqa_squad-train-69586", "mrqa_searchqa-validation-14214", "mrqa_hotpotqa-validation-1920", "mrqa_newsqa-validation-2214", "mrqa_searchqa-validation-15908", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-6871", "mrqa_searchqa-validation-15730", "mrqa_triviaqa-validation-3045", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-3870", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-5489", "mrqa_searchqa-validation-2878", "mrqa_newsqa-validation-2984", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-7001"], "EFR": 0.9166666666666666, "Overall": 0.7257587195692884}, {"timecode": 89, "before_eval_results": {"predictions": ["2012 NBA draft", "848", "Skipton Castle", "Consigliere", "midtempo hip hop ballad", "the Beatles", "shortstop", "Francis Egerton, 3rd Duke of Bridgewater", "Tamworth", "Taylor Swift's single \"Teenage Dream\"", "Eric Whitacre", "psychological horror adventure game", "Nicole Kidman", "Karl Kraus", "water", "chard County", "he turned 51, he died of cancer", "1947", "2004", "1903", "170", "2013", "Boulder High School in Boulder, Colorado", "Tian Tan Buddha", "Rocky Mountain Institute", "Macau, China", "La Nouba", "Stephanie Plum", "Memphis, Tennessee", "National Collegiate Athletic Association", "Figaro", "Eve Hewson", "Leonard Cohen", "U2 360\u00b0 Tour", "Eielson Air Force Base in Alaska", "1,462", "University of New South Wales", "the God of Israel", "three times", "Budget Rent a Car", "Dealey Plaza", "Terrence Jones", "Democratic", "2015", "\u00c6thelstan", "historically black", "half a million acres", "Kennedy John Victor", "Morning Edition", "Bill Miner", "50JJB Sports Fitness Clubs", "the Tigris and Euphrates rivers", "$2.187 billion", "the egg", "Goldie Hawn", "Home Guard", "tintin", "U.N. agencies", "$40 and a Bread of bread.", "hooked up with Mildred,", "Mike Wallace", "lava lakes", "a nocturnal mammal", "Capitol Hill,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6830729166666667}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, false, true, false, true, false, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, true], "QA-F1": [0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.2, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.25, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8000000000000002, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-3058", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-1074", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-3950", "mrqa_hotpotqa-validation-3350", "mrqa_hotpotqa-validation-436", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-2297", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-4715", "mrqa_hotpotqa-validation-3763", "mrqa_hotpotqa-validation-4221", "mrqa_hotpotqa-validation-3608", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-1699", "mrqa_hotpotqa-validation-2833", "mrqa_hotpotqa-validation-5603", "mrqa_naturalquestions-validation-6931", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-16801", "mrqa_searchqa-validation-4758"], "SR": 0.546875, "CSR": 0.5484375, "retrieved_ids": ["mrqa_squad-train-41177", "mrqa_squad-train-41096", "mrqa_squad-train-69100", "mrqa_squad-train-68909", "mrqa_squad-train-10639", "mrqa_squad-train-58809", "mrqa_squad-train-48863", "mrqa_squad-train-36910", "mrqa_squad-train-18719", "mrqa_squad-train-52621", "mrqa_squad-train-76702", "mrqa_squad-train-57185", "mrqa_squad-train-31489", "mrqa_squad-train-22188", "mrqa_squad-train-58537", "mrqa_squad-train-69988", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-3918", "mrqa_hotpotqa-validation-278", "mrqa_searchqa-validation-7331", "mrqa_newsqa-validation-3384", "mrqa_triviaqa-validation-5919", "mrqa_naturalquestions-validation-2865", "mrqa_triviaqa-validation-240", "mrqa_newsqa-validation-3326", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-2941", "mrqa_triviaqa-validation-5074", "mrqa_newsqa-validation-3052", "mrqa_searchqa-validation-6363", "mrqa_hotpotqa-validation-2588", "mrqa_searchqa-validation-10911"], "EFR": 0.9655172413793104, "Overall": 0.7355253232758621}, {"timecode": 90, "UKR": 0.794921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1336", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1547", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2267", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3779", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3812", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-681", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2684", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2798", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5647", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6754", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7190", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7516", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8546", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1974", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13104", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13297", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-13830", "mrqa_searchqa-validation-14348", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-5831", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6635", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3813", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-4469", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-5042", "mrqa_squad-validation-5325", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-682", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8529", "mrqa_squad-validation-8754", "mrqa_squad-validation-9306", "mrqa_squad-validation-9531", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1636", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-2575", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3399", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4234", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4585", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5167", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5837", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7564", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7640", "mrqa_triviaqa-validation-7717", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-920"], "OKR": 0.875, "KG": 0.51328125, "before_eval_results": {"predictions": ["Delaware", "The Shirehorses", "Detroit", "\"The Expendables 2\"", "Bob Hill", "Pennsylvania State University", "Princess Aisha bint Hussein", "1951", "Muskogean", "River Clyde", "aging issues", "Indooroopilly Shopping Centre", "Chow Tai Fook Enterprises", "Rhode Island School of Design", "Bill Walton", "Vanessa Hudgens", "1983", "Andrew Preston", "Eddie Gottlieb Trophy", "Rhodesia", "Jay Pritchett", "My Backyard", "Lommel", "Bharat Ratna", "Aberdeenshire", "three-part", "1969", "Stalybridge Celtic", "tenant management", "Vincent Anthony Guaraldi", "Pulitzer Prize for drama", "one live album", "2002", "Neon City", "Province of New York", "between 1932 and 1934", "Hopi", "American", "pop music and popular culture", "Australian", "SKUM", "Rain Man", "Wake Island", "Carlos Santana", "1942", "2027 Fairmount Avenue", "historic buildings, arts, and published works", "great man", "The Indianapolis Times and the Cleveland Press", "143,007", "James I", "committed suicide", "1969", "nucleus", "lung", "discus throw", "Bo Derek", "three", "police dogs", "the Impeccable", "Sierra Leone", "Budapest", "a parachute jump", "11th year in a row."], "metric_results": {"EM": 0.59375, "QA-F1": 0.6990327380952381}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, false, true, false, false, true, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-699", "mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-4514", "mrqa_hotpotqa-validation-946", "mrqa_hotpotqa-validation-3311", "mrqa_hotpotqa-validation-1048", "mrqa_hotpotqa-validation-5498", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-2599", "mrqa_hotpotqa-validation-1925", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4864", "mrqa_naturalquestions-validation-425", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-5991", "mrqa_newsqa-validation-2935", "mrqa_newsqa-validation-414", "mrqa_newsqa-validation-3310", "mrqa_searchqa-validation-6970", "mrqa_searchqa-validation-10274"], "SR": 0.59375, "CSR": 0.5489354395604396, "retrieved_ids": ["mrqa_squad-train-23326", "mrqa_squad-train-76369", "mrqa_squad-train-71768", "mrqa_squad-train-74668", "mrqa_squad-train-54245", "mrqa_squad-train-61929", "mrqa_squad-train-68691", "mrqa_squad-train-23697", "mrqa_squad-train-74698", "mrqa_squad-train-878", "mrqa_squad-train-62987", "mrqa_squad-train-50527", "mrqa_squad-train-77044", "mrqa_squad-train-16318", "mrqa_squad-train-42945", "mrqa_squad-train-27773", "mrqa_newsqa-validation-2905", "mrqa_searchqa-validation-1420", "mrqa_hotpotqa-validation-5485", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-1330", "mrqa_naturalquestions-validation-6490", "mrqa_naturalquestions-validation-129", "mrqa_squad-validation-3667", "mrqa_newsqa-validation-3277", "mrqa_triviaqa-validation-5074", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-1241", "mrqa_searchqa-validation-15730", "mrqa_hotpotqa-validation-2696", "mrqa_newsqa-validation-2903"], "EFR": 1.0, "Overall": 0.746427712912088}, {"timecode": 91, "before_eval_results": {"predictions": ["Milwaukee", "Nathan Lane", "E.M. Forster", "James Joyce", "Jamestown", "Helen Hayes", "a dollar", "Philip", "Dobermann", "Vermont", "Moses", "the Moors", "Margaret Mitchell", "Henrik Ibsen", "a cruller", "Witch", "Hungary", "Sugar Smacks", "Groundhog Day", "New Balance", "a Siberian Husky", "Animal Crackers", "nitrogen and oxygen", "a platypus", "Nixon", "a bicycle", "the Magic Mountain", "Fiji", "Prince William County", "Jacqueline Kennedy Onassis", "Forbes", "Death Row", "Jimmy Hoffa", "a brain", "a rabbit", "a Bait-and-switch", "a Wiener Journal", "apartheid", "Browning", "interest", "Molly Ringwald", "a Eyebrow", "the Marine Corps", "Ho Chi Minh", "Ghost", "Arkansas", "the Stone of Destiny", "remoulade", "Florida", "Night Fever", "Latin", "Jane Addams, Grace Abbott, Edith Abbott and Sophonisba Breckinridge", "Thomas Chisholm", "Norman Pritchard", "Ron Howard", "spider", "Istanbul", "Battle of Rosebud Creek", "40 Acres and a Mule Filmworks", "1851", "26", "$22 million", "Sixteen", "Ricky Nelson"], "metric_results": {"EM": 0.703125, "QA-F1": 0.8061011904761904}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4712", "mrqa_searchqa-validation-8297", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-8663", "mrqa_searchqa-validation-524", "mrqa_searchqa-validation-8901", "mrqa_searchqa-validation-13732", "mrqa_searchqa-validation-392", "mrqa_searchqa-validation-8137", "mrqa_searchqa-validation-6520", "mrqa_searchqa-validation-9931", "mrqa_searchqa-validation-12969", "mrqa_searchqa-validation-13793", "mrqa_searchqa-validation-3990", "mrqa_searchqa-validation-13167", "mrqa_hotpotqa-validation-3007"], "SR": 0.703125, "CSR": 0.5506114130434783, "retrieved_ids": ["mrqa_squad-train-69002", "mrqa_squad-train-71387", "mrqa_squad-train-71608", "mrqa_squad-train-57331", "mrqa_squad-train-35255", "mrqa_squad-train-51940", "mrqa_squad-train-59027", "mrqa_squad-train-24228", "mrqa_squad-train-77376", "mrqa_squad-train-77850", "mrqa_squad-train-62148", "mrqa_squad-train-3952", "mrqa_squad-train-25252", "mrqa_squad-train-55466", "mrqa_squad-train-26275", "mrqa_squad-train-3163", "mrqa_newsqa-validation-1989", "mrqa_triviaqa-validation-1922", "mrqa_newsqa-validation-3753", "mrqa_searchqa-validation-349", "mrqa_squad-validation-133", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-724", "mrqa_hotpotqa-validation-1257", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-4182", "mrqa_searchqa-validation-1744", "mrqa_hotpotqa-validation-3965", "mrqa_hotpotqa-validation-3321", "mrqa_squad-validation-10316"], "EFR": 0.9473684210526315, "Overall": 0.736236591819222}, {"timecode": 92, "before_eval_results": {"predictions": ["a bust", "A Chorus Line", "Pamplona", "suppe dorate", "Pop-Tarts", "Aesop", "Shawnee", "P.S.", "Babe Ruth", "prostitutes", "Orinoco", "John Bunyan", "Sicilian", "Punjabi", "insulin", "Jimmy Hoffa", "Utah", "Newton", "Cincinnati", "Alexander Hamilton", "\"Boots\"", "bronchodilators", "the Perseid", "Phil Cavilleri", "the wall", "a porter", "Davy Crockett", "Michelangelo", "Penny Lane", "grease", "Kathleen Kennedy Townsend", "Henry Cavendish", "Israel", "euphoria", "Don Quixote", "Charlie and the Chocolate Factory", "baboon", "Last Summer", "Jerusalem", "heredity", "The Wild Thornberrys", "The Bionic Woman", "Selma", "Christina Milian", "pardis", "knight", "HIV/AIDS", "the Smothers Brothers", "Henry Hudson", "Sidecar", "Ford", "The management team", "the United States", "2017", "table salt", "ThunderCats", "Caitlin", "AMC Entertainment Holdings, Inc.", "The Danny Kaye Show", "Toronto", "Matthew Fisher", "$150 billion", "Tennessee.", "Johannes Gutenberg"], "metric_results": {"EM": 0.671875, "QA-F1": 0.70625}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12498", "mrqa_searchqa-validation-7518", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-6725", "mrqa_searchqa-validation-16940", "mrqa_searchqa-validation-1094", "mrqa_searchqa-validation-16819", "mrqa_searchqa-validation-13243", "mrqa_searchqa-validation-8294", "mrqa_searchqa-validation-14776", "mrqa_searchqa-validation-9090", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-3896", "mrqa_searchqa-validation-9483", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-8750", "mrqa_naturalquestions-validation-305", "mrqa_triviaqa-validation-1759", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-582"], "SR": 0.671875, "CSR": 0.5519153225806452, "retrieved_ids": ["mrqa_squad-train-78605", "mrqa_squad-train-16982", "mrqa_squad-train-4917", "mrqa_squad-train-68890", "mrqa_squad-train-13890", "mrqa_squad-train-21753", "mrqa_squad-train-44181", "mrqa_squad-train-41548", "mrqa_squad-train-50144", "mrqa_squad-train-19071", "mrqa_squad-train-24777", "mrqa_squad-train-68594", "mrqa_squad-train-17449", "mrqa_squad-train-29009", "mrqa_squad-train-69913", "mrqa_squad-train-73218", "mrqa_searchqa-validation-13909", "mrqa_searchqa-validation-5340", "mrqa_searchqa-validation-1657", "mrqa_hotpotqa-validation-1011", "mrqa_newsqa-validation-2811", "mrqa_searchqa-validation-4764", "mrqa_naturalquestions-validation-2250", "mrqa_hotpotqa-validation-5632", "mrqa_newsqa-validation-2291", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-7274", "mrqa_naturalquestions-validation-8062", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-5766", "mrqa_newsqa-validation-1098"], "EFR": 0.9047619047619048, "Overall": 0.72797607046851}, {"timecode": 93, "before_eval_results": {"predictions": ["the three-Cornered Hat", "7-Eleven", "Memphis", "Australia", "the Grail", "the Hippopotamus", "Grant & Sherman", "carbon", "Britain", "the metacarpals", "a carriage", "the catacombs", "Bahrain", "the Cohans", "Marilyn Monroe", "the yardarm", "a fish", "February 29", "mistletoe", "the council", "Islam", "Tijuana", "Cleopatra", "the Irish The Diffy", "the Capitol", "Bauhaus", "the Department of Homeland Security", "Jumper", "the batrachus", "paradise", "the Baltic Sea", "Mozart", "C.T. Eisler", "While You Were Out", "3", "after C", "Love Story", "Ramen", "Truman", "St. Louis", "the Confederate Flag", "the Chiles Rellenos", "Elizabeth Edward", "Stephen I", "Sgt. Pepper\\'s Lonely Hearts Club Band", "Cheddar", "the Amistad", "Prince", "the Milky Way", "part of speech", "DANE", "displacement", "Office of Inspector General", "February 6, 2005", "congregation", "Massachusetts", "Jane Austen", "the Napoleonic Wars", "The Sun", "24", "ice jam", "jobs", "removal of his diamond-studded braces.", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6665364583333333}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, false, true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, false, false, true, false, false, true, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.125, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1189", "mrqa_searchqa-validation-7905", "mrqa_searchqa-validation-11754", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-2718", "mrqa_searchqa-validation-16332", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-13462", "mrqa_searchqa-validation-5163", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-9754", "mrqa_searchqa-validation-3622", "mrqa_searchqa-validation-13212", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-10682", "mrqa_searchqa-validation-7097", "mrqa_searchqa-validation-15694", "mrqa_searchqa-validation-5137", "mrqa_searchqa-validation-13616", "mrqa_naturalquestions-validation-6993", "mrqa_triviaqa-validation-6153", "mrqa_hotpotqa-validation-5555", "mrqa_hotpotqa-validation-2759", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-4042"], "SR": 0.59375, "CSR": 0.5523603723404256, "retrieved_ids": ["mrqa_squad-train-4306", "mrqa_squad-train-26394", "mrqa_squad-train-7410", "mrqa_squad-train-3240", "mrqa_squad-train-19211", "mrqa_squad-train-63148", "mrqa_squad-train-74823", "mrqa_squad-train-23190", "mrqa_squad-train-52321", "mrqa_squad-train-75354", "mrqa_squad-train-19360", "mrqa_squad-train-11871", "mrqa_squad-train-67685", "mrqa_squad-train-1515", "mrqa_squad-train-51843", "mrqa_squad-train-14524", "mrqa_searchqa-validation-13732", "mrqa_newsqa-validation-3375", "mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-3918", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-4257", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-8568", "mrqa_hotpotqa-validation-946", "mrqa_searchqa-validation-7219", "mrqa_squad-validation-10174", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-391", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-7614"], "EFR": 1.0, "Overall": 0.747112699468085}, {"timecode": 94, "before_eval_results": {"predictions": ["Southend Pier", "9 February 2018", "the 7th century", "France's Legislative Assembly", "Peter Gardner Ostrum", "Jesse McCartney", "1902", "Jonathon Dutton", "February 27, 2007", "A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "Chuck Noland", "his last starring role was as Boston police detective Barry Frost on the TNT police drama series Rizzoli & Isles ( 2010 -- 14 )", "1939", "60 by West All - Stars", "1872", "the plane crash", "Majo to Hyakkihei 2", "the coronary arteries", "Pittsburgh", "the beginning of the American colonies", "George Harrison", "Los Angeles", "`` the bush ''", "regulate the employment and working conditions of civil servants", "Abigail Hawk", "January 2018", "a contemporary drama in a rural setting", "Cody Fern", "homicidal thoughts of a troubled youth", "1937", "San Francisco", "Black Tuesday ( October 29 ), the Great Crash, or the Stock Market Crash of 1929", "The Magician", "Teddy Randazzo, Bobby Weinstein, and Lou Stallman", ". java", "`` Killer Within ''", "Juliet", "1963", "Roman Reigns", "1901", "1972", "Michael Rooker", "1977", "Richard Stallman", "Keeley Clare Julia Hawes", "Abid Ali Neemuchwala", "Jay Baruchel", "March 26, 1973", "July 25, 2017", "1922", "Steve Lukather", "Cyclades", "Some Like It Hot", "bird", "Bhushan Patel", "Dar es Salaam", "96,867", "Bob Johnson that appeared to criticize Obama's admitted past drug use were played on Martin's show.", "Iran's parliament speaker", "Sri Lanka", "love", "Mathew Brady", "the Rings", "Labau"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6251717322029822}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.8, 0.5714285714285715, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.6666666666666666, 0.19999999999999998, 0.0, 0.0, 0.25, 0.4, 1.0, 0.0, 0.0, 0.5925925925925926, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-5936", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-9836", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-6468", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-3469", "mrqa_naturalquestions-validation-8837", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-5264", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-1134", "mrqa_newsqa-validation-2831", "mrqa_searchqa-validation-9281", "mrqa_triviaqa-validation-7098"], "SR": 0.53125, "CSR": 0.5521381578947369, "retrieved_ids": ["mrqa_squad-train-85855", "mrqa_squad-train-38032", "mrqa_squad-train-56314", "mrqa_squad-train-59702", "mrqa_squad-train-61759", "mrqa_squad-train-18308", "mrqa_squad-train-1054", "mrqa_squad-train-58645", "mrqa_squad-train-62301", "mrqa_squad-train-44760", "mrqa_squad-train-84572", "mrqa_squad-train-75496", "mrqa_squad-train-64200", "mrqa_squad-train-84254", "mrqa_squad-train-21522", "mrqa_squad-train-16275", "mrqa_searchqa-validation-16718", "mrqa_squad-validation-1760", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-589", "mrqa_hotpotqa-validation-2612", "mrqa_newsqa-validation-1816", "mrqa_triviaqa-validation-2810", "mrqa_newsqa-validation-1639", "mrqa_searchqa-validation-1399", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-8062", "mrqa_squad-validation-2754", "mrqa_hotpotqa-validation-3578", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-8995", "mrqa_newsqa-validation-2220"], "EFR": 0.9333333333333333, "Overall": 0.7337349232456141}, {"timecode": 95, "before_eval_results": {"predictions": ["Jet Republic", "Frank Ricci,", "abusing its dominant position in the computer processing unit (CPU) market.", "Vicente Carrillo Leyva, a leader of the Carrillo Fuentes drug cartel,", "soldiers from the 101st Airborne Division, an elite Army unit,", "five", "Ben Roethlisberger", "Graham's wife", "Muslim", "Asashoryu", "\"Slumdog Millionaire\"", "Ma Khin Khin Leh,", "$17,000", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "the pirates", "10 municipal police officers", "Bangladesh", "a Muslim with Lebanese heritage, but her family is \"not defined by religion,\"", "Mikkel Kessler", "Friday,", "would slow economic growth with higher taxes.", "fourth", "suppress the memories and to live as normal a life as possible; the culture of his time said that he should get on with his", "Hayden", "Malcolm X", "Dangjin", "a North Korean Foreign Ministry spokesman described U.S. Vice President Dick Cheney as a \"mentally deranged person", "surrounding areas of the bustling capital faced further inundation at the next high tide.", "The patient, who prefers to be anonymous,", "stole", "one", "Black History Month", "opposition parties", "YouTube", "Cambodian territory", "2.5 million", "using injectable vitamin supplements because the quantities are not regulated.", "\"I sort of had a fascination with John Dillinger when I was about 10, 11 years old, for some reason,\"", "we seek a new way forward, based on mutual interest and mutual respect.", "Abhisit Vejjajiva", "iPods", "Yemen", "228", "More than 15,000", "Theoneste Bagosora, 67, a colonel in the Rwandan army,", "Operation Pipeline Express.", "$5.5 billion to build.", "$106,482,500", "Long troop deployments in Iraq, above, and Afghanistan", "prostate cancer,", "bartering", "William Wyler", "1608", "the eleventh book in the New Testament", "kachhi", "a line representing points of equal air speed", "fractal geometry", "Paper", "SBS", "The Lord of the Rings", "Harry Potter and the Chamber of Secrets", "a diphthong", "water lily", "nuclear weapons"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6714078150199474}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.1818181818181818, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.07692307692307693, 0.0, 0.08333333333333334, 1.0, 0.5714285714285715, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285714, 0.15384615384615385, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.3333333333333333, 0.6666666666666666, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3913", "mrqa_newsqa-validation-3555", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-3322", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-408", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-1044", "mrqa_newsqa-validation-2891", "mrqa_naturalquestions-validation-7728", "mrqa_triviaqa-validation-1304", "mrqa_triviaqa-validation-2184", "mrqa_hotpotqa-validation-3247"], "SR": 0.59375, "CSR": 0.5525716145833333, "retrieved_ids": ["mrqa_squad-train-43844", "mrqa_squad-train-73740", "mrqa_squad-train-26959", "mrqa_squad-train-65566", "mrqa_squad-train-3985", "mrqa_squad-train-60083", "mrqa_squad-train-18505", "mrqa_squad-train-76012", "mrqa_squad-train-43527", "mrqa_squad-train-47958", "mrqa_squad-train-31356", "mrqa_squad-train-30380", "mrqa_squad-train-72763", "mrqa_squad-train-17513", "mrqa_squad-train-29130", "mrqa_squad-train-3496", "mrqa_naturalquestions-validation-7715", "mrqa_searchqa-validation-12993", "mrqa_naturalquestions-validation-1786", "mrqa_newsqa-validation-2231", "mrqa_triviaqa-validation-3878", "mrqa_naturalquestions-validation-10680", "mrqa_newsqa-validation-3159", "mrqa_hotpotqa-validation-4117", "mrqa_newsqa-validation-2385", "mrqa_squad-validation-8066", "mrqa_searchqa-validation-6234", "mrqa_naturalquestions-validation-5537", "mrqa_hotpotqa-validation-2625", "mrqa_newsqa-validation-1388", "mrqa_squad-validation-8386", "mrqa_newsqa-validation-2519"], "EFR": 0.8846153846153846, "Overall": 0.7240780248397436}, {"timecode": 96, "before_eval_results": {"predictions": ["free enterprise in history", "off Somalia's coast.", "2009", "club managers,", "5,600", "a construction site in the heart of Los Angeles.", "central Cairo,", "free services.", "delivers a big speech", "Basilan", "the highest ranking former member of Saddam Hussein's regime still at large.", "Fareed Zakaria", "Gov. Mark Sanford", "56,", "$17,000", "Climatecare,", "Australian officials", "fake his own death", "The torch for the 2010 Vancouver Olympics was lit in a ceremony at the ancient Greek site of Olympia", "Police", "one", "The two men wandered through the jungle for three days and nights before encountering a Colombian army patrol Sunday,", "that the soldiers were exposed to sodium dichromate, an inorganic compound containing a highly toxic form of chromium known as hexavalent chromium.", "a student who admitted to hanging a noose in a campus library,", "July", "the two remaining crew members from the helicopter,", "Darrel Mohler", "some of the best stunt ever pulled off", "$1.5 million", "\"People have lost their homes, their jobs, their hope,\"", "30", "Larry King", "HPV (human papillomavirus)", "A union representing thousands of transit workers went on strike early Tuesday", "Body Tap,", "the Southern Baptist Convention", "Stephen Tyrone Johns", "Mawise Gumba", "\"The first line of law and order", "Facebook", "promotes fuel economy and safety while boosts the economy.", "police", "Susan Atkins", "off the coast of Dubai", "\"She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "\"falling space debris,\"", "part", "Silicon Valley.", "63", "\"Slumdog Millionaire\"", "Lillo Brancato Jr.", "Maria works in a bridal shop with Anita", "Sylvester Stallone", "A tree - topper or treetopper", "butterflies", "vanilla", "France", "1241 until his death in 1250", "October", "Westfield Tea Tree Plaza", "Jacob Marley", "Johnny Weissmuller", "Pin the Tail on the Donkey", "Mount Hood"], "metric_results": {"EM": 0.703125, "QA-F1": 0.812869957010582}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.19047619047619044, 1.0, 1.0, 0.0, 0.2962962962962963, 0.5, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.875, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-752", "mrqa_newsqa-validation-1163", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-4039", "mrqa_newsqa-validation-3800", "mrqa_newsqa-validation-2261", "mrqa_naturalquestions-validation-5305", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-5864", "mrqa_searchqa-validation-14622"], "SR": 0.703125, "CSR": 0.5541237113402062, "retrieved_ids": ["mrqa_squad-train-24764", "mrqa_squad-train-83330", "mrqa_squad-train-71202", "mrqa_squad-train-3710", "mrqa_squad-train-82112", "mrqa_squad-train-15109", "mrqa_squad-train-29991", "mrqa_squad-train-39204", "mrqa_squad-train-64395", "mrqa_squad-train-81492", "mrqa_squad-train-61465", "mrqa_squad-train-46921", "mrqa_squad-train-77173", "mrqa_squad-train-40284", "mrqa_squad-train-54131", "mrqa_squad-train-41820", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-3428", "mrqa_naturalquestions-validation-4247", "mrqa_searchqa-validation-14848", "mrqa_searchqa-validation-3896", "mrqa_newsqa-validation-1047", "mrqa_searchqa-validation-2960", "mrqa_newsqa-validation-3592", "mrqa_hotpotqa-validation-3058", "mrqa_searchqa-validation-13243", "mrqa_newsqa-validation-3905", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1854", "mrqa_newsqa-validation-995", "mrqa_searchqa-validation-9261", "mrqa_naturalquestions-validation-8014"], "EFR": 0.7894736842105263, "Overall": 0.7053601041101465}, {"timecode": 97, "before_eval_results": {"predictions": ["Fred Astaire", "winnie Mae", "Manchester United", "Pocahontas", "chaucer", "Stubbs", "Jordan", "John Donne", "West Virginia", "dogs", "watchmaking", "Niger", "ballet", "South Bank", "Cornwall", "the Severn", "green", "Fred Trueman", "The Mayor of Casterbridge", "Athens", "Yemen", "Loch Morar", "leprosy", "Manhunt 2", "phone", "piano", "hand gun", "collies", "Chicago", "Ramadan", "uriah", "George Fox", "bat", "secretary", "France", "Tina Russo Duck", "wood-smoked haddock", "few", "Ross MacManus", "music (to be performed) in a fiery manner", "dry rot", "cuckoo", "Northumberland", "6", "Midnight Cowboy", "three", "1911", "a skein", "Northern Ireland", "Jorge Lorenzo", "Pat Houston", "McFerrin, Robin Williams, and Bill Irwin", "1260 cubic centimeters ( cm )", "John von Neumann", "Florida Panthers", "Giuseppe Fortunino Francesco Verdi", "67,575", "in July", "sailing", "Mary Procidano", "Eva Maria Kiesler", "Quinn", "the Komodo Dragon", "goalkeeper"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7651041666666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-5438", "mrqa_triviaqa-validation-2342", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-7539", "mrqa_triviaqa-validation-5753", "mrqa_triviaqa-validation-7413", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-2595", "mrqa_triviaqa-validation-1585", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-7458", "mrqa_hotpotqa-validation-3953", "mrqa_newsqa-validation-271", "mrqa_searchqa-validation-15102"], "SR": 0.703125, "CSR": 0.5556441326530612, "retrieved_ids": ["mrqa_squad-train-84216", "mrqa_squad-train-26896", "mrqa_squad-train-79070", "mrqa_squad-train-8310", "mrqa_squad-train-13327", "mrqa_squad-train-31696", "mrqa_squad-train-67820", "mrqa_squad-train-66899", "mrqa_squad-train-5437", "mrqa_squad-train-72143", "mrqa_squad-train-54128", "mrqa_squad-train-40110", "mrqa_squad-train-34989", "mrqa_squad-train-80711", "mrqa_squad-train-72932", "mrqa_squad-train-49103", "mrqa_newsqa-validation-3020", "mrqa_triviaqa-validation-5474", "mrqa_searchqa-validation-14214", "mrqa_searchqa-validation-757", "mrqa_naturalquestions-validation-4519", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3767", "mrqa_hotpotqa-validation-4463", "mrqa_naturalquestions-validation-6874", "mrqa_triviaqa-validation-3033", "mrqa_triviaqa-validation-2247", "mrqa_triviaqa-validation-3549", "mrqa_hotpotqa-validation-2741", "mrqa_newsqa-validation-2040", "mrqa_searchqa-validation-13793", "mrqa_searchqa-validation-3398"], "EFR": 1.0, "Overall": 0.7477694515306121}, {"timecode": 98, "before_eval_results": {"predictions": ["The Kite Runner", "Yves Saint Laurent", "Emily Post", "a shepherd", "Chopin", "mulata", "glass", "Bolshoi Ballet", "Mending Wall", "Nathan Lane", "Cheaper by the Dozen", "Ferdinand", "Marlon Brando", "Sagamore Hill", "Peru", "Copenhagen", "Arctic Ocean", "Hudson", "Blofeld", "Amantine", "Richard Cory", "Franois Truffaut", "Barbie", "1", "chlorine", "Fidel Castro", "Hanoi", "the Byzantine Empire", "Mali", "Flav", "McDonald's", "Macedonia", "Hawaii", "Jimmy Hoffa", "coffee", "Cincinnati", "\" Bulldog\" Drummond", "gin", "John Paul Jones", "walk the plank", "Three Amigos!", "Haunted Mansion", "George II", "Stonehenge", "Grease", "Nevada", "a tick", "alkaline", "Halsey", "Chiang Mai", "arteries", "DeWayne Warren", "Doug Diemoz", "New England Patriots", "Philippines", "micky dolenz", "brashy", "Iynx", "400 MW", "high court of Admiralty", "held to a 1-1 draw at Stoke City.", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "the conversion", "\"godfather\""], "metric_results": {"EM": 0.609375, "QA-F1": 0.697048611111111}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-2008", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-15451", "mrqa_searchqa-validation-9719", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-13423", "mrqa_searchqa-validation-14990", "mrqa_searchqa-validation-9525", "mrqa_searchqa-validation-7874", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-7997", "mrqa_searchqa-validation-14239", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-12015", "mrqa_searchqa-validation-9807", "mrqa_naturalquestions-validation-8903", "mrqa_triviaqa-validation-6849", "mrqa_hotpotqa-validation-3975", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1424", "mrqa_hotpotqa-validation-4241"], "SR": 0.609375, "CSR": 0.5561868686868687, "retrieved_ids": ["mrqa_squad-train-35490", "mrqa_squad-train-12574", "mrqa_squad-train-72297", "mrqa_squad-train-85372", "mrqa_squad-train-50853", "mrqa_squad-train-24795", "mrqa_squad-train-45585", "mrqa_squad-train-50761", "mrqa_squad-train-74810", "mrqa_squad-train-40438", "mrqa_squad-train-85084", "mrqa_squad-train-67164", "mrqa_squad-train-404", "mrqa_squad-train-28575", "mrqa_squad-train-41186", "mrqa_squad-train-27857", "mrqa_searchqa-validation-14617", "mrqa_naturalquestions-validation-9499", "mrqa_searchqa-validation-16391", "mrqa_newsqa-validation-1429", "mrqa_hotpotqa-validation-973", "mrqa_triviaqa-validation-5628", "mrqa_squad-validation-10386", "mrqa_hotpotqa-validation-3934", "mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-944", "mrqa_newsqa-validation-500", "mrqa_searchqa-validation-1011", "mrqa_newsqa-validation-3139", "mrqa_hotpotqa-validation-4289", "mrqa_triviaqa-validation-2937", "mrqa_searchqa-validation-5137"], "EFR": 1.0, "Overall": 0.7478779987373737}, {"timecode": 99, "UKR": 0.794921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1336", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1547", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2267", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3779", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3812", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-681", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2005", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2798", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-425", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5647", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6754", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7190", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7516", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-9157", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9836", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1974", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2797", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3152", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-1021", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11412", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13104", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13167", "mrqa_searchqa-validation-13297", "mrqa_searchqa-validation-13374", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-13830", "mrqa_searchqa-validation-14348", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14826", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15694", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-1654", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-2620", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-5423", "mrqa_searchqa-validation-5635", "mrqa_searchqa-validation-5831", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6635", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7267", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8297", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9807", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-4469", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-5042", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-682", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8529", "mrqa_squad-validation-8754", "mrqa_squad-validation-9306", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1636", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-218", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-2391", "mrqa_triviaqa-validation-2575", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3173", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3399", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4234", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4585", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5167", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5837", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-7564", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7640", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-920"], "OKR": 0.87890625, "KG": 0.51640625, "before_eval_results": {"predictions": ["Gunpei Yokoi", "2022", "Thomas Mundy Peterson", "$2 million", "an instant messaging client", "first Sunday after Easter", "Acid rain", "pretends to be Rico's father for two - thousand dollars so he can get money to see Siena modeling in Peru", "In 1943", "Coriolis effect", "ranking used in combat sports, such as boxing or mixed martial arts", "Marley & Me", "1966", "transmission", "an arm", "England and Wales", "Nicole Gale Anderson", "Massachusetts", "an explosion", "John Goodman", "`` Kobol's Last `` ''", "31 December 1600", "Ethiopia and Liberia", "third season", "the Mongol Yuan Dynasty", "Bart Millard", "Rationing Stamps and Cards", "The management team", "Nick Kroll", "Longliners", "Valens", "FUE harvesting method", "a minority report", "Massachusetts", "the therefore sign ( \u2234 ) is generally used before a logical consequence, such as the conclusion of a syllogism", "states", "2017 / 18", "2001", "mass cadmium poisoning of Toyama Prefecture, Japan", "Jason Momoa", "Phillip Paley", "Nick Chopper", "July 21, 1861", "Bonnie Aarons", "Aaron Harrison", "nucleus", "drizzle, rain, sleet, snow, graupel and hail", "internal epithelia", "the National League ( NL ) champion Cleveland Indians", "a multilayer", "in Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Coco Chanel", "George Washington", "Lily Allen", "Tatton Park", "Selina D'Arcy", "1964", "Ashley \"A.J.\" Jewell,", "42 years old", "step up.\"", "Argentina", "Candice Bergen", "William Shakespeare", "HBO World Championship Boxing"], "metric_results": {"EM": 0.625, "QA-F1": 0.7268301218708828}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.7272727272727273, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.25, 1.0, 0.6666666666666666, 0.0, 0.9565217391304348, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-3188", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-5283", "mrqa_naturalquestions-validation-1950", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-6564", "mrqa_triviaqa-validation-1348", "mrqa_searchqa-validation-872"], "SR": 0.625, "CSR": 0.556875, "retrieved_ids": ["mrqa_squad-train-39085", "mrqa_squad-train-30634", "mrqa_squad-train-66802", "mrqa_squad-train-29779", "mrqa_squad-train-16491", "mrqa_squad-train-40007", "mrqa_squad-train-22631", "mrqa_squad-train-45957", "mrqa_squad-train-78506", "mrqa_squad-train-84326", "mrqa_squad-train-14988", "mrqa_squad-train-18197", "mrqa_squad-train-76372", "mrqa_squad-train-84282", "mrqa_squad-train-29928", "mrqa_squad-train-43029", "mrqa_hotpotqa-validation-1867", "mrqa_triviaqa-validation-4471", "mrqa_naturalquestions-validation-6771", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1894", "mrqa_searchqa-validation-11496", "mrqa_newsqa-validation-3209", "mrqa_naturalquestions-validation-1178", "mrqa_newsqa-validation-2018", "mrqa_hotpotqa-validation-2300", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-3468", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-12607", "mrqa_hotpotqa-validation-4514", "mrqa_newsqa-validation-4024"], "EFR": 0.8333333333333334, "Overall": 0.7160885416666666}]}