{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5310, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["high cost injectable, oral, infused, or inhaled medications", "a plastid that lacks chlorophyll", "Observations on the Geology of the United States", "1887", "2000", "gain support from China", "the south", "push", "New England Patriots", "A cylindrical Service Module", "gold", "Fermat primality test", "highly diversified", "WWSB and WOTV", "the end itself", "Chen's theorem", "La Rochelle", "Fort Caroline", "around half", "the move from the manufacturing sector to the service sector", "1.7 billion years ago", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "July 18, 2006", "electromagnetic force", "Robert Bork", "East Smithfield burial site in England", "non-violent", "John Houghton", "Enthusiastic teachers", "high voltage", "Johann Walter", "Shoushi Li", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "priest", "business districts", "BankAmericard", "Bruno Mars", "Jamukha", "German New Guinea", "Onon", "good, clear laws", "the International Stanis\u0142aw Moniuszko Vocal Competition", "forces", "Factory Project", "2010", "fundraising drives", "1000 CE", "Van Nuys Airport", "overinflated", "basic design typical of Eastern bloc countries", "the tax rate", "sequential proteolytic activation of complement molecules", "customs of his tribe", "Robert Guiscard", "wide sidewalks", "CBS Sports.com", "the March Battle of Fort Bull", "a rendezvous", "6 feet 2 inches", "formalism", "the sale of indulgences", "the English Court of Appeal, the German Bundesgerichtshof, the Belgian Cour du travail", "British failures in North America", "Besan\u00e7on Hugues"], "metric_results": {"EM": 0.75, "QA-F1": 0.7846657363104732}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.2, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.3636363636363636, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6393", "mrqa_squad-validation-8452", "mrqa_squad-validation-5", "mrqa_squad-validation-6091", "mrqa_squad-validation-7382", "mrqa_squad-validation-9489", "mrqa_squad-validation-10483", "mrqa_squad-validation-4902", "mrqa_squad-validation-2145", "mrqa_squad-validation-7430", "mrqa_squad-validation-680", "mrqa_squad-validation-9896", "mrqa_squad-validation-6645", "mrqa_squad-validation-6072", "mrqa_squad-validation-525", "mrqa_squad-validation-4361"], "SR": 0.75, "CSR": 0.75, "EFR": 0.9375, "Overall": 0.84375}, {"timecode": 1, "before_eval_results": {"predictions": ["fast forwarding", "applications such as on-line betting, financial applications", "San Jose State", "DeMarcus Ware", "two poles", "the Presiding Officer", "1206", "high fuel prices and new competition from low-cost air services", "lens-shaped", "Regis Philbin", "defensins", "Sweden", "linebacker", "the Calvin cycle", "ships", "Archbishop of Westminster", "coherent theory", "\"Roentgen rays\" or \"X-Rays\"", "Fridays", "M\u00e9ni\u00e8re's disease, vertigo, fainting, tinnitus, and a cataract in one eye", "Oahu", "1784", "William of Volpiano and John of Ravenna", "yellow fever outbreaks", "the Philippines", "$125 per month", "in any other group of chloroplasts", "Abercynon", "Michael Weinenberger", "only \"essentials\"", "a pointless pursuit", "the United Nations", "plug-n-play", "Roone Arledge", "driving them in front of the army", "business districts", "1726", "lower rates of social goods", "main hymn", "France", "extinction", "ABC Entertainment Group", "the 17th century", "the degree to which these flags retain their original colors remains unknown", "T cells", "1080i HD", "the state", "30 July 1891", "Inherited wealth", "the journal Science", "administration", "elected by citizens", "Trypanosoma brucei", "Falls", "1975", "over half", "1835", "France", "a mutualistic relationship", "its initial home range spanning from Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "Principal photography began on November 2, 2016", "The song was written by Mitch Murray", "Parashara", "1958"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7884011243386243}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.14814814814814817, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_squad-validation-5758", "mrqa_squad-validation-10427", "mrqa_squad-validation-1504", "mrqa_squad-validation-2506", "mrqa_squad-validation-8662", "mrqa_squad-validation-4240", "mrqa_squad-validation-7571", "mrqa_squad-validation-4206", "mrqa_squad-validation-7457", "mrqa_squad-validation-8576", "mrqa_squad-validation-3922", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3942", "mrqa_hotpotqa-validation-5465"], "SR": 0.734375, "CSR": 0.7421875, "EFR": 1.0, "Overall": 0.87109375}, {"timecode": 2, "before_eval_results": {"predictions": ["magnetic field", "photosynthetic function", "Egyptians", "gold", "fund travelers who would come back with tales of their discoveries", "reactive allotrope of oxygen", "his personal goals with his academic goals", "ABC Circle Films", "Jews", "Kaifeng", "passion", "Metropolitan Statistical Areas", "European Union law", "monophyletic", "\"Provisional Registration\"", "biochemical oxygen demand", "electrical repair jobs", "hospitals and other institutions", "gold", "1998", "160 kPa", "The General Board of Church and Society, and the United Methodist Women", "prevented it from being cut down", "lab monitoring, adherence counseling, and assist patients with cost-containment strategies", "St. Johns River", "The increasing use of technology, specifically the rise of the internet over the past decade", "10 years", "Genghis Khan", "HIV", "1857", "Rijn", "Caris & Co.", "Stage 2", "\u00d6gedei", "breaches of law in protest against international organizations and foreign governments", "Anglo-Saxon language of their subjects", "two populations of rodents", "The Deadly Assassin and Mawdryn undead", "Dave Logan", "the top row of windows", "fast forwarding", "The Dornbirner Ach", "combustion chamber", "a gift", "104 \u00b0F (40 \u00b0C)", "strict", "the property owner", "Muslim Iberia", "1913", "patient compliance issues", "20th century", "ambiguity", "\"Bells\" was introduced by Bob Hope in the 1951 movie The Lemon Drop Kid", "the culture of maiko, who replace the... white one upon becoming one of these | a geisha.", "The Man and the Secrets", "The Sky This Week for September 2 to September 11", "Carefully paddling down this Congolese river that lends its name to a deadly virus", "The 10 dog breeds with the best sense of smell - Dogtime  Basset Hound", "The Dardanelles formerly known as Hellespont is a narrow, natural strait and internationally", "\"Guilt by Association\"", "half the northbound cars wait 90 minutes", "The Viking Ship Museum (Roskilde)", "James Edward Kelly", "2 March 1972"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7470238095238095}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.4, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.5333333333333333, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714285, 0.14285714285714288, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 0.4, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3497", "mrqa_squad-validation-1875", "mrqa_squad-validation-1308", "mrqa_squad-validation-3692", "mrqa_squad-validation-7013", "mrqa_squad-validation-1880", "mrqa_squad-validation-6244", "mrqa_squad-validation-1108", "mrqa_squad-validation-7162", "mrqa_squad-validation-1808", "mrqa_squad-validation-6361", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16877", "mrqa_searchqa-validation-3385", "mrqa_hotpotqa-validation-1393"], "SR": 0.65625, "CSR": 0.7135416666666667, "EFR": 0.9545454545454546, "Overall": 0.8340435606060607}, {"timecode": 3, "before_eval_results": {"predictions": ["a strange odor in their spacesuits", "Muqali", "inversely to member state size", "if they are distinct or equal classes", "1884", "Isaac Komnenos", "the printing press", "1997", "June 6, 1951", "Marshall Cohen", "1.7 billion years ago", "a not-for-profit United States computer networking consortium", "contemporary accounts were exaggerations", "residency registration", "Tower District", "individual state laws", "October 2007", "Moscone Center", "Voice in the Wilderness", "September 1944", "\u015ar\u00f3dmie\u015bcie", "burning a mixture of acetylene and compressed O2", "9.6%", "Commander", "macrophages and lymphocytes", "kill Luther", "his son Duncan", "\"an idealized and systematized version of conservative tribal village customs\" under the label of Sharia", "the Dongshan Dafo Dian", "Jean Cauvin", "220 miles", "\"Blue Harvest\" and \"420\"", "Thomas Commerford Martin", "oxygen from building up in them and disrupting rubisco activity. Because of this, they lack thylakoids organized into grana stacks", "\"The Book of Roger\"", "the Earth's surface", "Africa", "Pierre Bayle", "confirmed and amended", "32.9%", "30\u201360%", "1368\u20131644", "reduction gears", "Pedro Men\u00e9ndez de Avil\u00e9s", "a liquid oxygen tank exploded", "$105 billion", "1688\u20131692", "He currently plays for AFC.  Walkington in the Premier Division of the East Riding League, where his ex-teammate Leigh Palin is the manager.", "Parlophone", "Super Bowl XXIX", "The Number Twelve", "end of the 18th century", "Tulsa", "26,788", "Richa Sharma", "Stage Stores", "25 laps", "to be identified as transgender", "672 km2", "Boston and Maine Railroad's Southern Division", "Dusty Dvoracek", "He is telling me to regain the trust of those customers who are driving our vehicles", "Himalayan", "murder"], "metric_results": {"EM": 0.75, "QA-F1": 0.8208655621339445}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.8181818181818181, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.09523809523809523, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.09523809523809523, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4210", "mrqa_squad-validation-3456", "mrqa_squad-validation-4019", "mrqa_squad-validation-1116", "mrqa_squad-validation-9740", "mrqa_squad-validation-4631", "mrqa_squad-validation-8872", "mrqa_squad-validation-10413", "mrqa_squad-validation-3370", "mrqa_hotpotqa-validation-1964", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-5251", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-3564"], "SR": 0.75, "CSR": 0.72265625, "retrieved_ids": ["mrqa_squad-train-59526", "mrqa_squad-train-56925", "mrqa_squad-train-40081", "mrqa_squad-train-12068", "mrqa_squad-train-23183", "mrqa_squad-train-35397", "mrqa_squad-train-73026", "mrqa_squad-train-75405", "mrqa_squad-train-54264", "mrqa_squad-train-83973", "mrqa_squad-train-79571", "mrqa_squad-train-21957", "mrqa_squad-train-26012", "mrqa_squad-train-82786", "mrqa_squad-train-58805", "mrqa_squad-train-4956", "mrqa_squad-validation-4206", "mrqa_searchqa-validation-7896", "mrqa_squad-validation-1808", "mrqa_squad-validation-7430", "mrqa_squad-validation-3922", "mrqa_squad-validation-3692", "mrqa_squad-validation-8452", "mrqa_squad-validation-9896", "mrqa_squad-validation-6361", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-15243", "mrqa_squad-validation-6091", "mrqa_searchqa-validation-12371", "mrqa_naturalquestions-validation-7393", "mrqa_squad-validation-7571", "mrqa_squad-validation-8662"], "EFR": 0.875, "Overall": 0.798828125}, {"timecode": 4, "before_eval_results": {"predictions": ["consultant", "reformers", "Modern English", "Commission v Italy", "West", "1893", "demand for a Scottish Parliament", "1881", "1421", "W. E. B. Du Bois", "25-minute", "captive import policy", "15th century", "two", "two", "a pivotal event", "Mexico", "Black Sea", "a single output (of a total function)", "The Central Region", "Battle of Fort Bull", "Murray Gold and Ben Foster", "ambiguity", "Super Bowl XLIV", "Urarina", "a global scale", "force model", "lost in the 5th Avenue laboratory fire of March 1895", "Westwood One", "free", "Independence Day: Resurgence", "issues related to the substance of the statement", "1763\u20131775", "classical position variables", "512-bit", "Deabolis", "necessity", "adenosine triphosphate", "cartels", "Hughes Hotel", "88", "8 November 2010", "Jean Baptiste Say", "The Perfect Storm", "Terry & June Whitfield", "architectural equivalent of the Nobel Prize", "arrows", "the common mole", "a complex number raised to the zero power", "Mikhail Gorbachev", "Good Will Hunting", "Quentin Blake", "The History Boys", "a complete Caribbean vacation experience", "a metric horsepower", "a neutron", "James Hoban", "elia Earhart", "1963", "a large cricket bat shaped piece willow ready to be shaped into a bat proper", "Asuka", "The United States of America", "the iPods", "Charles M. Schulz"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7401785714285715}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-2437", "mrqa_squad-validation-9334", "mrqa_squad-validation-6197", "mrqa_squad-validation-1601", "mrqa_squad-validation-7537", "mrqa_squad-validation-10466", "mrqa_squad-validation-603", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-873", "mrqa_searchqa-validation-4355"], "SR": 0.671875, "CSR": 0.7125, "EFR": 1.0, "Overall": 0.85625}, {"timecode": 5, "before_eval_results": {"predictions": ["7:00 to 9:00 a.m. weekdays", "ammed", "vaccination", "62 acres", "Maciot de Bethencourt", "Spain", "C. J. Anderson", "Cam Newton", "eastwards", "accessory pigments that override the chlorophylls' green colors", "his last statement", "Pleistocene epoch", "he published his findings first", "Nurses", "time and space complexity", "1951", "Marches", "black earth", "Nederrijn", "opposite end from the mouth", "british", "the mid-sixties", "Kuznets curve hypothesis", "the lost chloroplast's existence", "Schr\u00f6dinger", "90\u00b0 out of phase with each other", "anticlines", "Tanaghrisson", "Siegfried", "Sydney", "220 miles (350 km)", "Northern San Diego", "Video On Demand content", "Genghis Khan", "Arizona Cardinals", "Pleurobrachia", "chloroplast's stroma", "cotton spinning", "2010", "baeocystin", "\"Krabby Road\"", "English folk-song", "England", "2009", "Ella Fitzgerald", "sarod", "1981", "Kris Kristofferson", "Nia Sanchez", "German", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Fran", "Fred Gwynne", "Kristine Leahy", "1999 Odisha", "Fat Albert", "Frontline", "shrews", "Shinola", "modern genetics", "a person trained for travelling in space", "he was shot by three gunmen outside the facility where aid distribution is coordinated", "brits", "an independent homeland for the country's ethnic Tamil minority"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6110829274891776}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.8, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.45454545454545453, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8333333333333333]}}, "before_error_ids": ["mrqa_squad-validation-9029", "mrqa_squad-validation-1064", "mrqa_squad-validation-9176", "mrqa_squad-validation-5450", "mrqa_squad-validation-10386", "mrqa_squad-validation-3257", "mrqa_squad-validation-5112", "mrqa_squad-validation-6044", "mrqa_squad-validation-4458", "mrqa_squad-validation-8900", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_naturalquestions-validation-3663", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-539", "mrqa_searchqa-validation-1523", "mrqa_newsqa-validation-1718"], "SR": 0.53125, "CSR": 0.6822916666666667, "EFR": 0.9666666666666667, "Overall": 0.8244791666666667}, {"timecode": 6, "before_eval_results": {"predictions": ["2010", "an immunological memory", "Calvin cycle", "his grandfather", "education and training", "June 11, 1962", "Jean-Claude Juncker", "68,511", "queuing", "1880", "8 mm cine film", "Pittsburgh", "the seal of the Federal Communications Commission", "\u00a3250,000", "Michael Jayston", "radiography", "Norway", "the courts of member states", "Texas", "shortening the cutoff", "12.5 acres", "a few hundred feet", "an innate force of impetus", "Conservative Party", "an international data communications network", "the environment in which they lived", "Darian Stewart", "the Great Fire of London", "acular", "the Moscone Center in San Francisco", "The View and The Chew", "Parliament of the United Kingdom at Westminster", "preventing it from being cut down", "baptism", "England", "one hundred pennies", "a coffee house", "Parkinson's", "Tintin", "piu forte", "1", "Jimmy Greaves", "McKinney", "Spock", "Solomon", "Jane Jagger and Lady Gaga", "geomorphology", "Earth", "kurkama", "Richmond in North Yorkshire", "The Passenger Pigeon", "Richard Wagner", "false teeth", "Debbie Rowe", "Russia", "1973", "The Return of the Pink Panther", "London", "Jane Thompson", "Southaven, Mississippi", "a residential area in East Java", "Jesus Walks", "President Paul Biya", "trading goods and services without exchanging money"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5973958333333333}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.3333333333333333, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6680", "mrqa_squad-validation-8167", "mrqa_squad-validation-6284", "mrqa_squad-validation-4297", "mrqa_squad-validation-8295", "mrqa_squad-validation-89", "mrqa_squad-validation-512", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-4197", "mrqa_naturalquestions-validation-8889", "mrqa_newsqa-validation-3541", "mrqa_searchqa-validation-13016", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-714"], "SR": 0.546875, "CSR": 0.6629464285714286, "retrieved_ids": ["mrqa_squad-train-81341", "mrqa_squad-train-70718", "mrqa_squad-train-44863", "mrqa_squad-train-18767", "mrqa_squad-train-47003", "mrqa_squad-train-66586", "mrqa_squad-train-54213", "mrqa_squad-train-12953", "mrqa_squad-train-27802", "mrqa_squad-train-48907", "mrqa_squad-train-19242", "mrqa_squad-train-82646", "mrqa_squad-train-43753", "mrqa_squad-train-27559", "mrqa_squad-train-18961", "mrqa_squad-train-42671", "mrqa_naturalquestions-validation-3663", "mrqa_squad-validation-809", "mrqa_triviaqa-validation-2758", "mrqa_squad-validation-1880", "mrqa_squad-validation-3922", "mrqa_triviaqa-validation-1603", "mrqa_squad-validation-1108", "mrqa_squad-validation-8576", "mrqa_searchqa-validation-5591", "mrqa_naturalquestions-validation-3942", "mrqa_hotpotqa-validation-1964", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-3591", "mrqa_squad-validation-7430", "mrqa_squad-validation-2145", "mrqa_triviaqa-validation-6761"], "EFR": 0.9655172413793104, "Overall": 0.8142318349753694}, {"timecode": 7, "before_eval_results": {"predictions": ["ten times their own weight", "Cape of Good Hope", "Time magazine", "Rhine-kilometers", "14", "150", "North American Aviation", "to register as a professional on the General Pharmaceutical Council (GPhC) register", "the Sovereign", "the weakness in school discipline", "Fort Caroline", "the concept Distributed Adaptive Message Block Switching", "at elevated partial pressures", "His lab was torn down", "interacting and working directly with students", "Omnicare, Kindred Healthcare and PharMerica", "Tiffany & Co.", "conservative", "the forts Shirley had erected at the Oneida carry", "swimming-plates", "eleven", "it would undermine the law", "1332", "separately from physicians", "the south", "Geordie", "fuel", "US$10 a week", "the harvests of their Chinese tenants eaten up by costs of equipping and dispatching men for their tours of duty", "142 pounds", "1806-07", "british", "he built a shed", "Bill Clinton", "a police car", "dead man's curve", "Edward Waverley", "the Chetniks", "b-body Dodge Charger", "paris", "Rookwood Necropolis", "Prada", "Edward R. Murrow", "british", "neptune", "british", "british", "watermelons", "neptune", "Christopher Marlowe", "british", "congruent", "Domenico Colombo", "sons of Isaac and Rebekah", "singer", "didn't work out that way", "US Federal Reserve Bank", "eight", "Jane Eyre", "World War II", "Hussein's Revolutionary Command Council", "the meter reader", "cowardly lion", "March 22"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6341145833333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4166666666666667, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6324", "mrqa_squad-validation-10251", "mrqa_squad-validation-6773", "mrqa_squad-validation-3483", "mrqa_squad-validation-1272", "mrqa_squad-validation-8238", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-8040", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-4533", "mrqa_searchqa-validation-14514", "mrqa_newsqa-validation-858"], "SR": 0.609375, "CSR": 0.65625, "EFR": 1.0, "Overall": 0.828125}, {"timecode": 8, "before_eval_results": {"predictions": ["a flour mill Boulton & Watt were building", "every four years", "Alan Turing", "2\u20133 years", "the Working Group chairs", "effectiveness of treatment regimens", "43 million tons", "720p high definition", "Panthers", "Singing Revolution", "The Newlywed Game", "17th", "the usual counterflow cycle", "pattern recognition receptors", "climate change in addition to deforestation", "Glucocorticoids", "The Late Late Show", "international footballers", "Newcastle Student Radio", "immunoglobulins and T cell receptors", "the City council", "Torchwood: Miracle Day", "November 1979", "linear", "breaches of law in protest against international organizations and foreign governments", "Cobham's", "Sir Edward Poynter", "Behind the Sofa", "the Simien Mountains", "Florida State University", "MC Hammer", "Mao Zedong", "-- Arroz con Leche", "Hawaii", "the Kiwanis Club", "the log cabin", "saxophones", "a tornado", "George Sand", "the letters of the American alphabet", "the Clinica Regina Margherita", "the dizygotic type of twins", "the DASH Diet", "Hawaii", "the Nova Scotia type of this", "neurotransmitters", "the balloon", "the Princess Diaries", "the Konabar", "Massachusetts", "larynx", "John Galt", "Arbor Day", "the spice Cloves", "the right angle", "Kentucky", "the War Hawks", "the Chinese Exclusion Act", "a crust", "1995", "the king and crown prince of Thailand", "Mineola", "Blender's \"500 Greatest Songs Since You Were Born\"", "2018\u201319 UEFA Europa League group stage"], "metric_results": {"EM": 0.578125, "QA-F1": 0.64921875}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, false, true, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3373", "mrqa_squad-validation-8553", "mrqa_squad-validation-787", "mrqa_squad-validation-3310", "mrqa_squad-validation-434", "mrqa_squad-validation-8747", "mrqa_searchqa-validation-16960", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5814", "mrqa_naturalquestions-validation-10012", "mrqa_triviaqa-validation-4730", "mrqa_hotpotqa-validation-1159", "mrqa_newsqa-validation-698", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-1263"], "SR": 0.578125, "CSR": 0.6475694444444444, "EFR": 1.0, "Overall": 0.8237847222222222}, {"timecode": 9, "before_eval_results": {"predictions": ["Holyrood area of Edinburgh", "Dutch law", "terra nullius", "assisting in fabricating evidence or committing perjury", "kicker", "relativity", "Red Turban rebels", "Jurassic Period", "Presque Isle", "William S. Paley", "anaerobic bacteria", "catch more sunlight in deep water", "eicosanoids and cytokines", "live", "50-yard line", "he followed the fishermen and captured the mermaid", "1/6", "DC traction motor", "the \"richest 1 percent in the United States now own more wealth than the bottom 90 percent\"", "the divinity of Jesus", "EastEnders", "J. S. Bach", "highest", "a few drops", "1882", "the Beldam / Other Mother", "North America", "Alastair Cook", "Swadlincote", "inversely proportional to the wave frequency", "flytrap", "the chalk ridge line west of the Needles breached to form the island", "Allison Janney", "2026", "Georgia", "it showed such a disregard for the life and safety of others as to amount to a crime and deserve punishment", "1984", "4 September 1936", "Andrew Moray and William Wallace", "Jane", "Pangaea", "Have I Told You Lately", "the sinoatrial node", "September", "the 2013 non-fiction book of the same name by David Finkel", "to prevent further offense by convincing the offender that their conduct was wrong", "Bob Dylan", "September of that year", "a judge", "Lynda Carter", "virtually no limit to the number of reads", "A substitute good", "September 27, 2017", "President Gerald Ford", "Monk's Caf\u00e9", "Robert Duncan McNeill", "Tintin", "Alaska", "140 million", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "Fa'aleaga Young Yen", "energy", "Billy Budd, Billy Budd", "a hearing or argument"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6426321462047351}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, false, true, true, false, false, false, true, true, true, false, false, false, false, false], "QA-F1": [0.4, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.14814814814814814, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8372093023255813, 0.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 1.0, 0.15999999999999998, 0.0, 0.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-4460", "mrqa_squad-validation-9764", "mrqa_squad-validation-10204", "mrqa_squad-validation-805", "mrqa_squad-validation-7459", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-3481", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-2507", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-12968"], "SR": 0.53125, "CSR": 0.6359375, "retrieved_ids": ["mrqa_squad-train-38382", "mrqa_squad-train-26822", "mrqa_squad-train-81038", "mrqa_squad-train-19862", "mrqa_squad-train-28564", "mrqa_squad-train-35372", "mrqa_squad-train-41298", "mrqa_squad-train-69623", "mrqa_squad-train-72515", "mrqa_squad-train-85784", "mrqa_squad-train-14126", "mrqa_squad-train-10782", "mrqa_squad-train-113", "mrqa_squad-train-19100", "mrqa_squad-train-60370", "mrqa_squad-train-40628", "mrqa_squad-validation-8872", "mrqa_triviaqa-validation-4730", "mrqa_naturalquestions-validation-3942", "mrqa_squad-validation-6072", "mrqa_searchqa-validation-6900", "mrqa_hotpotqa-validation-5101", "mrqa_triviaqa-validation-6413", "mrqa_squad-validation-680", "mrqa_hotpotqa-validation-171", "mrqa_searchqa-validation-5814", "mrqa_squad-validation-1272", "mrqa_squad-validation-6284", "mrqa_squad-validation-1108", "mrqa_newsqa-validation-1664", "mrqa_hotpotqa-validation-2327", "mrqa_triviaqa-validation-478"], "EFR": 0.9666666666666667, "Overall": 0.8013020833333333}, {"timecode": 10, "UKR": 0.7421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1124", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-956", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10305", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-5814", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-10067", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10127", "mrqa_squad-validation-10204", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-1023", "mrqa_squad-validation-10251", "mrqa_squad-validation-10251", "mrqa_squad-validation-10260", "mrqa_squad-validation-10287", "mrqa_squad-validation-10351", "mrqa_squad-validation-10386", "mrqa_squad-validation-10387", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-10504", "mrqa_squad-validation-1051", "mrqa_squad-validation-1064", "mrqa_squad-validation-1071", "mrqa_squad-validation-1078", "mrqa_squad-validation-1104", "mrqa_squad-validation-1108", "mrqa_squad-validation-1108", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1142", "mrqa_squad-validation-1181", "mrqa_squad-validation-1236", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1282", "mrqa_squad-validation-1301", "mrqa_squad-validation-1308", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1378", "mrqa_squad-validation-1401", "mrqa_squad-validation-1461", "mrqa_squad-validation-1504", "mrqa_squad-validation-1506", "mrqa_squad-validation-1552", "mrqa_squad-validation-1553", "mrqa_squad-validation-1554", "mrqa_squad-validation-159", "mrqa_squad-validation-1601", "mrqa_squad-validation-1636", "mrqa_squad-validation-1706", "mrqa_squad-validation-1780", "mrqa_squad-validation-1808", "mrqa_squad-validation-1813", "mrqa_squad-validation-1831", "mrqa_squad-validation-1856", "mrqa_squad-validation-1875", "mrqa_squad-validation-1880", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-2040", "mrqa_squad-validation-2069", "mrqa_squad-validation-2097", "mrqa_squad-validation-2135", "mrqa_squad-validation-2145", "mrqa_squad-validation-2210", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2449", "mrqa_squad-validation-2451", "mrqa_squad-validation-2453", "mrqa_squad-validation-2476", "mrqa_squad-validation-2506", "mrqa_squad-validation-2571", "mrqa_squad-validation-2603", "mrqa_squad-validation-2643", "mrqa_squad-validation-2643", "mrqa_squad-validation-2717", "mrqa_squad-validation-2753", "mrqa_squad-validation-2780", "mrqa_squad-validation-2807", "mrqa_squad-validation-2832", "mrqa_squad-validation-2865", "mrqa_squad-validation-2888", "mrqa_squad-validation-2955", "mrqa_squad-validation-3086", "mrqa_squad-validation-3092", "mrqa_squad-validation-31", "mrqa_squad-validation-3109", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3196", "mrqa_squad-validation-3223", "mrqa_squad-validation-3257", "mrqa_squad-validation-3310", "mrqa_squad-validation-3320", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3370", "mrqa_squad-validation-3374", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3456", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-351", "mrqa_squad-validation-3551", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3607", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3692", "mrqa_squad-validation-3724", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3823", "mrqa_squad-validation-3865", "mrqa_squad-validation-3890", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3939", "mrqa_squad-validation-3998", "mrqa_squad-validation-401", "mrqa_squad-validation-4018", "mrqa_squad-validation-4019", "mrqa_squad-validation-4100", "mrqa_squad-validation-4110", "mrqa_squad-validation-4162", "mrqa_squad-validation-4206", "mrqa_squad-validation-4210", "mrqa_squad-validation-4232", "mrqa_squad-validation-4240", "mrqa_squad-validation-4297", "mrqa_squad-validation-4316", "mrqa_squad-validation-4343", "mrqa_squad-validation-441", "mrqa_squad-validation-4430", "mrqa_squad-validation-4458", "mrqa_squad-validation-4460", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4615", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4665", "mrqa_squad-validation-4729", "mrqa_squad-validation-4783", "mrqa_squad-validation-4791", "mrqa_squad-validation-4795", "mrqa_squad-validation-4824", "mrqa_squad-validation-4841", "mrqa_squad-validation-4857", "mrqa_squad-validation-4860", "mrqa_squad-validation-4870", "mrqa_squad-validation-4901", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-5", "mrqa_squad-validation-50", "mrqa_squad-validation-510", "mrqa_squad-validation-5115", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5187", "mrqa_squad-validation-525", "mrqa_squad-validation-5275", "mrqa_squad-validation-5310", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5422", "mrqa_squad-validation-5450", "mrqa_squad-validation-5471", "mrqa_squad-validation-5492", "mrqa_squad-validation-5591", "mrqa_squad-validation-5602", "mrqa_squad-validation-5624", "mrqa_squad-validation-5638", "mrqa_squad-validation-5714", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5844", "mrqa_squad-validation-5883", "mrqa_squad-validation-5889", "mrqa_squad-validation-5943", "mrqa_squad-validation-5971", "mrqa_squad-validation-5978", "mrqa_squad-validation-60", "mrqa_squad-validation-6015", "mrqa_squad-validation-603", "mrqa_squad-validation-6044", "mrqa_squad-validation-6070", "mrqa_squad-validation-6072", "mrqa_squad-validation-6091", "mrqa_squad-validation-6120", "mrqa_squad-validation-6143", "mrqa_squad-validation-6181", "mrqa_squad-validation-6197", "mrqa_squad-validation-62", "mrqa_squad-validation-6255", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-6361", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6408", "mrqa_squad-validation-6428", "mrqa_squad-validation-6454", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6625", "mrqa_squad-validation-6626", "mrqa_squad-validation-6645", "mrqa_squad-validation-6657", "mrqa_squad-validation-6658", "mrqa_squad-validation-6658", "mrqa_squad-validation-6680", "mrqa_squad-validation-6725", "mrqa_squad-validation-6753", "mrqa_squad-validation-6753", "mrqa_squad-validation-6773", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6958", "mrqa_squad-validation-6997", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7013", "mrqa_squad-validation-7021", "mrqa_squad-validation-7040", "mrqa_squad-validation-7082", "mrqa_squad-validation-7101", "mrqa_squad-validation-7162", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7317", "mrqa_squad-validation-7382", "mrqa_squad-validation-7395", "mrqa_squad-validation-7430", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7463", "mrqa_squad-validation-7537", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7670", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7708", "mrqa_squad-validation-7765", "mrqa_squad-validation-7831", "mrqa_squad-validation-7837", "mrqa_squad-validation-7867", "mrqa_squad-validation-787", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-7959", "mrqa_squad-validation-7961", "mrqa_squad-validation-7961", "mrqa_squad-validation-805", "mrqa_squad-validation-806", "mrqa_squad-validation-8135", "mrqa_squad-validation-8227", "mrqa_squad-validation-8233", "mrqa_squad-validation-8238", "mrqa_squad-validation-8242", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-8553", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8596", "mrqa_squad-validation-8602", "mrqa_squad-validation-8627", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8755", "mrqa_squad-validation-8781", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-89", "mrqa_squad-validation-8900", "mrqa_squad-validation-8971", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9109", "mrqa_squad-validation-9154", "mrqa_squad-validation-9176", "mrqa_squad-validation-9226", "mrqa_squad-validation-9240", "mrqa_squad-validation-9304", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9371", "mrqa_squad-validation-9405", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9546", "mrqa_squad-validation-9562", "mrqa_squad-validation-9611", "mrqa_squad-validation-9619", "mrqa_squad-validation-968", "mrqa_squad-validation-9750", "mrqa_squad-validation-9764", "mrqa_squad-validation-9856", "mrqa_squad-validation-9890", "mrqa_squad-validation-9895", "mrqa_squad-validation-9896", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-873"], "OKR": 0.90625, "KG": 0.4921875, "before_eval_results": {"predictions": ["Warszawa", "the SI unit of magnetic flux density the tesla", "2007", "Duval County", "2003", "the father of the house when in his home", "Electrical Experimenter", "Richard Wilkinson and Kate Pickett", "some teachers and parents", "Governor Vaudreuil", "pastors and teachers", "Justin Tucker", "1543", "None", "Yosemite Freeway/Eisenhower Freeway", "Switzerland", "unit-dose, or a single doses of medicine", "War of Currents", "the \"vanguard of change and Islamic reform\" centered around the Muslim Brotherhood.", "continental European", "Roger Goodell", "festivals", "9", "Adelaide", "once", "Around 200,000 passengers", "itty Hawk", "Nidal Hasan", "the University of Maryland", "priest Charles Coughlin", "Sean", "Consigliere", "Pierce County", "Harry F. Sinclair", "Homebrewing", "December 1974", "2012", "1999", "2004", "an Academy Award in the category Best Sound", "Nelson Rockefeller", "Fort Snelling, Minnesota", "James Gay-Rees, George Pank, and Paul Bell", "State House in Augusta", "1970", "1978", "the Democratic National Committee", "My Cat from Hell", "Richard B. Riddick", "Colonel", "1999", "17", "La Liga", "Buffalo Soldiers", "Kal Ho Naa Ho", "Key West, Florida", "gastrocnemius", "John Roberts", "repechage", "Carl Johan Bernadotte", "two", "Madonna", "Freddie Mercury", "the Marine Band"], "metric_results": {"EM": 0.625, "QA-F1": 0.7510550213675213}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333334, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1251", "mrqa_squad-validation-2318", "mrqa_squad-validation-10259", "mrqa_squad-validation-2337", "mrqa_squad-validation-4562", "mrqa_squad-validation-6526", "mrqa_squad-validation-9578", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-674", "mrqa_naturalquestions-validation-7608", "mrqa_triviaqa-validation-3265"], "SR": 0.625, "CSR": 0.6349431818181819, "EFR": 1.0, "Overall": 0.7551136363636364}, {"timecode": 11, "before_eval_results": {"predictions": ["pr\u00e9tendus r\u00e9form\u00e9s", "587,000", "Northumberland", "Mnemiopsis", "from tomb and memorial, to portrait, allegorical, religious, mythical, statues for gardens including fountains, as well as architectural decorations", "Beirut", "smaller trade relations with their neighbours", "Tommy Lee Jones", "150", "four", "308", "Queen Victoria", "large compensation pools", "the main opposition party, the Orange Democratic Movement (ODM)", "Charlesfort", "by the immune system", "Battle of the Restigouche", "Boston", "force of gravity", "head writer and executive producer", "David Lynch", "b-Man", "every ten years", "the son of a vicar in the British Midlands", "Batmitten", "Hong Kong", "ambilevous", "Batman", "a goat", "Irrawaddy River", "Ed White", "River Hull", "the lunar new year holiday", "Samuel Johnson", "Copenhagen", "Troy", "non-governmental organisation", "John Gorman", "bison", "Edinburgh", "Viking", "Paul Gauguin", "Action Comics", "CNN.com", "change in the energy of the system", "Novak Djokovic", "New Zealand", "Oasis", "The Golden Girls", "green, red, white", "Rajasthan", "Gary Puckett", "floating ribs", "annual meeting between leaders from eight of the most powerful countries in the world", "golf", "Secretary of Homeland Security", "Bee Gees", "Adelaide", "Eddie Izzard", "Sabina Guzzanti", "largest and perhaps most sophisticated ring of its kind in U.S. history", "a quark", "krypton", "the word is used to translate several Hebrew words, including Hod ( \u05d4\u05d5\u05d3 ) and kabod ( \u03b4\u03cc\u03be\u03b1 )"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5895213931770535}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.11320754716981131, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4256", "mrqa_squad-validation-5388", "mrqa_squad-validation-5545", "mrqa_squad-validation-5303", "mrqa_squad-validation-7083", "mrqa_squad-validation-8421", "mrqa_squad-validation-6449", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-3095", "mrqa_naturalquestions-validation-5094", "mrqa_newsqa-validation-3199", "mrqa_searchqa-validation-7976", "mrqa_naturalquestions-validation-9323"], "SR": 0.515625, "CSR": 0.625, "EFR": 0.9354838709677419, "Overall": 0.7402217741935484}, {"timecode": 12, "before_eval_results": {"predictions": ["convulsions (normally fatal for divers)", "Henry Cavendish", "Lower Norfolk County", "melatonin", "90-60's", "the deaths of two friends", "1985", "Ismailiyah, Egypt", "England", "the ability to pursue valued goals", "tentilla", "political support in his struggle against leftists", "$5 million", "Lake George", "Jadaran", "Dwight D. Eisenhower", "decreases", "one", "Secretariat", "1952", "Australia", "September 1901", "The United States of America (USA)", "The Dragon", "psilocin", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Eurasia", "Boyd Gaming", "MGM Resorts International", "James G. Kiernan", "Omega SA", "September 14, 1877", "A third jersey, alternate jersey, third kit or alternate uniform", "Yasir Hussain", "Malayalam", "Kennedy Road", "2002", "31", "Grant Field", "Bill Boyd", "Jack Ryan", "Emilia-Romagna", "Buckingham Palace", "322,520", "Chief Strategy Officer", "3 September 1943", "Bedknobs and Broomsticks", "Jane", "William Bradford", "140 million", "\"Beauty and the Beast\"", "Gary Ross", "International Boxing Hall of Fame (IBHOF)", "The 1996 PGA Championship", "Revolver", "Jack Nicklaus", "A term suggests repudiation, change of mind, repentance, and atonement", "Mussolini", "Ryan O' Neal", "off the coast of Dubai", "1918", "butter", "Boston", "an extended period of abundant rainfall lasting many thousands of years"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7373342803030303}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, false, false, true, false, true, true, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3639", "mrqa_squad-validation-2657", "mrqa_squad-validation-9565", "mrqa_squad-validation-6108", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-278", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-47", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-4173", "mrqa_newsqa-validation-2790", "mrqa_triviaqa-validation-2147"], "SR": 0.65625, "CSR": 0.6274038461538461, "retrieved_ids": ["mrqa_squad-train-64571", "mrqa_squad-train-60345", "mrqa_squad-train-84803", "mrqa_squad-train-10236", "mrqa_squad-train-32285", "mrqa_squad-train-29022", "mrqa_squad-train-29360", "mrqa_squad-train-40278", "mrqa_squad-train-80941", "mrqa_squad-train-59024", "mrqa_squad-train-17749", "mrqa_squad-train-33267", "mrqa_squad-train-64043", "mrqa_squad-train-2564", "mrqa_squad-train-60822", "mrqa_squad-train-45275", "mrqa_squad-validation-5112", "mrqa_searchqa-validation-16960", "mrqa_naturalquestions-validation-1694", "mrqa_searchqa-validation-14307", "mrqa_triviaqa-validation-7470", "mrqa_squad-validation-5758", "mrqa_searchqa-validation-8139", "mrqa_triviaqa-validation-253", "mrqa_searchqa-validation-12649", "mrqa_squad-validation-5545", "mrqa_triviaqa-validation-1686", "mrqa_squad-validation-7013", "mrqa_searchqa-validation-15243", "mrqa_triviaqa-validation-2321", "mrqa_searchqa-validation-14471", "mrqa_squad-validation-3310"], "EFR": 1.0, "Overall": 0.7536057692307693}, {"timecode": 13, "before_eval_results": {"predictions": ["the riches of Croesus", "Fred Silverman", "occupational burnout", "Israel", "\"Guilt implies wrong-doing. I feel I have done no wrong, but I am guilty of doing no wrong. I therefore plead not guilty.\"", "huge mouths armed with groups of large, stiffened cilia that act as teeth", "$20.4 billion", "twelve", "Anglo-Saxons", "Doctor Who Confidential documentary", "stricter discipline based on their power of expulsion", "killed in a horse-riding accident", "1522", "eight", "Of course [the price of oil] is going to rise", "Roman law", "John Maler Collier", "chipmunk", "The Red King", "Melbourne", "Albania", "brown trout", "Mayflower", "Mike Henry", "lacrimal", "George Best", "experience", "Bake Off", "Red Lion", "Fenn Street School", "Smiths", "Peter Crouch", "The Nobel Prize in Literature", "Pakistan", "The Observer", "United States", "Big Fat Gypsy Wedding", "bearded", "Andes", "Thor", "The Comitium", "Moon River", "Tina Turner", "SW19", "Lancashire", "Pacific Ocean", "racing", "Oscar Wilde", "climatology", "Charlie Brown", "vinaya", "aguacate", "Black Sea", "pyruvate", "1933", "the next leap", "Abu Dhabi", "Craig William Macneill", "terminal brain cancer", "800,000", "lava", "gigante", "Serie B", "Saoirse Ronan"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5640128968253968}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, true, false, false, true, false, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.4444444444444444, 0.1111111111111111, 1.0, 1.0, 1.0, 0.8, 0.2222222222222222, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3863", "mrqa_squad-validation-6913", "mrqa_squad-validation-4621", "mrqa_squad-validation-7811", "mrqa_squad-validation-7112", "mrqa_squad-validation-3730", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-1386", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2989", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1330", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-4951", "mrqa_triviaqa-validation-2335", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-9026", "mrqa_hotpotqa-validation-3607", "mrqa_searchqa-validation-1416", "mrqa_searchqa-validation-15315"], "SR": 0.484375, "CSR": 0.6171875, "EFR": 1.0, "Overall": 0.7515625}, {"timecode": 14, "before_eval_results": {"predictions": ["Cathedral of Saint John the Divine", "The Ruhr", "Hulu", "average case complexity", "southern Europe", "10 o'clock", "NYPD Blue", "AAUW study", "Magnetophon tape recorder", "he explored the mountains in hunter's garb", "Rotterdam", "if (as WWF argued), population levels would start to drop to a sustainable level (1/3 of current levels, so about 2 billion people", "Charles Dickens", "force", "best teachers", "imperfect", "albatross", "wind", "go and save the best for last", "The National Gallery of Art", "netherlands", "water", "Geneva", "menelaus", "the pheasant that's its state bird nests in the Black Hills", "turkeys", "martin peters", "lionhead", "William", "a Asian parrot", "a light-year", "netherlands", "Don Juan", "Tim Russert", "Prince of Wales", "cocoa butter", "Violent Femmes", "oats", "a guardian angel", "laser", "James Fenimore Cooper", "Veep", "fiery", "rudyard Kipling", "lead villain", "a big grouch", "Copenhagen", "Madonna", "Jose de San", "Madrid", "fiery", "Jaime lachica Cardinal Sin", "Rocky Mountain National Park", "Blackwell's Island", "fertilization", "India is the world's second most populous country after the People's Republic of China", "Nissan", "wood", "Kind Hearts and Coronets", "2012", "poems", "Cyprus", "netherlands", "russell hudson"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5371527777777778}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7428571428571429, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.4444444444444444, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1700", "mrqa_squad-validation-9895", "mrqa_squad-validation-7632", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10428", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1948", "mrqa_searchqa-validation-9250", "mrqa_searchqa-validation-8283", "mrqa_searchqa-validation-14628", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-6931", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-8607", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-5613", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-14502", "mrqa_naturalquestions-validation-8420", "mrqa_triviaqa-validation-4416", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-7610", "mrqa_triviaqa-validation-7170"], "SR": 0.453125, "CSR": 0.60625, "EFR": 0.9714285714285714, "Overall": 0.7436607142857142}, {"timecode": 15, "before_eval_results": {"predictions": ["trial division", "Go-Ahead", "three to five", "heavy/highway, heavy civil or heavy engineering", "Osama bin Laden", "September 1944", "paramagnetic", "criminal", "complexity classes", "April 1, 1963", "Jamukha", "consultant", "711,988", "Mumbai Rajdhani Express", "Speaker of the House of Representatives", "Hugo Weaving", "the passing of the year", "Number 4, Privet Drive, Little Whinging in Surrey, England", "(Bob) Golding UK", "the nerves and ganglia outside the brain and spinal cord", "Aman Gandotra", "Daya Jethalal Gada", "Kevin Sumlin", "a tree species", "the American colonies", "Canada", "two - stroke engines and chain drive", "the English", "a writ of certiorari", "Emma Watson", "Guant\u00e1namo Bay", "a limited period of time", "Colony of Virginia", "January 2017", "2013", "Tatsumi", "December 15, 2017", "the Sunni Muslim family", "Magnavox Odyssey", "Koch Records", "Christianity", "India", "The neck", "between 1923 and 1925", "Moscazzano", "the stems and roots of certain vascular plants", "Lager", "the most recent Super Bowl champions", "in the reverse direction", "San Francisco, California", "Hal Derwin", "to oversee the local church", "2007", "0.116 mm", "Robert Boyle", "the solar system", "Ascona", "Ludwig van Beethoven", "former Boca Juniors", "Akshay Kumar", "Harriet", "(Robert) Zemeckis", "(temperature)", "a bouquet"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6567918192918194}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.7499999999999999, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.30769230769230765, 0.5, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.22222222222222224, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.16666666666666666, 0.5, 0.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_squad-validation-6113", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-144", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-8962", "mrqa_hotpotqa-validation-1409", "mrqa_newsqa-validation-3042", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-11316", "mrqa_searchqa-validation-8385"], "SR": 0.53125, "CSR": 0.6015625, "retrieved_ids": ["mrqa_squad-train-51154", "mrqa_squad-train-55247", "mrqa_squad-train-25026", "mrqa_squad-train-78867", "mrqa_squad-train-46275", "mrqa_squad-train-76075", "mrqa_squad-train-42870", "mrqa_squad-train-8637", "mrqa_squad-train-50006", "mrqa_squad-train-50703", "mrqa_squad-train-1261", "mrqa_squad-train-60595", "mrqa_squad-train-79209", "mrqa_squad-train-10086", "mrqa_squad-train-38490", "mrqa_squad-train-42198", "mrqa_naturalquestions-validation-75", "mrqa_searchqa-validation-5613", "mrqa_naturalquestions-validation-5199", "mrqa_hotpotqa-validation-2896", "mrqa_triviaqa-validation-3172", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-15243", "mrqa_squad-validation-7112", "mrqa_triviaqa-validation-3751", "mrqa_hotpotqa-validation-1159", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-13016", "mrqa_squad-validation-434", "mrqa_hotpotqa-validation-1473", "mrqa_squad-validation-6284"], "EFR": 0.9666666666666667, "Overall": 0.7417708333333334}, {"timecode": 16, "before_eval_results": {"predictions": ["512-bit", "the worst-case time complexity T(n)", "National Broadcasting Company", "the Venetian merchant Marco Polo", "November 2006 and May 2008", "complex", "temperatures that are too cold in northern Europe for the survival of fleas", "Chloroplasts are highly dynamic", "xenoliths", "approximately 80 avulsions", "The Premier of Victoria is the leader of the political party or coalition with the most seats", "April 1887", "cheese, \u201cspecial sauce\u201d (a variant of Thousand Island dressing), iceberg lettuce, pickles, and onions, served in a three-part sesame seed bun", "Rock Follies of \u201977", "Montmorency", "Nut & Honey Crunch", "Elton John", "beer", "Thanet Life", "a double dip recession", "Corfu", "midrib", "Leopoldville", "8 minutes", "Federal Reserve System", "four", "cyclops", "oxygen", "Silent Spring", "the value of unknown electrical resistance", "white spirit", "Possumhaw Viburnum", "Harold Wilson", "Denmark", "William", "James Mason", "Shooting Star", "West Point", "ostrich", "Ishmael", "Artur Lundkvist", "the 5th fret", "Cherie Currie", "Clijsters", "Les Dennis", "the A38", "Catherine Cawood", "Virgin empire", "1948", "Port Talbot", "rain", "\"The best is yet to come\"", "Nicola Adams", "Sax Rohmer", "EU Data Protection Directive 1995", "May 2010", "Bruce R. Cook", "Viscount Barnewall", "The Obama administration", "The man ran away, police chased him and a gunfight ensued", "the Equator", "Aerosmith", "Cesar Millan", "Princeton University"], "metric_results": {"EM": 0.5, "QA-F1": 0.5977678571428572}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.2666666666666667, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1708", "mrqa_squad-validation-5605", "mrqa_squad-validation-8105", "mrqa_squad-validation-8560", "mrqa_squad-validation-9357", "mrqa_squad-validation-2852", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-5135", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-1382", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-1360", "mrqa_naturalquestions-validation-3930", "mrqa_hotpotqa-validation-1542", "mrqa_newsqa-validation-1537", "mrqa_hotpotqa-validation-4298"], "SR": 0.5, "CSR": 0.5955882352941176, "EFR": 1.0, "Overall": 0.7472426470588236}, {"timecode": 17, "before_eval_results": {"predictions": ["after the end of the Mexican War", "cut off", "journalist", "Seventy percent", "modern hatred of the Jews", "Germany and Austria", "the principle of inclusions and components", "Sweynforkbeard", "the King", "eight", "Sierra Freeway", "Mickey Mouse", "manly characters", "Spain and Portugal", "may", "Lego bricks", "dance", "born into Brothels", "Quebec", "Planet of the Apes", "Prince Edward Island", "bilirubin", "frosted", "Virginia Woolf", "Vasco da Gama", "canter", "Musculus gluteus maximus", "Munich massacre", "Arbor Day", "Countrywide Financial", "speed cameras", "mania bin Laden", "manhattan", "manav", "Nikita Khrushchev", "Other Rooms", "hair", "black Forest", "robert stempel", "boo", "sepoy", "last", "manhattan man", "submarines", "Joan", "pea soup", "Trinidad and Tobago", "Vladimir Nabokov", "frosting", "Peter Pan", "synonymous", "a laser beam", "Phi Beta Phi", "Elizabeth Weber", "Numbers 22 : 28", "manhattan", "Prince Philip", "Athenion", "5.3 million", "pilot", "amanda Lynn Touma", "Pandora", "manhattan", "paper sales company"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5235119047619048}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.5714285714285715, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-10273", "mrqa_squad-validation-4260", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-6", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-9389", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-4933", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-9551", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-2347", "mrqa_naturalquestions-validation-3284", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-6259", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2525", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-6435"], "SR": 0.421875, "CSR": 0.5859375, "EFR": 1.0, "Overall": 0.7453125}, {"timecode": 18, "before_eval_results": {"predictions": ["quantum mechanics", "near the surface", "Alfred Stevens", "state intervention through taxation", "algebraic", "third", "1886/1887", "clerical", "Apollo", "Linebacker", "2000", "Richard Street", "Jack Chick", "1926 Paris", "burlesque", "Polk County", "Skyscraper", "schoolteacher", "Dunlop India Ltd.", "David Anthony O'Leary", "a family member", "Tamil Nadu", "Attorney General and as Lord Chancellor of England", "Fort Berthold Reservation", "fennec", "Norwood, Massachusetts", "1993", "switzerland World Championionship", "liquidambar", "Battle of Chester", "Flashback: The Quest for Identity", "Kentucky", "Marco Fu", "Francis the Talking Mule", "Kristin Scott Thomas, Anne Bancroft, James Fox, Derek Jacobi, and Sean Penn", "Clark Gable", "Christian", "paternalistic policies enacted upon Native American tribes", "The Hindu Group", "Kealakekua Bay", "1919", "Julia Verdin", "2013", "Guthred", "Centers for Medicare & Medicaid Services", "Australian", "1945", "1912", "Teatro Carlo Felice", "The Maze Runner", "pronghorn", "ambassador to Ghana", "Life Is a Minestrone", "the coffee shop Monk's", "Sir Ernest Rutherford", "a leg break", "Dot Cotton (June Brown)", "France", "at least $20 million to $30 million", "(Ulysses S. Grant)", "Virgil Tibbs", "Guantanamo Bay prison and CIA \"black site\" prisons", "CNN", "Michael Arrington"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5880016684704186}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.25, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.9090909090909091, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.5, 0.0, 0.42857142857142855, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9286", "mrqa_squad-validation-9888", "mrqa_squad-validation-8026", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-0", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-7209", "mrqa_newsqa-validation-2601", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-12442", "mrqa_newsqa-validation-1114"], "SR": 0.46875, "CSR": 0.5797697368421053, "retrieved_ids": ["mrqa_squad-train-85679", "mrqa_squad-train-37807", "mrqa_squad-train-5048", "mrqa_squad-train-55769", "mrqa_squad-train-3406", "mrqa_squad-train-2503", "mrqa_squad-train-39139", "mrqa_squad-train-78486", "mrqa_squad-train-45906", "mrqa_squad-train-64910", "mrqa_squad-train-46678", "mrqa_squad-train-16916", "mrqa_squad-train-16068", "mrqa_squad-train-43741", "mrqa_squad-train-27528", "mrqa_squad-train-15856", "mrqa_squad-validation-10204", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-1523", "mrqa_hotpotqa-validation-1542", "mrqa_squad-validation-4019", "mrqa_triviaqa-validation-4534", "mrqa_searchqa-validation-10428", "mrqa_squad-validation-4361", "mrqa_searchqa-validation-1151", "mrqa_squad-validation-5303", "mrqa_squad-validation-4297", "mrqa_squad-validation-8105", "mrqa_triviaqa-validation-2523", "mrqa_squad-validation-7382", "mrqa_squad-validation-7112", "mrqa_searchqa-validation-15030"], "EFR": 0.9705882352941176, "Overall": 0.7381965944272446}, {"timecode": 19, "before_eval_results": {"predictions": ["40,000 plant species", "swimming-plates", "MHC I", "Denver's Executive Vice President of Football Operations and General Manager", "10", "Continental Edison Company in France", "Time", "Nafzger", "Warszawa", "Troggs", "schizophrenia", "Cressida", "Tom Osborne", "Moses", "a shih tzu", "skinner", "Fiddler on the Roof", "Monopoly", "In 1963 she said, \"I feel as though I'm suddenly on stage for a part I never rehearsed\"", "Stanislaw Leszczyska", "Ceiba pentandra", "Alien", "Tower of London", "crocodile", "Cher", "onion", "Tommy Lasorda", "Benazir Bhutto", "Coca-Cola", "Red Bull", "Chaillot", "Ibrahim Petrovich Hannibal", "butter", "Westfield, New York", "\"Jerry Seinfeld\"", "Pyrrhus", "Guatemala", "bonds", "Edgar Allan Poe", "chicken & egg", "August Wilson", "Sacher Torte", "Palestine: Peace Not Apartheid", "a person falling with a parachute", "parrot", "South African", "dessert glasses", "Daisy Miller", "a calculator", "American opposition to British policy", "Frank Sinatra", "the Sonnets", "South Africa", "the standard for the Navy's commissioned ships while in commission", "Pearl Harbor", "Costa del Sol", "River Stour", "Annales de chimie et deimens", "gull-wing doors", "\"Jersey Shore\"", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "1994", "state senators", "38"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5143418943994602}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, false, true, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.9473684210526316, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.15384615384615385, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4264", "mrqa_squad-validation-4730", "mrqa_squad-validation-375", "mrqa_squad-validation-1239", "mrqa_searchqa-validation-835", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1800", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-3762", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-6208", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9809", "mrqa_hotpotqa-validation-4813", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-406"], "SR": 0.46875, "CSR": 0.57421875, "EFR": 1.0, "Overall": 0.74296875}, {"timecode": 20, "UKR": 0.7421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1340", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3480", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5054", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-928", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4657", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1843", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14502", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5583", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8385", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-10251", "mrqa_squad-validation-10273", "mrqa_squad-validation-10285", "mrqa_squad-validation-10335", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1095", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1180", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1461", "mrqa_squad-validation-1552", "mrqa_squad-validation-1554", "mrqa_squad-validation-161", "mrqa_squad-validation-1636", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1706", "mrqa_squad-validation-1808", "mrqa_squad-validation-1949", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2069", "mrqa_squad-validation-2318", "mrqa_squad-validation-2369", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2453", "mrqa_squad-validation-2458", "mrqa_squad-validation-2476", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2717", "mrqa_squad-validation-2768", "mrqa_squad-validation-2780", "mrqa_squad-validation-2832", "mrqa_squad-validation-2888", "mrqa_squad-validation-3046", "mrqa_squad-validation-3138", "mrqa_squad-validation-3153", "mrqa_squad-validation-3197", "mrqa_squad-validation-3217", "mrqa_squad-validation-3223", "mrqa_squad-validation-3243", "mrqa_squad-validation-3326", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-3500", "mrqa_squad-validation-3551", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3922", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4210", "mrqa_squad-validation-4226", "mrqa_squad-validation-4240", "mrqa_squad-validation-4256", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4348", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-441", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4631", "mrqa_squad-validation-4666", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4870", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5106", "mrqa_squad-validation-5112", "mrqa_squad-validation-5118", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5242", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5344", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5624", "mrqa_squad-validation-5714", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5958", "mrqa_squad-validation-6015", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6074", "mrqa_squad-validation-6181", "mrqa_squad-validation-6196", "mrqa_squad-validation-6244", "mrqa_squad-validation-6284", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-6728", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6920", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7083", "mrqa_squad-validation-7112", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7398", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7867", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8190", "mrqa_squad-validation-8233", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8557", "mrqa_squad-validation-8596", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8900", "mrqa_squad-validation-8905", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9176", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9345", "mrqa_squad-validation-9351", "mrqa_squad-validation-9371", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-958", "mrqa_squad-validation-9614", "mrqa_squad-validation-9619", "mrqa_squad-validation-964", "mrqa_squad-validation-9750", "mrqa_squad-validation-9761", "mrqa_squad-validation-9892", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3606", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-708", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7742"], "OKR": 0.841796875, "KG": 0.4703125, "before_eval_results": {"predictions": ["keratinocytes and macrophages", "Roone Arledge", "Muslims in the semu class", "The John W. Weeks Bridge", "9th", "inside hospitals and clinics", "US$3 per barrel", "Trajan's Column", "the Mascarene Plateau", "virgin Anderson Lee", "Golda Meir", "xerophyte", "anions", "Uranus", "King George III", "Mike Danger", "Iolani Palace", "Gandalf", "Mungo Park", "squash", "Bill Pertwee", "magnetite", "Sam Mendes", "R\u00edo del Norte", "Emeril Lagasse", "\"Shine\"", "Karl Marx", "an ornamental figure or illustration", "four and a half hours", "norway", "Jamaica", "Skylab", "Sydney", "Peter Purves", "Boreas", "Baffin Island", "Dumbo", "Thackeray", "Botany Bay", "Peterborough United", "FC Porto", "albedo", "11", "Washington State", "red", "remnants of very massive stars with gravity", "Groucho Marx", "Andrew Nicholson", "King Edward VII", "Algeria", "the House of Bourbon", "Barry White", "gin", "Michael Biehn", "1966", "guitar feedback", "The LA Galaxy", "Stephen Johns", "Veracruz, Mexico", "tantalus", "Simon & Garfunkel", "Alan Graham", "2009", "Robert Kimmitt"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6611979166666666}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6567", "mrqa_squad-validation-3635", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-371", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-922", "mrqa_naturalquestions-validation-1008", "mrqa_newsqa-validation-2939", "mrqa_newsqa-validation-3091"], "SR": 0.59375, "CSR": 0.5751488095238095, "EFR": 1.0, "Overall": 0.7258891369047619}, {"timecode": 21, "before_eval_results": {"predictions": ["1349", "the center of mass", "July 23, 1963", "very rare", "James E. Webb", "eight", "the Panic of 1901", "enterprise application development", "February 6, 2005", "the 1950s", "159", "an Easter egg", "Andhra Pradesh and Odisha", "1975", "John Vincent Calipari", "winter", "King", "Robert Hooke", "rocks and minerals", "October 30, 2017", "to avoid the inconvenienceiences of a pure barter system", "four", "spain", "the Arab World", "in the pachytene stage of prophase I of meiosis", "Baltimore -- Washington metropolitan area", "Lagaan", "Hank J. Deutschendorf II", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "Dan Stevens", "moral", "May 19, 2008", "Albert Einstein", "May 26, 2017", "1992", "restored to life", "Master Christopher Jones", "to solve its problem of lack of food self - sufficiency", "Bud '' Bergstein", "Spanish", "the bloodstream or surrounding tissue", "the church sexton Robert Newman and Captain John Pulling", "Fox Ranch in Malibu Creek State Park", "Gibraltar", "Dmitri Mendeleev", "breaking off its imports, and strike a winning below with German soldiers transferred from the Eastern front, where Russia had surrendered", "31", "the disputed 1824 presidential election", "12", "a form of business network", "for control purposes", "twelve", "Paige O'Hara", "ghee", "The Crow", "Corinna and seven-time Formula One World Champion Michael Schumacher", "micronutrient-rich", "mexican anderson", "top designers", "mexican", "gold", "blackfield Cathedral", "spain and his wife", "liver"], "metric_results": {"EM": 0.5, "QA-F1": 0.6064841671701191}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.8571428571428571, 1.0, 0.7058823529411764, 0.0, 0.0, 0.0, 0.7368421052631579, 0.8571428571428571, 0.16666666666666669, 1.0, 0.4615384615384615, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 0.7368421052631579, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-2635", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3054", "mrqa_searchqa-validation-14780", "mrqa_searchqa-validation-7212", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-4182"], "SR": 0.5, "CSR": 0.5717329545454546, "retrieved_ids": ["mrqa_squad-train-76325", "mrqa_squad-train-45593", "mrqa_squad-train-59739", "mrqa_squad-train-39717", "mrqa_squad-train-75449", "mrqa_squad-train-51190", "mrqa_squad-train-21903", "mrqa_squad-train-23948", "mrqa_squad-train-59151", "mrqa_squad-train-34104", "mrqa_squad-train-4994", "mrqa_squad-train-81634", "mrqa_squad-train-55468", "mrqa_squad-train-19568", "mrqa_squad-train-76661", "mrqa_squad-train-25231", "mrqa_hotpotqa-validation-3606", "mrqa_squad-validation-5388", "mrqa_triviaqa-validation-2523", "mrqa_searchqa-validation-2347", "mrqa_squad-validation-8295", "mrqa_searchqa-validation-6531", "mrqa_triviaqa-validation-1686", "mrqa_squad-validation-4902", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-2777", "mrqa_searchqa-validation-5916", "mrqa_triviaqa-validation-3101", "mrqa_squad-validation-10483", "mrqa_searchqa-validation-668", "mrqa_squad-validation-1064", "mrqa_hotpotqa-validation-4899"], "EFR": 0.96875, "Overall": 0.7189559659090909}, {"timecode": 22, "before_eval_results": {"predictions": ["machine labor", "an intuitive understanding", "evenly round the body", "2,869", "president of NBC's entertainment division", "the Wesel-Datteln Canal", "Melanie Griffith", "fowls", "lexicographer", "an Islamic Republic", "One Flew Over the Cuckoo's Nest", "mayo-based white sauce", "Royal Wives", "Harpers Ferry", "Confeitaria Colombo", "the Canterbury Tales", "Versailles", "Target", "meadow grasshopper", "the land they worked on", "Tom Terrific", "magnesium", "South Carolina", "The New York Times", "German Shepherd", "peanuts", "Xinjiang-Uygur Autonomous Region", "Parker House", "Damascus", "Jennies", "a logo", "Greg (TV Series 19972002)", "1906 San Francisco earthquake", "the Buonapartes", "Mother Vineyard", "Virginia Woolf", "apogee", "Cherry Garcia", "in his magic books", "Diamond Jim Brady", "an axiom", "Princeton", "Eric Knight", "Apple", "The Hills are Alive", "Pygmalion", "T. S. Eliot", "Andes", "Diamonds", "asteroids", "the Nutcracker", "7.07.9", "Labour Party", "1933", "a stuffing with onions and sometimes other vegetables, such as peas, celery or carrots, and topped with grated cheese", "Falstaff", "redheaded", "Republican", "Wojtek", "Bangor Air National Guard Base", "1995", "cancer awareness", "12-hour-plus shifts", "end her trip in Crawford"], "metric_results": {"EM": 0.5, "QA-F1": 0.5805803571428572}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, false, false, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.16666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-9255", "mrqa_searchqa-validation-9788", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-4038", "mrqa_searchqa-validation-3419", "mrqa_searchqa-validation-9368", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-9991", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-10625", "mrqa_triviaqa-validation-6403", "mrqa_hotpotqa-validation-314", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-403"], "SR": 0.5, "CSR": 0.5686141304347826, "EFR": 0.96875, "Overall": 0.7183322010869565}, {"timecode": 23, "before_eval_results": {"predictions": ["Department for Culture, Media and Sport", "Kevin Harlan", "Khongirad", "the Solim\u00f5es Basin", "seven", "the 21st century", "Mombasa, Kenya", "(T.) Biden", "eight", "Adidas", "she relied on the memory of how her mom would always convince her that she was going to be on the Olympic medals podium.", "the body of the aircraft", "serfs", "the United States", "Michigan", "in Iraq", "Two", "Russia", "the Tinkler", "$106,482,500", "Tuesday", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "misdemeanor", "three out of four", "tennis", "Toy Story", "Christmas", "90", "involved in an Internet broadband deal with a Chinese firm.", "$75", "free laundry service", "Florida", "Jeffrey Jamaleldine", "insurgent small arms fire", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "1.2 million", "Romney", "citizens", "healing and \"Penny Whistles.\"", "near Grand Ronde, Oregon", "Seasons of My Heart", "raping and murdering", "150", "Anil Kapoor", "misdemeanor assault charges", "reiterated the Vatican's policy on condom use", "an allergic reaction to peanuts", "Martin Aloysius Culhane,", "a model of sustainability", "Kenyan and Somali governments", "a long-term goal for reducing", "in a motor motorcycle accident.", "the Isthmus of Corinth", "Needtobreathe", "Old Trafford", "Shakyamuni", "Bobbi Kristina Brown", "Shayne Ward", "Christina Ricci", "Justin Bieber, Monica, Britney Spears, Usher, Keri Hilson, T.I., Nelly Furtado, Kevin Cossom, Ciara, Mariah Carey, Timbaland, Madonna", "Hong Kong", "flour", "Henry Wadsworth", "Melbourne"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5907632131800089}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.2105263157894737, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.9411764705882353, 0.4, 0.4, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4356", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2167", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-8272", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-2919", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5469", "mrqa_searchqa-validation-5398", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-12527"], "SR": 0.46875, "CSR": 0.564453125, "EFR": 0.9705882352941176, "Overall": 0.7178676470588236}, {"timecode": 24, "before_eval_results": {"predictions": ["John Harvard", "late 1886", "AKS primality test", "Command Module design, workmanship and quality control.", "Gold footballs", "1967", "Dunlop", "XVideos", "Niger\u2013Congo", "Sports Illustrated", "Robert A. Iger", "Regional League North", "2002", "Harsh Times", "Fade Out: The Calamitous Final Days of MGM", "Restoration Hardware", "Louis Silvie \"Louie\" Zamperini", "Taipei City", "Minneapolis, Minnesota", "Idisi", "American composer Ambroise Thomas", "Marcella", "May 4, 2004.", "Everything Is wrong", "Captain", "Smoothie King Center", "Martin Scorsese", "Viacom Media Networks", "1853", "imp My Ride", "Columbia Records", "Q\u0307adar A\u1e8bmat-khant Ramzan", "Derry City F.C.", "Fort Hood, Texas", "Bonny Hills", "London", "1999", "2006", "Minnesota Timberwolves", "Samuel Joel \" Zero\" Mostel", "October 13, 1980", "Chechen Republic", "House of Commons", "1926", "Nikolai Alexandrovich Morozov", "1968", "Berthold Heinrich K\u00e4mpfert", "Girl Meets World", "January 15, 1975", "Pansexuality", "Javan leopard", "2,463,431", "Paul's letter is addressed `` to the churches of Galatia '' ( Galatians 1 : 2 )", "She had been greeted with a hostile reception from the White Witch after arriving at her castle alone, and even more so after informing her that Aslan had come to Narnia.", "The 2018 Winter Olympics", "Great Britain", "Rudolph", "Chechnya", "Aryan Airlines Flight 1625", "Sen. Barack Obama", "her son has strong values.", "Jay Gillespie", "Glenda", "Mare Erythraeum"], "metric_results": {"EM": 0.53125, "QA-F1": 0.654023542574598}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8235294117647058, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.4, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.42857142857142855, 0.12903225806451613, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3930", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-1730", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-3210", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-3909", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-4818", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-1797", "mrqa_triviaqa-validation-7434", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1892", "mrqa_searchqa-validation-4914", "mrqa_searchqa-validation-8010", "mrqa_searchqa-validation-5368"], "SR": 0.53125, "CSR": 0.563125, "retrieved_ids": ["mrqa_squad-train-24520", "mrqa_squad-train-64560", "mrqa_squad-train-72294", "mrqa_squad-train-32011", "mrqa_squad-train-17756", "mrqa_squad-train-50005", "mrqa_squad-train-5217", "mrqa_squad-train-70594", "mrqa_squad-train-81981", "mrqa_squad-train-21894", "mrqa_squad-train-28069", "mrqa_squad-train-85113", "mrqa_squad-train-529", "mrqa_squad-train-76399", "mrqa_squad-train-69903", "mrqa_squad-train-55404", "mrqa_naturalquestions-validation-10625", "mrqa_hotpotqa-validation-1657", "mrqa_squad-validation-4206", "mrqa_hotpotqa-validation-4273", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-371", "mrqa_naturalquestions-validation-7468", "mrqa_hotpotqa-validation-3929", "mrqa_searchqa-validation-10063", "mrqa_naturalquestions-validation-1770", "mrqa_triviaqa-validation-5595", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-2896", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-7976"], "EFR": 0.9666666666666667, "Overall": 0.7168177083333334}, {"timecode": 25, "before_eval_results": {"predictions": ["1894", "Effective", "a pointless pursuit", "manually suppress the fire", "Northern Rail", "South Korea", "paralysis", "golf", "Romania", "Pocahontas", "Matlock", "George Washington", "Mendoza and Valparaiso", "The Blue Boy", "the book Three Worlds", "Liriope", "NUT", "Pennsylvania", "eastern Pyrenees", "Chapters I\u2013II", "Dutch", "Salem witch trials", "Gryffendor", "Sam Allardyce", "Olympics", "Nick Hancock", "Edward Yorke", "keeper of the Longstone (Fame Islands) lighthouse", "Mase", "Superman: The Movie", "Richard Walter Jenkins", "Burkina Faso", "Billy Cox", "Javier Bardem", "Independence Day", "hydrogen", "Jordan", "So Solid Crew", "Martin Reynolds", "Matthew", "albion", "(Hons)", "Ash", "Ian Botham", "squash", "Leander Club", "Sir Stirling Moss", "Charlotte's Web", "Poland", "Lingerie Football League", "Patricia", "Janna", "Authority", "1 mile ( 1.6 km )", "Steve Valentine", "Out of Control", "Virgin", "UFC Fight Pass", "the Airbus A330-200", "fill a million sandbags and place 700,000 around our city", "75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "PIE", "amelia earhart", "Final Cut"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6557291666666667}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3207", "mrqa_triviaqa-validation-2266", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-5472", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-4836", "mrqa_hotpotqa-validation-5822", "mrqa_searchqa-validation-8379"], "SR": 0.59375, "CSR": 0.5643028846153846, "EFR": 1.0, "Overall": 0.7237199519230769}, {"timecode": 26, "before_eval_results": {"predictions": ["at the narrow end", "Levi's Stadium", "specific catechism questions", "a nobleman who owned a village located at the modern-day site of Mariensztat neighbourhood", "the disk", "2016", "his influential uncle Abu Talib", "Mel Tillis", "Pangaea", "Stephen Lang", "2018", "Erika Mitchell Leonard", "eight years after an amendment increased the tenure length by two years", "Edd Kimber", "Romancing the Stone", "Orange Juice", "a photodiode", "September 9, 2010, at 8 p.m. ET", "ABC", "dromedary", "Dan Stevens", "Jackie Van Beek", "Grey Wardens", "1979", "October 27, 2016", "authority", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "Luther Ingram", "Jodie Foster", "Barry Watson", "Sanchez Navarro", "excessive growth", "1936", "British Columbia, Canada", "New York University", "2007", "the first instalment in the long - running Harry Potter film series", "Washington Redskins", "the books of Exodus and Deuteronomy", "September 14, 2008", "the Vital Records Office of the states, capital district, territories and former territories", "Pasek & Paul", "the Chicago metropolitan area", "conquistador Francisco Pizarro", "the 1930s", "the Germanic elements `` hrod '' meaning renown and `` beraht '' meaning bright", "The Rose", "John Smith", "The eighth and final season", "1623", "neutrality", "he cheated on Miley", "a stringed musical instrument", "anabaptists", "bing.com", "Taylor Swift", "jenkins", "Michael Crawford", "$22 million", "five days.", "flooding", "pisco", "david", "business"], "metric_results": {"EM": 0.390625, "QA-F1": 0.48374003957643663}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, false, false, true, false, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.16666666666666669, 0.125, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.35294117647058826, 0.33333333333333337, 1.0, 0.2666666666666667, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.33333333333333337, 1.0, 0.625, 1.0, 1.0, 0.8, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.2, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-869", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-6131", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-2872", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7563", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-5461"], "SR": 0.390625, "CSR": 0.5578703703703703, "EFR": 0.9487179487179487, "Overall": 0.7121770388176638}, {"timecode": 27, "before_eval_results": {"predictions": ["multiple revisions", "time and storage", "BAFTA Television Award for Best Actor", "278", "the all-day event", "2007", "Larry Richard Drake", "she was the first to recognise the full potential of a \"computing machine\"", "London", "currently Ron Kouchi", "Hanford Nuclear Reservation", "Native American", "Mindy Kaling", "Ricardo \u201cEl Finito\u201d L\u00f3pez Nava", "Blackstone", "Ginger Rogers", "\"U.S. Marshals\"", "churros", "Christies Beach", "eastern", "Arsenal F.C.", "Don Bluth and Gary Goldman", "torpedo boats and later submarines", "1969 until 1974", "skiing and mountaineering", "June 11, 1973", "John Raymond Kavanagh", "Protestant Christian", "defender", "Henry J. Kaiser", "Saoirse Ronan", "122,067", "Wandsworth, London", "YouTube", "Daniel Andre Sturridge", "USS Essex (CV-9)", "Ron Cowen and Daniel Lipman", "(born at Cairnburgh Castle in the Scottish Highlands and baptised on 4 May 1759 \u2013 died on 25 June 1857 in London", "Captain while retaining the substantive rank of Commodore", "Giuseppe Verdi", "Andrzej Go\u0142ota and Tomasz Adamek", "Russell T Davies", "Geraldine Sue Page", "Manchester", "3,000", "Albert II, Prince of Monaco, Umberto II", "Minnesota to the west, and Wisconsin and the Upper Peninsula of Michigan", "Stravinsky's \"The Rite of Spring\"", "saloon-keeper and Justice of the Peace", "John Lennon", "Ernest Hemingway", "Masahiko Takehita", "Mary Rose Foster", "11 February 2012", "the pitches used may change and introduce a different scale", "fortieth", "Australia", "language-dominant area of the brain", "the Cambodian government used a map drawn during the French occupation of Cambodia", "Amanda Knox's aunt", "there could be 100,000 snakes in the Everglades, but no one knows for sure.", "brandy", "The Beatles", "North Carolina"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5283042478354978}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 0.08333333333333333, 1.0, 1.0, 0.5714285714285715, 1.0, 0.8, 1.0, 1.0, 0.4444444444444445, 0.9090909090909091, 0.0, 0.33333333333333337, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.14285714285714288, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1672", "mrqa_squad-validation-7819", "mrqa_squad-validation-1546", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1559", "mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-2335", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-1237", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-4497", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-2192", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3907", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-9122"], "SR": 0.40625, "CSR": 0.5524553571428572, "retrieved_ids": ["mrqa_squad-train-83536", "mrqa_squad-train-71119", "mrqa_squad-train-33289", "mrqa_squad-train-7392", "mrqa_squad-train-35393", "mrqa_squad-train-77759", "mrqa_squad-train-60946", "mrqa_squad-train-86577", "mrqa_squad-train-12222", "mrqa_squad-train-70129", "mrqa_squad-train-76649", "mrqa_squad-train-66684", "mrqa_squad-train-71904", "mrqa_squad-train-5500", "mrqa_squad-train-55314", "mrqa_squad-train-63686", "mrqa_squad-validation-3373", "mrqa_hotpotqa-validation-1730", "mrqa_squad-validation-8105", "mrqa_searchqa-validation-11392", "mrqa_triviaqa-validation-3876", "mrqa_squad-validation-10466", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-2408", "mrqa_naturalquestions-validation-5838", "mrqa_searchqa-validation-8411", "mrqa_squad-validation-3207", "mrqa_searchqa-validation-1565", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-376", "mrqa_naturalquestions-validation-3663", "mrqa_squad-validation-9029"], "EFR": 1.0, "Overall": 0.7213504464285714}, {"timecode": 28, "before_eval_results": {"predictions": ["left foot", "USSR", "common flagellated", "Henry", "Adidas", "Hillary Clinton", "billions of dollars", "one", "the Beatles", "Communist Party of Nepal (Unified Marxist-Leninist)", "Pope Benedict XVI", "around 8 p.m. local time Thursday", "Sri Lanka's Tamil rebels", "64", "CNN", "at least 12 months", "A witness", "Adriano", "he would actually go along with this program because it was the only way that this whole thing would be sorted out.", "183", "American Civil Liberties Union", "deployment of unmanned drones, including possibly the Predator drones used in Iraq and Afghanistan.", "40 militants and six Pakistan soldiers dead", "Aldgate East", "137", "54-year-old", "Government Accountability Office report", "Jacob", "South Africa", "on the Ohio River near Warsaw, Kentucky", "4,000", "Oaxaca, Mexico", "provided Syria and Iraq 500 cubic meters of water", "the Catholic League", "August 19, 2007", "10 years", "all three pleaded not guilty", "Japan's fisheries agency", "she is God-sent", "consumer confidence", "he was mad at the U.S. military because of what they had done to Muslims", "six", "nearly 28 years", "July 18, 1994", "Dan Brown", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "Khalid Sheikh Mohammed, seen in a December sketch, was waterboarded 183 times in a month,", "Chao Phraya River", "two", "an antihistamine and an epinephrine auto-injector", "more than 4,000", "he lost his virginity at age 14", "Jean F Kernel", "10 : 30am", "Johannes Gutenberg", "tide-wise", "Christian Wulff", "Laurent Planchon", "general secretary", "the George Washington Bridge", "Highlands Course", "vast wasteland", "Aristotle's lantern", "tuna"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6621669076478606}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5454545454545454, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5, 1.0, 0.25, 0.9473684210526316, 1.0, 1.0, 0.6666666666666666, 0.888888888888889, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 0.0, 0.5, 0.3870967741935484, 0.060606060606060615, 0.0, 1.0, 1.0, 0.5, 0.2, 0.4, 0.8, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1314", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-3530", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-9007", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-435", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-12506", "mrqa_searchqa-validation-4780"], "SR": 0.46875, "CSR": 0.5495689655172413, "EFR": 1.0, "Overall": 0.7207731681034482}, {"timecode": 29, "before_eval_results": {"predictions": ["Cadeby", "Near Sankt Goarshausen", "10,006,721", "anchovy", "lovebirds", "Chicago", "monk seal", "Wilhelm", "requerre", "Take Me Out to the Ballgame", "music", "\"What hath God wrought\"", "Stewart Island", "St. Erasmus", "a knife", "Henry Holt and Company", "cow", "illegible", "Scrabble", "Mussolini", "Valkyries", "rain", "shank", "Jodie Foster", "Elysium", "Five Easy pieces", "Thomas Edison", "Manhattan Project", "Charles I", "divorce", "Enchanted", "Liberty Bell", "USB", "Autobahn", "Destiny's Child", "Byron's", "a spoonful", "Prednisone & Prednisolone", "Margot Fonteyn", "eel", "\"McMillan and wife\"", "C.B. Blethen", "professor", "Galileo Galilei", "Existentialism", "John Donne", "Beijing", "Annies", "murder", "Wallis Warfield Simpson", "a queen", "synaptic vesicles", "Vatican City", "James W. Marshall at Sutter's Mill in Coloma, California", "a single, implicitly structured data item", "South Korea", "\"Slow\"", "M*A*S*H", "Boeing EA-18G Growler", "5249", "Fleetwood Mac", "around 3.5 percent of global greenhouse emissions.", "The e-mails", "papillomavirus"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6978219696969696}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.7272727272727272, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.5, 0.8333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-10014", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-8042", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-5788", "mrqa_searchqa-validation-14749", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-11420", "mrqa_searchqa-validation-5715", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-9472", "mrqa_searchqa-validation-15174", "mrqa_searchqa-validation-10889", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-2956", "mrqa_triviaqa-validation-935", "mrqa_newsqa-validation-1372"], "SR": 0.59375, "CSR": 0.5510416666666667, "EFR": 1.0, "Overall": 0.7210677083333332}, {"timecode": 30, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2020", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3206", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5677", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1707", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9311", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2939", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-668", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11465", "mrqa_searchqa-validation-12078", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14468", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-2040", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5368", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7233", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-915", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10008", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-10251", "mrqa_squad-validation-10279", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10427", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-121", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1449", "mrqa_squad-validation-1461", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1856", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2194", "mrqa_squad-validation-2318", "mrqa_squad-validation-2434", "mrqa_squad-validation-2506", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2768", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3223", "mrqa_squad-validation-3326", "mrqa_squad-validation-3363", "mrqa_squad-validation-3456", "mrqa_squad-validation-3497", "mrqa_squad-validation-354", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3930", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4226", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4343", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4621", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4902", "mrqa_squad-validation-4965", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5118", "mrqa_squad-validation-5242", "mrqa_squad-validation-525", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5389", "mrqa_squad-validation-5590", "mrqa_squad-validation-5605", "mrqa_squad-validation-5624", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5865", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5973", "mrqa_squad-validation-6025", "mrqa_squad-validation-6181", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-629", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6467", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6921", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7040", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8233", "mrqa_squad-validation-8295", "mrqa_squad-validation-8452", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8596", "mrqa_squad-validation-89", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-9614", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9920", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1382", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-2919", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-922"], "OKR": 0.857421875, "KG": 0.49453125, "before_eval_results": {"predictions": ["before episode two", "1892", "motivated students", "Nikolai Trubetzkoy", "June 1925", "\"bushwhackers\"", "British", "Argentina", "Baudot code", "Jacksonville", "DTM", "Switzerland", "Accokeek, Maryland", "The State of Franklin", "John Ford", "Operation Watchtower", "34.9 kilometres from Adelaide station", "1 December 1948", "omnisexuality", "Westfield Tea Tree Plaza", "southwest Denver, Colorado near Bear Creek", "Atlanta, Georgia", "Atlanta Braves", "Scunthorpe", "2004", "Donald McNichol Sutherland", "Towards the Sun", "the heart of the southern (Dolomitic) Alps in the Veneto region of Northern Italy", "Angus", "An impresario", "Sufism", "January 30, 1930", "Sulla", "Female Socceroos", "Jaguar Land Rover and General Motors", "tempo", "Milk Barn Animation", "McLaren-Honda", "Timothy Dowling", "London", "Jane", "Tim Burton", "Otto Hahn", "AMC", "three", "Robert Paul \"Robbie\" Gould III", "Eddie Collins", "Jude", "twenty-three", "Gararish", "Haricharan and Shreya Ghoshal", "Whoopi Goldberg", "September 8, 2017", "volcanic activity", "Burbank, California", "horse stories of all time", "Heisenberg", "the Kiel Canal", "contract talks just before midnight,", "Eintracht Frankfurt", "Republican", "a poodle", "Nickelback", "Will & Grace"], "metric_results": {"EM": 0.5, "QA-F1": 0.6169542696886448}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.625, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.28571428571428575, 0.3333333333333333, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.33333333333333337, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7744", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1311", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-1128", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-4487", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-4389", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-5187", "mrqa_naturalquestions-validation-6012", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-5208", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2032"], "SR": 0.5, "CSR": 0.5493951612903225, "retrieved_ids": ["mrqa_squad-train-46812", "mrqa_squad-train-51808", "mrqa_squad-train-59945", "mrqa_squad-train-74026", "mrqa_squad-train-84642", "mrqa_squad-train-81961", "mrqa_squad-train-50577", "mrqa_squad-train-79112", "mrqa_squad-train-76409", "mrqa_squad-train-78812", "mrqa_squad-train-11952", "mrqa_squad-train-6664", "mrqa_squad-train-31271", "mrqa_squad-train-52246", "mrqa_squad-train-53800", "mrqa_squad-train-48030", "mrqa_searchqa-validation-15174", "mrqa_triviaqa-validation-7707", "mrqa_hotpotqa-validation-3929", "mrqa_searchqa-validation-5814", "mrqa_naturalquestions-validation-1325", "mrqa_squad-validation-8553", "mrqa_searchqa-validation-5920", "mrqa_newsqa-validation-3530", "mrqa_triviaqa-validation-695", "mrqa_searchqa-validation-10014", "mrqa_squad-validation-805", "mrqa_triviaqa-validation-2266", "mrqa_squad-validation-1504", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-1948"], "EFR": 1.0, "Overall": 0.7298790322580645}, {"timecode": 31, "before_eval_results": {"predictions": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "500", "7.63\u00d725mm Mauser", "the Harpe brothers", "French", "1944", "Clara Petacci", "2002", "Marko Tapani \" Marco\" Hietala", "Shankar", "Cody Miller", "\"The Manhunter from Mars\" in \" Detective Comics\" #225 (Nov. 1955)", "film and short novels", "Carson City", "The Nick Cannon Show", "\"Mickey's Christmas Carol\"", "ten", "Bergen County", "the 1824 Constitution of Mexico", "Hellenism", "Tomorrowland", "Jaffrey", "Frederick Alexander Lindemann", "Rawhide", "astronomer and composer of German and Czech-Jewish origin", "Don DeLillo", "The Seduction of Hillary Rodham", "Balloon Street, Manchester", "9,984", "the Rose Garden", "Spain", "Deep Purple", "Abdul Razzak Yaqoob", "Port Macquarie", "Dan Castellaneta", "Roseann O'Donnell", "Saturday", "Taylor Alison Swift", "Miller Brewing", "State Children's Health Insurance Program", "Indianapolis Motor Speedway", "Nevada", "Tampa Bay Storm", "Jango Fett", "High Court of Admiralty", "\"An All-Colored Vaudeville Show\"", "German", "Lucy Muringo Gichuhi", "Valley Falls", "dice", "Nicholas \" Nick\" Offerman", "Jewish", "JackScanlon", "Leonard Bernstein", "19", "France", "carbonic acid", "secretary", "eight", "building a nuclear weapon", "2005", "Beastie Boys", "Madison", "a Cobra"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6971117424242423}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, true, true, false, true, false, false, false, false, true, false, true, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.1818181818181818, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-728", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-3737", "mrqa_triviaqa-validation-1534", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-725", "mrqa_newsqa-validation-3106", "mrqa_searchqa-validation-10363"], "SR": 0.5625, "CSR": 0.5498046875, "EFR": 0.9642857142857143, "Overall": 0.7228180803571429}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh Downs", "education", "Till Death Us Do Part", "Laputa", "Leeds", "Colonel Tom Parker", "LSD", "Stephen of Blois", "Albania", "Tombstone", "Travis", "Jaguar Land Rover", "Diego Maradona", "Sudan", "Bubba", "football", "a multi-user real-time virtual world described entirely in text", "fondu", "Greece", "13 murders, some of whom were innocent people, killed during one of Clyde's many bungled robberies.", "Steve Coogan", "Elektra King", "Boston Marathon", "Carl Smith", "Humble pie", "Jorge Lorenzo", "Rescue Aid Society", "chess", "Les Dawson", "Arthur, Prince of Wales", "Grail", "Ronald Reagan", "Barry Copeland", "climate", "at the Coney Island Old Island Pier", "Hammer", "liver", "Guildford Dudley", "Amoco Cadiz", "John Howard", "Hammer", "\"His Holiness\"", "frets", "Cornell University", "Flybe", "The Altamont Speedway Free Festival", "a fat like oil or lard", "The Lost Weekend", "Stockholm", "Switzerland", "taekwondo", "tomato", "senior-most judge of the supreme court", "early Christians of Mesopotamia", "Representatives are not restricted to voting for one of the nominated candidates and may vote for any person, even for someone who is not a member of the House at all", "Machine Gun Kelly", "Central Avenue", "middleweight", "Jacob Zuma", "digging ditches.", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Canterbury", "Harold Macmillan", "marsh"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6424156409168081}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19354838709677416, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-3007", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-1179", "mrqa_triviaqa-validation-3813", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-7163", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-2217", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-2330", "mrqa_naturalquestions-validation-3569", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1023", "mrqa_newsqa-validation-3089", "mrqa_searchqa-validation-6833"], "SR": 0.609375, "CSR": 0.5516098484848485, "EFR": 1.0, "Overall": 0.7303219696969697}, {"timecode": 33, "before_eval_results": {"predictions": ["Brown v. Board of Education of Topeka", "after midnight one night", "its", "Ennis", "on websites on the 24th.", "tide", "Jaime Andrade", "children ages 3 to 17", "girls", "possible victims of physical and sexual abuse.", "liberto", "gasoline", "stephen", "The plane, an Airbus A320-214", "two", "ice jam", "mmorpgs", "abduction of minors.", "vivian liberto", "j. Crew", "vivian liberto", "Florida", "Dhaka", "T.I.", "Pew Research Center", "Nirvana", "doctors", "vivian liberto", "race or its understanding of what the law required it to do.", "unparalleled fundraising and an overwhelming ground game.", "between June 20 and July 20", "paul fidler", "misdemeanor", "1.2 million", "100,000", "vivian diplomat", "crossfire by insurgent small arms fire,", "vivian liberto", "Noriko Savoie was given custody of the children and agreed to remain in the United States.", "a \"new chapter\" of improved governance in Afghanistan", "vivian liberto", "outside the municipal building of Abu Ghraib in western Baghdad", "shelling of the compound", "in the clubs of Hollywood", "Atlantic Ocean", "Majid Movahedi", "Nepal", "Jiverly Wong", "uriah Stiles, 38, faces 22 felony counts in connection with the videotape,", "Carrousel du Louvre", "September 21", "grayback", "oxygen", "Yongzheng Emperor", "Narendra Modi", "stephen Hendry", "74", "vivian liberto", "musical research", "Randall Boggs", "Mick Jackson", "West Virginia", "Gary Oldman", "Paris"], "metric_results": {"EM": 0.390625, "QA-F1": 0.43656994047619047}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.34285714285714286, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.21428571428571427, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1626", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2086", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4062", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9569", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-7079", "mrqa_hotpotqa-validation-4112", "mrqa_searchqa-validation-44"], "SR": 0.390625, "CSR": 0.546875, "retrieved_ids": ["mrqa_squad-train-77107", "mrqa_squad-train-57887", "mrqa_squad-train-7725", "mrqa_squad-train-53194", "mrqa_squad-train-75124", "mrqa_squad-train-15678", "mrqa_squad-train-72055", "mrqa_squad-train-66760", "mrqa_squad-train-23706", "mrqa_squad-train-10422", "mrqa_squad-train-52776", "mrqa_squad-train-35075", "mrqa_squad-train-27952", "mrqa_squad-train-15195", "mrqa_squad-train-23890", "mrqa_squad-train-65958", "mrqa_newsqa-validation-725", "mrqa_squad-validation-805", "mrqa_newsqa-validation-892", "mrqa_searchqa-validation-9389", "mrqa_searchqa-validation-145", "mrqa_triviaqa-validation-1360", "mrqa_newsqa-validation-2032", "mrqa_searchqa-validation-2115", "mrqa_triviaqa-validation-7470", "mrqa_searchqa-validation-7828", "mrqa_triviaqa-validation-3948", "mrqa_searchqa-validation-8010", "mrqa_searchqa-validation-12968", "mrqa_naturalquestions-validation-6637", "mrqa_newsqa-validation-3264", "mrqa_searchqa-validation-2674"], "EFR": 1.0, "Overall": 0.729375}, {"timecode": 34, "before_eval_results": {"predictions": ["most common", "boudins", "Robert A. Heinlein", "Mumbai", "Indiana", "zoology", "\"Billie Jean\"", "Laos", "Peter Davison", "Westminster Abbey", "Battle of Agincourt", "white spirit", "King George III", "Kent", "railway station", "Diptera", "a turkey", "transuranic", "Harold Shipman", "River Wyre", "Carson City", "All Things Must Pass", "Hong Kong", "Mercury", "Doctor Who", "North Yorkshire", "George Blake", "Nirvana", "Janis Joplin", "Kenya", "Manchester City", "Moscow", "Caracas", "oil of Olay", "hair and fur", "Decoupage", "Bathsheba", "Ennio Morricone", "DitaVon Teese", "support assembly", "Republican", "Argentina", "French", "dennis taylor", "the internal kidney structures", "britia Petite", "Rocky Marciano", "Benedictine Order", "vivian liberto", "Hilda", "John Uhler", "four", "1982", "2018", "a lightning strike", "Danny Glover", "Trey Parker and Matt Stone", "140 to 219", "Hundreds", "Democrats", "31 meters (102 feet)", "Sir Lancelot", "Sacramento", "Hawaii"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6932291666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5333333333333333, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-3979", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-2474", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-798", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-7774", "mrqa_triviaqa-validation-4133", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-3995", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-10490", "mrqa_hotpotqa-validation-398", "mrqa_newsqa-validation-3976", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-3920"], "SR": 0.640625, "CSR": 0.5495535714285714, "EFR": 0.9130434782608695, "Overall": 0.7125194099378882}, {"timecode": 35, "before_eval_results": {"predictions": ["late 1970s", "Aristotle", "daiquiri", "calvary", "armadillos", "joe mercer", "Jewels", "Absalom", "joe joe johns", "The Goonies", "flag", "Quito", "Seine", "aperitif", "Jennifer Yamamoto", "bites a dog", "\"The Star-Spangled Banner\"", "The Rolling Stones", "London", "a knight", "(1706-1790)", "dennis taylor", "a joe", "Apollo 11", "Spain", "Cadillac", "Matt Damon", "great American Novel", "shalom", "white", "balfour", "law", "Easton", "Scrabble", "Iceland", "mama mercer", "an incubation chamber", "joe mercer", "Stephen Vincent Bent", "Brooke Ellen Bollea", "dog", "Nancy Sinatra", "David", "vinifera", "Robert Lowell", "ACTIVE", "Richmond", "donations", "Amy Tan", "Florence", "Pandora", "Grenada", "the Mahalangur Himal sub-range of the Himalayas", "Kusha", "`` Heroes and Villains ''", "joe vivian liberto", "emerald", "1", "2015", "October 20, 2017", "Columbus", "Gustav's top winds weakened to 110 mph,", "piedad Cordoba,", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6095486111111111}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, false, false, true, false, false, true, false, true, true, false, false, false, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-1076", "mrqa_searchqa-validation-1187", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-2248", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-9559", "mrqa_searchqa-validation-245", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-11471", "mrqa_searchqa-validation-15863", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-3703", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-559", "mrqa_searchqa-validation-9192", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-6212", "mrqa_newsqa-validation-2307"], "SR": 0.578125, "CSR": 0.5503472222222222, "EFR": 1.0, "Overall": 0.7300694444444444}, {"timecode": 36, "before_eval_results": {"predictions": ["1082", "Pfc. Bowe Bergdahl,", "\"It didn't matter if you were 60, 40 or 20 like I am.", "a Columbian mammoth", "Symbionese Liberation Army", "a steam-driven, paddlewheeled overnight passenger boat.", "a mechanism at the federal level to ensure that drivers comply.", "Tim Clark, Matt Kuchar and Bubba Watson", "a long-range missile", "75", "prisoners", "women", "CNN/Opinion Research Corporation", "Kingdom City", "CEO of an engineering and construction company with a vast personal fortune", "Ku Klux Klan", "Felipe Calderon", "137", "3-3", "\"Dancing With the Stars\"", "love and loss", "Michael Jackson", "\"a striking blow to due process and the rule of law", "Venezuela", "their business books were being handled.", "the Nazi war crimes suspect who had been ordered deported to Germany,", "a number of calls,", "Mandi Hamlin", "Iraq", "Janet Napolitano", "Russian concerns that the defensive shield could be used for offensive aims.", "Bob Dole", "they can demonstrate they have been satisfactorily treated for at least 12 months.", "Tennessee", "\"Dance Your Ass Off\"", "Malawi", "246", "skull", "six", "people of Palestine", "eight in 10", "one-shot victory in the Bob Hope Classic", "in the Muslim north of Sudan", "37", "Clifford Harris,", "Bea Arthur,", "Susan Boyle", "Colorado", "UNICEF", "united States, NATO member states, Russia and India", "27-year-old", "11 %", "(Carrie) Wakefield", "September 4, 2000", "One Direction", "Runcorn", "oxygen", "Ben R. Guttery", "Preston", "from 1993 to 1996", "Ecuador", "Halloween", "Gregor Mendel", "Dick & Jane"], "metric_results": {"EM": 0.5, "QA-F1": 0.6456217903828199}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.1, 0.0, 1.0, 0.0, 0.47058823529411764, 1.0, 0.6666666666666666, 0.6666666666666666, 0.2857142857142857, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.888888888888889, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1755", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3380", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3069", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5564", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10010"], "SR": 0.5, "CSR": 0.5489864864864865, "retrieved_ids": ["mrqa_squad-train-16621", "mrqa_squad-train-47714", "mrqa_squad-train-57278", "mrqa_squad-train-12072", "mrqa_squad-train-43", "mrqa_squad-train-86579", "mrqa_squad-train-80867", "mrqa_squad-train-62159", "mrqa_squad-train-8681", "mrqa_squad-train-11304", "mrqa_squad-train-27237", "mrqa_squad-train-85118", "mrqa_squad-train-17466", "mrqa_squad-train-17660", "mrqa_squad-train-14518", "mrqa_squad-train-1219", "mrqa_naturalquestions-validation-9809", "mrqa_searchqa-validation-9679", "mrqa_newsqa-validation-1114", "mrqa_squad-validation-4902", "mrqa_searchqa-validation-4780", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-858", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-14514", "mrqa_squad-validation-3930", "mrqa_naturalquestions-validation-4222", "mrqa_searchqa-validation-7828", "mrqa_hotpotqa-validation-2113", "mrqa_triviaqa-validation-5135"], "EFR": 0.96875, "Overall": 0.7235472972972973}, {"timecode": 37, "before_eval_results": {"predictions": ["$20 billion", "the Veneto region of Northern Italy", "Preston, Lancashire, UK", "Daniel Auteuil", "George Orwell", "Eric Allan Kramer", "eight", "Kathryn Bigelow", "George Orwell", "Ben Ainslie", "1905", "Sex Drive", "Yoruba", "Archbishop of Canterbury", "brother-in-law", "Chrysler", "Portal", "chronological collection of critical quotations", "Salim Stoudamire", "fifth", "one", "Evey", "O", "The Grandmaster", "Scotland", "1980", "half of the Nobel Prize in Physics", "Russian Empire", "Cold Spring", "Hilary Duff", "Ogallala", "October 21, 2016", "fifth", "Everything Is wrong", "Massapequa", "1988", "Dan Brandon Bilzerian", "Ny-\u00c5lesund", "1967", "commercial", "Andrea Maffei", "band director", "1875", "$10\u201320 million", "Mandarin", "Uncle Fester,", "March", "The Frog Prince", "Esp\u00edrito Santo", "Los Angeles", "The New Yorker", "Walter Egan", "-- namely, Paul McCartney's over- assertiveness and criticism of his guitar playing, John Lennon's lack of engagement with the project", "Confederate", "Alison Krauss", "Graham Henry", "earwax", "mental health and recovery.", "the Bronx", "billions of dollars", "Diamond", "Simon Legree", "Sideways", "Ode"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6200231481481481}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.14814814814814814, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-365", "mrqa_hotpotqa-validation-459", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-3402", "mrqa_naturalquestions-validation-4148", "mrqa_searchqa-validation-12752"], "SR": 0.546875, "CSR": 0.5489309210526316, "EFR": 1.0, "Overall": 0.7297861842105264}, {"timecode": 38, "before_eval_results": {"predictions": ["Turing machines", "Nepal", "Wang Chung", "Panama", "a gastropod shell", "Thailand", "Abraham Lincoln", "Alex Hamilton", "Georgie Porgie", "Mork & Mindy", "Catherine de Medici", "dressage", "Benito Mussolini", "Southern California", "Fort Leavenworth", "INXS", "\"Longitudes and Attitudes: The World is Flat\"", "wildebeest", "Extra-Terrestrial Intelligence", "Arthur", "Pablo Picasso", "Clara Barton", "Nine to Five", "snakes", "moose", "Winnipeg", "Nicaragua", "Arthur Miller", "Princess Margaret, Countess of Snowdon", "1937", "algae", "feminism", "San Diego's House of Blues", "the gallbladder", "the Good Earth", "midway", "Liechtenstein", "Custer", "Mount Gilead", "salt", "Gloria Steinem", "Catherine de Medici", "Tonga", "Minos", "Gulliver's Travels", "rum", "Sea World", "Coup de grce", "Tyra Banks", "Richard Gephardt", "Bucharest", "Manley", "synthesizing vitamin B and vitamin K as well as metabolizing bile acids, sterols, and xenobiotics", "attack on Pearl Harbor", "negative", "inch", "Polish", "Province of Canterbury", "Lowndes County", "Northern Rhodesia", "his son-in-law Cleve Landsberg", "Goa", "stuck to with remarkably little internal drama. He won it after facing various challenges and turning them to", "4 other states"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6764026089159068}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.7659574468085107, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-5765", "mrqa_searchqa-validation-1647", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15746", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-15378", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-13649", "mrqa_searchqa-validation-11425", "mrqa_triviaqa-validation-1183", "mrqa_hotpotqa-validation-4989", "mrqa_hotpotqa-validation-4053", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-3579"], "SR": 0.578125, "CSR": 0.5496794871794872, "EFR": 1.0, "Overall": 0.7299358974358975}, {"timecode": 39, "before_eval_results": {"predictions": ["18", "Nalini Negi", "florida", "The 1980 Summer Olympics", "the IB Diploma Program and the IB Career - related Program for students aged 15 to 18", "the medulla oblongata", "Andreas Vesalius", "`` The Crossing ''", "Nicole DuPort", "Angus Young", "Palmer Williams Jr.", "After World War I", "studies that examine epidemiology and the long - term effects of nutrition", "Michigan State Spartans", "Wake County", "60", "RMS Titanic", "Sally Field", "Elizabeth Dean Lail", "Ravi Shastri", "chili con carne", "6 March 1983", "Amanda Leighton", "James Arthur", "James Watson and Francis Crick", "Arctic Ocean", "during the American Civil War", "Thomas Middleitch", "secession", "Sir Ernest Rutherford", "Buddhist", "After the Reform Act of 1832", "parthenogenesis", "on the two tablets", "Buffalo Bill", "$19.8 trillion", "Sleeping with the Past", "boy or girl", "1820s", "Chernobyl Nuclear Power Plant", "Vienna", "Dmitri Mendeleev", "Dalveer Bhandari", "at standard temperature and pressure", "John Ernest Crawford", "2013", "Cathy Dennis and Rob Davis", "1924", "Americans", "`` central '' or `` middle ''", "Sedimentary rock", "Carmen", "a waterfowl", "glass", "Rikki Farr's", "Israeli Declaration of Independence", "two Nobel Peace Prizes", "18", "2002", "in the Gaslight Theater", "Dragnet", "Depeche Mode", "Louis Rukeyser", "Matt Groening"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6718191964285714}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, false, false, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.7499999999999999, 1.0, 0.33333333333333337, 0.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.5, 1.0, 1.0, 1.0, 0.125, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.8, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8118", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1699", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-5624", "mrqa_triviaqa-validation-590", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-269", "mrqa_searchqa-validation-1911"], "SR": 0.578125, "CSR": 0.550390625, "retrieved_ids": ["mrqa_squad-train-52378", "mrqa_squad-train-51395", "mrqa_squad-train-61942", "mrqa_squad-train-29670", "mrqa_squad-train-35468", "mrqa_squad-train-45538", "mrqa_squad-train-74194", "mrqa_squad-train-55026", "mrqa_squad-train-69629", "mrqa_squad-train-60024", "mrqa_squad-train-32609", "mrqa_squad-train-51997", "mrqa_squad-train-26659", "mrqa_squad-train-73097", "mrqa_squad-train-71090", "mrqa_squad-train-35591", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8010", "mrqa_squad-validation-1880", "mrqa_naturalquestions-validation-1694", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-2507", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-5978", "mrqa_hotpotqa-validation-1237", "mrqa_searchqa-validation-15581", "mrqa_hotpotqa-validation-5835", "mrqa_newsqa-validation-1898", "mrqa_hotpotqa-validation-1159", "mrqa_searchqa-validation-16016", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-7468"], "EFR": 0.9629629629629629, "Overall": 0.7226707175925926}, {"timecode": 40, "UKR": 0.734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2240", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3867", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3988", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-855", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1236", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7039", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3028", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4018", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-779", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-1190", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3249", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9559", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1071", "mrqa_squad-validation-1088", "mrqa_squad-validation-1138", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1808", "mrqa_squad-validation-1814", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2609", "mrqa_squad-validation-2888", "mrqa_squad-validation-3086", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-3415", "mrqa_squad-validation-350", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4162", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-434", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4783", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5001", "mrqa_squad-validation-5098", "mrqa_squad-validation-5167", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5318", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5889", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6511", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-7961", "mrqa_squad-validation-806", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8602", "mrqa_squad-validation-8647", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8910", "mrqa_squad-validation-8910", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1336", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3488", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4397", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-7415", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-822"], "OKR": 0.86328125, "KG": 0.47890625, "before_eval_results": {"predictions": ["architect or engineer", "Naples", "dengue fever", "President Jefferson", "Rubik's Cube", "a kettledrum", "Lemon Meringue pie", "\"No hostage will be released until all our demands are met,\"", "an axe", "Department of Justice", "Jimmy Doolittle", "John Brown", "FDR", "One Hundred Years of Solitude", "Trotsky", "Aziraphale", "Jeeves & Wooster", "Corsica", "litho", "Winston McRae", "Popcorn", "Madonna", "welterweight", "yoyo", "Winston-Salem", "\"There Is Nothin' Like A Dame\"", "Edinburgh, Scotland", "spirochaete", "defensive", "Colorado columbine", "Italy", "kwanza", "Woody Guthrie", "Nigeria", "William Jennings Bryan", "the Spiderwick Chronicles", "a petition", "Chicago", "the Great Pyramid", "Herod", "Alaska", "ever", "Asia", "anaphylaxis", "\"Wendy's Story\"", "Kuwait", "the rd in 3 and the th", "Nathanael West", "diamond", "Charlie Sheen", "The Call of the Wild", "Gibraltar", "Cleveland Indians", "1923", "Bahrain", "El Hiero", "Hans Lippershey", "\"Sippin' on Some Sizzurp,\"", "Larry Eustachy", "Isabella II", "Stanford University", "Vicente Carrillo Leyva,", "a shortfall in their pension fund", "2018"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5786458333333333}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, true, true, false, true, true, false, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.5, 0.16666666666666669, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.20000000000000004, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.4, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-8782", "mrqa_searchqa-validation-1729", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-4950", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-7034", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-11346", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-16742", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-13271", "mrqa_searchqa-validation-11968", "mrqa_searchqa-validation-57", "mrqa_searchqa-validation-16298", "mrqa_searchqa-validation-13046", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-14691", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-3504", "mrqa_searchqa-validation-5600", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-5758", "mrqa_searchqa-validation-15319", "mrqa_naturalquestions-validation-6197", "mrqa_triviaqa-validation-6424", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4568", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-3554"], "SR": 0.4375, "CSR": 0.5476371951219512, "EFR": 1.0, "Overall": 0.7248399390243903}, {"timecode": 41, "before_eval_results": {"predictions": ["$40,000", "the Stockton & Darlington Railway", "aurochs", "Israel", "Prince Rainier", "Harper", "Jeff Bridges", "Humphrey Bogart", "honda", "Alan Bartlett Shepard Jr.", "by burning nitrates and mercuric oxides together.", "Smiley", "Jacks", "Rosslyn Chapel", "Hispaniola", "the Zulus", "blood", "Ironside", "Aristotle", "(Prunella Scales)", "South Sudan", "Monday", "the Dannebrog", "Secretary of State William H. Seward", "the east coast", "Antoine Lavoisier", "NOW Magazine", "Tuscany", "Battle of the Alamo", "Beaujolais Nouveau", "Edmund Cartwright", "Der Stern", "duke of Mantua", "the popes", "Kippis", "Barry McGuigan", "Wisconsin", "Sir Charles Hall\u00e9", "Eton College", "Harrods", "Charles Dickens", "Ted Hankey", "General Joseph W. Stilwell", "the midrib", "sternum", "Portuguese", "Guerrero", "Greece", "Ed Miliband", "marriage", "iron lung", "the Emperor", "in the fascia surrounding skeletal muscle", "Robin", "the Distinguished Service Cross", "Indian classical", "1998", "11", "\"an eye for an eye,\"", "Arabic, French and English", "Schwalbe", "the owl and the Pussycat", "Seinfeld", "Cress"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6545386904761905}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-2666", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-2826", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-2154", "mrqa_naturalquestions-validation-6109", "mrqa_hotpotqa-validation-1596", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-11859"], "SR": 0.578125, "CSR": 0.5483630952380952, "EFR": 1.0, "Overall": 0.7249851190476191}, {"timecode": 42, "before_eval_results": {"predictions": ["Sybilla of Normandy", "beta decay", "Norman", "George Strait", "Andrew Gold", "1983", "a virtual reality simulator", "Banquo", "Pakistan", "during initial entry training", "MFSK and Olivia", "Isaiah Amir Mustafa", "negotiates treaties with foreign nations", "Paracelsus", "John C. Reilly", "Strabo", "Gloria", "Utah, Arizona, Wyoming, and Oroville, California", "epidermis", "( 27 January -- 16 April 1898 )", "1770 BC", "360", "a single, implicitly structured data item in a table", "1959", "Gunpei Yokoi", "216", "Justin Bieber", "Red Sea and the east African coast", "ideology", "160km / hour", "edible - nest swiftlets", "Andrew Garfield", "the 90s", "Gibraltar", "electron pairs", "cut off close by the hip, and under the left shoulder", "Alice Cooper", "ranking used in combat sports", "Tokyo for the 2020 Summer Olympics", "1972", "Virgil Tibbs", "Ethel Merman", "1961", "usernames, passwords, commands and data", "National Industrial Recovery Act", "adenosine diphosphate", "General George Washington", "Richard Masur", "Lake Wales", "1560s", "Gutenberg", "Wichita", "Tina Turner", "john galliano", "Henry John Kaiser", "Marilyn Martin", "SARS", "tax", "linda evionow", "23 million square meters (248 million square feet)", "neon", "Prisoner of Azkaban", "the ark of acacia", "island of Basilan"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6646664915966387}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.33333333333333337, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.28571428571428575, 0.16666666666666666, 0.0, 0.3846153846153846, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4225", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-6901", "mrqa_hotpotqa-validation-23", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-153", "mrqa_newsqa-validation-1091", "mrqa_newsqa-validation-748", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-7408", "mrqa_newsqa-validation-3408"], "SR": 0.53125, "CSR": 0.5479651162790697, "retrieved_ids": ["mrqa_squad-train-75686", "mrqa_squad-train-34053", "mrqa_squad-train-70935", "mrqa_squad-train-85069", "mrqa_squad-train-48398", "mrqa_squad-train-58705", "mrqa_squad-train-3026", "mrqa_squad-train-21854", "mrqa_squad-train-8454", "mrqa_squad-train-68791", "mrqa_squad-train-7677", "mrqa_squad-train-20563", "mrqa_squad-train-13002", "mrqa_squad-train-62521", "mrqa_squad-train-42757", "mrqa_squad-train-64330", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-539", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-3813", "mrqa_searchqa-validation-12649", "mrqa_hotpotqa-validation-398", "mrqa_triviaqa-validation-435", "mrqa_searchqa-validation-7828", "mrqa_naturalquestions-validation-9130", "mrqa_hotpotqa-validation-108", "mrqa_newsqa-validation-3459", "mrqa_hotpotqa-validation-1730", "mrqa_triviaqa-validation-778", "mrqa_naturalquestions-validation-1699", "mrqa_squad-validation-4458", "mrqa_naturalquestions-validation-4072"], "EFR": 0.9666666666666667, "Overall": 0.7182388565891473}, {"timecode": 43, "before_eval_results": {"predictions": ["article 30", "chocolate macchiato", "Sheffield United", "Microsoft", "Wat Tyler", "john Wayne", "Dutch", "Earth", "James Hogg", "Texas", "Rhino", "Pears soap", "Bavarian", "Louis XVI", "(George) Adams", "fifty-six", "Uranus", "Xenophon", "chord", "Chubby Checker", "Separate Tables", "Wilson", "coal", "Stephen of Blois", "the Department of Justice", "eukharistos", "baseball", "Bear Grylls", "jaws", "Tanzania", "Val Doonican", "a tittle", "E. T. A. Hoffmann", "Republic of Upper Volta", "Alexander Borodin", "an elephant", "German", "New Zealand", "Mendip", "stencils, and other art pieces", "Jane Austen", "God bless America, My home sweet home", "trade mark number 1", "boxing", "Benjamin Disraeli", "The Jungle Book", "Grand jete", "Jan van Eyck", "Rabin", "Shania Twain", "John Nash", "electron donors", "Yogiism", "used as a pH indicator, a color marker, and a dye", "Nicolas Winding Refn", "137\u201373", "Elvis' Christmas Album", "troops to \"conduct an analysis\" of whether it is militarily essential to conduct a raid at night or whether it can be put off until daylight,", "Robert Park", "21 percent", "Cairo", "Jackson Pollock", "an elk", "job training"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6102430555555556}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.2222222222222222, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-2611", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-5629", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-3105", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-7849", "mrqa_hotpotqa-validation-501", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-1305"], "SR": 0.53125, "CSR": 0.5475852272727273, "EFR": 0.9666666666666667, "Overall": 0.7181628787878788}, {"timecode": 44, "before_eval_results": {"predictions": ["1994\u20131999", "Aamir Khan", "Euripides", "Alfonso Cuar\u00f3n", "2013", "end of the 18th century", "June 24, 1935", "Frederick Martin \"Fred\" Mac Murray", "Kauffman Stadium", "concentration camp", "2013\u201314", "the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union", "1995 to 2012", "Martin Ruhe (born 1970)", "Rothschild", "China", "lead female role of London tipton", "model", "alternate", "1874", "Citric acid", "North Dakota and Minnesota", "Matt Lucas", "Zambia", "The Sun", "Christopher Tin", "Saint Louis", "Chesley Burnett \"Sullenberger\"", "Francis", "1909 Cuban-American Major League Clubs Series", "Cleveland Browns", "a fibre optic cable with TOSLINK connectors", "Dutch", "Battle of Prome", "35,000", "eastern", "only American-born", "2015", "19th", "smith", "Lev Ivanovich Yashin", "Carrefour", "John Monash", "Benjam\u00edn", "Bank of China Tower", "the first Spanish conquistadors in the region of North America now known as Texas", "Battle of Etowah", "12", "Margiana", "Gatwick", "200,000", "2,140 kilometres ( 1,330 mi )", "Highlands County", "honey bees", "squash", "Chicago", "soy", "Nineteen", "How I Met Your Mother", "ammonia leaks and a fire that was not extinguished", "Everest", "I.M. Pei", "Florence Nightingale", "plato"], "metric_results": {"EM": 0.5, "QA-F1": 0.5989397321428571}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, false, true, true, true, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.125, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1050", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-2715", "mrqa_hotpotqa-validation-3060", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-10098", "mrqa_naturalquestions-validation-8186", "mrqa_triviaqa-validation-3878", "mrqa_newsqa-validation-2766", "mrqa_searchqa-validation-16341"], "SR": 0.5, "CSR": 0.5465277777777777, "EFR": 0.96875, "Overall": 0.7183680555555555}, {"timecode": 45, "before_eval_results": {"predictions": ["pamphlets on Islam", "United Kingdom", "Jesse of Bethlehem", "Oklahoma City", "insulin", "Miene \"Minnie\" Schoenberg", "Sir John Mortimer", "John Walsh", "Moldova", "Mnemosyne", "London", "Sunset Boulevard", "the Duke of Buccleuch", "The Lion King", "perfume", "Wyoming", "Benedictus", "Cast", "Javier Bardem", "8", "Lee Harvey Oswald", "virtual image", "Sherlock Holmes", "Bayern", "Rotherham United", "Pesach", "Bobby Kennedy", "Skylab", "Portugal", "Rhine River", "Confucius", "Japan", "stewardi(i)", "Beijing", "Christian Dior", "Phoenicia", "(C) Bobby Moore", "The Frighteners", "Jerez de la Frontera", "plac\u0113b\u014d", "FORUM FRIend", "porto", "in an industry dominated by men perfectly fits both of these characters.", "argument form", "Rochdale", "Portuguese", "Madagascar", "Helsinki", "The Landlord's Game", "myxoma virus", "Ceylon", "between 8.7 % and 9.1 %", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "a mid-size four - wheel drive luxury Volvo Mercedes -Benz GL - Class", "Denmark", "eastern India", "World Famous Gold & Silver Pawn Shop", "In what year", "A Colorado prosecutor", "South Africa", "modern mathematics", "ABBA", "Phoenicia", "New Orleans Saints"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6052923387096774}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true, true, false], "QA-F1": [0.5, 0.0, 0.5, 0.4, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8387096774193548, 0.33333333333333337, 0.0, 1.0, 1.0, 0.4, 1.0, 0.5, 0.0, 1.0, 1.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_squad-validation-2291", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-3330", "mrqa_triviaqa-validation-3970", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-5064", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2776", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-2485", "mrqa_triviaqa-validation-2853", "mrqa_triviaqa-validation-3756", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1586", "mrqa_hotpotqa-validation-4222", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-5528", "mrqa_hotpotqa-validation-3195"], "SR": 0.484375, "CSR": 0.5451766304347826, "retrieved_ids": ["mrqa_squad-train-63326", "mrqa_squad-train-37683", "mrqa_squad-train-84968", "mrqa_squad-train-28749", "mrqa_squad-train-77636", "mrqa_squad-train-18886", "mrqa_squad-train-31989", "mrqa_squad-train-48034", "mrqa_squad-train-5256", "mrqa_squad-train-19934", "mrqa_squad-train-10710", "mrqa_squad-train-56041", "mrqa_squad-train-13810", "mrqa_squad-train-58566", "mrqa_squad-train-15299", "mrqa_squad-train-80171", "mrqa_hotpotqa-validation-5101", "mrqa_naturalquestions-validation-10208", "mrqa_squad-validation-680", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-276", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-5521", "mrqa_triviaqa-validation-7595", "mrqa_squad-validation-4631", "mrqa_naturalquestions-validation-10367", "mrqa_hotpotqa-validation-1581", "mrqa_naturalquestions-validation-4433", "mrqa_hotpotqa-validation-1457", "mrqa_searchqa-validation-6319", "mrqa_triviaqa-validation-6537", "mrqa_naturalquestions-validation-9278"], "EFR": 1.0, "Overall": 0.7243478260869566}, {"timecode": 46, "before_eval_results": {"predictions": ["several", "Mattel", "stromatolites", "Rugby", "a modem", "Clinton", "George Herbert Walker Bush", "Penn State", "Luxor", "Berlusconi", "leviathan", "Mending Wall", "wombat", "a crystal", "thunder", "Josephine", "the Three Musketeers", "iTunes", "Neptune", "Annie", "Romeo and Juliet", "KLM", "Captain Marvel", "X-Men", "the retina", "a goat", "Planet of the Apes", "a knish", "in Calcutta, India", "Reading Railroad", "Leon Trotsky", "Anejo cheese", "the Justice Department", "Melissa Etheridge", "Ignace Jan Paderewski", "julius", "Charles M. Schulz", "the Chesapeake Bay", "Frida Kahlo", "Jane Austen", "Rikki-Tikki", "mutual funds", "polygons", "state", "lm", "a ferry", "Tiananmen Square", "Aeschylus", "cereal", "ewin rommel", "the (Miami) Dolphins", "Thomas Mundy Peterson", "USS Chesapeake", "1900", "george terrier", "alligators", "Hindi", "London", "John Snow", "Ghana's Asamoah Gyan", "soldiers had not gone anywhere they were not permitted to be.", "Afghanistan", "Tuesday in Los Angeles.", "1955"], "metric_results": {"EM": 0.625, "QA-F1": 0.6619791666666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, false, false, false, false, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5523", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-13179", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-8593", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-14169", "mrqa_searchqa-validation-15513", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-14944", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-6336", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-5255", "mrqa_triviaqa-validation-4443", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1277"], "SR": 0.625, "CSR": 0.546875, "EFR": 0.9583333333333334, "Overall": 0.7163541666666667}, {"timecode": 47, "before_eval_results": {"predictions": ["Kauai", "The Lord Mayor", "Shel Silverstein", "the Iberas del Estero", "a trolley", "Liverpool", "Mount Rushmore", "Cyrus the Younger", "Greece", "Jim Bunning", "John Fogerty", "the Logan's Run", "a woofer", "Cubism", "Dune", "the Panama Canal", "Eragon", "vacuum tubes", "drug and alcohol", "the Dinka and Nuer of frontier regions", "the bicentennial", "the Midway", "George Gershwin", "alpacas", "the Atlantic Ocean", "heredity", "The Bicentennial Man", "rod", "heart attack", "Elke Sommer", "Ivan the Terrible", "Flav", "Fulgencio Batista", "The Indianapolis 500", "the Twist", "(Rabbie) Burns", "the cuckoo", "London", "beetle", "Joan of Arc", "palindrome", "the quid", "Vanilla Ice", "Saturday Night Live", "Steinbeck", "Eric Knight", "Heroes", "Ganges", "Thomas Mann", "The book of Samuel", "Sing Sing", "Rajendra Prasad", "August 9, 1945", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Bedfordshire", "Charles V", "The Lion", "Lord's Resistance Army", "In South Asia and the Middle East", "Netflix", "The Uighurs fled Afghanistan shortly after the U.S.-led bombing campaign began in 2001.", "Casa de Campo International Airport", "July", "the greater risk-adjusted return of value stocks over growth stocks"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6913910742035743}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.22222222222222218, 0.0, 0.0, 0.0, 0.18181818181818182, 0.9090909090909091, 1.0, 0.0, 0.7692307692307693, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-2555", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-1850", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-8106", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-2691", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-344", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-2504", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3958", "mrqa_hotpotqa-validation-741"], "SR": 0.578125, "CSR": 0.5475260416666667, "EFR": 0.9629629629629629, "Overall": 0.717410300925926}, {"timecode": 48, "before_eval_results": {"predictions": ["Jaws 2", "Lady Gaga", "Elizabeth Taylor", "James Patterson", "the Incredibles", "a cheetah", "Charlie Brown", "Odin", "Japan", "Monkeys", "the Daffodils", "\"24\"", "Neil Simon", "Voyager", "a gull", "Chief Joseph", "Eva Peron", "incense", "the Hawkeyes", "the 2016 NBA Draft", "Swiffer", "Huckleberry Hound", "Austria", "Jason Bourne", "Peru", "tendon", "atolls", "the Colosseum", "Cambodia", "Dr. Hook and the Medicine Show", "Songs of Innocence", "Uvula", "extreme", "Jacob", "Scrubs", "Cheyenne", "the Black Sea", "King George", "Frank Sinatra", "Zambezi", "a warrior", "Samuel", "The Police", "Jamestown", "the American funk rock band Wild Cherry", "(Robert) Ford", "St. Francis of Assisi", "Lemon Meringue", "(George) Melville", "Tarzan and Jane", "Brett Favre", "1919", "eight years", "Taron Egerton", "Batman", "Stieg Larsson", "The Merchant of Venice and The Taming of the Shrew", "Tomasz Adamek", "The Thomas Crown Affair", "1866", "ultra-high-strength steel and boron", "India", "The EU naval force", "Ruritania"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7842013888888889}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16827", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-9001", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-14611", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-15461", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-15335", "mrqa_triviaqa-validation-6041"], "SR": 0.71875, "CSR": 0.5510204081632653, "retrieved_ids": ["mrqa_squad-train-36674", "mrqa_squad-train-16089", "mrqa_squad-train-31744", "mrqa_squad-train-58893", "mrqa_squad-train-11089", "mrqa_squad-train-8446", "mrqa_squad-train-46166", "mrqa_squad-train-76196", "mrqa_squad-train-83175", "mrqa_squad-train-69386", "mrqa_squad-train-29490", "mrqa_squad-train-61721", "mrqa_squad-train-50108", "mrqa_squad-train-14585", "mrqa_squad-train-5251", "mrqa_squad-train-70279", "mrqa_squad-validation-3257", "mrqa_newsqa-validation-1247", "mrqa_hotpotqa-validation-4961", "mrqa_searchqa-validation-1850", "mrqa_triviaqa-validation-1561", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-14070", "mrqa_hotpotqa-validation-5094", "mrqa_naturalquestions-validation-7849", "mrqa_newsqa-validation-1604", "mrqa_triviaqa-validation-3437", "mrqa_naturalquestions-validation-7608", "mrqa_hotpotqa-validation-2126", "mrqa_naturalquestions-validation-3048", "mrqa_searchqa-validation-4355", "mrqa_triviaqa-validation-4759"], "EFR": 0.9444444444444444, "Overall": 0.714405470521542}, {"timecode": 49, "before_eval_results": {"predictions": ["1972", "the Stonemason's Yard", "Carmen", "shetland Islands", "the Temple Mount", "feminist", "fourteen", "kidneys", "apple", "Athina Onassis", "rafa nadal", "Apollo 11", "five", "Kirk Douglas", "John Ford", "tin", "Longchamp", "Nippon or Nihon-koku", "Henry Ford", "joey", "Maine", "USS Missouri", "Pyrenees mountains", "basketball", "Janis Joplin", "Mr. Stringer", "basketball", "South Africa", "Pet Sounds by The Beach Boys", "Ed Miliband", "Scotland", "aeoline", "Margaret Mitchell", "Republic of Upper Volta", "Fred Perry", "40", "75", "Winston Churchill", "John Masefield", "Rio de Janeiro", "party of God", "Bengali", "Claire", "Guatemala", "Carousel", "Leicester", "Jimmy Robertson", "radishes", "Lister", "Downton Abbey", "achlais", "Garfield Sobers", "Herman Hollerith", "September 2017", "Golden Gate National Recreation Area", "Forbes", "English Electric Canberra", "Mark Fields of Ford", "a meritocracy", "one of 10 gunmen who attacked several targets in Mumbai", "a shrew rat", "yucca", "Maria Callas", "Hern\u00e1n Jorge Crespo"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7076817279942279}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.4, 1.0, 0.6666666666666666, 0.6666666666666666, 0.9090909090909091, 0.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-5762", "mrqa_triviaqa-validation-4967", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-7720", "mrqa_naturalquestions-validation-8483", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-4559", "mrqa_searchqa-validation-12808", "mrqa_hotpotqa-validation-3207"], "SR": 0.609375, "CSR": 0.5521875, "EFR": 0.92, "Overall": 0.70975}, {"timecode": 50, "UKR": 0.71875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-464", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-732", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7785", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9451", "mrqa_searchqa-validation-9800", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6786", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-996"], "OKR": 0.876953125, "KG": 0.49453125, "before_eval_results": {"predictions": ["Ross Elliott", "Annette", "the 1980s", "Nodar Kumaritashvili", "Carpenter", "Dan Stevens", "human colon", "December 1886", "July 1, 1890", "March 31, 2013", "Fawcett", "1978", "Judiththia Aline Keppel", "BC Jean and Toby Gad", "The ladies'single figure skating competition of the 2018 Winter Olympics was held at the Gangneung Ice Arena", "Charlie Adlard", "in Christian eschatology", "1962", "non-ferrous", "the state sector", "The sacroiliac joint or SI joint", "Joudeh Al - Goudia family", "after World War II", "Cheshire", "The Massachusetts Compromise", "L.K. Advani", "30 months", "Jason Marsden", "Louis Le Vau", "Ashrita Furman", "St. Augustine renders it as clara notitia cum laude, `` brilliant celebrity with praise ''", "By the early 1960s", "Phoenix neighborhood of Ahwatukee", "the beginning", "2013", "Diego Tinoco", "If there are no repeated data values", "January 2004", "Glenn Close", "Cefal\u00f9, Caen, Durham", "Johannes Gutenberg", "Dan Stevens", "Alex Ryan", "Kathleen Erin Walsh", "Carolyn Sue Jones", "Leon Battista Alberti", "a symbol of Shiva", "in various submucosal membrane sites of the body", "Article 1, Section 2", "birch", "a response to the sensation of food within the esophagus", "dolly parton", "Stoke Bruerne", "durham", "Jack Murphy Stadium", "Black Abbots", "Prince Amedeo", "a real person to talk to.", "Suba Kampong township", "for a full facial transplant since 2004.", "Laryngitis", "Pequod", "Calvin Coolidge", "Dan Parris, 25, and Rob Lehr, 26"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6330755471380471}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.5, 0.5714285714285715, 0.28571428571428575, 0.0, 0.0, 0.0, 0.14814814814814814, 0.1818181818181818, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.28571428571428575, 0.4666666666666667, 0.8, 1.0, 0.36363636363636365, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-6810", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6353", "mrqa_hotpotqa-validation-5522", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-2294"], "SR": 0.515625, "CSR": 0.5514705882352942, "EFR": 0.967741935483871, "Overall": 0.721889379743833}, {"timecode": 51, "before_eval_results": {"predictions": ["Domhnall Gleeson", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Alicia Vikander", "the person compelled to pay for reformist programs", "Scottish post-punk band Orange Juice", "1837", "Zoe Badwi, Jade Thirlwall's cousin, was supporting the gigs in Australia", "22 November 1914", "Shareef Abdur - Rahim", "2018", "the breast or lower chest of beef or veal", "in the mid - to late 1920s", "near Camarillo, California", "douglas", "2018", "Exodus 20 : 1 -- 17", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "15 February 1998", "brothers Henry, Jojo and Ringo Garza", "Thomas Alva Edison", "Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "`` Mirror Image ''", "the population, serving staggered terms of six years", "senior enlisted sailor ( `` E-9 '' )", "1603", "Eduardo", "a child with Treacher Collins syndrome trying to fit in", "Kansas", "Efren Manalang Reyes, OLD, PLH ( born August 26, 1954 ), nicknamed the Magician and Bata", "Jim Carrey", "Baseball Writers'Association of America ( or BBWAA ), or the Veterans Committee", "Herman Hollerith", "ulnar nerve", "December 18, 2017", "the Brewster family, descended from the Mayflower", "2015", "Buddhism", "Rodney Crowell", "Atlanta", "peninsular", "2007", "chairman ( more usually now called the `` chair '' or `` chairperson '' ), who holds whatever title is specified in the bylaws or articles of association", "Germany", "Gamora", "Darlene Cates", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "in Poems : Series 1, a collection of Dickinson's poems assembled and edited by her friends Mabel Loomis Todd and Thomas Wentworth Higginson", "birch", "Tony Kilgallen, Arlene Francis, and Bennett Cerf", "Matt Monro", "Joe Willie Kirk", "gully", "Vito Corleone", "supply chain management", "Baugur Group", "Venice", "Hyundai Steel's", "gaylord Opryland", "100 percent", "Boston", "Roger Clemens", "Andrew Carnegie", "an independent homeland for the country's ethnic"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6169858615625331}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false, true, false, false, true, false, true, false, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.9090909090909091, 0.5714285714285715, 1.0, 0.5714285714285715, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.15384615384615383, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 0.375, 0.0, 0.7999999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8205128205128205, 0.9302325581395349, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-407", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-6482", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1756", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-8208"], "SR": 0.484375, "CSR": 0.5501802884615384, "retrieved_ids": ["mrqa_squad-train-5656", "mrqa_squad-train-58075", "mrqa_squad-train-86174", "mrqa_squad-train-53106", "mrqa_squad-train-50594", "mrqa_squad-train-22747", "mrqa_squad-train-42080", "mrqa_squad-train-14737", "mrqa_squad-train-80447", "mrqa_squad-train-86087", "mrqa_squad-train-85678", "mrqa_squad-train-76893", "mrqa_squad-train-78854", "mrqa_squad-train-13050", "mrqa_squad-train-51858", "mrqa_squad-train-85676", "mrqa_newsqa-validation-3579", "mrqa_searchqa-validation-245", "mrqa_triviaqa-validation-3390", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4336", "mrqa_searchqa-validation-3762", "mrqa_naturalquestions-validation-9278", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-12185", "mrqa_squad-validation-3370", "mrqa_searchqa-validation-8449", "mrqa_triviaqa-validation-2154", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-14849"], "EFR": 0.8484848484848485, "Overall": 0.6977799023892775}, {"timecode": 52, "before_eval_results": {"predictions": ["Vilnius Old Town", "Roc-A-Fella Records and Priority Records", "United States Army", "White Horse", "Serial (Bad) Weddings", "created the American Land-Grant universities and colleges", "Pacific War", "1949", "The Dark Tower", "John Samuel Waters Jr.", "1945", "Sacramento Kings", "S6", "Captain Beefheart & His Magic Band", "Supergirl", "1949", "Northern Ireland national team", "Standard Oil", "William Harold \"Bill\" Ponsford", "Anatoly Vasilyevich Lunacharsky", "Bob Hurley", "Navarasa", "Brad Silberling", "1987", "Italy", "Vaisakhi List", "\"Twice in a Lifetime\"", "7 Series", "Len Wiseman", "31 July 1975", "his tenure", "Walldorf", "Elvis' Christmas Album", "sarod", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Sarah Winnemucca Hopkins", "\"master builder\"", "Godiva", "Manchester United and the England national team", "\"Futurama\"", "nuclear weapons", "Russia", "Lush Ltd.", "Telugu", "1952", "Georgia Southern University", "Restoration Hardware", "1942", "Kauffman Stadium", "Eminem", "C. H. Greenblatt", "Stephen Graham", "President", "by functions", "Belgium", "big bopper", "Jackson Pollock", "Alwin Landry's supply vessel Damon Bankston", "3,000 kilometers (1,900 miles)", "Casalesi clan", "Linda Darnell", "Scrabble", "Wendell", "an intercalary year"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7000744047619047}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.13333333333333333, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-1555", "mrqa_hotpotqa-validation-79", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-5189", "mrqa_hotpotqa-validation-2084", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1510", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-920", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-5889", "mrqa_naturalquestions-validation-4714", "mrqa_naturalquestions-validation-6706", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-7977", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-2103"], "SR": 0.59375, "CSR": 0.5510023584905661, "EFR": 1.0, "Overall": 0.7282473466981132}, {"timecode": 53, "before_eval_results": {"predictions": ["$250,000 for Rivers' charity: God's Love We Deliver.", "The cervical cancer vaccine,", "eight-day", "97 years of age.", "a delegation of American Muslim and Christian leaders", "18", "Darrel Mohler", "Spc. Megan Lynn Touma,", "Operation Pipeline Express.", "admitting they learned of the death from TV news coverage,", "in the head with a.40-caliber pistol,", "full Senate Sotomayor,", "faultless, Mother Nature has proven to be a challenge.", "Grand Ronde, Oregon.", "a bag", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "14-day", "the fact that the teens were charged as adults.", "Conway", "a desire by each of these institutions to reach beyond their individual capabilities and build a practical framework that could help the U.S. government better respond to threats of genocide", "rwanda", "Arsene Wenger", "scored a hat-trick as AC Milan went second in Serie A with a 5-1 win over Torino in the San Siro on Sunday.", "Genocide Prevention Task Force", "Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed", "\"pleased\"", "Jacob Zuma", "the return of a fallen U.S. service member was able to be recorded by the media.", "Sporting Lisbon", "The opposition group, also known as the \"red shirts,\"", "Saturday", "social networking sites", "keyboardist and original member of The Charlie Daniels Band", "Democratic VP candidate", "Strindberg", "Theyon are said to be willing to cash in at the right price with Spanish giants Barcelona and Real Madrid", "three men with suicide vests who were plotting to carry out the attacks,", "between June 20 and July 20", "President Richard M. Nixon and his Brazilian counterpart, Emilio Medici", "Piedad Cordoba,", "Buddhism", "Bollywood superstar Amitabh Bachchan", "Pakistani territory", "fight outside of an Atlanta strip club", "Britain's Got Talent", "Sen. Barack Obama", "the game", "the man facing up, with his arms out to the side.", "stand down.", "in a muddy barley field owned by farmer Alan Graham outside Bangor,", "The ACLU", "serves as the physical link between the mRNA and the amino acid sequence of proteins", "Coldplay", "2018", "surfer", "phylum", "white", "November 6, 2018", "1898", "My Beautiful Dark Twisted Fantasy", "Ned Kelly", "Ophiuchus", "a fish-only diet", "a crust of mashed potato"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6257069942441459}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, true, false, false, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.6153846153846153, 0.5714285714285715, 0.4, 0.0, 1.0, 1.0, 0.9523809523809523, 1.0, 1.0, 0.0, 0.058823529411764705, 1.0, 0.0, 0.1904761904761905, 1.0, 1.0, 0.0, 1.0, 0.631578947368421, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.36363636363636365, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-2679", "mrqa_newsqa-validation-2672", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-3097", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-3783", "mrqa_triviaqa-validation-2038", "mrqa_searchqa-validation-12411", "mrqa_searchqa-validation-16162", "mrqa_naturalquestions-validation-10616"], "SR": 0.53125, "CSR": 0.5506365740740741, "EFR": 1.0, "Overall": 0.7281741898148149}, {"timecode": 54, "before_eval_results": {"predictions": ["Haikou on the Hainan Island", "Squamish, British Columbia, Canada", "2018", "according to the Gospels of Matthew, Mark, Luke and John", "on the table", "illegitimate son of Ned Stark, the honorable lord of Winterfell, an ancient fortress in the North of the fictional continent of Westeros", "Tony Rydinger", "Latavious Williams", "Hans Raffert", "31", "Jesse Frederick James Conaway", "Adwaita", "neutrality", "Number 4, Privet Drive, Little Whinging in Surrey, England", "in positions 14 - 15, 146 - 147 and 148 - 149", "2018", "Malibu, California", "desublimation", "eight", "from the Anglo - Norman French waleis", "the three mystic apes", "in lymph", "in the intermembrane space", "Kansas", "the eventual Super Bowl champion New England Patriots", "Chesapeake Bay", "Fred Ott", "to refer to a former sexual or romantic partner, especially a former spouse", "the body - centered cubic ( BCC ) lattice", "Joan Baez", "set in a Norwegian town", "the pretribulation, premillennial, Christian eschatological interpretation of the Biblical apocalypse", "the topography and the dominant wind direction", "Development of Substitute Materials", "a pagan custom", "in various submucosal membrane sites of the body", "the 2005 novel The Book thief by Markus Zusak and adapted by Michael Petroni", "John Garfield as Al Schmid", "the Islamic Community", "C. Sankaran Nair", "the volume", "no longer a fundamental right", "Robert Gillespie Adamson IV", "the end of the 18th century", "1998", "the left atrium of the heart", "Gladys Knight & the Pips", "Sir Ernest Rutherford", "Hendersonville, North Carolina", "the internal auditory canal of the temporal bone", "1858", "deliver appliances and other goods for department stores", "The Wrestling Classic", "The Kennel Club", "Timothy Dalton", "Grammy awards", "John D Rockefeller's Standard Oil Company", "second-degree attempted murder and conspiracy,", "$106,482,500 to an unidentified telephone bidder,", "introduce legislation Thursday to improve the military's suicide-prevention programs.\"", "Stone Temple Pilots", "a real estate investment trust", "Hubert H. Humphrey", "Tim Clark, Matt Kuchar and Bubba Watson"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5318519565132817}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false, false, false, false, false, true, true, true, true, false, false, true, true, false, false, true], "QA-F1": [0.888888888888889, 1.0, 1.0, 0.0, 0.0, 0.4347826086956522, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 0.0, 0.21428571428571425, 0.6666666666666666, 0.0, 1.0, 0.6, 0.0, 1.0, 0.625, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.4666666666666667, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.29629629629629634, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9087", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5330", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-9759", "mrqa_naturalquestions-validation-2242", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3174", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-6727", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-3624", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-900", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3606"], "SR": 0.390625, "CSR": 0.5477272727272727, "retrieved_ids": ["mrqa_squad-train-30672", "mrqa_squad-train-82768", "mrqa_squad-train-26791", "mrqa_squad-train-39212", "mrqa_squad-train-44070", "mrqa_squad-train-54656", "mrqa_squad-train-26823", "mrqa_squad-train-9266", "mrqa_squad-train-81962", "mrqa_squad-train-41364", "mrqa_squad-train-2233", "mrqa_squad-train-50794", "mrqa_squad-train-55888", "mrqa_squad-train-2034", "mrqa_squad-train-22196", "mrqa_squad-train-67897", "mrqa_triviaqa-validation-1389", "mrqa_searchqa-validation-2555", "mrqa_newsqa-validation-1510", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-7244", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-327", "mrqa_naturalquestions-validation-10258", "mrqa_newsqa-validation-1899", "mrqa_triviaqa-validation-6654", "mrqa_naturalquestions-validation-10114", "mrqa_triviaqa-validation-2776", "mrqa_naturalquestions-validation-9741", "mrqa_squad-validation-8560", "mrqa_searchqa-validation-7976"], "EFR": 0.9230769230769231, "Overall": 0.7122077141608392}, {"timecode": 55, "before_eval_results": {"predictions": ["Dr. Ruth Westheimer", "John Updike", "North Rhine-Westphalia", "clouds", "Jericho", "swab", "asteroids", "whale shark", "(George) Bush", "Eleanor Roosevelt", "Battle of LAKE ERIE", "Bangladesh", "The Secret", "Sudan", "Judd Apatow", "laser", "Jamaica", "Walt Disney World", "Mexico", "Artemis", "pH", "Aladdin", "Nine to Five", "Jan & Dean", "walk the plank", "ice cream", "Mike Huckabee", "catherine the great", "Texas", "constellations", "As I Lay Dying", "Kate Winslet", "Ross Perot", "the Black Sea", "C.S. Lewis", "Thomas Paine", "Back to the Future", "antelope", "Anne Boleyn", "Q'umarkaj", "Dizzy", "soup", "the ACT", "Fermi", "daedalus", "suspension bridge", "Tigger", "the breath", "the marathon", "QWERTY", "Deuteronomy", "collect menstrual flow", "13 May 1787", "the protruding part of the face", "jinxed Jensen-Healey", "Kansas", "the recorder", "UFC 50: The War of '04", "newspapers, television, radio, cable television", "March 17, 2015", "4.6 million", "the Dalai Lama", "Landry", "Geoffrey Zakarian"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6834449404761904}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true], "QA-F1": [0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14322", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-717", "mrqa_searchqa-validation-8235", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-1425", "mrqa_searchqa-validation-4506", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-11807", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-13455", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-11295", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-14266", "mrqa_searchqa-validation-960", "mrqa_searchqa-validation-2219", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-5113", "mrqa_triviaqa-validation-4151", "mrqa_hotpotqa-validation-1190", "mrqa_newsqa-validation-478"], "SR": 0.578125, "CSR": 0.5482700892857143, "EFR": 1.0, "Overall": 0.7277008928571429}, {"timecode": 56, "before_eval_results": {"predictions": ["Pegida", "Tardis", "sugar", "the Potteries", "bowley", "iron", "Little arrows", "from florentia", "cats", "Reanne Evans", "Niger", "Battle of Camlann", "british marie Winston Newson", "1905", "Strasbourg, France", "british", "Jack London", "Hard Times", "Muhammad Ali", "carbon", "Sierra One from Sierra Oscar", "LateRooms.com", "Boxing Day", "cheers", "Taliban", "alpestrine", "a toad", "to make something better", "Noreg in Nynorsk", "skirts", "Australia", "Blucher", "Artemis", "Sachin Tendulkar", "a black Ferrari", "Hull", "Canary Islands", "South Africa", "bone", "Nutbush", "Robert Boothby", "Shinto", "Cleckheaton", "the Greater Antilles", "Scotch", "Pluto", "pensioner Jim Branning", "cryonics", "Fleet Street", "Scafell Pike", "baseball", "President pro tempore", "Athens", "the second major series of Macintosh operating systems", "Leslie James \"Les\" Clark", "fourth season of \"American Idol\"", "Realty Bites", "Former Mobile County Circuit Judge Herman Thomas", "News of the World tabloid", "propofol", "Emmett Kelly", "from rosie the queen of corona", "Queen Victoria", "a desire to be reckoned with as an openly wounded and unabashedly portentous rock balladeer"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5858630952380952}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.05714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-4559", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-6783", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-6228", "mrqa_naturalquestions-validation-8114", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2748", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2374", "mrqa_newsqa-validation-1282", "mrqa_searchqa-validation-1086", "mrqa_searchqa-validation-378", "mrqa_naturalquestions-validation-7270"], "SR": 0.515625, "CSR": 0.5476973684210527, "EFR": 0.967741935483871, "Overall": 0.7211347357809847}, {"timecode": 57, "before_eval_results": {"predictions": ["outside influences in next month's run-off election,", "Monday", "eight-week", "dummies", "coalition", "fritter his cash away on fast cars, drink and celebrity parties.", "Stratfor", "from Paktika province in southeastern Afghanistan,", "Anseeded Frenchwoman Aravane Rezai", "murder in the beating death of a company boss who fired them.", "David Beckham", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Islamabad", "Dennis Davern,", "sailor", "\"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "The opposition group, also known as the \"red shirts,\"", "Madhav Kumar Nepal", "in a federal sting after his bodyguard-turned-informant delivered three machine guns and two silencers to the hip-hop star,", "foxes marie marie ivy gergiev, and London-based Russian art collector Nonna Materkova", "around 8 p.m. local time Thursday", "11", "sodomized him with a broomstick, a pair of scissors and a wooden dowel used to hang clothes in a closet.", "A planned missile defense system in Eastern Europe poses no threat to Russia,", "Citizens are picking members of the lower house of parliament,", "refusal or inability to \"turn it off\"", "returning combat veterans could be recruited by right-wing extremist groups.", "nine newly-purchased bicycles at the scene,", "The alleged surviving attacker from last month's Mumbai terror attacks is seeking help from Pakistani officials,", "Afghanistan", "the fact that the teens were charged as adults.", "Siri", "dogs who walk on ice in Alaska.", "10 to 15 percent", "Israel", "british", "The incident Sunday evening", "Alwin Landry", "Barack Obama", "Alexandre Caizergues,", "Steven Gerrard", "three", "Golden Gate Yacht Club of San Francisco", "The 725-mile Veracruz", "Grease", "Camp Lejeune, North Carolina", "2002 for British broadcaster Channel 4", "of wild mustangs and unwanted horses near Lancaster, California.", "the job bill's controversial millionaire's surtax,", "seven", "One of Osama bin Laden's sons", "in Africa", "Britain of Florida", "Manuel `` Manny '' Heffley is Greg and Rodrick's younger brother", "2004", "foxes", "Ambassador Bridge", "The University of Liverpool", "Count Schlieffen", "Chillingham Castle", "the 400th anniversary", "River Liffey", "Scrabble", "Rickie Lee Skaggs"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5322722021136543}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, true, false, false, false, true, false, true, false, true, false, false, false, true, true, true, false, true, false, false, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, true, true], "QA-F1": [0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.23529411764705885, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0625, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 0.0, 0.33333333333333337, 0.0606060606060606, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.5714285714285715, 1.0, 0.2222222222222222, 0.0, 0.0, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-5355", "mrqa_triviaqa-validation-5508", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-3212", "mrqa_searchqa-validation-7178"], "SR": 0.4375, "CSR": 0.5457974137931034, "retrieved_ids": ["mrqa_squad-train-84797", "mrqa_squad-train-49640", "mrqa_squad-train-49507", "mrqa_squad-train-68286", "mrqa_squad-train-30887", "mrqa_squad-train-15432", "mrqa_squad-train-44705", "mrqa_squad-train-20598", "mrqa_squad-train-63324", "mrqa_squad-train-40725", "mrqa_squad-train-24266", "mrqa_squad-train-20793", "mrqa_squad-train-2152", "mrqa_squad-train-25445", "mrqa_squad-train-27992", "mrqa_squad-train-26468", "mrqa_hotpotqa-validation-1555", "mrqa_triviaqa-validation-4151", "mrqa_searchqa-validation-559", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-6545", "mrqa_triviaqa-validation-2385", "mrqa_naturalquestions-validation-7553", "mrqa_searchqa-validation-10014", "mrqa_searchqa-validation-4533", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-3669", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-1948", "mrqa_naturalquestions-validation-180", "mrqa_searchqa-validation-8782", "mrqa_newsqa-validation-1806"], "EFR": 1.0, "Overall": 0.7272063577586206}, {"timecode": 58, "before_eval_results": {"predictions": ["Ted", "1,467 rooms", "1989", "Nicole Kidman", "14", "National Basketball Development League", "Gust Avrakotos", "involuntary euthanasia", "naval aviator, test pilot, and businessman", "duck", "The Summer Olympic Games", "Glendale", "St. Louis Cardinals", "November 23, 1992", "1993", "La Salle College", "Jack Ridley", "The Pennsylvania State University", "Chicago", "William Corcoran Eustis", "evangelical Christian", "Hanoi", "ITV", "Australia and New Zealand", "suburb", "Flex-fuel", "The Savannah River Site", "swingman", "Patriots Day", "Scotland", "Todd Emmanuel Fisher", "1944", "Suicide Squad", "1883", "23", "Mach number", "James Gay-Rees", "1872", "poetry", "Who's That Girl", "musicologist", "Lauren Alaina", "Prince Amedeo, 5th Duke of Aosta", "Ben Ainslie", "Forbidden Quest", "non-alcoholic", "paper-based card", "White Horse", "Duncan Kenworthy", "Malayalam movies", "Peter Nowalk", "Annette", "an exultation of spirit", "Bumblebee", "Riyadh", "Lady Gaga", "African violet", "three", "There's no chance", "the Carrousel du Louvre", "A Tale of Two Cities", "Angel Gabriel", "Braveheart", "( Boss) Tweed"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6572916666666666}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-4747", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-3935", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3420", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6523", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2955"], "SR": 0.578125, "CSR": 0.5463453389830508, "EFR": 1.0, "Overall": 0.7273159427966103}, {"timecode": 59, "before_eval_results": {"predictions": ["New Croton Reservoir", "connotations of the passing of the year", "Matt Monro", "Aristotle", "in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "2010", "Coroebus of Elis", "Ewan McGregor as Obi - Wan Kenobi", "1961", "iron", "Jesse Frederick James Conaway", "bypasses", "supported modern programming practices and enabled business applications to be developed with Flash", "Anne Murray", "1957", "49 Stat. 449 ) 29 U.S.C. \u00a7 151 -- 169", "a four - page pamphlet", "Have I Told You Lately", "2.4 %", "the straits between the mainland and Salamis", "Lana Del Rey", "April 1979", "season seven", "Janie Crawford", "the adoption of the first ten amendments, the Bill of Rights", "2018", "Byzantine Greek culture and Eastern Christianity", "ordain presbyters / bishops and to exercise general oversight", "11 January 1923", "1961", "the Indians", "to condense the steam coming out of the cylinders or turbines", "two Frenchmen", "Felix Baumgartner", "1995", "2026", "Gupta Empire", "Abigail Hawk", "Hal Derwin", "East Asia", "in the 1970s", "1919", "1889", "halogenated paraffin hydrocarbons", "October 27, 2017", "three", "Richard Crispin Armitage", "Missouri River", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "Jack Barry", "headdresses", "\"We Can Love\"", "the Andaman & Nicobar Islands", "One Direction", "Delacorte Press", "Drifting", "1949", "-- the industry was worth $15 billion in 2008 and is projected to grow by 10 percent,", "Iran", "\"wipe out\" the United States", "Fahrenheit", "Chicago", "Jonathan Swift", "Linux Format"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6196065483429614}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, false], "QA-F1": [0.5454545454545454, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.34782608695652173, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-870", "mrqa_triviaqa-validation-6837", "mrqa_triviaqa-validation-4980", "mrqa_hotpotqa-validation-5386", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-213", "mrqa_searchqa-validation-2403", "mrqa_hotpotqa-validation-4642"], "SR": 0.5625, "CSR": 0.5466145833333333, "EFR": 0.8928571428571429, "Overall": 0.7059412202380952}, {"timecode": 60, "UKR": 0.740234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1288", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2008", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-4096", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5374", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-5864", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-97", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10124", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11828", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12230", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-996"], "OKR": 0.86328125, "KG": 0.48984375, "before_eval_results": {"predictions": ["Batista", "St. Vincent", "5,042", "Mandalay Entertainment", "Debbie Reynolds", "(1963\u201393)", "Mike Holmgren", "2,627", "theScanian War", "Sparky", "Frederick Louis", "American", "Virgin", "October 21, 2016", "Kiss", "Ferdinand Magellan", "Sun Records founder Sam Phillips", "the Corps of Discovery", "receive the benefits of the Morrill Acts of 1862 and 1890", "the Crab Orchard Mountains", "Miss Universe 2010", "Maryland", "2010", "democracy and personal freedom", "Terrina Chrishell Stause", "French Canadians", "1964 to 1974", "the National League", "City Mazda Stadium", "Continental Army", "Wes Archer", "1994", "Vancouver", "Lego", "Thomas Mawson", "Tony Aloupis", "various", "North Dakota", "Francis Nethersole", "The Panther", "British", "twelfth", "The University of California", "City of Onkaparinga", "2 February 1940", "thirteen", "Princes Park", "The Bye Bye Man", "Germanic", "Blue", "1698", "a progressive radial orientation to a common point", "the Constitution of India came into effect on 26 January 1950 replacing the Government of India Act ( 1935 ) as the governing document of India", "an alien mechanoid", "David Letterman", "for Gallantry", "an observation deck", "Government Accountability Office", "12-year veteran", "$50 less,", "high and dry", "An American Tail", "a cat", "Peru"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6575508832565284}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, true], "QA-F1": [0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6451612903225806, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-2954", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-5586", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-4986", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-10257", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-5468", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-3315", "mrqa_searchqa-validation-8784"], "SR": 0.59375, "CSR": 0.5473872950819672, "retrieved_ids": ["mrqa_squad-train-83683", "mrqa_squad-train-17098", "mrqa_squad-train-3463", "mrqa_squad-train-4426", "mrqa_squad-train-2570", "mrqa_squad-train-65148", "mrqa_squad-train-63582", "mrqa_squad-train-3825", "mrqa_squad-train-15792", "mrqa_squad-train-74937", "mrqa_squad-train-79043", "mrqa_squad-train-22984", "mrqa_squad-train-24732", "mrqa_squad-train-79804", "mrqa_squad-train-12537", "mrqa_squad-train-52648", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-593", "mrqa_naturalquestions-validation-7935", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-1965", "mrqa_squad-validation-1272", "mrqa_searchqa-validation-11859", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-5667", "mrqa_newsqa-validation-3101", "mrqa_searchqa-validation-5255", "mrqa_squad-validation-89", "mrqa_newsqa-validation-771", "mrqa_searchqa-validation-14852", "mrqa_hotpotqa-validation-450", "mrqa_newsqa-validation-2205"], "EFR": 1.0, "Overall": 0.7281493340163935}, {"timecode": 61, "before_eval_results": {"predictions": ["The Blades", "George Blake", "russell", "trout", "Aidensfield Arms", "jeremy", "France", "Manchester", "sky", "Susan Bullock", "Angel Cabrera", "November", "Wonga", "Alan Ladd", "Genghis Khan", "Kofi Annan", "jon stewart", "left", "Istanbul", "lamb", "Space Oddity", "collies", "35", "shark", "florida", "Mike Hammer", "Jessica Smith", "Evelyn Glennie", "brain", "Zaragoza", "David Bowie", "Billy Wilder", "\"Mr Loophole\"", "a palla", "4.4 million", "jeremy Thatcher", "Westminster Abbey", "Ralph Lauren", "Whitsunday", "Morgan Spurlock", "Piled peaches and cream", "Debbie Reynolds", "Caroline Aherne", "cations", "George Santayana", "Rudolf Nureyev", "Paul Wellens", "cat", "apple", "argos", "Rodgers & Hammerstein", "part of a pre-recorded television program, Rendezvous with Destiny", "By 1770 BC", "The United States Secretary of State", "2", "Amal Clooney", "C. J. Cherryh", "autonomy.", "Heshmatollah Attarzadeh", "Mark Obama Ndesandjo", "the Roanoke Island", "the Louvre", "Kansas City, Missouri", "Yiddish Scientific Institute"], "metric_results": {"EM": 0.625, "QA-F1": 0.6877604166666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666665, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-6339", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-4021", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-2955", "mrqa_newsqa-validation-2489", "mrqa_searchqa-validation-4730", "mrqa_searchqa-validation-5842", "mrqa_hotpotqa-validation-3793"], "SR": 0.625, "CSR": 0.5486391129032258, "EFR": 1.0, "Overall": 0.7283996975806452}, {"timecode": 62, "before_eval_results": {"predictions": ["Curtis James Martin Jr.", "Gabriel Iglesias", "The Snowman", "Vikram Bhatt", "Helsinki, Finland", "Nayvadius DeMun Wilburn", "Tommy Cannon", "Scottish national team", "203", "Ward Bond", "Illinois's 15 congressional district", "Buffalo", "7,500 and 40,000", "5,112", "UTH Russia", "the lead roles of Timmy Sanders and Jack in the series \"King Jack\"", "four months in jail", "Michael Redgrave", "Sturt", "Taylor Swift", "\"Beauty and the Beast\"", "Europe", "Trilochanapala", "deadpan sketch group", "small family car", "Spanish", "Algernod Lanier Washington", "14,000 people", "in photographs, film and television", "37", "Taoiseach of Ireland", "137th", "Mr. Nice Guy", "Charkhi Dadri mid-air collision", "professional wrestling tag team", "Bury St Edmunds, Suffolk, England", "Loretta Lynn", "Ford Island", "barcode", "video game", "The United States of America", "Lerotholi Polytechnic", "Ribhu Dasgupta", "virginia", "orange", "Memphis, Tennessee", "Swiss Confederation has adopted various provisions of European Union law in order to participate in the Union's single market", "Lake Erie", "Sophie Monk", "Reinhard Heydrich", "lo Stivale", "Mesopotamia, the land in and around the Tigris and Euphrates rivers", "September 2000", "Woodrow Wilson", "our mutual friend", "zebras", "Volkswagen", "Roland S. Martin", "Pope Benedict XVI", "St. Louis, Missouri.", "pearl", "sarsaparilla", "Malocclusion", "Iran of trying to build nuclear bombs,"], "metric_results": {"EM": 0.5, "QA-F1": 0.6476944062881562}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, true, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666665, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.16666666666666666, 0.0, 0.6666666666666666, 1.0, 0.8, 0.6153846153846153, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-1675", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-4554", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-2297", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4604", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-5666", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-3842", "mrqa_newsqa-validation-2278", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-9813"], "SR": 0.5, "CSR": 0.5478670634920635, "EFR": 0.96875, "Overall": 0.7219952876984126}, {"timecode": 63, "before_eval_results": {"predictions": ["Tinseltown", "Claude Monet", "Brazil", "Mokotedi Mpshe,", "apartment building", "July", "2005 & 2006 Acura MDX", "Ryan Adams.", "80 percent of the woman's face", "The torch for the 2010 Vancouver Olympics was lit in a ceremony at the ancient Greek site of Olympia,", "27-year-old's", "next week", "April 26, 1913,", "7-1", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "next year", "\"Swamp Soccer\"", "Noriko Savoie", "The Falklands, known as Las Malvinas in Argentina, lie in the South Atlantic Ocean off the Argentinean coast", "state-of-the-art solar technology.", "Fernando Verdasco", "tennis", "two", "1950s", "Gary Player", "about 12 million", "\"Rin Tin Tin: The Life and the Legend\"", "litter reduction and recycling.", "President George Bush", "an average of 25 percent", "The plane had a crew of 14 people and was carrying an additional 98 passengers,", "800,000", "Sporting Lisbon", "President Sheikh Sharif Sheikh Ahmed", "2005", "\"He went there to receive this bullet. If he would not have been wounded; he wouldn't be in the hospital; he would be living in peace with his family.\"", "Johan Persson and Martin Schibbye", "Israel", "Sunday's", "Swat Valley.", "31-year-old", "The Rev. Alberto Cutie", "all day starting at 10 a.m.", "\"a fantastic five episodes.\"", "organizing the distribution of wheelchairs, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "In a court filing for a protective order, Wimunc said that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "recite her poetry", "in the head", "\"Nazi Party members digging up American bodies at Berga.\"", "neck", "dining scene", "Andrew Garfield", "New England Patriots", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "79", "The Mystery of Edwin Drood", "On 26 May 1787 (JJ Colledge/D Lyon say 23 May)", "Melbourne", "1998", "20 May 1973", "Tuesday", "Volvic", "Ashbury", "Kind Hearts and Coronets"], "metric_results": {"EM": 0.5, "QA-F1": 0.5830976434432223}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, false, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.23529411764705882, 1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.5, 0.125, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.4, 0.21428571428571427, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.7659574468085107, 0.13333333333333333, 0.6666666666666666, 0.1818181818181818, 1.0, 0.8, 1.0, 1.0, 0.4347826086956522, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-2995", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1346", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-146", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-3434", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-2862", "mrqa_hotpotqa-validation-5662", "mrqa_searchqa-validation-1122", "mrqa_searchqa-validation-5963"], "SR": 0.5, "CSR": 0.547119140625, "retrieved_ids": ["mrqa_squad-train-63230", "mrqa_squad-train-73862", "mrqa_squad-train-15411", "mrqa_squad-train-15912", "mrqa_squad-train-57769", "mrqa_squad-train-84330", "mrqa_squad-train-16142", "mrqa_squad-train-37350", "mrqa_squad-train-55240", "mrqa_squad-train-1017", "mrqa_squad-train-83771", "mrqa_squad-train-72691", "mrqa_squad-train-64432", "mrqa_squad-train-41697", "mrqa_squad-train-20717", "mrqa_squad-train-83115", "mrqa_newsqa-validation-2742", "mrqa_triviaqa-validation-2080", "mrqa_hotpotqa-validation-412", "mrqa_searchqa-validation-362", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-1319", "mrqa_searchqa-validation-5620", "mrqa_triviaqa-validation-5143", "mrqa_naturalquestions-validation-3569", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-1824", "mrqa_newsqa-validation-3111", "mrqa_triviaqa-validation-331", "mrqa_squad-validation-6645", "mrqa_searchqa-validation-2733", "mrqa_triviaqa-validation-7060"], "EFR": 1.0, "Overall": 0.7280957031249999}, {"timecode": 64, "before_eval_results": {"predictions": ["\"Den V\u00e6gelsindede\"", "Max Martin and Shellback", "George III", "6,396", "Reinhard Heydrich", "Standard Oil", "over 50 million", "Lieutenant Colonel Horace Meek Hickam", "Charles Russell", "May 1, 2011", "Konstant\u012bns Raudive", "South West Peninsula League", "Transporter 3", "1983", "December 13, 1920", "Gaelic", "more than 265 million", "2004", "The Eisenhower Executive Office Building", "Big 12 Conference", "Thocmentony", "thirteen", "The Hawaii House of Representatives", "New Jersey", "Black Panther Party", "Walt Disney and Ub Iwerks", "\"Queen In-hyun's Man\"", "Woodsy owl", "Dan Castellaneta", "other individuals, teams, or entire organizations", "1,467", "Ian Rush", "Manchester United", "The 2008\u201309 UEFA Champions League", "Kramer Guitars", "El Nacimiento in M\u00fazquiz Municipality", "1968", "Holston River", "July 10, 2017", "London", "science fiction", "Neon City", "Stephen Mangan", "largest Mission Revival Style building in the United States", "Emile Ardolino", "The Terminator", "Samoa", "\"Bad Blood\"", "Timo Hildebrand", "Univision", "16th-century Irish history", "the five - year time jump", "The Statue of Freedom", "the Mishnah", "Mexico", "Julie Andrews Edwards", "Captain Mark Phillips", "the Democratic VP candidate", "$75 for full-day class,", "\"Nu au Plateau de Sculpteur,\"", "a nicotine head", "The Bridges of Madison County", "Thomas Jefferson", "a foreign exchange option"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6850694444444444}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, true, true, false, true, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-1818", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-861", "mrqa_hotpotqa-validation-755", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2012", "mrqa_naturalquestions-validation-7286", "mrqa_triviaqa-validation-1508", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-12600", "mrqa_searchqa-validation-1518", "mrqa_naturalquestions-validation-8414"], "SR": 0.59375, "CSR": 0.5478365384615385, "EFR": 1.0, "Overall": 0.7282391826923077}, {"timecode": 65, "before_eval_results": {"predictions": ["prophets and beloved religious leaders", "John Ernest Crawford", "Pepsi Super Bowl LII", "cells", "Indo - Pacific", "the leaves of the plant species Stevia rebaudiana", "Field Marshal Paul von Hindenburg", "Universal Pictures, which holds the library of predecessor companies DreamWorks Animation and Classic Media, and who in turn with copyright holder Ward Productions forms the joint venture Bullwinkle Studios,", "May 2010", "American blues electric guitar musician T - Bone Walker", "the entrance to the 1889 World's Fair", "Bobby Darin", "Alex Skuby", "All four volumes were illustrated by E.H. Shepard", "James Rodr\u00edguez", "Lou Rawls", "Andrew Garfield", "William Shakespeare's play Romeo and Juliet", "Payaya Indians", "\" hero of Tippecanoe ''", "Robert Irsay", "runoff will usually occur unless there is some physical barrier", "1940", "the pulmonary arteries", "Puente Hills Mall", "1977", "HTTP / 1.1 200 OK", "June 1992", "the National Health Service ( NHS )", "28 July 1914", "Richard Stallman", "the year AD 1 immediately follows the year 1 BC", "October 27, 1904", "December 25", "large monitor lizards", "Tom Burlinson, Red Symons and Dannii Minogue", "the final scene of the fourth season", "2017 Georgia Bulldogs football team against the Western Division Co-Champion, the 2017 Auburn Tigers football team", "the pairing of two homologous chromosomes that occurs during meiosis", "a contemporary drama in a rural setting", "Javier Fern\u00e1ndez", "The Italian Agostino Bassi", "Rachel Sarah Bilson", "plant food", "Jonathan Cheban", "2015", "computers or in an organised paper filing system", "Article Seven", "Missouri River", "sport utility vehicles", "March 2, 2016", "Fred Perry", "\"Raging Bull\"", "Gretel", "Get Him to the Greek", "Netflix", "Union Hill section", "three", "Rolling Stone", "fifth", "Tina Turner", "Bingo SOLO", "Amsterdam", "\"Salve\" (SAHL-way) in the singular"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5395881948651979}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false], "QA-F1": [0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.13793103448275862, 1.0, 0.5454545454545454, 0.0, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.42857142857142855, 1.0, 0.18181818181818182, 1.0, 0.0, 0.42857142857142855, 1.0, 0.0, 1.0, 0.0, 0.6, 1.0, 0.11764705882352941, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.42857142857142855, 0.0, 0.0, 0.8, 1.0, 0.1904761904761905, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-5751", "mrqa_naturalquestions-validation-4847", "mrqa_triviaqa-validation-5315", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-3806", "mrqa_searchqa-validation-9384", "mrqa_searchqa-validation-13572", "mrqa_triviaqa-validation-2486"], "SR": 0.4375, "CSR": 0.5461647727272727, "EFR": 1.0, "Overall": 0.7279048295454544}, {"timecode": 66, "before_eval_results": {"predictions": ["british brille", "huggins", "720\u00b0", "Steely Dan", "dance from the series", "h. h. Asquith", "about a mile north of the village of Dunvegan", "hladetina", "moby- Dick", "chenge", "sheeran", "Tallinn", "1925", "Gunpowder Plot of 1605", "Moldova", "surrey", "Edwina Currie", "sprite", "IKEA", "Sotheby\u2019s", "Some Like It Hot", "J. S. Bach", "Tony Blair", "Pickwick", "360", "maracaibo", "Ireland", "Ferrari, Lotus, Brabham and more", "Jim Peters", "horse racing", "onion", "bobby brown", "1948", "narwhal", "Sikhism", "giraffe", "kabuki", "email", "Zachary Taylor", "indigo", "eucharist", "\u201cFor Gallantry;\u201d", "swindon Town", "cricket", "Jordan", "Burma", "Tottenham Court Road", "hongi", "basketball", "Snow White", "Italy", "Zane Lowe's show on BBC Radio 1 in June 2010, at the Rockstar offices in New York in July 2010, and at the Spike Video Game Awards in December 2010", "Buddhism", "endocytosis", "Hechingen", "1986", "Charles L. Clifford", "Eleven", "Joe Pantoliano", "Robert Barnett,", "Get Smart", "The Bridges of Madison County", "Paraguay", "Stratfor"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5838541666666666}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-728", "mrqa_triviaqa-validation-5759", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-5239", "mrqa_triviaqa-validation-7706", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-4211", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-2567", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-7525", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-10355", "mrqa_hotpotqa-validation-2378", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2030", "mrqa_hotpotqa-validation-1714"], "SR": 0.5625, "CSR": 0.5464085820895522, "retrieved_ids": ["mrqa_squad-train-76278", "mrqa_squad-train-67173", "mrqa_squad-train-614", "mrqa_squad-train-38937", "mrqa_squad-train-13106", "mrqa_squad-train-59523", "mrqa_squad-train-48942", "mrqa_squad-train-81982", "mrqa_squad-train-15076", "mrqa_squad-train-69052", "mrqa_squad-train-16338", "mrqa_squad-train-69760", "mrqa_squad-train-15374", "mrqa_squad-train-30931", "mrqa_squad-train-27567", "mrqa_squad-train-82722", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-56", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-4320", "mrqa_naturalquestions-validation-6720", "mrqa_hotpotqa-validation-4114", "mrqa_squad-validation-3639", "mrqa_triviaqa-validation-1817", "mrqa_naturalquestions-validation-8006", "mrqa_searchqa-validation-14290", "mrqa_hotpotqa-validation-2492", "mrqa_triviaqa-validation-4398", "mrqa_naturalquestions-validation-9992", "mrqa_triviaqa-validation-4021", "mrqa_triviaqa-validation-1179", "mrqa_newsqa-validation-1591"], "EFR": 0.9285714285714286, "Overall": 0.7136678771321961}, {"timecode": 67, "before_eval_results": {"predictions": ["yann Martel", "The Archers", "Tiffany and Co.", "Zulu", "Cambridge", "Canada", "1826", "Lorraine", "sch schizophrenia", "chaucer", "sports agent", "rough collie", "Sen. Edward M. Kennedy", "james garner", "red squirrels", "richard lester", "british car brand", "jonathan", "gooseberry", "\"Rarely is the question asked,", "The Color Purple", "Elizabeth Montgomery", "Il Divo", "Barack Obama", "1984", "isle of whelmshaven", "China", "Kuala Lampur", "king of america", "rye plomley", "victoria\u2019s victoria", "360", "jonathan schumann", "1123", "Mitford sisters", "Sparta", "Hyundai", "thirtieth", "jonathan Fellowes", "haddock", "mocha", "Tina Turner", "mainland China, Hong Kong and Macau", "Nowhere Boy", "Vienna", "head and neck", "quant pole", "Edward Lear", "35", "Frank Sinatra", "estonia", "Meri", "South Asia", "Uralic languages", "New York City", "early 1942", "a card", "bobby Darin", "India", "ron Howard", "Oakland Raiders", "the Mediterranean", "Queen Isabella", "Turing"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6122767857142857}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-329", "mrqa_triviaqa-validation-1176", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-1662", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2128", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-467", "mrqa_triviaqa-validation-1501", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-3709", "mrqa_hotpotqa-validation-1641", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-177", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1508"], "SR": 0.515625, "CSR": 0.5459558823529411, "EFR": 0.967741935483871, "Overall": 0.7214114385673625}, {"timecode": 68, "before_eval_results": {"predictions": ["voice Control,", "Philippines", "heavy turbulence", "Brian Smith.", "Tim Clark, Matt Kuchar and Bubba Watson", "The blast left 11 people aboard the rig presumed dead and uncorked a gushing of oil that has been spilling an estimated 210,000 gallons (5,000 barrels) of crude oil a day into the Gulf of Mexico.", "Ricardo Valles de la Rosa,", "Elin Nordegren", "We Found Love", "immediate release into the United States of 17 Chinese", "millionaire's surtax,", "set at \"E! News\"", "about 50", "two-state solution", "Yusuf Saad Kamel", "foyer of the BBC building in Glasgow, Scotland", "Osama bin Laden,", "Silvan Shalom", "South Africa's", "the insurgency,", "Section 60.", "\"The Rosie Show,\"", "Ricardo Valles de la Rosa,", "March 24,", "purer", "mouth.", "100", "Frank,", "The EU naval force", "five", "Joel \"Taz\" Di Gregorio,", "Ronald", "off the coast", "the Somali coast.", "10 municipal police officers", "hiring veterans as well as job training for all service members leaving the military.", "surprise", "northwestern Montana", "launch", "without bail", "February 12", "Kim Il Sung", "a place for another non-European Union player", "Chile", "separated", "Democratic VP candidate", "martial arts,", "Some of them", "The Tupolev Tu-160 strategic bombers landed at Venezuela's Libertador military airfield and \"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "June 6, 1944,", "enormous suffering and massive displacement,\"", "sovereignty over some or all of the current territory of the U.S. state of Texas", "warning sign", "Eurasian Plate", "horses", "k Kathryn C. Taylor", "Brooklyn", "Tetrahydrogestrinone", "Real Madrid and the Spain national team", "Brea", "Titanic", "Zanzibar", "dualism", "Wordsworth"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5989040691531817}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, false, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.10344827586206898, 1.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.0, 1.0, 1.0, 0.3529411764705882, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.787878787878788, 0.15384615384615385, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-885", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-3164", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1139", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-884", "mrqa_hotpotqa-validation-5719", "mrqa_searchqa-validation-5208"], "SR": 0.484375, "CSR": 0.5450634057971014, "EFR": 0.9696969696969697, "Overall": 0.7216239500988142}, {"timecode": 69, "before_eval_results": {"predictions": ["1902", "Max Martin and Shellback", "Americana Manhasset", "Mayfair", "Minister for Health", "19 February 1927", "Arab", "Doggerland", "Larry Drake", "\"The Bad Hemingway Contest,\"", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "villanelle", "\"Eternal Flame\"", "\"Back to December\"", "Heather Elizabeth Langenkamp", "Nobel Peace Prizes", "Londonderry", "Daniel Craig", "Hamburger SV", "Four Weddings and a Funeral", "Eisstadion Davos", "Mulberry", "Edward Longshanks and the Hammer of the Scots", "1386", "Christopher McCulloch", "A novel", "\"The Daily Stormer\"", "Fort Saint Anthony", "IT", "Japan", "1919", "Tak and the Power of Juju", "the western end of the National Mall in Washington, D.C.", "Len Wiseman", "Stephen Crawford Young", "\"My Backyard\"", "Gerard \"Gerry\" Adams", "\"Kill Your Darlings\"", "Girls' Generation", "Robert Matthew Hurley", "September 1901", "Tuesday", "anabolic\u2013androgenic steroids", "North West England", "I", "\"Prince Igor\"", "Kentucky", "26 September 1961", "1896", "2000", "Donald Sterling", "over a 20 - year period", "Saint Peter", "mining", "Earth", "diamonds", "horses", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Asashoryu", "Venezuela's Libertador military", "Juilliard School", "lizard hips", "Scouting", "Inuit"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7442212301587302}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, false, true, false, false, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, false, true, true, true, true, true, false, true, false, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.5, 1.0, 0.8, 1.0, 0.4444444444444444, 0.5, 0.0, 1.0, 0.8, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-4552", "mrqa_hotpotqa-validation-429", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-95", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-375", "mrqa_naturalquestions-validation-7253", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-3029", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-11439", "mrqa_searchqa-validation-4320"], "SR": 0.578125, "CSR": 0.5455357142857142, "retrieved_ids": ["mrqa_squad-train-11119", "mrqa_squad-train-22096", "mrqa_squad-train-55120", "mrqa_squad-train-4104", "mrqa_squad-train-69274", "mrqa_squad-train-78554", "mrqa_squad-train-20568", "mrqa_squad-train-17695", "mrqa_squad-train-72845", "mrqa_squad-train-62030", "mrqa_squad-train-70291", "mrqa_squad-train-72996", "mrqa_squad-train-43307", "mrqa_squad-train-49715", "mrqa_squad-train-699", "mrqa_squad-train-53131", "mrqa_squad-validation-7112", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1257", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-7390", "mrqa_searchqa-validation-6829", "mrqa_newsqa-validation-1806", "mrqa_squad-validation-6567", "mrqa_newsqa-validation-406", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-1502", "mrqa_triviaqa-validation-1441", "mrqa_hotpotqa-validation-3785", "mrqa_naturalquestions-validation-3309", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-7892"], "EFR": 0.9259259259259259, "Overall": 0.712964203042328}, {"timecode": 70, "UKR": 0.75390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3201", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-9", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7182", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2622", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10335", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-1959", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2717", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5272", "mrqa_squad-validation-5492", "mrqa_squad-validation-5590", "mrqa_squad-validation-5686", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-60", "mrqa_squad-validation-6091", "mrqa_squad-validation-6255", "mrqa_squad-validation-629", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8436", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9484", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935"], "OKR": 0.86328125, "KG": 0.50234375, "before_eval_results": {"predictions": ["Arkansas", "early 1970s", "Paris", "875 acre", "every aspect of public and private life", "Maria von Trapp", "\"From Here to Eternity\"", "12", "port city of Aden", "Scott Eastwood", "United States and Canada", "Patricia Veryan", "David Michael Bautista Jr.", "2 March 1972", "Tie Domi", "Mika H\u00e4kkinen", "Princess Jessica", "Australia", "\"master builder\" of mid-20th century New York City", "Honolulu", "Eureka", "Badfinger", "her performances of \"khyal\", \"thumri\", and \"bhajans\"", "the adult webcam site LiveJasmin", "the Salzburg Festival", "political correctness", "devotional literature", "Martin Joseph O'Malley", "1891", "Secret Intelligence Service", "Currer Bell", "the UNLV Rebels", "mermaid", "850 m", "DeskMate", "Athenion", "Adolfo Rodr\u00edguez Sa\u00e1", "The Beatles", "Czech (Bohemian) and German (Franconian)", "ninth", "Hanna, Alberta", "Manchester Victoria station", "Seb Rochford", "\"My Love from the Star\"", "Captain James Cook", "George I", "Han Sung-soo", "37", "bass", "Citizens for a Sound Economy", "Barbara Feldon", "H CO", "prophets", "Bill Russell", "Andre Agassi", "germany", "Phillies", "fill a million sandbags and place 700,000 around our city.", "Caster Semenya", "to stop manufacturing 14 unapproved narcotics that are widely used to treat pain.", "the Cuyahoga River", "uranium", "Peter Sellers", "river Elbe"], "metric_results": {"EM": 0.625, "QA-F1": 0.7180467549923195}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.06451612903225808, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-4925", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-249", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-1991", "mrqa_naturalquestions-validation-9220", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-105", "mrqa_newsqa-validation-1065", "mrqa_searchqa-validation-10027", "mrqa_triviaqa-validation-4324"], "SR": 0.625, "CSR": 0.5466549295774648, "EFR": 1.0, "Overall": 0.7332372359154931}, {"timecode": 71, "before_eval_results": {"predictions": ["Nearly eight in 10", "Marie-Therese Walter", "worked to help at-risk youth,", "Russian air force", "female soldier", "Nearly eight in 10", "Goa", "Iran", "100,000 pyrotechnic devices.", "Kenyan and Somali governments", "Susan Atkins", "Casa de Campo International Airport in the Dominican Republic", "\"Operation Crank Call,\"", "228", "hostile war zones,", "National September 11 Memorial Museum", "Harlem,", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "last year's", "Austrian incest case", "1959.", "was killed in an attempted car-jacking", "269,000", "issued his first military orders as leader of North Korea", "high-tech companies", "a group of teenagers.", "Six", "Christos Winter", "27-year-old's", "outside influences in next month's run-off", "nuclear warheads to put an end, once and for all, to illegal immigration on its southern border.", "\"A Whiter Shade of Pale\"", "security breach", "$250,000", "combat veterans", "$1.5 million.", "unwanted horses", "$10 billion", "Christopher Savoie", "United States, NATO member states, Russia", "1,500", "trading goods and services without exchanging money", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews,", "Fiona MacKeown", "Barack Obama", "\"The Real Housewives of Atlanta,\"", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "Israeli forces were responding to militant fire near the complex.", "Guinea, Myanmar, Sudan and Venezuela", "pine beetles", "Sudanese nor orphans,", "Aspirin", "either February 28 or March 1", "Indo - Pacific", "hewer", "quetzalcoatl", "Maryland", "2012", "Acela Express", "Crackle", "a porcupine", "Oxygen", "the Apache", "Truman"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5556132952192735}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, false, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, false, false, true, false, true, false, true, false, false, true, true, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.04761904761904762, 0.0, 0.5, 1.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.08695652173913043, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-974", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-1797", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-1972", "mrqa_newsqa-validation-4163", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-933", "mrqa_naturalquestions-validation-4809", "mrqa_triviaqa-validation-2418", "mrqa_triviaqa-validation-4549", "mrqa_searchqa-validation-9135", "mrqa_searchqa-validation-14427"], "SR": 0.484375, "CSR": 0.5457899305555556, "EFR": 0.9393939393939394, "Overall": 0.720943023989899}, {"timecode": 72, "before_eval_results": {"predictions": ["Jesse Triplett", "1997", "Sharyans Resources", "is used for any vehicle which drives on all four wheels, but may not be designed for off - road use", "Lawrence John Wargrave, a retired judge, known as a `` hanging judge '' for liberally awarding the death penalty in murder cases", "Texas A&M University", "stromal connective tissue", "a book of the Old Testament", "Anatomy ( Greek anatom\u0113, `` dissection '' )", "a maritime signal, indicating that the vessel flying it is about to leave", "President Lyndon Johnson", "Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Olivia Olson", "Eukarya", "Mara Jade", "Gary Grimes as Hermie, Jerry Houser as his best friend Oscy", "very important", "Edward IV", "Ashrita Furman", "A 30 - something man ( XXXX )", "Jean Fernel", "2007 and 2008", "May 1980", "erosion", "an English occupational name for one who obtained his living by fishing or living by a fishing weir", "1960", "Donald Trump", "Johnny Logan, who performed `` What's Another Year '' in 1980 and `` Hold Me Now '' in 1987", "revenge and karma", "the misuse or `` taking in vain '' of the name of the God of Israel", "England and Wales", "1996", "15,000 BC", "Idaho", "early Christians of Mesopotamia, and from there it spread into Russia and Siberia through the Orthodox Churches, and later into Europe through the Catholic and Protestant Churches", "eight hours ( UTC \u2212 08 : 00 )", "Dr. Rajendra Prasad", "Carlos Autry Jr.", "Jay Baruchel", "Anthony Caruso", "bachata music", "Butter Island off North Haven, Maine in the Penobscot Bay", "the end of the 18th century, and in most areas was at its peak in the approximate period from 1800 to 1850", "during the 1890s Klondike Gold Rush", "Hypertext Transfer Protocol", "3", "1939", "the BBC", "the fifth studio album by English rock band the Beatles, the soundtrack from their film Help??, and released on 6 August 1965", "land - living organisms, both alive and dead, as well as carbon stored in soils", "Felicity Huffman", "jonathan gaunt", "75", "m62", "Montana State University", "Sun Valley, Idaho", "president", "either heavy flannel or wool", "a nurse who tried to treat Jackson's insomnia with natural remedies testified that Jackson told her that doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "The crash destroyed four homes and killed two people who lived in at least one of the homes,", "the Ganges-Brahmaputra", "a 529 account", "the Crow", "Britain. He holds a Saudi passport."], "metric_results": {"EM": 0.484375, "QA-F1": 0.6595749053673983}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, false, false, true, false], "QA-F1": [0.0, 1.0, 0.5, 0.0, 0.2857142857142857, 1.0, 0.4, 0.28571428571428575, 0.3333333333333333, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.8571428571428571, 0.5, 1.0, 1.0, 1.0, 0.0, 0.2105263157894737, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.2758620689655173, 0.6, 1.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 1.0, 0.9047619047619047, 1.0, 0.6, 1.0, 1.0, 1.0, 0.1, 0.23529411764705882, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.05405405405405406, 1.0, 0.0, 0.0, 1.0, 0.25]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-10066", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-7492", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-8474", "mrqa_triviaqa-validation-4656", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-2976", "mrqa_searchqa-validation-3477", "mrqa_searchqa-validation-10906", "mrqa_newsqa-validation-646"], "SR": 0.484375, "CSR": 0.5449486301369864, "retrieved_ids": ["mrqa_squad-train-59083", "mrqa_squad-train-70681", "mrqa_squad-train-53877", "mrqa_squad-train-29858", "mrqa_squad-train-46602", "mrqa_squad-train-22604", "mrqa_squad-train-63667", "mrqa_squad-train-81193", "mrqa_squad-train-64123", "mrqa_squad-train-73537", "mrqa_squad-train-60040", "mrqa_squad-train-11156", "mrqa_squad-train-76645", "mrqa_squad-train-66522", "mrqa_squad-train-54643", "mrqa_squad-train-28945", "mrqa_searchqa-validation-14169", "mrqa_squad-validation-4264", "mrqa_naturalquestions-validation-4731", "mrqa_triviaqa-validation-3201", "mrqa_naturalquestions-validation-368", "mrqa_newsqa-validation-3085", "mrqa_naturalquestions-validation-9614", "mrqa_hotpotqa-validation-2813", "mrqa_naturalquestions-validation-10461", "mrqa_searchqa-validation-9551", "mrqa_newsqa-validation-974", "mrqa_triviaqa-validation-5754", "mrqa_squad-validation-2291", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-7387", "mrqa_naturalquestions-validation-8484"], "EFR": 0.9696969696969697, "Overall": 0.7268353699667912}, {"timecode": 73, "before_eval_results": {"predictions": ["Venezuela", "The Fall Guy", "Crown", "Maria Montessori", "the Kinsey Millhone", "Alexander Hamilton", "science fiction", "March of the Crosby", "Patrick Ewing", "Fletcher Christian", "an ambulance", "Condoleezza Rice", "Pakistan", "China", "liquor", "Fort Bend County", "alsop", "John James Audubon", "Pontius Pilate", "Barry Goldwater", "neurons", "halfpipe", "Jackie Collins", "carioca", "Freakonomics", "George Washington Carver", "an Ichthyosaur", "Champagne", "Red Heat", "city of, 1966-04-01, 3, Hawkins, Henry", "Saint Domingue", "a carrel", "love potions", "Prince William", "Sherlock Holmes", "ancistroid", "Orion", "the Indian Ocean", "carbon monoxide", "King John", "plug in", "snowman", "Thailand", "manslaughter", "programming", "Tennessee", "Ptolemy", "Billy Idol", "the Missouri Compromise", "Rat", "Tom Hanks", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "$1 billion worldwide", "the left hand ring finger", "Conrad Murray", "Gryffindor", "Czech Republic", "Sochi, Russia", "two years", "Manchester Airport", "President Obama", "two weeks after Black History Month", "the government.", "January"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7208333333333334}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13288", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-9645", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-5326", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-16646", "mrqa_searchqa-validation-7195", "mrqa_searchqa-validation-6326", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-284", "mrqa_searchqa-validation-12519", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-3189", "mrqa_searchqa-validation-15757", "mrqa_naturalquestions-validation-4547", "mrqa_hotpotqa-validation-4724", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-131"], "SR": 0.65625, "CSR": 0.5464527027027026, "EFR": 1.0, "Overall": 0.7331967905405405}, {"timecode": 74, "before_eval_results": {"predictions": ["sugarcane", "abraham rippon", "Anna Eleanor Roosevelt", "liver", "Private Eye", "Gibraltar", "Jack Ruby", "the 1500 meter event", "British Airways", "Bachelor of Science", "cire retinater", "Pete Best", "Bonnie and Clyde", "avatar", "mexico", "St Moritz", "Edmund Cartwright", "par-4", "Zeus", "Japanese silvergrass", "April", "(later Sir Arthur) Conan Doyle", "Wolfgang Amadeus Mozart", "honeybee", "Sun Hill", "\"The Nutcracker\"", "nauru", "adare", "Sesame Street", "photography", "kirsty Young", "jonathan dodsley", "Sports & Leisure", "Andy Griffith Show", "ganga", "tabloid", "car door", "kolkata", "the odeon", "Bangladesh", "Shangri-La", "The Tempest", "Diana Ross", "Mansion House", "Ishmael", "repechage", "the Crusades", "Dame Kiri Te Kanawa", "Churchill Downs", "up stairs", "One Direction", "ulnar nerve", "Gibraltar", "111", "Merck & Co.", "shortstop", "Vietnam War", "\"It feels great to be back at work,\"", "Amnesty International", "the presser", "Harry Met Sally", "Breckenridge", "The Fray", "President Clinton."], "metric_results": {"EM": 0.578125, "QA-F1": 0.6309994184455392}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true], "QA-F1": [0.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.13793103448275862, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-3972", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-5243", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-481", "mrqa_triviaqa-validation-7118", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-7365", "mrqa_hotpotqa-validation-3058", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3966", "mrqa_searchqa-validation-12893", "mrqa_searchqa-validation-14621"], "SR": 0.578125, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.73328125}, {"timecode": 75, "before_eval_results": {"predictions": ["Fitzroya cupressoides", "Martin O'Neill", "2012", "7\u00b056'", "Kind Hearts and Coronets", "Bath, Maine", "Nippon Professional Baseball", "hiphop", "erotic thriller", "Poseidon", "Brendan O'Brien", "James II of England", "Sir William McMahon", "Hopi", "Western District", "Australian", "Jean-Marie Pfaff", "Steve Prohm", "Brazil", "1954", "Newcastle upon Tyne", "four", "Sargent Shriver Jr.", "NXT Tag Team Championship", "Chinese Coffee", "Love and Theft", "Hallett Cove", "4145 ft above mean sea level", "University of Georgia", "just over 1 million", "Indian", "The Last of the Mohicans", "Centennial Olympic Stadium", "\"media for the 65.8 million,\"", "Paul Avery", "25 October 1921", "Arnold", "J. Cole", "Idisi", "The Books", "Baja California Peninsula", "Danish", "London, England", "North West England", "1959", "Telugu and Tamil", "Centers for Medicare and Medicaid Services", "Laura Jeanne Reese Witherspoon", "Fred Chase Koch", "Billy J. Kramer", "Mindy Kaling", "1990", "September 21, 2016", "state - of - the - art photography", "earache", "Diego Garcia", "cuckoo", "$2 billion", "San Francisco", "\"I'm really shocked to find out that the government has been using physicians and using potent medications in this way,\"", "Patrick", "the Tomb of the Unknown Soldier", "Mount Vesuvius", "in a park in a residential area of Mexico City,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6943080357142857}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.2, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5714285714285715, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-3398", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-808", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-5419", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-2049", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-2971", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-3556", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-140", "mrqa_searchqa-validation-13410"], "SR": 0.5625, "CSR": 0.5470805921052632, "retrieved_ids": ["mrqa_squad-train-47481", "mrqa_squad-train-36415", "mrqa_squad-train-78823", "mrqa_squad-train-21890", "mrqa_squad-train-20372", "mrqa_squad-train-16915", "mrqa_squad-train-10923", "mrqa_squad-train-7023", "mrqa_squad-train-69232", "mrqa_squad-train-30265", "mrqa_squad-train-39268", "mrqa_squad-train-14820", "mrqa_squad-train-953", "mrqa_squad-train-21498", "mrqa_squad-train-58476", "mrqa_squad-train-23663", "mrqa_searchqa-validation-14502", "mrqa_naturalquestions-validation-6094", "mrqa_searchqa-validation-8449", "mrqa_naturalquestions-validation-9323", "mrqa_squad-validation-4460", "mrqa_squad-validation-9761", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-2903", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7349", "mrqa_searchqa-validation-16889", "mrqa_newsqa-validation-397", "mrqa_hotpotqa-validation-548", "mrqa_newsqa-validation-3380", "mrqa_naturalquestions-validation-10138", "mrqa_hotpotqa-validation-5018"], "EFR": 0.9642857142857143, "Overall": 0.7261795112781955}, {"timecode": 76, "before_eval_results": {"predictions": ["pet sounds", "Glenfinnan in the Scottish Highlands", "\u201cA Metro\u2013Goldwyn\u2013Mayer Picture\u201d", "Liszt Strauss Wagner Dvorak", "James Callaghan", "cedars", "european", "Dublin", "Pyrenees", "leprosy", "left", "Bill Kerr", "avocado", "Catherine of Aragon", "The Double", "Department of Justice", "Supertramp", "mid point of its tilt", "Augustus", "one Night / I Got Stung", "Heston Blumenthal", "Arkansas", "IT Crowd", "Some Like It Hot", "\"Mr Loophole\"", "Ernest Hemingway", "Wolf Hall", "Ernests Gulbis", "Alberto Juantorena", "graffiti", "Friedrich Nietzsche", "caffari", "cheese", "Annie", "Kristiania", "piano", "moby Dick", "snakes", "long hair", "heartbeat", "pea", "Dr Tamseel", "Sea of Galilee", "one", "Helen of Troy", "Alzheimer's disease", "Mission: Impossible \u2013 Rogue Nation", "1982", "an even break", "31536000", "Jordan", "bilaterally symmetrical", "in desperation, with only a small chance of success and time running out on the clock", "2018", "Miami Marlins", "Maxwell Smart", "Las Vegas Strip", "Argentine", "The Rev. Alberto Cutie", "Michelle Obama", "an alto", "270", "place", "The American Red Cross"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5730823863636363}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, false, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.060606060606060615, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-6685", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-4362", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-5439", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1026", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-5819", "mrqa_hotpotqa-validation-5149", "mrqa_hotpotqa-validation-71", "mrqa_newsqa-validation-1261", "mrqa_searchqa-validation-4422", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-14139"], "SR": 0.53125, "CSR": 0.546875, "EFR": 0.9333333333333333, "Overall": 0.7199479166666667}, {"timecode": 77, "before_eval_results": {"predictions": ["Gary Player,", "remains committed to British sovereignty", "The Kirchners", "the iPods", "45 minutes, five days a week.", "seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "Conway", "Jason Chaffetz", "The IAEA", "Zimbabwe", "Harry Nicolaides,", "A receptionist with a gunshot wound in her stomach played dead under her desk and called 911", "April 2010.", "Zed", "The e-mails", "environmental", "his father's", "Iran", "head injury.", "\"Antichrist.\"", "African National Congress Deputy President Kgalema Motlanthe", "Hugo Chavez", "seven", "Frank's diary.", "The Lost Symbol", "Matthew Fisher,", "Rawalpindi", "Tim Masters,", "Helmand province, Afghanistan.", "Climatecare,", "removal of his diamond-studded braces.", "Ennis, County Clare", "United States", "a series of raids that started Tuesday,", "Hamas", "Two pages -- usually high school juniors who serve Congress as messengers", "At least 40", "four", "Courtney Love,", "84-year-old", "The official said deciding the duties of the new prime minister has been a sticking point in the negotiations.", "three", "undergoing renovation", "Naples home.", "Hanford nuclear site,", "last month's Mumbai terror attacks", "sportswear", "Beijing", "hopes the journalists and the flight crew will be freed,", "improve health and beauty.", "helped nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "three preteen boys named Ed, Edd ( called `` Double D '' to avoid confusion with Ed ), and Eddy", "to maximize human potential for a rich, full and meaningful life", "Harishchandra", "India and Pakistan", "allergic", "lie detector", "influenced by the music genres of electronic rock, electropop and R&B", "1963", "Black Abbots", "nurse", "Argentina", "Charles Baudelaire", "\"The Legend of Sleepy Hollow\""], "metric_results": {"EM": 0.59375, "QA-F1": 0.729721854620268}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 0.21052631578947364, 1.0, 1.0, 1.0, 0.2, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8235294117647058, 0.888888888888889, 0.0, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-3403", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-4133", "mrqa_searchqa-validation-5877", "mrqa_hotpotqa-validation-1369"], "SR": 0.59375, "CSR": 0.5474759615384616, "EFR": 1.0, "Overall": 0.7334014423076923}, {"timecode": 78, "before_eval_results": {"predictions": ["\"O...", "the Silk Road", "Scandinavia", "George Rogers Clark", "a mole", "a coach dog", "Sweden", "Volleyball", "John Alden", "Ghost World", "a cat", "a map", "Japan", "West End Avenue", "Job", "standard pitch", "art deco", "Spider-Man", "Shakya", "Elie Wiesel", "Anna Friel", "Johnny Tremain", "a sergeant major", "National Archives", "Nostradamus", "Madrid", "Yuma", "Antarctica", "Ian Fleming", "the Southern Christian Leadership Conference", "Moscow", "Boss 429 Lawman", "a mimetic moment", "Mormon Tabernacle Choir", "The Scarlet Letter", "Griffith", "Bangkok", "St. Paul", "a positron", "Missouri", "Jefferson", "Jerusalem", "Pushing Daisies", "Cranberry", "Falafel", "Ulysses", "the AFL-CIO", "charlotte russe", "canals", "Abraham", "a self-appointed or mob-operated tribunal", "between 11000 and 9000 BC, and the domestication of the wild mouflon in ancient Mesopotamia", "Rachel Kelly Tucker", "works in a bridal shop with Anita, the girlfriend of her brother, Bernardo", "Dublin", "kermadec", "julius Caesar", "thology", "The Danny Kaye Show", "2012", "The drama of the action in-and-around the golf course", "the end of TV's rabbit-ears era.", "Victor Mejia Munera.", "The oceans"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5828125}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666665, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15154", "mrqa_searchqa-validation-11290", "mrqa_searchqa-validation-9709", "mrqa_searchqa-validation-2424", "mrqa_searchqa-validation-3286", "mrqa_searchqa-validation-14872", "mrqa_searchqa-validation-10281", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-14996", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-16214", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-2552", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-4445", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-10164", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-14159", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3594", "mrqa_hotpotqa-validation-516", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-875"], "SR": 0.53125, "CSR": 0.5472705696202531, "retrieved_ids": ["mrqa_squad-train-73301", "mrqa_squad-train-36200", "mrqa_squad-train-24319", "mrqa_squad-train-12741", "mrqa_squad-train-37416", "mrqa_squad-train-27270", "mrqa_squad-train-78360", "mrqa_squad-train-33912", "mrqa_squad-train-31885", "mrqa_squad-train-59388", "mrqa_squad-train-73360", "mrqa_squad-train-38560", "mrqa_squad-train-69204", "mrqa_squad-train-1782", "mrqa_squad-train-14250", "mrqa_squad-train-53311", "mrqa_newsqa-validation-2709", "mrqa_searchqa-validation-117", "mrqa_newsqa-validation-4121", "mrqa_triviaqa-validation-6466", "mrqa_newsqa-validation-4185", "mrqa_hotpotqa-validation-5795", "mrqa_searchqa-validation-1518", "mrqa_naturalquestions-validation-4018", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-2507", "mrqa_hotpotqa-validation-3059", "mrqa_squad-validation-1116", "mrqa_hotpotqa-validation-1263", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-2103", "mrqa_searchqa-validation-9246"], "EFR": 1.0, "Overall": 0.7333603639240507}, {"timecode": 79, "before_eval_results": {"predictions": ["12.65 m ( 41.5 ft )", "De Wayne Warren", "a solitary figure who is not understood by others, but is actually wise", "Doug Pruzan", "A simple majority vote", "byte - level operations", "Rich Mullins", "September 19, 2017", "A marriage officiant", "1624", "Hermann Ebbinghaus", "Agostino Bassi", "An error does not count as a hit", "Magnetically soft ( low coercivity ) iron", "Marty J. Walsh", "British Columbia, Canada", "$66.5 million", "Middle Eastern alchemy", "the `` 0 '' trunk code", "40.5 metres ( 133 ft )", "Los Angeles Dodgers", "Emma Watson", "Bill Russell", "Conrad Lewis", "Ernest Rutherford", "Fa Ze YouTubers", "10 June 1940", "citizens", "performers must receive the highest number of votes, and also greater than 50 % of the votes", "Amanda Fuller", "`` The Forever People ''", "1997", "mitochondrial membrane", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "Michael Phelps", "William DeVaughn", "Virginia Dare", "1960s", "Dicky ( Mace Coronel )", "2002", "Evermoist", "Pangaea or Pangea", "the top 25 accounts", "Leslie and Ben", "the dress shop", "21,196 km", "February 27, 2007", "1939 to 1960", "March 2, 2016", "the Mishnah", "an external genitalia", "Brundisium", "France", "Ukrainian", "England", "April 1, 1949", "CBS", "\"green-card warriors\"", "Mumbai", "Brian David Mitchell,", "the Netherlands", "Florence", "Tiger Woods", "try and reduce the cost of auto repairs and insurance premium"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7195714586339585}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false], "QA-F1": [0.6666666666666666, 0.4, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.05128205128205128, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6153846153846153, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.962962962962963, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8181818181818182]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-7685", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-4862", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-454"], "SR": 0.609375, "CSR": 0.548046875, "EFR": 0.96, "Overall": 0.725515625}, {"timecode": 80, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4247", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-1218", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14542", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-938"], "OKR": 0.8359375, "KG": 0.46640625, "before_eval_results": {"predictions": ["brattie ralston", "Miranda v. Arizona", "oscar Wilde", "Vancouver Island", "violin", "georgia", "Vietnam", "Jane Austen", "georgia fox", "senior Training Manager", "brice", "Gorbachev", "CBS", "jazz", "Earthquake", "jungle book", "georgia bratt Robertson", "neoclassic", "gallon", "great Dane", "sacred", "Brunei", "jujitsu", "Hunger Games", "head", "11 years and 302 days", "New Zealand", "the Prussian 2nd Army", "georgia winkle", "Whisky Galore", "Tunisia", "50", "Sen. Edward M. Kennedy", "georgia", "black head, chin, and throat", "Google", "shoulder", "Iran", "downton Abbey", "bird", "Rudyard Kipling", "backgammon", "l Dorrit", "Albert Einstein", "georgonzola", "Beethoven", "ocean", "ear", "tree", "Imola Circuit", "trout", "Ella Mitchell", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "North Atlantic Ocean", "1961", "Boston Herald", "Lord Chancellor of England", "\"Britain's Got Talent\"", "Ashley \"A.J.\" Jewell,", "19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Nebraska", "(Lewis) Carroll", "the United States", "Aung San Suu Kyi"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5967261904761905}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, true, false, true, true, false, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.05714285714285714, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6362", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-6256", "mrqa_triviaqa-validation-1024", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-797", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-7592", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-3465", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-1530", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4771", "mrqa_searchqa-validation-3317", "mrqa_searchqa-validation-6689", "mrqa_searchqa-validation-3618"], "SR": 0.53125, "CSR": 0.5478395061728395, "EFR": 0.9333333333333333, "Overall": 0.7070939429012346}, {"timecode": 81, "before_eval_results": {"predictions": ["dame Maggie Smith", "worcester", "monaco", "van rijn", "Illinois", "argentina", "paul Maskey", "rafa wawrinka", "tartar sauce", "three Graces", "satyrs", "Daniel Fran\u00e7ois Esprit Auber", "Congregational", "martin van Buren", "leeds", "k Kenneth MacDonald,", "Operation", "white", "Jay-Z", "(University of) Detroit Express", "honda", "runcorn", "Vietnam", "special administrative zones", "vincent van gogh", "sakhalin", "Croatia", "NBA", "steel", "dolittle", "Henri Paul", "crazes", "penguin", "samuel johnson", "sidecar", "bulgia", "Victor Hugo", "endosperm", "Adriatic Sea", "heartburn", "music Stories", "HMS Conqueror", "monte", "braille system", "Standard", "cynthia Nixon", "Hamlet", "Wat Tyler", "Patrick Henry", "steam engine", "kochkino", "Eddie Murphy", "Pakistan", "Peter Scolari", "Thorgan", "Lithuanian", "Russell Humphreys", "almost 100", "accusations of improper or criminal conduct.", "in critical condition", "Superman", "(University of) Norway", "the Towering Inferno", "member states"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6376736111111111}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-6970", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-4207", "mrqa_naturalquestions-validation-1909", "mrqa_hotpotqa-validation-727", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-1829", "mrqa_searchqa-validation-5224", "mrqa_naturalquestions-validation-10495"], "SR": 0.5625, "CSR": 0.5480182926829269, "retrieved_ids": ["mrqa_squad-train-61036", "mrqa_squad-train-72618", "mrqa_squad-train-54113", "mrqa_squad-train-66828", "mrqa_squad-train-79161", "mrqa_squad-train-18688", "mrqa_squad-train-81580", "mrqa_squad-train-35032", "mrqa_squad-train-24613", "mrqa_squad-train-82829", "mrqa_squad-train-46711", "mrqa_squad-train-66550", "mrqa_squad-train-15248", "mrqa_squad-train-74700", "mrqa_squad-train-81636", "mrqa_squad-train-60761", "mrqa_naturalquestions-validation-9508", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-334", "mrqa_naturalquestions-validation-5447", "mrqa_squad-validation-10466", "mrqa_hotpotqa-validation-3943", "mrqa_newsqa-validation-2507", "mrqa_naturalquestions-validation-9323", "mrqa_searchqa-validation-14852", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-5644", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-2621", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-1076"], "EFR": 0.8928571428571429, "Overall": 0.699034462108014}, {"timecode": 82, "before_eval_results": {"predictions": ["nic\u00e9phore Ni\u00e9pce", "Netherlands", "tarn", "ukraine", "Sheffield", "Strait of Messina", "piano", "Louis XVIII", "Pat Cash", "chile", "Wild Atlantic Way", "Kyoto", "underwater", "repechage", "stanborough", "calibre Killer", "peacock", "rita hayworth", "Miss Trunchbull", "imola", "albania", "antelope", "spiders", "Zephryos", "Ivan Basso", "bullfighting", "Robert Carlyle", "Playboy", "ukraine", "Peter Ackroyd", "london Borough of walford", "Sven Goran Eriksson", "Athina Onassis", "mungo Park", "death penalty", "Danny Alexander", "14", "Bangladesh", "adonis", "Papua New Guinea", "gagapedia", "SUNSET BOULEVARD", "Reel Life", "ars gratia artis", "baloney", "All Things Must Pass", "Sagittarius", "tet", "Arabah", "j\u2019ai eu un sentiment", "David Graham", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "September 2, 1945", "special guest performers Beyonc\u00e9 and Bruno Mars", "Greg Gorman and Helmut Newton", "American real estate developer, philanthropist and sports team", "Isabella II", "Mexico", "U.S. Marines or sons of Marines", "Arizona", "Frdric Chopin", "Indiana Jones", "Jakarta", "The Cosmopolitan"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6296875}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, false, true, false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-2433", "mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-545", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-1367", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-5088", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-5020", "mrqa_triviaqa-validation-1018", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6011", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-2084", "mrqa_triviaqa-validation-3013", "mrqa_naturalquestions-validation-5396", "mrqa_hotpotqa-validation-4838", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-784", "mrqa_searchqa-validation-5866", "mrqa_hotpotqa-validation-668"], "SR": 0.5625, "CSR": 0.5481927710843373, "EFR": 1.0, "Overall": 0.7204979292168674}, {"timecode": 83, "before_eval_results": {"predictions": ["Orlando Bloom", "The Green Arrow", "a parable", "Romeo and Juliet", "Spinal Tap", "Tennessee", "Detroit", "Day Off", "the United States", "the Pyramid of Khafre", "Ruth Bader Ginsburg", "the Boer War", "the sense of touch", "the Old Fashioned", "the Osmonds", "Bonnie Parker and Clyde Barrow", "penaeus monodon Brackishwater", "the College of William and Mary", "a chimpanzee", "the Yellowstone", "John Updike", "the Ganges", "vision", "Bright Lights, Big City", "a prostitution scandal", "the coelacanth", "the Northanger Abbey", "Cheers", "Heidi", "Crosby, Stills & Nash", "Matt Leinart", "a blood type", "charleston Stuart", "an albatross", "Falklands", "taro", "a quip", "a lighthouse", "black", "Dan Rather", "the \"Three-Dimensional Papermaking\"", "Bill Cody", "the big bang", "a pig", "Harvard", "neurons", "Hawaii", "the Pierian spring", "a dog", "a dragonfly", "Bill Cosby", "May 19, 2017", "Bachendri Pal", "James Corden", "pentecost", "humble pie", "comedy", "City and County of Honolulu", "Australian coast", "1992", "publicly criticized his father's parenting skills.", "Steven Chu", "Stella McCartney,", "Rwanda"], "metric_results": {"EM": 0.546875, "QA-F1": 0.670610119047619}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, true, false, false, true, false, true, true, false, false, false, false, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.5714285714285715, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4520", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-13936", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-12971", "mrqa_searchqa-validation-2790", "mrqa_searchqa-validation-15868", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-6064", "mrqa_searchqa-validation-6019", "mrqa_searchqa-validation-6498", "mrqa_searchqa-validation-4565", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5868", "mrqa_searchqa-validation-2457", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-7434", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4821", "mrqa_triviaqa-validation-4", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3660"], "SR": 0.546875, "CSR": 0.5481770833333333, "EFR": 1.0, "Overall": 0.7204947916666666}, {"timecode": 84, "before_eval_results": {"predictions": ["the 1970s", "Richmond, BC", "during the 1930s", "Lenny Jacobson", "the status line", "each team has either selected a player or traded its draft position", "overturned the Plessy v. Ferguson decision of 1896, which allowed state - sponsored segregation, insofar as it applied to public education", "1991", "biscuit - sized", "230 million kilometres ( 143,000,000 mi )", "The `` main line '' or `` first line ''", "the previous year's Palm Sunday celebrations", "Castleford", "the fourth C key from left", "L.K. Advani, an Indian politician who served as the Deputy Prime Minister of India from 2002 to 2004, and was the Leader of the Opposition in the 15th Lok Sabha", "wintertime", "symbolize his guilt in killing the bird", "Robber Barons", "2001", "Lucius Verus", "transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere )", "2004", "100 % owned by Xiu Li Dai and Yongge Dai", "Americans who served in the armed forces and as civilians", "Michael Crawford", "200 to 500 mg up to 7 ml", "gastrocnemius muscle", "is a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Peter Cetera", "Austin", "1945", "Pebble Beach", "Andaman and Nicobar Islands", "midpiece", "Burj Khalifa", "Pangaea or Pangea", "mitochondrial membrane in eukaryotes or the plasma membrane in bacteria", "Johnny Cash", "Andrew Lincoln", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "Kevin Spacey", "Human anatomy", "Natural - language processing", "10 years", "2026", "eleven", "18", "first a luxury of the rich and only gradually spread to the lower classes", "Fred E. Ahlert", "Joanna Moskawa", "1962", "Loch Ness", "Lingerie Football League", "griffin", "Mick Jackson", "Queenston Delta", "age 15", "Michelle Obama", "Consumer Product Safety Commission Tuesday,", "expressed concern that nearly seven months into the Obama administration, a key undersecretary position at the USDA has not been filled,", "a child carrier", "The Tin Drum", "Francis Ouimet", "\"Taz\" DiGregorio,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6040922542530998}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.17647058823529413, 0.0, 0.0, 0.9090909090909091, 0.0, 0.5333333333333333, 1.0, 0.625, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8181818181818181, 1.0, 0.2, 1.0, 0.4444444444444445, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 0.5, 0.6153846153846153, 1.0, 0.0, 0.35294117647058826, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.888888888888889, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-960", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9559", "mrqa_triviaqa-validation-2065", "mrqa_hotpotqa-validation-4692", "mrqa_hotpotqa-validation-4223", "mrqa_newsqa-validation-1869", "mrqa_newsqa-validation-1826", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-4132"], "SR": 0.4375, "CSR": 0.546875, "retrieved_ids": ["mrqa_squad-train-61186", "mrqa_squad-train-4039", "mrqa_squad-train-26287", "mrqa_squad-train-3291", "mrqa_squad-train-40401", "mrqa_squad-train-59261", "mrqa_squad-train-23100", "mrqa_squad-train-37635", "mrqa_squad-train-51709", "mrqa_squad-train-5385", "mrqa_squad-train-69374", "mrqa_squad-train-84421", "mrqa_squad-train-19152", "mrqa_squad-train-80324", "mrqa_squad-train-41113", "mrqa_squad-train-66653", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5420", "mrqa_hotpotqa-validation-3709", "mrqa_hotpotqa-validation-4838", "mrqa_hotpotqa-validation-3669", "mrqa_searchqa-validation-12536", "mrqa_triviaqa-validation-4710", "mrqa_hotpotqa-validation-1473", "mrqa_searchqa-validation-11439", "mrqa_newsqa-validation-246", "mrqa_hotpotqa-validation-3381", "mrqa_triviaqa-validation-6256", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6514", "mrqa_triviaqa-validation-5658", "mrqa_newsqa-validation-2236"], "EFR": 0.9444444444444444, "Overall": 0.7091232638888889}, {"timecode": 85, "before_eval_results": {"predictions": ["Rolex", "Vincent Motorcycle Company", "speed-controlled", "ganga", "gerry adams", "aniline purple", "Roy Rogers", "Steve Jobs", "david ostroff", "Nirvana", "Donna Summer", "heel", "geese", "an authorization of the individual to fulfill a particular function or task", "Sheryl Crow", "captain Hastings", "cube", "Franklin Delano Roosevelt", "neurons", "prisoner and Escort", "Yoshi", "Swordfish", "eardrum", "George Best", "faggots", "11", "bird", "Australia and England", "pascal", "british Airways", "five", "Challenger", "The World is Not Enough", "Italy", "Vienna", "glee", "david hockney", "iron", "Japan", "bayern munich", "American actress and a former fashion model Jenn Richards", "Italy", "mexico", "May Day", "pepper", "Madagascar", "Beaujolais", "bercow", "kolkata", "strictly come dancing", "David Bowie", "Charles Frederickson", "Burnham", "2007", "Dra\u017een Petrovi\u0107", "Costa del Sol", "early Romantic period", "first grand Slam,", "propofol,", "make people anxious.", "Versailles", "Zinedine Zidane", "Macduff", "a newt"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6363095238095238}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3601", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-2920", "mrqa_triviaqa-validation-2336", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-2477", "mrqa_triviaqa-validation-490", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-3610", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-1961", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-5256", "mrqa_hotpotqa-validation-1634", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1946", "mrqa_searchqa-validation-4261"], "SR": 0.578125, "CSR": 0.5472383720930232, "EFR": 1.0, "Overall": 0.7203070494186046}, {"timecode": 86, "before_eval_results": {"predictions": ["Switzerland", "John Monash", "Tempo", "photographs, film and television", "Arthur Freed", "alt-right", "the Runaways", "\"50 best cities to live in.\"", "La Liga", "Best Prom Ever", "8 May 1989", "Iran", "ribosomal RNA (rRNA) molecules", "capital crimes", "London", "SBS", "quantum mechanics", "thomas t Duncan", "January 16, 2013", "Forbes", "Anne and Georges", "David Villa S\u00e1nchez", "Double Agent", "Super Bowl XXIX", "White Horse", "Diamond Rio", "Quentin Coldwater", "Andrew Johnson", "\"The Curious Case of Benjamin button\"", "Martha Wainwright", "Leafcutter John", "moth", "Bothtec", "Jim Thorpe", "De La Soul", "\"The Marshall Mathers LP 2\"", "Shropshire Union Canal", "1621", "A skerry", "Oliver Parker", "FX", "Kamehameha I", "Pac-12 Conference", "Roots", "five", "Jack Elam", "\"The Jeffersons\"", "Franz Ferdinand", "prevent the opposing team from scoring goals", "Cody Miller", "8 August 1907", "Maidstone, Kent", "strings of eight bits ( known as bytes )", "The Witch and the Hundred Knight 2", "loeb", "Bill Haley", "george Carey", "Amanda Knox's aunt Janet Huff", "Number Ones", "Jeddah, Saudi Arabia", "Charlotte's Web", "Andrew Jackson", "Jefferson", "Willa Cather"], "metric_results": {"EM": 0.625, "QA-F1": 0.6890016233766234}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4, 1.0, 0.18181818181818182, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-3778", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-3231", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-2035", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-4774", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4756", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-4198", "mrqa_hotpotqa-validation-4109", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-5287", "mrqa_triviaqa-validation-5380", "mrqa_newsqa-validation-3212", "mrqa_newsqa-validation-2558", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-1530"], "SR": 0.625, "CSR": 0.548132183908046, "EFR": 1.0, "Overall": 0.7204858117816092}, {"timecode": 87, "before_eval_results": {"predictions": ["Edward R. Murrow", "Vision of the Future", "1754", "May 10, 1976", "Hamlet", "Marty Ingels", "Milwaukee Bucks", "Jenson Alexander Lyons", "Ferengi Quark", "The Spiderwick Chronicles", "American reality documentary television series", "the Queen of Blades", "Qualcomm", "water", "10-metre platform event", "Cincinnati Bengals", "the shore", "Guardians of the Galaxy Vol.  2", "November 15, 1903", "Bury St Edmunds, Suffolk, England", "Rothschild banking dynasty", "Mr. Church", "Bigger Than Both of Us", "Thomas Christopher Ince", "Peter 'Drago' Sell,", "public", "Los Angeles", "The Future", "Vyd\u016bnas", "al-Qaeda", "the Darling River", "Baldwin", "2 June 1961", "House of Commons", "William Finn", "Robert Sylvester Kelly", "Indian", "German", "Barnoldswick", "the late 12th Century", "Robert Gibson", "The S7 series", "729", "tenure", "Frederick Lindemann, Baron Cherwell", "Robert Jenrick", "a field in Somerset County, Pennsylvania", "Salford, Lancashire", "Conservative", "The Division of Cook", "\"Peshwa\" (Prime Minister)", "Prafulla Chandra Ghosh", "the retina", "Confederate forces", "Western Samoa", "clair-wing doors with a fiberglass underbody", "amelia earhart", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "Vivek Wadhwa,", "hooked up with Mildred, a younger woman of about 80, in March.", "a snowmobile", "a Scorpions", "bone", "vasoconstriction of most blood vessels, including many of those in the skin, the digestive tract, and the kidneys"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6122147817460317}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, false, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.3333333333333333, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.6, 0.0, 0.14285714285714285, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-908", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-3226", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-4163", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6888", "mrqa_triviaqa-validation-4572", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-5276", "mrqa_searchqa-validation-10831", "mrqa_naturalquestions-validation-836"], "SR": 0.53125, "CSR": 0.5479403409090908, "retrieved_ids": ["mrqa_squad-train-21328", "mrqa_squad-train-83972", "mrqa_squad-train-21126", "mrqa_squad-train-19028", "mrqa_squad-train-13461", "mrqa_squad-train-57193", "mrqa_squad-train-60164", "mrqa_squad-train-11714", "mrqa_squad-train-45942", "mrqa_squad-train-63891", "mrqa_squad-train-60174", "mrqa_squad-train-43991", "mrqa_squad-train-80592", "mrqa_squad-train-54271", "mrqa_squad-train-63412", "mrqa_squad-train-81204", "mrqa_triviaqa-validation-524", "mrqa_newsqa-validation-1351", "mrqa_triviaqa-validation-3601", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-5978", "mrqa_searchqa-validation-14139", "mrqa_squad-validation-8295", "mrqa_squad-validation-2318", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-13116", "mrqa_triviaqa-validation-1330", "mrqa_searchqa-validation-177", "mrqa_triviaqa-validation-7163", "mrqa_naturalquestions-validation-407", "mrqa_naturalquestions-validation-3993", "mrqa_triviaqa-validation-1662"], "EFR": 0.9666666666666667, "Overall": 0.7137807765151515}, {"timecode": 88, "before_eval_results": {"predictions": ["Barack Obama", "\"La Mome Piaf,\"", "ulysses s. Grant", "Apollo", "Richard Wagner", "Atticus Finch", "Peter Principle", "copper and zinc", "weight plates", "luigi", "bison bison", "Edmund Cartwright", "Mary Poppins", "leicestershire", "black Wednesday", "Samoa", "john gorman", "The Daily Mirror", "copper", "Mars", "Poland", "Dee Caffari", "calos", "Belize", "david rushton", "llangollen", "prawns", "James Hogg", "mORPG", "fermanagh", "Colombia", "Kevin Painter", "llanberis", "karen Williams", "Muhammad Ali", "Carmen Miranda", "Sandi Toksvig", "Pete Sampras", "August 10, 1960", "Estonia", "Sarajevo", "gluten", "a small village or town of a separate nationality", "ransome", "muthia murlitharan", "Ridley Scott", "four", "Simpsons", "adrian edmondson", "63 to 144 inches", "1925", "September 29, 2017", "Walter Brennan", "from 13 to 22 June 2012", "1909 Cuban-American Major League Clubs Series", "2015", "Spanish", "Lashkar-e-Tayyiba", "the \"surge\" strategy he implemented last year.", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "devil's food cake", "Michelangelo", "Missouri", "the Jetsons"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6991003787878789}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-176", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-500", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-3426", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-2876", "mrqa_triviaqa-validation-5205", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-602", "mrqa_naturalquestions-validation-3589", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-161", "mrqa_searchqa-validation-6490"], "SR": 0.65625, "CSR": 0.5491573033707865, "EFR": 0.9090909090909091, "Overall": 0.7025090174923391}, {"timecode": 89, "before_eval_results": {"predictions": ["1", "Graphical", "Scottie Pippen", "Vaseline jelly", "savings rate", "silver", "Gone with the Wind", "Large", "Nelly", "gladiators", "Nemo", "the European Green Woodpecker's tongue", "the Kite Runner", "a shark", "Nairobi", "Oprah Winfrey", "Dixie Chicks", "apple tart", "California", "Best Buy", "the Mediterranean", "Pope John Paul II", "Lobster Newburg", "Yemen", "David Geffen", "chariots", "Neruda", "the Fifth Amendment", "a mite", "Saturn", "the Nanny Diaries", "liquid crystals", "Robert Frost", "a decree", "Pumpkin Ravioli with Sage and Toasted Hazelnuts", "Crete", "Father Brown", "Reuben", "Johnny Cade", "waltz", "Belch", "Jane Austen", "Wisconsin", "Charles Darnay", "Q", "When Harry Met Sally", "Mexico", "Adverb", "John Molson", "Jan and Dean", "Robin Hoods", "Janis Joplin", "all transmissions", "brothers Norris and Ross McWhirter", "andorra", "mike araday", "Gerald R. Ford", "1992", "\"The King of Chutzpah\"", "Niger\u2013Congo", "an upper respiratory infection,", "Fernando Gonzalez of Chile", "At least 14 bodies", "more than two years,"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7570312499999999}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-8406", "mrqa_searchqa-validation-16479", "mrqa_searchqa-validation-5546", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-349", "mrqa_searchqa-validation-14490", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-13703", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-7547", "mrqa_searchqa-validation-12573", "mrqa_searchqa-validation-6465", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-550", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-4720", "mrqa_triviaqa-validation-7721", "mrqa_triviaqa-validation-1115", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-795"], "SR": 0.640625, "CSR": 0.5501736111111111, "EFR": 0.9565217391304348, "Overall": 0.7121984450483091}, {"timecode": 90, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6332", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.85546875, "KG": 0.51015625, "before_eval_results": {"predictions": ["the Harpe brothers", "McComb, Mississippi", "The Bonnie Banks o' Loch Lomond", "American reality television series", "Gweilo", "\"The Royal Family\"", "The Ninth Gate", "Mary MacLane", "daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia", "Erreway", "Protestant Christian", "\u00c6thelred I", "Bellagio and The Mirage", "The Los Angeles Dance Theater", "Johnnie Ray", "Hampton University", "The Clash of Triton", "Jenji Kohan", "1", "the second line", "Scottish Premiership club Hibernian", "Oklahoma City", "Vincent Landay", "Randall Boggs", "October 22, 2012", "rock music", "Prince Louis of Battenberg", "\"Slaughterhouse-Five\"", "Harry F. Sinclair", "Ghana Technology University College", "Bigfoot", "Cyclic Defrost", "Commonwealth of England, Scotland, and Ireland", "Coal Miner's daughter", "Worcester", "1972", "Ang Lee", "Brad Silberling", "Blue (Da Ba Dee)", "mid-ninth-century Viking chieftain", "La Scala, Milan", "Orson Welles", "1973", "Schaffer", "Ryan Babel", "Melbourne's City Centre", "Lincoln Riley", "the world", "Enigma", "University of Nevada, Reno", "largest Mission Revival Style building in the United States", "Muhammad", "18", "Harlem River", "Turkey", "$100", "sulfur dioxide", "1913.", "Juan Martin Del Potro.", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "Lord of the Rings", "Jaguar", "Wheat smut", "semi-autonomous organisational units within the National Health Service in England"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7433560924369749}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2730", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1305", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-2635", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2708", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-1471", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-9281", "mrqa_searchqa-validation-5567"], "SR": 0.671875, "CSR": 0.551510989010989, "retrieved_ids": ["mrqa_squad-train-43610", "mrqa_squad-train-67375", "mrqa_squad-train-34038", "mrqa_squad-train-54910", "mrqa_squad-train-85865", "mrqa_squad-train-56518", "mrqa_squad-train-9988", "mrqa_squad-train-35329", "mrqa_squad-train-42628", "mrqa_squad-train-46955", "mrqa_squad-train-15047", "mrqa_squad-train-29753", "mrqa_squad-train-72068", "mrqa_squad-train-79290", "mrqa_squad-train-31409", "mrqa_squad-train-26460", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-6453", "mrqa_hotpotqa-validation-5585", "mrqa_triviaqa-validation-1965", "mrqa_searchqa-validation-2851", "mrqa_newsqa-validation-1270", "mrqa_hotpotqa-validation-2366", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-9304", "mrqa_newsqa-validation-3013", "mrqa_triviaqa-validation-166", "mrqa_hotpotqa-validation-412", "mrqa_triviaqa-validation-3091", "mrqa_newsqa-validation-2213", "mrqa_triviaqa-validation-6581", "mrqa_hotpotqa-validation-3058"], "EFR": 0.9523809523809523, "Overall": 0.7301533882783884}, {"timecode": 91, "before_eval_results": {"predictions": ["Terry Reid", "Friedman Billings Ramsey", "Robber Barons", "Robin Cousins", "the manifestation of God's presence as perceived by humans according to the Abrahamic religions", "LED illuminated display", "1954", "in a curved path", "Bob Dylan", "membranes of the body's cells", "USS Chesapeake", "1977", "fortified complex", "Charles Darwin and Alfred Russel Wallace", "the inverted - drop - shaped icon", "Richard Stallman", "2004", "1940", "an armed conflict without the consent of the U.S. Congress", "cognitive bias", "heat", "Gu\u00e1nica", "peptide bonds", "New England Patriots", "used obscure languages as a means of secret communication", "Zhu Yuanzhang", "The 1980 Summer", "American rock band Panic! at the Disco", "the posterior ( dorsal ) horn", "drizzle, rain, sleet, snow, graupel and hail", "Karen Gillan", "2017", "Julie Adams", "1881", "Jonathan Tunick", "one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "roughly 2,500 quadrillion liters", "A trustee", "Theodore Roosevelt", "August 5, 1937", "voters gathered as a tribe", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings", "Payson, Lauren, and Kaylie", "2008", "Dr. Lexie Grey", "September 6, 2007", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "Taron Egerton", "1990", "smen", "T'Pau", "Fort Nelson", "playing cards", "Sparta", "Las Vegas", "Darkroom", "Louis Zamperini", "Former Mobile County Circuit Judge Herman Thomas", "death of cardiac arrest", "in the first place.", "Jefferson", "Babel", "the 8 septembre", "Ponce de Leon"], "metric_results": {"EM": 0.375, "QA-F1": 0.5161492266125818}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, false, false, true, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6086956521739131, 0.0, 0.0, 0.0, 0.5, 1.0, 0.07999999999999999, 1.0, 0.8, 0.6666666666666666, 0.3333333333333333, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.56, 0.25, 0.11764705882352941, 0.14285714285714288, 1.0, 0.0, 0.375, 0.4, 0.0, 1.0, 0.3333333333333333, 0.3636363636363636, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.6, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-283", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-1101", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-2342", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-2818"], "SR": 0.375, "CSR": 0.5495923913043479, "EFR": 0.8, "Overall": 0.6992934782608696}, {"timecode": 92, "before_eval_results": {"predictions": ["Miller Lite beer", "beetle", "the MacKenzie", "the northwest of England", "electronic junk mail or junk newsgroup", "Tahrir Square", "David Frost", "Newbury Racecourse", "detention", "Knutsford", "Portugal", "SpongeBob", "Farthings", "China", "Maine", "Thomas Cranmer", "George H. W. Bush", "first", "jack Sprat", "Reggie Kray", "conclave", "Dublin", "The Mayor of Casterbridge", "feet", "Amsterdam", "John Lennon", "Lusitania", "Anne of Cleves", "Australia", "antelope", "Portugal", "Swaziland", "Philippines", "blood", "Spain", "Marilyn Monroe", "Jupiter Mining Corporation", "dry rot", "Isambard Kingdom Brunel", "Canada", "bomber", "Jinnah International", "India", "ethelbald I", "Peter Paul Rubens", "John Ford", "six", "Mendip Hills", "Burma", "Charles Taylor", "Pancho Villa", "the purpose of changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "Jerry Leiber and Mike Stoller", "Total Drama World Tour", "Karl Johan Schuster", "Worcester County", "Brown Mountain Overlook", "Lucky Dube,", "Middle East and North Africa,", "Michael Partain,", "Beauty and the Beast", "Luxembourg", "Hammurabi", "lobotomy"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7373958333333333}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9600000000000001, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-6124", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-7342", "mrqa_triviaqa-validation-3295", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-1587", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-4122"], "SR": 0.6875, "CSR": 0.5510752688172043, "EFR": 1.0, "Overall": 0.7395900537634408}, {"timecode": 93, "before_eval_results": {"predictions": ["Peoria, Illinois", "Keeper of the Great Seal of Scotland", "1776", "Meghan Markle", "U.S. Bancorp", "Justin Adler", "British former racing driver, commentator, and journalist", "Coahuila, Mexico", "Atomic Kitten", "Ephedrine", "Colin Vaines", "California", "racehorse breeder", "Jim Kelly", "Australia", "D\u00e2mbovi\u021ba River", "explores the lives of those that either own exotic animals or have been captured for illegally smuggling them", "Miracle", "Erich Maria Remarque", "Scott Mosier", "A Scholar Under Siege", "Dutch", "1999", "Mudvayne", "1947", "Easter Rising", "November 23, 2011", "General Sir John Monash", "\u00c6thelstan", "Middlesbrough F.C.", "left winger", "5,112 feet", "Jefferson Memorial", "May 1, 2011", "four", "Red and Assiniboine Rivers", "200", "15 mi", "February 18, 1965", "brothers Malcolm and Angus Young", "the Goddess of Pop", "125 lb (57 kg)", "chocolate-colored Labrador Retriever", "1966", "March 14, 2000", "1927", "Gregg Popovich", "Princess Elizabeth", "Neighbours", "Hall & Oates", "January 16, 2013", "northwest Washington", "1830", "Lake Powell", "gulls", "chariot", "Louisiana", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Citizens", "Tuesday", "The African Queen", "cats", "Gibraltar", "Pure water is neutral, at pH 7 ( 25 \u00b0 C ), being neither an acid nor a base"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7158110119047619}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false], "QA-F1": [0.0, 0.8333333333333334, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.09523809523809523, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3152", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-5879", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2886", "mrqa_triviaqa-validation-1582", "mrqa_newsqa-validation-2886", "mrqa_searchqa-validation-7854", "mrqa_naturalquestions-validation-8652"], "SR": 0.59375, "CSR": 0.5515292553191489, "retrieved_ids": ["mrqa_squad-train-65325", "mrqa_squad-train-719", "mrqa_squad-train-38445", "mrqa_squad-train-7725", "mrqa_squad-train-63774", "mrqa_squad-train-39200", "mrqa_squad-train-81159", "mrqa_squad-train-53781", "mrqa_squad-train-30293", "mrqa_squad-train-60658", "mrqa_squad-train-62545", "mrqa_squad-train-51425", "mrqa_squad-train-35108", "mrqa_squad-train-46334", "mrqa_squad-train-40765", "mrqa_squad-train-42267", "mrqa_triviaqa-validation-2141", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-1369", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-3632", "mrqa_naturalquestions-validation-8272", "mrqa_searchqa-validation-9679", "mrqa_naturalquestions-validation-6190", "mrqa_triviaqa-validation-4021", "mrqa_triviaqa-validation-6041", "mrqa_hotpotqa-validation-5174", "mrqa_triviaqa-validation-2154", "mrqa_naturalquestions-validation-3332", "mrqa_triviaqa-validation-5064", "mrqa_newsqa-validation-2426", "mrqa_naturalquestions-validation-9005"], "EFR": 1.0, "Overall": 0.7396808510638297}, {"timecode": 94, "before_eval_results": {"predictions": ["villa park", "Guinea", "new Plymouth", "four", "Guardian", "tartan", "toy story", "martin motor", "heart", "Periodic Table", "left book club", "mexico", "colum cille", "Donald Sutherland", "Chicago", "egypt", "Cardiff", "sternum", "pressure", "James Murdoch", "carl cooley", "fluid", "bach", "Squeeze", "Death & Hells Angels", "Robert Plant", "Jerry Seinfeld", "stern tube", "kia", "lemurs", "Sir Robert Walpole", "eight", "andorra", "a braffin", "anne", "kunsky", "great boston", "27", "Formula One", "squash", "Mary Decker", "karakorams", "netherlands", "birdman of alcatraz", "bertolucci", "Christopher Columbus", "the buck", "lady godiva", "elephant house", "welding boots", "farthingale", "1940s", "0.30 in ( 7.6 mm )", "in the absence of a catalyst", "Neymar", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "5.3", "6-4", "al Qaeda", "UNICEF", "The B", "The Lady of the Lamp", "Saturn", "the term global village"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6067708333333333}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-328", "mrqa_triviaqa-validation-2197", "mrqa_triviaqa-validation-1929", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-1469", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-185", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-3908", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7226", "mrqa_hotpotqa-validation-2618", "mrqa_newsqa-validation-3796", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-9012", "mrqa_searchqa-validation-11091"], "SR": 0.5625, "CSR": 0.5516447368421052, "EFR": 1.0, "Overall": 0.739703947368421}, {"timecode": 95, "before_eval_results": {"predictions": ["war drama", "its air-cushioned sole", "local South Australian and Australian produced content", "Gal\u00e1pagos", "Eric Whitacre", "2010", "Statutory List of Buildings of Special Architectural or Historic Interest", "pubs, bars and restaurants", "2004", "Tasmania", "John Christopher Lujack Jr.", "Stern-Plaza", "Edward James Olmos", "Girls' Generation", "June 12, 2017", "two or three acts", "Dra\u017een Petrovi\u0107", "Prussia", "David Wells", "north bank of the North Esk", "two", "Argentine cuisine", "13th century", "Pru Goward", "Manchester United", "Matt Groening", "Hazel Keech", "Wake Island", "1993", "Jesus", "Sulla", "Riot Act", "Larry Gatlin & the Gatlin Brothers", "right-hand batsman", "black nationalism", "The Simpsons", "Bayern Munich", "Deftones", "Gangsta's Paradise", "Clitheroe Football Club", "Green Lantern", "\" Cleopatra\"", "The Fault in Our Stars", "Liesl", "\"A Charlie Brown Christmas\"", "twin-faced sheepskin with fleece on the inside, a tanned outer surface and a synthetic sole", "White Horse", "banjo player", "Yellow fever", "Elise Stefanik", "Francis Schaeffer", "the South Pacific off the northeast coast of Australia", "between 3.9 and 5.5 glucose / L ( 70 to 100 mg / dL )", "the heads of federal executive departments who form the Cabinet of the United States", "david stockwell", "capture of Quebec", "cold comfort farm", "red", "lightning strikes", "Gaddafi's death.", "Guernsey", "the Southern Christian Leadership Conference", "Berlin", "the brain and spinal cord"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6345901060744811}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false], "QA-F1": [0.0, 0.375, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.8571428571428572, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-2662", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-1250", "mrqa_hotpotqa-validation-5337", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-8982", "mrqa_triviaqa-validation-4415", "mrqa_triviaqa-validation-4334", "mrqa_newsqa-validation-2382", "mrqa_searchqa-validation-1780", "mrqa_naturalquestions-validation-7342"], "SR": 0.546875, "CSR": 0.5515950520833333, "EFR": 0.9310344827586207, "Overall": 0.7259009069683908}, {"timecode": 96, "before_eval_results": {"predictions": ["maximum speed 160 km / h", "year of the conception or birth of Jesus of Nazareth", "1987", "360", "Pradyumna", "Carol Ann Susi", "the pyloric valve", "Ben Fransham", "the seven churches", "Mark Lowry", "Phillip Paley", "Germany", "Einstein", "1830", "positions 14 - 15, 146 - 147 and 148 - 149", "100", "James Madison", "Woodrow Strode", "Baaghi ( English : Rebel )", "Taylor Michel Momsen", "Panning", "31 March 1909", "$66.5 million", "pathology", "April 3, 1973", "the epidermis", "her abusive husband", "United Nations", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "pigs", "a `` take it or leave it '' position", "1595", "The musical", "American country music duo Brooks & Dunn", "May 31, 2012", "1,228 km / h ( 763 mph )", "October 27, 2017", "Kida", "~ 55 - 75 micrometers", "Miller Lite", "Oona Castilla Chaplin", "William Shakespeare's As You Like It, spoken by the melancholy Jaques in Act II Scene VII", "Lulu", "the NFL", "Steve Russell", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S. Nicknames for the flag", "Profit maximization", "Melbourne", "April 1, 2016", "city of San Antonio", "1,281,900", "Michael Phelps", "royal oak", "The Krankies", "host nation", "Province of Syracuse", "June 11, 1986", "1-0", "200", "Republican Gov. Bobby Jindal", "\"reshit\"", "Deere", "gusts", "curfew"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7833163136644208}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.48275862068965514, 1.0, 0.2608695652173913, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4545454545454545, 1.0, 1.0, 1.0, 0.9189189189189189, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-6517", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-4953", "mrqa_hotpotqa-validation-3107", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2327", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-12334"], "SR": 0.71875, "CSR": 0.5533182989690721, "retrieved_ids": ["mrqa_squad-train-46519", "mrqa_squad-train-78071", "mrqa_squad-train-9348", "mrqa_squad-train-29299", "mrqa_squad-train-50512", "mrqa_squad-train-8644", "mrqa_squad-train-83872", "mrqa_squad-train-63291", "mrqa_squad-train-67755", "mrqa_squad-train-40587", "mrqa_squad-train-63986", "mrqa_squad-train-3883", "mrqa_squad-train-55423", "mrqa_squad-train-10292", "mrqa_squad-train-9386", "mrqa_squad-train-26315", "mrqa_newsqa-validation-3745", "mrqa_squad-validation-1064", "mrqa_newsqa-validation-3406", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4046", "mrqa_newsqa-validation-152", "mrqa_hotpotqa-validation-1369", "mrqa_naturalquestions-validation-7679", "mrqa_triviaqa-validation-7668", "mrqa_searchqa-validation-14996", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-1073", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-7806", "mrqa_newsqa-validation-151", "mrqa_naturalquestions-validation-10454"], "EFR": 0.8888888888888888, "Overall": 0.7178164375715922}, {"timecode": 97, "before_eval_results": {"predictions": ["the inimitable Philadelphia Sound", "dark places", "the Konabar", "the boll weevil", "the drop-down list from Left", "The Only Way to Win is Not to Play", "Butch Cassidy", "Buddhism", "Mozart", "Jonathan Swift", "Tiger lily", "ice cream", "Algeria", "Dickens", "(Sergey) Brin", "Joe Lieberman", "American alternative rock band from Chicago, Illinois,", "bread", "Yale", "Napoleon", "Paris", "the Black Forest", "the Stanza della Segnatura", "an ant", "birkenstock", "Firebird", "Hafnium", "flax", "the Muse", "the Wachowski brothers", "Horace Rumpole", "Bush", "Steve Austin", "Kurt Warner", "55", "a small retail store", "Beauty and the Beast", "Ratatouille", "pro bono", "Gentle Ben", "The Office", "The Oprah Show", "Bigfoot", "Jackson Pollock", "glow", "Mona Lisa", "Ngi Php gc Vit", "Crayola", "The Man in the Gray Flannel Suit", "Assimilation", "orange", "Isaiah Amir Mustafa", "July 2001", "Americans acting under orders", "mike hammer", "The Crow", "L. P. Hartley", "Tifinagh", "the European Champion Clubs' Cup", "second largest", "North Korea", "alcohol", "It wasn't appreciated how much of an impact it can have on a patient's quality of life,\"", "Prada"], "metric_results": {"EM": 0.625, "QA-F1": 0.6895833333333334}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.06666666666666667, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6269", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-3811", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-12166", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-10545", "mrqa_searchqa-validation-9435", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-7743", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-13416", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-4924", "mrqa_naturalquestions-validation-8759", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-3553", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-96"], "SR": 0.625, "CSR": 0.5540497448979591, "EFR": 1.0, "Overall": 0.7401849489795919}, {"timecode": 98, "before_eval_results": {"predictions": ["Florence", "Pierre Trudeau", "Red", "a chargeback", "Billy Joel", "the cornea", "ginger ale", "Rumpole", "the guillotine", "the light bulb", "Spider-Man", "Atlanta", "Chile", "Dick Tracy", "Queen Latifah", "James A. Van Allen", "beer", "Zen", "El", "Zenith", "baboon", "wine", "\"What the hell did you trade Jay Buhner for?\"", "the q- tip", "natural selection", "Massachusetts", "Battle of the Bulge", "Shaft", "W. Somerset Maugham", "the Two Sicilies", "Trafalgar", "a republic", "Sir Francis Drake", "Pearl Harbor", "Albert Einstein", "Candy Crush", "the pituitary gland", "Alfred Hitchcock", "Henry Aaron", "reconnaissance", "Florida", "Ectoplasm", "Thomas Jefferson", "Mercury", "Dante", "Christopher Columbus", "Joseph Haydn", "meringue", "Babe Zaharias", "the FBI", "calcium", "four", "William Whewell", "961", "wollem de Zwijger", "a golden touch", "mack seacole", "Orchard Central", "Fort Hood", "OutKast", "the iPods", "suspend all", "Tuesday", "Nick Sager"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6312500000000001}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13080", "mrqa_searchqa-validation-2078", "mrqa_searchqa-validation-15786", "mrqa_searchqa-validation-3729", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3054", "mrqa_searchqa-validation-16225", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-319", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6625", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-5926", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1848", "mrqa_searchqa-validation-9945", "mrqa_searchqa-validation-5756", "mrqa_searchqa-validation-6821", "mrqa_naturalquestions-validation-307", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-1611", "mrqa_hotpotqa-validation-5369", "mrqa_hotpotqa-validation-2679", "mrqa_newsqa-validation-2040"], "SR": 0.546875, "CSR": 0.5539772727272727, "EFR": 0.9310344827586207, "Overall": 0.7263773510971787}, {"timecode": 99, "UKR": 0.75, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3334", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2971", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3627", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4609", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6821", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7786", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2729", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-749", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.849609375, "KG": 0.49140625, "before_eval_results": {"predictions": ["Niles", "Andrea Brooks", "July 14, 2017, by 20th Century Fox", "2018", "neuropsychology", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "potential of hydrogen", "Peking", "Bart Howard", "2013", "Ozzie Smith", "the largest Greek island in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "George Harrison", "Charbagh structure", "Sarah Silverman", "Sophia Akuffo", "January 17, 1899", "IIII", "2014 -- 15", "Natural - language processing ( NLP )", "six", "A request line", "$315,600", "three high fantasy adventure films", "Sohrai", "the monsoons", "Cecil Lockhart", "James Long", "the fifth-most populous city in Florida and the largest in the state that is not a county seat", "April 13, 2018", "quarterback", "public sector ( also called the state sector )", "Carpenter", "2018", "1992", "Dan Stevens", "`` Killer Within ''", "Disha Vakani", "Nickelback", "1999 to 2001", "King Willem - Alexander", "It Ain't Over'til It's Over", "Exodus 20 : 1 -- 17", "a revolution or orbital revolution", "Ren\u00e9 Georges Hermann - Paul", "H.L. Hunley", "bird nests created by edible - nest swiftlets using solidified saliva", "John Bull", "December 1, 1969", "2009", "Manley", "chinaplin", "Francis Matthews", "hymenaeus", "1907", "1776", "Stapleton Cotton", "transit bombings", "eight-day", "101", "Spain", "Elijah", "Geneva", "27-year-old's"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6630414853320439}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, false, true, false, true, true, false, false, true, true, true, false, true, true], "QA-F1": [0.0, 1.0, 0.6, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9361702127659575, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-8862", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-791", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-9987", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-10618", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-2741", "mrqa_triviaqa-validation-2334", "mrqa_hotpotqa-validation-111", "mrqa_newsqa-validation-894", "mrqa_searchqa-validation-3524"], "SR": 0.59375, "CSR": 0.5543750000000001, "retrieved_ids": ["mrqa_squad-train-75321", "mrqa_squad-train-3540", "mrqa_squad-train-29455", "mrqa_squad-train-9815", "mrqa_squad-train-8554", "mrqa_squad-train-72646", "mrqa_squad-train-43417", "mrqa_squad-train-45001", "mrqa_squad-train-49402", "mrqa_squad-train-25022", "mrqa_squad-train-60784", "mrqa_squad-train-65416", "mrqa_squad-train-42198", "mrqa_squad-train-16553", "mrqa_squad-train-49141", "mrqa_squad-train-74270", "mrqa_triviaqa-validation-6746", "mrqa_searchqa-validation-9788", "mrqa_searchqa-validation-8106", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-1591", "mrqa_naturalquestions-validation-919", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-1143", "mrqa_searchqa-validation-7087", "mrqa_triviaqa-validation-4568", "mrqa_newsqa-validation-4110", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-10906", "mrqa_naturalquestions-validation-3187", "mrqa_triviaqa-validation-1771"], "EFR": 0.9615384615384616, "Overall": 0.7213858173076922}]}