{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5320, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["high cost injectable, oral, infused, or inhaled medications", "a plastid that lacks chlorophyll", "Observations on the Geology of the United States", "1887", "2000", "gain support from China", "the south", "push", "New England Patriots", "A cylindrical Service Module", "gold", "Fermat primality test", "highly diversified", "WWSB and WOTV", "the end itself", "Chen's theorem", "La Rochelle", "Fort Caroline", "around half", "the move from the manufacturing sector to the service sector", "1.7 billion years ago", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "July 18, 2006", "electromagnetic force", "Robert Bork", "East Smithfield burial site in England", "non-violent", "John Houghton", "Enthusiastic teachers", "high voltage", "Johann Walter", "Shoushi Li", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "priest", "business districts", "BankAmericard", "Bruno Mars", "Jamukha", "German New Guinea", "Onon", "good, clear laws", "the International Stanis\u0142aw Moniuszko Vocal Competition", "forces", "Factory Project", "2010", "fundraising drives", "1000 CE", "Van Nuys Airport", "overinflated", "basic design typical of Eastern bloc countries", "the tax rate", "sequential proteolytic activation of complement molecules", "customs of his tribe", "Robert Guiscard", "wide sidewalks", "CBS Sports.com", "the March Battle of Fort Bull", "a rendezvous", "6 feet 2 inches", "formalism", "the sale of indulgences", "the English Court of Appeal, the German Bundesgerichtshof, the Belgian Cour du travail", "British failures in North America", "Besan\u00e7on Hugues"], "metric_results": {"EM": 0.75, "QA-F1": 0.7846657363104732}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.2, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.3636363636363636, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6393", "mrqa_squad-validation-8452", "mrqa_squad-validation-5", "mrqa_squad-validation-6091", "mrqa_squad-validation-7382", "mrqa_squad-validation-9489", "mrqa_squad-validation-10483", "mrqa_squad-validation-4902", "mrqa_squad-validation-2145", "mrqa_squad-validation-7430", "mrqa_squad-validation-680", "mrqa_squad-validation-9896", "mrqa_squad-validation-6645", "mrqa_squad-validation-6072", "mrqa_squad-validation-525", "mrqa_squad-validation-4361"], "SR": 0.75, "CSR": 0.75, "EFR": 0.9375, "Overall": 0.84375}, {"timecode": 1, "before_eval_results": {"predictions": ["fast forwarding", "applications such as on-line betting, financial applications", "San Jose State", "DeMarcus Ware", "two poles", "the Presiding Officer", "1206", "high fuel prices and new competition from low-cost air services", "lens-shaped", "Regis Philbin", "defensins", "Sweden", "linebacker", "the Calvin cycle", "ships", "Archbishop of Westminster", "coherent theory", "\"Roentgen rays\" or \"X-Rays\"", "Fridays", "M\u00e9ni\u00e8re's disease, vertigo, fainting, tinnitus, and a cataract in one eye", "Oahu", "1784", "William of Volpiano and John of Ravenna", "yellow fever outbreaks", "the Philippines", "$125 per month", "in any other group of chloroplasts", "Abercynon", "Michael Weinenberger", "only \"essentials\"", "a pointless pursuit", "the United Nations", "plug-n-play", "Roone Arledge", "driving them in front of the army", "business districts", "1726", "lower rates of social goods", "main hymn", "France", "extinction", "ABC Entertainment Group", "the 17th century", "the degree to which these flags retain their original colors remains unknown", "T cells", "1080i HD", "the state", "30 July 1891", "Inherited wealth", "the journal Science", "administration", "elected by citizens", "Trypanosoma brucei", "Falls", "1975", "over half", "1835", "France", "a mutualistic relationship", "its initial home range spanning from Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "Principal photography began on November 2, 2016", "The song was written by Mitch Murray", "Parashara", "1958"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7884011243386243}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.14814814814814817, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_squad-validation-5758", "mrqa_squad-validation-10427", "mrqa_squad-validation-1504", "mrqa_squad-validation-2506", "mrqa_squad-validation-8662", "mrqa_squad-validation-4240", "mrqa_squad-validation-7571", "mrqa_squad-validation-4206", "mrqa_squad-validation-7457", "mrqa_squad-validation-8576", "mrqa_squad-validation-3922", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3942", "mrqa_hotpotqa-validation-5465"], "SR": 0.734375, "CSR": 0.7421875, "EFR": 1.0, "Overall": 0.87109375}, {"timecode": 2, "before_eval_results": {"predictions": ["magnetic field", "photosynthetic function", "Egyptians", "gold", "fund travelers who would come back with tales of their discoveries", "reactive allotrope of oxygen", "his personal goals with his academic goals", "ABC Circle Films", "Jews", "Kaifeng", "passion", "Metropolitan Statistical Areas", "European Union law", "monophyletic", "\"Provisional Registration\"", "biochemical oxygen demand", "electrical repair jobs", "hospitals and other institutions", "gold", "1998", "160 kPa", "The General Board of Church and Society, and the United Methodist Women", "prevented it from being cut down", "lab monitoring, adherence counseling, and assist patients with cost-containment strategies", "St. Johns River", "The increasing use of technology, specifically the rise of the internet over the past decade", "10 years", "Genghis Khan", "HIV", "1857", "Rijn", "Caris & Co.", "Stage 2", "\u00d6gedei", "breaches of law in protest against international organizations and foreign governments", "Anglo-Saxon language of their subjects", "two populations of rodents", "The Deadly Assassin and Mawdryn undead", "Dave Logan", "the top row of windows", "fast forwarding", "The Dornbirner Ach", "combustion chamber", "a gift", "104 \u00b0F (40 \u00b0C)", "strict", "the property owner", "Muslim Iberia", "1913", "patient compliance issues", "20th century", "ambiguity", "\"Bells\" was introduced by Bob Hope in the 1951 movie The Lemon Drop Kid", "the culture of maiko, who replace the... white one upon becoming one of these | a geisha.", "The Man and the Secrets", "The Sky This Week for September 2 to September 11", "Carefully paddling down this Congolese river that lends its name to a deadly virus", "The 10 dog breeds with the best sense of smell - Dogtime  Basset Hound", "The Dardanelles formerly known as Hellespont is a narrow, natural strait and internationally", "\"Guilt by Association\"", "half the northbound cars wait 90 minutes", "The Viking Ship Museum (Roskilde)", "James Edward Kelly", "2 March 1972"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7470238095238095}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.4, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.5333333333333333, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714285, 0.14285714285714288, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 0.4, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3497", "mrqa_squad-validation-1875", "mrqa_squad-validation-1308", "mrqa_squad-validation-3692", "mrqa_squad-validation-7013", "mrqa_squad-validation-1880", "mrqa_squad-validation-6244", "mrqa_squad-validation-1108", "mrqa_squad-validation-7162", "mrqa_squad-validation-1808", "mrqa_squad-validation-6361", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16877", "mrqa_searchqa-validation-3385", "mrqa_hotpotqa-validation-1393"], "SR": 0.65625, "CSR": 0.7135416666666667, "EFR": 0.9545454545454546, "Overall": 0.8340435606060607}, {"timecode": 3, "before_eval_results": {"predictions": ["a strange odor in their spacesuits", "Muqali", "inversely to member state size", "if they are distinct or equal classes", "1884", "Isaac Komnenos", "the printing press", "1997", "June 6, 1951", "Marshall Cohen", "1.7 billion years ago", "a not-for-profit United States computer networking consortium", "contemporary accounts were exaggerations", "residency registration", "Tower District", "individual state laws", "October 2007", "Moscone Center", "Voice in the Wilderness", "September 1944", "\u015ar\u00f3dmie\u015bcie", "burning a mixture of acetylene and compressed O2", "9.6%", "Commander", "macrophages and lymphocytes", "kill Luther", "his son Duncan", "\"an idealized and systematized version of conservative tribal village customs\" under the label of Sharia", "the Dongshan Dafo Dian", "Jean Cauvin", "220 miles", "\"Blue Harvest\" and \"420\"", "Thomas Commerford Martin", "oxygen from building up in them and disrupting rubisco activity. Because of this, they lack thylakoids organized into grana stacks", "\"The Book of Roger\"", "the Earth's surface", "Africa", "Pierre Bayle", "confirmed and amended", "32.9%", "30\u201360%", "1368\u20131644", "reduction gears", "Pedro Men\u00e9ndez de Avil\u00e9s", "a liquid oxygen tank exploded", "$105 billion", "1688\u20131692", "He currently plays for AFC.  Walkington in the Premier Division of the East Riding League, where his ex-teammate Leigh Palin is the manager.", "Parlophone", "Super Bowl XXIX", "The Number Twelve", "end of the 18th century", "Tulsa", "26,788", "Richa Sharma", "Stage Stores", "25 laps", "to be identified as transgender", "672 km2", "Boston and Maine Railroad's Southern Division", "Dusty Dvoracek", "He is telling me to regain the trust of those customers who are driving our vehicles", "Himalayan", "murder"], "metric_results": {"EM": 0.75, "QA-F1": 0.8208655621339445}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.8181818181818181, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.09523809523809523, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.09523809523809523, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4210", "mrqa_squad-validation-3456", "mrqa_squad-validation-4019", "mrqa_squad-validation-1116", "mrqa_squad-validation-9740", "mrqa_squad-validation-4631", "mrqa_squad-validation-8872", "mrqa_squad-validation-10413", "mrqa_squad-validation-3370", "mrqa_hotpotqa-validation-1964", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-5251", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-3564"], "SR": 0.75, "CSR": 0.72265625, "retrieved_ids": ["mrqa_squad-train-63866", "mrqa_squad-train-16199", "mrqa_squad-train-39012", "mrqa_squad-train-59420", "mrqa_squad-train-19484", "mrqa_squad-train-40266", "mrqa_squad-train-38228", "mrqa_squad-train-7017", "mrqa_squad-train-22970", "mrqa_squad-train-41780", "mrqa_squad-train-63816", "mrqa_squad-train-64556", "mrqa_squad-train-11816", "mrqa_squad-train-15940", "mrqa_squad-train-54844", "mrqa_squad-train-84571", "mrqa_squad-validation-4206", "mrqa_searchqa-validation-7896", "mrqa_squad-validation-1808", "mrqa_squad-validation-7430", "mrqa_squad-validation-3922", "mrqa_squad-validation-3692", "mrqa_squad-validation-8452", "mrqa_squad-validation-9896", "mrqa_squad-validation-6361", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-15243", "mrqa_squad-validation-6091", "mrqa_searchqa-validation-12371", "mrqa_naturalquestions-validation-7393", "mrqa_squad-validation-7571", "mrqa_squad-validation-8662"], "EFR": 0.9375, "Overall": 0.830078125}, {"timecode": 4, "before_eval_results": {"predictions": ["consultant", "reformers", "Modern English", "Commission v Italy", "the West", "1893", "demand for a Scottish Parliament", "1881", "1421", "W. E. B. Du Bois", "between 25-minute episodes", "captive import policy", "15th century", "two", "two", "a pivotal event", "Mexico", "Black Sea", "a single output", "The Central Region", "Battle of Fort Bull", "Murray Gold and Ben Foster", "ambiguity", "Super Bowl XLIV", "Urarina", "a global scale", "force model", "lost in the 5th Avenue laboratory fire of March 1895", "Westwood One", "free", "Independence Day: Resurgence", "issues related to the substance of the statement", "1763\u20131775", "classical position variables", "512-bit", "Deabolis", "necessity", "adenosine triphosphate", "cartels", "Hughes Hotel", "88", "8 November 2010", "Jean Baptiste Say", "The Perfect Storm", "Terry & June", "architectural equivalent of the Nobel Prize", "arrows", "the common mole", "a complex number raised to the zero power", "Mikhail Gorbachev", "Good Will Hunting", "Quentin Blake", "The History Boys", "a valid passport", "\"caliper\"", "a neutron", "James Hoban", "elia Earhart", "1963", "cricket bat making process", "Asuka", "The United States of America", "iPods", "Charles M. Schulz"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7453125}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-2437", "mrqa_squad-validation-9334", "mrqa_squad-validation-7708", "mrqa_squad-validation-6197", "mrqa_squad-validation-7537", "mrqa_squad-validation-10466", "mrqa_squad-validation-603", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-873", "mrqa_searchqa-validation-4355"], "SR": 0.671875, "CSR": 0.7125, "EFR": 1.0, "Overall": 0.85625}, {"timecode": 5, "before_eval_results": {"predictions": ["7:00 to 9:00 a.m.", "ammed", "vaccination", "62 acres", "Maciot de Bethencourt", "Spain", "C. J. Anderson", "Cam Newton", "eastwards", "accessory pigments that override the chlorophylls' green colors", "his last statement", "Pleistocene epoch", "he published his findings first", "Nurses", "time and space complexity", "1951", "the Marches", "black earth", "Nederrijn", "opposite end from the mouth", "Buddhist sculptures", "the mid-sixties", "the Kuznets curve hypothesis", "chloroplast's existence", "Schr\u00f6dinger equation", "90\u00b0 out of phase with each other", "anticlines", "Tanaghrisson", "Siegfried", "Sydney", "220 miles (350 km)", "Northern San Diego", "Video On Demand content", "Genghis Khan", "Arizona Cardinals", "Pleurobrachia", "chloroplast's stroma", "operations requiring constant speed", "10 November 2017", "baeocystin", "\"Krabby Road\"", "music from its German-dominated style of the 19th century", "England", "2009", "Ella Fitzgerald", "sarod", "1981", "Kris Kristofferson", "Nia Sanchez", "German", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Fran", "Al Bundy", "Kristine Leahy", "Odisha", "Fat Albert", "Frontline", "lady", "Tom Kartsotis", "modern genetics", "one person trained to pilot, navigate, or otherwise participate as a crew member of a spacecraft", "it appears that Ali Baashi was also specifically targeted by gunmen", "cirrocumulus", "an independent homeland for the country's ethnic Tamil minority"], "metric_results": {"EM": 0.53125, "QA-F1": 0.630599868881119}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 0.25, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.8, 1.0, 0.5, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.45454545454545453, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8333333333333333]}}, "before_error_ids": ["mrqa_squad-validation-9029", "mrqa_squad-validation-1064", "mrqa_squad-validation-9176", "mrqa_squad-validation-5450", "mrqa_squad-validation-8755", "mrqa_squad-validation-10386", "mrqa_squad-validation-3257", "mrqa_squad-validation-5112", "mrqa_squad-validation-6044", "mrqa_squad-validation-4458", "mrqa_squad-validation-8900", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-5526", "mrqa_naturalquestions-validation-3663", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-539", "mrqa_searchqa-validation-1523", "mrqa_newsqa-validation-1718"], "SR": 0.53125, "CSR": 0.6822916666666667, "EFR": 0.9666666666666667, "Overall": 0.8244791666666667}, {"timecode": 6, "before_eval_results": {"predictions": ["2010", "for the adaptive immune system to mount faster and stronger attacks each time this pathogen is encountered", "Calvin cycle", "Zhenjin", "education and training", "June 11, 1962", "Jean-Claude Juncker", "68,511", "for voters were supposed to line up behind their favoured candidates instead of a secret ballot.", "1880", "8 mm cine film", "Pittsburgh", "the seal of the Federal Communications Commission", "\u00a3250,000", "Michael Jayston", "radiography", "Norway", "the courts of member states", "Texas", "shortening the cutoff", "12.5 acres", "a few hundred feet", "an innate force of impetus", "the Conservative Party", "an international data communications network", "the environment in which they lived", "Darian Stewart", "the Great Fire of London", "acular", "the Moscone Center in San Francisco", "The View and The Chew", "Parliament of the United Kingdom at Westminster", "preventing it from being cut down", "baptism", "London", "one hundred pennies", "a coffee house", "Parkinson's disease", "Tintin", "piu forte (piu f)", "1", "West Germany", "McKinney", "Spock", "Solomon", "Blackstar", "geomorphology", "Earth", "kurkama", "Richmond in North Yorkshire", "The Passenger Pigeon", "Richard Wagner", "false teeth", "Debbie Rowe", "Russia", "1973", "Duck Soup", "London", "Jane Thompson", "Southaven, Mississippi", "Papua province", "Jesus Walks", "Paul Biya", "for people to be able to buy things or get things"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6418389724310778}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.3157894736842105, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6680", "mrqa_squad-validation-6284", "mrqa_squad-validation-4297", "mrqa_squad-validation-8295", "mrqa_squad-validation-89", "mrqa_squad-validation-512", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-4197", "mrqa_naturalquestions-validation-8889", "mrqa_newsqa-validation-3541", "mrqa_searchqa-validation-13016", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-714"], "SR": 0.578125, "CSR": 0.6674107142857143, "retrieved_ids": ["mrqa_squad-train-84588", "mrqa_squad-train-18641", "mrqa_squad-train-79227", "mrqa_squad-train-15475", "mrqa_squad-train-62950", "mrqa_squad-train-64828", "mrqa_squad-train-29467", "mrqa_squad-train-30141", "mrqa_squad-train-8214", "mrqa_squad-train-63348", "mrqa_squad-train-63001", "mrqa_squad-train-37884", "mrqa_squad-train-1097", "mrqa_squad-train-40802", "mrqa_squad-train-65182", "mrqa_squad-train-74797", "mrqa_squad-validation-2506", "mrqa_searchqa-validation-15243", "mrqa_squad-validation-7537", "mrqa_hotpotqa-validation-5101", "mrqa_triviaqa-validation-2758", "mrqa_squad-validation-7457", "mrqa_squad-validation-7013", "mrqa_squad-validation-5450", "mrqa_searchqa-validation-13651", "mrqa_squad-validation-4361", "mrqa_squad-validation-4019", "mrqa_naturalquestions-validation-5672", "mrqa_squad-validation-7708", "mrqa_squad-validation-8662", "mrqa_squad-validation-8900", "mrqa_squad-validation-809"], "EFR": 1.0, "Overall": 0.8337053571428572}, {"timecode": 7, "before_eval_results": {"predictions": ["ten times their own weight", "Cape of Good Hope", "Time magazine", "length of the Rhine", "12", "150", "North American Aviation", "supervised and managed", "the Sovereign", "weakness in school discipline", "Fort Caroline", "Distributed Adaptive Message Block Switching", "at elevated partial pressures", "His lab was torn down", "interacting", "Omnicare, Kindred Healthcare and PharMerica", "Tiffany & Co.", "conservative", "forts Shirley had erected at the Oneida carry", "swimming-plates", "eleven", "it would undermine the law", "1332", "separately from physicians", "the south", "Geordie", "fuel", "US$10 a week", "the harvests of their Chinese tenants", "142 pounds", "1806-07", "british", "j. ross moore", "Bill Clinton", "a police car", "Dead Man's Curve", "Edward Waverley", "bosnia", "neptune", "british", "Rookwood", "Prada", "Edward R. Murrow", "british", "jedoublen", "british", "british", "british", "british", "Christopher Marlowe", "4GB", "congruent", "Domenico Colombo", "jostled against Esau", "jon frusciante", "the battle for this port", "The Federal", "eight", "Jane Eyre", "World War II", "Hussein's Revolutionary Command Council", "The meter reader", "cowardly lion", "March 22"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5871031746031746}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9109", "mrqa_squad-validation-8602", "mrqa_squad-validation-6324", "mrqa_squad-validation-2097", "mrqa_squad-validation-10251", "mrqa_squad-validation-6773", "mrqa_squad-validation-3483", "mrqa_squad-validation-1272", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-8040", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-4533", "mrqa_searchqa-validation-14514", "mrqa_newsqa-validation-858"], "SR": 0.5625, "CSR": 0.654296875, "EFR": 1.0, "Overall": 0.8271484375}, {"timecode": 8, "before_eval_results": {"predictions": ["a flour mill Boulton & Watt were building", "every four years", "Alan Turing", "2\u20133 years", "coordinating lead authors", "effectiveness of treatment regimens", "43 million tons", "720p high definition", "Denver", "Singing Revolution", "The Newlywed Game", "1641\u20131679", "the usual counterflow cycle", "pattern recognition receptors", "severe reduced rainfall and increased temperatures", "Glucocorticoids", "The Late Show", "entertainers", "Newcastle Student Radio", "immunoglobulins and T cell receptors", "the City council", "Doctor Who", "November 1979", "linear", "breaches of law in protest against international organizations and foreign governments", "Cobham's", "Sir Edward Poynter", "Behind the Sofa", "the Simien Mountains", "Florida State University", "the malleus", "Mao Zedong", "Arroz con leche", "Hawaii", "the Kiwanis Club", "the log cabin", "the jazzy horns", "a tornado", "the Chateau", "the J", "the Clinica Regina Margherita", "the dizygotic type of twins", "the DASH Diet", "Hawaii", "lox", "neurotransmitters", "the gas molecules are not connected", "the Princess Diaries", "B Teresaola", "Massachusetts", "the larynx", "John Galt", "April", "cinnamomum", "the right angle", "Kentucky", "the War Hawks", "the Chinese Exclusion Act", "a jonathan tree", "1953", "Harry Nicolaides", "Mineola", "Blender", "2018\u201319 UEFA Europa League group stage"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6309895833333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3373", "mrqa_squad-validation-3415", "mrqa_squad-validation-3310", "mrqa_squad-validation-4357", "mrqa_squad-validation-434", "mrqa_squad-validation-5374", "mrqa_squad-validation-7959", "mrqa_squad-validation-8747", "mrqa_searchqa-validation-16960", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5814", "mrqa_naturalquestions-validation-10012", "mrqa_triviaqa-validation-4730", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-1263"], "SR": 0.5625, "CSR": 0.6440972222222222, "EFR": 1.0, "Overall": 0.8220486111111112}, {"timecode": 9, "before_eval_results": {"predictions": ["Holyrood area of Edinburgh", "Dutch law said only people established in the Netherlands could give legal advice", "terra nullius", "assisting in fabricating evidence or committing perjury", "kicker", "relativity", "Red Turban rebels", "Jurassic Period of the Mesozoic Era", "Fort Presque Isle (near present-day Erie, Pennsylvania) on Lake Erie's south shore", "William S. Paley", "anaerobic bacteria", "more sunlight in deep water", "eicosanoids and cytokines", "live", "50-yard line", "heard her songs; he followed the fishermen and captured the mermaid.", "1/6", "DC traction motor", "the \"richest 1 percent in the United States now own more wealth than the bottom 90 percent\"", "the divinity of Jesus", "EastEnders", "J. S. Bach", "highest", "a few drops", "1882", "Mel Jones", "North America", "Sachin Tendulkar and Kumar Sangakkara", "Coton in the Elms", "inversely proportional to the wave frequency", "flytrap", "last Ice Age", "Allison Janney", "2026", "Georgia", "for the life and safety of others as to amount to a crime and deserve punishment", "1984 Summer Olympics in Los Angeles", "4 September 1936", "Andrew Moray and William Wallace", "Andrea Brooks", "Pangaea or Pangea", "Have I Told You Lately ''", "through the right atrium to the atrioventricular node, along the Bundle of His and through bundle branches", "the fourth quarter of the preceding year", "2013 non-fiction book of the same name by David Finkel", "prevent further offense by convincing the offenders that their conduct was wrong", "Bob Dylan", "September of that year", "judges", "Lynda Carter as Wonder Woman / Diana Prince and Lyle Waggoner as Steve Trevor Sr. & Jr.", "virtually no limit to the number of reads from such flash memory", "A substitute good", "September 27, 2017", "President Gerald Ford", "Monk's Caf\u00e9", "Robert Duncan McNeill -- Kevin Corrigan   Anthony De Longis -- Blade   Tony Carroll -- Beast Man   Pons Maar -- Saurod   Robert Towers -- Karg   Peter Brooks", "Tintin", "Alaska", "over 140 million", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "People Against Switching Sides", "around 3.5 percent of global greenhouse emissions", "Billy Budd, Billy Budd", "En banc"], "metric_results": {"EM": 0.515625, "QA-F1": 0.694836507169373}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, false, true, false, true, false, true, false, false, true, false, true, false, false, true, true, false, false, false, true, true, false, false, false, true, false, false], "QA-F1": [0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.7368421052631579, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.14814814814814814, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.717948717948718, 0.4444444444444444, 1.0, 0.8333333333333333, 1.0, 0.5, 1.0, 0.125, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.23529411764705882, 0.1142857142857143, 1.0, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 0.8, 0.15999999999999998, 0.888888888888889, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-9764", "mrqa_squad-validation-9325", "mrqa_squad-validation-10204", "mrqa_squad-validation-8596", "mrqa_squad-validation-805", "mrqa_squad-validation-7459", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-3481", "mrqa_newsqa-validation-1319", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-12968"], "SR": 0.515625, "CSR": 0.63125, "retrieved_ids": ["mrqa_squad-train-30289", "mrqa_squad-train-56166", "mrqa_squad-train-68616", "mrqa_squad-train-7136", "mrqa_squad-train-69967", "mrqa_squad-train-70736", "mrqa_squad-train-30464", "mrqa_squad-train-58914", "mrqa_squad-train-24038", "mrqa_squad-train-74590", "mrqa_squad-train-12368", "mrqa_squad-train-75768", "mrqa_squad-train-29653", "mrqa_squad-train-32409", "mrqa_squad-train-19265", "mrqa_squad-train-84761", "mrqa_searchqa-validation-2499", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-3080", "mrqa_newsqa-validation-1577", "mrqa_squad-validation-9896", "mrqa_searchqa-validation-12243", "mrqa_squad-validation-10251", "mrqa_searchqa-validation-455", "mrqa_hotpotqa-validation-1964", "mrqa_squad-validation-3370", "mrqa_hotpotqa-validation-171", "mrqa_searchqa-validation-14307", "mrqa_triviaqa-validation-4730", "mrqa_squad-validation-603", "mrqa_triviaqa-validation-1064", "mrqa_searchqa-validation-12649"], "EFR": 0.9354838709677419, "Overall": 0.7833669354838709}, {"timecode": 10, "UKR": 0.7890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1124", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-956", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10305", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-5814", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-10067", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10127", "mrqa_squad-validation-10204", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-1023", "mrqa_squad-validation-10251", "mrqa_squad-validation-10251", "mrqa_squad-validation-10260", "mrqa_squad-validation-10287", "mrqa_squad-validation-10351", "mrqa_squad-validation-10386", "mrqa_squad-validation-10387", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-10504", "mrqa_squad-validation-1051", "mrqa_squad-validation-1064", "mrqa_squad-validation-1071", "mrqa_squad-validation-1078", "mrqa_squad-validation-1104", "mrqa_squad-validation-1108", "mrqa_squad-validation-1108", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1142", "mrqa_squad-validation-1181", "mrqa_squad-validation-1236", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1282", "mrqa_squad-validation-1301", "mrqa_squad-validation-1308", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1378", "mrqa_squad-validation-1401", "mrqa_squad-validation-1461", "mrqa_squad-validation-1504", "mrqa_squad-validation-1506", "mrqa_squad-validation-1552", "mrqa_squad-validation-1553", "mrqa_squad-validation-1554", "mrqa_squad-validation-159", "mrqa_squad-validation-1601", "mrqa_squad-validation-1636", "mrqa_squad-validation-1706", "mrqa_squad-validation-1780", "mrqa_squad-validation-1808", "mrqa_squad-validation-1813", "mrqa_squad-validation-1831", "mrqa_squad-validation-1856", "mrqa_squad-validation-1875", "mrqa_squad-validation-1880", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-2040", "mrqa_squad-validation-2069", "mrqa_squad-validation-2097", "mrqa_squad-validation-2135", "mrqa_squad-validation-2145", "mrqa_squad-validation-2210", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2449", "mrqa_squad-validation-2451", "mrqa_squad-validation-2453", "mrqa_squad-validation-2476", "mrqa_squad-validation-2506", "mrqa_squad-validation-2571", "mrqa_squad-validation-2603", "mrqa_squad-validation-2643", "mrqa_squad-validation-2643", "mrqa_squad-validation-2717", "mrqa_squad-validation-2753", "mrqa_squad-validation-2780", "mrqa_squad-validation-2807", "mrqa_squad-validation-2832", "mrqa_squad-validation-2865", "mrqa_squad-validation-2888", "mrqa_squad-validation-2955", "mrqa_squad-validation-3086", "mrqa_squad-validation-3092", "mrqa_squad-validation-31", "mrqa_squad-validation-3109", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3196", "mrqa_squad-validation-3223", "mrqa_squad-validation-3257", "mrqa_squad-validation-3310", "mrqa_squad-validation-3320", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3370", "mrqa_squad-validation-3374", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3456", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-351", "mrqa_squad-validation-3551", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3607", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3692", "mrqa_squad-validation-3724", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3823", "mrqa_squad-validation-3865", "mrqa_squad-validation-3890", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3939", "mrqa_squad-validation-3998", "mrqa_squad-validation-401", "mrqa_squad-validation-4018", "mrqa_squad-validation-4019", "mrqa_squad-validation-4100", "mrqa_squad-validation-4110", "mrqa_squad-validation-4162", "mrqa_squad-validation-4206", "mrqa_squad-validation-4210", "mrqa_squad-validation-4232", "mrqa_squad-validation-4240", "mrqa_squad-validation-4297", "mrqa_squad-validation-4316", "mrqa_squad-validation-4343", "mrqa_squad-validation-441", "mrqa_squad-validation-4430", "mrqa_squad-validation-4458", "mrqa_squad-validation-4460", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4615", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4665", "mrqa_squad-validation-4729", "mrqa_squad-validation-4783", "mrqa_squad-validation-4791", "mrqa_squad-validation-4795", "mrqa_squad-validation-4824", "mrqa_squad-validation-4841", "mrqa_squad-validation-4857", "mrqa_squad-validation-4860", "mrqa_squad-validation-4870", "mrqa_squad-validation-4901", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-5", "mrqa_squad-validation-50", "mrqa_squad-validation-510", "mrqa_squad-validation-5115", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5187", "mrqa_squad-validation-525", "mrqa_squad-validation-5275", "mrqa_squad-validation-5310", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5422", "mrqa_squad-validation-5450", "mrqa_squad-validation-5471", "mrqa_squad-validation-5492", "mrqa_squad-validation-5591", "mrqa_squad-validation-5602", "mrqa_squad-validation-5624", "mrqa_squad-validation-5638", "mrqa_squad-validation-5714", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5844", "mrqa_squad-validation-5883", "mrqa_squad-validation-5889", "mrqa_squad-validation-5943", "mrqa_squad-validation-5971", "mrqa_squad-validation-5978", "mrqa_squad-validation-60", "mrqa_squad-validation-6015", "mrqa_squad-validation-603", "mrqa_squad-validation-6044", "mrqa_squad-validation-6070", "mrqa_squad-validation-6072", "mrqa_squad-validation-6091", "mrqa_squad-validation-6120", "mrqa_squad-validation-6143", "mrqa_squad-validation-6181", "mrqa_squad-validation-6197", "mrqa_squad-validation-62", "mrqa_squad-validation-6255", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-6361", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6408", "mrqa_squad-validation-6428", "mrqa_squad-validation-6454", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6625", "mrqa_squad-validation-6626", "mrqa_squad-validation-6645", "mrqa_squad-validation-6657", "mrqa_squad-validation-6658", "mrqa_squad-validation-6658", "mrqa_squad-validation-6680", "mrqa_squad-validation-6725", "mrqa_squad-validation-6753", "mrqa_squad-validation-6753", "mrqa_squad-validation-6773", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6958", "mrqa_squad-validation-6997", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7013", "mrqa_squad-validation-7021", "mrqa_squad-validation-7040", "mrqa_squad-validation-7082", "mrqa_squad-validation-7101", "mrqa_squad-validation-7162", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7317", "mrqa_squad-validation-7382", "mrqa_squad-validation-7395", "mrqa_squad-validation-7430", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7463", "mrqa_squad-validation-7537", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7670", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7708", "mrqa_squad-validation-7765", "mrqa_squad-validation-7831", "mrqa_squad-validation-7837", "mrqa_squad-validation-7867", "mrqa_squad-validation-787", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-7959", "mrqa_squad-validation-7961", "mrqa_squad-validation-7961", "mrqa_squad-validation-805", "mrqa_squad-validation-806", "mrqa_squad-validation-8135", "mrqa_squad-validation-8227", "mrqa_squad-validation-8233", "mrqa_squad-validation-8238", "mrqa_squad-validation-8242", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-8553", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8596", "mrqa_squad-validation-8602", "mrqa_squad-validation-8627", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8755", "mrqa_squad-validation-8781", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-89", "mrqa_squad-validation-8900", "mrqa_squad-validation-8971", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9109", "mrqa_squad-validation-9154", "mrqa_squad-validation-9176", "mrqa_squad-validation-9226", "mrqa_squad-validation-9240", "mrqa_squad-validation-9304", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9371", "mrqa_squad-validation-9405", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9546", "mrqa_squad-validation-9562", "mrqa_squad-validation-9611", "mrqa_squad-validation-9619", "mrqa_squad-validation-968", "mrqa_squad-validation-9750", "mrqa_squad-validation-9764", "mrqa_squad-validation-9856", "mrqa_squad-validation-9890", "mrqa_squad-validation-9895", "mrqa_squad-validation-9896", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-873"], "OKR": 0.912109375, "KG": 0.49296875, "before_eval_results": {"predictions": ["Warszawa", "the SI unit of magnetic flux density the tesla", "2007", "Duval County", "2003", "the father of the house when in his home", "Electrical Experimenter", "Richard Wilkinson and Kate Pickett", "some teachers and parents", "Governor Vaudreuil", "pastors and teachers", "Justin Tucker", "1543", "None", "State Route 41", "Switzerland", "unit-dose, or a single doses of medicine", "War of Currents", "the \"vanguard of change and Islamic reform\" centered around the Muslim Brotherhood", "continental European", "Roger Goodell", "events", "9", "Adelaide", "once", "Around 200,000", "itty Hawk", "Nidal Hasan", "the Atlantic Coast Conference", "priest Charles Coughlin", "Sean", "Consigliere", "Pierce County", "Harry F. Sinclair", "Homebrewing", "December 1974", "2012", "1999", "2004", "Best Sound", "Nelson Rockefeller", "Fort Snelling", "Amy Winehouse", "State House in Augusta", "1970", "1978", "2006", "My Cat from Hell", "Richard B. Riddick", "Colonel", "1999", "17", "La Liga", "Buffalo Soldiers", "Kal Ho Naa Ho", "Key West", "gastrocnemius", "John Roberts", "repechage", "Carl Johan", "two", "Madonna", "Freddie Mercury", "the Marine Band"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7826522435897436}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.3333333333333333, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1251", "mrqa_squad-validation-2318", "mrqa_squad-validation-10259", "mrqa_squad-validation-2337", "mrqa_squad-validation-6526", "mrqa_squad-validation-9578", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4967", "mrqa_naturalquestions-validation-7608"], "SR": 0.6875, "CSR": 0.6363636363636364, "EFR": 1.0, "Overall": 0.7661008522727273}, {"timecode": 11, "before_eval_results": {"predictions": ["pr\u00e9tendus r\u00e9form\u00e9s", "587,000 square kilometres", "Northumberland house", "Mnemiopsis", "from tomb and memorial, to portrait, allegorical, religious, mythical, statues for gardens including fountains, as well as architectural decorations", "Beirut", "smaller trade relations with their neighbours", "Tommy Lee Jones", "150", "four", "308", "Queen Victoria", "huge", "the Orange Democratic Movement", "Charlesfort", "rapidly evolve and adapt", "Battle of the Restigouche", "Boston", "forces", "executive producer", "David Lynch", "psychologist", "every year", "Conan Doyle", "Blood Light", "Hong Kong", "ambilevous", "Bruce Wayne", "a horse", "Irrawaddy River", "Jim McDivitt", "River Hull", "lunar new year", "Lord Chesterfield", "Copenhagen", "Troy", "human rights", "John Gorman", "bison", "Edinburgh", "Viking", "Paul Gauguin", "Action Comics", "Bombe", "\"phase changes\"", "Novak Djokovic", "New Zealand", "Oasis", "The Golden Girls", "green, red, white", "Rajasthan", "the Monkees", "false ribs", "take it in turns to be president of the G8", "golf", "Secretary of Homeland Security", "Bee Gees", "Adelaide", "Edward John \"Eddie\" Izzard", "Sabina Guzzanti", "$13 million global crime ring", "a quark", "neptune", "one of the most common words in scripture"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5577132936507936}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4256", "mrqa_squad-validation-5388", "mrqa_squad-validation-5545", "mrqa_squad-validation-5303", "mrqa_squad-validation-7083", "mrqa_squad-validation-9255", "mrqa_squad-validation-7887", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-3095", "mrqa_naturalquestions-validation-5094", "mrqa_hotpotqa-validation-462", "mrqa_newsqa-validation-3199", "mrqa_searchqa-validation-7976", "mrqa_naturalquestions-validation-9323"], "SR": 0.484375, "CSR": 0.6236979166666667, "EFR": 0.9090909090909091, "Overall": 0.7453858901515151}, {"timecode": 12, "before_eval_results": {"predictions": ["Oxygen toxicity to the lungs and central nervous system", "Henry Cavendish", "Lower Norfolk County", "melatonin", "90-60's", "deaths of two friends", "1985", "Ismailiyah, Egypt", "England", "the ability to pursue valued goals", "tentilla", "political support in his struggle against leftists", "$5 million", "Lake George", "Keraites", "Dwight D. Eisenhower", "decreases", "one Commissioner", "Secretariat", "1952", "Australian", "September 1901", "The United States of America", "The Dragon", "psilocin", "Fundamentalist Church of Jesus Christ of Latter Day Saints", "Eurasia", "Boyd Gaming", "MGM Resorts International", "James G. Kiernan", "Omega SA", "September 14, 1877", "A third jersey, alternate jersey, third kit or alternate uniform is a jersey or uniform that a sports team wear in games instead of its home outfit or its away outfit", "Yasir Hussain", "Malayalam movies", "Kennedy Road", "2002", "31", "Grant Field", "Bill Boyd", "Jack Ryan", "Northern", "Buckingham Palace", "322,520", "Chief Strategy Officer", "Dave Lee Travis", "Bedknobs and Broomsticks", "Jane", "Henry H. Babcock", "140 million", "\"Beauty and the Beast\"", "Gary Ross", "International Boxing Hall of Fame", "1996", "Revolver", "Jack Nicklaus", "atransformiation, change of mind, repentance, and atonement", "Mussolini", "Ryan MacGraw", "off the coast of Dubai", "1918", "butter", "Boston", "a geologic episode, change, process, deposit, or feature that is the result of the action or effects of rain"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7285878745437568}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.13333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941]}}, "before_error_ids": ["mrqa_squad-validation-2657", "mrqa_squad-validation-9565", "mrqa_squad-validation-4294", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-278", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-1893", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-4173", "mrqa_newsqa-validation-2790", "mrqa_triviaqa-validation-2147"], "SR": 0.671875, "CSR": 0.6274038461538461, "retrieved_ids": ["mrqa_squad-train-60045", "mrqa_squad-train-6392", "mrqa_squad-train-77026", "mrqa_squad-train-30494", "mrqa_squad-train-58135", "mrqa_squad-train-64591", "mrqa_squad-train-40351", "mrqa_squad-train-25880", "mrqa_squad-train-20473", "mrqa_squad-train-74705", "mrqa_squad-train-41550", "mrqa_squad-train-68598", "mrqa_squad-train-3630", "mrqa_squad-train-35787", "mrqa_squad-train-50993", "mrqa_squad-train-5408", "mrqa_searchqa-validation-8411", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-4534", "mrqa_hotpotqa-validation-1161", "mrqa_squad-validation-1808", "mrqa_hotpotqa-validation-3807", "mrqa_searchqa-validation-13016", "mrqa_hotpotqa-validation-5101", "mrqa_naturalquestions-validation-2851", "mrqa_squad-validation-7382", "mrqa_squad-validation-7013", "mrqa_hotpotqa-validation-5174", "mrqa_squad-validation-7162", "mrqa_naturalquestions-validation-7407", "mrqa_squad-validation-7887", "mrqa_hotpotqa-validation-893"], "EFR": 1.0, "Overall": 0.7643088942307693}, {"timecode": 13, "before_eval_results": {"predictions": ["poverty for the riches of Croesus", "Fred Silverman", "occupational burnout", "Saudi", "\"I plead for the beauty that surrounds us\"; this is known as a \"creative plea,\" and will usually be interpreted as a plea of not guilty.", "different types of prey, which they capture by as wide a range of methods as spiders use", "$20.4 billion", "twelve residential Houses", "Anglo-Saxons", "Doctor Who Confidential documentary", "stricter discipline based on their power of expulsion", "killed in a horse-riding accident", "1522", "eight", "Of course [the price of oil] is going to rise", "Roman law meaning 'empty land'", "Henry Hudson", "chipmunk", "james boswell", "Melbourne", "Albania", "brown trout", "Mayflower", "Ron Ely", "lacrimal fluid", "George Best", "neuter ablative of posterius", "The Great British Bake Off", "Red Lion", "Fenn Street School", "Smiths", "joseph carey boswell", "The Nobel Prize in Literature", "Pakistan", "The Observer", "United States", "Big Fat Gypsy Wedding", "beards", "Andes", "Thor", "the Forum", "\"Moon River\"", "Tina Turner", "SW19", "Lancashire", "Pacific Ocean", "racing", "Rustle My Davies", "climatology", "Charlie Brown", "vinaya", "avocado", "Black Sea", "glucose", "1933", "The episode typically ends as a cliffhanger showing the first few moments of Sam's next leap", "Abu Dhabi", "Craig William Macneill", "terminal brain cancer", "800,000", "volcano", "giant slalom", "Serie B league", "Saoirse Ronan"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6367243867243868}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, false, false, false, true], "QA-F1": [0.9090909090909091, 1.0, 1.0, 0.6666666666666666, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 0.8, 0.2222222222222222, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2516", "mrqa_squad-validation-3863", "mrqa_squad-validation-6913", "mrqa_squad-validation-4621", "mrqa_squad-validation-7811", "mrqa_squad-validation-7112", "mrqa_squad-validation-3730", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2989", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-2335", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-9026", "mrqa_hotpotqa-validation-3607", "mrqa_searchqa-validation-1416", "mrqa_searchqa-validation-15315", "mrqa_hotpotqa-validation-1687"], "SR": 0.53125, "CSR": 0.6205357142857143, "EFR": 0.9666666666666667, "Overall": 0.7562686011904762}, {"timecode": 14, "before_eval_results": {"predictions": ["Cathedral of Saint John the Divine", "The Ruhr", "Hulu", "time complexity", "Muslim Iberia", "10 o'clock", "NYPD Blue", "AAUW study", "Magnetophon tape recorder", "he explored the mountains in hunter's garb", "Rotterdam", "human inequality can be addressed/corrected, while still not resulting in an increase of environmental damage", "Charles Dickens", "force", "best teachers", "imperfect", "laysan albatross", "wood", "capella", "The National Gallery of Art", "cape horn", "wood", "neptune", "russellas", "a number whose 5th power contains every digit at least once", "turkeys", "woodlands", "wood", "William", "woodlands", "a light-year", "wood", "horn horn horn", "wood Russert", "Prince of Wales", "cocoa butter", "Violent Femmes", "wood", "a guardian angel", "laser", "James Fenimore Cooper", "The New Adventures of Old Christine", "wood", "a pastry-cook", "cape horn", "wood horn", "Copenhagen", "cape horn", "Jose de San Martn", "Madrid", "a cape horn", "rufino Cardinal Santos", "cape horn", "a human rat-trap", "fertilization", "India is the world's second most populous country after the People's Republic of China", "Nissan", "wood", "Kind Hearts and Coronets", "2012", "poems", "Cyprus", "woodhorses", "netherlands"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4624131944444444}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444444, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7632", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10428", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-3019", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1948", "mrqa_searchqa-validation-9250", "mrqa_searchqa-validation-8283", "mrqa_searchqa-validation-14628", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-6931", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-8607", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-5613", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-14502", "mrqa_naturalquestions-validation-8420", "mrqa_triviaqa-validation-4416", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-7610", "mrqa_triviaqa-validation-7170"], "SR": 0.40625, "CSR": 0.60625, "EFR": 0.9736842105263158, "Overall": 0.7548149671052632}, {"timecode": 15, "before_eval_results": {"predictions": ["trial division", "Go-Ahead", "three to five", "heavy/highway, heavy civil or heavy engineering", "Osama bin Laden", "September 1944", "paramagnetic", "criminal investigations", "complexity classes", "1963", "Jamukha", "consultant", "711,988", "Mumbai Rajdhani Express", "President pro tempore of the Senate", "Hugo Weaving", "the passing of the year", "The Dursley family", "amanda burton", "the somatic nervous system and the autonomic nervous system", "Aman Gandotra", "Daya Jethalal Gada", "Kevin Sumlin", "( clay)", "the beginning of the American colonies", "Canada", "two - stroke engines and chain drive", "the English", "a writ of certiorari", "Dan Stevens", "a United States military prison", "a limited period of time", "the Colony of Virginia", "January 2017", "2013", "Tatsumi", "December 15, 2017", "the Sunni Muslim family", "Magnavox Odyssey", "The Buckwheat Boyz", "Christianity", "India", "the nucleus", "between 1923 and 1925", "Moscazzano", "the bark", "the beer is only produced for export and is not sold in Germany", "the most recent Super Bowl champions", "in the reverse direction", "San Francisco, California", "Hal Derwin", "oversee the local church", "2007", "~ 0.116 mm", "Robert Boyle", "the solar system", "Ascona", "Ludwig van Beethoven", "former Boca Juniors teammate", "Akshay Kumar", "Harriet Fitzhugh", "Bananas", "temperature", "a centerpiece"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6417124542124543}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.2857142857142857, 1.0, 1.0, 0.7499999999999999, 0.20000000000000004, 1.0, 0.7499999999999999, 0.0, 0.0, 0.13333333333333333, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.30769230769230765, 0.5, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.4, 1.0, 0.8571428571428571, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_squad-validation-6025", "mrqa_squad-validation-6113", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-144", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-8962", "mrqa_hotpotqa-validation-1409", "mrqa_newsqa-validation-3042", "mrqa_searchqa-validation-5471"], "SR": 0.515625, "CSR": 0.6005859375, "retrieved_ids": ["mrqa_squad-train-15248", "mrqa_squad-train-28208", "mrqa_squad-train-63282", "mrqa_squad-train-75288", "mrqa_squad-train-43545", "mrqa_squad-train-7814", "mrqa_squad-train-74666", "mrqa_squad-train-28545", "mrqa_squad-train-16049", "mrqa_squad-train-50941", "mrqa_squad-train-76747", "mrqa_squad-train-43945", "mrqa_squad-train-26360", "mrqa_squad-train-72501", "mrqa_squad-train-35994", "mrqa_squad-train-9140", "mrqa_squad-validation-6284", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-7407", "mrqa_squad-validation-7162", "mrqa_triviaqa-validation-778", "mrqa_hotpotqa-validation-278", "mrqa_searchqa-validation-5075", "mrqa_newsqa-validation-1718", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-4533", "mrqa_hotpotqa-validation-1893", "mrqa_searchqa-validation-16156", "mrqa_naturalquestions-validation-1091", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-15030", "mrqa_triviaqa-validation-5194"], "EFR": 1.0, "Overall": 0.7589453125000001}, {"timecode": 16, "before_eval_results": {"predictions": ["512-bit", "the worst-case time complexity T(n)", "National Broadcasting Company", "Marco Polo", "November 2006 and May 2008", "complex", "the symptoms of the Black Death are not unique", "pinch in two", "xenoliths", "approximately 80 avulsions", "the leader of the political party or coalition with the most seats", "April 1887", "cheese, \u201cspecial sauce\u201d (a variant of Thousand Island dressing), iceberg lettuce, pickles, and onions, served in a three-part sesame seed bun", "Rock Follies", "Montmorency", "Nut & Honey Crunch", "Elton John", "beer", "Simon Moores", "Dip", "Corfu", "the midrib", "Leopoldville", "8 minutes", "Federal Reserve", "four", "Cyclops", "oxygen", "Silent Spring", "the value of unknown electrical resistance", "white spirit", "Possumhaw Viburnum", "Harold Wilson", "Denmark", "Anna (Julia Roberts)", "James Mason", "Hypervelocity star", "West Point", "the ostrich", "Moby Dick", "William Golding", "the 5th fret", "The Runaways", "Kim Clijsters", "Les Dennis", "the A38", "Nicola Walker", "Virgin", "1997", "Port Talbot", "rainy", "\"The best is yet to come\"", "Nicola Adams", "Sax Rohmer", "the EU Data Protection Directive 1995 protection", "May 2010", "Bruce R. Cook", "Sir Patrick Barnewall", "the Obama administration", "Baitullah Mehsud", "the Equator", "Aerosmith", "Cesar Millan", "Princeton University"], "metric_results": {"EM": 0.625, "QA-F1": 0.6791349823422191}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.2105263157894737, 1.0, 1.0, 0.5, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1708", "mrqa_squad-validation-5605", "mrqa_squad-validation-5001", "mrqa_squad-validation-9357", "mrqa_squad-validation-2852", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-287", "mrqa_naturalquestions-validation-3930", "mrqa_hotpotqa-validation-1542", "mrqa_newsqa-validation-1537", "mrqa_hotpotqa-validation-4298"], "SR": 0.625, "CSR": 0.6020220588235294, "EFR": 1.0, "Overall": 0.7592325367647059}, {"timecode": 17, "before_eval_results": {"predictions": ["after the end of the Mexican War", "captured Fort Beaus\u00e9jour in June 1755", "journalist", "Seventy percent", "modern hatred of the Jews", "Germany and Austria", "inclusions (or clasts) are found in a formation", "Sweynforkbeard", "the King", "eight", "the Sierra Freeway", "Mickey Mouse", "Rugby", "Spain", "may", "Google", "dance", "children of prostitutes in Sonagachi", "Quebec", "Planet of the Apes", "Prince Edward Island", "bilirubin", "frosted", "Virginia Woolf", "Vasco da Gama", "canter", "Musculus gluteus maximus", "1972", "Arbor Day", "Countrywide Financial", "light cameras", "man Conan O'Brien", "eagles", "may", "Nikita Khrushchev", "other rooms", "hair", "manhattan", "Roger Smith", "boo", "sepoy", "last", "Wayne Brady", "submarines", "Joan of Arc", "turtles", "Trinidad and Tobago", "Vladimir Nabokov", "may", "Peter Pan", "synonymous", "a laser beam", "Phi Beta Phi Society", "Joel", "in the morning with the princes of Moab", "neptune", "Prince Philip", "Cleopatra VII Philopator", "over 12 million", "pilot", "Lance Cpl. Maria Lauterbach", "Pandora", "Tears for Fears", "paper sales company"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5719494047619047}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.33333333333333337, 0.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-10273", "mrqa_squad-validation-4260", "mrqa_squad-validation-5121", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-6", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-9389", "mrqa_searchqa-validation-4933", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-9551", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-2347", "mrqa_naturalquestions-validation-230", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-6259", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-2618", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2525", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-6435"], "SR": 0.484375, "CSR": 0.5954861111111112, "EFR": 1.0, "Overall": 0.7579253472222223}, {"timecode": 18, "before_eval_results": {"predictions": ["quantum mechanics", "near the surface", "Alfred Stevens", "domestic social reforms", "algebraic", "eight", "1886/1887", "clerical", "Apollo", "Linebacker", "2000", "Richard Street", "Fred Carter", "1926 Paris", "American burlesque", "Polk County", "Skyscraper", "schoolteacher and publisher", "Player's No 10, Skol, Leyland Cars, Gauntlet, Daily Mirror, TNT Sameday and Dunlop", "David Anthony O'Leary", "a family member", "Tranquebar (Tharangambadi)", "Attorney General and as Lord Chancellor of England", "North Dakota", "fennec", "Norwood, Massachusetts", "1993", "the 10-metre platform event", "liquidambar", "Battle of Chester", "Flashback", "Portland, OR", "Marco Fu", "Francis the Talking Mule", "Kristin Scott Thomas", "The King of Hollywood", "evangelical Christian", "the paternalistic policies enacted upon Native American tribes", "The Hindu Group", "Kealakekua Bay", "1919", "Julia Verdin", "2007", "Guthred", "The Arizona Health Care Cost Containment System", "Australian", "1945", "1941", "the Teatro Carlo Felice", "The Maze Runner", "pronghorn", "number one box-office draw", "Life Is a Minestrone", "the coffee shop Monk's", "Henry Moseley", "a leg break", "May Skinner", "France", "at least $20 million to $30 million", "(Ulysses) Grant", "Virgil Tibbs", "The president ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons", "CNN", "Michael Arrington"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6162097953216374}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.14285714285714288, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.5263157894736842, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9286", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-0", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-5700", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-7209", "mrqa_newsqa-validation-2601", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-12442", "mrqa_newsqa-validation-1114"], "SR": 0.515625, "CSR": 0.591282894736842, "retrieved_ids": ["mrqa_squad-train-49002", "mrqa_squad-train-24661", "mrqa_squad-train-74047", "mrqa_squad-train-62481", "mrqa_squad-train-75801", "mrqa_squad-train-74492", "mrqa_squad-train-48647", "mrqa_squad-train-85178", "mrqa_squad-train-78852", "mrqa_squad-train-17748", "mrqa_squad-train-3811", "mrqa_squad-train-10367", "mrqa_squad-train-75613", "mrqa_squad-train-13157", "mrqa_squad-train-10175", "mrqa_squad-train-65345", "mrqa_squad-validation-7382", "mrqa_squad-validation-8910", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-75", "mrqa_squad-validation-10427", "mrqa_squad-validation-5112", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-9026", "mrqa_searchqa-validation-10093", "mrqa_hotpotqa-validation-4810", "mrqa_triviaqa-validation-1114", "mrqa_searchqa-validation-2499", "mrqa_squad-validation-6680", "mrqa_searchqa-validation-8139", "mrqa_newsqa-validation-539"], "EFR": 0.9354838709677419, "Overall": 0.7441814781409168}, {"timecode": 19, "before_eval_results": {"predictions": ["2,200 fishes", "swimming-plates", "MHC I", "Executive Vice President of Football Operations and General Manager", "10 times", "Continental Edison Company", "Time magazine", "Nafzger", "Warszawa", "Troggs", "Sch schizophrenia", "Cressida", "Tom Osborne", "Moses", "a shih tzu", "Israeli prime minister", "Fiddler on the Roof", "Monopoly", "Al Czervik", "Leszczyski", "mask", "Alien", "Tower of London", "crocodiles", "Madonna", "Tomatillos", "Tommy Lasorda", "Pakistan", "Coca-Cola", "schuss", "Chaillot", "Ibrahim Petrovich Gannibal", "beurre mani", "grow a beard", "Soup Nazi", "Pyrrhus", "Guatemala", "bonds", "the Rue Morgue", "huevos rancheros", "August Strindberg", "Sacher Torte", "Palestine: Peace Not Apartheid", "descend", "Lovebird", "Rhodesian", "strawberries", "Henry James", "the arithmometer", "American opposition to British policy", "Frank Sinatra Jr.", "Sonnets", "South Africa", "Auxiliaries and ships of allied services", "the Infamy Speech of US President Franklin D. Roosevelt", "Costa Del Sol", "Kidderminster", "Annales de chimie et de physique", "gull-wing doors", "Jersey Economic Development Authority", "rape her in a Milledgeville, Georgia, bar during a night of drinking in March", "1994", "state senators", "38"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5109375}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, false, true, true, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, true], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4264", "mrqa_squad-validation-4730", "mrqa_squad-validation-167", "mrqa_squad-validation-1239", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-835", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1800", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-3762", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-429", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-6633", "mrqa_hotpotqa-validation-4813", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-406"], "SR": 0.4375, "CSR": 0.58359375, "EFR": 1.0, "Overall": 0.7555468750000001}, {"timecode": 20, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1340", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3480", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5054", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-928", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4657", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1843", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14502", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5583", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8385", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-10251", "mrqa_squad-validation-10273", "mrqa_squad-validation-10285", "mrqa_squad-validation-10335", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1095", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1180", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1461", "mrqa_squad-validation-1552", "mrqa_squad-validation-1554", "mrqa_squad-validation-161", "mrqa_squad-validation-1636", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1706", "mrqa_squad-validation-1808", "mrqa_squad-validation-1949", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2069", "mrqa_squad-validation-2318", "mrqa_squad-validation-2369", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2453", "mrqa_squad-validation-2458", "mrqa_squad-validation-2476", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2717", "mrqa_squad-validation-2768", "mrqa_squad-validation-2780", "mrqa_squad-validation-2832", "mrqa_squad-validation-2888", "mrqa_squad-validation-3046", "mrqa_squad-validation-3138", "mrqa_squad-validation-3153", "mrqa_squad-validation-3197", "mrqa_squad-validation-3217", "mrqa_squad-validation-3223", "mrqa_squad-validation-3243", "mrqa_squad-validation-3326", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-3500", "mrqa_squad-validation-3551", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3922", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4210", "mrqa_squad-validation-4226", "mrqa_squad-validation-4240", "mrqa_squad-validation-4256", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4348", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-441", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4631", "mrqa_squad-validation-4666", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4870", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5106", "mrqa_squad-validation-5112", "mrqa_squad-validation-5118", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5242", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5344", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5624", "mrqa_squad-validation-5714", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5958", "mrqa_squad-validation-6015", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6074", "mrqa_squad-validation-6181", "mrqa_squad-validation-6196", "mrqa_squad-validation-6244", "mrqa_squad-validation-6284", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-6728", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6920", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7083", "mrqa_squad-validation-7112", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7398", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7867", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8190", "mrqa_squad-validation-8233", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8557", "mrqa_squad-validation-8596", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8900", "mrqa_squad-validation-8905", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9176", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9345", "mrqa_squad-validation-9351", "mrqa_squad-validation-9371", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-958", "mrqa_squad-validation-9614", "mrqa_squad-validation-9619", "mrqa_squad-validation-964", "mrqa_squad-validation-9750", "mrqa_squad-validation-9761", "mrqa_squad-validation-9892", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3606", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-708", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7742"], "OKR": 0.884765625, "KG": 0.48359375, "before_eval_results": {"predictions": ["keratinocytes and macrophages", "Roone Arledge", "Muslims in the semu class", "The John W. Weeks Bridge", "9th", "clinical pharmacy movement", "US$3 per barrel", "Trajan's Column", "Indian Ocean", "Barb", "Golda Meir", "xerophyte", "anions", "Uranus", "George III", "Frank Spillane", "Iolani Palace", "Gandalf", "Mungo Park", "Squash", "(Bill) Pertwee", "iron", "Sam Mendes", "P'Osoge", "Emeril Lagasse", "\"Shine,\u201d", "Karl Marx", "the principal front of a building", "four and a half hours", "norway", "Jamaica", "Skylab", "Sydney", "Steven Taylor", "Zephyrus", "Frobisher Bay", "Dumbo", "William Makepeace Thackeray", "Botany Bay", "Peterborough United", "FC Porto", "albedo", "11", "Washington", "red", "remnants of very massive stars", "Groucho Marx", "Andrew Nicholson", "Prince Eddy", "Algerian", "(Dos) Sicilias", "Barry White", "gin", "Frank Avalon", "1966", "guitar feedback", "The LA Galaxy", "Stephen Johns", "Veracruz, Mexico", "Peter Jennings", "Simon & Garfunkel", "Peter Robinson", "2009", "Robert Kimmitt"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5768229166666666}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6567", "mrqa_squad-validation-6316", "mrqa_squad-validation-3635", "mrqa_triviaqa-validation-371", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-5088", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-922", "mrqa_naturalquestions-validation-1008", "mrqa_newsqa-validation-2939", "mrqa_searchqa-validation-13803", "mrqa_newsqa-validation-3091"], "SR": 0.515625, "CSR": 0.5803571428571428, "EFR": 1.0, "Overall": 0.7401339285714286}, {"timecode": 21, "before_eval_results": {"predictions": ["1349", "the center of mass", "July 23, 1963", "very rare", "James E. Webb", "eight", "Panic of 1901", "supported modern programming practices and enabled business applications to be developed with Flash", "February 6, 2005", "development of electronic computers", "159", "an Easter egg", "Andhra Pradesh and Odisha", "1975", "John Vincent Calipari", "winter", "29 - year - old Billie Jean King", "Robert Hooke", "in rocks and minerals", "October 2, 2017", "to avoid the inconvenienceiences of a pure barter system", "four", "Sardis", "Lykan", "in the pachytene stage of prophase I of meiosis", "Baltimore", "Once Upon a Time in India", "Hank J. Deutschendorf II", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "princess - Marie Leprince de Beaumont", "The Canterbury Tales", "May 19, 2008", "Albert Einstein", "May 26, 2017", "1992", "restored to life", "Master Christopher Jones", "to solve its problem of lack of food self - sufficiency", "princess Tomlin", "Spanish surname", "in the bloodstream or surrounding tissue", "sexton Robert Newman and Captain John Pulling", "Fox Ranch in Malibu Creek State Park", "Gibraltar", "Dmitri Mendeleev", "1818", "31", "the disputed 1824 presidential election", "12", "a form of business network", "for a bandwidth of 2.048 Mbit / s", "twelve", "Paige O'Hara", "ghee", "The Crow", "Michael Schumacher", "micronutrient-rich", "and husband Bill Klein", "top designers", "honda", "gold", "blackfield Cathedral", "Connally", "liver"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6201458960662231}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.7058823529411764, 0.0, 1.0, 0.6666666666666666, 0.7368421052631579, 0.0, 0.625, 1.0, 0.4615384615384615, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.0, 0.6666666666666666, 0.0, 0.9333333333333333, 0.7368421052631579, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-2635", "mrqa_hotpotqa-validation-4181", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3054", "mrqa_searchqa-validation-14780", "mrqa_searchqa-validation-7212", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-4182"], "SR": 0.46875, "CSR": 0.5752840909090908, "retrieved_ids": ["mrqa_squad-train-62179", "mrqa_squad-train-64345", "mrqa_squad-train-76538", "mrqa_squad-train-12007", "mrqa_squad-train-32819", "mrqa_squad-train-6719", "mrqa_squad-train-16901", "mrqa_squad-train-20333", "mrqa_squad-train-55568", "mrqa_squad-train-46427", "mrqa_squad-train-29618", "mrqa_squad-train-66219", "mrqa_squad-train-65032", "mrqa_squad-train-56845", "mrqa_squad-train-19734", "mrqa_squad-train-82233", "mrqa_squad-validation-6324", "mrqa_naturalquestions-validation-7101", "mrqa_searchqa-validation-14572", "mrqa_naturalquestions-validation-75", "mrqa_searchqa-validation-478", "mrqa_squad-validation-10483", "mrqa_hotpotqa-validation-1227", "mrqa_searchqa-validation-10428", "mrqa_searchqa-validation-9389", "mrqa_triviaqa-validation-7360", "mrqa_newsqa-validation-539", "mrqa_squad-validation-7708", "mrqa_searchqa-validation-8607", "mrqa_triviaqa-validation-1088", "mrqa_hotpotqa-validation-471", "mrqa_searchqa-validation-2347"], "EFR": 0.9705882352941176, "Overall": 0.7332369652406416}, {"timecode": 22, "before_eval_results": {"predictions": ["machine labor in wealthier nations", "an intuitive understanding", "evenly round the body", "2,869", "president of NBC's entertainment division", "Wesel-Datteln Canal", "Melanie Griffith", "fowls", "lexicographer", "an Islamic Republic", "One Flew Over the Cuckoo's Nest", "mustard", "Royal Wives", "Harpers Ferry", "the Confeitaria Colombo", "gretter", "Versailles", "Target", "meadow", "the Tsardom of Russia", "Terrific", "magnesium", "\"The Swamp Fox\"", "the Confederacy", "German Shepherd", "peanuts", "Bhutan", "Buttery", "Damascus", "Jennies", "a hologram", "Greg Montgomery", "the 1906 earthquake", "Genoa and Lucca", "Mother Vineyard", "Virginia Woolf", "apogee", "Cherry Garcia", "the book of \"Aladdin, or, The wonderful lamp\"", "Diamond Jim Brady", "an axiom or postulate", "Princeton", "Eric Knight", "Apple", "The Sound of Music", "Pygmalion", "T. S. Eliot", "Andes", "Emeralds", "asteroids", "the Nutcracker", "an earthquake", "Labour", "1933", "grated cheese", "The Merry Wives of Windsor", "fiery-tempered", "Republican", "Wojtek", "Bangor Air National Guard Base", "1995", "spandex and leg warmers", "12-hour-plus shifts of backbreaking labor", "end her trip in Crawford"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5936011904761904}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false], "QA-F1": [0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.16666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-7541", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-9788", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-4038", "mrqa_searchqa-validation-9368", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-9991", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-10625", "mrqa_triviaqa-validation-7685", "mrqa_hotpotqa-validation-314", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-403"], "SR": 0.515625, "CSR": 0.5726902173913043, "EFR": 1.0, "Overall": 0.7386005434782608}, {"timecode": 23, "before_eval_results": {"predictions": ["Department for Culture, Media and Sport", "Kevin Harlan", "Khongirad", "Solim\u00f5es Basin", "seven", "the 21st century", "Mombasa, Kenya", "(Ed.) Biden", "18", "Adidas", "she was going to be on the Olympic medals podium.", "the body of the aircraft", "18th century tapestries", "the United States", "Michigan", "in Iraq", "Two", "Russia", "(Oprah.com)", "$106,482,500", "March 31.", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "misdemeanor", "three out of four", "tennis", "Toy Story", "\"Saturnistas\"", "90", "directly involved in an Internet broadband deal with a Chinese firm.", "$75", "a free laundry service", "the WGC-CA Championship", "Jeffrey Jamaleldine", "insurgent small arms fire", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "more than 1.2 million", "Mitt Romney", "a new constitution", "the traditional Jewish prayer recited during times of mourning.", "near Grand Ronde, Oregon", "Seasons of My Heart", "raping and murdering", "150", "Anil Kapoor", "misdemeanor assault charges", "Pope Benedict XVI refused", "people thought this was a small problem", "( Martin) Aloysius) Culhane", "a model of sustainability", "al-Shabaab", "a long-term goal for reducing\" greenhouse emissions.", "motorbike", "the Isthmus of Corinth", "Bear and Bo Rinehart", "Old Trafford", "(Sakyamuni)", "(Bobbi Kristina Brown)", "Shayne Ward", "Christina Ricci", "Fayetteville", "Hong Kong", "melting", "\"Paul Revere's Ride\"", "18th century"], "metric_results": {"EM": 0.40625, "QA-F1": 0.49714285714285716}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.16, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.4, 0.0, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4356", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2167", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-8272", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-2919", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5469", "mrqa_searchqa-validation-5398", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-12527"], "SR": 0.40625, "CSR": 0.5657552083333333, "EFR": 1.0, "Overall": 0.7372135416666665}, {"timecode": 24, "before_eval_results": {"predictions": ["John Harvard", "late 1886", "AKS primality test", "Command Module design, workmanship and quality control.", "Gold footballs", "1967", "Dunlop", "XVideos", "Niger\u2013Congo", "Sports Illustrated", "Christine McCarthy", "Regional League North", "The 2002 United States Senate election", "Harsh Times", "Fade Out: The Calamitous Final Days of MGM", "Restoration Hardware", "Louis Silvie \"Louie\" Zamperini", "Keelung", "Minneapolis", "Idisi", "French composer Ambroise Thomas", "Hans Rosenfeldt", "May 4, 2004.", "Everything Is wrong", "Royal Navy rank of Captain", "Smoothie King Center", "Martin Scorsese", "Viacom Media Networks", "1853", "\"Pimp My Ride\"", "April 19, 1994", "\"Q\u0307adar A\u1e8bmat-khant Ramzan\"", "Derry City F.C.", "Fort Hood, Texas", "Bonny Hills", "London", "1872", "2006", "small forward", "Samuel Joel \" Zero\" Mostel", "October 13, 1980", "Chechen Republic", "House of Commons", "1926", "Nikolai Morozov", "1968", "Bernd Bertie", "Girl Meets World", "January 15, 1975", "Pansexuality", "Javan leopard", "2,463,431", "`` to the churches of Galatia '' ( Galatians 1 : 2 )", "Narnia", "( phj\u028c\u014b. t\u0255ha\u014b )", "and", "Rudolph", "Chechnya", "Aryan Airlines Flight 1625", "Sen. Barack Obama", "She spoke with CNN's Larry King about her new book, a recent tabloid report detailing her son's partying ways and the infamous bong photo.", "Baseball Player", "Glinda", "Persian Gulf"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6648021708683474}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8235294117647058, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3930", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-3909", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-4818", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-1797", "mrqa_triviaqa-validation-7434", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1892", "mrqa_searchqa-validation-4914", "mrqa_searchqa-validation-8010", "mrqa_searchqa-validation-5368"], "SR": 0.578125, "CSR": 0.5662499999999999, "retrieved_ids": ["mrqa_squad-train-31354", "mrqa_squad-train-75327", "mrqa_squad-train-30436", "mrqa_squad-train-34157", "mrqa_squad-train-27726", "mrqa_squad-train-51800", "mrqa_squad-train-47478", "mrqa_squad-train-53663", "mrqa_squad-train-11430", "mrqa_squad-train-7135", "mrqa_squad-train-41039", "mrqa_squad-train-25291", "mrqa_squad-train-55644", "mrqa_squad-train-26343", "mrqa_squad-train-83271", "mrqa_squad-train-12894", "mrqa_triviaqa-validation-7707", "mrqa_searchqa-validation-5460", "mrqa_naturalquestions-validation-8619", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-7209", "mrqa_hotpotqa-validation-1542", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-4573", "mrqa_squad-validation-5112", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5960", "mrqa_squad-validation-10413", "mrqa_squad-validation-9489", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-7896"], "EFR": 1.0, "Overall": 0.7373125}, {"timecode": 25, "before_eval_results": {"predictions": ["1894", "Effective", "a pointless pursuit", "warning the operators, who may then manually suppress the fire", "Northern Rail", "South Korea", "botulism", "metalwoods", "Chad", "Pocahontas", "Matlock", "Washington", "Chile", "The Blue Boy", "Three Worlds", "Narcissus", "NUT", "Pennsylvania", "eastern Pyrenees", "The Mayor of Casterbridge", "Dutch", "Salem witch trials", "Gryffindor", "Allardyce", "Olympics", "Nick Hancock", "Edward Yorke", "keeper of the Longstone (Fame Islands) lighthouse", "Mase", "Superman: The Movie", "Richard Walter Jenkins", "Burkina Faso", "Jimi Hendrix", "Javier Bardem", "Independence Day", "proton", "Jordan", "So Solid Crew", "John Regis", "Magi", "albion", "the Bachelor of Science", "Ash", "Ian Botham", "squash", "Leander Club", "Sir Stirling Craufurd Moss", "The Real Miracle of Charlotte's Web", "Poland", "full-contact", "albion", "albion", "Authority", "1 mile ( 1.6 km )", "Steve Valentine", "Out of Control", "Virgin", "UFC Fight Pass", "Airbus A330-200", "fill a million sandbags and place 700,000 around our city", "75 percent of utilities had taken steps to mitigate the Aurora vulnerability", "a scribe", "amelia earhart", "Final Cut Pro"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6380208333333334}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3207", "mrqa_triviaqa-validation-2393", "mrqa_triviaqa-validation-5112", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-5974", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-4836", "mrqa_hotpotqa-validation-5822", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-5324"], "SR": 0.5625, "CSR": 0.5661057692307692, "EFR": 0.9642857142857143, "Overall": 0.7301407967032967}, {"timecode": 26, "before_eval_results": {"predictions": ["at the narrow end", "Levi's Stadium", "specific catechism questions", "a 12th/13th-century nobleman", "between 100,000 and 180,000 light - years", "2016", "Muhammad", "Mel Tillis", "Panthalassa", "Stephen Lang", "2018", "Erika Mitchell Leonard", "eight years after an amendment increased the tenure length by two years", "Edd Kimber", "Romancing the Stone", "Orange Juice", "photodiode", "September 9, 2010", "ABC", "dromedary", "Dan Stevens", "Ben Fransham", "Grey Wardens", "1979", "October 27, 2016", "auctoritas", "knowledge", "Luther Ingram", "Jodie Foster", "Barry Watson", "Sanchez Navarro", "growing faster than the rate of economic growth", "1936", "British Columbia, Canada", "New York University", "2007", "2001", "Washington Redskins", "Hebrew Bible", "September 14, 2008", "Consular Report of Birth Abroad for children born to U.S. citizens", "Pasek & Paul", "Chicago metropolitan area", "conquistador Francisco Pizarro", "1940", "Norman", "Mary Rose Foster", "John Smith", "The eighth and final season of the fantasy drama television series", "1603", "neutrality", "cheated on Miley", "a stringed musical instrument", "anabaptists", "The Rocky and Bullwinkle", "Taylor Swift", "Nick Weidenfeld and Keith Crofford", "Michael Crawford", "$22 million", "14-day", "flooding", "chile", "david", "a year of less"], "metric_results": {"EM": 0.515625, "QA-F1": 0.582445283984806}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.19047619047619044, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.35294117647058826, 0.33333333333333337, 1.0, 0.2666666666666667, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.12500000000000003, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 0.5555555555555556, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-6131", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-2872", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7563", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-5461"], "SR": 0.515625, "CSR": 0.5642361111111112, "EFR": 0.967741935483871, "Overall": 0.7304581093189964}, {"timecode": 27, "before_eval_results": {"predictions": ["Luther's concern to clarify and strengthen the text and to provide an appropriately prayerful tune", "computational complexity theory", "Best Supporting Actress", "around 300", "an anvil", "1999", "Robert G. Durant", "she was the first to recognise that the machine had applications beyond pure calculation, and created the first algorithm", "London", "currently Ron Kouchi", "Hanford Site", "Native American tradition", "Mindy Kaling", "Alonso L\u00f3pez", "Blackstone", "Ginger Rogers", "The Double Life of V\u00e9ronique", "churros", "Christies Beach", "Odisha", "Arsenal", "Don Bluth and Gary Goldman", "torpedoes", "1969 until 1974", "skiing and mountaineering", "June 11, 1973", "January 18, 1977", "Protestant Christian", "defender", "Henry J. Kaiser", "Saoirse Ronan", "122,067", "Wandsworth, London", "association football YouTube", "Daniel Andre Sturridge", "USS \"Essex\" (CV/CVA/CVS-9)", "Ron Cowen and Daniel Lipman", "1991's \"The Changing Scottish Landscape\"", "Captain while retaining the substantive rank of Commodore", "Giuseppe Verdi", "Andrzej Go\u0142ota and Tomasz Adamek", "Russell T Davies", "Geraldine Page", "Manchester", "3,000", "Albert II, Prince of Monaco, Umberto II", "Ontario to the north, the US state of Minnesota to the west, and Wisconsin and the Upper Peninsula of Michigan", "The Rite of Spring", "saloon-keeper and Justice of the Peace", "John Lennon", "The International Imitation Hemingway Competition", "The Emperor of Japan", "Mary Rose Foster", "1986", "changes the relationship of the overall pitch range compared to the range of the instruments or voices that perform the music", "Fortieth", "nellie", "aphasia", "Thailand sent troops to retrieve the trio and gradually built up their numbers", "Amanda Knox's aunt", "100,000", "brandy", "Sgt. Pepper", "North Carolina"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5943522472394756}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, true, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 0.8, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.7857142857142857, 0.0, 0.33333333333333337, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.08695652173913043, 1.0, 1.0, 0.0, 0.25, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2347", "mrqa_squad-validation-1672", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4543", "mrqa_naturalquestions-validation-4497", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-5866", "mrqa_triviaqa-validation-2192", "mrqa_newsqa-validation-310", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-9122"], "SR": 0.46875, "CSR": 0.5608258928571428, "retrieved_ids": ["mrqa_squad-train-35384", "mrqa_squad-train-74994", "mrqa_squad-train-48691", "mrqa_squad-train-47654", "mrqa_squad-train-9510", "mrqa_squad-train-38534", "mrqa_squad-train-30374", "mrqa_squad-train-66273", "mrqa_squad-train-16349", "mrqa_squad-train-77605", "mrqa_squad-train-55087", "mrqa_squad-train-75920", "mrqa_squad-train-30573", "mrqa_squad-train-26896", "mrqa_squad-train-57277", "mrqa_squad-train-68959", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-8728", "mrqa_newsqa-validation-1836", "mrqa_hotpotqa-validation-5303", "mrqa_naturalquestions-validation-2148", "mrqa_squad-validation-9286", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-7896", "mrqa_hotpotqa-validation-508", "mrqa_squad-validation-2506", "mrqa_squad-validation-7382", "mrqa_newsqa-validation-2601", "mrqa_naturalquestions-validation-808", "mrqa_squad-validation-6316", "mrqa_squad-validation-3373"], "EFR": 1.0, "Overall": 0.7362276785714286}, {"timecode": 28, "before_eval_results": {"predictions": ["left foot", "the USSR", "flagellated", "Juliet", "Adidas", "Secretary of State Hillary Clinton", "billions of dollars", "one", "the Beatles", "Communist Party of Nepal", "Pope Benedict XVI", "8 p.m. local time", "Sri Lanka's Tamil rebels", "64", "CNN", "within a few months", "A witness", "Adriano", "he eventually gave up 70 percent of his father-in-law's farm", "183", "American Civil Liberties Union", "air support", "40 militants and six Pakistan soldiers dead", "Liverpool Street Station", "137", "54-year-old", "Congressional auditors", "Jacob", "South Africa", "Ohio River near Warsaw, Kentucky", "4,000", "Baja California Language College", "provided Syria and Iraq 500 cubic meters of water", "the Catholic League", "August 19, 2007", "10 years", "all three pleaded not guilty in an appearance last week in Broward County Circuit Court.", "Japan", "she is God-sent", "consumer confidence", "political and religious", "six", "nearly 28 years", "July 18, 1994", "Dan Brown", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves", "waterboarding was among the operational details that had not been declassified.", "Chao Phraya River", "two", "antihistamine", "4,000", "he lost his virginity at age 14.", "Jean F Kernel", "10 : 30am", "1439", "tide-wise", "Christian Wulff", "Gardiner", "general secretary", "the George Washington Bridge", "Johns Creek", "junk", "Aristotle's lantern", "albacore"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6557047849811009}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.33333333333333337, 0.5454545454545454, 1.0, 1.0, 0.4, 1.0, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.9473684210526316, 1.0, 1.0, 0.6666666666666666, 0.4444444444444445, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.0, 0.5, 0.5, 0.13333333333333333, 0.0, 1.0, 0.4, 1.0, 0.2, 0.4, 0.8, 0.6666666666666666, 0.0, 1.0, 0.0, 0.2, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8652", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1314", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-3530", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9060", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-2787", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-12506"], "SR": 0.46875, "CSR": 0.5576508620689655, "EFR": 0.9705882352941176, "Overall": 0.7297103194726166}, {"timecode": 29, "before_eval_results": {"predictions": ["Cadeby", "Near Sankt Goarshausen", "10,004,721", "July 7, 2015", "lovebirds", "Chicago", "monk seal", "Wilhelm", "british", "Take Me Out to the Ballgame", "an expression used in drinking a person's health.", "\"What hath God wrought\"", "Stewart Island", "Saint Elmo's Fire", "white bread and butter", "The Time Machine", "milk", "illegible", "Scrabble", "Mussolini", "valkyries", "rain", "bach", "Jodie Foster", "Elysian Fields", "\"Vietnam.\"", "Thomas Edison", "Manhattan Project", "Charles I", "divorce", "Enchanted", "the Liberty Bell", "USB", "Arizona", "Destiny", "Byron", "a spoonful of sugar", "Prednisone & Prednisolone", "Margot Fonteyn", "a Coral reef fish", "McMillan & wife", "(Whizzer) White", "member of the law-degreed Jeff Spencer", "Galileo Galilei", "Existentialism", "John Donne", "Beijing", "Annie", "member of another human being", "Wallis Warfield Simpson", "a queen", "neurons", "Vatican City", "James W. Marshall", "a set of related data", "South Korea", "\"Slow\"", "Monster M*A*S*H", "F/A-18F Super Hornet", "5249", "Fleetwood Mac", "around 3.5 percent of global greenhouse emissions", "The e-mails] are almost like reading a novel that you would embarrassed to buy,\" Mosteller said.", "HPV (human papillomavirus) vaccine the next time she went to see her doctor."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6183712121212122}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, false, false, true, false, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.7272727272727272, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-9015", "mrqa_searchqa-validation-10014", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-8042", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-5788", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-5715", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-9472", "mrqa_searchqa-validation-15174", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-2956", "mrqa_triviaqa-validation-935", "mrqa_hotpotqa-validation-739", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1372"], "SR": 0.546875, "CSR": 0.5572916666666667, "EFR": 0.9655172413793104, "Overall": 0.7286242816091953}, {"timecode": 30, "UKR": 0.73046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2020", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3206", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5677", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1707", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9311", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2939", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-668", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11465", "mrqa_searchqa-validation-12078", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14468", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-2040", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5368", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7233", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-915", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10008", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-10251", "mrqa_squad-validation-10279", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10427", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-121", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1449", "mrqa_squad-validation-1461", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1856", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2194", "mrqa_squad-validation-2318", "mrqa_squad-validation-2434", "mrqa_squad-validation-2506", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2768", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3223", "mrqa_squad-validation-3326", "mrqa_squad-validation-3363", "mrqa_squad-validation-3456", "mrqa_squad-validation-3497", "mrqa_squad-validation-354", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3930", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4226", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4343", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4621", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4902", "mrqa_squad-validation-4965", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5118", "mrqa_squad-validation-5242", "mrqa_squad-validation-525", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5389", "mrqa_squad-validation-5590", "mrqa_squad-validation-5605", "mrqa_squad-validation-5624", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5865", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5973", "mrqa_squad-validation-6025", "mrqa_squad-validation-6181", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-629", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6467", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6921", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7040", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8233", "mrqa_squad-validation-8295", "mrqa_squad-validation-8452", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8596", "mrqa_squad-validation-89", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-9614", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9920", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1382", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-2919", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-922"], "OKR": 0.84765625, "KG": 0.4875, "before_eval_results": {"predictions": ["ten minutes", "1892", "motivated students", "Nikolai Trubetzkoy", "June 1925", "\"bushwhackers\"", "British", "Argentina", "the Baudot code", "Jacksonville", "DTM", "the Switzerland national team", "Maryland", "eastern Tennessee", "John Ford", "Operation Watchtower", "34.9 kilometres", "1 December 1948", "omnisexuality", "Westfield Tea Tree Plaza", "Avoca Lodge", "Atlanta, Georgia", "Atlanta Braves", "Scunthorpe", "2004", "Donald McNichol Sutherland", "Towards the Sun", "the heart of the southern (Dolomitic) Alps in the Veneto region of Northern Italy", "Angus Brayshaw", "An impresario", "Islamic philosophy", "January 30, 1930", "Sulla", "the Matildas", "Jaguar Land Rover", "tempo", "Milk Barn Animation", "McLaren-Honda", "Timothy Dowling", "London", "Jane", "Patricia Arquette", "Otto Robert Frisch", "AMC Entertainment Holdings, Inc.", "31 low-budget British comedy motion pictures", "Robert Paul \"Robbie\" Gould III", "Eddie Collins", "James Anthony Sturgess", "twenty-three", "Gararish", "P. C. Sreeram", "Akosua Busia", "September 8, 2017", "volcanic activity", "Burbank, California", "a horses life, and the friendships", "Heisenberg", "the White Sea Canal", "contract talks", "Eintracht Frankfurt", "Republican", "a poodle", "Nickelback", "Will & Grace"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6453768887362638}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.625, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.2857142857142857, 0.28571428571428575, 0.3333333333333333, 0.0, 0.4, 0.0, 0.0, 0.8, 1.0, 1.0, 0.3076923076923077, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1311", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-4173", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-4389", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-5187", "mrqa_naturalquestions-validation-6012", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-6433", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2032"], "SR": 0.53125, "CSR": 0.5564516129032258, "retrieved_ids": ["mrqa_squad-train-21101", "mrqa_squad-train-47060", "mrqa_squad-train-83311", "mrqa_squad-train-60185", "mrqa_squad-train-15219", "mrqa_squad-train-37012", "mrqa_squad-train-67612", "mrqa_squad-train-35276", "mrqa_squad-train-63218", "mrqa_squad-train-5164", "mrqa_squad-train-22931", "mrqa_squad-train-52502", "mrqa_squad-train-68653", "mrqa_squad-train-11024", "mrqa_squad-train-2351", "mrqa_squad-train-78051", "mrqa_hotpotqa-validation-4436", "mrqa_newsqa-validation-3786", "mrqa_searchqa-validation-9250", "mrqa_naturalquestions-validation-75", "mrqa_squad-validation-6244", "mrqa_naturalquestions-validation-3048", "mrqa_searchqa-validation-6076", "mrqa_triviaqa-validation-922", "mrqa_searchqa-validation-2162", "mrqa_triviaqa-validation-192", "mrqa_squad-validation-8596", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-3640", "mrqa_searchqa-validation-9123", "mrqa_squad-validation-5450", "mrqa_hotpotqa-validation-4273"], "EFR": 1.0, "Overall": 0.7244153225806451}, {"timecode": 31, "before_eval_results": {"predictions": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "500", "7.63\u00d725mm Mauser", "the Harpe brothers", "French", "1944", "Clara Petacci", "2007", "Marko Tapani \" Marco\" Hietala", "Shankar", "Cody Miller", "\"The Manhunter from Mars\" in \" Detective Comics\" #225 (Nov. 1955)", "film and short novels", "Carson City", "MTV's", "Mickey's Christmas Carol", "ten", "Alpine, New Jersey", "the 1824 Constitution of Mexico", "Hellenism", "Tomorrowland", "Jaffrey", "Professor Frederick Lindemann,", "Rawhide", "astronomer and composer of German and Czech-Jewish origin", "Don DeLillo", "The Seduction of Hillary Rodham", "Balloon Street, Manchester", "9,984", "the Rose Garden", "Spain", "Deep Purple", "Abdul Razzak Yaqoob", "Port Macquarie", "Dan Castellaneta", "Roseann O'Donnell", "Saturday", "Taylor Alison Swift", "Pearl Brewing Company", "Centers for Medicare and Medicaid Services", "Indianapolis Motor Speedway", "Creech Air Force Base", "Tampa Bay Storm", "Jango", "High Court of Admiralty", "An All-Colored Vaudeville Show", "German", "Lucy Muringo Gichuhi", "Valley Falls", "Craps", "Nicholas \" Nick\" Offerman", "Dutch", "JackScanlon", "Leonard Bernstein", "62", "France", "chloronium", "secretary", "five", "a nuclear weapon", "2005", "Beastie Boys", "Madison", "Cobra"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7221117424242425}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, true, false, true, true, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.1818181818181818, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.8, 0.4, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-5501", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-1505", "mrqa_naturalquestions-validation-4995", "mrqa_triviaqa-validation-1534", "mrqa_newsqa-validation-3106", "mrqa_searchqa-validation-10363"], "SR": 0.609375, "CSR": 0.55810546875, "EFR": 1.0, "Overall": 0.72474609375}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh Downs", "aggregate savings and investment", "Till Death Us Do Part", "Laputa", "Leeds", "\"Colonel\" Thomas Andrew \"Tom\" Parker", "LSD", "Geoffrey Plantagenet", "lek\u00eb Dukagjini", "Tombstone", "Charlie", "Jaguar Land Rover", "Diego Maradona", "Sudan", "Bubba", "football", "a web-based teaching aid", "a cheese wheel", "Greece", "1932", "Steve Coogan", "Elektra", "Boston Marathon", "Carl Smith", "Humble Pie", "Jorge Lorenzo", "Rescuers", "checkers", "Les Dawson", "Arthur, Prince of Wales", "Grail", "Ronald Reagan", "Barry Copeland", "climate of the regions of the planet", "Paris", "Hammer", "the esophagus", "Guildford Dudley", "british", "John Howard", "Uriah the Hittite", "His Holiness", "12th", "Cornell University", "Flybe", "Speedway Free Festival", "duck", "Lost Weekend", "Stockholm", "Swiss", "taekwondo", "tomato", "the senior-most judge of the supreme court", "early Christians of Mesopotamia", "Representatives are not restricted to voting for one of the nominated candidates and may vote for any person, even for someone who is not a member of the House at all", "2006", "Central Avenue", "middleweight", "Opposition parties", "preventative forestry", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Canterbury", "Harold Macmillan", "marsh"], "metric_results": {"EM": 0.53125, "QA-F1": 0.592006414726332}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19354838709677416, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4210526315789474, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7331", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-5892", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-3007", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-3636", "mrqa_triviaqa-validation-6680", "mrqa_triviaqa-validation-7163", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-2330", "mrqa_naturalquestions-validation-3569", "mrqa_hotpotqa-validation-1023", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-3089", "mrqa_searchqa-validation-6833"], "SR": 0.53125, "CSR": 0.5572916666666667, "EFR": 1.0, "Overall": 0.7245833333333334}, {"timecode": 33, "before_eval_results": {"predictions": ["Brown v. Board of Education of Topeka", "midnight one night", "its", "Ennis", "Stratfor's website", "in the last few months,", "Jaime Andrade", "parker's disorder", "girls", "possible victims of physical and sexual abuse.", "the island's dining scene", "gasoline", "david hemery", "\"AS IS/ HERE IS\"", "he won two Emmys for work on the 'Columbo' series starring Peter Falk.", "ice jam", "mike latway", "abduction of minors.", "regia", "navy", "jenny Sanford,", "Florida", "Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Clifford Harris,", "Pew Research Center", "Nirvana", "vivian", "james polis", "it was based on the latter.", "he won it with a clear strategy that was stuck to with remarkably little internal drama.", "between June 20 and July 20", "johnny Clarkson", "misdemeanor", "1.2 million", "pythons", "Heshmatullah Attarzadeh", "crossfire by insurgent small arms fire,", "2002", "if she would try to travel to Japan for summer vacation.", "a \"new chapter\" of improved governance in Afghanistan", "Arsene Wenger", "outside the municipal building of Abu Ghraib in western Baghdad", "shelling of the compound", "in the mouth.", "Atlantic Ocean", "Bahrami's eyes", "Nepal", "Jiverly Wong,", "uriah stiles, faces 22 felony counts in connection with the videotape,", "the Louvre", "September 21.", "ditches.", "Supplemental oxygen", "Prince Bao", "Narendra Modi", "david hemery", "74", "mike heeran", "Musicology", "Randall Boggs", "Mick Jackson,", "West Virginia", "Sid Vicious", "paris"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5236538461538462}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.56, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1626", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2086", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-3907", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4062", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-7079", "mrqa_searchqa-validation-8669", "mrqa_searchqa-validation-44"], "SR": 0.453125, "CSR": 0.5542279411764706, "retrieved_ids": ["mrqa_squad-train-19463", "mrqa_squad-train-9521", "mrqa_squad-train-62577", "mrqa_squad-train-13492", "mrqa_squad-train-69581", "mrqa_squad-train-52666", "mrqa_squad-train-68083", "mrqa_squad-train-52619", "mrqa_squad-train-3758", "mrqa_squad-train-7086", "mrqa_squad-train-18479", "mrqa_squad-train-17558", "mrqa_squad-train-17462", "mrqa_squad-train-78824", "mrqa_squad-train-50473", "mrqa_squad-train-17160", "mrqa_searchqa-validation-14723", "mrqa_hotpotqa-validation-471", "mrqa_squad-validation-1808", "mrqa_triviaqa-validation-2154", "mrqa_squad-validation-8755", "mrqa_searchqa-validation-5713", "mrqa_hotpotqa-validation-1964", "mrqa_squad-validation-8602", "mrqa_triviaqa-validation-6847", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-5174", "mrqa_searchqa-validation-9096", "mrqa_naturalquestions-validation-808", "mrqa_triviaqa-validation-7226", "mrqa_hotpotqa-validation-1473", "mrqa_searchqa-validation-457"], "EFR": 1.0, "Overall": 0.723970588235294}, {"timecode": 34, "before_eval_results": {"predictions": ["most common", "boudins", "Starship Troopers", "Mumbai", "Indiana", "zoology", "The Moonwalk", "Laos", "Peter Davison", "Westminster Abbey", "Battle of Agincourt", "white spirit", "King George III", "Kent", "Bayswater", "Diptera", "a turkey", "transuranic", "Harold Shipman", "River Wyre", "Carson City", "All Things Must Pass", "Sino-British Joint Declaration", "Mercury", "Doctor Who", "North Yorkshire", "George Blake", "Nirvana", "Janis Joplin", "Kenya", "Manchester City F.C.", "Moscow", "Caracas", "skin care", "fur", "art supply stores", "Bathsheba", "Quentin Tarantino", "Dita Von Teese", "a collapsible support assembly", "Republican", "Argentina", "French", "dennis taylor", "internal kidney structures", "bach", "Rocky Marciano", "The Benedictine Order", "m69. Coventry to Leicester Motorway", "June Brae", "Jack Lemmon", "four", "1965", "2018", "a lightning strike", "Danny Glover", "Trey Parker and Matt Stone", "140 to 219 passengers", "Hundreds", "Democrats", "31 meters (102 feet)", "dennis Galahad", "Sacramento, California", "Hawaii"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6890624999999999}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5333333333333333, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7728", "mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-7774", "mrqa_triviaqa-validation-4133", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4317", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-10490", "mrqa_hotpotqa-validation-398", "mrqa_newsqa-validation-3976", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-4416", "mrqa_searchqa-validation-3920"], "SR": 0.640625, "CSR": 0.5566964285714286, "EFR": 0.9130434782608695, "Overall": 0.7070729813664596}, {"timecode": 35, "before_eval_results": {"predictions": ["late 1970s", "Aristotle", "daiquiri", "Calvary", "armadillos", "the Elizabethan Theatres", "Danielle Steel", "Absalom", "joe mercer", "The Goonies", "Annin", "Quito", "Seine", "wine", "Alyssa Milano", "bites a dog", "the Star-Spangled Banner", "The Rolling Stones", "London", "a roosevelt", "Benjamin Franklin", "Bob Dylan", "a urinal", "Apollo 11", "Portugal", "Cadillac", "George Clooney", "a great American Novel", "peace", "white", "dame of balfour", "a statement", "Easton", "Scrabble", "Iceland", "the Ozark Trail", "a baby", "upton Sinclair", "Stephen", "Brooke Bollea", "a war", "Nancy Sinatra", "David", "wine from northern Italy", "Robert Lowell", "forgo", "Richmond", "\"He doesn't want you to know,\"", "Amy Tan", "Florence", "Pandora", "Grenada", "Himalayas", "Kusha", "`` Heroes and Villains ''", "Costa Brava", "silver", "1", "2015", "October 20, 2017", "Columbus", "Gustav's top winds weakened to 110 mph,", "Sen. Piedad Cordoba", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6496527777777777}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-1076", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-2248", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-16289", "mrqa_searchqa-validation-245", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-11471", "mrqa_searchqa-validation-15863", "mrqa_searchqa-validation-8076", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-3703", "mrqa_searchqa-validation-559", "mrqa_searchqa-validation-3922", "mrqa_searchqa-validation-9192", "mrqa_naturalquestions-validation-10026", "mrqa_triviaqa-validation-6212", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-513"], "SR": 0.59375, "CSR": 0.5577256944444444, "EFR": 1.0, "Overall": 0.7246701388888889}, {"timecode": 36, "before_eval_results": {"predictions": ["1082", "Pfc. Bowe Bergdahl,", "fans of Michael Jackson", "the mammoth's skull,", "Symbionese Liberation Army", "a floating National Historic Landmark,", "a mechanism at the federal level to ensure that drivers comply.", "Tim Clark, Matt Kuchar and Bubba Watson", "a long-range missile", "75 percent", "prisoners at the South Dakota State Penitentiary", "women", "CNN/Opinion Research Corporation", "Kingdom City", "an engineering and construction company with a vast personal fortune. As mayor of Seoul from 2002 to 2004,", "the Ku Klux Klan", "President Felipe Calderon", "137", "3-3 draw at Karlsruhe.", "\"Dancing With the Stars\"", "\"It shot up the Japanese singles chart, reaching No 4, the highest ever position for a first time enka release.", "Michael Jackson", "\"a striking blow to due process and the rule of law.\"", "Venezuela", "the way their business books were being handled.", "the Nazi war crimes suspect", "a number of calls,", "Mandi Hamlin", "the South Dakota State Penitentiary", "Department of Homeland Security Secretary Janet Napolitano", "Russian concerns that the defensive shield could be used for offensive aims.", "Bob Dole,", "they can demonstrate they have been satisfactorily treated for at least 12 months.", "Tennessee", "\"Dance Your Ass Off\"", "Malawi", "246", "\"significant skeletal remains\" consistent with those of a small child on the outer perimeter of the search area.", "six", "al-Douri,", "Nearly eight in 10", "a one-shot victory in the Bob Hope Classic", "in the Muslim north of Sudan", "Jeannie Longo-Ciprelli", "Clifford Harris,", "Kyra and Violet", "Susan Boyle", "Colorado", "UNICEF", "the United States, NATO member states, Russia and India", "27", "45 %", "you", "April 1, 2002", "One Direction", "Runcorn", "oxygen", "Ben R. Guttery", "Preston", "from 1993 to 1996", "Ecuador", "Halloween", "the inheritance of each trait", "you"], "metric_results": {"EM": 0.53125, "QA-F1": 0.644246429632459}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, true, false, true, false, true, true, false, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.18181818181818182, 0.0, 1.0, 0.0, 0.47058823529411764, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.4444444444444445, 1.0, 1.0, 0.0, 0.2222222222222222, 0.4, 1.0, 1.0, 0.33333333333333337, 1.0, 0.5, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1755", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3380", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3069", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5006", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10010", "mrqa_searchqa-validation-4136"], "SR": 0.53125, "CSR": 0.5570101351351351, "retrieved_ids": ["mrqa_squad-train-6674", "mrqa_squad-train-28417", "mrqa_squad-train-70748", "mrqa_squad-train-5679", "mrqa_squad-train-28140", "mrqa_squad-train-81529", "mrqa_squad-train-23192", "mrqa_squad-train-77919", "mrqa_squad-train-19822", "mrqa_squad-train-173", "mrqa_squad-train-60272", "mrqa_squad-train-23208", "mrqa_squad-train-9850", "mrqa_squad-train-56364", "mrqa_squad-train-74035", "mrqa_squad-train-34499", "mrqa_naturalquestions-validation-3569", "mrqa_searchqa-validation-15243", "mrqa_naturalquestions-validation-75", "mrqa_searchqa-validation-177", "mrqa_newsqa-validation-2934", "mrqa_searchqa-validation-429", "mrqa_squad-validation-8910", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-13377", "mrqa_triviaqa-validation-6979", "mrqa_squad-validation-7430", "mrqa_squad-validation-7632", "mrqa_newsqa-validation-274", "mrqa_squad-validation-89", "mrqa_hotpotqa-validation-290", "mrqa_newsqa-validation-858"], "EFR": 0.9666666666666667, "Overall": 0.7178603603603604}, {"timecode": 37, "before_eval_results": {"predictions": ["$20 billion.", "the Veneto region of Northern Italy", "Harris Museum, Art Gallery & Preston Free Public Library", "Jean de Florette", "George Orwell", "Eric Allan Kramer", "eight", "Kathryn Bigelow", "George Orwell", "Ben Ainslie", "1905", "Sex Drive", "Yoruba", "Archbishop of Canterbury", "first wife Anna", "Chrysler", "Portal", "a chronological collection of critical quotations", "Salim Stoudamire", "fifth", "one", "Evey", "O", "The Grandmaster", "Scotland", "1960s", "half of the Nobel Prize in Physics", "Russian Empire", "West Point", "Hilary Duff", "Ogallala Aquifer", "October 21, 2016", "fifth", "Everything Iswrong", "Massapequa", "1988", "Dan Bilzerian", "Ny-\u00c5lesund", "1967", "commercial", "Andrea Maffei", "band director", "1837", "$10\u201320 million", "Mandarin", "Fester Addams", "March", "Michael-Leon Wooley", "Esp\u00edrito Santo Financial Group", "Los Angeles", "The New Yorker", "Walter Egan", "his 1970 triple album All Things Must Pass", "Union", "Alison Krauss", "Graham Henry", "earwax", "mental health and recovery.", "the Bronx", "billions of dollars", "Diamond", "Simon Legree", "Sideways", "pindar"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5803104575163398}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-459", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-4687", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-767", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-8753"], "SR": 0.53125, "CSR": 0.5563322368421053, "EFR": 1.0, "Overall": 0.724391447368421}, {"timecode": 38, "before_eval_results": {"predictions": ["Turing machines", "Nepal", "Wang Chung", "Panama", "a gastropod", "Thailand", "Abraham Lincoln", "a small gut", "Georgie Porgie", "Mork & Mindy", "Catherine de Medici", "a form of competitive horse training", "Benito", "Southern California", "Fort Leavenworth", "INXS", "Flat", "the blue wildebeest", "Extra-Terrestrial Intelligence", "Arthur", "Kenneth Noland", "Clara Barton", "Nine to Five", "snake", "moose", "Winnipeg", "Nicaragua", "Arthur Miller", "Princess Margaret,", "1937", "a large brown seaweed", "feminism", "San Diego Comic-Con", "the gallbladder", "the cousin of his wife, Mattie Silver", "midway", "Liechtenstein", "Custer", "Gilead", "salt", "Gloria Steinem", "Catherine de Medici", "Tonga", "Minos", "Gulliver", "a pina colada", "SeaWorld", "a final killing blow", "Tyra Banks", "Dick Gephardt", "Bucharest", "Manley", "to function like an endocrine organ", "attack on Pearl Harbor", "positive", "an inch", "American", "Province of Canterbury", "Valdosta", "Northern Rhodesia", "the actor who created one of British television's most surreal thrillers", "Goa", "facing various challenges and turning them to", "1,500"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6208964646464645}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.3888888888888889, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-8692", "mrqa_searchqa-validation-10873", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-1647", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-16148", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-2836", "mrqa_searchqa-validation-2041", "mrqa_searchqa-validation-13649", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-2115", "mrqa_hotpotqa-validation-4053", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-3579"], "SR": 0.546875, "CSR": 0.5560897435897436, "EFR": 1.0, "Overall": 0.7243429487179487}, {"timecode": 39, "before_eval_results": {"predictions": ["18", "Nalini Negi", "Blue laws", "The 1980 Summer Olympics", "The International Baccalaureate", "the medulla oblongata", "Andreas Vesalius", "`` The Crossing ''", "Nicole DuPort", "Brian Johnson", "Palmer Williams Jr.", "late as the 1890s", "prospective studies", "Big Ten Conference Champions Michigan State Spartans", "Franklin and Wake counties", "60 by West All", "( / ta\u026a\u02c8t\u00e6n\u026ak / )", "Sally Field", "Elizabeth Dean Lail", "Ravi Shastri", "chili con carne", "6 March 1983", "David Kaye", "James Arthur", "James Watson and Francis Crick", "Arctic Ocean", "during the American Civil War", "Thomas Middleditch", "slavery", "Sir Ernest Rutherford", "Buddhist", "1889", "parthenogenesis", "on the two tablets", "Buffalo Bill", "$19.8 trillion", "Sleeping with the Past", "boy or girl", "1820s", "Soviet Union", "Vienna", "Dmitri Mendeleev", "Dalveer Bhandari", "at standard temperature and pressure", "John Ernest Crawford", "July 2014", "Cathy Dennis and Rob Davis", "1924", "Americans", "China", "metamorphic rock", "Carmen", "a waterfowl", "glass", "Rikki Farr's", "the Israeli Declaration of Independence", "two Nobel Peace Prizes", "18", "2002", "Gaslight Theater", "Dragnet", "Depeche Mode", "Louis Rukeyser", "Matt Groening"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6613806460084033}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false, false, true, true, true, false, true, true, false, true, false, true, false, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 0.0, 0.11764705882352941, 0.4, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.5, 0.0, 1.0, 1.0, 0.125, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1699", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-6940", "mrqa_triviaqa-validation-590", "mrqa_newsqa-validation-2020", "mrqa_searchqa-validation-1911"], "SR": 0.59375, "CSR": 0.55703125, "retrieved_ids": ["mrqa_squad-train-45352", "mrqa_squad-train-50465", "mrqa_squad-train-36383", "mrqa_squad-train-19294", "mrqa_squad-train-3650", "mrqa_squad-train-8325", "mrqa_squad-train-27537", "mrqa_squad-train-10638", "mrqa_squad-train-45203", "mrqa_squad-train-44708", "mrqa_squad-train-14172", "mrqa_squad-train-85105", "mrqa_squad-train-27281", "mrqa_squad-train-75563", "mrqa_squad-train-56168", "mrqa_squad-train-46252", "mrqa_hotpotqa-validation-65", "mrqa_squad-validation-6361", "mrqa_naturalquestions-validation-8254", "mrqa_triviaqa-validation-287", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-2438", "mrqa_searchqa-validation-16595", "mrqa_squad-validation-434", "mrqa_newsqa-validation-3557", "mrqa_searchqa-validation-679", "mrqa_hotpotqa-validation-2425", "mrqa_searchqa-validation-457", "mrqa_hotpotqa-validation-5866", "mrqa_searchqa-validation-8348", "mrqa_triviaqa-validation-5993", "mrqa_searchqa-validation-2337"], "EFR": 0.9615384615384616, "Overall": 0.7168389423076922}, {"timecode": 40, "UKR": 0.765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2240", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3867", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3988", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-855", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1236", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7039", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3028", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4018", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-779", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-1190", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3249", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9559", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1071", "mrqa_squad-validation-1088", "mrqa_squad-validation-1138", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1808", "mrqa_squad-validation-1814", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2609", "mrqa_squad-validation-2888", "mrqa_squad-validation-3086", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-3415", "mrqa_squad-validation-350", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4162", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-434", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4783", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5001", "mrqa_squad-validation-5098", "mrqa_squad-validation-5167", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5318", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5889", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6511", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-7961", "mrqa_squad-validation-806", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8602", "mrqa_squad-validation-8647", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8910", "mrqa_squad-validation-8910", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1336", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3488", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4397", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-7415", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-822"], "OKR": 0.861328125, "KG": 0.503125, "before_eval_results": {"predictions": ["the architect or engineer", "Naples", "malaria", "Jefferson", "Rubik's Cube", "a kettledrum", "Meringue", "(company)", "an axe", "Department of Justice", "Jimmy Doolittle", "John Brown", "Anamosa", "One Hundred Years of Solitude", "Frida Kahlo", "Aziraphale", "Wodehouse", "Corsica", "(litho)", "William Pitt the Younger", "Popcorn", "Madonna", "Welterweight", "yoyo", "Winston-Salem", "A Streetcar Named Desire", "Edinburgh", "(kloram-fen-kol)", "Forward, midfielder, defender", "Colorado columbine", "Italy", "Kwanzaa", "Woody Guthrie", "Nigeria", "William Jennings Bryan", "the Spiderwick Chronicles", "a petition", "Chicago", "the Great Pyramid", "Herod", "Alaska", "Marriage Crunch", "Asia", "anaphylaxis", "Peter Pan", "Kuwait", "the rd", "The Day of the Locust", "diamond", "Charlie Sheen", "The Call of the Wild", "Gibraltar", "the National League ( NL ) champion Chicago Cubs and the American League ( AL ) champion Cleveland Indians", "1923", "Bahrain", "El Hiero", "Hans Lippershey", "\"Sippin' on Some Syrup,\"", "Larry Eustachy", "Isabella II", "Stanford", "Vicente Carrillo Leyva,", "a shortfall in their pension fund", "2018"], "metric_results": {"EM": 0.546875, "QA-F1": 0.644764254385965}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.20000000000000004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.631578947368421, 1.0, 1.0, 0.5, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8782", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-4950", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-7034", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-11346", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-16742", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-13271", "mrqa_searchqa-validation-57", "mrqa_searchqa-validation-16298", "mrqa_searchqa-validation-13046", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-14691", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-3504", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-5758", "mrqa_searchqa-validation-15319", "mrqa_naturalquestions-validation-6197", "mrqa_triviaqa-validation-6424", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4568", "mrqa_newsqa-validation-3554"], "SR": 0.546875, "CSR": 0.5567835365853658, "EFR": 0.9655172413793104, "Overall": 0.7304757805929352}, {"timecode": 41, "before_eval_results": {"predictions": ["$40,000", "the Stockton & Darlington Railway", "ure", "Israel", "prince Rainier", "Harper", "Fred Astaire", "Humphrey Bogart", "honda", "Alan Bartlett Shepard Jr.", "by burning nitrates and mercuric oxides", "the le Carr\u00e9 Omnibus", "\"flipping\"", "Rosslyn Chapel", "Hispaniola", "the Zulus", "blood", "Ironside", "Aristotle", "Basil Fawlty", "South Sudan", "Tuesday", "the Dannebrog", "Secretary of State William H. Seward", "the east coast", "Lavoisier", "NOW Magazine", "Tuscany", "March 6, 1836", "Beaujolais", "Edmund Cartwright", "Der Stern", "(Jefferson) van Veen", "Plautius Lateranus", "kautta", "Barry McGuigan", "Wisconsin", "John Barbirolli", "Eton College", "hampers for picnics, bespoke fragrances and event planning", "Charles Dickens", "Ted Hankey", "(Jefferson) Stilwell", "a leaf", "sternum", "Portuguese", "mexico", "Greece", "Ed Miliband", "commitment", "polio", "the Mandate of Heaven", "in the fascia surrounding skeletal muscle", "Robin", "the Distinguished Service Cross", "Indian classical music", "1998", "11", "\"an eye for an eye,\"", "Arabic, French and English", "the Schwalbe", "the owl", "Seinfeld", "Cress"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6170386904761904}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-2666", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-2826", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-3062", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-1309", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-3086", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-762"], "SR": 0.53125, "CSR": 0.5561755952380952, "EFR": 1.0, "Overall": 0.7372507440476191}, {"timecode": 42, "before_eval_results": {"predictions": ["Sybilla of Normandy", "beta decay", "Norman", "George Strait", "Andrew Gold", "1983", "virtual reality simulator", "Banquo", "Pakistan", "fall 2010", "transceivers", "Isaiah Amir Mustafa", "negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "Paracelsus", "John C. Reilly", "Marshall Sahlins", "Gloria", "Utah, Arizona, Wyoming, and Oroville, California", "the epidermis", "27 January -- 16 April 1898", "1770 BC", "360", "a single, implicitly structured data item in a table", "1959", "Gunpei Yokoi", "38 million", "Justin Bieber", "Red Sea and the east African coast", "ideology", "160km / hour", "Chinese", "Andrew Garfield", "the 90s", "Gibraltar", "electron pairs", "cut off close by the hip, and under the left shoulder", "Alice Cooper", "the best fighter", "Los Angeles", "1972", "Virgil Tibbs", "Ethel Merman", "1961", "passwords, commands and data", "National Industrial Recovery Act", "adenosine diphosphate", "General George Washington", "Barbara Eve Harris as Colonel O'Donnell", "Lake Wales", "1560s", "Gutenberg", "Wichita", "Tina Turner", "lVMH", "Henry John Kaiser", "Marilyn Martin", "SARS", "tax incentives for businesses hiring veterans out of work for more than six months,", "a donor cadaver", "23 million square meters (248 million square feet)", "neon", "evans", "the ark of acacia", "Basilan"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5434434438110909}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.0, 0.0, 1.0, 0.45454545454545453, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.33333333333333337, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.13333333333333333, 0.0, 0.15384615384615385, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4225", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-6901", "mrqa_triviaqa-validation-7013", "mrqa_hotpotqa-validation-23", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-153", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1091", "mrqa_newsqa-validation-748", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-7408", "mrqa_newsqa-validation-3408"], "SR": 0.4375, "CSR": 0.5534156976744187, "retrieved_ids": ["mrqa_squad-train-65253", "mrqa_squad-train-1521", "mrqa_squad-train-58640", "mrqa_squad-train-78600", "mrqa_squad-train-12692", "mrqa_squad-train-8192", "mrqa_squad-train-53154", "mrqa_squad-train-60298", "mrqa_squad-train-61503", "mrqa_squad-train-5184", "mrqa_squad-train-69660", "mrqa_squad-train-28811", "mrqa_squad-train-10643", "mrqa_squad-train-30024", "mrqa_squad-train-59573", "mrqa_squad-train-67628", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-224", "mrqa_naturalquestions-validation-8619", "mrqa_triviaqa-validation-3751", "mrqa_naturalquestions-validation-1008", "mrqa_hotpotqa-validation-3975", "mrqa_triviaqa-validation-7728", "mrqa_searchqa-validation-10771", "mrqa_hotpotqa-validation-5091", "mrqa_squad-validation-2437", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2702", "mrqa_triviaqa-validation-7209", "mrqa_searchqa-validation-10363", "mrqa_triviaqa-validation-1183"], "EFR": 0.9722222222222222, "Overall": 0.7311432089793282}, {"timecode": 43, "before_eval_results": {"predictions": ["article 30", "espresso", "Sheffield United", "Microsoft", "Wat Tyler", "Lone Ranger", "Scotland", "Earth", "James Hogg", "Texas", "Leeds Rhinos", "Pears soap", "the Rhine River", "Louis XVI", "Franklin D. Roosevelt", "fifty-six", "Uranus", "Plato", "chord", "Chubby Checker", "Separate Tables", "Wilson", "coal", "Stephen of Blois", "the Antitrust Documents Group", "eukharistos", "baseball", "Bear Grylls", "jaws", "Tanzania", "Val Doonican", "tittle", "E. T. A. Hoffmann", "Republic of Upper Volta", "Alexander Borodin", "an elephant", "the Creel Committee", "New Zealand", "Mendip Hills", "street artist", "Jane Austen", "God bless America, My home sweet home.", "the UK\u2019s Trade Mark Registration Act 1875", "boxing", "Benjamin Disraeli", "The Jungle Book", "the Great Leap", "Jan van Eyck", "Rabin", "Shania Twain", "John Nash", "electron donors", "`` It ain't over'til it's over ''", "as a pH indicator, a color marker, and a dye", "Nicolas Winding Refn", "private liberal arts college", "RCA Victor", "troops to \"conduct an analysis\" of whether it is militarily essential to conduct a raid at night or whether it can be put off until daylight,", "Robert Park", "21", "Cairo", "Jackson Pollock", "an elk", "tax incentives for businesses hiring veterans as well as job training"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6331063034188034}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, false, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077]}}, "before_error_ids": ["mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-3105", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-7849", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-4875", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1551"], "SR": 0.546875, "CSR": 0.5532670454545454, "EFR": 1.0, "Overall": 0.7366690340909091}, {"timecode": 44, "before_eval_results": {"predictions": ["Between 1975 and 1990", "tariq Khan", "Euripides", "Alfonso Cuar\u00f3n", "2013", "end of the 18th century", "1935", "Frederick Martin \"Fred\" Mac Murray", "Kauffman Stadium", "concentration camp", "2013\u201314", "the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union", "1995 to 2012", "George Clooney", "Rothschild", "Soviet Union", "lexy", "actress and model", "uniform", "1874", "Citric acid", "North Dakota and Minnesota", "Matt Lucas", "Zambia", "The Sun", "Christopher Tin", "Saint Louis", "Chesley Sullenberger III", "Francis", "1909 Cuban-American Major League Clubs Series", "Cleveland Browns", "a fibre optic cable with TOSLINK connectors", "mexico", "Battle of Prome", "35,000", "eastern", "first and only U.S. born world grand prix champion", "2012", "19th", "sexy Star", "Lev Yashin", "Carrefour", "General Sir John Monash", "Benjam\u00edn", "Hong Kong", "the first Spanish conquistadors in the region of North America now known as Texas", "Battle of Hightower", "9", "Margiana", "Gatwick Airport", "200,000", "2,140 kilometres ( 1,330 mi )", "Highlands County, Florida, United States", "honey bees", "squash", "Chicago", "soybeans", "Nineteen", "\"How I Met Your Mother\"", "collapsed ConAgra Foods plant", "Mount Everest", "I.M. Pei", "Florence Nightingale", "the Citadel"], "metric_results": {"EM": 0.5, "QA-F1": 0.5901313530219781}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.28571428571428575, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3076923076923077, 0.5, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.125, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.25, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1050", "mrqa_hotpotqa-validation-872", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-4544", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-1698", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-2715", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-8186", "mrqa_newsqa-validation-2766", "mrqa_searchqa-validation-16434", "mrqa_searchqa-validation-16341"], "SR": 0.5, "CSR": 0.5520833333333333, "EFR": 0.96875, "Overall": 0.7301822916666666}, {"timecode": 45, "before_eval_results": {"predictions": ["pamphlets on Islam", "Spain", "Jesse of Bethlehem", "Oklahoma City", "insulin", "\"Frenchie\"", "Sir John Mortimer", "John Walsh", "Moldova", "Mnemosyne", "London", "Sunset Boulevard", "the Church", "The Lion King", "licensing deals", "Wyoming", "Benedictus", "Oasis and Blur", "Javier Bardem", "8", "Lee Harvey Oswald", "virtual", "Sherlock Holmes", "Bayern Munchen", "Rotherham United", "Pesach", "Bobby Kennedy", "Skylab", "Portugal", "Rhine", "Confucius", "Japan", "apprentice", "Beijing", "Christian Dior", "Phoenicia", "(C) Bobby Moore", "The Frighteners", "Jerez de la Frontera", "plac\u0113b\u014d", "OUT MUTUAL FRIend", "Porto", "writing", "argument form", "Rochdale", "Portuguese", "Madagascar", "Helsinki", "The Landlord's Game", "myxoma virus", "Ceylon", "between 8.7 % and 9.1 %", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "a mid-size four - wheel drive luxury SUV", "Denmark", "eastern India", "World Famous Gold & Silver Pawn Shop", "The students, who became known as the Little Rock Nine,", "A Colorado prosecutor", "South Africa", "brackets", "ABBA", "Phoenician", "New York Giants"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5641465053763441}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, true], "QA-F1": [0.5, 0.0, 0.5, 0.4, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8387096774193548, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2291", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-6095", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-3970", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-5064", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2776", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-2485", "mrqa_triviaqa-validation-2853", "mrqa_triviaqa-validation-3756", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1586", "mrqa_hotpotqa-validation-4222", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-4760"], "SR": 0.484375, "CSR": 0.5506114130434783, "retrieved_ids": ["mrqa_squad-train-4555", "mrqa_squad-train-34391", "mrqa_squad-train-47272", "mrqa_squad-train-54891", "mrqa_squad-train-5123", "mrqa_squad-train-57299", "mrqa_squad-train-48566", "mrqa_squad-train-18026", "mrqa_squad-train-34900", "mrqa_squad-train-25719", "mrqa_squad-train-4113", "mrqa_squad-train-2360", "mrqa_squad-train-52805", "mrqa_squad-train-61430", "mrqa_squad-train-85723", "mrqa_squad-train-38503", "mrqa_naturalquestions-validation-6506", "mrqa_newsqa-validation-466", "mrqa_hotpotqa-validation-23", "mrqa_newsqa-validation-3069", "mrqa_triviaqa-validation-5950", "mrqa_naturalquestions-validation-1044", "mrqa_triviaqa-validation-4569", "mrqa_searchqa-validation-1523", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-6554", "mrqa_searchqa-validation-16341", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-4294", "mrqa_squad-validation-6913", "mrqa_newsqa-validation-2709", "mrqa_triviaqa-validation-6842"], "EFR": 0.9393939393939394, "Overall": 0.7240166954874836}, {"timecode": 46, "before_eval_results": {"predictions": ["several", "Mattel", "stromatolites", "Rugby School", "a modem", "president", "George Herbert Walker Bush", "Penn State", "Luxor", "Vladimir Putin", "leviathan", "Mending Wall", "wombat", "a crystal", "thunder", "Josephine", "The Three Musketeers", "iTunes", "Neptune", "Annie", "Romeo at the Rose", "KLM", "Captain Marvel", "X-Men", "the retina", "a goat", "Planet of the Apes", "a knish", "India", "Reading Railroad", "Leon Trotsky", "cheese", "the Justice Department", "Melissa Etheridge", "Ignace Jan Paderewski", "julius", "Charles Schulz", "the Chesapeake Bay", "Frida", "Jane Austen", "Rikki-Tavi", "mutual fund", "polygons", "country", "lm", "a bourn", "Tiananmen Square", "The Oresteia Trilogy", "cereal", "metropolis", "Dolphins", "Thomas Mundy Peterson", "USS Chesapeake", "From 1900", "george terrier", "alligators", "Hindi", "London", "John Snow", "Ghana's", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "Afghanistan", "Tuesday", "1955"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6357421875}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6875000000000001, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5523", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-13179", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-8593", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-14169", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-2362", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-14944", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-6336", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-14657", "mrqa_naturalquestions-validation-4341", "mrqa_triviaqa-validation-4443", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-1216"], "SR": 0.5625, "CSR": 0.5508643617021276, "EFR": 0.9642857142857143, "Overall": 0.7290456401975683}, {"timecode": 47, "before_eval_results": {"predictions": ["Kauai", "the Lord Mayor", "Shel Silverstein", "beers", "a celebrity house tours", "Liverpool", "los angeles", "Cyrus the Younger", "Greece", "Jim Bunning", "John Lennon", "the Starfighter", "a woofer", "Cubism", "Dune", "the Panama Canal", "Eragon", "vacuum tubes", "drug and alcohol", "the Great Lakes region", "the bicentennial", "the Midway", "Gershwin", "alpaca", "the Atlantic", "Heredity", "Bicentennial Man", "the rod", "heart attack", "Elke Sommer", "Ivan the Terrible", "Flav", "Fidel Castro", "The Indianapolis 500", "the Twist", "(Rabbie) Burns", "the cuckoo", "London", "beetles", "Joan of Arc", "palindrome", "quid", "Vanilla Ice", "A Night at the Roxbury", "Steinbeck", "Eric Knight", "Heroes", "the Ganges (Ganga) River", "Thomas Mann", "Samuel, Kings & Chronicles", "Sing Sing", "Rajendra Prasad", "1945", "an edited version of a film", "Bedfordshire", "Charlemagne", "The Lion, The Witch", "Lord's Resistance Army", "South Asia and the Middle East", "Netflix", "The Uighurs fled Afghanistan shortly after the U.S.-led bombing campaign began in 2001.", "Casa de Campo International Airport", "July", "the greater risk-adjusted return of value stocks over growth stocks"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6867351398601398}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, false, true, false, false, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 0.7692307692307693, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-3857", "mrqa_searchqa-validation-8918", "mrqa_searchqa-validation-2555", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-3452", "mrqa_searchqa-validation-1850", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-7103", "mrqa_searchqa-validation-2691", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-344", "mrqa_hotpotqa-validation-757", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3958", "mrqa_hotpotqa-validation-741"], "SR": 0.609375, "CSR": 0.5520833333333333, "EFR": 0.96, "Overall": 0.7284322916666666}, {"timecode": 48, "before_eval_results": {"predictions": ["Jaws 2", "Deseo", "Elizabeth Taylor", "James Patterson", "The Incredibles", "a cheetah", "Charlie Brown", "Odin", "Japan", "Sea-Monkeys", "the daffodils", "\"24\"", "Neil Simon", "Voyager 2", "a gulls", "Nez Perce", "Eva Peron", "a solas", "the Hawkeyes", "the 2016 NBA Draft", "Swiffer", "Huckleberry Hound", "Austria", "Jason Bourne", "Brazil", "The Trojan War", "atolls", "the Colosseum", "Cambodia", "Dr. Hook and the Medicine Show", "Innocence", "the Uvula", "the catechism of the Council of Trent", "Ben-Oni", "Scrubs", "Cheyenne", "the Black Sea", "King George", "Frank Sinatra", "the Zambezi river", "serving the tea", "The Theocracy", "The Police", "Jamestown", "American funk rock band", "Robert Ford", "St. Francis of Assisi", "The Lemon Meringue", "(Jose) Melville", "Tarzan & Jane", "Favre", "1919", "eight years", "Taron Egerton", "Batman", "Stieg Larsson", "The Merry Wives of Windsor", "Tomasz Adamek", "The Thomas Crown Affair", "1866", "new materials -- including ultra-high-strength steel and boron", "India", "The EU naval force", "Ruritania"], "metric_results": {"EM": 0.625, "QA-F1": 0.6971275252525253}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16827", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-9860", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-4739", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-14611", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-8195", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-15461", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-5532", "mrqa_triviaqa-validation-6041", "mrqa_newsqa-validation-455"], "SR": 0.625, "CSR": 0.5535714285714286, "retrieved_ids": ["mrqa_squad-train-37284", "mrqa_squad-train-56041", "mrqa_squad-train-79653", "mrqa_squad-train-59811", "mrqa_squad-train-80580", "mrqa_squad-train-45285", "mrqa_squad-train-9505", "mrqa_squad-train-23011", "mrqa_squad-train-65198", "mrqa_squad-train-7033", "mrqa_squad-train-76956", "mrqa_squad-train-7785", "mrqa_squad-train-29646", "mrqa_squad-train-61322", "mrqa_squad-train-6320", "mrqa_squad-train-15734", "mrqa_newsqa-validation-2245", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-9921", "mrqa_triviaqa-validation-1114", "mrqa_hotpotqa-validation-4818", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-2783", "mrqa_hotpotqa-validation-2811", "mrqa_newsqa-validation-3784", "mrqa_searchqa-validation-5788", "mrqa_triviaqa-validation-2666", "mrqa_hotpotqa-validation-2341", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-5817", "mrqa_newsqa-validation-1718"], "EFR": 0.9583333333333334, "Overall": 0.7283965773809523}, {"timecode": 49, "before_eval_results": {"predictions": ["1972", "Stonemason's Yard", "Carmen", "Isles of Scilly", "a Holocaust memorial", "sexual imagination", "fourteen", "the kidneys", "apple", "Athina Onassis", "Rafa Nadal", "Apollo 11", "five smooth stones", "Kirk Douglas", "John Ford", "tin", "Longchamp", "state of Japan", "Henry Ford", "a joey", "Maine", "USS Missouri", "Pyrenees mountains", "basketball", "Janis Joplin", "Stringer", "basketball", "South Africa", "Pet Sounds", "Ed Miliband", "Scotland", "an aeoline", "Margaret Mitchell", "Republic of Upper Volta", "martina Navratilova", "40", "75", "King George VI", "John Masefield", "Rio de Janeiro", "\"Party of God\"", "Bengali", "grimace", "Guatemala", "Carousel", "Leicester", "Jimmy Robertson", "radishes", "martin", "Downton Abbey", "a knife", "Garfield Sobers", "Herman Hollerith", "September 2017", "Golden Gate", "Forbes", "English Electric Canberra", "Mark Fields", "pattern matching", "one of 10 gunmen who attacked several targets in Mumbai on November 26,", "a rat", "tapas", "Maria Callas", "Hern\u00e1n Jorge Crespo"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7242063492063493}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.5714285714285715, 0.4, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-5762", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-1657", "mrqa_naturalquestions-validation-8483", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-4559", "mrqa_hotpotqa-validation-3207"], "SR": 0.640625, "CSR": 0.5553125, "EFR": 0.9130434782608695, "Overall": 0.7196868206521738}, {"timecode": 50, "UKR": 0.732421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-464", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-732", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7785", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9451", "mrqa_searchqa-validation-9800", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6786", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-996"], "OKR": 0.833984375, "KG": 0.4703125, "before_eval_results": {"predictions": ["Ross Elliott", "Annette", "the 1980s", "Nodar Kumaritashvili", "Carpenter", "Dan Stevens", "human colon", "December 1886", "July 1, 1890", "March 31, 2013", "Manley", "1978", "Judiththia Aline Keppel", "BC Jean and Toby Gad", "2018 Winter Olympics", "The Walking Dead ( franchise )", "Koine Greek : apokalypsis, meaning `` unveiling '' or `` revelation ''", "1962", "non-ferrous", "the economy", "the sacroiliac joint or SI joint ( SIJ )", "Sunni Muslim family", "after World War II", "the common name of a typical child's friend", "The Massachusetts Compromise", "L.K. Advani", "the period of service for men was 30 months and for women 18 months", "Jason Marsden", "Charles Lebrun", "Ashrita Furman", "St. Augustine renders it as clara notitia cum laude, `` brilliant celebrity with praise ''", "By the early 1960s", "623", "the beginning", "2013", "Diego Tinoco", "when each of the variables is a perfect monotone function of the other", "January 2004", "Glenn Close", "the roofs of the choir side - aisles at Durham Cathedral", "Johannes Gutenberg", "Dan Stevens", "Alex Ryan", "Dr. Addison Montgomery", "Carolyn Sue Jones", "De pictura", "a mark that reminds of the Omnipotent Lord, which is formless", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "Article 1, Section 2, Clause 3", "birch", "push the food down the esophagus", "Dolly Parton", "northamptonshire", "durham", "Jack Murphy Stadium", "Black Abbots", "Prince Amedeo", "a real person to talk to,\"", "Suba Kampong township", "2004", "laryngitis", "a whaling ship", "'The chief business of the American people,'", "Dan Parris, 25, and Rob Lehr, 26,"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6463554657488482}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.14814814814814814, 0.6363636363636364, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-1336", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6353", "mrqa_hotpotqa-validation-5522", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-3406", "mrqa_searchqa-validation-14676", "mrqa_searchqa-validation-7913", "mrqa_newsqa-validation-2294"], "SR": 0.578125, "CSR": 0.5557598039215687, "EFR": 0.9629629629629629, "Overall": 0.7110883033769062}, {"timecode": 51, "before_eval_results": {"predictions": ["Domhnall Gleeson", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Alicia Vikander", "Franklin Roosevelt", "Scottish post-punk band Orange Juice", "1837", "Zoe Badwi, Jade Thirlwall's cousin, was supporting the gigs in Australia", "22 November 1914", "Shareef Abdur - Rahim", "2018", "the breast or lower chest of beef or veal", "in the mid - to late 1920s", "near Camarillo, California", "birmingham", "2018 and 2019", "Deuteronomy 5 : 4 -- 25", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "to connect the CNS to the limbs and organs", "15 February 1998", "brothers Henry, Jojo and Ringo Garza", "Thomas Alva Edison", "Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas '', cf. English Andrew", "`` Mirror Image ''", "the population, serving staggered terms of six years", "the most junior enlisted sailor ( `` E-8s senior chief petty officer, and E-9s master Chief petty officer", "1603", "Eduardo", "a child with Treacher Collins syndrome trying to fit in", "Kansas currently has the longest streak of consecutive NCAA tournament appearances of all - time ( 29 )", "Efren Manalang Reyes, OLD, PLH", "Jesse McCartney", "Baseball Writers'Association of America ( or BBWAA ), or the Veterans Committee, which now consists of four subcommittees, each of which considers and votes for candidates from a separate era of baseball", "Herman Hollerith", "ulnar nerve", "December 18, 2017", "Brooklyn, New York", "2009", "Buddhism", "Rodney Crowell", "in Atlanta", "peninsular mainland", "21 June 2007", "`` chair '' or `` chairperson ''", "Germany", "Gamora", "Darlene Cates", "the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "in Poems : Series 1, a collection of Dickinson's poems assembled and edited by her friends Mabel Loomis Todd and Thomas Wentworth Higginson", "birch", "Bennett Cerf", "Matt Monro", "Joe Willie Kirk", "douglas", "Vito Corleone", "supply chain management", "Baugur Group", "Venice", "Hyundai", "birmingham", "100 percent", "birmingham", "Roger Clemens", "Andrew Carnegie", "an independent homeland for the country's ethnic"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5698317600293197}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, true, true, true, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 0.9090909090909091, 0.5714285714285715, 0.0, 0.5714285714285715, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.5, 1.0, 0.0, 0.5185185185185185, 0.0, 0.0, 1.0, 0.0, 1.0, 0.15384615384615383, 0.5, 1.0, 0.0, 1.0, 0.13333333333333333, 0.7499999999999999, 0.4444444444444445, 0.4102564102564102, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.15384615384615383, 1.0, 0.0, 1.0, 0.7894736842105263, 0.9302325581395349, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-8248", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-407", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-6482", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1756", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-8208"], "SR": 0.40625, "CSR": 0.5528846153846154, "retrieved_ids": ["mrqa_squad-train-21775", "mrqa_squad-train-17434", "mrqa_squad-train-53540", "mrqa_squad-train-3626", "mrqa_squad-train-3110", "mrqa_squad-train-43387", "mrqa_squad-train-33320", "mrqa_squad-train-33595", "mrqa_squad-train-67828", "mrqa_squad-train-27158", "mrqa_squad-train-3089", "mrqa_squad-train-24938", "mrqa_squad-train-24067", "mrqa_squad-train-83635", "mrqa_squad-train-19464", "mrqa_squad-train-76714", "mrqa_searchqa-validation-12442", "mrqa_newsqa-validation-1131", "mrqa_triviaqa-validation-5923", "mrqa_searchqa-validation-2929", "mrqa_squad-validation-7541", "mrqa_squad-validation-4264", "mrqa_squad-validation-9015", "mrqa_hotpotqa-validation-5307", "mrqa_squad-validation-10427", "mrqa_newsqa-validation-3048", "mrqa_searchqa-validation-15716", "mrqa_triviaqa-validation-2264", "mrqa_naturalquestions-validation-808", "mrqa_triviaqa-validation-575", "mrqa_newsqa-validation-3106", "mrqa_naturalquestions-validation-9921"], "EFR": 0.8947368421052632, "Overall": 0.6968680414979758}, {"timecode": 52, "before_eval_results": {"predictions": ["Vilnius Old Town", "Roc-A-Fella Records and Priority Records", "United States Army", "White Horse", "Qu'est-ce qu'on a fait au Bon Dieu", "increased education in the United States designated by a state to receive the Morrill Acts of 1862 and 1890", "Asia-Pacific War", "1949", "The Dark Tower", "John Samuel Waters Jr.", "1945", "Sacramento Kings", "S7", "Magic Band", "Supergirl", "April 1, 1949", "Scottish Premiership club Hearts", "Standard Oil", "William Harold \"Bill\" Ponsford", "Anatoly Lunacharsky", "Robert Matthew Hurley", "Navarasa", "Brad Silberling", "1987", "Italy", "Vaisakhi List", "\"Twice in a Lifetime\"", "7 Series", "Len Wiseman", "31 July 1975", "Texas Tech Red Raiders", "Walldorf", "Elvis' Christmas Album", "sarod", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "\"Thocmentony\", meaning \"Shell Flower\"", "\"coordinator\"", "Godiva Chocolatier", "England national team", "\"Futurama\"", "Manhattan Project", "land area", "Lush Ltd.", "Telugu", "1952", "a land grant college", "Restoration Hardware", "1942", "Kansas City Chiefs", "Eminem", "C. H. Greenblatt", "Stephen Graham", "President", "introverted Sensing ( Si )", "Belgium", "Jape", "Jackson Pollock", "alwin Landry's supply vessel Damon Bankston", "about 3,000 kilometers (1,900 miles)", "Casalesi Camorra clan", "Linda Darnell", "Scrabble", "Wendell", "leap year"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7439930555555555}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.56, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.8, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5728", "mrqa_hotpotqa-validation-5189", "mrqa_hotpotqa-validation-2084", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1510", "mrqa_hotpotqa-validation-1835", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-5553", "mrqa_naturalquestions-validation-4714", "mrqa_triviaqa-validation-2582", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-7977", "mrqa_searchqa-validation-1784"], "SR": 0.640625, "CSR": 0.5545400943396226, "EFR": 1.0, "Overall": 0.7182517688679245}, {"timecode": 53, "before_eval_results": {"predictions": ["$250,000 for Rivers' charity: God's Love We Deliver.", "The cervical cancer vaccine", "eight-day journey", "9-week-old", "American Muslim and Christian leaders", "18", "Darrel Mohler", "Lance Cpl. Maria Lauterbach and her fetus", "Operation Pipeline Express.", "admitting they learned of the death of TV news coverage,", "a house party in Crandon, Wisconsin", "President Obama", "The catamaran and its message has been warmly received.", "The drivers of the cars -- consisting of Ferraris, a Lamborghini and an Acura NSX", "a bag", "The report was an unclassified assessment sent to law enforcement agencies.", "14-day mission", "the fact that the teens were charged as adults.", "Conway", "The United States has an interest in promoting a system of international norms and institutions that averts potential genocide", "rwanda", "Arsene Wenger", "went second in Serie A with a 5-1 win over Torino in the San Siro on Sunday.", "Genocide Prevention Task Force", "Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed", "The FDA is silent on our request that it also send a warning letter to physicians clearly describing possible adverse reactions, such as tendon pain,", "Jacob Zuma,", "the return of a fallen U.S. service member", "Sporting Lisbon", "The opposition group, also known as the \"red shirts,\"", "Saturday", "Jezebel.com's Crap E-mail", "40 years", "The Democratic VP candidate", "Gainsbourg", "Theyon", "three", "between the ages of 14 to 17", "President Richard M. Nixon,", "Piedad Cordoba,", "Buddhism", "amalgamation of Indian and western talent", "Pakistani territory", "fight outside of an Atlanta strip club", "Britain's Got Talent", "Sen. Barack Obama", "the game", "the man facing up, with his arms out to the side.", "stand down.", "in a muddy barley field owned by farmer Alan Graham outside Bangor,", "The military commission", "serves as the physical link between the mRNA and the amino acid sequence of proteins", "Beyonc\u00e9 and Bruno Mars", "2018", "surfer", "arthropods", "white", "2018 governor's race", "1898", "My Beautiful Dark Twisted Fantasy", "Ned Kelly", "Beta Monocerotis", "fish", "a crust of mashed potato"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5844598393001925}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, true, true, false, true, true, true, false, true, false, false, true, false, false, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.8333333333333333, 1.0, 1.0, 0.0, 0.3333333333333333, 0.46153846153846156, 0.28571428571428575, 0.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.08695652173913043, 1.0, 0.0, 0.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.16666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.36363636363636365, 0.7272727272727273, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-2672", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-3864", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-4204", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-3783", "mrqa_triviaqa-validation-2038", "mrqa_hotpotqa-validation-5406", "mrqa_searchqa-validation-12411", "mrqa_naturalquestions-validation-10616"], "SR": 0.46875, "CSR": 0.5529513888888888, "EFR": 1.0, "Overall": 0.7179340277777777}, {"timecode": 54, "before_eval_results": {"predictions": ["Haikou on the Hainan Island", "in Squamish, British Columbia, Canada", "2018", "2004", "on the table", "Jon", "Jason Lee", "ThonMaker", "Hans Raffert", "31", "Jesse Frederick James Conaway", "an Aldabra giant tortoise", "Wilson's ideals", "Number 4, Privet Drive, Little Whinging in Surrey, England", "in the pancreas", "2018", "on a beach in Malibu, California", "desublimation", "one complete set", "Anglo - Norman French waleis", "The three wise monkeys", "in a vertebrate's immune system", "into the intermembrane space", "Kansas", "New England Patriots", "Chesapeake Bay", "Fred Ott", "in a relationship, marriage, or once talked to", "the body - centered cubic ( BCC ) lattice", "President Lyndon Johnson", "in a Norwegian town", "16 best - selling religious novels by Tim LaHaye and Jerry B. Jenkins", "the presence of different thermal floors", "Development of Substitute Materials", "a pagan custom", "in various submucosal membrane sites of the body", "2013", "John Ridgely as Jim Merchant", "Ummat al - Islamiyah", "Lord Irwin", "the volume", "right to property is no longer a fundamental right, though it is still a constitutional right", "Robert Gillespie Adamson IV", "1800", "1998", "to the lungs", "Norman Whitfield and Barrett Strong", "Sir Ernest Rutherford", "Hendersonville, North Carolina", "within the internal auditory canal of the temporal bone", "1858", "UPS", "The Wrestling Classic", "The Kennel Club", "Timothy Dalton", "Grammy awards", "John D Rockefeller's", "second-degree attempted murder and conspiracy,", "$106,482,500 to an unidentified telephone bidder,", "introduce legislation Thursday to improve the military's suicide-prevention programs.\"", "Stone Temple Pilots", "real estate", "Hubert Humphrey", "Tim Clark, Matt Kuchar and Bubba Watson"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5553199404761905}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, false, true, true, true, false, true], "QA-F1": [0.888888888888889, 0.888888888888889, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.21428571428571425, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.4666666666666667, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.25, 1.0, 0.08333333333333333, 1.0, 0.6666666666666666, 0.0, 1.0, 0.3333333333333333, 0.22222222222222224, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.25, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9087", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-2242", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3174", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-6727", "mrqa_triviaqa-validation-3624", "mrqa_hotpotqa-validation-574", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-900", "mrqa_searchqa-validation-3606"], "SR": 0.421875, "CSR": 0.5505681818181818, "retrieved_ids": ["mrqa_squad-train-15304", "mrqa_squad-train-53253", "mrqa_squad-train-17650", "mrqa_squad-train-28913", "mrqa_squad-train-49336", "mrqa_squad-train-21690", "mrqa_squad-train-16510", "mrqa_squad-train-84169", "mrqa_squad-train-12135", "mrqa_squad-train-60211", "mrqa_squad-train-60399", "mrqa_squad-train-16214", "mrqa_squad-train-26640", "mrqa_squad-train-49812", "mrqa_squad-train-52558", "mrqa_squad-train-31144", "mrqa_hotpotqa-validation-5503", "mrqa_triviaqa-validation-2147", "mrqa_newsqa-validation-3222", "mrqa_squad-validation-6316", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-6561", "mrqa_searchqa-validation-16827", "mrqa_newsqa-validation-455", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-8379", "mrqa_naturalquestions-validation-1327", "mrqa_hotpotqa-validation-2047", "mrqa_newsqa-validation-3167", "mrqa_naturalquestions-validation-10012", "mrqa_triviaqa-validation-1836"], "EFR": 0.9459459459459459, "Overall": 0.7066465755528255}, {"timecode": 55, "before_eval_results": {"predictions": ["Dr. Ruth Westheimer", "John Updike", "Smokey Bear", "clouds", "Makkedah", "to clean a ship's deck", "asteroids", "plankton", "all the US presidents elected without getting a plurality of", "Eleanor Roosevelt", "the War of 1812", "Bangladesh", "The Secret", "Sudan", "Seth Rogen", "a laser", "Jamaica Inn", "Walt Disney World", "Mexico", "Artemis", "pH", "the Aladdin", "Nine to Five", "Beach Boys", "force his", "ice cream", "Huckabee", "Count Grigory Orlov", "Texas", "constellations", "AILD", "Isabella (Witherspoon)", "Ross Perot", "the Black Sea", "C. S. Lewis", "Thomas Paine", "Back to the Future", "an antelope", "Anne", "tent", "Dizzy", "a liquid food made by boiling or simmering meat, fish", "the ACT", "Fermi", "Icarus", "suspension bridge", "Tigger", "the body of songs", "the marathon", "QWERTY", "Deuteronomy", "collect menstrual flow", "1787", "cartilage", "spen King of Triumph", "Victoria", "the recorder", "mixed martial arts", "thirty-seventh richest person in the United States", "March 17, 2015", "4.6 million", "a small minority who said they wanted to demand Tibet's independence,", "Alwin Landry's supply vessel Damon Bankston", "Geoffrey Zakarian"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5543154761904762}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true], "QA-F1": [0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14322", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-717", "mrqa_searchqa-validation-755", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-3328", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-4506", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-11807", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-2969", "mrqa_searchqa-validation-12353", "mrqa_searchqa-validation-12390", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-6375", "mrqa_searchqa-validation-11295", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-960", "mrqa_searchqa-validation-2219", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-5113", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-1245", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-4855", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-2205"], "SR": 0.484375, "CSR": 0.5493861607142857, "EFR": 1.0, "Overall": 0.7172209821428571}, {"timecode": 56, "before_eval_results": {"predictions": ["Pegida", "tardis", "honey", "The Potteries", "nelson-on-Trent", "iron", "Little arrows", "british", "cats", "Reanne Evans", "Niger", "Battle of Camlann", "David Hilbert", "1905", "United Kingdom", "Lab\u00e8que", "Jack London", "\"Book 1: Sowing\"", "Muhammad Ali", "carbon", "Sierra One from Sierra Oscar", "M65", "Boxing Day", "cheers", "hizb-e-Islami", "alpestrine", "a toad", "ameliorate", "noreg", "skirts", "Australia", "Blucher", "Atlas", "Sachin Tendulkar", "55", "the Humber", "tenerife", "South Africa", "bone", "Nutbush", "John le Mesurier", "Shinto", "Batley", "the Greater Antilles", "malts", "Pluto", "Jim Branning", "cryonic suspension", "Fleet Street", "Scafell Pike", "Baseball", "Speaker of the House of Representatives", "Athens", "iOS, watchOS, and tvOS", "Leslie James \"Les\" Clark", "American Idol", "\"Realty Bites\"", "Former Mobile County Circuit Judge Herman Thomas", "News of the World tabloid", "propofol", "Emmett Kelly", "Jesse Malin", "Shakespeare", "Can't Change Me"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6322172619047619}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.05714285714285715, 1.0, 1.0, 0.0, 0.5, 0.5714285714285715]}}, "before_error_ids": ["mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-4401", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-3619", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-6783", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-3696", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2748", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-2374", "mrqa_newsqa-validation-1282", "mrqa_searchqa-validation-1086", "mrqa_searchqa-validation-378", "mrqa_naturalquestions-validation-7270"], "SR": 0.59375, "CSR": 0.5501644736842105, "EFR": 0.9615384615384616, "Overall": 0.7096843370445345}, {"timecode": 57, "before_eval_results": {"predictions": ["outside influences in next month's run-off election,", "Monday,", "eight-week", "all TV personalities set such a sincerely loving example. It's also a good place to learn which type of guy you should avoid.", "coalition", "to fritter his cash away on fast cars, drink and celebrity parties.", "Stratfor", "of being \"scared I won't be able to go home,\"", "Unseeded Frenchwoman Aravane Rezai", "murder in the beating death of a company boss who fired them.", "on-loan David Beckham", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Islamabad", "Dennis Davern, the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "kite surfers and wind surfers", "\"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "anti-government protesters", "Madhav Kumar Nepal of the Communist Party of Nepal (Unified Marxist-Leninist)", "Saturday", "Russian residents and worldwide viewers, in English or in Russian, what they think about Russia's role in the international community.", "dropped his children off at a relative's house,", "11", "have a broomstick, a pair of scissors and a wooden dowel used to hang clothes in a closet.", "The missile defense system is not aimed at Russia,\"", "Citizens", "refusal or inability to \"turn it off\"", "Janet Napolitano", "nine newly-purchased bicycles at the scene, and think they were used to carry the explosives.", "The alleged surviving attacker from last month's Mumbai terror attacks is seeking help from Pakistani officials,", "Afghanistan", "that the teens were charged as adults.", "robo-female", "dogs who walk on ice in Alaska.", "10 to 15 percent", "Israel", "british leeds", "The incident Sunday evening", "Landry", "President Bush", "Alexandre Caizergues, of France,", "Steven Gerrard", "three", "Golden Gate Yacht Club of San Francisco", "The 725-mile Veracruz", "Grease", "Camp Lejeune, North Carolina", "2002 for British broadcaster Channel 4", "have lost their homes, their jobs, their hope,\"", "the job bill's controversial millionaire's surtax", "Fourteen", "One of Osama bin Laden's sons", "outside cultivated areas", "Britain", "Manuel `` Manny '' Heffley", "2004", "Pokemon", "Ambassador Bridge", "The University of Liverpool", "Alfred Graf von Schlieffen", "Chillingham Castle", "the 400th anniversary of Lima's founding", "the Liffey", "Scrabble", "American country music artist Ricky Skaggs"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5406325815218278}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, true, true, false], "QA-F1": [0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.09523809523809525, 0.4, 0.6666666666666666, 0.8, 0.0, 1.0, 0.25, 0.0, 0.0625, 0.0, 0.4615384615384615, 1.0, 0.0, 0.25, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0606060606060606, 1.0, 0.923076923076923, 0.0, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.9411764705882353, 0.5714285714285715, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-2674", "mrqa_newsqa-validation-964", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-889", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-1448", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-4123", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-5355", "mrqa_hotpotqa-validation-3212", "mrqa_searchqa-validation-1257", "mrqa_hotpotqa-validation-2798"], "SR": 0.390625, "CSR": 0.5474137931034483, "retrieved_ids": ["mrqa_squad-train-16263", "mrqa_squad-train-76814", "mrqa_squad-train-25487", "mrqa_squad-train-5310", "mrqa_squad-train-10981", "mrqa_squad-train-73575", "mrqa_squad-train-77988", "mrqa_squad-train-17745", "mrqa_squad-train-11469", "mrqa_squad-train-1559", "mrqa_squad-train-60546", "mrqa_squad-train-43211", "mrqa_squad-train-31924", "mrqa_squad-train-78047", "mrqa_squad-train-63006", "mrqa_squad-train-21755", "mrqa_hotpotqa-validation-435", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-3004", "mrqa_searchqa-validation-815", "mrqa_newsqa-validation-2032", "mrqa_hotpotqa-validation-1080", "mrqa_squad-validation-4458", "mrqa_newsqa-validation-3380", "mrqa_hotpotqa-validation-4667", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-3541", "mrqa_triviaqa-validation-331", "mrqa_searchqa-validation-8076", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-4185"], "EFR": 1.0, "Overall": 0.7168265086206896}, {"timecode": 58, "before_eval_results": {"predictions": ["Ted 2", "1,467", "1911", "Nicole Kidman, Meryl Streep", "14", "National Basketball Development League", "Gust Avrakotos", "involuntary euthanasia", "Moon shot", "duck", "Summer Olympic Games", "Glendale", "St. Louis Cardinals", "November 23, 1992", "1993", "University of Vienna", "Jack Ridley", "Pennsylvania State University", "Willis (Sears) Tower", "William Corcoran Eustis", "evangelical Christian", "Hanoi", "ITV", "Australia", "suburb", "Flexible-fuel vehicle", "Savannah River Site", "swingman", "Patriots Day", "Scotland", "Todd Emmanuel Fisher", "1944", "Suicide Squad", "1883", "23", "Mach number", "James Gay-Rees", "1999", "poetry", "Who's That Girl: Original Motion Picture Soundtrack", "musicologist", "Lauren Alaina", "Prince Amedeo, 5th Duke of Aosta", "Ben Ainslie", "Forbidden Quest", "non-alcoholic", "Scratchcard", "White Horse", "Duncan Kenworthy", "Malayalam movies", "Peter Nowalk", "Annette", "an exultation of spirit", "Bumblebee", "riyadh", "Lady Gaga", "African violet", "three", "There's no chance", "Carrousel du Louvre,", "A Tale of Two Cities", "Angel Gabriel", "Braveheart", "( Boss) Tweed"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6395833333333333}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, false, false, false, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true], "QA-F1": [0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.6, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-4747", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-3935", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3420", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6523", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2955"], "SR": 0.546875, "CSR": 0.5474046610169492, "EFR": 1.0, "Overall": 0.7168246822033898}, {"timecode": 59, "before_eval_results": {"predictions": ["two reservoirs in the eastern Catskill Mountains", "connotations of the passing of the year", "John Barry", "Thespis", "Saronic Gulf", "2010", "Coroebus", "2", "1952", "iron", "Jesse Frederick James Conaway", "autopistas, or tolled ( quota ) highways", "modern programming practices", "Gene MacLellan", "1957", "Labor Relations Act of 1935 ( 49 Stat. 449 ) 29 U.S.C. \u00a7 151 -- 169 ( also known as the NLRA and the Wagner Act after NY Senator Robert F. Wagner )", "a four - page pamphlet", "Have I Told You Lately", "the world's second most populous country", "Greek city - states under Themistocles and the Persian Empire", "Lana Del Rey", "1979", "season seven", "Janie Crawford", "The Massachusetts Compromise", "2018", "Greek as their common language", "ordain presbyters / bishops and to exercise general oversight", "11 January 1923", "1961", "the Indians", "to condense the steam coming out of the cylinders or turbines", "the engineer \u00c9mile Gagnan and Naval Lieutenant ( `` lieutenant de vaisseau '' ) Jacques Cousteau", "Felix Baumgartner", "between 2004 and 2007", "2026", "the Golden Age of India", "Abigail Hawk", "Hal Derwin", "South Korea", "in the 1970s", "1919", "1889", "halogenated paraffin hydrocarbons", "October 27, 2017", "The tower has three levels for visitors, with restaurants on the first and second levels", "Richard Crispin Armitage", "Missouri River", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophia Monk and Eddie Perfect", "Jack Barry", "headdresses", "Wet Wet Wet", "surrey", "One Direction", "Delacorte Press", "Drifting", "1927", "Bollywood", "Iran", "to \"wipe out\" the United States if provoked.", "Celsius", "Washington D.C.", "Jonathan Swift", "Linux Format"], "metric_results": {"EM": 0.53125, "QA-F1": 0.63808859517729}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.375, 1.0, 0.0, 0.8125000000000001, 0.0, 1.0, 0.7272727272727272, 0.11764705882352941, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7777777777777778, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.5, 0.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-8908", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-870", "mrqa_triviaqa-validation-4980", "mrqa_newsqa-validation-213", "mrqa_searchqa-validation-14090", "mrqa_hotpotqa-validation-4642"], "SR": 0.53125, "CSR": 0.5471354166666667, "EFR": 0.9333333333333333, "Overall": 0.7034374999999999}, {"timecode": 60, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1288", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2008", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-4096", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5374", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-5864", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-97", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10124", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11828", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12230", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-996"], "OKR": 0.83203125, "KG": 0.49375, "before_eval_results": {"predictions": ["Fulgencio Batista", "St. Vincent", "5,042", "Mandalay Entertainment", "Debbie Reynolds", "(1963\u201393)", "Mike Holmgren", "2,627", "the Swedish-Brandenburgian War", "Sparky", "Fort Frederick", "American", "Virgin", "October 21, 2016", "Kiss", "Ferdinand Magellan", "Sun Records founder Sam Phillips", "the Corps of Discovery", "receive the benefits of the Morrill Acts of 1862 and 1890", "the Crab Orchard Mountains", "Miss Universe 2010", "Maryland", "2010", "democracy and personal freedom", "Sami Brady", "French Canadians", "1964 to 1974", "the National League, the fifth tier of English football", "City Mazda Stadium", "Continental Army", "Wes Archer", "1994", "Vancouver", "Lego", "Thomas Mawson", "Tony Aloupis", "various", "North Dakota", "Francis Nethersole", "Jagdpanther", "British", "Fainaru Fantaj\u012b Tuerubu", "The University of California", "City of Onkaparinga", "February", "thirteen", "Princes Park", "The Bye Bye Man", "Germanic", "Blue", "1698", "a rotation", "the Constitution of India came into effect on 26 January 1950", "Raza Jaffrey", "Jon Stewart", "\u201cFor Gallantry;\u201d", "ArcelorMittal Orbit", "Government Accountability Office", "the Dallas suspect, Brian Smith.", "$50 less,", "high and dry", "An American Tail", "a cat", "Peru"], "metric_results": {"EM": 0.625, "QA-F1": 0.6854910714285714}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, false, false, true, true, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-2954", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-5586", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-1107", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-4986", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-10257", "mrqa_triviaqa-validation-2096", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-3315", "mrqa_searchqa-validation-8784"], "SR": 0.625, "CSR": 0.5484118852459017, "retrieved_ids": ["mrqa_squad-train-82853", "mrqa_squad-train-31104", "mrqa_squad-train-51640", "mrqa_squad-train-45529", "mrqa_squad-train-1395", "mrqa_squad-train-54273", "mrqa_squad-train-41241", "mrqa_squad-train-30283", "mrqa_squad-train-33287", "mrqa_squad-train-27454", "mrqa_squad-train-53013", "mrqa_squad-train-22089", "mrqa_squad-train-15575", "mrqa_squad-train-17841", "mrqa_squad-train-42964", "mrqa_squad-train-4042", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-4113", "mrqa_squad-validation-1251", "mrqa_searchqa-validation-4533", "mrqa_triviaqa-validation-3095", "mrqa_newsqa-validation-319", "mrqa_naturalquestions-validation-688", "mrqa_triviaqa-validation-2141", "mrqa_newsqa-validation-3406", "mrqa_squad-validation-8662", "mrqa_searchqa-validation-762", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7151", "mrqa_naturalquestions-validation-7608", "mrqa_squad-validation-9740"], "EFR": 1.0, "Overall": 0.7236667520491803}, {"timecode": 61, "before_eval_results": {"predictions": ["Blades", "George Blake", "Orson Welles", "trout", "The Aidensfield Arms", "Bahasa Indonesian", "france", "Manchester", "sky", "Britten", "Angel Cabrera", "November", "Wonga", "Van Heflin", "Genghis Khan", "Kofi Annan", "Constance", "left side", "Istanbul", "lamb", "Space Oddity", "collie", "35", "shark", "florida", "Mike Hammer", "london zavoroni", "Dame Evelyn Glennie", "the brain", "Zaragoza", "David Bowie", "Billy Wilder", "Mr Loophole", "palla", "6.4 million", "Today", "france", "Beau Brummel", "Pentecost", "Morgan Spurlock", "Oliver", "Madeline Boardman", "Barry Humphries", "ions", "George Santayana", "Rudolf Nureyev", "london", "the African wild cat", "apple", "Argos", "Rodgers & Hammerstein", "San Francisco, California", "By 1770 BC", "United States Secretary of State", "5", "Amal Clooney", "C. J. Cherryh", "autonomy.", "Heshmatollah Attarzadeh", "Mark Obama Ndesandjo", "france", "the Louvre", "Kansas City, Missouri", "YIVO"], "metric_results": {"EM": 0.53125, "QA-F1": 0.63125}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666665, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-1017", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-6779", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-2680", "mrqa_triviaqa-validation-2652", "mrqa_triviaqa-validation-6339", "mrqa_triviaqa-validation-4780", "mrqa_triviaqa-validation-366", "mrqa_triviaqa-validation-6342", "mrqa_triviaqa-validation-2487", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-4021", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-2955", "mrqa_newsqa-validation-2489", "mrqa_searchqa-validation-4730", "mrqa_searchqa-validation-5842"], "SR": 0.53125, "CSR": 0.5481350806451613, "EFR": 0.9666666666666667, "Overall": 0.7169447244623656}, {"timecode": 62, "before_eval_results": {"predictions": ["Curtis James Martin Jr.", "Gabriel Iglesias", "The Snowman", "bushan Patel", "Helsinki, Finland", "Nayvadius DeMun Wilburn", "Tommy Cannon", "Scottish national team", "203", "Ward Bond", "Illinois's 15 congressional district", "Buffalo", "7,500 and 40,000", "5,112", "Prof Media", "the lead roles of Timmy Sanders and Jack", "four months in jail", "Michael Redgrave", "Sturt", "big Machine Records", "two Manhattan high school students who share a tentative month-long romance", "Europe", "Trilochanapala", "deadpan sketch group", "small family car", "Mexican", "Algernod Lanier Washington", "14,000 people", "in photographs, film and television", "37", "Taoiseach of Ireland", "137th", "Mr. Nice Guy", "Japan Airlines Flight 123", "professional wrestling", "Bury St Edmunds, Suffolk, England", "Loretta Lynn", "Ford Island", "barcodes", "video game", "The United States of America", "Lerotholi Polytechnic Football Club", "Ribhu Dasgupta", "Peter Thiel", "orange", "Memphis, Tennessee", "Switzerland\u2013European Union relations", "New Jersey", "Sophie Monk", "Reinhard Heydrich", "lo Stivale", "Mesopotamia, the land in and around the Tigris and Euphrates rivers", "September 2000", "Woodrow Wilson", "our mutual friend", "oregon", "Volkswagen", "l Larry Flynt.", "Pope Benedict XVI", "Windsor, Ontario,", "pearl", "sarsaparilla", "overbite", "Iran of trying to build nuclear bombs,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7061896653693529}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, true, false, true, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666665, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0625, 0.6666666666666666, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4444444444444444, 1.0, 0.6666666666666666, 1.0, 0.8, 0.6153846153846153, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-1675", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-4554", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-2297", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-5666", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-3842", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-353", "mrqa_searchqa-validation-6948"], "SR": 0.5625, "CSR": 0.5483630952380952, "EFR": 0.9642857142857143, "Overall": 0.7165141369047618}, {"timecode": 63, "before_eval_results": {"predictions": ["Tinseltown", "Claude Monet", "Brazil.", "Jacob Zuma,", "apartment building in Cologne, Germany,", "in July", "2005 & 2006 Acura MDX", "Ryan Adams.", "The forehead and chin", "Olympia,", "27-year-old's", "next week", "April 26, 1913.", "12-1", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "next year", "Swamp Soccer", "his son, Isaac, and daughter, Rebecca.", "The Falklands, known as Las Malvinas in Argentina,", "everyone can use solar and renewable energy at home everyday,\"", "Roger Federer", "tennis", "two", "1950s", "Gary Player,", "1 out of every 17 children under 3 years old", "\"Rin Tin Tin: The Life and the Legend\" [ Simon & Schuster],", "litter reduction and recycling.", "President George Bush", "an average of 25 percent", "The crash destroyed four homes and killed two people who lived in at least one of the homes,", "800,000", "Sporting Lisbon", "President Sheikh Sharif Ahmed", "2005.", "\"He went there to receive this bullet. If he would not have been wounded; he wouldn't be in the hospital; he would be living in peace with his family.\"", "Tomas Olsson, the journalists' Swedish attorney.", "Israel", "Sunday's", "Swat Valley.", "Jeffrey Jamaleldine,", "The Rev. Alberto Cutie", "all day starting at 10 a.m.", "\"a fantastic five episodes.\"", "to try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "recite her poetry", "in the head", "U.S. soldiers", "neck", "The island's dining scene", "Andrew Garfield", "New England Patriots", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "gold", "The Mystery of Edwin Drood", "William Bligh", "Melbourne", "1998", "23 July 1989", "Tuesday", "Evian", "Ashbury", "Kind Hearts and Coronets"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7199144718506132}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, false, false, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2666666666666667, 0.923076923076923, 0.13333333333333333, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4347826086956522, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-271", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1346", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-3261", "mrqa_searchqa-validation-5963"], "SR": 0.59375, "CSR": 0.549072265625, "retrieved_ids": ["mrqa_squad-train-83222", "mrqa_squad-train-67960", "mrqa_squad-train-8632", "mrqa_squad-train-84762", "mrqa_squad-train-42104", "mrqa_squad-train-66902", "mrqa_squad-train-45057", "mrqa_squad-train-5968", "mrqa_squad-train-39674", "mrqa_squad-train-68912", "mrqa_squad-train-32209", "mrqa_squad-train-34297", "mrqa_squad-train-21037", "mrqa_squad-train-6577", "mrqa_squad-train-73432", "mrqa_squad-train-44057", "mrqa_naturalquestions-validation-7262", "mrqa_triviaqa-validation-4512", "mrqa_naturalquestions-validation-7101", "mrqa_triviaqa-validation-7563", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-3864", "mrqa_searchqa-validation-5715", "mrqa_hotpotqa-validation-5500", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-1064", "mrqa_naturalquestions-validation-8660", "mrqa_newsqa-validation-1319", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-4544", "mrqa_triviaqa-validation-4730", "mrqa_searchqa-validation-960"], "EFR": 1.0, "Overall": 0.723798828125}, {"timecode": 64, "before_eval_results": {"predictions": ["\"Den V\u00e6gelsindede\"", "Max Martin and Shellback", "William J. Weaver", "6,396", "Reinhard Heydrich", "Standard Oil", "over 50 million singles", "Lieutenant Colonel Horace Meek Hickam", "Charles Russell", "May 1, 2011", "Konstant\u012bns Raudive", "West Cheshire Association Football League", "Transporter 3", "1983", "December 13, 1920", "the U\u00ed \u00cdmair", "more than 265 million", "2004", "The Eisenhower Executive Office Building", "Big 12 Conference", "Thocmentony", "thirteen", "Robert Bunda", "New Jersey", "Black Panther Party", "Walt Disney and Ub Iwerks", "\"Queen In-hyun's Man\"", "\"Lend a hand \u2014 care for the land!\"", "Daniel Louis Castellaneta", "other individuals, teams, or entire organizations", "1,467 rooms", "Ian Rush", "John Alexander", "The 2008\u201309 UEFA Champions League", "Kramer Guitars", "El Nacimiento in M\u00fazquiz Municipality", "1968", "Holston River", "July 10, 2017", "London", "jazz homeland section of New Orleans and on that part of the South in particular", "Neon City", "Stephen Mangan", "largest Mission Revival Style building in the United States", "Darci Kistler", "The Terminator", "Samoa", "The single became Lamar's second number-one single on the US \"Billboard\" Hot 100 after \"Bad Blood\"", "Timo Hildebrand", "Univision", "first flume ride in Ireland", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "The Statue of Freedom", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah ; and the Gemara ( c. 500 CE )", "Mexico", "Julie Andrews Edwards", "Timothy Laurence", "Sen. Barack Obama", "$75 for full-day class,", "\"Nu au Plateau de Sculpteur,\"", "scotch", "The Bridges of Madison County", "Thomas Jefferson", "foreign exchange option"], "metric_results": {"EM": 0.625, "QA-F1": 0.693311403508772}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.10526315789473684, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-2758", "mrqa_hotpotqa-validation-1818", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-861", "mrqa_hotpotqa-validation-755", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-5191", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-4073", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-12600", "mrqa_searchqa-validation-1518", "mrqa_naturalquestions-validation-8414"], "SR": 0.625, "CSR": 0.5502403846153846, "EFR": 1.0, "Overall": 0.7240324519230769}, {"timecode": 65, "before_eval_results": {"predictions": ["prophets and beloved religious leaders", "John Ernest Crawford", "Justin Timberlake", "cells", "Indo - Pacific", "the leaves of the plant species Stevia rebaudiana", "Paul von Hindenburg", "Universal Pictures", "July 31, 2010", "T - Bone Walker", "the entrance to the 1889 World's Fair", "Bobby Darin", "Alex Skuby", "four", "James Rodr\u00edguez", "Lou Rawls", "Andrew Garfield", "Juliet", "Payaya Indians", "a popular and influential campaign song of the Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "Robert Irsay", "the ground surface enters the soil", "1940", "the pulmonary trunk or main pulmonary artery", "Puente Hills Mall", "1977", "HTTP / 1.1 200 OK", "June 1992", "general taxation", "28 July 1914", "Richard Stallman", "before the start of the era", "October 27, 1904", "the early - to - mid fourth century", "small marsupial mole ( Notoryctes typhlops )", "Tom Burlinson, Red Symons and Dannii Minogue", "the final scene of the fourth season", "Auburn by a score of 40 - 17, on November 11, 2017", "during meiosis", "a contemporary drama in a rural setting", "Yuzuru Hanyu", "Italian Agostino Bassi", "Rachel Sarah Bilson", "grass and sedges", "Jonathan Cheban", "2005", "computers or in an organised paper filing system", "the executive", "Missouri River", "sport utility vehicles", "March 2, 2016", "Andy Murray", "In Reel Life:", "Gretel", "Get Him to the Greek", "Netflix", "Union Hill section of Kansas City, Missouri", "three", "The controversial quote is part of a eight-page feature article about Hogan to be published in the magazine's Friday edition.", "fifth", "Tina Turner", "Bingo SOLO", "Amsterdam", "\"Salve\" (SAHL-way) in the singular"], "metric_results": {"EM": 0.5, "QA-F1": 0.5940125517442327}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, false, true, false, true, true, false, false, false], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.20689655172413793, 1.0, 0.0, 1.0, 0.0, 0.42857142857142855, 1.0, 0.0, 1.0, 1.0, 0.6, 1.0, 0.15384615384615383, 1.0, 1.0, 0.5, 0.0, 1.0, 0.16666666666666669, 0.5714285714285715, 0.0, 1.0, 0.8, 1.0, 0.2727272727272727, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-9972", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-5751", "mrqa_naturalquestions-validation-4847", "mrqa_triviaqa-validation-266", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-5315", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-3806", "mrqa_newsqa-validation-2388", "mrqa_searchqa-validation-9384", "mrqa_searchqa-validation-13572", "mrqa_triviaqa-validation-2486"], "SR": 0.5, "CSR": 0.5494791666666667, "EFR": 0.96875, "Overall": 0.7176302083333334}, {"timecode": 66, "before_eval_results": {"predictions": ["british", "huggins", "720", "Steely Dan", "glen Goodman", "fred perry", "about a mile north of the village", "hladetina", "Moby- Dick", "the Iron Age", "fred sheeran", "Tallinn", "The Great Gatsby", "The Gunpowder Plot", "Moldova", "surrey", "Edwina Currie", "iSnare Free Encyclopedia", "IKEA", "Picasso", "Some Like It Hot", "j. S. Bach", "Tony Blair", "Pickwick", "540", "la Guaira", "Ireland", "fred and white machines which Ayrton Senna and Alain Prost battled in, as well as the 1993 machine,", "Jim Peters", "racing", "onion", "bobby brown", "1948", "british", "Sikh", "giraffa camelopardalis", "kabuki", "email", "Zachary Taylor", "indigo", "Friday", "for gallantry", "swindon town", "cricket", "Jordan", "british", "Northern", "hongi", "basketball", "Snow White", "Italy", "Zane Lowe's show on BBC Radio 1 in June 2010, at the Rockstar offices in New York in July 2010", "Buddhism", "endocytosis", "Swabia", "1986", "Charles L. Clifford", "Eleven", "Joe Pantoliano,", "Robert Barnett,", "Jeopardy", "The Bridges of Madison County", "Paraguay", "WikiLeaks"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5638020833333333}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-5759", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-5239", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-682", "mrqa_triviaqa-validation-4211", "mrqa_triviaqa-validation-5362", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7503", "mrqa_triviaqa-validation-2013", "mrqa_triviaqa-validation-1031", "mrqa_triviaqa-validation-1134", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-2567", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-7545", "mrqa_triviaqa-validation-7525", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-10355", "mrqa_hotpotqa-validation-2378", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2030", "mrqa_searchqa-validation-3081", "mrqa_hotpotqa-validation-1714"], "SR": 0.53125, "CSR": 0.5492070895522387, "retrieved_ids": ["mrqa_squad-train-57762", "mrqa_squad-train-41450", "mrqa_squad-train-62628", "mrqa_squad-train-73810", "mrqa_squad-train-57728", "mrqa_squad-train-22898", "mrqa_squad-train-52039", "mrqa_squad-train-41543", "mrqa_squad-train-50746", "mrqa_squad-train-31706", "mrqa_squad-train-48679", "mrqa_squad-train-29615", "mrqa_squad-train-12220", "mrqa_squad-train-69584", "mrqa_squad-train-43954", "mrqa_squad-train-17160", "mrqa_naturalquestions-validation-5094", "mrqa_triviaqa-validation-6052", "mrqa_naturalquestions-validation-390", "mrqa_hotpotqa-validation-4766", "mrqa_triviaqa-validation-344", "mrqa_newsqa-validation-3579", "mrqa_hotpotqa-validation-23", "mrqa_squad-validation-10273", "mrqa_naturalquestions-validation-8733", "mrqa_searchqa-validation-4485", "mrqa_hotpotqa-validation-577", "mrqa_newsqa-validation-1611", "mrqa_hotpotqa-validation-2901", "mrqa_triviaqa-validation-4232", "mrqa_hotpotqa-validation-1052", "mrqa_triviaqa-validation-6633"], "EFR": 0.9666666666666667, "Overall": 0.7171591262437811}, {"timecode": 67, "before_eval_results": {"predictions": ["Yann Martel", "The Archers", "Tiffany and Co.", "ndebele", "Cambridge", "Singapore", "1830", "Lorraine r\u00e9gion", "parisitosis", "chaucer", "sports agent", "rough collie", "Sen. Edward M. Kennedy", "jeremy will", "red squirrel", "Richard Lester", "car Company Logos", "isle of krak\u00f3w", "gooseberry", "George W. Bush's", "The Color Purple", "isle of williams", "Il Divo", "Barack Obama", "1984", "isle of willhelmshaven", "China", "Ecuador", "clement", "louis williams", "victoria", "360", "jeremy clems", "1123", "estavros", "Sparta", "Hyundai", "estonia", "julian Fellowes", "haddock", "Yemen", "Tina Turner", "mainland China, Hong Kong and Macau", "Nowhere Boy", "victoria", "head and neck", "quant", "Edward Lear", "35", "fred anne", "williams", "Kody and Janelle", "South Asia", "Uralic", "New York City", "1942", "a reward for ability or finding an easy way out of an unpleasant situation by dishonest means", "Larry King", "New Delhi.", "Ron Howard", "Oakland Raiders", "the Mediterranean", "Queen Isabella", "Turing"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5721726190476191}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.13333333333333333, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-4878", "mrqa_triviaqa-validation-1176", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-1662", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-4792", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-467", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-1501", "mrqa_naturalquestions-validation-5787", "mrqa_naturalquestions-validation-2399", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-1641", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-177", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1508"], "SR": 0.484375, "CSR": 0.5482536764705883, "EFR": 0.9696969696969697, "Overall": 0.7175745042335115}, {"timecode": 68, "before_eval_results": {"predictions": ["Siri", "Philippines", "heavy turbulence", "Brian Smith.", "Tim Clark, Matt Kuchar and Bubba Watson", "\"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship.", "Ricardo Valles de la Rosa,", "Elin Nordegren", "We Found Love", "immediate release into the United States of 17 Chinese Muslims who have been held for several years in the U.S. military facility at Guantanamo Bay, Cuba.", "millionaire's surtax,", "set at \"E! News\"", "about 50", "two-state solution", "Yusuf Saad Kamel", "central London offices", "his father, Osama", "Iran", "South Africa", "the insurgency,", "of all faiths", "The Rosie Show", "Zapata Reyes,", "March 24,", "completely changed the business of music,", "in the mouth.", "100", "Anne Frank,", "The EU naval force", "five", "Joel \"Taz\" Di Gregorio", "The father of Haleigh Cummings,", "man-made island shaped like a date palm tree", "near the Somali coast", "high-ranking drug cartel member Arnoldo Rueda Medina.", "hiring veterans as well as job training for all service members leaving the military.", "general astonishment", "northwestern Montana", "test-launched a rocket capable of carrying a satellite,", "without bail", "February 12", "general astonishment", "frees up a place for another non-European Union player in Frank Rijkaard's squad.", "Chile", "separated", "Democratic VP candidate", "martial arts,", "poor, older than 55, rural residents or racial minorities,", "Two Russian bombers have landed at a Venezuelan airfield,", "65 years ago", "causing enormous suffering and massive displacement,\"", "nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "warning signs", "between India and Nepal", "horse", "k Kathryn C. Taylor", "Brooklyn", "anabolic-androgenic steroids", "Real Madrid and the Spain national team", "Brea, California", "Titanic", "Zanzibar", "dualism", "Wordsworth"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5598463559262122}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.18604651162790697, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.8, 0.0, 0.3529411764705882, 0.6666666666666666, 0.5714285714285715, 0.923076923076923, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2162162162162162, 0.0, 0.0, 0.9189189189189189, 0.3076923076923077, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-498", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-3025", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-3164", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1139", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-884", "mrqa_hotpotqa-validation-3117", "mrqa_searchqa-validation-5208"], "SR": 0.421875, "CSR": 0.5464221014492754, "EFR": 0.972972972972973, "Overall": 0.7178633898844496}, {"timecode": 69, "before_eval_results": {"predictions": ["1902", "Max Martin and Shellback", "high-end premium open-air shopping center, the Americana Manhasset", "Mayfair", "the Dublin West constituency", "28 January 1864, Halifax, Yorkshire, England", "Arab", "the southern North Sea", "Larry Richard Drake", "The Bad Hemingway Contest", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "villanelle", "\"Eternal Flame\"", "\"Back to December\"", "Heather Elizabeth Langenkamp", "two Nobel Peace Prizes", "Londonderry", "Daniel Craig", "Hamburger SV", "Four Weddings and a Funeral", "HC Davos", "Mulberry", "Edward Longshanks and the Hammer of the Scots", "7 November 1435", "Christopher McCulloch", "novel", "The Daily Stormer", "Fort Saint Anthony", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services", "Japan", "1919", "\"Danger Mouse\"", "the western end of the National Mall in Washington, D.C., across from the Washington Monument", "Len Wiseman", "Stephen Crawford Young", "\"My Backyard\" in Jacksonville, Florida,", "Gerard \"Gerry\" Adams", "\"Kill Your Darlings\"", "Girls' Generation", "Robert Matthew Hurley", "September 1901", "Friday", "anabolic\u2013androgenic steroids", "North West England", "Division I", "\"Polovetskie plyaski\"", "Kentucky", "26 September 1961", "1896", "2000", "Donald Sterling", "20 - year period", "Saint Peter", "mining", "the Earth", "the best value diamond", "horses", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Asashoryu's wrestling style,", "The Tupolev Tu-160 strategic bombers", "the Juilliard School", "lizard hips", "Scouts of America", "Inuit"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6987912489557226}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.4444444444444444, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.8, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.5, 0.4, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-4552", "mrqa_hotpotqa-validation-429", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-95", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-375", "mrqa_triviaqa-validation-984", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-3029", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-11439", "mrqa_searchqa-validation-4320"], "SR": 0.546875, "CSR": 0.5464285714285715, "retrieved_ids": ["mrqa_squad-train-45108", "mrqa_squad-train-4758", "mrqa_squad-train-4371", "mrqa_squad-train-50210", "mrqa_squad-train-48845", "mrqa_squad-train-3053", "mrqa_squad-train-5546", "mrqa_squad-train-50000", "mrqa_squad-train-13216", "mrqa_squad-train-30416", "mrqa_squad-train-31025", "mrqa_squad-train-7945", "mrqa_squad-train-36125", "mrqa_squad-train-79108", "mrqa_squad-train-64205", "mrqa_squad-train-86078", "mrqa_newsqa-validation-502", "mrqa_triviaqa-validation-55", "mrqa_naturalquestions-validation-1586", "mrqa_hotpotqa-validation-307", "mrqa_naturalquestions-validation-1053", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-3075", "mrqa_searchqa-validation-1437", "mrqa_newsqa-validation-3459", "mrqa_naturalquestions-validation-4961", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-305", "mrqa_newsqa-validation-2167", "mrqa_searchqa-validation-960", "mrqa_hotpotqa-validation-4967", "mrqa_naturalquestions-validation-2024"], "EFR": 1.0, "Overall": 0.7232700892857143}, {"timecode": 70, "UKR": 0.765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3201", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-9", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7182", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2622", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10335", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-1959", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2717", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5272", "mrqa_squad-validation-5492", "mrqa_squad-validation-5590", "mrqa_squad-validation-5686", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-60", "mrqa_squad-validation-6091", "mrqa_squad-validation-6255", "mrqa_squad-validation-629", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8436", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9484", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935"], "OKR": 0.841796875, "KG": 0.5265625, "before_eval_results": {"predictions": ["Arkansas", "the early 1970s", "Paris", "875 acre", "every aspect of public and private life", "Maria von Trapp", "the Rat Pack", "12", "port city of Aden", "Scott Eastwood", "Springfield, Massachusetts", "Eva Ibbotson", "David Michael Bautista Jr.", "2 March 1972", "Tie Domi", "Mika H\u00e4kkinen", "Princess Jessica", "Australia", "Robert Moses", "Honolulu", "St. Louis County", "Jack Richardson", "his virtuoso playing techniques and compositions in orchestral fusion", "YouPorn", "the Salzburg Festival", "political correctness", "devotional literature", "Martin Joseph O'Malley", "1890s", "the Secret Intelligence Service", "Currer Bell", "UNLV Rebels", "mermaid", "850 m", "DeskMate", "Athenion", "Adolfo Rodr\u00edguez Sa\u00e1", "The Beatles", "Czech (Bohemian) and German (Franconian)", "ninth", "Hanna", "Manchester Victoria station", "Pete Wareham and Mark Lockheart", "My Love from the Star", "Captain Cook's Landing Place", "George I", "Kim Yeon-soo", "37", "bass", "Citizens for a Sound Economy", "Barbara Feldon", "H CO", "beloved religious leaders", "Bill Russell", "Andre Agassi", "Vienna", "the Phillies", "fill a million sandbags and place 700,000 around our city,\"", "Caster Semenya", "to stop manufacturing 14 unapproved narcotics that are widely used to treat pain.", "the Cuyahoga River", "uranium", "Peter Sellers", "the river Elbe"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6719158026113672}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.06451612903225808, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-1510", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-3675", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-4925", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-1991", "mrqa_naturalquestions-validation-9220", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-105", "mrqa_newsqa-validation-1065", "mrqa_searchqa-validation-10027", "mrqa_triviaqa-validation-4324"], "SR": 0.59375, "CSR": 0.5470950704225352, "EFR": 1.0, "Overall": 0.736215889084507}, {"timecode": 71, "before_eval_results": {"predictions": ["Nearly eight in 10", "Marie-Therese Walter.", "help at-risk youth,", "the Russian air force,", "a female soldier,", "Nearly eight in 10", "Goa", "Iran to Nazi Germany", "100 percent", "al-Shabaab", "Susan Atkins", "Casa de Campo International Airport in the Dominican Republic", "\"Operation Crank Call,\"", "228", "hostile war zones,", "National September 11 Memorial Museum", "Harlem, New York.", "\"I don't think I'll be particularly extravagant.\"", "during last year's Gaza campaign", "Austrian incest case", "1959.", "his record label said Friday.", "269,000", "issued his first military orders as leader of North Korea", "Pixar's \"Toy Story\"", "a group of teenagers.", "Six", "Luis Carlos Ameida and his friends", "27-year-old's", "outside influences in next month's run-off", "federal government is asleep at the switch", "\"A Whiter Shade of Pale\"", "security breach", "$250,000", "combat veterans", "$1.5 million.", "unwanted horses", "$10 billion", "Christopher Savoie", "United States, NATO member states, Russia", "1,500", "trading goods and services without exchanging money", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "Fiona MacKeown", "Sen. Barack Obama", "\"The Real Housewives of Atlanta,\"", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "Israeli forces were responding to militant fire near the complex.", "Guinea, Myanmar, Sudan and Venezuela.", "pine beetles", "Sudanese nor orphans,", "Aspirin", "March 1", "Indo - Pacific", "hewer", "quetzalcoatl", "Maryland", "2012", "Acela Express", "Crackle", "a porcupine", "oxygen", "the Bird of Prey", "Harry Truman"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5765323218448218}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.5, 1.0, 0.33333333333333337, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-1797", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-4163", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-933", "mrqa_triviaqa-validation-2418", "mrqa_triviaqa-validation-4549", "mrqa_searchqa-validation-9135", "mrqa_searchqa-validation-14427"], "SR": 0.53125, "CSR": 0.546875, "EFR": 0.8333333333333334, "Overall": 0.7028385416666667}, {"timecode": 72, "before_eval_results": {"predictions": ["Jesse Triplett", "1997", "Sharyans Resources", "an vehicle that is both four - wheel - drive and primarily a road car", "U.N. Owen '' ( i.e., `` Unknown '' ), he entices to an island various people who have been responsible for the deaths of other people, but escaped justice", "Texas A&M University", "stromal connective tissue", "a book of the Old Testament", "Anatomy", "a maritime signal, indicating that the vessel flying it is about to leave", "President Lyndon Johnson", "the Russian Civil War", "Olivia Olson", "Eukarya -- called eukaryotes", "Mara Jade", "Gary Grimes as Hermie", "very important", "Edward IV of England", "Ashrita Furman", "A 30 - something man ( XXXX )", "Jean Fernel", "in 2007 and 2008 at a cost of CDN $51 million", "in May 1980", "erosion", "English occupational name for one who obtained his living by fishing or living by a fishing weir", "in both instrumental and vocal versions, and has also appeared in a number of subsequent films and television programs", "John F. Kennedy", "Rolf L\u00f8vland ( Norway, 1985 and 1995 ) and Brendan Graham ( Ireland, 1994 and 1996 )", "revenge and karma", "the misuse or `` taking in vain '' of the name of the God of Israel", "England and Wales", "1996", "c. 8000 BC", "Idaho", "early Christians of Mesopotamia, and from there it spread into Russia and Siberia through the Orthodox Churches, and later into Europe through the Catholic and Protestant Churches", "eight hours ( UTC \u2212 08 : 00 )", "Dr. Rajendra Prasad", "Carlos Alan Autry Jr.", "Jay Baruchel", "Ann E. Todd", "bachata music", "Butter Island off North Haven, Maine in the Penobscot Bay", "the end of the 18th century, and in most areas was at its peak in the approximate period from 1800 to 1850", "during the 1890s Klondike Gold Rush, when strong sled dogs were in high demand", "an extension of the Hypertext Transfer Protocol ( HTTP ) for secure communication over a computer network", "3", "1939", "the BBC", "it was the fifth UK album release by the band, and contains fourteen songs in its original British form", "in all land - living organisms, both alive and dead, as well as carbon stored in soils", "Felicity Huffman", "John of Gaunt", "75 or older", "the A162", "Montana State University", "Sun Valley, Idaho", "president of Guggenheim Partners", "bikinis made out of either heavy flannel or wool -- fabrics that would not be transparent when wet -- and covered the entire body from neck to toe.", "doctors", "The plane had a crew of 14 people and was carrying an additional 98 passengers,", "the river", "an online education management platform", "the Crow", "Madrid's Barajas International Airport during a stopover late Monday and informed authorities that he planned to request political asylum,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6070934399226209}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, false, false, true, false, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, false, true, true, false, false, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.5, 0.0, 0.07692307692307693, 1.0, 0.4, 0.28571428571428575, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 0.5714285714285715, 0.4, 1.0, 1.0, 0.0, 0.0, 0.11764705882352941, 1.0, 0.0, 0.5, 1.0, 0.3333333333333333, 1.0, 0.2758620689655173, 0.6, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.9047619047619047, 0.5555555555555556, 0.7368421052631579, 1.0, 1.0, 1.0, 0.0, 0.21052631578947367, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.4, 0.2758620689655173, 1.0, 0.21428571428571427, 0.0, 0.0, 1.0, 0.09523809523809525]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-10066", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-6859", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-133", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-7492", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-8474", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-5787", "mrqa_hotpotqa-validation-1509", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3544", "mrqa_searchqa-validation-3477", "mrqa_searchqa-validation-10906", "mrqa_newsqa-validation-646"], "SR": 0.4375, "CSR": 0.5453767123287672, "retrieved_ids": ["mrqa_squad-train-33168", "mrqa_squad-train-49266", "mrqa_squad-train-47196", "mrqa_squad-train-63291", "mrqa_squad-train-35229", "mrqa_squad-train-58669", "mrqa_squad-train-52920", "mrqa_squad-train-49900", "mrqa_squad-train-68680", "mrqa_squad-train-45532", "mrqa_squad-train-8304", "mrqa_squad-train-16907", "mrqa_squad-train-31736", "mrqa_squad-train-17380", "mrqa_squad-train-2956", "mrqa_squad-train-66679", "mrqa_hotpotqa-validation-5835", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-339", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-14691", "mrqa_triviaqa-validation-3332", "mrqa_naturalquestions-validation-9342", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-3122", "mrqa_naturalquestions-validation-2990", "mrqa_hotpotqa-validation-4528", "mrqa_naturalquestions-validation-9591", "mrqa_triviaqa-validation-3642", "mrqa_naturalquestions-validation-7635", "mrqa_triviaqa-validation-5426", "mrqa_squad-validation-805"], "EFR": 0.9444444444444444, "Overall": 0.7247611063546422}, {"timecode": 73, "before_eval_results": {"predictions": ["Brazil", "The Fall Guy", "Crown", "Montessori", "Deadbeat", "Alexander Hamilton", "science fiction", "March of the Crosby", "Adidas", "Fletcher Christian", "an ambulance", "Condoleezza Rice", "Pakistan", "China", "liquor", "Texas", "a Conductor", "John James Audubon", "Pilate", "Barry Goldwater", "synapses", "the half-pipe", "Jackie Collins", "carioca", "Freakonomics", "George Washington Carver", "Devonian", "Champagne", "Red Heat", "New Orleans", "France", "a carrel", "a flop", "Prince William", "Sherlock Holmes", "jumbo pasta", "Orion", "India", "carbon monoxide", "King John", "plug in", "snowman", "Thailand", "manslaughter", "computer programming", "the Tennessee River", "Hipparchus", "Billy Idol", "the Missouri Compromise", "the Rat", "Tom Hanks", "to encounter antigens passing through the mucosal epithelium", "$1.528 billion", "to be married", "Conrad Murray", "Gryffendor", "Czech Republic", "Sochi, Russia", "two years", "Manchester Airport", "President Obama", "two weeks after Black History Month", "American Civil Liberties Union", "monthly and then quarterly men's magazine"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6468750000000001}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7908", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-13288", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-9645", "mrqa_searchqa-validation-16712", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-5326", "mrqa_searchqa-validation-2696", "mrqa_searchqa-validation-10889", "mrqa_searchqa-validation-7195", "mrqa_searchqa-validation-6326", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-284", "mrqa_searchqa-validation-12519", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-3189", "mrqa_searchqa-validation-14970", "mrqa_searchqa-validation-10515", "mrqa_searchqa-validation-15757", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-10093", "mrqa_triviaqa-validation-5472", "mrqa_hotpotqa-validation-4724", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-131", "mrqa_hotpotqa-validation-1233"], "SR": 0.546875, "CSR": 0.5453969594594594, "EFR": 1.0, "Overall": 0.7358762668918919}, {"timecode": 74, "before_eval_results": {"predictions": ["sugarcane", "( Angela) Rippon", "Anna", "liver", "Private Eye", "Gibraltar", "Jack Ruby", "the 1500 meter event", "British Airways", "science", "b4425", "Pete Best", "Bonnie and Clyde", "Avatar", "Concepcion", "st Moritz", "Edmund Cartwright", "Par-5", "Zeus", "Japanese silvergrass", "April", "Sir Arthur", "Wolfgang Amadeus Mozart", "bees", "canley", "\"Dance of the Mirlitons\"", "Lightweight", "albion", "Sesame Street", "photography", "kirsty young", "(Samuel) Johnson", "geography", "a bear", "ganga", "tabloid", "car door", "kolkata", "the Temple of Artemis", "Bangladesh", "Shangri-La", "The Tempest", "Diana Ross", "Mansion House", "Ishmael", "repechage", "Crusades", "Kiri Te Kanawa", "Churchill Downs", "Up stairs Down stairs", "One Direction", "ulnar nerve", "Gibraltar", "111", "Merck Sharp & Dohme", "first-round", "Vietnam War", "\"It feels great to be back at work,\"", "Amnesty International.", "after Wood went missing off Catalina Island,", "When Harry Met Sally", "Breckenridge", "The Fray", "Clinton."], "metric_results": {"EM": 0.625, "QA-F1": 0.6620510057471265}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.13793103448275862, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-5243", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-7118", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-7365", "mrqa_hotpotqa-validation-4763", "mrqa_hotpotqa-validation-3058", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3966", "mrqa_searchqa-validation-14621", "mrqa_newsqa-validation-2566"], "SR": 0.625, "CSR": 0.5464583333333333, "EFR": 0.9583333333333334, "Overall": 0.7277552083333333}, {"timecode": 75, "before_eval_results": {"predictions": ["Robert FitzRoy", "Martin O'Neill", "2012", "3730 km", "Kind Hearts and Coronets", "Maine", "Japan", "hiphop", "erotic thriller \"Ch Chloe\" (2009)", "Pylos and Thebes", "Pearl Jam", "Arabella Churchill", "Sir William McMahon", "Hopi", "Western District", "Australian", "Jean-Marie Pfaff", "sixth year head coach Tim Cluess", "Brazil", "1954", "Newcastle upon Tyne, England", "four", "Robert Sargent Shriver Jr.", "NXT Tag Team Championship", "Chinese Coffee", "Love and Theft", "Adelaide", "top to bottom", "University of Georgia", "just over 1 million", "Indian", "The Last of the Mohicans", "Centennial Olympic Stadium", "\"media for the 65.8 million,\"", "Paul Avery", "25 October 1921", "Maersk Group", "J. Cole", "Idisi", "The Books", "Baja California Peninsula", "Danish", "London, England", "Rochdale, North West England", "1959", "Telugu and Tamil", "Centers for Medicare and Medicaid Services", "Laura Jeanne Reese Witherspoon", "Koch Industries", "Liverpudlian", "Mindy Kaling", "3 October 1990", "Wednesday, September 21, 2016", "state - of - the - art photography of the band's performance", "earache", "surrey", "melodious", "$2 billion", "San Simeon, California,", "Immigration and Customs Enforcement accuse the agency in a lawsuit of forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "Patrick", "a tomb of the Unknown Soldier", "Mount Vesuvius", "in a park in a residential area of Mexico City,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6758804563492063}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 0.25, 0.4, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.2, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.1111111111111111, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-808", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-4368", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-2049", "mrqa_hotpotqa-validation-2971", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-3556", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-1393", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-140", "mrqa_searchqa-validation-13410"], "SR": 0.5625, "CSR": 0.5466694078947368, "retrieved_ids": ["mrqa_squad-train-59908", "mrqa_squad-train-64392", "mrqa_squad-train-41138", "mrqa_squad-train-27596", "mrqa_squad-train-1116", "mrqa_squad-train-86299", "mrqa_squad-train-47465", "mrqa_squad-train-71365", "mrqa_squad-train-51542", "mrqa_squad-train-23248", "mrqa_squad-train-385", "mrqa_squad-train-66815", "mrqa_squad-train-72450", "mrqa_squad-train-51161", "mrqa_squad-train-74965", "mrqa_squad-train-32568", "mrqa_hotpotqa-validation-574", "mrqa_searchqa-validation-7976", "mrqa_hotpotqa-validation-3391", "mrqa_searchqa-validation-3485", "mrqa_naturalquestions-validation-8688", "mrqa_squad-validation-4730", "mrqa_naturalquestions-validation-6019", "mrqa_hotpotqa-validation-3729", "mrqa_newsqa-validation-1551", "mrqa_naturalquestions-validation-4416", "mrqa_squad-validation-8452", "mrqa_squad-validation-5303", "mrqa_naturalquestions-validation-3569", "mrqa_triviaqa-validation-2680", "mrqa_triviaqa-validation-6212", "mrqa_hotpotqa-validation-1023"], "EFR": 0.9642857142857143, "Overall": 0.7289878994360903}, {"timecode": 76, "before_eval_results": {"predictions": ["pet Sounds", "Culloden", "ars gratia artis", "Johann Strauss II", "James Callaghan", "bonsai", "Libor", "Dublin", "pyrenees", "leprosy", "left", "harridan Grizelda", "avocado", "Anne of Cleves", "The Double", "lexis", "Supertramp", "hula-Hoops", "Augustus", "one night / I Got Stung", "Heston Blumenthal", "Arkansas", "IT Crowd", "Some Like It Hot", "Mr Loophole", "Ken Purdy", "Wolf Hall", "Ernests Gulbis", "Alberto Juantorena", "graffiti", "Friedrich Nietzsche", "Caffari", "cheese", "niece", "Kristiania", "piano", "Herman Melville\u2019s Moby Dick", "moss", "cleopatra", "seven", "pea", "stuart tamseel", "the Sea of Galilee", "1", "Helen of Troy", "mild cognitive impairment", "The Firm", "1966", "an even break", "31536000", "Jordan", "fishes, amphibians, reptiles, birds, and mammals", "December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings", "2018", "Miami Marlins", "Maxwell Smart", "Las Vegas Strip in Paradise, Nevada", "starting place", "Rev. Alberto Cutie", "Michelle Obama", "the sackbut", "270", "cusa", "the American Red Cross"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5742305871212121}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.375, 1.0, 0.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-6685", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-4362", "mrqa_triviaqa-validation-1759", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-5439", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-1026", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-5819", "mrqa_hotpotqa-validation-5149", "mrqa_hotpotqa-validation-71", "mrqa_newsqa-validation-1261", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-14139"], "SR": 0.53125, "CSR": 0.5464691558441559, "EFR": 0.9, "Overall": 0.7160907061688311}, {"timecode": 77, "before_eval_results": {"predictions": ["Gary Player,", "remains committed to British sovereignty", "The Kirchners", "iPods", "20 minutes of cardio five days a week.", "not guilty by reason of insanity that would have resulted in psychiatric custody.", "Kris Allen,", "Jared Polis", "The IAEA", "Zimbabwe", "Harry Nicolaides,", "Most of those who managed to survive the incident hid in a boiler room and storage closets during the massacre.", "April 2010.", "Zed", "\"[The e-mails]", "eco", "his father's parenting skills.", "Iran", "head injury.", "\"Antichrist.\"", "African National Congress Deputy President Kgalema Motlanthe", "Hugo Chavez", "seven", "Frank's diary.", "The Lost Symbol", "Matthew Fisher,", "Rawalpindi", "The motion seeks dismissal of the charges \"in the interest of justice.\"", "Helmand province, Afghanistan.", "climatecare,", "dental work done, including removal of his diamond-studded braces.", "Ireland.", "United States", "a series of conversations in Arabic, Russian and Mandarin", "Hamas,", "two pages -- usually high school juniors who serve Congress as messengers --", "at least 40", "20,000", "Courtney Love,", "84-year-old", "signed a power-sharing deal with the opposition party's breakaway faction,", "three", "a third beluga whale belonging to the world's largest aquarium has died,", "Naples home.", "Washington State's decommissioned Hanford nuclear site,", "November 26", "sportswear", "Shanghai", "the charity of kidnapping the children and concealing their identities.", "improve health and beauty.", "help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "three preteen boys named Ed, Edd ( called `` Double D '' to avoid confusion with Ed ), and Eddy -- collectively known as `` the Eds ''", "meditation", "Harishchandra", "India and Pakistan", "allergic", "a lie detector", "two Coldplay songs U.F.O and Princess of China were written", "1963", "Black Abbots", "nurse", "Argentina", "(Charles) Baudelaire", "Sleepy Hollow"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6975433554928483}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, true, true, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.21052631578947364, 1.0, 1.0, 0.6666666666666666, 0.21276595744680854, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.18181818181818182, 0.7142857142857143, 0.0, 1.0, 0.16666666666666666, 1.0, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 0.8823529411764706, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-1190", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-3403", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-4133", "mrqa_searchqa-validation-5877"], "SR": 0.59375, "CSR": 0.5470753205128205, "EFR": 1.0, "Overall": 0.736211939102564}, {"timecode": 78, "before_eval_results": {"predictions": ["a \"blind\" spot", "the Silk Road", "Denmark", "George Rogers Clark", "an amu", "a coach dog", "Sweden", "Volleyball", "John Alden", "Ghost World", "deuteronomy", "a map", "Japan", "West End Avenue", "Job", "standard pitch", "art deco", "Spider-Man", "Siddhartha Gautama", "Elie Wiesel", "Lee Pace", "Johnny Tremain", "a lieutenant colonel", "the National Archives", "Nostradamus", "Madrid", "3:10 to Yuma", "Antarctica", "Ian Fleming", "the Southern Christian Leadership Conference", "Moscow", "a Mercedes-Benz", "Beaux", "the Mormon Tabernacle Choir", "The Scarlet Letter", "Cavelier", "Bangkok", "St. Paul", "positron", "Ted Kennedy", "Jefferson", "Jerusalem", "Pushing Daisies", "cranberry", "Tzatziki", "(Ch'ien) Lung", "the Laborers' International Union", "sharlotka", "canals", "Ishmael", "a self-appointed or mob-operated tribunal", "in ancient Mesopotamia", "Rachel Kelly Tucker", "makes Maria a dress to wear to the neighborhood dance", "London", "Kermadec Islands", "Julius Caesar's", "strongly associated with Gaia and Cybele", "The Danny Kaye Show", "2012", "The Stooges comedic farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "TV's rabbit-ears era.", "identity documents belonging to Miguel Mejia Munera.", "The oceans"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6085937499999999}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.13333333333333333, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.2666666666666667, 0.7499999999999999, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1478", "mrqa_searchqa-validation-9709", "mrqa_searchqa-validation-2424", "mrqa_searchqa-validation-3286", "mrqa_searchqa-validation-14872", "mrqa_searchqa-validation-10281", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-14996", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3782", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-4445", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-10164", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-14542", "mrqa_searchqa-validation-14159", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-3594", "mrqa_triviaqa-validation-579", "mrqa_hotpotqa-validation-516", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-1424", "mrqa_newsqa-validation-875"], "SR": 0.53125, "CSR": 0.546875, "retrieved_ids": ["mrqa_squad-train-79314", "mrqa_squad-train-60686", "mrqa_squad-train-47285", "mrqa_squad-train-47583", "mrqa_squad-train-62625", "mrqa_squad-train-35027", "mrqa_squad-train-82755", "mrqa_squad-train-53624", "mrqa_squad-train-35056", "mrqa_squad-train-953", "mrqa_squad-train-28082", "mrqa_squad-train-74567", "mrqa_squad-train-24719", "mrqa_squad-train-61450", "mrqa_squad-train-73297", "mrqa_squad-train-53606", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-2038", "mrqa_naturalquestions-validation-5550", "mrqa_newsqa-validation-177", "mrqa_triviaqa-validation-4630", "mrqa_hotpotqa-validation-1233", "mrqa_hotpotqa-validation-1687", "mrqa_triviaqa-validation-6352", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-1386", "mrqa_searchqa-validation-10889", "mrqa_naturalquestions-validation-4123", "mrqa_hotpotqa-validation-4554", "mrqa_triviaqa-validation-7332", "mrqa_hotpotqa-validation-4687", "mrqa_searchqa-validation-16595"], "EFR": 1.0, "Overall": 0.7361718749999999}, {"timecode": 79, "before_eval_results": {"predictions": ["12.65 m ( 41.5 ft )", "DeWayne Warren", "a solitary figure who is not understood by others, but is actually wise", "Doug Pruzan", "twelve", "byte - level operations", "JPS, in the old - fashioned meaning `` awe - inspiring ''", "September 19, 2017", "A marriage officiant", "17th Century", "Hermann Ebbinghaus", "Agostino Bassi", "An error does not count as a hit but still counts as an at bat for the batter", "Magnetically soft ( low coercivity ) iron", "Incumbent Democratic mayor Marty J. Walsh", "British Columbia, Canada", "$66.5 million", "Middle Eastern alchemy", "`` 0 '' trunk code", "14 : 46 JST ( 05 : 46 UTC )", "Los Angeles Dodgers", "Dan Stevens", "Bill Russell", "Conrad Lewis", "Ernest Rutherford", "Fa Ze Rug", "10 June 1940", "citizens", "A nominating committee composed of rock and roll historians selects names for the `` Performers '' category ( singers, vocal groups, bands, and instrumentalists of all kinds )", "Amanda Fuller", "`` 200 ''", "1997", "mitochondrial membrane", "the late 1980s", "American swimmer Michael Phelps", "William DeVaughn", "Virginia Dare", "1960s", "Mace Coronel", "2002", "Evermoist", "Pangaea or Pangea", "Selena Gomez", "Leslie and Ben", "dress shop", "6,259 km ( 3,889 mi )", "September 18, 2009", "1960", "March 2, 2016", "the Mishnah", "a Y chromosome", "brindisi", "France", "kennadiy Samokhin", "England", "April 1, 1949", "CBS", "\"green-card warriors\"", "Mumbai", "Brian David Mitchell,", "the Netherlands", "Florence", "Tiger Woods", "try and reduce the cost of auto repairs and insurance Premium for consumers"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6926836738173174}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.2978723404255319, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.4, 1.0, 1.0, 0.1714285714285714, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-5829", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-4862", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-454"], "SR": 0.609375, "CSR": 0.54765625, "EFR": 0.92, "Overall": 0.720328125}, {"timecode": 80, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4247", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-1218", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14542", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-938"], "OKR": 0.830078125, "KG": 0.5015625, "before_eval_results": {"predictions": ["Richard Attenborough and wife Sheila Sim", "Miranda v. Arizona", "Oscar Wilde", "Vancouver Island", "guarneri", "Utrecht", "Vietnam", "georgia austen", "georgia fox", "all-weather", "jodhpurs", "glasnost", "georgia caff", "jazz", "georgia caffers", "king of the Apes", "joanne Woodward", "rococo", "gallons", "Great Dane", "to put under a curse", "Cambodia", "jujitsu", "74th Hunger Games", "neck", "11 years and 302 days", "New Zealand", "the Prussian 2nd Army", "kitty in Boots", "Whisky Galore", "Tunisia", "13", "Sen. Edward M. Kennedy", "georgremont", "head", "Google, the Internet search engine", "shoulder", "Iran", "downton abbey", "bird", "Rudyard Kipling", "backgammon", "\u00ef\u00bf\u00bds", "ejaz Khan", "georgonzola", "beethoven", "exploits on the Island", "ear", "trees", "Imola Circuit", "trout", "Martin Lawrence", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "North Atlantic Ocean", "1961", "Boston Herald", "Lord Chancellor of England", "\"Britain's Got Talent\"", "Ashley \"A.J.\" Jewell,", "19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Nebraska", "Tweedledee", "the East Asian Library", "Aung San Suu Kyi"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5503720238095238}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.05714285714285714, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6362", "mrqa_triviaqa-validation-2821", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-6256", "mrqa_triviaqa-validation-1024", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-4889", "mrqa_triviaqa-validation-7561", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-722", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-550", "mrqa_triviaqa-validation-7592", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-3465", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-1530", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4771", "mrqa_searchqa-validation-3317", "mrqa_searchqa-validation-3618"], "SR": 0.453125, "CSR": 0.5464891975308642, "EFR": 0.9142857142857143, "Overall": 0.7088737323633156}, {"timecode": 81, "before_eval_results": {"predictions": ["george coe", "worcester Cathedral", "monaco", "van rijn", "Illinois", "germany", "paul Maskey", "Stanislas Wawrinka", "tartar sauce", "the Three Graces", "satyrs", "ustavus III", "a synagogue", "martin van buren", "leeds", "george Webb", "ulnar nerve", "white", "Jay-Z", "george clough", "honda", "runcorn", "Vietnam", "macau", "c\u00ef\u00bf\u00bdzanne", "sakhalin", "Croatia", "NBA", "steel", "dolittle", "Dodi Fayed", "crazes", "bird", "Samuel Johnson", "cointreau or similar orange-flavoured liqueur (Cointreau, Grand Marnier or another triple sec)", "belgian", "Victor Hugo", "endosperm", "Adriatic Sea", "heartburn", "news feed", "HMS Conqueror", "george", "braille", "Standard Oil Company", "Cynthia Nixon", "Hamlet", "Wat Tyler", "(University of) Virginia", "126 mph", "Ukraine", "Eddie Murphy", "England and Wales", "Tom Hanks", "Thorgan Hazard", "Lithuanian national team", "kent Hovind", "almost 100", "accusations of improper or criminal conduct.", "\"Janet Frank, the spokesperson for Utah Valley Regional Medical Center, confirmed that Coleman, 42, was being treated there after being admitted on Wednesday.", "Superman", "(University of) Norway", "Towering Inferno", "member states"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5532986111111111}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-6970", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-5770", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-3108", "mrqa_triviaqa-validation-2287", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-1909", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-1039", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-1829", "mrqa_searchqa-validation-5224", "mrqa_naturalquestions-validation-10495"], "SR": 0.46875, "CSR": 0.5455411585365854, "retrieved_ids": ["mrqa_squad-train-31976", "mrqa_squad-train-28486", "mrqa_squad-train-21281", "mrqa_squad-train-80395", "mrqa_squad-train-65539", "mrqa_squad-train-30092", "mrqa_squad-train-33723", "mrqa_squad-train-50620", "mrqa_squad-train-68407", "mrqa_squad-train-25664", "mrqa_squad-train-86299", "mrqa_squad-train-26154", "mrqa_squad-train-20452", "mrqa_squad-train-43595", "mrqa_squad-train-9868", "mrqa_squad-train-66802", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-832", "mrqa_squad-validation-6526", "mrqa_naturalquestions-validation-10257", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-4277", "mrqa_newsqa-validation-591", "mrqa_triviaqa-validation-590", "mrqa_squad-validation-6361", "mrqa_newsqa-validation-854", "mrqa_triviaqa-validation-6561", "mrqa_newsqa-validation-3714", "mrqa_triviaqa-validation-1355", "mrqa_newsqa-validation-3541", "mrqa_triviaqa-validation-2527", "mrqa_squad-validation-3730"], "EFR": 1.0, "Overall": 0.7258269817073171}, {"timecode": 82, "before_eval_results": {"predictions": ["london daguerre", "france", "tarn", "germany", "Sheffield", "subtropical", "piano", "Louis XVIII", "pat Cash", "patagonia", "Wild Atlantic Way", "Kyoto", "swimming goggles", "repechage", "Steve Biko", "charleston", "peacock", "rita hayworth", "Miss Trunchbull", "imola", "albania", "antelope", "anything and everything", "boreas", "Ivan Basso", "bullfighting", "one", "Playboy", "bulgaria", "Peter Ackroyd", "albion", "drogba", "Aristotle", "brian adams", "death penalty", "Danny Alexander", "14", "Bangladesh", "adonis", "toea", "Lady Gaga", "SUNSET BOULEVARD", "raging bull", "charleston", "bologna", "All Things Must Pass", "maggie", "tet", "Arabah", "d\u00e9j\u00e0-vu", "henry patrick", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "September 2, 1945", "special guest performers Beyonc\u00e9 and Bruno Mars", "Greg Gorman and Helmut Newton", "American jewelry designer", "Isabella II", "Mexico", "Michael Partain,", "YFZ ranch,", "Frdric Chopin", "Indiana Jones", "Jakarta", "The Cosmopolitan"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6098958333333333}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2664", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-2433", "mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-1367", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-1664", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-5020", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6011", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3013", "mrqa_naturalquestions-validation-5396", "mrqa_hotpotqa-validation-4838", "mrqa_newsqa-validation-784", "mrqa_searchqa-validation-5866", "mrqa_hotpotqa-validation-668"], "SR": 0.5625, "CSR": 0.5457454819277108, "EFR": 0.9642857142857143, "Overall": 0.718724989242685}, {"timecode": 83, "before_eval_results": {"predictions": ["Johnny Depp", "The Green Arrow", "a parable", "what's in a name", "Spinal Tap", "Tennessee", "Detroit", "Day Off", "the United States", "pyramids", "Ruth Bader Ginsburg", "Article VII", "the sense of touch", "the Old Fashioned", "the Osmonds", "Bonnie and Clyde", "an arthropod", "College of William and Mary", "a chimpanzee", "reservations", "John Updike", "the Ganges", "vision", "\" Bright Lights\"", "his involvement in the 2007 \"D.C. Madam\" scandal", "coelacanth", "the Castle of Otranto", "Cheers", "Heidi", "Crosby, Stills, Nash & Young", "Matt Leinart", "a blood type", "charlie Stuart", "an albatross", "the Falkland Islands", "a taro", "a quip", "a lighthouse", "a rainbow", "Dan Rather", "the paper", "Buffalo Bill", "a", "a pig", "Harvard", "neurons", "the girl", "a little learning", "a dog", "a dragonfly", "Bill Cosby", "May 19, 2017", "Bachendri Pal", "James Corden", "witsunday", "humble pie", "mulodrama", "City and County of Honolulu", "Australian coast", "1992", "criticized his father's parenting skills.", "Steven Chu", "top designers, such as Stella McCartney,", "killing"], "metric_results": {"EM": 0.5, "QA-F1": 0.6156994047619048}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 0.5, 1.0, 1.0, 0.8, 0.0, 0.5714285714285715, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-13516", "mrqa_searchqa-validation-4520", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-13936", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-4318", "mrqa_searchqa-validation-12971", "mrqa_searchqa-validation-2790", "mrqa_searchqa-validation-15868", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-6064", "mrqa_searchqa-validation-6019", "mrqa_searchqa-validation-6498", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-11967", "mrqa_searchqa-validation-5868", "mrqa_searchqa-validation-2457", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-7434", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-6440", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4821", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-4", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3660"], "SR": 0.5, "CSR": 0.5452008928571428, "EFR": 1.0, "Overall": 0.7257589285714285}, {"timecode": 84, "before_eval_results": {"predictions": ["1970s", "Vancouver", "1930s", "Lenny Jacobson", "the status line", "the league's most common source of player recruitment", "paved the way for integration and was a major victory of the Civil Rights Movement, and a model for many future impact litigation cases", "1991", "biscuit - sized", "687 ( Earth ) days", "a jazz funeral without a body", "Palm Sunday", "Castleford", "note number 60", "L.K. Advani, an Indian politician who served as the Deputy Prime Minister of India from 2002 to 2004, and was the Leader of the Opposition in the 15th Lok Sabha", "winter", "his guilt in killing the bird", "Robber Barons", "2001", "Spencer Treat Clark", "marks the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere )", "2004", "Renhe Sports Management Ltd", "Americans who served in the armed forces and as civilians", "Michael Crawford", "200 to 500 mg up to 7 ml", "gastrocnemius muscle", "is a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Peter Cetera", "Austin", "1945", "Pebble Beach", "New Delhi", "midpiece", "Burj Khalifa", "Pangaea or Pangea", "mitochondrial membrane in eukaryotes or the plasma membrane in bacteria", "Johnny Cash", "Andrew Lincoln", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "Kevin Spacey", "Human anatomy", "Natural - language processing ( NLP )", "10 years", "2026", "eleven", "Despacito", "After World War I", "Fred E. Ahlert", "Joanna Moskawa", "1962", "Loch Ness", "play style", "griffin", "Mick Jackson", "Queenston Delta", "15", "Michelle Obama", "Consumer Product Safety Commission", "would have significant public health experience and understand how these processes work, how meat enters the chain of commerce,\"", "a trailgator bars", "Stones from the River", "Dwight D. Eisenhower", "Joel \"Taz\" DiGregorio,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5539421644391218}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false, true, false, false, true, false, true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.1142857142857143, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8181818181818181, 1.0, 0.2, 1.0, 0.4444444444444445, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.5, 0.6153846153846153, 1.0, 0.0, 0.35294117647058826, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.06896551724137931, 0.0, 0.0, 0.5, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-960", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-9559", "mrqa_triviaqa-validation-2065", "mrqa_triviaqa-validation-3036", "mrqa_hotpotqa-validation-4692", "mrqa_newsqa-validation-1826", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-4132", "mrqa_newsqa-validation-3992"], "SR": 0.4375, "CSR": 0.5439338235294118, "retrieved_ids": ["mrqa_squad-train-23439", "mrqa_squad-train-49648", "mrqa_squad-train-6522", "mrqa_squad-train-1429", "mrqa_squad-train-62427", "mrqa_squad-train-31770", "mrqa_squad-train-31431", "mrqa_squad-train-46119", "mrqa_squad-train-81385", "mrqa_squad-train-4530", "mrqa_squad-train-26588", "mrqa_squad-train-53880", "mrqa_squad-train-85910", "mrqa_squad-train-62193", "mrqa_squad-train-82567", "mrqa_squad-train-35627", "mrqa_searchqa-validation-9928", "mrqa_newsqa-validation-1829", "mrqa_squad-validation-4206", "mrqa_newsqa-validation-349", "mrqa_searchqa-validation-11471", "mrqa_newsqa-validation-2521", "mrqa_naturalquestions-validation-10066", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-1968", "mrqa_triviaqa-validation-305", "mrqa_naturalquestions-validation-392", "mrqa_hotpotqa-validation-671", "mrqa_newsqa-validation-276", "mrqa_searchqa-validation-3122", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-2400"], "EFR": 0.9444444444444444, "Overall": 0.7143944035947712}, {"timecode": 85, "before_eval_results": {"predictions": ["Rolex", "Vincent Motorcycle Company", "cycling", "the Ganges", "gerry adams", "aniline purple", "Roy Rogers", "Steve Jobs", "evan Rachel Wood", "Nirvana", "Donna Summer", "the heel", "geese", "a special messenger of Jesus Christ", "Sheryl Crow", "lacey", "the largest two digit number", "Franklin Delano Roosevelt", "neurons", "seven of One", "Yoshi", "Swordfish", "eardrum", "George Best", "faggots", "11", "jennifer brown", "Australia and England", "pascal", "British Airways", "five", "Challenger", "The World is Not Enough", "Giglio Island", "Vienna", "glee", "kennie birtwell", "iron", "jordan", "bayern munich", "Amanda Richards", "Italy", "El Paso", "May Day", "a Tam o\u2019Shanter hat", "Madagascar", "burgundy", "\"I wish everyone, friend or foe, well.", "kolkata", "strictly come dancing", "davie bieber", "Candace", "Forbes Burnham", "2007", "Dra\u017een Petrovi\u0107", "Costa del Sol", "early Romantic period", "first grand Slam,", "propofol,", "going out of business for one reason or another,", "Treaty of Versailles", "Zinedine Zidane", "a killer whale", "a newt"], "metric_results": {"EM": 0.5, "QA-F1": 0.5576636904761905}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3601", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-2920", "mrqa_triviaqa-validation-2336", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-2477", "mrqa_triviaqa-validation-490", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-3610", "mrqa_triviaqa-validation-2517", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-1995", "mrqa_naturalquestions-validation-6711", "mrqa_hotpotqa-validation-1634", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1946", "mrqa_searchqa-validation-4261", "mrqa_searchqa-validation-350"], "SR": 0.5, "CSR": 0.5434229651162791, "EFR": 1.0, "Overall": 0.7254033430232558}, {"timecode": 86, "before_eval_results": {"predictions": ["Switzerland", "John Monash", "tempo", "photographs, film and television", "Arthur Freed", "alt-right", "the Runaways", "\"50 best cities to live in.\"", "La Liga", "first season", "June 13, 1960", "Isfahan, Iran", "ribosomes", "death", "London", "SBS", "quantum mechanics", "king Duncan", "February 12, 2014", "Forbes", "Anne and Georges", "David Villa", "Double Agent", "Super Bowl XXIX", "White Horse", "Diamond Rio", "Quentin Coldwater", "Andrew Johnson", "The Social Network", "Martha Wainwright", "Leafcutter John", "moth", "Final Fantasy Tactics", "Jim Thorpe", "De La Soul", "unapologetic", "Shropshire Union Canal", "1670", "A skerry", "Oliver Parker", "The Strain", "Kalokuokamaile", "Pac-12", "Roots: The Saga of an American Family", "five", "William Scott Elam", "The Jeffersons", "The Rakes", "prevent the opposing team from scoring goals", "Cody Miller", "1907", "The Maidstone Studios in Maidstone, Kent", "strings of eight bits ( known as bytes )", "The Witch and the Hundred Knight 2", "nathan leopold", "Elvis Presley", "george Carey", "Amanda Knox's aunt Janet Huff", "Number Ones", "Jeddah, Saudi Arabia,", "Charlotte White", "Andrew Jackson", "Madison", "Willa Cather"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6951028138528138}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, false, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-4774", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-4109", "mrqa_hotpotqa-validation-1656", "mrqa_hotpotqa-validation-2060", "mrqa_hotpotqa-validation-3788", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-5287", "mrqa_triviaqa-validation-5380", "mrqa_newsqa-validation-3212", "mrqa_newsqa-validation-2558", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-1530"], "SR": 0.640625, "CSR": 0.5445402298850575, "EFR": 1.0, "Overall": 0.7256267959770114}, {"timecode": 87, "before_eval_results": {"predictions": ["Edward R. Murrow", "Vision of the Future", "1754", "May 10, 1976", "Hamlet", "Marty Ingels", "Milwaukee Bucks", "McLaren-Honda", "Ferengi bartender Quark", "The Spiderwick Chronicles", "a American reality documentary television series", "Cylon Number Six", "Qualcomm", "water sprite", "10-metre platform event", "Cincinnati Bengals", "on the shore", "\"Nanny McPhee\", \"The Legend of Tarzan\"", "November 15, 1903", "Bury St Edmunds,", "Rothschild banking dynasty", "Mr. Church", "\"Bigger Than Both of Us\"", "Thomas Christopher Ince", "'Drago' Sell,", "public house", "Los Angeles", "\"Me and You and everyone We Know\"", "Prussian Lithuanian poet and philosopher Vyd\u016bnas", "al-Qaeda", "the Darling River", "Baldwin", "2 April 1977", "House of Commons", "William Finn", "\"Love Letter\"", "Indian", "Type 212", "Barnoldswick", "the late 12th Century", "Bob Gibson", "The S7 series", "729", "tenure", "Frederick Alexander", "Robert Jenrick", "a field in Somerset County, Pennsylvania", "Salford, Lancashire", "Conservative", "The Division of Cook", "\"Peshwa\" (Prime Minister)", "Prafulla Chandra Ghosh", "in the eye", "Confederate forces attacked Fort Sumter in South Carolina, shortly after U.S. President Abraham Lincoln was inaugurated", "Samoa", "delorean dMC-12", "comets", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "Michael Arrington,", "laid 11 healthy eggs and, this week, all 11 of them hatched -- the last one on Wednesday.", "a sled", "a comets", "porcelain", "blood flow to those organs involved in intense physical activity"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6432043650793651}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, false, false, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.4, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.6, 0.0, 0.07142857142857144, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-908", "mrqa_hotpotqa-validation-958", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-870", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-4163", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6888", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-6935", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-5276", "mrqa_searchqa-validation-10831", "mrqa_naturalquestions-validation-836"], "SR": 0.546875, "CSR": 0.5445667613636364, "retrieved_ids": ["mrqa_squad-train-47439", "mrqa_squad-train-42991", "mrqa_squad-train-15113", "mrqa_squad-train-64060", "mrqa_squad-train-71493", "mrqa_squad-train-25478", "mrqa_squad-train-58378", "mrqa_squad-train-37678", "mrqa_squad-train-34094", "mrqa_squad-train-17640", "mrqa_squad-train-64995", "mrqa_squad-train-7821", "mrqa_squad-train-3442", "mrqa_squad-train-47856", "mrqa_squad-train-76781", "mrqa_squad-train-29015", "mrqa_squad-validation-3207", "mrqa_hotpotqa-validation-1544", "mrqa_searchqa-validation-4739", "mrqa_naturalquestions-validation-2837", "mrqa_triviaqa-validation-5484", "mrqa_newsqa-validation-3660", "mrqa_triviaqa-validation-4046", "mrqa_hotpotqa-validation-1893", "mrqa_searchqa-validation-1236", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-6435", "mrqa_naturalquestions-validation-870", "mrqa_newsqa-validation-220", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-861", "mrqa_naturalquestions-validation-4359"], "EFR": 1.0, "Overall": 0.7256321022727272}, {"timecode": 88, "before_eval_results": {"predictions": ["Sarah Palin", "la M\u00f4me Piaf", "Ulysses S. Grant", "Apollo", "Richard Wagner", "Atticus Finch", "Peter Principle", "copper and zinc", "hammertone", "Dunfermline", "european bison", "Edmund Cartwright", "Mary Poppins", "leicestershire", "black Wednesday", "Samoa", "John Gorman", "The Daily Mirror", "copper", "Mars", "Poland", "Dee Caffari", "a great invetor", "Belize", "Humphrey Lyttelton", "wilterley", "prawns", "James Hogg", "mORPG", "Fermanagh", "Colombia", "Kevin Painter", "llanberis", "Anne of Cleves", "Muhammad Ali", "Carmen Miranda", "Mishal Husain", "mary bennet", "August 10, 1960", "Estonia", "Sarajevo", "gluten", "entirely enclosed", "ransome", "muthia muralitharan", "Ridley Scott", "four", "Simpsons", "mike", "63 to 144 inches", "1925", "September 29, 2017", "Chance's game - legged deputy", "from 13 to 22 June 2012", "Cuban-American Major League Clubs Series", "2015", "Spanish", "Lashkar-e-Tayyiba", "the surge,", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "devil's food cake", "Michelangelo", "Missouri", "George Jetson"], "metric_results": {"EM": 0.625, "QA-F1": 0.6909090909090909}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.9090909090909091, 0.9090909090909091, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-176", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-500", "mrqa_triviaqa-validation-5159", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-3426", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-2876", "mrqa_triviaqa-validation-5205", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-602", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-3589", "mrqa_hotpotqa-validation-3114", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-161"], "SR": 0.625, "CSR": 0.5454705056179776, "EFR": 0.9166666666666666, "Overall": 0.7091461844569288}, {"timecode": 89, "before_eval_results": {"predictions": ["1", "graphical", "Scottie Pippen", "Vaseline jelly", "savings", "silver", "Gone with the Wind", "Large", "Nelly Dr. Seuss", "gladiators", "Nemo", "the hyoid horns", "The Kite Runner", "shark", "nairobi", "Oprah Winfrey", "Dixie Chicks", "apple pie", "Sonoma", "Best Buy", "the Mediterranean", "Pope John Paul II", "Lobster Newburg", "Yemen", "David Geffen", "chariots", "Neruda", "The due process clause of the Fifth Amendment", "a mite", "Saturn", "Nanny Diaries", "liquid crystals", "Robert Frost", "a dictum", "nutella Cheesecake", "Crete", "Father Brown", "reuben", "The Outsiders", "waltz", "Belch", "Jane Austen", "Wisconsin", "Charles Darnay", "Q", "Harry Met Sally", "mexican", "pumice", "John Molson", "Jan and Dean", "American physician and novelist", "Janis Joplin", "all transmissions", "brothers Norris and Ross McWhirter", "andorra", "mike faraday", "g Gerald R. Ford", "1992", "The King of Chutzpah", "Niger\u2013Congo", "upper respiratory infection,", "Fernando Gonzalez", "At least 14", "more than two years,"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7633928571428572}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-8406", "mrqa_searchqa-validation-2321", "mrqa_searchqa-validation-5546", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-4328", "mrqa_searchqa-validation-14490", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-13703", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-12573", "mrqa_searchqa-validation-12893", "mrqa_searchqa-validation-2779", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-550", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-4720", "mrqa_triviaqa-validation-7721", "mrqa_triviaqa-validation-1115", "mrqa_newsqa-validation-795"], "SR": 0.65625, "CSR": 0.5467013888888889, "EFR": 1.0, "Overall": 0.7260590277777778}, {"timecode": 90, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6332", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.83984375, "KG": 0.478125, "before_eval_results": {"predictions": ["the Harpe brothers", "Mississippi", "\"Loch Lomond\"", "American reality television series", "Gweilo or gwailou", "Arthur & Mike", "The Ninth Gate", "James G. Kiernan", "emperor", "Erreway", "Protestant Christian", "\u00c6thelred I", "Bellagio and The Mirage", "Los Angeles Dance Theater", "johnnie bennett", "Hampton University", "the special episode of the American animated television series \"SpongeBob SquarePants\"", "Jenji Kohan", "1", "God Save the Queen", "Hibernian", "Oklahoma City", "Being John Malkovich", "Randall Boggs", "October 22, 2012", "Hard rock", "Prince Louis of Battenberg", "\"Slaughterhouse-Five\"", "Harry F. Sinclair", "Ghana Technology University College", "Bigfoot", "Cyclic Defrost", "England, Scotland, and Ireland", "Coal Miner's daughter", "Worcester County", "1972", "Ang Lee", "Brad Silberling", "Blue (Da Ba Dee)", "mid-ninth-century Viking chieftain", "La Scala, Milan", "Orson Welles", "1973", "Scott Mechlowicz", "Ryan Babel", "Melbourne's City Centre", "Lincoln Riley", "the world", "Enigma", "University of Nevada, Reno", "largest Mission Revival Style building in the United States", "Muhammad", "18", "Harlem River", "Turkish Empire", "$1", "sulfur dioxide", "1913.", "Juan Martin Del Potro.", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "J.R. R. Tolkien", "Jaguar", "wheat smut", "semi-autonomous organisational units within the National Health Service in England"], "metric_results": {"EM": 0.640625, "QA-F1": 0.722953869047619}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.25, 0.4, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-1189", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-2635", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2708", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-1471", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-9281", "mrqa_searchqa-validation-5567"], "SR": 0.640625, "CSR": 0.5477335164835164, "retrieved_ids": ["mrqa_squad-train-86302", "mrqa_squad-train-57374", "mrqa_squad-train-70160", "mrqa_squad-train-69919", "mrqa_squad-train-63205", "mrqa_squad-train-15243", "mrqa_squad-train-5566", "mrqa_squad-train-60683", "mrqa_squad-train-61912", "mrqa_squad-train-15628", "mrqa_squad-train-2174", "mrqa_squad-train-17418", "mrqa_squad-train-61889", "mrqa_squad-train-52752", "mrqa_squad-train-35618", "mrqa_squad-train-27244", "mrqa_newsqa-validation-1386", "mrqa_hotpotqa-validation-5489", "mrqa_searchqa-validation-5920", "mrqa_newsqa-validation-2953", "mrqa_naturalquestions-validation-10118", "mrqa_squad-validation-5388", "mrqa_hotpotqa-validation-4899", "mrqa_searchqa-validation-929", "mrqa_naturalquestions-validation-7714", "mrqa_triviaqa-validation-5439", "mrqa_hotpotqa-validation-4002", "mrqa_squad-validation-10466", "mrqa_searchqa-validation-14159", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-1357", "mrqa_triviaqa-validation-2821"], "EFR": 0.9565217391304348, "Overall": 0.7132729261227901}, {"timecode": 91, "before_eval_results": {"predictions": ["Terry Reid", "Friedman Billings Ramsey", "Robber Barons", "Nicky Slater", "the manifestation of God's presence as perceived by humans according to the Abrahamic religions", "LED illuminated display", "Bart Howard", "transmission", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "abundant in the brain, muscles, and liver", "USS Chesapeake", "1977", "official residence of the President of the Russian Federation", "Charles Darwin and Alfred Russel Wallace", "marks locations in Google Maps", "Richard Stallman", "January 2004", "1940", "an armed conflict without the consent of the U.S. Congress", "when an individual noticing that the person in the photograph is attractive, well groomed, and properly attired,", "heat", "Spain", "peptide bonds", "Kansas City Chiefs", "used obscure languages as a means of secret communication during wartime", "Zhu Yuanzhang", "The 1980 Summer Olympics", "Heather Stebbins", "the central branch goes to the posterior ( dorsal ) horn of the spinal cord", "drizzle, rain, sleet, snow, graupel and hail", "Karen Gillan", "2017", "Julie Adams", "1881", "Music producer Mike Higham", "a proverbial phrase referring to one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "550 quadrillion Imperial gallons", "A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "Theodore Roosevelt, Robert M. La Follette, Sr., and Charles Evans Hughes on the Republican side", "August 5, 1937", "an outsider could be spotted", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings,", "Payson, Lauren, and Kaylie", "2015, 2016", "Dr. Lexie Grey", "February 27, 2007", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "Taron Egerton", "2017", "smen", "T'Pau", "Fort Nelson near Portsmouth", "board games", "Sparta", "World Famous Gold & Silver Pawn Shop", "Darkroom", "Louis \"Louie\" Zamperini", "Former Mobile County Circuit Judge Herman Thomas", "\"take him down\" at night and \"bring him back up\"", "\"How did this brand of box get on my back balcony?\"", "Monroe", "Babel", "Wikipdia", "Juan Ponce de Len"], "metric_results": {"EM": 0.375, "QA-F1": 0.5511637130237267}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.2857142857142857, 1.0, 0.0, 0.3076923076923077, 0.2857142857142857, 1.0, 0.0, 0.18181818181818182, 0.5, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.6086956521739131, 0.5833333333333334, 0.0, 1.0, 0.5, 1.0, 0.14814814814814817, 1.0, 1.0, 0.0, 0.15384615384615383, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4827586206896552, 0.5, 1.0, 0.7000000000000001, 1.0, 0.0, 0.375, 0.4, 0.6666666666666666, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.4, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.0, 0.10526315789473685, 0.0, 1.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-1101", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-5579"], "SR": 0.375, "CSR": 0.5458559782608696, "EFR": 0.8, "Overall": 0.681593070652174}, {"timecode": 92, "before_eval_results": {"predictions": ["beer", "beetle", "the Saskatchewan River", "Carlisle", "email", "Tahrir Square", "David Frost", "newbury", "torture", "Knutsford", "Portugal", "Spongebob", "make Me an offer", "China", "Maine", "Edward VI", "president of the United States", "the Caribbean Sea", "jack Sprat", "Ronnie Kray", "conclave", "Dublin", "The Mayor of Casterbridge", "foot", "Amsterdam", "John Lennon", "Lusitania", "queen anne boleyn", "Australia", "antelope", "Portugal", "bechuanaland", "Philippines", "blood", "Spain", "Marilyn Monroe", "Jupiter Mining Corporation", "dry rot", "isambard Kingdom Brunel", "Canada", "a Bristol Box Kite", "Jinnah International Airport", "India", "king anne", "peter paul Rubens", "John Ford", "six", "Mendip Hills", "Burma", "Charles Taylor", "Pancho Villa", "for the purpose of changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "Jerry Leiber and Mike Stoller", "the third season", "Karl Johan Schuster", "Worcester County", "Brown Mountain Overlook", "Lucky Dube,", "social media sites,", "Marines and their families", "Beauty and the Beast", "Luxembourg", "Hammurabi", "prefrontal leukotomy"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6376225490196079}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, true, true, true, false], "QA-F1": [0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7666", "mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-3295", "mrqa_triviaqa-validation-729", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-5156", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-3217", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-7264", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-4122", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-1105", "mrqa_searchqa-validation-16895"], "SR": 0.578125, "CSR": 0.5462029569892473, "EFR": 0.9629629629629629, "Overall": 0.7142550589904421}, {"timecode": 93, "before_eval_results": {"predictions": ["American", "keeper of the Great Seal of Scotland", "1776", "Meghan Markle", "U.S. Bancorp", "Justin Adler", "BBC Formula One coverage on TV, radio and online", "Coahuila, Mexico", "Atomic Kitten", "methylenedioxy meth", "Colin Vaines", "California", "racehorse breeder", "Jim Kelly", "Australian", "the D\u00e2mbovi\u021ba River", "explores the lives of those that either own exotic animals or have been captured for illegally smuggling them", "Miracle", "Erich Maria Remarque", "Scott Mosier", "Georgia Southern University", "Dutch", "1999", "Mudvayne", "1947", "Easter Rising of 1916", "January 24, 2012", "General Sir John Monash", "\u00c6thelstan", "Middlesbrough Football Club", "video clip", "5,112 feet", "Jefferson Memorial", "May 1, 2011", "four", "Red and Assiniboine Rivers", "200", "15 mi", "February 18, 1965", "AC/DC founders Angus Young and Malcolm Young", "Goddess of Pop", "Flyweight", "chocolate-colored", "1966", "March 14, 2000", "1927", "Gregg Popovich", "Princess Anne", "Neighbours", "Hall & Oates", "January 16, 2013", "northwest Washington", "1830", "Lake Powell", "gulls", "chariots", "Louisiana", "a \"stressed and tired force\" made vulnerable by multiple deployments,", "Egyptians", "Tuesday", "The African Queen", "cats", "Gibraltar", "Pure water is neutral, at pH 7 ( 25 \u00b0 C ), being neither an acid nor a base"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6728422619047619}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.8333333333333334, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.09523809523809523, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3152", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-1077", "mrqa_triviaqa-validation-1582", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2066", "mrqa_searchqa-validation-7854", "mrqa_naturalquestions-validation-8652"], "SR": 0.5625, "CSR": 0.546376329787234, "retrieved_ids": ["mrqa_squad-train-26364", "mrqa_squad-train-47583", "mrqa_squad-train-7536", "mrqa_squad-train-81998", "mrqa_squad-train-57597", "mrqa_squad-train-8240", "mrqa_squad-train-19725", "mrqa_squad-train-3510", "mrqa_squad-train-46917", "mrqa_squad-train-83402", "mrqa_squad-train-68796", "mrqa_squad-train-14692", "mrqa_squad-train-84584", "mrqa_squad-train-6677", "mrqa_squad-train-46281", "mrqa_squad-train-79842", "mrqa_triviaqa-validation-682", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-10906", "mrqa_triviaqa-validation-6212", "mrqa_naturalquestions-validation-2379", "mrqa_triviaqa-validation-5394", "mrqa_hotpotqa-validation-3075", "mrqa_naturalquestions-validation-9818", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-6210", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-4098", "mrqa_triviaqa-validation-7461", "mrqa_hotpotqa-validation-4927", "mrqa_searchqa-validation-7109", "mrqa_naturalquestions-validation-9878"], "EFR": 1.0, "Overall": 0.7216971409574467}, {"timecode": 94, "before_eval_results": {"predictions": ["Villa Park", "Guinea", "Mayflower", "four", "Daily Mail Online", "the tartan", "toy story", "GM Korea", "lungs", "Periodic Table", "Left Book Club", "Chile", "st Columba", "Donald Sutherland", "New York City", "nekemte", "Cardiff", "sternum", "pressure", "James Murdoch", "Chicago", "Fluids", "Ambroz Bajec-Lapajne", "Squeeze", "altamont speedway free", "Robert Plant", "Jerry Seinfeld", "stern tube", "kia", "lemurs", "Sir Robert Walpole", "eight", "Andorra", "a braffin", "best", "kunsky", "best paul's", "27", "Formula One", "squash", "Mary Decker", "karakorams", "France", "Birdman of Alcatraz", "Bernardo Bertolucci", "Christopher Columbus", "the buck", "lady Godiva", "Britannia", "feet", "a skirt", "1940s", "0.30 in", "in the absence of a catalyst", "Neymar", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "5.3 million", "6-4", "al Qaeda,", "UNICEF", "a B movie", "The Lady of the Lamp", "Saturn", "a global village"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6878720238095237}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, true, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5663", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-1469", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-3908", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7226", "mrqa_newsqa-validation-3796", "mrqa_searchqa-validation-9012", "mrqa_searchqa-validation-11091"], "SR": 0.65625, "CSR": 0.5475328947368421, "EFR": 1.0, "Overall": 0.7219284539473685}, {"timecode": 95, "before_eval_results": {"predictions": ["war drama", "its air-cushioned sole", "local South Australian and Australian produced content", "Gal\u00e1pagos", "Eric Whitacre", "2010", "Statutory List of Buildings of Special Architectural or Historic Interest", "pubs, bars and restaurants", "2004", "Hobart", "Jim Kelly", "Stern-Plaza", "Edward James Olmos", "Girls' Generation", "June 12, 2017", "two or three", "Derrick Coleman", "Prussia", "David Wells", "just downstream", "two", "Argentine", "13th century", "Pru Goward", "Manchester United", "Matt Groening", "Hazel Keech", "Minami-Tori-shima", "1993", "Jesus", "Sulla", "Riot Act", "Larry Gatlin & the Gatlin Brothers", "right-hand batsman", "black nationalism", "David X. Cohen", "Bayern Munich", "Deftones", "Gangsta's Paradise", "Clitheroe Football Club", "Green Lantern", "The Birds", "The Fault in Our Stars", "Liesl", "rudolph", "twin-faced sheepskin with fleece on the inside", "White Horse", "Albert Bridge", "Yellow fever", "Elise Marie Stefanik", "Francis Schaeffer", "northeast coast of Australia", "between 3.9 and 5.5 braces / L ( 70 to 100 mg / dL )", "the heads of federal executive departments", "david stockwell", "king arthur", "cold comfort farm", "red", "lightning strikes", "murders of his father and brother", "the Isle of Man", "the Southern Christian Leadership Conference", "capital of Prussia", "the brain and spinal cord"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5865885416666667}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, false, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 0.375, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.4, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.2857142857142857, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.8571428571428572, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-2662", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-4302", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-1250", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-1745", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-8982", "mrqa_triviaqa-validation-4415", "mrqa_triviaqa-validation-4334", "mrqa_newsqa-validation-2382", "mrqa_searchqa-validation-1780", "mrqa_searchqa-validation-14797", "mrqa_naturalquestions-validation-7342"], "SR": 0.453125, "CSR": 0.5465494791666667, "EFR": 0.9714285714285714, "Overall": 0.7160174851190476}, {"timecode": 96, "before_eval_results": {"predictions": ["12951 / 52 Mumbai Rajdhani Express", "year of the conception or birth of Jesus of Nazareth", "1987", "360", "Pradyumna", "Carol Ann Susi", "the pyloric valve", "Karen O'Leary", "Ephesus", "Christian recording artist Michael English", "Phillip Paley", "Germany", "Einstein", "1830", "positions 14 - 15, 146 - 147 and 148 - 149", "100", "James Madison", "Woodrow Strode", "Baaghi", "Taylor Michel Momsen", "Panning", "31 March 1909", "$66.5 million", "pathology", "April 3, 1973", "keeping water in the body and keeping other harmful chemicals and pathogens out", "her abusive husband", "United Nations", "people who jointly oversee the activities of an organization", "pigs", "A standard form contract", "1595", "The musical", "American country music duo Brooks & Dunn", "first to be built in the complex in over seven years", "1,228 km / h ( 763 mph )", "October 27, 2017", "Kida", "~ 55 - 75 micrometers", "Miller Lite", "Oona Castilla Chaplin", "The speech compares the world to a stage and life to a play, and catalogues the seven stages of a man's life, sometimes referred to as the seven ages of man", "Lulu", "the NFL", "Steve Russell", "the national flag of the United States", "Profit maximization", "Melbourne", "April 1, 2016", "the Alamodome", "1,281,900", "Michael Phelps", "royal oak", "The Krankies", "Dutch", "Province of Syracuse", "June 11, 1986", "1-0", "200", "CNN's Larry King", "reshit", "Deere", "gusts", "curfew"], "metric_results": {"EM": 0.6875, "QA-F1": 0.765671178704722}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8235294117647058, 1.0, 0.046511627906976744, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06451612903225806, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-4953", "mrqa_hotpotqa-validation-3107", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2327", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-12334"], "SR": 0.6875, "CSR": 0.5480025773195876, "retrieved_ids": ["mrqa_squad-train-31483", "mrqa_squad-train-81913", "mrqa_squad-train-29098", "mrqa_squad-train-31572", "mrqa_squad-train-72301", "mrqa_squad-train-16770", "mrqa_squad-train-17927", "mrqa_squad-train-50705", "mrqa_squad-train-83595", "mrqa_squad-train-23532", "mrqa_squad-train-48980", "mrqa_squad-train-16835", "mrqa_squad-train-17994", "mrqa_squad-train-73374", "mrqa_squad-train-15974", "mrqa_squad-train-53807", "mrqa_searchqa-validation-10027", "mrqa_hotpotqa-validation-3926", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-8578", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-134", "mrqa_searchqa-validation-5752", "mrqa_squad-validation-4264", "mrqa_triviaqa-validation-663", "mrqa_naturalquestions-validation-56", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-832", "mrqa_triviaqa-validation-4232", "mrqa_hotpotqa-validation-3975", "mrqa_searchqa-validation-7854", "mrqa_hotpotqa-validation-4123"], "EFR": 0.9, "Overall": 0.7020223904639175}, {"timecode": 97, "before_eval_results": {"predictions": ["Tchaikovsky", "dark places", "the Konabar", "Boll Weevil", "touchpad", "Wikipedia", "Sundance", "Buddhism", "Mozart", "Jonathan Swift", "Tiger lily", "ice cream", "Algeria", "Edgar Allan Poe", "(Sergey) Brin", "Democrats", "American alternative rock band", "bread", "Yale", "Napoleon", "Paris", "the Black Forest", "the Vatican", "a bivouac", "birkenstock", "Firebird", "Hafnium", "flax", "the Muse", "the Wachowski brothers", "Rumpole for the Prosecution", "the 2000 presidential election", "Steve Austin", "Kurt Warner", "40", "a small retail store", "Beauty and the Beast", "Ratatouille", "pro bono", "a brown bear", "The Office", "The Oprah Show", "Bigfoot", "Jackson Pollock", "glow", "Mona Lisa", "Vietnamese", "Crayola", "The Man in the Gray Flannel Suit", "a process of demonstrating that an invention works correctly for its desired", "orange", "Isaiah Amir Mustafa", "1999", "Americans acting under orders", "mike hammer", "The Crow", "Hartley", "Tifinagh", "European Champion Clubs' Cup", "second largest", "North Korea", "Al Alcohol", "antispasmodic drugs", "AMC"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6182598039215687}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.23529411764705882, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6269", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-3811", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-10747", "mrqa_searchqa-validation-12166", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-10545", "mrqa_searchqa-validation-9435", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-7743", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-13416", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-4924", "mrqa_searchqa-validation-10403", "mrqa_naturalquestions-validation-8759", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-3553", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-1760", "mrqa_newsqa-validation-96", "mrqa_hotpotqa-validation-2138"], "SR": 0.546875, "CSR": 0.5479910714285714, "EFR": 1.0, "Overall": 0.7220200892857143}, {"timecode": 98, "before_eval_results": {"predictions": ["Florence", "Pierre Trudeau", "Red", "a barge", "Billy Joel", "the corneal", "Crystal Light", "Rumpole", "the pastry", "the incandescent light bulb", "Spider-Man", "Atlanta", "China", "Dick Tracy", "Queen Latifah", "Van Allen", "beer", "Zen", "El", "Zenith", "baboon", "wine", "The Sopranos", "the q- tip", "natural selection", "Massachusetts", "Battle of the Bulge", "Shaft", "Poe", "the Two Sicilies", "Victory", "a constitution", "Sir Francis Drake", "Nisei", "Enrico Fermi", "Candy Crush", "the pituitary", "Alfred Hitchcock", "Hank Aaron", "Special Boat Teams", "Florida", "Ectoplasm", "Thomas Jefferson", "Mercury", "Dante", "Christopher Columbus", "(Jose) Haydn", "scotch", "Babe Zaharias", "the Nigeria police", "kidney stones", "four", "Charles Lyell", "961", "William the Silent", "the god Dionysus", "Mary Seacole", "Orchard Central", "Fort Hood, Texas", "OutKast", "the iPods", "suspend all", "Wednesday", "Nick Sager"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6208333333333333}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, false, false, true, false, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13080", "mrqa_searchqa-validation-2078", "mrqa_searchqa-validation-15786", "mrqa_searchqa-validation-6497", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-8851", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-16225", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-319", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-11670", "mrqa_searchqa-validation-15328", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-6444", "mrqa_searchqa-validation-5926", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-1848", "mrqa_searchqa-validation-11669", "mrqa_searchqa-validation-1729", "mrqa_searchqa-validation-9945", "mrqa_searchqa-validation-5756", "mrqa_naturalquestions-validation-307", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-7667", "mrqa_hotpotqa-validation-2679", "mrqa_newsqa-validation-2040"], "SR": 0.515625, "CSR": 0.5476641414141414, "EFR": 0.967741935483871, "Overall": 0.7155030903796025}, {"timecode": 99, "UKR": 0.779296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3334", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2971", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3627", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4609", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6821", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7786", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2729", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-749", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.853515625, "KG": 0.50078125, "before_eval_results": {"predictions": ["Niles", "Ben Rosenbaum", "July 14, 2017", "2020", "classical neurology", "Sir Edward Henry", "potential of hydrogen", "Peking", "Bart Howard", "2013 ( XLVIII )", "Ozzie Smith", "in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "the Beatles", "Persian", "Sarah Silverman", "Sophia Akuffo", "January 17, 1899", "IIII", "2014", "Natural - language processing", "six", "HTTP / 1.1", "$315,600", "three high fantasy adventure films directed by Peter Jackson", "Sohrai", "Jos Plateau", "Cecil Lockhart", "Michael Madhusudan Dutta", "257,083", "March 23, 2018", "starting quarterback", "public sector ( also called the state sector )", "Carpenter", "2018", "1992", "Dan Stevens", "`` Killer Within ''", "Disha Vakani", "Nickelback", "1999", "King Willem - Alexander", "his second studio album, Mama Said ( 1991 )", "Deuteronomy 5 : 4 -- 25", "a revolution or orbital revolution", "Ren\u00e9 Georges Hermann - Paul", "Horace Lawson Hunley", "bird nests created by edible - nest swiftlets using solidified saliva", "John Bull", "December 1, 1969", "2009", "Manley", "Charlie Chaplin", "Francis Matthews", "HYmenaeus", "2003", "1776", "Field Marshal Stapleton Cotton", "transit bombings", "eight-day", "101", "Spain", "(James) Michener", "Geneva", "27-year-old's"], "metric_results": {"EM": 0.625, "QA-F1": 0.6888032106782107}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-9962", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-3310", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-2953", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-2334", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-111", "mrqa_newsqa-validation-894", "mrqa_searchqa-validation-3524"], "SR": 0.625, "CSR": 0.5484375, "retrieved_ids": ["mrqa_squad-train-28645", "mrqa_squad-train-84656", "mrqa_squad-train-54141", "mrqa_squad-train-8287", "mrqa_squad-train-57637", "mrqa_squad-train-64320", "mrqa_squad-train-6851", "mrqa_squad-train-21866", "mrqa_squad-train-35964", "mrqa_squad-train-41621", "mrqa_squad-train-74117", "mrqa_squad-train-15635", "mrqa_squad-train-74110", "mrqa_squad-train-38182", "mrqa_squad-train-9385", "mrqa_squad-train-32891", "mrqa_naturalquestions-validation-2981", "mrqa_hotpotqa-validation-5117", "mrqa_triviaqa-validation-6615", "mrqa_searchqa-validation-5471", "mrqa_triviaqa-validation-6973", "mrqa_searchqa-validation-4038", "mrqa_hotpotqa-validation-4173", "mrqa_searchqa-validation-13235", "mrqa_triviaqa-validation-522", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-1792", "mrqa_triviaqa-validation-2821", "mrqa_triviaqa-validation-1686", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-7553", "mrqa_hotpotqa-validation-4628"], "EFR": 0.9166666666666666, "Overall": 0.7197395833333333}]}