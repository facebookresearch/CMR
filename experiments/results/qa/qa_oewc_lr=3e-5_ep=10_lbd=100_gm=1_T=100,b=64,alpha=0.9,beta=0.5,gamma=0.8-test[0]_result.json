{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=100_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[0]', diff_loss_weight=0, ewc_gamma=1.0, ewc_lambda=100.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=100_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4220, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["67.9", "Mike Carey", "rapidly raising population and traffic in cities along SR 99, as well as the desirability of Federal funding", "their greatest common divisor is one", "the Official Report", "immunoinformatics", "lupus erythematosus", "Kublai Khan", "New Testament", "1926", "other ctenophores", "60%", "he was illiterate in Czech", "architect or engineer", "British", "Gateshead Council", "Book of Genesis", "Shing-Tung Yau", "complexity classes", "Mexico", "cabinet", "after its 1977 merger with Radcliffe College", "The Master", "chastity", "Mark Woods", "one another", "100% oxygen", "Steam engines", "the wedding banquet", "the Ohio Company of Virginia", "CBS and NBC", "aircraft manufacturing", "a school or other place of formal education", "Times Square Studios", "Normans and Norman", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "decreases", "Duke Richard II of Normandy, and King Ethelred II of England", "Royal Shakespeare Company", "books and articles", "between 1835 and 1842", "Edmonton, Canada", "rises in sea levels", "it is neither zero nor a unit", "University of Chicago College Bowl Team", "algorithms have been written that solve the problem in reasonable times in most cases", "13.34%", "FMCG manufacturing, metal processing, steel and electronic manufacturing and food processing", "49\u201315", "The Mongols' extensive West Asian and European contacts", "east-west", "the kip", "Croatia", "railway locomotives", "the law is no longer to be taught to Christians but belonged only to city hall", "Josh Norman", "1964 and 1968", "expelled Jews", "Elton Rule", "gravel", "11:28", "Super Bowl XLVII", "a fee per unit of information transmitted", "seven"], "metric_results": {"EM": 0.875, "QA-F1": 0.9101325145442792}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3846153846153846, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.5, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1187", "mrqa_squad-validation-2853", "mrqa_squad-validation-6986", "mrqa_squad-validation-2704", "mrqa_squad-validation-1891", "mrqa_squad-validation-1161", "mrqa_squad-validation-1089", "mrqa_squad-validation-2473"], "SR": 0.875, "CSR": 0.875, "EFR": 1.0, "Overall": 0.9375}, {"timecode": 1, "before_eval_results": {"predictions": ["\"Fall of the Eleventh\"", "The input string", "European Parliament", "the Meuse", "overseas colonies", "Kings Canyon Avenue and Clovis Avenue", "entertainment", "Astra 2A", "poison", "atoon", "Orange", "the BBC", "Dolby Digital", "shocked", "Executive Vice President of Football Operations", "Thomas Edison", "was particularly forceful, stating that British colonists would not be safe as long as the French were present.", "least prejudiced", "inferior", "Johann Tetzel", "miniature cydippids", "main porch", "the courts of member states", "Shah's decision to divide his army into small groups concentrated in various cities", "1st century BC", "Iberia", "Silas B. Cobb", "Aristotle", "The Swahili", "the sheepshanks Gallery", "1968", "heavy/highway, heavy civil or heavy engineering", "type III secretion system", "Commission v Italy", "Gary Kubiak", "The European Court of Justice", "oxygen compounds", "1933", "Endosymbiotic gene transfer", "proplastids", "Triassic Period of the Mesozoic Era", "Conrad of Montferrat", "sacramental union", "reciprocating", "the Arabs and much of the rest of the Third World", "mineral deposits", "Grand Canal d'Alsace", "Vince Lombardi Trophy", "\u00dcberseering BV v Nordic Construction GmbH", "first 15 years", "Neoclassical economics", "Variable lymphocyte receptors", "Exploration", "Elder", "internal migration and urbanisation", "high risk preparations and some other compounding functions", "water level", "two", "James", "A driver's license is an official document permitting a specific individual to operate one or more types of motorized vehicles", "radioisotope thermoelectric generator", "Mickey Mantle", "A lymphocytes is one of the subtypes of white blood cell in a vertebrate's immune system", "Amsterdam"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7073051948051948}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7689", "mrqa_squad-validation-1767", "mrqa_squad-validation-3922", "mrqa_squad-validation-6029", "mrqa_squad-validation-2672", "mrqa_squad-validation-6211", "mrqa_squad-validation-1902", "mrqa_squad-validation-375", "mrqa_squad-validation-10186", "mrqa_squad-validation-6163", "mrqa_squad-validation-3511", "mrqa_squad-validation-8927", "mrqa_squad-validation-9325", "mrqa_squad-validation-3370", "mrqa_squad-validation-7635", "mrqa_squad-validation-2315", "mrqa_naturalquestions-validation-5378", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-9342", "mrqa_newsqa-validation-1945"], "SR": 0.671875, "CSR": 0.7734375, "EFR": 0.9523809523809523, "Overall": 0.8629092261904762}, {"timecode": 2, "before_eval_results": {"predictions": ["six quadrangles", "1883\u201384", "1870 to 1939", "2010", "unit-dose, or a single doses of medicine", "2003", "Budapest Telephone Exchange", "induction motor", "markets", "to avoid being targeted by the boycott", "socialist realism", "Ten", "city's tax base dissipated", "4000", "St. Bartholomew's Day massacre", "early twentieth century homes, many of which have been restored in recent decades", "Thesis 86", "five or more seats", "in a glass case", "1,320 kilometres (820 miles)", "force-free magnetic fields", "a Tatar chieftain, Tem\u00fcjin-\u00fcge", "The Three Doctors", "San Francisco Bay Area's Levi's Stadium", "1", "St. Lawrence and Mississippi watersheds", "2010", "cholera", "168,637", "four", "faith", "25", "river Deabolis", "ten", "to spearhead the regeneration of the North-East", "the late 1920s", "2007", "1936", "1968", "the European Parliament and the Council of the European Union", "typhus, smallpox and respiratory infections", "California", "Spanish", "muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins", "the sheepshanks Gallery", "Four thousand", "Prime ideals", "Stairs", "a lack of understanding of the legal ramifications, or due to a fear of seeming rude", "MHC class I molecules", "the Great Fire of London", "in his lab and elsewhere", "Rudy Clark", "Art Carney", "Ren\u00e9 Verdon", "honey bees may be the state's most valuable export", "July 4, 1776", "May 5, 1904", "The Lykan Hypersport is a Lebanese limited production supercar built by W Motors, a United Arab Emirates based company, founded in 2012 in Lebanon with the collaboration of Lebanese, French and Italian engineers", "E \u00d7 12, A \u00d7 9, I \u00d7 9, O \u00d7 8, N \u00d7 6, L \u00d7 4, S \u00d7 4", "the most recent technological change to the u.s. economy was?", "Trace Adkins", "he checked himself into a Los Angeles mental institution in an effort to kick the habit", "refugees agency said on Tuesday.Islamist fighters exchange gunfire with government forces in Mogadishu on July 3."], "metric_results": {"EM": 0.734375, "QA-F1": 0.8028958559230144}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.8333333333333334, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.12903225806451613, 0.0, 0.0, 1.0, 0.761904761904762, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9855", "mrqa_squad-validation-6526", "mrqa_squad-validation-1277", "mrqa_squad-validation-7246", "mrqa_squad-validation-5751", "mrqa_squad-validation-4669", "mrqa_squad-validation-133", "mrqa_squad-validation-4546", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-9392", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-3164"], "SR": 0.734375, "CSR": 0.7604166666666666, "EFR": 1.0, "Overall": 0.8802083333333333}, {"timecode": 3, "before_eval_results": {"predictions": ["the Hamiltonian path problem", "Thomas Edison", "systematic economic inequalities", "July 31, 1995", "vitamin D", "Thomas Edison", "1987", "Aboriginal", "Leukocytes", "research, exhibitions and other shows", "Erg\u00e4nzungsschulen", "along the coast", "a \"principal hostile country\"", "17", "The Book of Discipline", "67.9", "Turkey", "over 100%", "Saffir-Simpson Scale", "New Orleans", "Nearly 3,000", "Earth", "Bruno Mars", "Merwede", "Antigone", "Soviet Union", "the Dodge D-50", "over the age of 18", "various causes", "polynomial-time reduction", "the WMO", "several years", "Danny Trevathan", "Albert of Mainz", "patient care rounds drug product selection", "Thomas Coke", "Colony of Victoria Act 1855", "Anglo-Saxon populations", "John Debney", "Geordie", "the Ancient Greeks", "Budget cuts", "Systemic acquired resistance (SAR)", "one darkened lens; the picture would look normal to those viewers who watched without the glasses.", "the Legislative Assembly", "the desire to prevent things that are indisputably bad", "Any member", "Tyrion", "Manchuria", "Kenneth Cook ( 2018 )  Samuel F. Herd ( 1999 )   Geoffrey Jackson ( 2005)", "to capitalize on her publicity", "1937", "Wah - Wah ''", "James W. Marshall", "1979", "January 12, 2017", "a loop", "Kyrie Irving", "Charles Carson", "Morgan Freeman", "do n 't tell mom the babysitter's dead", "The Seven Cities of Cibola", "in the keyboard", "The fibula is a long, thin bone running parallel to the tibia"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7734221283420368}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, false], "QA-F1": [0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4210526315789473, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.4615384615384615, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.19999999999999998]}}, "before_error_ids": ["mrqa_squad-validation-1760", "mrqa_squad-validation-3770", "mrqa_squad-validation-100", "mrqa_squad-validation-9157", "mrqa_squad-validation-1436", "mrqa_squad-validation-1764", "mrqa_squad-validation-7713", "mrqa_squad-validation-7838", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-849", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-6338"], "SR": 0.734375, "CSR": 0.75390625, "EFR": 0.8823529411764706, "Overall": 0.8181295955882353}, {"timecode": 4, "before_eval_results": {"predictions": ["immune surveillance", "Arizona Cardinals", "10,000", "3 January 1521", "The Scotland Act 1998", "AC", "Chuck Howley", "1\u20133 \u03bcm thick", "United States Air Force", "Meiji Restoration", "peroxides, chlorates, nitrates, perchlorates, and dichromates", "842 pounds", "June 4, 2014", "recast as decision problems", "Gateshead Council", "more than 70", "Gamal Abdul Nasser", "the Bible", "completes replication with a rolling circle mechanism", "inner core", "Hangzhou", "genetic branches", "12 December 1963", "UNESCO World Heritage Site", "January 27, 1967", "cortisol and catecholamines", "constituency seats", "drummes", "Tower Theatre", "Min system", "to pressure the lazy, inspire the bored, deflate the cocky", "North American Aviation", "banded iron formations", "Arizona Cardinals", "Super Bowl XXXVIII", "major business districts", "HO", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "two", "destruction of the forest", "the Commission", "net mechanical energy", "Josh Cantrell", "Saul", "winter", "southwestern Colorado and northwestern New Mexico", "Paul to the Philippians", "Carol Worthington", "1997", "accomplish the objectives of the organization", "3 October 1990", "The federal government received only those powers which the colonies had recognized as belonging to king and parliament", "Jerry Leiber and Mike Stoller", "Heroes and Villains", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "positive, zero, or negative scalar quantity", "June 12, 2018", "Tim McGraw and Kenny Chesney", "N 80.000 \u00b0 W \ufeff", "James Martin Lafferty", "La Dame aux cam\u00e9lias", "C7 Stingrays", "InStyle", "\"blank slate\" without rules for"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7055803571428572}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.5714285714285715, 0.4, 1.0, 0.3333333333333333, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-635", "mrqa_squad-validation-8750", "mrqa_squad-validation-6327", "mrqa_squad-validation-3811", "mrqa_squad-validation-1649", "mrqa_squad-validation-8683", "mrqa_squad-validation-4949", "mrqa_squad-validation-1920", "mrqa_squad-validation-73", "mrqa_squad-validation-2632", "mrqa_squad-validation-6614", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-1748", "mrqa_naturalquestions-validation-5451", "mrqa_triviaqa-validation-4831", "mrqa_hotpotqa-validation-5838", "mrqa_newsqa-validation-1003", "mrqa_searchqa-validation-5338"], "SR": 0.59375, "CSR": 0.721875, "EFR": 0.9230769230769231, "Overall": 0.8224759615384616}, {"timecode": 5, "before_eval_results": {"predictions": ["orientalism", "Director", "the east end", "NL and NC", "1534", "April 1887", "\u00d6gedei Khan", "nearby open spaces", "the West Side", "John Pell, Lord of Pelham Manor", "Dai Setsen", "821,784", "two political parties would share power equally", "buoyancy", "ancestors", "divergence problem", "Islamist", "satellite television", "ordered the deportation of the French-speaking Acadian population from the area", "justifying grace", "131", "co-chair", "Maria Sk\u0142odowska-Curie Institute of Oncology", "winter of 1973\u201374", "P", "BAFTA Television Award", "Off-Off Campus", "the General Conference", "Super Bowl XLIV", "Sam Chisholm", "the mother", "LDS Church", "the most cost efficient bidder", "non-governmental agencies", "non-Catholics", "southern and central parts of France", "not been renewed", "Luther's education", "1998", "administrative supervision", "XXXX", "Thunder Road", "Sylvester Stallone", "present ( 2016 -- 2018, contemporaneous with airing ) and a storyline taking place at a set time in the past ; but some episodes are set in one time period or use multiple flashback time periods", "Mohammad Reza Pahlavi", "Thomas Mundy Peterson", "in the bible", "two goods", "season seven", "TLC", "the town of Acolman, just north of Mexico City,", "1987", "Asuka", "4 September 1936", "Amy Wong", "2002", "Pasek and Paul", "U.S. service members who have died without their remains being identified", "mostly by women, and rings can feature diamonds or other gemstones. In some cultures men and women wear matching rings, and engagement rings may also be used as wedding rings", "George Bernard Shaw", "Mary Porter ( Keyes) Babcock", "Bryant Purvis", "deuce", "40,400 members"], "metric_results": {"EM": 0.6875, "QA-F1": 0.71875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3492", "mrqa_squad-validation-10273", "mrqa_squad-validation-9416", "mrqa_squad-validation-8579", "mrqa_squad-validation-1759", "mrqa_squad-validation-7819", "mrqa_squad-validation-6752", "mrqa_squad-validation-1008", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-10093", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-1989"], "SR": 0.6875, "CSR": 0.7161458333333333, "EFR": 1.0, "Overall": 0.8580729166666666}, {"timecode": 6, "before_eval_results": {"predictions": ["monophyletic", "computational power", "late 1960s and early 1970s", "rapid combustion", "Stanford", "a continuous supply of gaseous oxygen to be pumped through a pipeline", "chloroplast", "capturing three traders and killing 14 people of the Miami nation, including Old Briton", "Citadel Media", "lands west of the Appalachian Mountains", "two", "ten", "a heretic", "average workers", "Catholic", "late 1886", "potentially dangerous", "Golovin", "produced between the object and the table surface", "chloroplast's stroma", "the last 7000 years", "3.6%", "God", "C. J. Anderson", "Robert R. Gilruth", "the Thirty Years' War", "north-eastern regions along the border with Somalia and Ethiopia", "$216,000", "slightly more than atmospheric pressure", "with observations", "6800", "after their second year", "consumes ATP and oxygen, releases CO2, and produces no sugar", "Giuliano da Sangallo", "two", "Chuck Connors", "Eva Marie", "Lincoln Logs", "a tin star", "in the muscle tissue", "Connochaetes", "Jerusalem", "Hamlet", "Plymouth Rock", "President Abraham Lincoln", "Martina Hingis", "tuna-like fishes", "TESLAR Satellite", "a kind of superman", "Gentlemen Prefer Blondes", "Decoupage", "treats the number of bonds formed by most elements in their compounds", "Bouvier", "Paris", "Titanic", "temperature", "New-Zealand", "treats himself the better man", "January 15, 2010", "Mark Neveldine and Brian Taylor", "four", "treats", "Edward Kenway ( Matt Ryan )", "1770 BC"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6428109217171717}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, false, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5454545454545454, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-5788", "mrqa_squad-validation-3676", "mrqa_squad-validation-8643", "mrqa_squad-validation-2754", "mrqa_squad-validation-10316", "mrqa_squad-validation-8900", "mrqa_squad-validation-9367", "mrqa_squad-validation-8399", "mrqa_squad-validation-1330", "mrqa_squad-validation-3479", "mrqa_squad-validation-8529", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6734", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-1800", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-1643", "mrqa_naturalquestions-validation-8909", "mrqa_searchqa-validation-9828", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-8659"], "SR": 0.5625, "CSR": 0.6941964285714286, "EFR": 1.0, "Overall": 0.8470982142857143}, {"timecode": 7, "before_eval_results": {"predictions": ["free", "Oireachtas funds", "May 21, 2013", "Warsaw", "communications between Yuan dynasty and its ally and the novel and the increased use of the written vernacular", "n > 3", "Spanish moss", "frequency and severity of micrometeorite impacts", "Charles Avison", "colonel", "Pedro Men\u00e9ndez de Avil\u00e9s", "a mainline Protestant Methodist denomination", "Capital Cities Communications", "up to 40 km wide", "ethnicity and race", "a very reactive allotrope of oxygen", "bitstrings", "markets", "a Gender pay gap in favor of males", "2014", "By 9000 BP", "Neoclassical economics", "d'Hondt method", "made tape recordings of the show", "coastal town of Kilifi", "first set of endosymbiotic events", "Abe Silverstein", "embroidery", "three", "1823", "British Sky Broadcasting Group plc", "Eumolpus", "Supreme Court Judge", "the north wing of the State Capitol in Saint Paul", "Syracuse", "artist and graffiti writer", "General Manager", "Bergen", "John Lennon", "David Irving", "a album", "croatan, Nantahala, and Uwharrie", "15,000 people", "2016 World Indoor Championships", "fighter ace credited with 35 victories", "psilocybin", "Washington, D.C.", "Na Na", "in Saint-Domingue, now the sovereign nation of Haiti", "Jena Malone", "1925", "National Hockey League", "one", "2009 Big 12 Conference", "Walldorf", "a Douglas- Long Beach built B-17G-95-DL", "nursery rhyme", "Necator americanus and Ancy Lostoma duodenale", "Hamelin", "11 healthy eggs", "tote bags", "alan mccain", "c. S. Forester", "6 verses"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6369973776223776}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.7692307692307693, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8066", "mrqa_squad-validation-5351", "mrqa_squad-validation-10174", "mrqa_squad-validation-9195", "mrqa_squad-validation-6351", "mrqa_squad-validation-3497", "mrqa_squad-validation-7447", "mrqa_squad-validation-9436", "mrqa_squad-validation-9532", "mrqa_squad-validation-7647", "mrqa_squad-validation-8453", "mrqa_hotpotqa-validation-3398", "mrqa_hotpotqa-validation-5591", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-4565", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-3672", "mrqa_hotpotqa-validation-2925", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-60", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-5346", "mrqa_naturalquestions-validation-6200", "mrqa_searchqa-validation-7219", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-1246", "mrqa_triviaqa-validation-221"], "SR": 0.53125, "CSR": 0.673828125, "EFR": 1.0, "Overall": 0.8369140625}, {"timecode": 8, "before_eval_results": {"predictions": ["a rock concert", "50-yard line", "an increase in skilled workers", "12 December 1964", "speed-up theorem", "Schr\u00f6dinger equation", "Guinness World Records", "nominate speakers", "Louis Agassiz", "Annual Status of Education Report", "locomotion", "geophysical surveys", "The later accidental introduction of Beroe", "socially", "most common forms of school discipline throughout much of the world", "the Romantic Rhine", "British colonists", "an adjustable spring-loaded valve", "complicated", "Super Bowl L", "7\u20134\u20132\u20133 system", "much higher school fees", "WatchESPN", "Economist", "Richard Lindzen", "emigration", "Political Islam", "826", "Figaro", "Mazda", "US Naval Submarine Base New London submarine school", "Secretary of Defense", "Nanyue", "Vyd\u016bnas", "1989", "American", "Anne of Green Gables", "Fainaru Fantaj\u012b Tuerubu", "Reverend Lovejoy", "his death", "Walldorf", "Kings Point, New York", "Bill Miner", "Agent 99", "Outside", "Christopher Nolan", "Umina Beach", "Chicago", "Thriller", "1992", "Let's Make Sure We Kiss Goodbye", "Andrzej Go\u0142ota", "Alonso L\u00f3pez", "post\u2013World War II", "Waylon Albright \"Shooter\" Jennings", "The New Yorker", "Type 10", "need to repent in time", "nirvana", "Jason Chaffetz", "a sealed glass tube with a metal electrode", "Speaker of the House of Representatives", "1947, 1956, 1975, 2015 and 2017", "49"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6543988997113998}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.4285714285714285, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.6666666666666666, 1.0, 0.0, 0.45454545454545453, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7436", "mrqa_squad-validation-8369", "mrqa_squad-validation-10386", "mrqa_squad-validation-4327", "mrqa_squad-validation-2085", "mrqa_squad-validation-477", "mrqa_squad-validation-7131", "mrqa_squad-validation-3124", "mrqa_squad-validation-9608", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-2481", "mrqa_naturalquestions-validation-4556", "mrqa_searchqa-validation-14617", "mrqa_naturalquestions-validation-5865"], "SR": 0.5625, "CSR": 0.6614583333333333, "EFR": 1.0, "Overall": 0.8307291666666666}, {"timecode": 9, "before_eval_results": {"predictions": ["Konwiktorska Street", "glass", "39", "1760", "CALIPSO", "the state (including the judges)", "The European Court of Justice", "in which two public agencies, especially two equally sovereign branches of government, conflict", "lymphokines", "solar", "specific terminology has no more (or no less) meaning than the individual orator intends it to have", "Finsteraarhorn", "1015 kelvins", "Aaron Spelling", "1770", "833,500", "1851", "Canada", "The Northern Chinese", "between 1859 and 1865", "A technical defense", "Gap", "\"missile gap\"", "Kensington and Chelsea", "Port of Long Beach", "an eccentric U.S. saloon-keeper", "made into a TV series", "close to 50 million", "George Clooney", "Matt Groening", "Hern\u00e1n Crespo", "William Finn", "Kenny Young", "Alistair Grant", "literary", "The Rebirth", "the sulfur mustards", "Scottish Premiership club", "Christian Maelen", "Kim Dae-woo", "The Terminator", "Saint Petersburg Conservatory", "Michael Phelps", "Bolton, England", "Quasimodo", "Cuban", "Cleveland Browns", "the Maldives", "\u00c9cole des Beaux-Arts", "the Kentucky Music Hall of Fame", "American 3D computer-animated comedy", "Agra", "Polka", "Esteban Ocon", "actress", "Kassie DePaiva", "Jaydev Shah", "Stephen Lang", "Doris Lessing", "April", "voluntary homicide", "Sodra nongovernmental organization", "Tutankhamun", "vowel"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7417649752208575}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.25, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3173", "mrqa_squad-validation-6803", "mrqa_squad-validation-6814", "mrqa_squad-validation-6837", "mrqa_squad-validation-5294", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-1980", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-534", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-9926"], "SR": 0.65625, "CSR": 0.6609375, "EFR": 1.0, "Overall": 0.83046875}, {"timecode": 10, "UKR": 0.75390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-2362", "mrqa_hotpotqa-validation-2481", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-2925", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3398", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3672", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3875", "mrqa_hotpotqa-validation-3898", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-476", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4903", "mrqa_hotpotqa-validation-4944", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5162", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-5366", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5512", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-5591", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5701", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-60", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-965", "mrqa_hotpotqa-validation-982", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1748", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-3330", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-476", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-6604", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-9536", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9842", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-534", "mrqa_searchqa-validation-13515", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-5338", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-9926", "mrqa_squad-validation-10001", "mrqa_squad-validation-10059", "mrqa_squad-validation-10063", "mrqa_squad-validation-1008", "mrqa_squad-validation-10123", "mrqa_squad-validation-10148", "mrqa_squad-validation-10174", "mrqa_squad-validation-10181", "mrqa_squad-validation-10186", "mrqa_squad-validation-10273", "mrqa_squad-validation-10316", "mrqa_squad-validation-10386", "mrqa_squad-validation-10457", "mrqa_squad-validation-10463", "mrqa_squad-validation-10470", "mrqa_squad-validation-1078", "mrqa_squad-validation-1089", "mrqa_squad-validation-1161", "mrqa_squad-validation-1177", "mrqa_squad-validation-1177", "mrqa_squad-validation-1182", "mrqa_squad-validation-1187", "mrqa_squad-validation-1219", "mrqa_squad-validation-1254", "mrqa_squad-validation-1277", "mrqa_squad-validation-133", "mrqa_squad-validation-134", "mrqa_squad-validation-1356", "mrqa_squad-validation-1423", "mrqa_squad-validation-1432", "mrqa_squad-validation-1436", "mrqa_squad-validation-1498", "mrqa_squad-validation-150", "mrqa_squad-validation-151", "mrqa_squad-validation-1593", "mrqa_squad-validation-1613", "mrqa_squad-validation-1614", "mrqa_squad-validation-1640", "mrqa_squad-validation-1649", "mrqa_squad-validation-1665", "mrqa_squad-validation-1678", "mrqa_squad-validation-168", "mrqa_squad-validation-1681", "mrqa_squad-validation-175", "mrqa_squad-validation-1759", "mrqa_squad-validation-1767", "mrqa_squad-validation-1779", "mrqa_squad-validation-1815", "mrqa_squad-validation-185", "mrqa_squad-validation-1859", "mrqa_squad-validation-1891", "mrqa_squad-validation-1898", "mrqa_squad-validation-1902", "mrqa_squad-validation-1920", "mrqa_squad-validation-194", "mrqa_squad-validation-1980", "mrqa_squad-validation-2079", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2144", "mrqa_squad-validation-215", "mrqa_squad-validation-2186", "mrqa_squad-validation-2197", "mrqa_squad-validation-2200", "mrqa_squad-validation-2214", "mrqa_squad-validation-2248", "mrqa_squad-validation-2272", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-242", "mrqa_squad-validation-2473", "mrqa_squad-validation-2490", "mrqa_squad-validation-2568", "mrqa_squad-validation-2586", "mrqa_squad-validation-2612", "mrqa_squad-validation-2632", "mrqa_squad-validation-2666", "mrqa_squad-validation-2672", "mrqa_squad-validation-270", "mrqa_squad-validation-270", "mrqa_squad-validation-2704", "mrqa_squad-validation-271", "mrqa_squad-validation-2725", "mrqa_squad-validation-2765", "mrqa_squad-validation-2775", "mrqa_squad-validation-2807", "mrqa_squad-validation-2811", "mrqa_squad-validation-2853", "mrqa_squad-validation-2873", "mrqa_squad-validation-2893", "mrqa_squad-validation-2950", "mrqa_squad-validation-2975", "mrqa_squad-validation-2986", "mrqa_squad-validation-3007", "mrqa_squad-validation-3076", "mrqa_squad-validation-3096", "mrqa_squad-validation-3124", "mrqa_squad-validation-3125", "mrqa_squad-validation-313", "mrqa_squad-validation-3140", "mrqa_squad-validation-3143", "mrqa_squad-validation-3173", "mrqa_squad-validation-3275", "mrqa_squad-validation-3319", "mrqa_squad-validation-3327", "mrqa_squad-validation-3370", "mrqa_squad-validation-3407", "mrqa_squad-validation-3464", "mrqa_squad-validation-3479", "mrqa_squad-validation-3492", "mrqa_squad-validation-3497", "mrqa_squad-validation-3511", "mrqa_squad-validation-3537", "mrqa_squad-validation-3550", "mrqa_squad-validation-3581", "mrqa_squad-validation-3676", "mrqa_squad-validation-3723", "mrqa_squad-validation-3740", "mrqa_squad-validation-375", "mrqa_squad-validation-3750", "mrqa_squad-validation-3770", "mrqa_squad-validation-3774", "mrqa_squad-validation-3788", "mrqa_squad-validation-38", "mrqa_squad-validation-3802", "mrqa_squad-validation-3811", "mrqa_squad-validation-3813", "mrqa_squad-validation-3842", "mrqa_squad-validation-3852", "mrqa_squad-validation-3871", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3923", "mrqa_squad-validation-3937", "mrqa_squad-validation-3939", "mrqa_squad-validation-3945", "mrqa_squad-validation-402", "mrqa_squad-validation-4034", "mrqa_squad-validation-4179", "mrqa_squad-validation-420", "mrqa_squad-validation-4227", "mrqa_squad-validation-4246", "mrqa_squad-validation-4292", "mrqa_squad-validation-4327", "mrqa_squad-validation-4430", "mrqa_squad-validation-4437", "mrqa_squad-validation-4473", "mrqa_squad-validation-4484", "mrqa_squad-validation-4607", "mrqa_squad-validation-4612", "mrqa_squad-validation-4636", "mrqa_squad-validation-4660", "mrqa_squad-validation-4737", "mrqa_squad-validation-4750", "mrqa_squad-validation-476", "mrqa_squad-validation-4882", "mrqa_squad-validation-4898", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-4981", "mrqa_squad-validation-5007", "mrqa_squad-validation-5019", "mrqa_squad-validation-5030", "mrqa_squad-validation-5042", "mrqa_squad-validation-5085", "mrqa_squad-validation-5135", "mrqa_squad-validation-5147", "mrqa_squad-validation-5196", "mrqa_squad-validation-5198", "mrqa_squad-validation-5276", "mrqa_squad-validation-5294", "mrqa_squad-validation-5325", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5351", "mrqa_squad-validation-5389", "mrqa_squad-validation-5434", "mrqa_squad-validation-5531", "mrqa_squad-validation-5621", "mrqa_squad-validation-5634", "mrqa_squad-validation-5671", "mrqa_squad-validation-5699", "mrqa_squad-validation-5724", "mrqa_squad-validation-5736", "mrqa_squad-validation-5751", "mrqa_squad-validation-5788", "mrqa_squad-validation-5797", "mrqa_squad-validation-5869", "mrqa_squad-validation-5875", "mrqa_squad-validation-5947", "mrqa_squad-validation-5961", "mrqa_squad-validation-6029", "mrqa_squad-validation-6089", "mrqa_squad-validation-611", "mrqa_squad-validation-6159", "mrqa_squad-validation-6163", "mrqa_squad-validation-624", "mrqa_squad-validation-6318", "mrqa_squad-validation-6341", "mrqa_squad-validation-635", "mrqa_squad-validation-6351", "mrqa_squad-validation-6381", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6546", "mrqa_squad-validation-6555", "mrqa_squad-validation-6569", "mrqa_squad-validation-6589", "mrqa_squad-validation-6594", "mrqa_squad-validation-6628", "mrqa_squad-validation-6636", "mrqa_squad-validation-6648", "mrqa_squad-validation-6676", "mrqa_squad-validation-6700", "mrqa_squad-validation-6716", "mrqa_squad-validation-6752", "mrqa_squad-validation-679", "mrqa_squad-validation-6814", "mrqa_squad-validation-682", "mrqa_squad-validation-6837", "mrqa_squad-validation-6838", "mrqa_squad-validation-6873", "mrqa_squad-validation-6877", "mrqa_squad-validation-6924", "mrqa_squad-validation-6960", "mrqa_squad-validation-6978", "mrqa_squad-validation-6981", "mrqa_squad-validation-6986", "mrqa_squad-validation-7126", "mrqa_squad-validation-7131", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7168", "mrqa_squad-validation-7194", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7246", "mrqa_squad-validation-7277", "mrqa_squad-validation-729", "mrqa_squad-validation-73", "mrqa_squad-validation-735", "mrqa_squad-validation-7354", "mrqa_squad-validation-7354", "mrqa_squad-validation-7367", "mrqa_squad-validation-7384", "mrqa_squad-validation-7436", "mrqa_squad-validation-7447", "mrqa_squad-validation-7476", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-7570", "mrqa_squad-validation-7585", "mrqa_squad-validation-76", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7652", "mrqa_squad-validation-7656", "mrqa_squad-validation-7689", "mrqa_squad-validation-7707", "mrqa_squad-validation-7713", "mrqa_squad-validation-773", "mrqa_squad-validation-7782", "mrqa_squad-validation-7788", "mrqa_squad-validation-7819", "mrqa_squad-validation-7838", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-8028", "mrqa_squad-validation-8043", "mrqa_squad-validation-8049", "mrqa_squad-validation-8066", "mrqa_squad-validation-8118", "mrqa_squad-validation-8139", "mrqa_squad-validation-816", "mrqa_squad-validation-824", "mrqa_squad-validation-8253", "mrqa_squad-validation-8273", "mrqa_squad-validation-8283", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8386", "mrqa_squad-validation-8399", "mrqa_squad-validation-8453", "mrqa_squad-validation-850", "mrqa_squad-validation-8500", "mrqa_squad-validation-8502", "mrqa_squad-validation-8505", "mrqa_squad-validation-8523", "mrqa_squad-validation-8529", "mrqa_squad-validation-8555", "mrqa_squad-validation-8579", "mrqa_squad-validation-8601", "mrqa_squad-validation-8603", "mrqa_squad-validation-8643", "mrqa_squad-validation-8680", "mrqa_squad-validation-8683", "mrqa_squad-validation-8750", "mrqa_squad-validation-8801", "mrqa_squad-validation-889", "mrqa_squad-validation-8900", "mrqa_squad-validation-8901", "mrqa_squad-validation-8924", "mrqa_squad-validation-8927", "mrqa_squad-validation-8948", "mrqa_squad-validation-8966", "mrqa_squad-validation-8969", "mrqa_squad-validation-8987", "mrqa_squad-validation-9048", "mrqa_squad-validation-9097", "mrqa_squad-validation-9135", "mrqa_squad-validation-9157", "mrqa_squad-validation-9165", "mrqa_squad-validation-9254", "mrqa_squad-validation-9269", "mrqa_squad-validation-9271", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9325", "mrqa_squad-validation-9328", "mrqa_squad-validation-9367", "mrqa_squad-validation-9416", "mrqa_squad-validation-9436", "mrqa_squad-validation-9459", "mrqa_squad-validation-9470", "mrqa_squad-validation-9531", "mrqa_squad-validation-9543", "mrqa_squad-validation-9553", "mrqa_squad-validation-9559", "mrqa_squad-validation-9608", "mrqa_squad-validation-9764", "mrqa_squad-validation-9787", "mrqa_squad-validation-9819", "mrqa_squad-validation-9868", "mrqa_squad-validation-9900", "mrqa_squad-validation-9901", "mrqa_squad-validation-9943", "mrqa_squad-validation-9989", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-1670", "mrqa_triviaqa-validation-1800", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-471", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-5627", "mrqa_triviaqa-validation-5960", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6577", "mrqa_triviaqa-validation-6734", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-71", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7601", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7683"], "OKR": 0.875, "KG": 0.334375, "before_eval_results": {"predictions": ["hydrogen and helium", "a supervisory church body", "AC", "Jerricho Cotchery", "Isaac Komnenos", "a certain number of teacher's salaries are paid by the State", "cylinders and valve gear", "ABC1", "ATP energy", "San Andreas fault system", "Hulu", "two", "social unrest and violence", "Toyota Corona, the Toyota Corolla, the Datsun B210", "Metro Light Rail system and a journey into Newcastle city centre takes approximately 20 minutes", "receiver", "A Turing machine", "Luther", "Tumor antigens", "1978", "Mark Twain", "1804", "seven", "an ankle-length skirt", "David Hilbert", "India", "Patsy Stone", "an abandoned studio", "anaerobe", "Kiss Me Kate", "Robert Agar ( Donald Sutherland), a pickpocket and screwsman", "Mumbai", "an artistic movement, and an aesthetic philosophy that aims for the liberation of the mind by emphasizing the critical and imaginative powers of the unconscious", "Jordan", "Nang Klao Rama III", "A.N. Whitehead", "A-ha", "N Africa", "William Holden", "Charlie Brooker", "horse racing", "Nugget", "an aniline dyes", "Egypt", "tennis", "\"Appaloosa\"", "NICE POINT", "Norns", "steel", "Old Trafford", "Anna Johanna Henrietta Juliana Von Griesheim", "rabbit", "The King and I", "Lisbeth Salander", "Standard Oil Company", "Miranda v. Arizona", "Janis Joplin", "the 1960s", "over 1.6 million", "Mark Neveldine and Brian Taylor", "Alina Cho", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves", "a talking stuffed bear", "paul mcc McCartney"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5484623015873016}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, false, true, false, true, false, false, false, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, true, true, false, false, false], "QA-F1": [0.42857142857142855, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.09523809523809525, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3667", "mrqa_squad-validation-2468", "mrqa_squad-validation-1402", "mrqa_squad-validation-780", "mrqa_squad-validation-5025", "mrqa_squad-validation-3708", "mrqa_squad-validation-5474", "mrqa_squad-validation-336", "mrqa_squad-validation-6579", "mrqa_triviaqa-validation-921", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-1160", "mrqa_triviaqa-validation-3657", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-2937", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-6821", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-4137", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-2372", "mrqa_triviaqa-validation-4982", "mrqa_triviaqa-validation-4005", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-1461", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-7737", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9135", "mrqa_hotpotqa-validation-1526", "mrqa_newsqa-validation-2422", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-10099"], "SR": 0.46875, "CSR": 0.6434659090909092, "EFR": 1.0, "Overall": 0.7213494318181819}, {"timecode": 11, "before_eval_results": {"predictions": ["high than normal O2 exposure for a fee", "center of the curving path", "polynomial time", "1206", "Dutch East India Company", "Newton", "after the Franco-German War", "By a plant is injured, or something else causes a plant cell to revert to a meristematic state", "June 4, 2014", "energy", "December 2014", "1483", "19", "meritocracy", "photooxidative damage", "tangential force", "St John the Baptist", "more or less rounded, sometimes nearly spherical and other times more cylindrical or egg-shaped", "By chloroplast ATP synthase projects out into the stroma, the ATP is synthesized there, in position to be used in the dark reactions", "Johann Eck", "marx", "the American Civil War", "cigarettes", "king David", "Bleak House", "butterfly", "John Flamsteed", "bison", "Anita Roddick", "River Cart", "Tamar", "Nizhny Novgorod", "The Word", "The Left Book Club", "Tchaikovsky", "Tony Blackburn", "dakil", "spa", "butterfly", "Tarzan", "James Hanratty", "Middlesbrough", "\"Gomer\" Pyle", "London Pride", "Butcher", "jumper", "Milton Keynes", "November", "koftas", "Coventry to Leicester Motorway", "Bon Jovi", "Little Arrows", "legion", "Saint Vitus", "Syriza", "from shore to shore", "an Ohio newspaper", "Channel 4", "Cartoon Network Too", "pattern matching", "to wear Islamic dress", "Mojave", "ha", "egypt"], "metric_results": {"EM": 0.5, "QA-F1": 0.5306186868686869}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3610", "mrqa_squad-validation-8229", "mrqa_squad-validation-361", "mrqa_squad-validation-8931", "mrqa_squad-validation-2018", "mrqa_squad-validation-4469", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2889", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-3955", "mrqa_triviaqa-validation-3045", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-7124", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-4400", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-6665", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-18", "mrqa_searchqa-validation-12699", "mrqa_searchqa-validation-5935", "mrqa_newsqa-validation-3918"], "SR": 0.5, "CSR": 0.6315104166666667, "EFR": 0.96875, "Overall": 0.7127083333333334}, {"timecode": 12, "before_eval_results": {"predictions": ["Marburg Colloquy", "a negative long-term impact", "with known magnitudes of force", "more expansive, extending east into Las Vegas, Nevada, and south across the Mexican border into Tijuana", "Ed Whitfield", "In the centre of Basel, the first major city in the course of the stream, is located the \"Rhine knee\"", "Levi's Stadium", "Miasma theory", "a freshwater lake", "Monte Gargano", "50-yard line", "National Galleries of Scotland", "Arts & Entertainment Television (A&E)", "gravity", "Paul Samuelson, the first American to win the Nobel Memorial Prize in Economic Sciences,", "fans", "pathogen attack", "a not-for-profit United States computer networking consortium led by members from the research and education communities, industry, and government", "1850", "by funeral convoys for fallen Canadian Forces personnel from CFB Trenton to the coroner's office in Toronto", "Spanish explorers", "eight", "Columbia University", "Waylon Jennings", "black and yellow", "Melissa Disney", "five", "1989", "Floyd   Patrice Lovely as Hattie   Tony Hightower as Frank   Alexis Jones as Diane   Jeffery Lewis as Wallie", "1980s and'90s", "Tyler, Ali, and Lydia", "May 18, 2018", "2015", "Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "$75,000", "332", "Hudson Bay", "unknown origin", "accomplish the objectives of the organization", "in the time span of an elevator ride, or approximately thirty seconds to two minutes", "1776", "pin - on", "a normally inaccessible mini-game", "Charles Darwin", "insulated shipping containers", "humped camel", "2016", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Richard Carpenter", "T.S. Eliot", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Joe Spano", "in San Francisco", "north", "Paul Lynde", "a sweet-and-sour, dark-brown vinegar", "David Seville", "Gracie Mansion", "Peter Seamus O'Toole", "Arthur E. Morgan III", "his wife, Cabinet members, governors and other public and private officials", "Wayne's World", "peter", "Arkansas"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6430759775522186}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false, true, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.8, 0.2857142857142857, 0.0, 1.0, 0.125, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.19354838709677422, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.38095238095238093, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4827586206896552, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.8, 0.4, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5213", "mrqa_squad-validation-10321", "mrqa_squad-validation-2553", "mrqa_squad-validation-9196", "mrqa_squad-validation-4877", "mrqa_squad-validation-10352", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1613", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-9142", "mrqa_naturalquestions-validation-627", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-1864", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-1575", "mrqa_triviaqa-validation-3727", "mrqa_hotpotqa-validation-3299", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-3444", "mrqa_searchqa-validation-15790"], "SR": 0.515625, "CSR": 0.6225961538461539, "EFR": 0.967741935483871, "Overall": 0.7107238678660049}, {"timecode": 13, "before_eval_results": {"predictions": ["phlogiston theory of combustion and corrosion", "1974", "Andrew Alper", "shaping ideas about the free market", "learning by providing a social networking support that allows them to reach their full cognitive potential", "The Prospect Studios", "Milton Friedman Institute", "the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "1972", "polytechnics became new universities", "conservation of momentum", "October 1973", "Frontex", "a dispute over control of the confluence of the Allegheny and Monongahela rivers", "720p high definition", "1924", "\"Sippin' on Some Sizzurp,\"", "Apple Lisa", "the Republic of Indonesia and the Dutch Empire", "part of a bet from a gambler", "October 21, 2016", "Donald Duck", "February 12, 2014", "Black Mountain College", "An aircraft", "University of Vienna", "November 2, 2003", "Golden Calf", "2012 Summer Olympics", "February 13, 1946", "Leslie James \"Les\" Clark", "the static test pressure that a sample of newly manufactured watches were exposed to", "Rothschild", "The Grandmaster", "Montana State University", "37 feature films", "\"Bombay Talkie\" (1970), \"Junoon\" (1978), \" Heat and Dust\" (1983), and \"Ghare Baire\" (1984)", "a Rugby Sevens competition for the twelve Aviva Premiership clubs that will play the following season", "Oliver Platt, Hank Azaria, Josh Gad, Chief Wiggum, Comic Book Guy, Carl Carlson and numerous others", "Supergirl", "a man attempting to get some kind of focus in his life as he deals with his girlfriend, his mother and stepfather", "16 January 1856", "Mauritian", "mixed martial arts", "Cape Cod", "B.J. Hunnicutt", "Humberside Airport", "Henry Luce", "part of the complex web of off-balance-sheet special purpose entities (limited partnerships which Enron controlled) used to conceal Enron's massive losses in their quarterly balance sheets", "the new king in 1714", "Melanie Owen", "Book of Judges", "12", "35", "the length of suspended roadway between the bridge's towers", "the seven ages of man : infant, schoolboy, lover, soldier, justice, Pantalone and old age", "HMS Thunderbolt", "Vinegar Joe", "Ralph Lauren", "launching a massive crackdown on terror groups that they say were planning numerous suicide attacks, including in the country's largest city of Karachi", "Floral, Saskatchewan", "The Band Concert", "Larry King", "terminal brain cancer"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5569827158593322}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, true, true, false, false, false, false, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.2666666666666667, 0.0, 0.2, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.27586206896551724, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1913", "mrqa_squad-validation-7763", "mrqa_squad-validation-5889", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-3736", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5387", "mrqa_hotpotqa-validation-4646", "mrqa_hotpotqa-validation-5239", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-3057", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-405", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-2844", "mrqa_triviaqa-validation-4212", "mrqa_newsqa-validation-1095", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-15877", "mrqa_newsqa-validation-2128"], "SR": 0.46875, "CSR": 0.6116071428571428, "EFR": 0.9705882352941176, "Overall": 0.7090953256302521}, {"timecode": 14, "before_eval_results": {"predictions": ["Chu'Tsai", "second-largest", "17 seconds", "Ed Mangan", "suspended sentences", "send aid", "nearly two-thirds", "a dam turbine", "SAP Center in San Jose", "Niels Jerne", "Central Bridge", "Vistula River", "an enzyme called rubisco", "a fire that started as a kitchen fire", "Louis Pasteur", "\"The Tales of Hoffmann\"", "Lake Placid, New York", "four", "Elton John", "1978", "Victorian England", "Indian", "polyphonic", "1970", "Tom Kitt", "Outside", "Forbes", "acidic", "Tampa", "Saoirse Ronan", "newspapers, television, radio, cable television, and other businesses", "University of Kansas", "the Prescription Drug User Fee Act", "S\u00f8nderjyskE Ishockey", "Foxborough", "abstract hip hop", "Lancashire Combination side Stalybridge Celtic", "John Delaney", "\"The Omega Man\"", "1955", "2016", "2 March 1972", "Londonderry", "11 November 1869", "Sam Bettley", "Larry Drake", "Wilhelmus Simon Petrus Fortuijn", "Eisstadion Davos", "\"All the Way\"", "private", "St Augustine's Abbey in Canterbury, Kent", "a fictional character", "Dan Fogelman", "La Boito's \"Mefistofele\"", "its genome", "1998", "niacamide", "Ottoman Palestine", "Kim Clijsters", "Rev. Alberto Cutie", "halsz", "the Lochner-.... son clerks reunion", "Ward Robe", "a confection made of nougat, peanut & Chocolate"], "metric_results": {"EM": 0.453125, "QA-F1": 0.569335699023199}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.7692307692307693, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-269", "mrqa_squad-validation-9695", "mrqa_squad-validation-457", "mrqa_squad-validation-7233", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-414", "mrqa_hotpotqa-validation-2574", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-2725", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-1696", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-866", "mrqa_hotpotqa-validation-4899", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-3651", "mrqa_triviaqa-validation-5714", "mrqa_triviaqa-validation-971", "mrqa_newsqa-validation-1150", "mrqa_searchqa-validation-13156", "mrqa_searchqa-validation-16559", "mrqa_searchqa-validation-13152", "mrqa_searchqa-validation-6300"], "SR": 0.453125, "CSR": 0.6010416666666667, "EFR": 1.0, "Overall": 0.7128645833333334}, {"timecode": 15, "before_eval_results": {"predictions": ["11", "1968", "teacher who stays with them for most of the week and will teach them the whole curriculum", "Westinghouse Electric", "nine months", "more than 70", "Inherited wealth may help explain why many Americans who have become rich may have had a \"substantial head start\"", "Super Bowl XXXIII", "Alan Turing", "North America", "Conservative", "contrasts with the newer areas of tract homes urban sprawl", "Clair Cameron Patterson", "Esp\u00edrito Santo Financial Group", "Filmed in Europe", "Scotiabank Saddledome", "Ralph Stanley", "S Pictures' \"Veyyil\"", "YouTube celebrity PewDiepie", "Seoul, South Korea", "Pittsburgh", "2012", "Chinese Coffee", "Sammy Gravano", "$7.3 billion", "a French natural philosopher, mathematician, physicist", "Umina Beach, New South Wales", "Port Macquarie", "\"Naked\"", "Edinburgh", "Anthony Lynn", "Levi Weeks", "the Mayor of the City of New York", "soccer", "Memphis Minnie's \"When the Levee Breaks\"", "The Five", "James Mitchum", "Candice Susan Swanepoel", "Bhaktivedanta Manor", "Thomas Christopher Ince", "Texas Tech Red Raiders", "U.S. military", "south", "historic buildings, arts, and published works", "June 24, 1935", "James K. Polk", "March 31, 1944", "Linux Format", "\"Confessions of a Teenage Drama Queen\"", "Koch Industries", "Rymill Park", "Eliot Cutler", "Tchaikovsky's \"Swan Lake\" ballet", "Thomas Jefferson", "Marie Van Brittan Brown", "Billy Idol", "Full Metal jacket", "Bombay", "nine", "Hussein's Revolutionary Command Council", "The Neapolitan type", "Anthony Fokker", "Jean-Baptiste Say", "chromatopsia"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6906249999999999}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.5, 1.0, 0.8, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1892", "mrqa_squad-validation-7456", "mrqa_squad-validation-4671", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-4564", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-2786", "mrqa_hotpotqa-validation-5319", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-5125", "mrqa_hotpotqa-validation-1192", "mrqa_triviaqa-validation-5715", "mrqa_newsqa-validation-3297", "mrqa_searchqa-validation-8148", "mrqa_searchqa-validation-1728", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-581"], "SR": 0.59375, "CSR": 0.6005859375, "EFR": 1.0, "Overall": 0.7127734375}, {"timecode": 16, "before_eval_results": {"predictions": ["Aston Webb", "Jin", "in his lab and elsewhere", "1253", "The clinical pharmacy movement", "RNA silencing", "50", "lectured on the Psalms, the books of Hebrews, Romans, and Galatians", "Yes\u00fcgei", "a renewed version of the Russian imperialism and colonialism", "American Philosophical Society", "Albert Einstein", "Bristol", "Javier Bardem", "steam", "Ben Whishaw", "2", "Tears for Fears", "Richard Noble", "Skylab", "Philistine", "Spain", "milk", "Z", "rue", "Filibuster", "a gold Georg Olden\u2013designed statuette", "bats", "Styal", "First pig building a house", "a senior private", "Margaret Beckett", "Brad Pitt", "a plug of keratin and sebum", "Canada", "a tax", "a marble campanile", "bitter", "Sesame Street", "a group of people live in a large house", "Victory Aircraft", "Leeds", "Jack Dee", "Joseph Priestley", "white", "a new Eurasian union", "St Asaph", "Fernando Torres", "ADNAMS", "amsterdam", "a collier", "Leicestershire", "Wyoming", "Greek historian, essayist, and soldier", "O'Meara", "3 total", "Paper Trail", "2013", "Mexico", "Newcastle retained fourth place with a 3-1 victory", "Hugh Grant", "Pink", "Tony Manero", "1"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6622395833333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6316", "mrqa_squad-validation-6547", "mrqa_squad-validation-2254", "mrqa_squad-validation-10128", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-5830", "mrqa_triviaqa-validation-5844", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-6246", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-6996", "mrqa_triviaqa-validation-7155", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3685", "mrqa_triviaqa-validation-4357", "mrqa_triviaqa-validation-5052", "mrqa_naturalquestions-validation-8087", "mrqa_newsqa-validation-2467"], "SR": 0.609375, "CSR": 0.6011029411764706, "EFR": 0.96, "Overall": 0.7048768382352941}, {"timecode": 17, "before_eval_results": {"predictions": ["pattern recognition receptors", "northern Mokot\u00f3w", "in the courtyard adjoining the Assembly Hall", "six", "1527", "Zhongdu", "art and design", "Chicago Bears", "Bill Clinton", "differences in value added by labor, capital and land", "geochemical evolution of rock units", "an assemblage of things laid", "eagle", "red", "Pink Floyd", "Constantinople", "oysseus", "10th wedding anniversary", "Mesozoic Era", "can we get along?", "ethyl mercaptan", "Coral Reef", "1849", "Clyde", "New Orleans", "bugle", "liver", "right-to-left", "nonfiction", "he founded the colony of Rhode Island", "mask", "token lands on or passes over \"GO\"", "Mediolanum", "his statement on the Second Day of the Diet of Worms", "rupture of membrane", "emperor Claudius Caesar", "white blood cells", "Green Lantern", "Zeus", "New York Giants", "Mouseketeers", "Yves Saint Laurent", "the Trucial States", "Macy's", "ljubljana", "scarpia", "mammals", "jedoublen", "bali", "corn", "the Crimean War", "oresteia", "pesto", "providing telecommunication services to enterprises and offices", "1834", "Panurge", "the Savoy", "Leonard Cohen and Leon Uris", "S6", "Nazi Party members", "open heart surgery", "Ritchie Cordell", "the tax rate paid by a small business", "along the Californian coast"], "metric_results": {"EM": 0.421875, "QA-F1": 0.48472222222222217}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, false, false, true, false, false, false, false, false, true, true, true, true, false, true, false, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.888888888888889, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3]}}, "before_error_ids": ["mrqa_squad-validation-1042", "mrqa_squad-validation-9400", "mrqa_squad-validation-5429", "mrqa_searchqa-validation-10868", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2329", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-15310", "mrqa_searchqa-validation-8485", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-6670", "mrqa_searchqa-validation-16258", "mrqa_searchqa-validation-4065", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-4154", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-15504", "mrqa_searchqa-validation-14923", "mrqa_searchqa-validation-2095", "mrqa_searchqa-validation-5698", "mrqa_searchqa-validation-8417", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-11410", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-1387", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2978", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-2546", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-2250"], "SR": 0.421875, "CSR": 0.5911458333333333, "EFR": 1.0, "Overall": 0.7108854166666666}, {"timecode": 18, "before_eval_results": {"predictions": ["Apollo Applications Program", "5.3%", "seizures", "Mercury", "a setup phase in each involved node before any packet is transferred", "Infrastructure", "MHC class I molecules on their surface", "the European Court of Human Rights", "it is neither zero nor a unit", "neuronal dendrites", "The Lost Symbol", "Robert Mugabe", "November 26", "Al Nisr Al Saudi", "two paintings", "Marcus Schrenker", "glass shards left by beer drinkers in the city center", "Chancellor Angela Merkel", "Tibet's independence from China", "drug trafficking", "steamboat", "Osama bin Laden's sons", "2002", "Yusuf Saad Kamel", "general secretary", "byproducts emitted during the process of burning and melting raw materials", "September", "Transport Workers Union leaders", "introduce legislation Thursday to improve the military's suicide-prevention programs", "Eintracht Frankfurt", "Jacob", "5 1/2-year-old son, Ryder Russell", "canyon", "eco-horror scenarios", "Bob Dole", "Kenyan Defense Minister Yusuf Haji", "unwanted baggage from the 80s and has grown beyond a resort town into something more substantial", "U.S. senators", "first or second week in April", "Chesley \"Sully\" Sullenberger", "Africa's largest producer", "Thursday", "emergency aid", "A third beluga whale", "Martin Aloysius Culhane", "Washington", "a repair shop", "Dubai", "South Africa", "snowstorm", "38", "Juan Martin Del Potro.", "If  your ex's loved ones ask why you broke up", "most junior enlisted sailor ( `` E-1 '' ) to the most senior enlisted sailor", "two easily observed features", "The Comedy of Errors", "xenophon", "devotional literature", "14th Street", "Lord Byron", "one finger tapping the left thigh", "Clark Irwin", "a cape", "a hickey"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5832317838751662}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, false, false, false, true, false, true, false, true, false, true, false, false, false, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8148148148148148, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9, 0.0, 0.6666666666666666, 0.2857142857142857, 0.0, 0.28571428571428575, 0.2857142857142857, 0.0, 0.5, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.25, 0.11764705882352941, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.09090909090909091, 1.0, 0.0, 1.0, 0.4, 1.0, 0.7499999999999999, 0.13333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4799", "mrqa_squad-validation-6522", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-961", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3365", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-456", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1184", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-2901", "mrqa_hotpotqa-validation-4925", "mrqa_searchqa-validation-15244", "mrqa_searchqa-validation-9803", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-8188", "mrqa_searchqa-validation-15495"], "SR": 0.453125, "CSR": 0.5838815789473684, "EFR": 1.0, "Overall": 0.7094325657894737}, {"timecode": 19, "before_eval_results": {"predictions": ["one of the most common forms of school discipline throughout much of the world", "April 1, 1963", "Islamism, and \"Islamization\" or implementation of Islamic law,", "he did not want disloyal men in his army.", "The conservation of momentum", "west", "154", "17 February 1546", "1996", "1960", "Friday", "Anil Kapoor.", "2005 for vote-tampered.", "2008", "Fred Bright, the district attorney in Milledgeville,", "President Obama's surge plan to head to Afghanistan's restive provinces to support Marines and soldiers fighting a dug-in Taliban force.", "Matthew Chance", "11th year in a row", "suspended", "\"The e-mails] are almost like reading a novel that you would embarrassed to buy,\"", "the Nazi war crimes suspect", "Lashkar-e-Tayyiba (LeT), an Islamic militant group based in Pakistan.", "the Movement for Democratic Change,", "Steven Chu", "Wednesday at the age of 95", "\"the most important discovery\" for the museum \"of the last 90 years.\"", "the Philippines", "\"The seemed to be kind of laid-back -- it didn't seem to be that dangerous,\"", "2,000 euros ($2,963)", "buckling under pressure from the ruling party.", "Russian air company Vertikal-T,", "The Everglades, known as the River of Grass,", "\"a striking blow to due process and the rule of law,\"", "federal officers' bodies", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help as factors contributing to the suicides.", "Los Angeles County Fire Department", "\"Goblin's Market,\"", "South African police have opened a criminal investigation into allegations that a dorm parent mistreated students at the school.", "hundreds", "girls", "\"an eye for an eye,\"", "\"The Obama campaign has kept the details on both the timing and selection of the running mate under wraps.", "\"horrible crime that is designed to sabotage reconciliatory efforts by the Iraqi people,", "Andrew Morris,", "2007", "\"The all-star film grossed an estimated $52.4 million over three days and is likely to top $60 million by the time the Presidents Day holiday weekend is over.", "Vernon Forrest", "The man ran away,", "Jason Chaffetz", "St. Louis, Missouri", "Alicia Keys", "Oxbow,", "Zhanar Tokhtabayeba", "1038", "1439", "husbands", "a medium", "the senior men's Lithuanian national team", "two years", "My Therapist", "James Corden", "American black bear", "burrito", "Constellation family"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5787342563837129}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false], "QA-F1": [0.5, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.1904761904761905, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.13333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.1739130434782609, 0.0, 0.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2085", "mrqa_squad-validation-9768", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-440", "mrqa_naturalquestions-validation-9060", "mrqa_triviaqa-validation-1427", "mrqa_triviaqa-validation-2337", "mrqa_hotpotqa-validation-4927", "mrqa_searchqa-validation-13321", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-10395", "mrqa_searchqa-validation-9391"], "SR": 0.515625, "CSR": 0.58046875, "EFR": 1.0, "Overall": 0.70875}, {"timecode": 20, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1334", "mrqa_hotpotqa-validation-1419", "mrqa_hotpotqa-validation-1484", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1696", "mrqa_hotpotqa-validation-1750", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1840", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-1912", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2456", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2574", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3057", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3299", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4182", "mrqa_hotpotqa-validation-4189", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4565", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4717", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4780", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-5034", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5319", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-5366", "mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5838", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-748", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-982", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-476", "mrqa_naturalquestions-validation-4876", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9545", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-3297", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3365", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-3931", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-456", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-872", "mrqa_searchqa-validation-11130", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-12104", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-13156", "mrqa_searchqa-validation-13321", "mrqa_searchqa-validation-13515", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-14923", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-16258", "mrqa_searchqa-validation-16559", "mrqa_searchqa-validation-1926", "mrqa_searchqa-validation-2095", "mrqa_searchqa-validation-2329", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-5338", "mrqa_searchqa-validation-5531", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-7219", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7938", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-9391", "mrqa_searchqa-validation-9683", "mrqa_squad-validation-100", "mrqa_squad-validation-10001", "mrqa_squad-validation-10059", "mrqa_squad-validation-10063", "mrqa_squad-validation-1008", "mrqa_squad-validation-10186", "mrqa_squad-validation-10256", "mrqa_squad-validation-10273", "mrqa_squad-validation-10316", "mrqa_squad-validation-10346", "mrqa_squad-validation-10352", "mrqa_squad-validation-10370", "mrqa_squad-validation-10386", "mrqa_squad-validation-10399", "mrqa_squad-validation-1042", "mrqa_squad-validation-10457", "mrqa_squad-validation-10463", "mrqa_squad-validation-10470", "mrqa_squad-validation-10475", "mrqa_squad-validation-10484", "mrqa_squad-validation-1078", "mrqa_squad-validation-1177", "mrqa_squad-validation-12", "mrqa_squad-validation-1207", "mrqa_squad-validation-1219", "mrqa_squad-validation-1254", "mrqa_squad-validation-1263", "mrqa_squad-validation-133", "mrqa_squad-validation-1330", "mrqa_squad-validation-134", "mrqa_squad-validation-1402", "mrqa_squad-validation-1432", "mrqa_squad-validation-1498", "mrqa_squad-validation-150", "mrqa_squad-validation-151", "mrqa_squad-validation-1565", "mrqa_squad-validation-1612", "mrqa_squad-validation-1640", "mrqa_squad-validation-168", "mrqa_squad-validation-1764", "mrqa_squad-validation-1813", "mrqa_squad-validation-185", "mrqa_squad-validation-185", "mrqa_squad-validation-1891", "mrqa_squad-validation-1980", "mrqa_squad-validation-2018", "mrqa_squad-validation-2085", "mrqa_squad-validation-2085", "mrqa_squad-validation-2109", "mrqa_squad-validation-215", "mrqa_squad-validation-2167", "mrqa_squad-validation-2214", "mrqa_squad-validation-2314", "mrqa_squad-validation-2370", "mrqa_squad-validation-246", "mrqa_squad-validation-2500", "mrqa_squad-validation-2559", "mrqa_squad-validation-2586", "mrqa_squad-validation-2666", "mrqa_squad-validation-2672", "mrqa_squad-validation-269", "mrqa_squad-validation-270", "mrqa_squad-validation-2704", "mrqa_squad-validation-2754", "mrqa_squad-validation-2853", "mrqa_squad-validation-2913", "mrqa_squad-validation-2939", "mrqa_squad-validation-2964", "mrqa_squad-validation-2975", "mrqa_squad-validation-3096", "mrqa_squad-validation-3124", "mrqa_squad-validation-3125", "mrqa_squad-validation-313", "mrqa_squad-validation-3140", "mrqa_squad-validation-3143", "mrqa_squad-validation-3275", "mrqa_squad-validation-3319", "mrqa_squad-validation-336", "mrqa_squad-validation-3370", "mrqa_squad-validation-3407", "mrqa_squad-validation-3492", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-350", "mrqa_squad-validation-3535", "mrqa_squad-validation-3550", "mrqa_squad-validation-36", "mrqa_squad-validation-3667", "mrqa_squad-validation-3740", "mrqa_squad-validation-375", "mrqa_squad-validation-3750", "mrqa_squad-validation-3770", "mrqa_squad-validation-3774", "mrqa_squad-validation-3802", "mrqa_squad-validation-3811", "mrqa_squad-validation-3813", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3904", "mrqa_squad-validation-3937", "mrqa_squad-validation-3939", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-4227", "mrqa_squad-validation-4246", "mrqa_squad-validation-4469", "mrqa_squad-validation-4473", "mrqa_squad-validation-4546", "mrqa_squad-validation-4591", "mrqa_squad-validation-4636", "mrqa_squad-validation-4669", "mrqa_squad-validation-468", "mrqa_squad-validation-4737", "mrqa_squad-validation-4754", "mrqa_squad-validation-4799", "mrqa_squad-validation-4860", "mrqa_squad-validation-4877", "mrqa_squad-validation-4898", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5007", "mrqa_squad-validation-5019", "mrqa_squad-validation-5135", "mrqa_squad-validation-5294", "mrqa_squad-validation-5325", "mrqa_squad-validation-5351", "mrqa_squad-validation-5613", "mrqa_squad-validation-5699", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5716", "mrqa_squad-validation-5721", "mrqa_squad-validation-5724", "mrqa_squad-validation-5797", "mrqa_squad-validation-5875", "mrqa_squad-validation-5933", "mrqa_squad-validation-5961", "mrqa_squad-validation-6025", "mrqa_squad-validation-6089", "mrqa_squad-validation-6116", "mrqa_squad-validation-6157", "mrqa_squad-validation-6159", "mrqa_squad-validation-6163", "mrqa_squad-validation-6341", "mrqa_squad-validation-635", "mrqa_squad-validation-6502", "mrqa_squad-validation-6526", "mrqa_squad-validation-6579", "mrqa_squad-validation-6614", "mrqa_squad-validation-6628", "mrqa_squad-validation-6669", "mrqa_squad-validation-6676", "mrqa_squad-validation-6700", "mrqa_squad-validation-6705", "mrqa_squad-validation-6803", "mrqa_squad-validation-6873", "mrqa_squad-validation-6986", "mrqa_squad-validation-7036", "mrqa_squad-validation-7168", "mrqa_squad-validation-719", "mrqa_squad-validation-7233", "mrqa_squad-validation-7246", "mrqa_squad-validation-7277", "mrqa_squad-validation-7294", "mrqa_squad-validation-735", "mrqa_squad-validation-7354", "mrqa_squad-validation-7367", "mrqa_squad-validation-7384", "mrqa_squad-validation-7428", "mrqa_squad-validation-7456", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-7531", "mrqa_squad-validation-7570", "mrqa_squad-validation-7585", "mrqa_squad-validation-76", "mrqa_squad-validation-7652", "mrqa_squad-validation-7671", "mrqa_squad-validation-7689", "mrqa_squad-validation-7707", "mrqa_squad-validation-7782", "mrqa_squad-validation-7788", "mrqa_squad-validation-780", "mrqa_squad-validation-7805", "mrqa_squad-validation-7838", "mrqa_squad-validation-7909", "mrqa_squad-validation-791", "mrqa_squad-validation-8028", "mrqa_squad-validation-8043", "mrqa_squad-validation-8045", "mrqa_squad-validation-8073", "mrqa_squad-validation-8283", "mrqa_squad-validation-8386", "mrqa_squad-validation-8500", "mrqa_squad-validation-8502", "mrqa_squad-validation-8529", "mrqa_squad-validation-8555", "mrqa_squad-validation-8603", "mrqa_squad-validation-8680", "mrqa_squad-validation-8754", "mrqa_squad-validation-8830", "mrqa_squad-validation-8834", "mrqa_squad-validation-8900", "mrqa_squad-validation-8901", "mrqa_squad-validation-8905", "mrqa_squad-validation-891", "mrqa_squad-validation-8927", "mrqa_squad-validation-8931", "mrqa_squad-validation-8939", "mrqa_squad-validation-8948", "mrqa_squad-validation-8966", "mrqa_squad-validation-8987", "mrqa_squad-validation-9200", "mrqa_squad-validation-9254", "mrqa_squad-validation-9269", "mrqa_squad-validation-9271", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9302", "mrqa_squad-validation-9322", "mrqa_squad-validation-9328", "mrqa_squad-validation-9436", "mrqa_squad-validation-95", "mrqa_squad-validation-9531", "mrqa_squad-validation-9532", "mrqa_squad-validation-9543", "mrqa_squad-validation-9695", "mrqa_squad-validation-9768", "mrqa_squad-validation-9819", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_squad-validation-9931", "mrqa_squad-validation-9989", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1246", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-1670", "mrqa_triviaqa-validation-1800", "mrqa_triviaqa-validation-1873", "mrqa_triviaqa-validation-1976", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-247", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2937", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3278", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-3955", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-4137", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4850", "mrqa_triviaqa-validation-486", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5025", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-5638", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-581", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-5960", "mrqa_triviaqa-validation-5965", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6246", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6734", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-7124", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-828"], "OKR": 0.8359375, "KG": 0.44296875, "before_eval_results": {"predictions": ["2002", "1,548", "American Institute of Electrical Engineers", "a deficit", "18 and 19", "Virgin Media", "in a number of stages", "light energy", "128", "voting directly or elect representatives from among themselves to form a governing body, such as a parliament.", "Mary Ellen Mark", "Australian Defence Force", "test pilot, and businessman.", "1296", "August 23, 1970", "Old Town of Vilnius", "Rounders", "music of pre-Hispanic and contemporary music of the Andes,", "Robert John Day", "Leeds United,", "National Lottery", "The Battle of Prome", "M2M", "Austria's", "Citizens for a Sound Economy", "right-hand", "Heathrow", "Australian", "Darkroom", "House of Commons", "La Familia Michoacana", "five months", "1983", "James Douglas Packer", "Northern Ireland", "Erich Maria Remarque", "playwright", "Floyd Mutrux and Colin Escott.", "poet and fellow rock musician, Patti Smith.", "the Mikoyan design bureau", "American", "A Little Princess", "1943", "TD Garden", "The scarp was first imaged by Voyager 2 spacecraft in January 1986.", "Vixen", "born September 6,", "The Ryukyuan people", "Styx", "J35-A-23", "Axl Rose", "Prudential Center in Newark, New Jersey", "Homer Hickam,", "CBS", "A Turtle's Tale : Sammy's Adventures and the TV show Suburgatory", "of the chemist and physicist,", "1976 in Sweden,", "right-wing extremist groups.", "\"very important that Brazil and the United States work closely in this field,\"", "The Beatles", "a key", "Operation Cast Lead", "future relations between the Middle East and Washington.", "650"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5925414862914863}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.09090909090909091, 0.0, 1.0, 0.0, 0.0, 0.5, 0.8571428571428571, 1.0, 0.2, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6, 1.0, 0.0, 0.0, 1.0, 0.5, 0.8, 1.0, 0.28571428571428575, 0.0, 0.5, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4010", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-5692", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5870", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-2486", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5201", "mrqa_naturalquestions-validation-714", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-3423", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-3865", "mrqa_searchqa-validation-9122", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-571"], "SR": 0.46875, "CSR": 0.5751488095238095, "EFR": 0.9705882352941176, "Overall": 0.7071161589635854}, {"timecode": 21, "before_eval_results": {"predictions": ["1622", "wars", "all age groups", "cabin depressurization", "largest gold rushes the world has ever seen", "1985", "whether the organelle carries out the last leg of the pathway or if it happens in the cytosol", "Sultan Selim II", "Tony Bellew", "A Hard Day's Night", "Lord Chancellor of England", "Ben Elton", "West Cheshire Association Football League", "KB", "1,467", "pioneering New Zealand food writer", "Old Executive Office Building (OEOB)", "\"Mash-Up\"", "John R. Leonetti", "\"The Smurfs\" comic book series", "Martin Louis Amis (born 25 August 1949)", "Harmony Korine", "137th", "Texas Tech University", "Croatan, Nantahala, and Uwharrie", "703", "King George IV and the Duke of Wellington", "Rochdale", "Tudor music", "Northern Ireland", "Pylos and Thebes", "Steve and Rudy", "the Beatles", "Lady Frederick Windsor", "Monica Seles", "2 November 1902", "for the most time in space (381.6 days)", "NYPD's 83rd Precinct", "Patterns of Sexual Behavior", "Peel Holdings", "about 560", "musician", "Theodor W. Adorno", "Gianna", "City of Newcastle", "John Christopher Lujack Jr.", "Genderqueer", "Kiss", "David Jolly", "Minette Walters", "2004", "Tamil", "67,038", "73", "Barbara Windsor", "Stephenie Meyer", "Jane Austen", "Pakistan's intelligence agency", "Michelle Rounds", "A Doll's House", "Florida", "horses", "for buying and selling timepieces at auction", "Mickey's"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6396735209235209}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666665, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.5714285714285715, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.3333333333333333, 0.4444444444444445, 1.0, 1.0, 1.0, 0.6666666666666666, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2987", "mrqa_hotpotqa-validation-1312", "mrqa_hotpotqa-validation-2016", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-5880", "mrqa_hotpotqa-validation-3232", "mrqa_hotpotqa-validation-5317", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-334", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-984", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-5295", "mrqa_hotpotqa-validation-693", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5052", "mrqa_naturalquestions-validation-2159", "mrqa_triviaqa-validation-6256", "mrqa_newsqa-validation-1218", "mrqa_triviaqa-validation-2227", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-7724"], "SR": 0.546875, "CSR": 0.5738636363636364, "EFR": 1.0, "Overall": 0.7127414772727272}, {"timecode": 22, "before_eval_results": {"predictions": ["around $960 billion", "European Court of Human Rights", "he began to suffer from kidney and bladder stones, and arthritis, and an ear infection ruptured an ear drum.", "NP", "Augustinian friars", "1948", "New Orleans", "mafia clan", "Charles Dickens Page - Mrs Gamp", "cowpox", "1984", "ravens", "Alfred Gilbert", "insulin", "bullfight", "17 pink \"double-word\" squares", "12", "The Pennine Way", "Muriel Spark", "basil", "La Mancha", "Martin Van Buren", "Bonnie and Clyde", "three other musicians", "Hillary Clinton", "Gettysburg", "Tom Hanks", "three dots high and two across", "\"sound and light\"", "Panama", "mushrooms", "Harrods", "Usain Bolt", "Mead", "To Kill a Mockingbird", "Che struck Greedo with an idol, covering it with the Rodian's blood.", "Sudan", "Hyperbole", "Russia", "Daleks", "Steve Jobs", "Christmas", "Rajasthan", "in love with a young man called Jenik", "lawn games", "David Hockney", "a compact bone that sits between the calcaneus (heel bone)", "Barnaby Rudge", "Surficial", "1861", "Thomas Jefferson", "Aberystwyth", "around 10 : 30am", "2014 Winter Olympics in Sochi, Russia", "in Seattle, Washington", "Capture of the Five Boroughs", "79 AD", "Wilmette", "Trevor Rees", "last week", "Robert Park", "Sounder", "Syracuse", "4 pecks"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5760093167701863}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false], "QA-F1": [0.8, 1.0, 0.5217391304347826, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6934", "mrqa_squad-validation-2505", "mrqa_triviaqa-validation-3577", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-2366", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-1286", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-3065", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-6681", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-5116", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-6130", "mrqa_triviaqa-validation-6659", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-6192", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-6458", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-7443", "mrqa_hotpotqa-validation-1874", "mrqa_newsqa-validation-2959", "mrqa_searchqa-validation-14932", "mrqa_searchqa-validation-2394"], "SR": 0.53125, "CSR": 0.5720108695652174, "EFR": 0.9666666666666667, "Overall": 0.7057042572463768}, {"timecode": 23, "before_eval_results": {"predictions": ["1759-60", "tyrosinase", "Keraites", "Israelis", "Konstantin Mereschkowski", "disease", "bizet", "Vienna", "May 20, 2003", "pouched", "meat", "spain", "cop", "1985", "The valley Club", "John Steinbeck", "Bob Barker", "beta", "sangersville, Maine", "John Peel", "spain", "Cheshire", "Rebecca Adlington", "fresh fruits and vegetables", "power station", "Manchester City", "Tigris", "jaundice", "spainning the Palazzo Rio, or Palace River,", "cows", "iron", "island countries", "spain", "davers", "Frank Harris", "Charles Atlas", "Alex Kramer", "harrow", "Isle of Wight", "violin", "Roberto Cammarelle", "elephant", "han maystein", "Dick Turpin", "cynthia", "restless leg syndrome", "spleen", "performance enhancing drugs", "pennsylvania state", "pennado Tuerto, Argentina,", "gargantua", "Late Ordovician period", "Pebe Sebert and Hugh Moffatt", "October 29, 2015", "in the season - five premiere episode `` Second Opinion ''", "Leslie Knope", "First Balkan War", "spy", "next year", "heavy turbulence", "shut down buses, subways and trolleys that carry almost a million people daily.", "poland", "Chuck Schumer", "neck"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5112980769230769}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.3333333333333333, 0.923076923076923, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8488", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-3634", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-7057", "mrqa_triviaqa-validation-821", "mrqa_triviaqa-validation-3350", "mrqa_triviaqa-validation-7404", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-7333", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-991", "mrqa_triviaqa-validation-6080", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-6876", "mrqa_naturalquestions-validation-3440", "mrqa_naturalquestions-validation-2818", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-5271", "mrqa_newsqa-validation-1893", "mrqa_searchqa-validation-7791", "mrqa_searchqa-validation-7996"], "SR": 0.453125, "CSR": 0.5670572916666667, "EFR": 1.0, "Overall": 0.7113802083333334}, {"timecode": 24, "before_eval_results": {"predictions": ["a comb jelly", "cicadas", "gaseous oxygen", "no known case of any U.S. citizens buying Canadian drugs for personal use with a prescription,", "9th century", "Teha'amana", "Gerald R. Ford", "gods", "goulue", "Aerosmith", "Sunday", "drew", "saddle oxfords", "NAFTA", "the earth", "jimmy johnson", "lyndon johnson", "venial sin", "Toronto", "upper devolved", "wolves", "fish", "ricky martin", "Alfred Nobel", "Smith & Wesson", "700", "Emma Watson", "Pan Am", "Anne of Austria", "1863", "roof", "oll korrect", "Nikita Sergeyevich Khrushchev", "caleslaw", "lawyer", "diamonds", "lyndon johnson", "morphine", "Mars", "ice age", "hand", "lyndon johnson", "lorin farr", "an American Tail", "pelican", "Specialist", "mountains", "Hofburg Palace", "lyndon johnson", "Solomon", "Death Valley", "lyndon johnson", "Wembley Stadium", "Mike Higham", "MGM Resorts International", "Olympic Games", "bats", "sodium", "Ferdinand Magellan", "November 23, 2011", "\"Famous Ghost Stories,\"", "police", "haitians", "leaders of more than 30 Latin American and Caribbean nations"], "metric_results": {"EM": 0.359375, "QA-F1": 0.392578125}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.125]}}, "before_error_ids": ["mrqa_squad-validation-6383", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-5385", "mrqa_searchqa-validation-1375", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-16821", "mrqa_searchqa-validation-12291", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-11213", "mrqa_searchqa-validation-11414", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-1004", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-7094", "mrqa_searchqa-validation-9261", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-4798", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-9698", "mrqa_searchqa-validation-8944", "mrqa_searchqa-validation-13740", "mrqa_searchqa-validation-263", "mrqa_searchqa-validation-6770", "mrqa_searchqa-validation-16246", "mrqa_searchqa-validation-10911", "mrqa_naturalquestions-validation-9773", "mrqa_triviaqa-validation-2047", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6439", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-4468", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-2225"], "SR": 0.359375, "CSR": 0.5587500000000001, "EFR": 1.0, "Overall": 0.70971875}, {"timecode": 25, "before_eval_results": {"predictions": ["Khitan rulers", "over 100%", "vaccination", "natural grass stadiums", "three", "Zuma", "\"full civil equality,\"", "a lump in Henry's nether regions was a cancerous tumor.", "a bag", "The people kill him with the blocks,", "felony drug charges", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "\"a potential hazard may occur due to re-entry of satellite debris into the earth's atmosphere.\"", "Weland", "three different videos", "D, E or F.", "stand down.", "hot and humid", "KBR", "Mother Nature", "Twilight", "private client", "Venus Williams", "Booches Billiard Hall,", "Peshawar", "23-year-old", "three", "creating and distributing affordable, durable and solar-powered laptops to the world's poorest children.", "\"There is no silver bullet and no quick, cheap or easy solutions.", "Sovereign Wealth Funds", "Besson", "Department of Homeland Security Secretary Janet Napolitano", "prison inmates.", "\"Larry King Live\"", "human rights abuses", "hours", "a full garden and pool, a tennis court, or several heli-pads.", "Bill", "Marcus", "Caylee", "iTunes", "The Screening Room", "five days a week.", "secure more funds from the region.", "upper respiratory infection", "\"It hurts my heart to see him in pain,", "Brown-Waite", "Shanghai", "a botched robbery that left an off-duty New York police officer dead.", "the Scudetto", "Zimbabwe's embassy in Washington.", "In the year 2026", "offered it to Adam Faith and Brian Poole", "the life of the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "Captain Mark Phillips", "Esther Hannaford", "Bruce Alexander", "vice president", "Pittsburgh Steelers", "Pearl Harbor", "Star Trek", "Chris Matthews", "books"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5245061678943258}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.4, 0.47619047619047616, 0.0, 0.9600000000000001, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.14814814814814814, 0.1111111111111111, 1.0, 0.4, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.7692307692307693, 1.0, 0.4736842105263158, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.9189189189189189, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-448", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-1137", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-7896", "mrqa_triviaqa-validation-1508", "mrqa_triviaqa-validation-4087", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3375"], "SR": 0.359375, "CSR": 0.5510817307692308, "EFR": 1.0, "Overall": 0.7081850961538462}, {"timecode": 26, "before_eval_results": {"predictions": ["David G. Booth", "pr\u00e9tendus r\u00e9form\u00e9s", "the true Islamic system", "$20,000", "intention to set up headquarters in Dublin.", "requiring the label warnings and a medication guide for fluoroquinolone drugs,", "Mississippi", "Venus Williams", "Elisabeth", "late Tuesday night,", "North Korea has positioned what is thought to be a long-range missile on its launch pad,", "\"exceptional circumstances surround these memos and require their release.\"", "Ricardo Urbina", "\"bad apples\"", "Christopher Savoie", "near Garacad, Somalia,", "allergies in general -- both food and inhalant -- are on the rise,", "depression", "Doogie Howser, M.D.", "preserved corpses having sex", "14", "Kenneth Brown", "as many as 50,000", "U.S. State Department and British Foreign Office", "the two remaining crew members", "an occupied building.", "up to $50,000", "Arthur E. Morgan III,", "Caster Semenya", "Tom Hanks", "a baseball bat", "J. Crew", "$60 million", "Iran test-launched a rocket capable of carrying a satellite,", "three", "Gov. Mark Sanford", "gun", "$1.5 million", "off the coast of Dubai", "Davidson college students", "Diego Milito", "upper respiratory infection", "officers at a Texas  airport appear to have properly followed procedures", "McDonald's", "the apartment building collapsed together with two other buildings on March 3.", "gang rape", "leftist Workers' Party.", "was arrested last week as they tried to put the children on a plane to France,", "full garden and pool, a tennis court,", "October 3,", "Chinese ships", "said the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.\"", "the head of the Imperial Family and the traditional head of state of Japan", "1998", "November 2016", "sewing machines", "exploits on the Island", "Duncan", "estimated half a million acres", "hiphop", "four", "Stanford", "a rough, broken, projecting part", "nickel"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5150507292918818}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.06896551724137932, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.2727272727272727, 0.0, 0.5, 0.0, 0.8, 0.6666666666666666, 0.0, 0.9714285714285714, 0.5263157894736842, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-3693", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-3307", "mrqa_newsqa-validation-1283", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-9119", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-7585", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5002", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-10210"], "SR": 0.359375, "CSR": 0.5439814814814814, "EFR": 1.0, "Overall": 0.7067650462962962}, {"timecode": 27, "before_eval_results": {"predictions": ["theoretical", "its own culture and atmosphere,", "Labor", "Children of Earth", "fractured pelvis and sacrum -- the triangular bone within the pelvis.", "the cojones to ask for it back now.", "133", "one count of attempted murder in the second degree", "Sri Lanka", "Muslim", "a man walked through an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening,", "200", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "innovative, exciting skyscrapers set to appear all over the world over the next 10 years.", "Egyptian security forces", "1831", "Glasgow, Scotland", "10 to 15 percent", "Kearny, New Jersey.", "19-12", "boats are expected to arrive in Veracruz, Mexico,", "a number of calls,", "Mark Obama Ndesandjo", "Sotomayor", "The great paddlewheel", "15", "opposition parties", "July 23.", "20 years of research and four years to find the right patient who understood the risks involved.", "citizenship", "\"I am sick of life -- what can I say to you?\"", "Gustav's top winds weakened to 110 mph,", "the 11th anniversary of the September 11, 2001, terror attacks.", "a curfew", "Omar bin Laden", "Tim Masters", "surgical anesthetic propofol", "Peshawar", "Pixar", "The Da Vinci Code", "martial arts", "a spurned suitor.", "producing rock music with a country influence.", "American president toured a mosque, laid a wreath at the grave of the founder of the Turkish republic,", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "$8.8 million", "near his home in Peshawar", "the refusal or inability to \"turn it off\"", "has been fighting charges of Nazi war crimes for well over two decades.", "Minerals Management Service Director Elizabeth Birnbaum", "$420,000", "U.S. ship that was hijacked off Somalia's coast.", "the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden", "1994", "disputes between two or more states", "a pen", "Maria Em\u00edlia", "Lesley Garrett", "Denmark", "the tissues of the outer third of the vagina", "Mark Neveldine and Brian Taylor.", "Ulysses S. Grant", "Kevin Bacon", "yellow"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4884592300879449}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, true, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.375, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.4444444444444445, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.08, 0.0, 1.0, 1.0, 1.0, 0.631578947368421, 0.5, 0.0, 0.5454545454545454, 0.8620689655172413, 0.5, 0.8, 0.0, 0.0, 1.0, 0.5, 0.1818181818181818, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1704", "mrqa_squad-validation-2640", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2215", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-1028", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-8092", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-2763", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-4194", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420"], "SR": 0.34375, "CSR": 0.5368303571428572, "EFR": 0.9761904761904762, "Overall": 0.7005729166666667}, {"timecode": 28, "before_eval_results": {"predictions": ["Ex post facto laws,", "temperature and light", "\"The particles in the beam of force... will travel much faster than such particles... and they will travel in concentrations.\"", "Tens of thousands of new voters became the key to his Iowa win and revealed the outline of a general election plan: Create a wide coalition to bring new voters to the polls in record numbers.", "Hillary Clinton", "volatile", "from an older generation", "Ken Plunkett,", "800,000", "Monday and Tuesday holidays", "a full garden and pool, a tennis court, or several heli-pads.", "\"I deal with families who lose their babies and I will cry with them, but I thought I would be stronger.", "The rebels have been fighting for an independent homeland for the country's ethnic Tamil minority since 1983.", "Tuesday night", "\"aware of what she's done and she's very sorry for it.\"", "prisoners", "Ashley \"A.J. Jewell,", "environmental and political events.", "Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "Iran of trying to build nuclear bombs,", "The U.S. State Department and British Foreign Office", "former U.S. secretary of state.", "misdemeanor assault charges", "Venezuela's", "Argentine", "your ex's loved ones ask why", "\"Piers Morgan Tonight\"", "$50,000", "Leo Frank", "Osama's son,", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "Iran's parliament speaker", "the National September 11 Memorial Museum", "St. Louis, Missouri.", "hand-painted Swedish wooden clogs", "Brazil's efforts to reverse the tide of the AIDS epidemic have become the object of admiration in the global health community,", "Pope Benedict XVI", "\"cliff effect.\"", "three searches", "Basel", "45 astride his horse in the snow at Valley Forge.", "first grand Slam,", "suppress the memories and to live as normal a life as possible;", "$1.4 million", "Jennifer Arnold and husband Bill Klein,", "murder", "role as a bride in the 2007 movie \"License to Wed\"", "The UNHCR recommended against granting asylum,", "2009", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "\"Five members of the Mohler family of Lafayette County, Missouri,", "Robert Barnett", "Javier Fern\u00e1ndez", "4 in ( 10 cm )", "the date of the widely publicized Scopes Trial in the United States,", "Patrick Chukwuemeka Okogwu", "Petula Clark", "Lord Snooty", "Roman Kostomarov", "August 1973", "ten episodes", "Algeria", "Los Angeles Times", "George Orwell"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5365211260160326}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.09999999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.5263157894736842, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.06451612903225808, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.923076923076923, 1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.30769230769230765, 0.6666666666666666, 0.0, 0.6666666666666666, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4096", "mrqa_squad-validation-1389", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-1316", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2291", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-636", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-2214", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1663", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2030", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6665", "mrqa_hotpotqa-validation-1824"], "SR": 0.421875, "CSR": 0.5328663793103448, "EFR": 1.0, "Overall": 0.7045420258620689}, {"timecode": 29, "before_eval_results": {"predictions": ["Through experimentation", "peer tuitions", "six", "\"It was a white plate with a \"lone", "Tennyson", "1087", "Francis Drake", "\"Svevo & Tozzi\"", "Shirley Schmidt", "aluminum", "timbers, piles or stoelwork", "Soviet", "The greater kudu", "South America", "Idi Amin", "will", "pink", "Tom", "anemia", "\"The Wizard of Oz\"", "African Cichlids", "Peter", "Vienna.", "Russian Empire", "\"a contest to see which candidate can answer the fewest questions\"", "1 billionth of a second", "Citizen Kane", "\"The Stag\"", "\"Jolly Roger\"", "bears", "Shirley Jackson", "JetBlue", "a \"Uncle Miltie\"", "a shot glass", "Peru", "the Phantom of the Opera", "an eye", "Holy Roman Empire", "Hawaii", "Bob", "Shorthand", "Sean John", "carrots", "Utah", "an allergic reaction to monosodium", "a black and white tuxedo cat", "The tooth of Crime, La Turista, Tongues, Savage / Love (Faber Contemporary Classics)", "oil", "August Wilson", "Dr. Jack Shephard, Kate Austen, Sayid Jarrah, Hugo \" Hurley", "bay leaf", "Blue Nile", "the five states", "Roman Reigns", "Mexican Seismic Alert System", "violin", "heelflip", "Simeon Williamson", "11 November 1869", "Ten Walls", "Nia Kay", "new kidney", "surgical anesthetic propofol", "the deployment of unmanned drones, including possibly the Predator drones used in Iraq and Afghanistan."], "metric_results": {"EM": 0.453125, "QA-F1": 0.5126021241830065}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_searchqa-validation-13463", "mrqa_searchqa-validation-1839", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-41", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-14158", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-13489", "mrqa_searchqa-validation-14060", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-12860", "mrqa_searchqa-validation-9533", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-16829", "mrqa_searchqa-validation-13176", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-8784", "mrqa_searchqa-validation-14066", "mrqa_searchqa-validation-16060", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-975", "mrqa_triviaqa-validation-2394", "mrqa_triviaqa-validation-6791", "mrqa_hotpotqa-validation-4819", "mrqa_newsqa-validation-1445"], "SR": 0.453125, "CSR": 0.5302083333333334, "EFR": 0.9714285714285714, "Overall": 0.698296130952381}, {"timecode": 30, "UKR": 0.712890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-1484", "mrqa_hotpotqa-validation-1574", "mrqa_hotpotqa-validation-1750", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1912", "mrqa_hotpotqa-validation-1914", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-2217", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2281", "mrqa_hotpotqa-validation-2310", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2440", "mrqa_hotpotqa-validation-2456", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2605", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3299", "mrqa_hotpotqa-validation-3310", "mrqa_hotpotqa-validation-3339", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4182", "mrqa_hotpotqa-validation-4189", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-5002", "mrqa_hotpotqa-validation-5034", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-5366", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5880", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-748", "mrqa_hotpotqa-validation-872", "mrqa_hotpotqa-validation-918", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2250", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-476", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4876", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-5390", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-975", "mrqa_naturalquestions-validation-9773", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2016", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-3083", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3154", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-456", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-86", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10395", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-11414", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-12104", "mrqa_searchqa-validation-12242", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-1265", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12897", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-13321", "mrqa_searchqa-validation-14752", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14932", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-15244", "mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-1595", "mrqa_searchqa-validation-16258", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1696", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1773", "mrqa_searchqa-validation-1926", "mrqa_searchqa-validation-2095", "mrqa_searchqa-validation-2329", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-5204", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5338", "mrqa_searchqa-validation-5478", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-6770", "mrqa_searchqa-validation-6958", "mrqa_searchqa-validation-7938", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8791", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-9391", "mrqa_searchqa-validation-9683", "mrqa_squad-validation-100", "mrqa_squad-validation-10001", "mrqa_squad-validation-1002", "mrqa_squad-validation-10063", "mrqa_squad-validation-10174", "mrqa_squad-validation-10186", "mrqa_squad-validation-10256", "mrqa_squad-validation-10316", "mrqa_squad-validation-10370", "mrqa_squad-validation-10386", "mrqa_squad-validation-10457", "mrqa_squad-validation-10463", "mrqa_squad-validation-10470", "mrqa_squad-validation-1078", "mrqa_squad-validation-1177", "mrqa_squad-validation-1219", "mrqa_squad-validation-1263", "mrqa_squad-validation-133", "mrqa_squad-validation-1330", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1498", "mrqa_squad-validation-150", "mrqa_squad-validation-151", "mrqa_squad-validation-1612", "mrqa_squad-validation-1613", "mrqa_squad-validation-168", "mrqa_squad-validation-175", "mrqa_squad-validation-1759", "mrqa_squad-validation-1779", "mrqa_squad-validation-1813", "mrqa_squad-validation-1891", "mrqa_squad-validation-1980", "mrqa_squad-validation-2018", "mrqa_squad-validation-2085", "mrqa_squad-validation-2085", "mrqa_squad-validation-2109", "mrqa_squad-validation-2129", "mrqa_squad-validation-2167", "mrqa_squad-validation-2214", "mrqa_squad-validation-2302", "mrqa_squad-validation-246", "mrqa_squad-validation-2586", "mrqa_squad-validation-2666", "mrqa_squad-validation-270", "mrqa_squad-validation-2704", "mrqa_squad-validation-2754", "mrqa_squad-validation-2913", "mrqa_squad-validation-2939", "mrqa_squad-validation-2975", "mrqa_squad-validation-3037", "mrqa_squad-validation-313", "mrqa_squad-validation-3213", "mrqa_squad-validation-3370", "mrqa_squad-validation-3479", "mrqa_squad-validation-350", "mrqa_squad-validation-3535", "mrqa_squad-validation-3581", "mrqa_squad-validation-3667", "mrqa_squad-validation-3740", "mrqa_squad-validation-375", "mrqa_squad-validation-3750", "mrqa_squad-validation-3811", "mrqa_squad-validation-3842", "mrqa_squad-validation-385", "mrqa_squad-validation-3922", "mrqa_squad-validation-3937", "mrqa_squad-validation-3939", "mrqa_squad-validation-4096", "mrqa_squad-validation-4179", "mrqa_squad-validation-4246", "mrqa_squad-validation-4292", "mrqa_squad-validation-4469", "mrqa_squad-validation-4591", "mrqa_squad-validation-4669", "mrqa_squad-validation-468", "mrqa_squad-validation-4860", "mrqa_squad-validation-4877", "mrqa_squad-validation-4898", "mrqa_squad-validation-5007", "mrqa_squad-validation-5030", "mrqa_squad-validation-5042", "mrqa_squad-validation-5135", "mrqa_squad-validation-5325", "mrqa_squad-validation-5351", "mrqa_squad-validation-5613", "mrqa_squad-validation-5699", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5716", "mrqa_squad-validation-5721", "mrqa_squad-validation-5809", "mrqa_squad-validation-5875", "mrqa_squad-validation-5933", "mrqa_squad-validation-6025", "mrqa_squad-validation-6116", "mrqa_squad-validation-6157", "mrqa_squad-validation-6163", "mrqa_squad-validation-6274", "mrqa_squad-validation-6341", "mrqa_squad-validation-6383", "mrqa_squad-validation-6579", "mrqa_squad-validation-6676", "mrqa_squad-validation-6700", "mrqa_squad-validation-6705", "mrqa_squad-validation-6803", "mrqa_squad-validation-7036", "mrqa_squad-validation-7168", "mrqa_squad-validation-7233", "mrqa_squad-validation-7233", "mrqa_squad-validation-7246", "mrqa_squad-validation-7294", "mrqa_squad-validation-735", "mrqa_squad-validation-7354", "mrqa_squad-validation-7367", "mrqa_squad-validation-7428", "mrqa_squad-validation-7491", "mrqa_squad-validation-7516", "mrqa_squad-validation-7531", "mrqa_squad-validation-7531", "mrqa_squad-validation-7570", "mrqa_squad-validation-7707", "mrqa_squad-validation-7782", "mrqa_squad-validation-7788", "mrqa_squad-validation-780", "mrqa_squad-validation-7805", "mrqa_squad-validation-7838", "mrqa_squad-validation-7909", "mrqa_squad-validation-8386", "mrqa_squad-validation-850", "mrqa_squad-validation-8500", "mrqa_squad-validation-8502", "mrqa_squad-validation-8523", "mrqa_squad-validation-8555", "mrqa_squad-validation-8601", "mrqa_squad-validation-8603", "mrqa_squad-validation-8680", "mrqa_squad-validation-8861", "mrqa_squad-validation-8900", "mrqa_squad-validation-8901", "mrqa_squad-validation-8905", "mrqa_squad-validation-8927", "mrqa_squad-validation-8931", "mrqa_squad-validation-8948", "mrqa_squad-validation-8966", "mrqa_squad-validation-9254", "mrqa_squad-validation-9269", "mrqa_squad-validation-9271", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9302", "mrqa_squad-validation-9322", "mrqa_squad-validation-9328", "mrqa_squad-validation-9412", "mrqa_squad-validation-9436", "mrqa_squad-validation-95", "mrqa_squad-validation-9590", "mrqa_squad-validation-9695", "mrqa_squad-validation-9901", "mrqa_squad-validation-9989", "mrqa_triviaqa-validation-1052", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1427", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1840", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-247", "mrqa_triviaqa-validation-2937", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3045", "mrqa_triviaqa-validation-3065", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3577", "mrqa_triviaqa-validation-3657", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3803", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4137", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4430", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-486", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-5226", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-5965", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6659", "mrqa_triviaqa-validation-6698", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6784", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7404", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7613"], "OKR": 0.8359375, "KG": 0.44765625, "before_eval_results": {"predictions": ["The Ozone case", "coronary thrombosis", "a Qutb", "ANNA", "Roy Ellsworth Harris", "the head of Medusa", "The Tin Drum", "kerosene", "England", "Shirley Temple", "Flagellant", "(H) Humphrey", "Bart Simpson", "Wesley Clark", "Frasier", "Hispanic heritage", "meat", "Jeremy Brett", "amyotrophic lateral sclerosis", "Bill Clinton", "Emily Dickinson", "Mexico", "cosmology", "a trace of copper", "English", "decoupage", "The Great American Novel", "Arethusa", "(ALA)", "Bucharest", "Down syndrome", "manager Joe Girardi", "a leavening agent", "(2)", "So Long", "insulin", "a Ninja", "The Winds of War", "a strawberry", "Bill Murray", "Deneb IV", "Anacondas", "the Beagle", "a Knesset", "Administrative Professionals Day", "Time", "Dante's Inferio", "a calves", "Lulu Kennedy-Cairns", "\" Down by the Sally Gardens\"", "eulogy", "William Todd Dahl", "Massachusetts", "third", "the leaves of the plant species", "Amy", "alpestrine", "Michael Sheen", "St. Patrick's Day in 1988", "Kona", "Santiago Herrera", "three", "Secretary of State", "Fernando Torres"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5869791666666667}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, true, false, true, true, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8719", "mrqa_searchqa-validation-14778", "mrqa_searchqa-validation-15141", "mrqa_searchqa-validation-3010", "mrqa_searchqa-validation-15830", "mrqa_searchqa-validation-3606", "mrqa_searchqa-validation-12276", "mrqa_searchqa-validation-1744", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-875", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-12346", "mrqa_searchqa-validation-9336", "mrqa_searchqa-validation-2416", "mrqa_searchqa-validation-16391", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-349", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-3032", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-2083", "mrqa_searchqa-validation-15784", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-16220", "mrqa_searchqa-validation-3267", "mrqa_searchqa-validation-16274", "mrqa_searchqa-validation-4325", "mrqa_searchqa-validation-8372", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-9172", "mrqa_triviaqa-validation-4167", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5627"], "SR": 0.4375, "CSR": 0.5272177419354839, "EFR": 1.0, "Overall": 0.7047404233870969}, {"timecode": 31, "before_eval_results": {"predictions": ["corrosion", "steamboats", "imperfect", "(Col. Eli Lilly)", "Manhattan Project", "Maryland", "the Scotch Egg", "carioca", "(D Dale)hardt Jr.", "(John) Pauls", "Ford Madox Ford", "the Cuyahoga River", "Barney Miller.", "(John) Lovell", "a coyote", "terminal", "the sun", "satin", "the Air Force Academy", "(Harry) Lime", "shrewd", "the nucleus", "Don Juan", "Texas", "the Computing-Tabulating-Recording Company", "Chuck Yeager", "CIA", "Shahjahanabad", "pachydermal", "the Wadi Hanifah valley", "Law", "goat milk", "Billy Idol", "anaphylaxis", "copper", "the Ropers", "Bank of America", "the Lampoon", "Terry Bradshaw", "Florence", "farce", "parasites.", "Columbia University", "Evita Peron", "Don Quixote", "Medium", "Seattle", "insulin", "a dilettante", "the Pilcro Hyphen", "Ricky Martin", "(Seth) Russell", "1989", "sea water and fresh water", "Toronto City Airport", "(Napoleon) Bonaparte", "a power factor", "Frederick Forsyth", "Daniil Shafran", "67,575", "various names", "a Christian farmer", "At least 13", "Florida's Everglades."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6315137987012986}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-757", "mrqa_searchqa-validation-9215", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-2914", "mrqa_searchqa-validation-9740", "mrqa_searchqa-validation-10027", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-12607", "mrqa_searchqa-validation-16219", "mrqa_searchqa-validation-4764", "mrqa_searchqa-validation-7211", "mrqa_searchqa-validation-14766", "mrqa_searchqa-validation-9617", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-13909", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-3155", "mrqa_searchqa-validation-12978", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-2405", "mrqa_searchqa-validation-11232", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-8628", "mrqa_triviaqa-validation-5038", "mrqa_newsqa-validation-3090", "mrqa_newsqa-validation-3911"], "SR": 0.546875, "CSR": 0.52783203125, "EFR": 1.0, "Overall": 0.70486328125}, {"timecode": 32, "before_eval_results": {"predictions": ["Eric Roberts.", "the Department for Culture, Media and Sport.", "wigs", "John Sevier", "Wayne Gretzky", "Boris Godunov", "Williamsburg", "Pitcairn", "air dominance", "Halloween", "port-wine", "hurricane", "The Producers", "one foot", "the American Automobile Association", "Jutland", "a mutual fund", "the Two Sicilies", "1773", "Zeus", "a thick cream soup", "4 cups", "Minnesota", "Violeta Barrios de Chamorro", "Alisa Hamilton", "Pillsbury", "oxygen", "The Last Mimzy", "jeans", "Jenna Bush", "Jonathan Demme", "silver", "the North Atlantic", "the ear", "Patti LaBelle", "Cape Cod", "dermal", "febreze", "the Exxon Valdez oil spill", "ice age", "the Army", "Polish", "Jim Brown", "Orleans", "Mars", "the B-47 Stratofortress", "copper", "Tom Ridge", "George Babbitt", "Meg Tilly", "Jeopardy", "imperative", "18", "George Halas", "Manhattan", "the Cheshire Cat", "Islam", "Kent", "Gareth Jones", "Prince Sung-won", "Newcastle upon Tyne, England", "an upper respiratory infection", "Dan Parris, 25, and Rob Lehr, 26,", "2,000 euros"], "metric_results": {"EM": 0.53125, "QA-F1": 0.60625}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_searchqa-validation-16822", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-13952", "mrqa_searchqa-validation-5519", "mrqa_searchqa-validation-8665", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-6114", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-9956", "mrqa_searchqa-validation-13050", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-16141", "mrqa_searchqa-validation-7358", "mrqa_searchqa-validation-11313", "mrqa_searchqa-validation-2960", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-12784", "mrqa_naturalquestions-validation-1223", "mrqa_naturalquestions-validation-5282", "mrqa_triviaqa-validation-7757", "mrqa_hotpotqa-validation-3971", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2730"], "SR": 0.53125, "CSR": 0.5279356060606061, "EFR": 1.0, "Overall": 0.7048839962121212}, {"timecode": 33, "before_eval_results": {"predictions": ["2100", "Apollo 5", "Benazir Bhutto", "the Democratic VP candidate", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "the i report form", "Muqtada al-Sadr", "@", "\"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\"", "iPod Touch", "success as a recording artist", "Department of Homeland Security Secretary Janet Napolitano", "cancer-causing toxic chemical.", "citizenship", "10,000", "4,000", "CNN/Opinion Research Corporation", "California", "software.", "Ralph Cifaretto", "\"Swingin' Down the Lane.\"", "Two", "hacker groups anonymous and LulzSec.", "Transportation Security Administration", "Maude", "was killed", "Nineteen", "weight-loss", "1918", "Liza", "Turkey", "the Cowardly Lion", "graduate from this school district.\"", "Gary Player", "capital murder and three counts of attempted murder", "Nicole", "guard in the jails", "a \"prostitute\"", "Itawamba County School District", "40", "April 22.", "bartering -- trading goods and services without exchanging money", "50,000", "Nairobi, Kenya", "the war years", "Silicon Valley.", "between 1917 and 1924", "June 6, 1944", "giving birth to baby daughter Jada,", "Seoul", "don't have to visit laundromats", "Metro transit trains that crashed the day before, killing nine", "Charlene Holt", "plantar flexing the foot at the ankle joint", "Miami Heat", "4", "Finch", "architect", "the Secret Intelligence Service", "Ready Player One", "alcoholic drinks", "Mars", "crushable", "Tartarus"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5702858339577089}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, false, true, false, false, true, true, true, false, false, false, false, true, false, false, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 0.5454545454545454, 0.3333333333333333, 1.0, 0.0, 1.0, 0.5, 1.0, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.2857142857142857, 1.0, 0.2222222222222222, 0.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 0.625, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-850", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-3020", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-1288", "mrqa_naturalquestions-validation-7609", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-4032", "mrqa_triviaqa-validation-3514", "mrqa_hotpotqa-validation-1791", "mrqa_searchqa-validation-201"], "SR": 0.46875, "CSR": 0.5261948529411764, "EFR": 1.0, "Overall": 0.7045358455882353}, {"timecode": 34, "before_eval_results": {"predictions": ["for complicity and to Odinga declaring himself the \"people's president\"", "KGPE", "Maseru", "John Kevin Delaney", "1979", "The Catholic Church", "Trey Parker and Matt Stone", "Canterbury", "310", "Hungary and Bohemia", "Czech (Bohemian) and German (Franconian)", "people working in film and the performing arts", "McLean, Virginia,", "Sophie Winkleman", "Liverpool and England international player Alex Oxlade-Chamberlain", "Sam the Sham", "coaxial", "Graham Hill", "Marika Nicolette Green", "Katherine Harris", "Frank Thomas' Big Hurt", "World War I", "John Richard Schlesinger,", "Nikolai Trubetzkoy", "3,500,000", "Rabies", "Big Bad Wolf", "Shakespeare's play of the same name.", "1974", "George Orwell", "July 8, 2014", "Lehmber Hussainpuri", "Costa del Sol", "Gabriel Iglesias", "Kolkata", "two", "Roscoe Lee Browne", "23", "video game", "a Peach", "U.S.", "Joe Frazier", "January 2001", "1984 in Kolkata", "Drunken Master II", "San Francisco, California", "\"The Brothers\"", "Daniel Espinosa,", "Adam Levine, Blake Shelton, and Pharrell Williams", "Benjamin Andrew \" Ben\" Stokes", "Duke University", "Royal Albert Hall and The Kennedy Center.", "Louis XV", "2,140 kilometres ( 1,330 mi )", "`` king ''", "Munich", "Admiral Van Galen", "a \u201cstupid term\u201d", "Myanmar", "Authorities in Fayetteville, North Carolina,", "pipelines and hostage-taking", "\"Like a Rolling Stone\"", "Nick", "Alexander Gardner"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5907271241830065}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, true, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, true, false, false], "QA-F1": [0.11764705882352941, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8422", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-4780", "mrqa_hotpotqa-validation-386", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-4729", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-1634", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-1199", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-3889", "mrqa_hotpotqa-validation-3576", "mrqa_naturalquestions-validation-10354", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-4748", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-3428", "mrqa_searchqa-validation-2753", "mrqa_searchqa-validation-15973"], "SR": 0.46875, "CSR": 0.5245535714285714, "EFR": 1.0, "Overall": 0.7042075892857143}, {"timecode": 35, "before_eval_results": {"predictions": ["downward pressure on wages", "ABC Television Center", "downtown Cincinnati", "North Sea", "Headless Body in Topless Bar", "\"Gliding Dance of the Maidens\"", "Patti Smith", "\"Darconville\u2019s Cat\"", "\"Kitty Hawk\"", "Tiberius", "the Netherlands", "Eva Ibbotson", "in the 70 m and 90 m events", "Charlie Puth", "\"Danger Mouse\"", "Don DeLillo", "the crossroads of the Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide.", "comedy", "1997", "University of Southern California (USC) Trojans", "New York City", "extreme nationalist, and nativist", "1st Marquess of Westminster", "Christopher McCulloch", "Marigold Newey", "an album", "The Wachowskis", "a fantasy role-playing game", "WikiLeaks", "Liga MX", "Ramsey County", "a fantasy role-playing game", "New Jersey", "Leinster", "Giacomo Puccini", "Thomas Jefferson", "Winchester", "his virtuoso playing techniques and compositions in orchestral fusion", "1912", "Eisenhower Executive Office Building", "Five Summer Stories", "\"American Chopper\"", "the first and second segment", "King James II", "Hindi", "Netherlands", "soccer", "in Kings Point, New York", "a Ballon d'Or", "The Westminster system", "Bruce Grobbelaar", "The Charkhi Dadri mid-air collision", "on Christmas Eve", "`` Mirror Image ''", "79", "Alaska", "football", "PDSA", "in an effort to make the animals' lives as natural as possible.\"", "Kurt Cobain", "paid tribute to pop legend Michael Jackson,", "the Mediterranean", "sedimentary", "Chekhov"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6595296451914099}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8333333333333333, 0.0, 0.0, 0.6666666666666666, 0.23529411764705882, 1.0, 0.0, 0.8, 1.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7182", "mrqa_hotpotqa-validation-3216", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-455", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-3767", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-2257", "mrqa_naturalquestions-validation-4338", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1352", "mrqa_searchqa-validation-6708"], "SR": 0.5625, "CSR": 0.5256076388888888, "EFR": 1.0, "Overall": 0.7044184027777778}, {"timecode": 36, "before_eval_results": {"predictions": ["a phylum of animals that live in marine waters worldwide", "The Walther P38", "the Commanding General", "Veneto region of Northern Italy", "various deities, beings, and heroes derived from numerous sources from both before and after the pagan period,", "a role-playing game or wargame campaign", "Wandsworth, London", "1967", "Washington, D.C.", "capital crimes", "around 8000 BC", "Smoothie King Center", "Nan Britton", "2013", "9\u201310 March 1945", "Currer Bell", "Jim Davis", "Theodore Robert Bundy", "Crawley Town", "Parlophone", "The town is the setting of the best-selling memoir \"October Sky\" by Homer Hickam", "Life Is a Minestrone", "Louis Silvie \"Louie\" Zamperini", "English professional footballer", "New York Shakespeare Festival", "Sphagnum", "South Australia", "Apple Lisa", "George Gordon Byron", "Singapore", "1853", "Big Bad Wolf", "seven nights a week 1:00 a.m. Eastern Time Zone", "Mickey Mouser", "Manchester\u2013Boston Regional Airport", "Christopher Francis \"Frank\" Ocean", "Gangsta's Paradise", "Ryan Guno Babel (] ; born 19 December 1986) is a Dutch footballer who plays for Turkish club Be\u015fikta\u015f.", "in most casinos are commonly called casino games", "331", "Kristina Ceyton and Kristian Moliere", "1835", "Helsinki", "Imelda Marcos", "Carrefour", "2014", "Empire Falls", "Wet 'n Wild Orlando", "North Queensland", "The iPhone 5", "203 people", "The Caucasus Mountains", "on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "Southwest Atlanta Christian Academy", "the burrowing owl", "Billie Holiday", "The Lincolnshire (sage) sausage", "low-calorie", "American Civil Liberties Union", "three of the bombers", "The Grasshopper and the Ants", "Spider-Man 2", "Cynthia Nixon", "France"], "metric_results": {"EM": 0.46875, "QA-F1": 0.605076433982684}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, false, true, true, true, false, false, true, false, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.47619047619047616, 0.25, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5454545454545454, 0.4, 1.0, 0.6666666666666666, 1.0, 0.14285714285714288, 0.18181818181818182, 0.6666666666666666, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4534", "mrqa_hotpotqa-validation-4576", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-5563", "mrqa_hotpotqa-validation-3301", "mrqa_hotpotqa-validation-3581", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-1606", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-5562", "mrqa_hotpotqa-validation-614", "mrqa_naturalquestions-validation-4653", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-7454", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-891", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-3295"], "SR": 0.46875, "CSR": 0.5240709459459459, "EFR": 1.0, "Overall": 0.7041110641891892}, {"timecode": 37, "before_eval_results": {"predictions": ["its many castles and vineyards", "SSM Cardinal Glennon Children's Medical Center in St. Louis.", "because I got back to Oklahoma and raise my girls. Sandy and I were getting a divorce at the time,", "The lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains poses a challenge", "will American go bankrupt?", "Chadian President Idriss Deby", "The tall 34-year-old, slouching exhausted in a Johannesburg church that has become a de facto transit camp,", "President Paul Biya,", "Herman Cain", "Zulfikar Ali Bhutto,", "money or other discreet aid for the effort if it could be made available,", "News of the World tabloid.", "The Falklands, known as Las Malvinas in Argentina, lie in the South Atlantic Ocean off the Argentinean coast and have been under British rule since 1833.", "prostate cancer", "July 23.", "Roqaya al-Sadat,", "the single-engine Cessna 206 went down,", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "software magnate Larry Ellison", "150 passengers", "The woman who accused Herman Cain of groping her", "bribing other wrestlers to lose bouts,", "did not speak to those who had gathered but shadow-boxed to spectators and cameras before meeting his distant relatives", "The Delta Queen will go out of service if Congress does not grant the ship another exemption from a 1960s federal law,", "the state's attorney", "photos", "\"Twilight\" book series", "The First Stop Resource Center assists veterans and their families through various periods of crises, including homelessness and addiction.", "cities throughout Canada", "\"made a brutal choice to step up attacks against innocent civilians.\"", "will be available under the inverted glass pyramid of the Louvre.", "writing and starring in 'The Prisoner'", "Friday", "in a muddy barley field owned by farmer Alan Graham outside Bangor,", "Swat Valley", "Former Beatles Paul McCartney and Ringo Starr", "Washington Redskins fan and loved to travel,", "Asashoryu", "the southern city of Naples", "last summer", "Mesut Oezil scored his first goal of the season to put Bremen 2-1 ahead on 29 minutes.", "revelry", "London Heathrow's Terminal 5", "18", "last April", "Casablanca, Morocco", "The Jacksons: A Family Dynasty\" television series will launch with two hours of programming on Sunday night, December 13.", "$250,000", "at a construction site in the heart of Los Angeles.", "$3 billion", "Tottenham", "7 July", "Bob Dylan", "St Pancras International", "eleanveda", "Denver", "Brazil", "Phelan Beale", "sandstone", "2005", "Babe Ruth", "New York Presbyterian Hospital", "ciolets", "David Lodge"], "metric_results": {"EM": 0.328125, "QA-F1": 0.45310311624649857}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, true, true, false, true, true, false, true, false, true, false, false, true], "QA-F1": [0.7499999999999999, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5555555555555556, 0.05714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0, 0.13333333333333333, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 0.1904761904761905, 0.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.7499999999999999, 0.11764705882352941, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.9333333333333333, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8990", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-241", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-1776", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2938", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-1516", "mrqa_newsqa-validation-316", "mrqa_naturalquestions-validation-5457", "mrqa_triviaqa-validation-2027", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-4180", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-15555"], "SR": 0.328125, "CSR": 0.5189144736842105, "EFR": 1.0, "Overall": 0.7030797697368422}, {"timecode": 38, "before_eval_results": {"predictions": ["death of a heretic.\"", "Charlotte Gainsbourg", "37", "cancerous tumor.", "her home", "is the U.N. nuclear watchdog agency's strongest warning yet that Iran could be aiming to build a nuclear bomb.", "evokes childhood memories in this four-line ode to Mom.", "84-year-old", "free laundry service.", "Oprah Winfrey, Michael Jordan, Robert De Niro, Janet Jackson and the Duchess of York", "opium", "Turkey,", "sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Texas,", "Six", "Carl and Ellie", "581 points", "make life a little easier", "in a remote part of northwestern Montana", "building bombs,", "forgery and flying without a valid license,", "2050,", "Eintracht Frankfurt", "Juan Martin Del Potro.", "50", "because its facilities are full.", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards", "from the capital, Dhaka, to their homes in Bhola", "Akshay Kumar", "133", "more than 100", "The Ski Train", "a man's lifeless, naked body", "Flint, Michigan.", "O2 Arena.", "Philippines", "$500,000", "work rule issues.", "not be allowed to use the words \"explosive device\" or \"bomb\" during the trial.", "The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza,", "The children that a French charity attempted to take to France from Chad for adoption are neither Sudanese nor orphans,", "second time", "Black History Month", "Congress", "Joe Lieberman", "Bastian Schweinsteiger", "CNN's Larry King talked with Republican Gov. Bobby Jindal in a prime-time exclusive interview Monday night.", "one bomber.", "Uma Bazaar (Ostra Forstadsgatan 13)", "\"17 Again\"", "Two United Arab Emirates based companies", "By petition for a writ of certiorari", "New Mexico", "In response, in 1947, U.S. Secretary of State George Marshall devised the `` European Recovery Program ''", "jewellers", "Africa and South America", "the M6", "ethereal", "youngest TV director ever", "Citizens for a Sound Economy", "bees, honey, and the Black Madonna who presides over their household.", "Zenda", "Jean Lafitte", "a 650-volt wallop"], "metric_results": {"EM": 0.40625, "QA-F1": 0.560340222736822}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, true, false, false, false, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 0.25, 0.06060606060606061, 1.0, 0.4, 0.8181818181818181, 1.0, 0.4, 0.0625, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.9090909090909091, 0.0, 0.2857142857142857, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.057142857142857134, 0.35294117647058826, 0.0, 0.3333333333333333, 1.0, 1.0, 0.8, 0.0, 0.23529411764705882, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-733", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3052", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-1920", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-4860", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3713", "mrqa_triviaqa-validation-1849", "mrqa_searchqa-validation-1177", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-14613"], "SR": 0.40625, "CSR": 0.516025641025641, "EFR": 0.9736842105263158, "Overall": 0.6972388453103914}, {"timecode": 39, "before_eval_results": {"predictions": ["brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil", "American Rock Salt", "on the urinary floor", "Andy Serkis", "Aldis Hodge", "Phillipa Soo", "sedimentary", "all land - living organisms", "John Young", "semi-autonomous organisational units", "W. Edwards Deming", "MacFarlane", "Cyndi Grecco", "can therefore be any unfavourable and unintended sign", "`` non rigid '' bodies", "Fleetwood Mac", "Jim Justice", "December 12, 2017", "South Africa", "1832", "Curtis Armstrong", "fibrous tissue", "St Pancras International", "Jerry Leiber and Mike Stoller", "DeWayne Warren", "Charles Woodson", "Jesse Wesley Williams", "Peter Cetera", "276 episodes", "near Flamborough Head", "Book of Exodus", "April 25 -- 30 in Park Avenue", "Kristy Swanson", "2001", "1994", "Washington", "Triple Alliance of Germany, Austria - Hungary, and Italy", "Australia", "Wisconsin", "from the Primal rib", "John F. Kennedy", "David Tennant", "Ben Findon, Mike Myers and Bob Puzey", "Charles Path\u00e9", "Barbara Windsor", "1857", "1997", "eleven", "Mel Gibson", "boy", "works in a bridal shop", "Venice", "The Wrestling Classic", "commercial, military, executive and agricultural aircraft", "Mike Fiers", "Rockland", "Delilah Rene", "three", "The Hutus were considered inferior,", "Frank Ricci", "Zanzibar", "Mao Zedong", "Telephone", "Drew Kesse,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.575669583523202}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false], "QA-F1": [0.10526315789473682, 0.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0, 1.0, 0.2777777777777778, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.8, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7272727272727273, 0.0, 0.0, 0.25, 0.4, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9513", "mrqa_naturalquestions-validation-4658", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-2365", "mrqa_hotpotqa-validation-2210", "mrqa_hotpotqa-validation-1952", "mrqa_newsqa-validation-3661", "mrqa_searchqa-validation-12993", "mrqa_newsqa-validation-3331"], "SR": 0.484375, "CSR": 0.515234375, "EFR": 0.9393939393939394, "Overall": 0.6902225378787878}, {"timecode": 40, "UKR": 0.728515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1604", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-1840", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-2225", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3310", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4443", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4729", "mrqa_hotpotqa-validation-4744", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4944", "mrqa_hotpotqa-validation-5034", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5317", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5424", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5590", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-854", "mrqa_hotpotqa-validation-866", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3525", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-370", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4658", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1610", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1818", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2470", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3286", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3521", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-973", "mrqa_searchqa-validation-10210", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-11232", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-1177", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-12959", "mrqa_searchqa-validation-13156", "mrqa_searchqa-validation-13489", "mrqa_searchqa-validation-13740", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-14978", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15004", "mrqa_searchqa-validation-15504", "mrqa_searchqa-validation-15505", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-15839", "mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16246", "mrqa_searchqa-validation-16353", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-3098", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-41", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-4567", "mrqa_searchqa-validation-4764", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5521", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-5698", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6659", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8176", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-8366", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8791", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-8995", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9683", "mrqa_searchqa-validation-9698", "mrqa_searchqa-validation-9803", "mrqa_squad-validation-100", "mrqa_squad-validation-10370", "mrqa_squad-validation-10457", "mrqa_squad-validation-12", "mrqa_squad-validation-1254", "mrqa_squad-validation-1330", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1498", "mrqa_squad-validation-1540", "mrqa_squad-validation-1586", "mrqa_squad-validation-1613", "mrqa_squad-validation-175", "mrqa_squad-validation-185", "mrqa_squad-validation-1902", "mrqa_squad-validation-1920", "mrqa_squad-validation-194", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2167", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-2473", "mrqa_squad-validation-2640", "mrqa_squad-validation-2672", "mrqa_squad-validation-270", "mrqa_squad-validation-2757", "mrqa_squad-validation-2762", "mrqa_squad-validation-2916", "mrqa_squad-validation-2986", "mrqa_squad-validation-3037", "mrqa_squad-validation-3125", "mrqa_squad-validation-3319", "mrqa_squad-validation-3407", "mrqa_squad-validation-3511", "mrqa_squad-validation-3545", "mrqa_squad-validation-3708", "mrqa_squad-validation-3770", "mrqa_squad-validation-3774", "mrqa_squad-validation-3786", "mrqa_squad-validation-3811", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3934", "mrqa_squad-validation-3939", "mrqa_squad-validation-4010", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-448", "mrqa_squad-validation-4484", "mrqa_squad-validation-4546", "mrqa_squad-validation-4671", "mrqa_squad-validation-4750", "mrqa_squad-validation-4799", "mrqa_squad-validation-4860", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5019", "mrqa_squad-validation-5050", "mrqa_squad-validation-5135", "mrqa_squad-validation-5276", "mrqa_squad-validation-5311", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5474", "mrqa_squad-validation-5634", "mrqa_squad-validation-5669", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5736", "mrqa_squad-validation-5751", "mrqa_squad-validation-6029", "mrqa_squad-validation-6318", "mrqa_squad-validation-6351", "mrqa_squad-validation-6481", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6594", "mrqa_squad-validation-6630", "mrqa_squad-validation-6981", "mrqa_squad-validation-7023", "mrqa_squad-validation-7168", "mrqa_squad-validation-7194", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7294", "mrqa_squad-validation-7466", "mrqa_squad-validation-7531", "mrqa_squad-validation-76", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7689", "mrqa_squad-validation-7763", "mrqa_squad-validation-7907", "mrqa_squad-validation-7932", "mrqa_squad-validation-8066", "mrqa_squad-validation-8066", "mrqa_squad-validation-8127", "mrqa_squad-validation-8209", "mrqa_squad-validation-8253", "mrqa_squad-validation-8386", "mrqa_squad-validation-8399", "mrqa_squad-validation-8488", "mrqa_squad-validation-8501", "mrqa_squad-validation-8754", "mrqa_squad-validation-8772", "mrqa_squad-validation-8901", "mrqa_squad-validation-8931", "mrqa_squad-validation-8948", "mrqa_squad-validation-9012", "mrqa_squad-validation-9322", "mrqa_squad-validation-9495", "mrqa_squad-validation-9506", "mrqa_squad-validation-955", "mrqa_squad-validation-9590", "mrqa_squad-validation-9768", "mrqa_squad-validation-9819", "mrqa_squad-validation-9855", "mrqa_squad-validation-9868", "mrqa_squad-validation-9901", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-1052", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1291", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1427", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-2889", "mrqa_triviaqa-validation-30", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4357", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6418", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6681", "mrqa_triviaqa-validation-6704", "mrqa_triviaqa-validation-6724", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-7601", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-895"], "OKR": 0.806640625, "KG": 0.48515625, "before_eval_results": {"predictions": ["Turkana", "Action Jackson", "Ben Willis", "Phillip Schofield and Christine Bleakley", "Miami, Jacksonville, Richmond, Washington, D.C., Baltimore, Philadelphia, New York City and Boston", "The Third Five - year Plan", "Hermann Ebbinghaus", "Ronald Reagan", "Baltimore, Maryland", "to collect menstrual flow", "noon of April 1st", "late 1980s", "Lesley Gore", "Andrew Lincoln", "Joe Spano", "inverted", "a political ideology", "Matt Monro", "Wednesday, 5 September 1666", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "The succession follows the order of Vice President, Speaker of the House of Representatives, President pro tempore of the Senate, and then the heads of federal executive departments who form the Cabinet of the United States", "multinational retail corporation", "July 2, 1776", "a balance sheet", "after the title page, copyright notices, and, in technical journals, the abstract", "2007", "Fred Ott", "Tevin Campbell", "October 6, 2017", "early Christians of Mesopotamia", "Anna Murphy", "peninsular mainland", "`` new version '' of Rent", "the revolutionaries named their newly independent country La Rep\u00fablica Dominicana", "1976", "Brad Johnson", "Egypt", "Montreal", "2003", "May 30, 2017", "a turlough", "Hathi Jr", "fifty small, white, five - pointed stars arranged in nine offset horizontal rows", "The euro", "local authorities", "`` One Son '', Jeffrey finds out that his father, the Smoking Man, forced his mother Cassandra to undergo medical treatments that led to several nervous breakdowns during his childhood years", "2017 / 18 Divisional Round game against the New Orleans Saints", "Nick Kroll", "Rick Marshall", "1,350", "Angel Cabrera", "Hanna-Barbera", "Venezuela", "point-coloration pattern", "Taipei City", "China", "Sunday", "Elizabeth Birnbaum", "Karen Floyd", "Michelob", "burritos", "Three's Company", "Australian"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5947850429658239}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.23529411764705882, 0.29411764705882354, 1.0, 1.0, 0.0, 0.6060606060606061, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.06896551724137931, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-6670", "mrqa_naturalquestions-validation-5876", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-7464", "mrqa_triviaqa-validation-4380", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-306", "mrqa_searchqa-validation-15726", "mrqa_hotpotqa-validation-1076"], "SR": 0.515625, "CSR": 0.5152439024390244, "EFR": 1.0, "Overall": 0.7071112804878049}, {"timecode": 41, "before_eval_results": {"predictions": ["middle eastern scientists", "1889", "to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "ummat al - Islamiyah", "2018", "Saint Alphonsa", "Peggy Lipton", "in formal education during the Roman Empire", "air moisture", "the right of the dinner plate", "Yondu Udonta", "Terry Kath", "by 1824", "November 5, 2017", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "4th", "one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "supervillains", "Atlanta", "21 June 2007", "Australia", "Bed and breakfast", "Masha Skorobogatov", "an empty line", "Jack Gleeson", "anticlockwise rotation", "October 6, 2017", "Joudeh Al - Goudia family", "Gatiman", "Escherichia coli", "Roman Reigns", "Ben Rosenbaum", "Audrey II", "acid rain", "Kaley Christine Cuoco", "statistical or other quantitative procedures", "Jules Shear", "Darren McGavin", "1961", "Randy VanWarmer", "April 3, 1973", "manta rays and Scorpion fish", "process large amounts of natural language data", "Detroit Tigers", "President Friedrich Ebert", "9.1", "Bruno Mars", "between the Mediterranean Sea to the north and the Red Sea in the south", "Lord's", "Kingsford, Michigan", "Honor\u00e9 Mirabeau", "Daniel Defoe", "1961", "birmingham", "First Street", "Selected Writings by Steve Biko", "Princes Park", "cancerous tumor.", "Kyra and Violet,", "\"Empire of the Sun,\"", "Bulldog", "Petsmart", "Carrie Underwood", "1970"], "metric_results": {"EM": 0.5, "QA-F1": 0.5977378299804771}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, true, false, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666665, 0.0, 1.0, 0.0, 1.0, 0.35294117647058826, 0.0, 0.56, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.5, 0.1818181818181818, 0.36363636363636365, 0.0, 1.0, 0.0, 0.4444444444444445, 0.9, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-1552", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2229", "mrqa_naturalquestions-validation-1455", "mrqa_triviaqa-validation-7151", "mrqa_hotpotqa-validation-5038", "mrqa_newsqa-validation-3380"], "SR": 0.5, "CSR": 0.5148809523809523, "EFR": 0.9375, "Overall": 0.6945386904761905}, {"timecode": 42, "before_eval_results": {"predictions": ["since Luther's increasingly antisemitic views developed during the years his health deteriorated, it is possible they were at least partly the product of a declining state of mind.", "1861\u20131865", "private", "Minnesota Timberwolves of the National Basketball Association (NBA)", "703", "December 1974", "Sufism", "Capellini", "1614", "international association football", "German and American", "age thirteen", "Kentwood, Louisiana", "Santa Fe", "Clarence Nash", "Kew Gardens", "the northeastern part", "American Horror Story", "Willis Tower", "Free Range Films", "The Deep Blue Sea", "Vernon L. Smith", "Australian actor", "Frank Fertitta, Jr.", "October 16, 2015", "the Battelle Energy Alliance", "Kaep", "I Write What I Like", "October", "The Flowers of Romance", "51,271", "Sun Valley, Idaho", "Guardians of the Galaxy Vol. 2", "American burlesque", "Pulitzer Prize", "Scott Carson", "IFFHS World's Best Goalkeeper", "Suspiria", "an English singer, songwriter, actress, and radio and television presenter", "Belladonna", "Russell T Davies", "\"Creed\"", "spot-fixing", "Erich Schmidt-Leichner", "\"as-Sindib\u0101du al-Ba\u1e25riyy\"", "A Rake's Progress", "Batman", "6,241", "Hennepin County", "Australian coast,", "Adelaide Botanic Garden, Hutt Street, and Victoria Park", "Paul Newman", "Michael Buffer", "Presley Smith", "AFC Wimbledon", "Erewhon", "Teppanyaki", "a share in the royalties for the tune.", "Asashoryu", "\"The Real Housewives of Atlanta\"", "Antnio Guterres", "to ratify the Constitution of the United States", "letter /letr/", "Dairy Queen"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6448485392720307}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, false, false, false, true], "QA-F1": [0.2758620689655173, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.4444444444444444, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2523", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-3352", "mrqa_hotpotqa-validation-4614", "mrqa_hotpotqa-validation-4296", "mrqa_hotpotqa-validation-1461", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-973", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5385", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-780", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-2395", "mrqa_naturalquestions-validation-5293", "mrqa_triviaqa-validation-3569", "mrqa_triviaqa-validation-748", "mrqa_newsqa-validation-2151", "mrqa_newsqa-validation-1125", "mrqa_searchqa-validation-3012", "mrqa_searchqa-validation-732", "mrqa_searchqa-validation-11496"], "SR": 0.546875, "CSR": 0.515625, "EFR": 1.0, "Overall": 0.7071875000000001}, {"timecode": 43, "before_eval_results": {"predictions": ["25", "Evey's mother", "from 1848 to 1852", "Adelaide", "American film industry", "1986", "Children's Mercy Park", "Jacking", "February 5, 2015", "\"The Worm\"", "MGM Resorts International", "Sam Raimi", "StubHub Center", "1946", "balloon Street, Manchester", "Liverpool Bay", "St. Louis, Missouri", "Stephen King", "\"Seducing Mr. Perfect\"", "British Labour Party", "the Five", "Ang Lee", "Taylor Swift", "Objectivism", "\"Traumnovelle\" (\"Dream Story\")", "Mandarin", "Lexy Gold", "KlingStubbins", "the Goddess of Pop", "Chevron Corporation", "Black Panther Party", "Baldwin", "John \"John\" Alexander Florence", "YouTube", "Kohlberg K Travis Roberts", "Salzkammergut", "Rain Man", "feats of exploration", "video game", "Father Dougal McGuire", "just over 1 million", "Mulberry", "London", "Subway restaurants", "cancer", "Campbellsville", "Mark Helfrich", "five-time", "Rickie Lee Skaggs", "Field Marshal Lord Gort", "Owsley Stanley", "Donna Mills", "16 seasons", "gas exchange", "ourselves alone", "Sam Allardyce", "gin", "almost 9 million", "vitamin injections", "to lose bouts,", "\"call\"", "Oxford University Dramatic Society", "Eppin' John", "Haiti"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7276684253246753}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.25, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.4, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5487", "mrqa_hotpotqa-validation-1003", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-5136", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-2770", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4257", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-3856", "mrqa_hotpotqa-validation-4501", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-992", "mrqa_naturalquestions-validation-8767", "mrqa_newsqa-validation-3325", "mrqa_searchqa-validation-10176", "mrqa_searchqa-validation-14771"], "SR": 0.609375, "CSR": 0.5177556818181819, "EFR": 1.0, "Overall": 0.7076136363636364}, {"timecode": 44, "before_eval_results": {"predictions": ["a broken arm", "Mexico", "Wyoming", "Franklin D. Roosevelt", "size zero", "Copenhagen", "the Pulitzer Prize for Drama", "the 904 Olympics", "New Zealand", "the Ziegfeld Girl", "Jeopardy", "the marinara sauce", "enamel", "Macy\\'s Christmas Parade", "method acting", "Sam Kinison", "the President dies, resigns or is removed from office", "Roman Empire", "Alaska", "Matt Leinart", "the great khan", "Jeremy Bentham", "the United States", "the Mekong", "King Neptune", "the 1984 Summer Olympics", "\"Ricochet\"", "(Original Israeli Cast)", "gas", "the Danforth Foundation", "Ivory Coast", "the Lord of the Rings: the Return of the King", "a Birch-tree", "Alanis Morissette", "tie", "Hoecakes", "King Minos", "\"Man Appeal\"", "(Ben) Kingsley", "King Henry VIII", "Stephen Crane", "Mississippi", "a Half Persian weave", "Yellow Brick Road", "the madding", "Steely Dan", "Linda Tripp", "the Sierra Nevada", "(Al) Ashraf Khalil", "the Volkswagen Passat W8", "the Gadsden Treaty", "New Zealand to New Guinea", "Roman Reigns", "A new ruler unites China", "the Marshall Plan", "Dick Turpin", "black", "15", "Cymbeline", "Mary-Kay Wilmers", "the shipping industry -- responsible for 5% of global greenhouse gas emissions, according to the United Nations --", "Hundreds", "2008", "three"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5536458333333333}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, false, true, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7525", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-3120", "mrqa_searchqa-validation-16746", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-14186", "mrqa_searchqa-validation-6399", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-508", "mrqa_searchqa-validation-12098", "mrqa_searchqa-validation-15476", "mrqa_searchqa-validation-8837", "mrqa_searchqa-validation-15203", "mrqa_searchqa-validation-3792", "mrqa_searchqa-validation-2669", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-11577", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-16207", "mrqa_searchqa-validation-540", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-13328", "mrqa_searchqa-validation-7176", "mrqa_searchqa-validation-14848", "mrqa_searchqa-validation-1399", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-7975", "mrqa_searchqa-validation-11005", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-6157", "mrqa_hotpotqa-validation-5093", "mrqa_newsqa-validation-3979"], "SR": 0.484375, "CSR": 0.5170138888888889, "EFR": 0.9090909090909091, "Overall": 0.6892834595959596}, {"timecode": 45, "before_eval_results": {"predictions": ["the mass", "the leader of a drug cartel that set off two grenades during a public celebration in September, killing eight people and wounding more than 100.", "Robert Barnett", "Too many glass shards left", "Sunday.", "\"evidence of the cozy relationship between some elements of [the agency] and the oil and gas industry.\"", "Alan Graham", "Islamabad", "he was battling a potentially fatal disease that required a life-saving lung transplant, his publicist responded that he was \"in fine health\" and that the story was \"a total fabrication.\"", "President Thabo Mbeki", "\"project work\"", "Dr. Maria Siemionow,", "12 hours in jail.", "the United States", "the Obama chief of staff and the Obama people", "$273 million", "Wigan Athletic", "-- you know -- black is beautiful,\"", "At least 25", "former U.S. secretary of state", "and called Israel's actions against Hamas militants \"a gift\" from U.S. President-elect Barack Obama.", "(Constantin) Blauser,", "U.S.", "1.2 million people", "in North Korea", "the Da Vinci Code", "German Chancellor Angela Merkel", "at least 12 months.", "Passers-by", "British", "Lance Cpl. Maria Lauterbach", "Picasso's muse and mistress, Marie-Therese Walter.", "80", "Polo", "bartering", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "the \"race for the future... and it won't be won with a president who is stuck in the past.\"", "fluoroquinolone", "North Korea", "human rights abuses against ethnic Somalis by rebels and Ethiopian troops are rampant.", "his comments", "Colombia's most sought-after criminals and ranked just below the leaders of Revolutionary Armed Forces of Colombia,", "a student who admitted to hanging a noose in a campus library,", "to protect ocean ecology, address climate change and promote sustainable ocean economies.", "North Korea", "South Africa inflicted the first home series defeat on Australia in almost 16 years", "Six", "flooding and debris", "then-Sen. Obama", "Unseeded Frenchwoman Aravane Rezai", "the radical Islamist militia that controls the city", "201", "IBM", "Gorakhpur railway station", "Inspector of Prisons", "Charlie Cairoli", "Sweden", "2017", "Edmund Ironside", "Comme des Gar\u00e7ons", "Booker T. Washington", "Romeo", "the Song Dynasty", "Germany"], "metric_results": {"EM": 0.34375, "QA-F1": 0.5109959959362533}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.08, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.36363636363636365, 1.0, 1.0, 0.7499999999999999, 0.4, 1.0, 0.11764705882352941, 0.0, 1.0, 0.8, 0.8, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.125, 0.5, 0.13333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.25, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-1684", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-182", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-1848", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3238", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-1708", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-19", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-3285", "mrqa_triviaqa-validation-4496", "mrqa_hotpotqa-validation-3844", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-11067", "mrqa_searchqa-validation-4478"], "SR": 0.34375, "CSR": 0.5132472826086957, "EFR": 1.0, "Overall": 0.7067119565217392}, {"timecode": 46, "before_eval_results": {"predictions": ["Thames River", "an independent homeland for the country's ethnic", "Turkish President Abdullah Gul,", "phone calls or by text messaging,", "no reports of ground strikes or interference with aircraft in flight,", "in Fayetteville, North Carolina,", "the \"surge\" strategy he implemented last year.", "Christopher Savoie", "work together to stabilize Somalia and cooperate in security and military operations.", "took a little over an hour to clear Gerrard of charges relating to a fracas in a nightclub bar in the north-western of England city on December 29 of last year.", "seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "You can go from rags to riches there.", "the United States", "Manny Pacquiao", "Garth Brooks", "The son of Gabon's former president", "super-yacht designers Wally", "in Cologne, Germany,", "Prince George's County Correctional Center,", "The train in front had stopped", "Karen Floyd", "opposed the Iraq war and considered Afghanistan the \"good war.\"", "since 1983.", "at least $20 million to $30 million,", "Daryeel Bulasho Guud", "Zulfikar Ali Bhutto,", "the insurgency,", "Stoke City.", "Former detainees", "three different videos", "a national telephone survey of more than 78,000 parents", "is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.\"", "his past and his future", "son of the most-wanted man in the world", "travel in cars with tinted windows -- which protected me from identification by terrorists -- or travel with privately armed guards,\"", "mild to moderate depression", "series of poems telling of the pain and suffering of children just like her", "job opportunities for nearly 200,000 Iraqi citizens in infrastructure, industrial projects, support services and other business activities.", "Mitt Romney", "\"The Sopranos,\"", "The Louvre", "Jared Polis", "a delegation of American Muslim and Christian leaders", "the legitimacy of that race.", "Australian officials", "misdemeanor", "$10 billion", "\"procedure on her heart,\"", "safety issues in the company's cars", "Newcastle", "54", "1 mile ( 1.6 km )", "Abanindranath Tagore CIE", "1996", "Kiri Te Kanawa", "Submarine Sunk", "1960's", "the onset and progression of Alzheimer's disease.", "the Future", "1770", "Jacob and Wilhelm Grimm", "Seasons in the sun", "Septimus Scott", "Barbara Eve Harris"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7040686497347681}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.9166666666666666, 1.0, 0.0, 1.0, 0.15384615384615385, 0.0, 0.2, 0.9333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.09090909090909091, 0.6666666666666666, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.8, 0.0, 1.0, 0.8799999999999999, 0.10526315789473684, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-851", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-2547", "mrqa_newsqa-validation-247", "mrqa_triviaqa-validation-4212", "mrqa_hotpotqa-validation-5485", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-13873", "mrqa_naturalquestions-validation-3802"], "SR": 0.578125, "CSR": 0.5146276595744681, "EFR": 1.0, "Overall": 0.7069880319148936}, {"timecode": 47, "before_eval_results": {"predictions": ["waffles", "President Obama", "Harry Potter", "a stagecoach", "the Cyrillic alphabet", "a choc. pecan pie", "the Charleston", "the Japanese Chin", "Siegfried", "taxonomy", "Anne Rice", "Anne Murray", "the lithosphere", "a billionaire's", "Lady Jane Grey", "Santeria", "the Duggar family", "the Lincoln cent", "Sydney", "a Mona Lisa", "Belarus", "Airplane", "French toast", "gingerbread", "Swiss Cheese", "the Monkees", "Johnson", "Agatha Christie", "Jack Dempsey", "conglomeratus", "brood", "Edison", "Mount Everest", "black-eyed pea dip", "Giacomo Puccini", "Battlestar Galactica", "Yugoslavia", "the Surgeon General", "a Place Bet", "War and Peace", "Frank Lloyd Wright", "Falcon Crest", "William the Conqueror", "Grant and Sherman", "Adam Smith", "a yeast", "Pearl S. Buck", "Avenue Victor-Hugo", "Atlanta", "Mona Lisa", "Sisyphus", "2017 season", "Nathan Hale", "on location in the United Kingdom", "Theodore Roosevelt", "non-pathogens", "the Army of Northern Virginia", "Sam Raimi", "1940s and 1950s", "Sleepy Brown", "\"They know of our respect for the civil community,\"", "a judge to order the pop star's estate to pay him a monthly allowance,", "ALS6", "Stevie Wonder"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6590204831932773}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7058823529411764, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12474", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-16128", "mrqa_searchqa-validation-15255", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-7903", "mrqa_searchqa-validation-482", "mrqa_searchqa-validation-11413", "mrqa_searchqa-validation-8160", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-9068", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-10067", "mrqa_searchqa-validation-1056", "mrqa_searchqa-validation-16434", "mrqa_searchqa-validation-11677", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-15591", "mrqa_searchqa-validation-2263", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-10394", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-1015", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-6789", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1952"], "SR": 0.53125, "CSR": 0.5149739583333333, "EFR": 1.0, "Overall": 0.7070572916666666}, {"timecode": 48, "before_eval_results": {"predictions": ["Argentine composer Lalo Schifrin", "The loss of steel industry jobs in the region coincided with the general deindustrialization of Rust Belt cities such as Youngstown as well as the United States as a whole", "the 1971 motion picture Willy Wonka & the Chocolate Factory", "annually in late January or early February", "The Turbo Charged Prelude", "John Adams of Massachusetts, Benjamin Franklin of Pennsylvania, Thomas Jefferson of Virginia, Robert R. Livingston of New York, and Roger Sherman of Connecticut", "from the Ute name for them, k\u0268mantsi ( enemy )", "the sixth series", "the summer of 2003", "season two", "the 1970s", "from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "P O", "the Student League for Industrial Democracy ( SLID )", "Donald", "Philippians", "the Great Plains and U.S. Interior Highlands", "a burden to be carried as penance", "Michael Phelps", "The British colonial government", "22 \u00b0 00 \u2032 N 80 \u00b0 00\u2032 W", "Ella Lawrence", "1986", "1932", "Sophocles", "live on BBC One on Saturday evenings", "New York and New Jersey", "One Night in the Tropics", "for the 2009 model year", "Mohammad Reza Pahlavi", "Saint Alphonsa", "Billie Jean King", "the Battle of Antietam", "C\u03bc and C\u03b4", "Andrew Taggart, Emily Warren and Scott Harris", "the four - letter suffix", "the Jews", "Andy", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "pools campaign contributions from members and donates those funds to campaign for or against candidates, ballot initiatives, or legislation", "The bills taken up under legislative power of parliament are treated as passed provided provided majority of members present at that time approved the bill either by voting or voice vote", "eight episode series", "Julie Deborah Kavner", "September 19, 2017", "May 2002", "Chelsea", "Alamodome in San Antonio, Texas", "Homer Banks, Carl Hampton and Raymond Jackson", "Omar Khayyam", "1984", "Joseph Nye Welch", "F\u00fcr Elise", "Puck", "Harrods", "Adolfo Rodr\u00edguez Sa\u00e1", "Tool", "seven years", "a satellite.", "Ali Bongo", "neck", "The Sisters Rosensweig", "My Name Is Earl", "Vatican City", "Timothy Dalton"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6263936181867793}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, true, true, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 0.4444444444444444, 0.0, 0.16, 0.8750000000000001, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.8421052631578948, 0.5, 1.0, 0.0, 1.0, 0.25, 0.9090909090909091, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8387096774193548, 1.0, 0.3888888888888889, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-156", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9098", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-74", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-3632", "mrqa_naturalquestions-validation-3122", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-9281", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-4796", "mrqa_hotpotqa-validation-2441", "mrqa_newsqa-validation-1656"], "SR": 0.453125, "CSR": 0.5137117346938775, "EFR": 0.9714285714285714, "Overall": 0.7010905612244899}, {"timecode": 49, "before_eval_results": {"predictions": ["Louisa May Alcott", "Davy Crockett", "Sugar Ray", "Elvis Presley", "Jupiter", "Elbotola", "the Three Wise Men", "Ladies Professional Golf Association", "Henry VIII", "corpulent", "Frdric Chopin", "Copacabana", "Krakow", "Daredevil", "Tunis", "emerald", "Agriform", "Mauna Loa", "John Lennon", "Macy\\'s", "Sonny", "Fred Claus", "the Deathly Hallows", "the Kremlin", "the raven", "New York Cosmos", "Richard Feynman", "John Grisham", "Positron emission tomography", "Catherine the Great", "Eisenhower", "Ellen Wilson", "arthritis", "the National Gallery of Art in Washington DC", "Crispix", "Gabriela Sabatini", "St. Louis", "Imperfect", "the Caspian tern", "the Beagle", "Gene Autry", "the Wing-T", "Luzon", "Henry Hudson", "a diamond", "Roger Brooke Taney", "the United Nations", "high crimes", "The Unbearable Lightness of Being", "the Appian Way", "Joseph", "`` Nearer, My God ''", "marriage officiant, solemniser, or `` vow master ''", "1 October 2006", "an American Internet entrepreneur", "Kenny Everett", "Matthew Bellamy", "Jack White", "Isabella II", "villanelle", "\"executioners\"", "Oaxaca, Mexico", "$5.5 billion", "not"], "metric_results": {"EM": 0.5625, "QA-F1": 0.65579568001443}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.25, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-6521", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-9168", "mrqa_searchqa-validation-14716", "mrqa_searchqa-validation-8091", "mrqa_searchqa-validation-1620", "mrqa_searchqa-validation-14204", "mrqa_searchqa-validation-2632", "mrqa_searchqa-validation-1000", "mrqa_searchqa-validation-6765", "mrqa_searchqa-validation-12896", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-15908", "mrqa_searchqa-validation-16894", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-683", "mrqa_searchqa-validation-15730", "mrqa_searchqa-validation-2231", "mrqa_naturalquestions-validation-8217", "mrqa_triviaqa-validation-2643", "mrqa_triviaqa-validation-1277", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-1038", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3761"], "SR": 0.5625, "CSR": 0.5146875, "EFR": 1.0, "Overall": 0.7070000000000001}, {"timecode": 50, "UKR": 0.677734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1003", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1417", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1604", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-2225", "mrqa_hotpotqa-validation-2302", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2458", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-2739", "mrqa_hotpotqa-validation-2766", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-310", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3767", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4744", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4999", "mrqa_hotpotqa-validation-5038", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5317", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-805", "mrqa_hotpotqa-validation-870", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3062", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3757", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3795", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9098", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9557", "mrqa_naturalquestions-validation-9778", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1610", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1818", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2470", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3286", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3521", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3676", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-973", "mrqa_searchqa-validation-1000", "mrqa_searchqa-validation-10210", "mrqa_searchqa-validation-10394", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-10752", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-10955", "mrqa_searchqa-validation-10981", "mrqa_searchqa-validation-11232", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-11973", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-12474", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12698", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-12959", "mrqa_searchqa-validation-13328", "mrqa_searchqa-validation-13740", "mrqa_searchqa-validation-1399", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15203", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-15839", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-16894", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-3098", "mrqa_searchqa-validation-3295", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-4567", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5521", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-5698", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-6659", "mrqa_searchqa-validation-6765", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7287", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-7903", "mrqa_searchqa-validation-7989", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8176", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-8995", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9683", "mrqa_searchqa-validation-9698", "mrqa_searchqa-validation-9803", "mrqa_squad-validation-100", "mrqa_squad-validation-10370", "mrqa_squad-validation-10457", "mrqa_squad-validation-12", "mrqa_squad-validation-1254", "mrqa_squad-validation-1330", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1540", "mrqa_squad-validation-1586", "mrqa_squad-validation-1613", "mrqa_squad-validation-175", "mrqa_squad-validation-185", "mrqa_squad-validation-1902", "mrqa_squad-validation-1920", "mrqa_squad-validation-194", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-2523", "mrqa_squad-validation-2640", "mrqa_squad-validation-270", "mrqa_squad-validation-2762", "mrqa_squad-validation-2916", "mrqa_squad-validation-2986", "mrqa_squad-validation-3037", "mrqa_squad-validation-3125", "mrqa_squad-validation-3319", "mrqa_squad-validation-3511", "mrqa_squad-validation-3545", "mrqa_squad-validation-3708", "mrqa_squad-validation-3770", "mrqa_squad-validation-3811", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3934", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-448", "mrqa_squad-validation-4546", "mrqa_squad-validation-4671", "mrqa_squad-validation-4750", "mrqa_squad-validation-4860", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5050", "mrqa_squad-validation-5135", "mrqa_squad-validation-5276", "mrqa_squad-validation-5311", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5474", "mrqa_squad-validation-5669", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5736", "mrqa_squad-validation-5751", "mrqa_squad-validation-6029", "mrqa_squad-validation-6351", "mrqa_squad-validation-6481", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6630", "mrqa_squad-validation-7023", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7531", "mrqa_squad-validation-76", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7689", "mrqa_squad-validation-7763", "mrqa_squad-validation-7932", "mrqa_squad-validation-8066", "mrqa_squad-validation-8209", "mrqa_squad-validation-8253", "mrqa_squad-validation-8386", "mrqa_squad-validation-8488", "mrqa_squad-validation-8754", "mrqa_squad-validation-8772", "mrqa_squad-validation-8931", "mrqa_squad-validation-8948", "mrqa_squad-validation-9012", "mrqa_squad-validation-9322", "mrqa_squad-validation-9495", "mrqa_squad-validation-9506", "mrqa_squad-validation-955", "mrqa_squad-validation-9590", "mrqa_squad-validation-9819", "mrqa_squad-validation-9868", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1291", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-30", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-4949", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6579", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6724", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-7601", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-895"], "OKR": 0.76953125, "KG": 0.43203125, "before_eval_results": {"predictions": ["collusion between the colossus of the North [the United States] and the col Colossus of the South [Brazil]\"", "recall", "American", "helicopters and unmanned aerial vehicles from the White House", "1-0", "President Obama", "Adam Lambert and Kris Allen", "Arabic, French and English,", "Saturday about 20 feet above flood stage.", "snakes -- and one snake in particular.", "a skilled hacker could disrupt the system and cause a blackout.", "in a Starbucks this summer.", "\"In the Middle East and North Africa, the Internet has offered many people access to information and the outside world that would have been unthinkable a few years ago,\"", "my own visits with him -- will serve him well in the White House.\"Dole also said Romney \"rescued a flailing Winter Olympics when it was mired in financial scandal.\"", "myrrh's parents", "for saying Chaudhary's death was warning to management.", "her fianc\u00e9,", "\"Our treatment met the legal definition of torture. And that's why I did not refer the case\" for prosecution.", "in the realm of politics and not the courtroom.", "Haiti.", "228", "to acquire nuclear weapons are \"not far away, not at all, to what Hitler did to the Jewish people just 65 years ago,\"", "\"The oceans are kind of the last frontier for use and development,\"", "J.Crew outfits", "India", "2009", "President Obama's", "At least 38", "for death squad killings carried out during his rule in the 1990s.", "Rod Blagojevich,", "10", "Dr. Maria Siemionow,", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "her boyfriend,", "flying", "14", "Britain.", "the U.S.-based venture capital company Polaris Venture Partners,", "two", "\"The situation is pretty much resolved,\"", "16", "\"bystander effect\":", "Tom Hanks", "\"a potential hazard may occur due to re-entry of satellite debris into the earth's atmosphere.\"", "\" Crash,\"", "the United States", "June 2002.", "Khaled El Islambouly,", "At least 15", "three", "London", "Australia", "the inner core", "Turducken", "Bonnie and Clyde", "'reactive to far ambush'", "coffee", "wineries", "Belgian", "Guthred", "a soothsayer", "Coca-Cola (3)", "Waldorf Astoria New York", "Fringillidae"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5757528128763804}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.7692307692307693, 0.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.4, 0.4, 0.07142857142857142, 0.07142857142857144, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5806451612903226, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8571428571428571, 0.2105263157894737, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9600000000000001, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3870", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-3228", "mrqa_newsqa-validation-1631", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-2561", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-3629", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-914", "mrqa_triviaqa-validation-1242", "mrqa_hotpotqa-validation-471", "mrqa_searchqa-validation-7167", "mrqa_searchqa-validation-15063", "mrqa_triviaqa-validation-4377"], "SR": 0.453125, "CSR": 0.5134803921568627, "EFR": 1.0, "Overall": 0.6785554534313726}, {"timecode": 51, "before_eval_results": {"predictions": ["Tuesday night,", "a judge to order the pop star's estate", "\"Red Lines,\"", "tax credits to companies hiring jobless veterans", "Herman Thomas", "growing", "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,", "2,000 euros ($2,963)", "Christopher Savoie", "growing up to lead Pakistan.\"", "eight-week", "\"Oprah is an angel, she is God-sent,\"", "D.J. Knight of Pearlman, Texas,", "Wilderness-", "January", "Anjuna beach in Goa", "Bryant Purvis", "in Japan", "Venus Williams", "two", "Monday", "Seoul", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "collaborating with the Colombian government,", "she also believed police were trying to cover up the truth behind her daughter's murder,", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "in an artificial coma", "Pakistan's", "Charles Lock", "\"This is not something that anybody can reasonably anticipate,\"", "Somali-based", "gun charges,", "\"Hairspray,\"", "20 times", "bipartisan", "helping on the sandbags", "to sniff out cell phones.", "2,000 people,", "41,", "a \"happy ending\" to the case.", "North Korea", "Liza Murphy", "Arsene Wenger", "four months ago,", "Liberation Tigers of Tamil Eelam,", "that Birnbaum had resigned \"on her own terms and own volition.\"", "hundreds", "photos", "Arizona", "raping and killing a 14-year-old Iraqi girl.", "from Spain to the Caribbean", "`` E-mail surveillance ''", "Warren Hastings", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Bactrian", "faggot", "vanilla", "body of water", "2007", "aldosterone", "growing large", "vice presidential running mate", "Animal Crackers", "Ogaden"], "metric_results": {"EM": 0.5, "QA-F1": 0.6007988972832723}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, false, true, true, false, true, false, false, false, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.3333333333333333, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7692307692307693, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3224", "mrqa_newsqa-validation-1545", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-1188", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-2985", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-1725", "mrqa_newsqa-validation-784", "mrqa_newsqa-validation-475", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-6874", "mrqa_triviaqa-validation-1488", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-397", "mrqa_searchqa-validation-6635", "mrqa_searchqa-validation-7781"], "SR": 0.5, "CSR": 0.5132211538461539, "EFR": 1.0, "Overall": 0.6785036057692307}, {"timecode": 52, "before_eval_results": {"predictions": ["Pope", "if they can demonstrate they have been satisfactorily treated", "Summer", "urged us to accelerate handing over responsibility to Afghan forces.\"", "VBS.TV", "children of street cleaners and firefighters.", "Switzerland", "2009", "Tim Clark, Matt Kuchar and Bubba Watson", "Araceli Valencia,", "400 farmers", "Friday,", "Lisa Brown", "President Robert Mugabe", "five", "planned attacks", "\"Teen Patti\"", "Seasons of My Heart", "an angry mob.", "do its part to improve the environment by taking on greenhouse gas emissions.", "Jared Polis", "dancing against a stripper's pole.", "bragging about his sex life on television", "\"Three Little Beers,\"", "12.3 million", "the 6.2-mile Moffat Tunnel,", "English", "Arkansas", "Titanic survivor Barbara Dainton-West,", "a pure meritocracy,", "American Bill Haas", "the war of words in the Republican Party centered around Rush Limbaugh.", "Tuesday", "after giving birth to baby daughter Jada,", "co-writing credits", "kite surfers", "a 31-year-old U.S. Army scout", "2.5 million", "many wives of cheating politicians,", "Somali", "NATO fighters", "U.S. Vice President Dick Cheney", "5:20 p.m.", "Former Beatles", "\"The Rosie Show,\"", "remote highway in Michoacan state,", "helicopters and unmanned aerial vehicles", "a \"prostitute\"", "Hanin Zoabi,", "two", "April 2010.", "1985", "1599", "`` Entropy ''", "\"rice paper\"", "PPTH", "helps managers understand employees' needs in order to further employees' motivation", "the Dutch Empire", "24 December 1692", "Cushman", "stubby squid (Rossia pacifica)", "myrrh", "the Arkansas Project", "\"Monster\""], "metric_results": {"EM": 0.546875, "QA-F1": 0.6240602355072464}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.20000000000000004, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.9565217391304348, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1590", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-146", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-2821", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-934", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-2536", "mrqa_hotpotqa-validation-5395", "mrqa_searchqa-validation-14214", "mrqa_searchqa-validation-9278", "mrqa_searchqa-validation-4725", "mrqa_searchqa-validation-5273"], "SR": 0.546875, "CSR": 0.5138561320754718, "EFR": 1.0, "Overall": 0.6786306014150945}, {"timecode": 53, "before_eval_results": {"predictions": ["Anvil firing", "Abigail", "Encore Las Vegas", "David Jolly", "9 November 1955", "Ch\u014dfu, Tokyo, Japan", "Adrian Peter McLaren", "Anthony John Herrera", "2008\u201309 UEFA Champions League", "Lev Ivanovich Yashin", "Wayne Rooney", "Mot\u00f6rhead", "Solace", "1943,", "Vivendi S.A.", "Wings of Desire", "9 November 1967", "Super Bowl XXIX", "Kristoffer Kristofferson", "Hudson Bay Mining and Smelting Company", "Cherokee County", "100 metres", "Acela Express", "5249", "Johnnie Ray", "Harry Booth", "John McClane", "James Packer", "black nationalism", "USC Marshall School of Business", "Saint Motel", "You're Next", "eclectic mix of musical styles", "Christian", "Fife", "Peter 'Drago' Sell", "Marktown", "Hidden America", "Bangkok", "Gregg Popovich", "Roy Spencer", "Chicago", "a union between spouses", "2012", "torpedoes", "Saint Paul", "1501", "Man Booker Prize for Fiction", "Wayne Rooney", "Nicholas John \"Nic\" Cester", "Kim Carnes", "a convergent plate", "during the 2013 -- 14 television season", "1947, 1956, 1975, 2015 and 2017", "10 Downing Street", "Thor", "Manchester", "Sri Lanka", "Michael Schumacher", "Security officer Stephen Johns reportedly opened the door for the man who shot him,", "President Kennedy", "Snowy", "Vestal Virgins", "Aaron Lewis"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6981703192640693}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.4, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.16666666666666666, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 0.0, 0.75, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-1854", "mrqa_hotpotqa-validation-5670", "mrqa_hotpotqa-validation-2235", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-649", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-154", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3024", "mrqa_hotpotqa-validation-4945", "mrqa_naturalquestions-validation-8699", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-5865", "mrqa_triviaqa-validation-4355", "mrqa_triviaqa-validation-2833", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-2941", "mrqa_searchqa-validation-127"], "SR": 0.609375, "CSR": 0.515625, "EFR": 0.96, "Overall": 0.670984375}, {"timecode": 54, "before_eval_results": {"predictions": ["Betty Crocker", "Frank Sinatra", "khaki", "Frank Sinatra", "Antarctica", "robota", "Easter Island", "the troposphere", "Huntsville, AL ( Rocket City)", "Khrushchev", "Venus", "the retina", "Jordan", "German", "toga", "George Washington", "the Columbia River", "Hairspray", "Fiddler", "Bangkok", "Nitrogen,", "Ponzi", "Frankie Go Boom", "Islamic Republic", "John Lennon", "the Spanish Republic", "Three Coins in the Fountain", "George Wallace", "tuna", "heaven", "the Byzantine Empire", "Fall Out Boy", "freezing", "silver", "a beak", "William Jennings Bryan", "The Blues Brothers", "Ezra Pound", "jedoublen/jeopardy", "Copenhagen", "flower", "the double bass", "the Bumblebee", "D", "Panama Canal", "Dan Marino", "Crustaceans", "Hestia", "Band of Brothers", "France", "Martin Luther King", "Profit maximization happens when marginal cost is equal to marginal revenue", "Havana Harbor", "2017 - 12 - 10", "Thai", "Helen Glover", "redheaded", "April 1, 1949", "Lantern Waste", "Glam metal", "July in the Philippines", "Abhisit Vejjajiva", "prisoners at the South Dakota State Penitentiary", "1989"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7229624542124542}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.3076923076923077, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6583", "mrqa_searchqa-validation-4968", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-13503", "mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-7142", "mrqa_searchqa-validation-1211", "mrqa_searchqa-validation-8663", "mrqa_searchqa-validation-15081", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-8786", "mrqa_searchqa-validation-4922", "mrqa_searchqa-validation-15434", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-12780", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-10730", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-53", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-7321", "mrqa_newsqa-validation-3407"], "SR": 0.640625, "CSR": 0.5178977272727272, "EFR": 0.9565217391304348, "Overall": 0.6707432682806325}, {"timecode": 55, "before_eval_results": {"predictions": ["Transportation Security Administration", "an open window", "helping to plan the September 11, 2001, terror attacks,", "Iran", "1960s", "Janet Napolitano", "legislation Wednesday requiring federal oil industry regulators to wait at least two years after leaving government service before going to work for companies they helped regulate.", "California's Stanford University,", "\"How I Met Your Mother,\"", "Opry Mills,", "three", "March 22,", "Arabic, French and English", "Second seed Fernando Gonzalez of Chile also went through as he beat Ivan Ljubicic of Croatia 6-4 6-3.", "The minister later apologized, telling CNN his comments had been taken out of context.", "10 below", "KBR managers", "L'Aquila earthquake,", "crocodile eggs", "200", "Joe Patane", "The Palm", "82", "1975", "Leo Frank,", "50", "the inspector-general", "Friday,", "two", "France's famous Louvre museum", "Ashley \"A.J.\" Jewell,", "be silent.", "SSM Cardinal Glennon Children's Medical Center in St. Louis.", "two weeks after Black History Month", "Two people", "one of Africa's most stable nations.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "The federal officers' bodies", "Hundreds of contraband cell phones were found behind bars or in transit to Texas inmates in 2008.", "an engineering and construction company", "$50,000", "a national telephone survey", "The oceans are kind of the last frontier for use and development,\"", "Michelle Rounds", "U.N.", "a strict interpretation of the law,", "Monday and Tuesday", "A good vegan cupcake has the power to transform everything for the better,\"", "sailing", "246", "Patrick McGoohan,", "a United States Internal Revenue Service form that provides the public with financial information about a nonprofit organization", "President Gerald Ford", "a thicker consistency and a deeper flavour than sauce", "windmills", "France", "Mt Kenya", "R&B", "Donald Duck", "English", "the Edo era", "the Grand Canal", "pennies", "merengue"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7319905534518664}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.4444444444444445, 0.9600000000000001, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.2666666666666667, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 0.0, 0.29629629629629634, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6956521739130436, 0.8, 0.25, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-2666", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-975", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-3183", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-2943", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-5309", "mrqa_hotpotqa-validation-2866", "mrqa_searchqa-validation-2063"], "SR": 0.609375, "CSR": 0.51953125, "EFR": 1.0, "Overall": 0.679765625}, {"timecode": 56, "before_eval_results": {"predictions": ["Chad", "$150 billion over 10 years in clean energy.", "Houston-based company Kellogg Brown", "Thursday.", "\"Empire of the Sun,\"", "Buenos Aires.", "a youth ballpark in his hometown of Aberdeen, Maryland,", "Sonia Sotomayor", "Herman Cain,", "There's no chance", "15-year-old", "137", "22-year-old", "Richmond students did nothing because of the \"bystander effect\":", "Bryant Purvis,", "570 billion pesos ($42 billion)", "from the Bronx", "will not support the Stop Online Piracy Act,", "66th annual Golden Globe Awards", "Krishna Rajaram,", "helping to plan the September 11, 2001,", "The pilot,", "$10 billion", "viruses in question are sexually transmitted.", "Washington State's decommissioned Hanford nuclear site,", "\"It hurts my heart to see him in pain, but it enlightens at the same time to know my son is strong enough to make it through on a daily basis,\"", "Afghanistan,", "in a canyon in the path of the blaze Thursday.", "left the medical engineering company where she worked.", "antihistamine and an epinephrine auto-injector", "U.S. Holocaust Memorial Museum,", "racial intolerance.", "Jobs", "Al-Shabaab, the radical Islamist militia that controls the city", "in an appearance last week in Broward County Circuit Court.", "3-0", "Tuesday", "and more radical Islamic groups.", "off the coast of Dubai", "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "Dr. Jennifer Arnold and husband Bill Klein,", "80 percent of the woman's face", "five", "British Prime Minister Gordon Brown's", "Pacific Ocean territory of Guam within striking distance,", "Angola,", "the same drama that pulls in the crowds", "May 4", "a \" happy ending\" to the case.", "Harry Nicolaides, 41,", "ALS6,", "dispense summary justice or merely deal with local administrative applications in common law jurisdictions", "autompne ( automne in modern French ) or autumpne in Middle English", "a hyper - active kinase, that confers an aberrant, ligand - independent, non-regulated growth stimulus to the", "hound", "a feisty creature", "Cambodia", "Cold Spring", "1946", "Arrowhead Stadium", "Thomas Jefferson", "refrigerator", "X-Files", "1961"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6603817770837432}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, false, false, true, true, false, true, true, false, false, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.4, 0.5, 1.0, 1.0, 1.0, 0.15384615384615383, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5882352941176471, 1.0, 0.16666666666666666, 1.0, 1.0, 0.8, 1.0, 0.782608695652174, 0.0, 0.923076923076923, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-933", "mrqa_newsqa-validation-675", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-19", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-698", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-9271", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-1316", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-2040"], "SR": 0.5625, "CSR": 0.5202850877192983, "EFR": 0.9642857142857143, "Overall": 0.6727735354010025}, {"timecode": 57, "before_eval_results": {"predictions": ["$55.7 million", "$55.7 million take (including $5.5 million from 124 IMAX screens)", "Kurdish militant group in Turkey", "Dr. Maria Siemionow,", "Alwin Landry's", "love for the \"Golden City,\"", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "free services.", "the number of new cases is falling.", "Amitabh Bachchan", "\"How I Met Your Mother,\"", "Harlem,", "closure of Guant Bay prison and CIA \"black site\" prisons,", "anyone wanting to harm them.", "Screen Actors Guild", "11 healthy eggs", "Salt Lake", "Guinea, Myanmar, Sudan and Venezuela.", "four", "a drug cartel that set off two grenades during a public celebration in September,", "Hutu militias and members of the general population sought out Tutsis and moderate Hutus", "$60 billion on America's infrastructure.", "\" Seeing bullet wounds in the back of a friend's head, seeing friends grabbing their arms, and blood just everywhere.", "Tim Clark, Matt Kuchar and Bubba Watson", "Mark Fields,", "three", "al Qaeda", "Isabella", "Larry Abrahamson", "near his home in Peshawar", "Dogpatch Labs", "Lavau's son, Sean, found his father after hearing \"faint yelling for help on the roadway from the canyon below,\"", "cross-country skiers", "no one is sure", "\"The Sopranos,\"", "a one-shot victory in the Bob Hope Classic", "bullet-riddled body was rushed to the Maadi Military Hospital and the president was proclaimed dead at 2.40 p.m. due to \"intense nervous shock and internal bleeding in the chest cavity.\"", "Stratfor,", "Adam Yahiye Gadahn,", "antihistamine and an epinephrine auto-injector", "EU naval force", "workers walked off the job January 28 to protest the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "Deputy Treasury Secretary", "heavy turbulence", "success as a recording artist", "SSM Cardinal Glennon Children's Medical Center in St. Louis.", "two Emmys", "Henrik Stenson", "Three", "Dore Gold, former Israeli ambassador to the United Nations said, \"The IAEA has inspected the known nuclear sites of Iran.", "for using recreational drugs in September,", "Marin \u010cili\u0107", "Iran", "December 2, 2013", "Rutherford Hayes", "blue tang", "Morgan Spurlock", "football", "\"War & Peace\"", "Christopher Guest", "Floyd Patterson", "Jan & Dean", "Sweeney Todd: The Demon Barber of Fleet Street", "bass"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5842999340241987}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, false, true, false, false, false], "QA-F1": [0.5, 0.23529411764705882, 0.0, 1.0, 0.0, 0.15384615384615383, 0.8918918918918919, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.15384615384615383, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.4444444444444445, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-1509", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2479", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-2058", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-1121", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2170", "mrqa_triviaqa-validation-2074", "mrqa_hotpotqa-validation-3321", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-10144"], "SR": 0.453125, "CSR": 0.5191271551724138, "EFR": 0.9714285714285714, "Overall": 0.6739705203201971}, {"timecode": 58, "before_eval_results": {"predictions": ["at a depth of about 1,300 meters in the Mediterranean Sea.", "machine guns", "the 3rd District of Utah.", "Kenneth Cole", "\"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "President Bush", "Stephen Johns", "Transportation Workers Union", "eight Indian army troopers, including one officer, and 17 militants,", "23-year-old", "prisoners at the South Dakota State Penitentiary", "its interpretation of Islamic law, or sharia, on Somalia.", "food, music, culture and language", "Workers'", "is fighting an unjust war for an America that went too far when it invaded Iraq five years ago", "Pope", "boyhood experience in a World War II internment camp", "Secretary of State", "mother", "in a canyon in the path of the blaze Thursday.", "opium poppies", "1979", "Alexandre Caizergues,", "sportswear,", "a legitimate forum for prosecution, while bringing them in line with the rule of law,\"", "Paul McCartney and Ringo Starr", "many years of extensive technical research.", "iTunes,", "Indonesian", "eight-week", "order", "Natalie Cole", "Somali-based", "folding table", "the Obama administration", "Ventures", "British", "Long Island", "former Procol Harum bandmate Gary Brooker", "Former Mobile County Circuit Judge Herman Thomas", "Sen. Barack Obama", "July 23.", "stigma, shame and dishonor of asking for help,\"", "the lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains", "Hamburg", "a preliminary injunction against a Mississippi school district and high school", "105-year", "promoting fuel economy and safety", "a man's lifeless, naked body", "a major influence on portable media,", "eight", "the summer of 1990 and continued until 1992", "Cheitharol Kummaba", "Ricky Nelson", "Blind Faith", "Portugal,", "sphinx", "John Wilkes Booth", "Charles Nungesser", "The United States of America (USA),", "cherry almond", "tides", "bananas", "Leonard Bernstein"], "metric_results": {"EM": 0.5, "QA-F1": 0.6131221719457014}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.8, 0.5714285714285715, 0.47058823529411764, 1.0, 1.0, 0.0, 0.7692307692307693, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.09523809523809525, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.9, 0.0, 0.1111111111111111, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.888888888888889, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-2944", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-2135", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-2888", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-2617", "mrqa_naturalquestions-validation-4798", "mrqa_triviaqa-validation-3070", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-722", "mrqa_searchqa-validation-14467", "mrqa_searchqa-validation-7269"], "SR": 0.5, "CSR": 0.5188029661016949, "EFR": 0.96875, "Overall": 0.6733699682203389}, {"timecode": 59, "before_eval_results": {"predictions": ["Supplemental oxygen", "John Smith", "McFerrin", "Brahmagupta's Brahmasputha Siddhanta ( 7th century )", "the main competition section of the 74th Venice International Film Festival", "multiple", "Egypt", "Alan Tudyk", "a United States military prison", "the 1840s", "the 1940s", "2002", "helps scientists better understand the spread of pollution around the globe", "electors", "The post translational modification of proinsulin to mature insulin only occurs in the beta cells of the islets of Langerhans", "Italy", "Phone - A- Friend", "John F. Kelly", "$2.18 billion", "the 9th century", "a bowl", "February 7, 2018", "White Sox", "`` Reveille ''", "the United States emerged from World War II as a global superpower, the first country to develop nuclear weapons, the only country to use them in warfare, and a permanent member of the United Nations Security Council", "Tara / Ghost of Christmas Past", "Stephen Foster", "1975", "every year from 6 -- 14 July", "majority of members present at that time", "2,000 kilometres ( 1,200 mi ) down the Australian northeast coast", "`` Blood is the New Black ''", "1,300 km ( 800 mi ) from the nearest open sea at Bay of Whales", "the Supreme Court of Canada", "the primal rib", "in positions 14 - 15, 146 - 147 and 148 - 149", "Steve Valentine", "a political ideology", "Anakin Skywalker", "Lesley Gore's", "Tom Hanks", "Ex Unitate Vires", "peninsular", "Terry Kath", "16 best - selling religious novels", "`` skin - changer ''", "U2", "pre-Columbian times", "human induced greenhouse warming, and changes in the frequency and magnitude of El Ni\u00f1o events are a trigger to this strong warming in the Indian Ocean", "Wally Palmar", "Gorakhpur Junction", "Maarten Tromp", "sternum", "European racing", "Tainy Sledstviya", "travel diary", "Mechante Navstrechu", "South African teenager Caster Semenya", "Berga an der Elster", "Human Korean Foreign Ministry spokesman described U.S. Vice President Dick Cheney as a \"mentally deranged person steeped in the inveterate enmity towards the system\" in the North.", "Nellie Bly", "Red Adair", "reverence", "stirred"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5332298063456151}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.125, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5625, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.23076923076923078, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-2646", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-9881", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-255", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-7953", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-9683", "mrqa_naturalquestions-validation-8093", "mrqa_triviaqa-validation-945", "mrqa_hotpotqa-validation-3429", "mrqa_hotpotqa-validation-116", "mrqa_newsqa-validation-1934", "mrqa_newsqa-validation-2405", "mrqa_searchqa-validation-8408", "mrqa_searchqa-validation-3666"], "SR": 0.453125, "CSR": 0.5177083333333333, "EFR": 0.9714285714285714, "Overall": 0.6736867559523809}, {"timecode": 60, "UKR": 0.72265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1003", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1417", "mrqa_hotpotqa-validation-1604", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-2225", "mrqa_hotpotqa-validation-2302", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2739", "mrqa_hotpotqa-validation-2850", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-310", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3429", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3474", "mrqa_hotpotqa-validation-3485", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3767", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-4233", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4999", "mrqa_hotpotqa-validation-5038", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5317", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-870", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2646", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3062", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3795", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-9281", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9557", "mrqa_naturalquestions-validation-9778", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1044", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1590", "mrqa_newsqa-validation-1610", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1678", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1708", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1818", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2058", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-2335", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-289", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3286", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3521", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3867", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-65", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-983", "mrqa_searchqa-validation-1000", "mrqa_searchqa-validation-10210", "mrqa_searchqa-validation-10394", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-10752", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-10955", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-11973", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12698", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12763", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-12959", "mrqa_searchqa-validation-13328", "mrqa_searchqa-validation-13503", "mrqa_searchqa-validation-1399", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-14969", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15730", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-15839", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16642", "mrqa_searchqa-validation-16746", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-16894", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2791", "mrqa_searchqa-validation-3098", "mrqa_searchqa-validation-3295", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-4219", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-4922", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5521", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-5698", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-6659", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7317", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-7989", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-9176", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9683", "mrqa_searchqa-validation-9698", "mrqa_squad-validation-100", "mrqa_squad-validation-10370", "mrqa_squad-validation-10457", "mrqa_squad-validation-12", "mrqa_squad-validation-1254", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1540", "mrqa_squad-validation-1613", "mrqa_squad-validation-175", "mrqa_squad-validation-185", "mrqa_squad-validation-1902", "mrqa_squad-validation-194", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-2523", "mrqa_squad-validation-270", "mrqa_squad-validation-2762", "mrqa_squad-validation-2916", "mrqa_squad-validation-2986", "mrqa_squad-validation-3037", "mrqa_squad-validation-3125", "mrqa_squad-validation-3319", "mrqa_squad-validation-3511", "mrqa_squad-validation-3545", "mrqa_squad-validation-3708", "mrqa_squad-validation-3811", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3934", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-448", "mrqa_squad-validation-4546", "mrqa_squad-validation-4750", "mrqa_squad-validation-4860", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5050", "mrqa_squad-validation-5135", "mrqa_squad-validation-5276", "mrqa_squad-validation-5311", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5474", "mrqa_squad-validation-5669", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5736", "mrqa_squad-validation-6029", "mrqa_squad-validation-6351", "mrqa_squad-validation-6481", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6630", "mrqa_squad-validation-7023", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7531", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7689", "mrqa_squad-validation-7763", "mrqa_squad-validation-8066", "mrqa_squad-validation-8209", "mrqa_squad-validation-8386", "mrqa_squad-validation-8754", "mrqa_squad-validation-8772", "mrqa_squad-validation-8931", "mrqa_squad-validation-9012", "mrqa_squad-validation-9322", "mrqa_squad-validation-9495", "mrqa_squad-validation-9506", "mrqa_squad-validation-955", "mrqa_squad-validation-9590", "mrqa_squad-validation-9868", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4371", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6579", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6724", "mrqa_triviaqa-validation-6730", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7683"], "OKR": 0.8125, "KG": 0.49140625, "before_eval_results": {"predictions": ["hydrogen", "Sax Rohmer", "F. Lee Bailey, Robert Shapiro, Robert Kardashian and Alan Dershowitz.", "the Dominican Republic", "Crystal Gayle", "carry On Cleo", "Matilda", "a common thyroid-malfunction-based condition", "C\u00f4te d'Or", "Wisconsin", "Damson plums", "having an affair with their maid", "Wonga", "Secretary of State William H. Seward", "cryonics", "The World is Not Enough", "Barnaby Rudge", "James Chadwick", "Gary and Andrew", "visible path of a meteoroid", "Lesley Lawson", "California", "nitrogen", "Picasso's", "Alzheimer's disease", "piano", "King George I", "whip cream", "index fingers", "pain in the ear", "90%", "muezzin's", "the moon passes directly between the sun and Earth", "watts", "Alan Ladd", "black bean", "Virginia", "Charles Darwin", "PJ Harvey", "Jim Jones", "the grueling decathlon,", "Runcorn", "Three Worlds", "Amnesty International", "Maxwell", "Ann Darrow", "Arthur Miller", "Carousel", "Frankenstein", "potash", "Paris", "headdresses", "Janie Crawford", "Julia Roberts", "their unusual behavior, such as the number of men killed and the manner of the attacks.", "The Seduction of Hillary Rodham", "Apsley George Benet Cherry-Garrard", "\"Toy Story\"", "86", "allegations that a dorm parent mistreated students at the school.", "a daiquiri", "apples", "Morocco", "the nerves and ganglia outside the brain and spinal cord"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5805989583333333}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-3379", "mrqa_triviaqa-validation-4764", "mrqa_triviaqa-validation-4827", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-6812", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-657", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-240", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-6549", "mrqa_triviaqa-validation-3020", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-3854", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-2261", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-6285", "mrqa_hotpotqa-validation-3308"], "SR": 0.53125, "CSR": 0.5179303278688525, "EFR": 0.9666666666666667, "Overall": 0.7022318989071038}, {"timecode": 61, "before_eval_results": {"predictions": ["a paragraph about the king and crown prince", "Mugabe and Tsvangirai", "al-Shabaab", "Samuel Herr", "at checkposts and military camps in the Mohmand agency,", "Cannes", "customers are lining up for vitamin injections that promise", "Yusuf Saad Kamel", "Andrew Morris,", "Alberto Espinoza Barron,", "22 million", "41,280", "Casey Anthony,", "18", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "Phoenix, Arizona,", "the secrets of the Ku Klux Klan,", "lion", "between 1917 and 1924", "Mississippi", "$3 billion,", "Steve Williams", "school in South Africa", "Dr. Jennifer Arnold and husband Bill Klein,", "Up", "The Real Housewives of Atlanta", "bollywood", "Wigan Athletic", "40-year-old", "1,500 Marines", "California-based Current TV", "civilians,", "Michelle Obama", "Ayegbeni Yakubu", "returning combat veterans", "Kit of Elsinore", "\"black rain\"", "thai", "staff sergeant", "nine", "Jason Voorhees", "george Washington", "chairman of the House Budget Committee", "al-Shabaab", "Werder Bremen", "discusses his roots as he castigates U.S. policies and deplores Israel's offensive in Gaza", "HSH Nordbank Arena", "German Chancellor", "rare thriller writer", "1994", "Carrousel du Louvre,", "Prem Lata Agarwal", "G. Hannelius", "Terry Kath", "geocentric", "William Shakespeare", "Nicolas Sarkozy", "Melanie Owen", "Thor", "Piedmont", "george III", "Taj Mahal", "colon", "Carol Worthington"], "metric_results": {"EM": 0.59375, "QA-F1": 0.630078125}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.125, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-626", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-3209", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-3139", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2955", "mrqa_naturalquestions-validation-10268", "mrqa_triviaqa-validation-4714", "mrqa_hotpotqa-validation-4692"], "SR": 0.59375, "CSR": 0.5191532258064516, "EFR": 0.8846153846153846, "Overall": 0.6860662220843672}, {"timecode": 62, "before_eval_results": {"predictions": ["500 feet down an embankment", "\"outlaws\"", "1960s song \"A Whiter Shade of Pale\"", "Ciudad Juarez,", "Kenneth Cole", "little blue booties.", "David McKenzie", "$8.8 million", "\"I'm just getting started.\"", "Saturday,", "few security risks.", "14 years", "planning processes are urgently needed", "Mandi Hamlin", "Malcolm X", "financial gain,", "to never make the cut for a face-to-face interview with the president", "Six", "Steve Jobs", "\"Fluid is building up in a contained space, creating pressure. Something's got to give, and that something is the brain,\"", "Daniel Radcliffe", "Tillakaratne Dilshan", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "a construction site in the heart of Los Angeles.", "Jaipur", "Al-Aqsa mosque", "Ameneh Bahrami", "criminals", "whether to close some entrances, bring in additional officers, and make security more visible,\"", "\"theoretically\"", "Transportation Security Administration", "E. coli", "Six members of Zoe's Ark", "The United States", "100 percent", "Brian Mabry", "150", "\"Watchmen\"", "autonomy.", "\"She was focused so much on learning that she didn't notice,\"", "\"We can travel around three days with no light,\"", "two", "responsibility for the abductions.", "Brian David Mitchell,", "Osama bin Laden's sons", "as part of its 18-month journey around the world.", "Italian Serie A title", "an independent homeland since 1983.", "Tutsi and Hutu", "How can we prevent this barbaric crime that assaults our humanitarian values and threatens our national security?", "is looking at designating the sedative as a \"scheduled\" drug,", "William Shakespeare's As You Like It", "De Wayne Warren", "1954", "jackstones", "goke", "scene", "designated hitter", "\"The 8th Habit\"", "11 June 1959", "Plutarch", "Buddhist", "cab", "seven nights a week"], "metric_results": {"EM": 0.5, "QA-F1": 0.6002793062915346}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, false, false, false, true, true, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 1.0, 0.08695652173913043, 0.0, 1.0, 0.0, 1.0, 1.0, 0.12121212121212123, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.4, 0.3333333333333333, 0.16666666666666666, 0.0, 0.7692307692307692, 0.4, 0.4, 0.0, 0.0, 0.0, 0.8, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-863", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2336", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-388", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3617", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-6383", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-6532", "mrqa_triviaqa-validation-6505", "mrqa_hotpotqa-validation-2543", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-3266", "mrqa_searchqa-validation-16570"], "SR": 0.5, "CSR": 0.5188492063492063, "EFR": 0.96875, "Overall": 0.7028323412698413}, {"timecode": 63, "before_eval_results": {"predictions": ["Jaime Andrade", "strongly denied charges of racism after his club canceled the swimming privileges of a nearby day care center whose children are predominantly African-American.", "President Obama and Britain's Prince Charles", "the insurgency,", "Reporters Without Borders", "the Genocide Prevention Task Force.", "Britain's", "five female pastors", "Bob Bogle,", "India", "82", "Fullerton, California,", "$249", "Zulfikar Ali Bhutto,", "two African-Americans in her senior class", "and Climatecare,", "\"Watchmen\"", "President Obama and Britain's Prince Charles", "public opinion in Turkey.\"", "10 municipal police officers", "left his indelible fingerprints on the entertainment industry.", "murder in the beating death of a company boss who fired them.", "\"And even though she's not here anymore, I'm not afraid to say it, sometimes she was a pain in the ass,\"", "\"political and religious\"", "Siri", "Derek Mears", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Heshmat Tehran Attarzadeh", "skull,", "because the Indians were gathering information about the rebels to give to the Colombian military.", "--the Louvre.", "Mubarak,", "Princess Diana", "revelry", "Human Rights Watch", "45 minutes,", "February 12", "1959,", "London", "\"illegitimate.\"", "in England and Scotland,", "Russia", "nations such as France had not been able to fulfill their promises due to pressures on budgets and suggested they needed to \"raise the priority so the promises are met.\"", "Zuma", "saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "to sniff out cell phones.", "shock,", "U.S. Food and Drug Administration", "$1.5 million.", "former Pakistani Prime Minister Nawaz Sharif", "raping and killing a 14-year-old Iraqi girl.", "in 1837", "raconteur ( Austin Winkler ) and his former lover ( Emmanuelle Chriqui )", "2005", "\"Raging Bull,\"", "bluebells", "FIFA World Cup", "Edmund Henry Hynman Allenby", "turns out to be a terrible date", "The Treaty of Gandamak", "the United Kingdom", "juliuss efface", "beak", "Candide"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6523864156676656}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.26666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.17142857142857143, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.125, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.36363636363636365, 1.0, 0.0, 1.0, 0.125, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.3636363636363636, 1.0, 1.0, 1.0, 0.8571428571428571, 0.8, 0.923076923076923, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1462", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-2894", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3293", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-3401", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-1098", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-3482", "mrqa_triviaqa-validation-3824", "mrqa_hotpotqa-validation-5720", "mrqa_hotpotqa-validation-4086", "mrqa_searchqa-validation-13103", "mrqa_searchqa-validation-1971"], "SR": 0.53125, "CSR": 0.51904296875, "EFR": 1.0, "Overall": 0.70912109375}, {"timecode": 64, "before_eval_results": {"predictions": ["Wolfgang Amadeus Mozart", "Denmark\u2013Norway, Brandenburg and Sweden", "John Schlesinger", "valuation method", "Phineas and Ferb", "Two Is Better Than One", "Karl-Anthony Towns,", "Omega SA", "May 27, 2016", "the designated hitter rule", "Jay Park", "Wayne County, Michigan", "Japan and Hong Kong", "Cleopatra VII Philopator", "8,211", "The Allies of World War I,", "August Heckscher", "Orange County, Florida, United States", "Gareth Barry", "capital crimes", "Ned Flanders", "Westley Sissel Unseld", "Ken Howard", "Fat Albert", "Thomas Joseph \"T. J.\" Lavin", "15 mi", "Germany", "I-League club Salgaocar.", "\"Fudge\"", "1887", "Tuesday", "January 2001", "Sony Studio Liverpool", "England", "Ry\u016bkyuan", "Charles White Whittlesey", "fennec", "from 1993 to 1996", "Yunnan-Fu", "Port Moresby, Papua New Guinea", "Nossa Senhora de F\u00e1tima, Macau, China.", "1993,", "The Bangor Daily News is an American newspaper covering a large portion of rural Maine,", "The Land of Enchantment", "the State of Hawaii", "William Finn", "My Backyard", "homosexuality, gay sex, and the gay bear subculture.", "The Tales of Hoffmann", "chronological collection of critical quotations about William Shakespeare", "October 3, 2017", "1984 Summer Olympics in Los Angeles", "Afghanistan", "Michael Crawford", "parallelogram", "Operation Frequent Wind", "Matthew 2:11", "toxic smoke from burn pits in Iraq", "15,000", "breast cancer.", "Troy", "Casablanca", "Vilna", "Johannes Kepler"], "metric_results": {"EM": 0.5, "QA-F1": 0.6397268963675213}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5714285714285715, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 0.125, 1.0, 0.5, 1.0, 1.0, 0.15384615384615383, 1.0, 0.7692307692307693, 1.0, 0.4444444444444444, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-4702", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-3337", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-589", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-5115", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-802", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-1394", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-5655", "mrqa_naturalquestions-validation-75", "mrqa_triviaqa-validation-6706", "mrqa_triviaqa-validation-305", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-4147", "mrqa_searchqa-validation-7441", "mrqa_searchqa-validation-4036", "mrqa_triviaqa-validation-2982"], "SR": 0.5, "CSR": 0.51875, "EFR": 1.0, "Overall": 0.7090624999999999}, {"timecode": 65, "before_eval_results": {"predictions": ["10 October 2010", "environmental activist", "Claudio Javier L\u00f3pez", "on the Las Vegas Strip in Paradise, Nevada", "the Swiss Confederation", "I Should Have Known Better", "Bruce Willis", "Dan Brandon Bilzerian", "Philadelphia Naval Shipyard", "The Pentagon", "1974", "our greatest comedienne - Australia's Lucille Ball", "Canadian", "Kim Yeon-soo", "spot-fixing", "The Monster", "the Easter Rising of 1916", "Spain, Mexico and France", "Juilliard School", "Mark \"Chopper\" Read", "small forward", "37,776", "Floridians", "Erreway", "Eric Liddell", "Sam Kinison", "Spring city", "Bob Hurley", "fortnightly", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "9,984", "Sunset Publishing Corporation", "the Wabanaki Confederacy", "1966", "the Donny & Marie Showroom, at the Flamingo Las Vegas", "The Soloist", "1st Earl Grosvenor", "Labour Party", "Walt Disney", "1983", "stolperstein", "heavy metal fan", "BMW X6", "Cleveland Celtics", "Larnelle Steward Harris", "May 5, 2015", "Bank of China Tower", "general secretary of the Norwegian Anthroposophical Society", "Barnoldswick", "New Orleans, Louisiana", "eight", "IB Primary Years Program", "Mark Jackson", "1991", "antigua and Barbuda", "trout", "Montpelier", "allergen-free", "African National Congress Deputy President Kgalema Motlanthe,", "13.", "Portugal", "splice", "Boston Legal", "the 2009 model year"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6527690018315019}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, true, true, false, true, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.8333333333333333, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.15999999999999998, 1.0, 0.3333333333333333, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 0.0, 0.0, 0.5714285714285715, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-71", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3904", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-3536", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-1123", "mrqa_naturalquestions-validation-9130", "mrqa_triviaqa-validation-1138", "mrqa_newsqa-validation-3733", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-16601"], "SR": 0.53125, "CSR": 0.5189393939393939, "EFR": 1.0, "Overall": 0.7091003787878788}, {"timecode": 66, "before_eval_results": {"predictions": ["October 25, 1881", "Harold Holt", "1970", "Despicable Me 3", "Bill Clinton", "1,800", "Frederick I", "Bring Me Sunshine (1994) was originally a three-part retrospective in tribute to Eric Morecambe", "Juve", "November 6, 2018", "four sections", "Championnat National 3", "five", "Macau, China", "American black bear", "1345 to 1377", "sandstone", "August 19, 2016", "VIMN Russia", "rickyard", "Harlem neighborhood of New York City", "season one episode \"Boating School\"", "Parapsychologist", "2014 New Year Honours", "Greg Gorman and Helmut Newton", "Prince Ioann Konstantinovich", "2015", "Winecoff Hotel fire", "B-17 Flying Fortress bomber", "Coinapult", "Hawaii", "The Emperor of Japan", "the Ruul", "Dana Fox", "remixes", "Issaquah", "Ghana's", "Scott Paul Carson", "John Snow", "Manchester, England", "\"Twister\"", "Free and Sovereign State of Tamaulipas", "boundary river", "\"Naked\"", "technical director", "Queen Victoria", "Las Vegas", "Cleveland, Ohio", "Brenton Thwaites", "AVN Adult Entertainment Expo", "Boyd Gaming", "Hirschman", "The Romantics", "the brain, muscles, and liver", "Neville Chamberlain", "the French Open", "Czech Republic", "18", "dual nationality", "Cpl. Richard Findley,", "rain", "the R.M.S Titanic", "neon", "customers are lining up for vitamin injections that promise"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6299603174603174}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 0.8, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.4, 1.0, 0.2222222222222222, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-3578", "mrqa_hotpotqa-validation-2541", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-1676", "mrqa_hotpotqa-validation-3287", "mrqa_hotpotqa-validation-5544", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-5359", "mrqa_hotpotqa-validation-3126", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-5281", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-843", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4117", "mrqa_hotpotqa-validation-618", "mrqa_hotpotqa-validation-4743", "mrqa_hotpotqa-validation-2786", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-7704", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-95", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-2698"], "SR": 0.546875, "CSR": 0.5193563432835822, "EFR": 0.9655172413793104, "Overall": 0.7022872169325785}, {"timecode": 67, "before_eval_results": {"predictions": ["Brian", "1956", "Portugal", "North Atlantic Ocean", "eleven", "Anthony Hopkins", "Andrew Gold", "$19.8 trillion", "a hyper - active kinase", "January 15, 2007", "April 17, 1982", "Super Bowl XXXIX", "Andrea Framm", "31", "Louis XV", "White Sox", "Korean Republic Won", "Gospel of Luke", "Executive chef Danny Veltri", "Narendra Modi", "Ossie Schectman", "Max Martin", "Marty Robbins", "Percy Jackson", "state legislators of Assam", "by hunting with his mother's side of the family", "to feel close to his son", "Chris Martin", "`` The person who has existence in two paradise", "Spanish / Basque", "circa 1439", "Jason Momoa", "Terrence Howard", "1868 war veterans", "Elena Anaya", "body", "2014", "Pangaea or Pangea", "from Fort Kent, Maine, at the Canada -- US border", "Robin", "Mount Sinai", "Vincent Price", "Nepal", "actions taken by employers or unions that violate the National Labor Relations Act of 1935", "on Blu - ray and DVD in region 1 on December 12, 2017", "March 31 to April 8", "Manhattan", "1969", "on location", "Neil Young", "provinces along the Yangtze River and in provinces in the south", "Thomas Jefferson", "stratovolcanic archipelago", "attario Arch on the song Something In The Air", "Adam Dawes", "Robbie Gould", "Musicology", "two years,", "Lashkar-e-Tayyiba", "HPV (human papillomavirus)", "Jackie Kennedy", "necropolis", "Spider-Man", "Fidel"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6895963794171515}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.35294117647058826, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.28571428571428575, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.13333333333333333, 0.25, 1.0, 0.7999999999999999, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.125, 1.0, 0.5, 0.761904761904762, 1.0, 0.5, 1.0, 1.0, 0.5384615384615384, 0.4, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-5775", "mrqa_naturalquestions-validation-70", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-2381", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6670", "mrqa_naturalquestions-validation-8796", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-10261", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-9107", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-1629", "mrqa_newsqa-validation-1170", "mrqa_searchqa-validation-13420", "mrqa_searchqa-validation-8650"], "SR": 0.515625, "CSR": 0.5193014705882353, "EFR": 0.9032258064516129, "Overall": 0.6898179554079695}, {"timecode": 68, "before_eval_results": {"predictions": ["the Archies", "Tzeitel", "2018", "James Madison", "one", "September 29, 2017 for the 2017 -- 18 television season", "c. 1000 AD", "Samantha Jo `` Mandy '' Moore", "March 1995", "1957", "Andrew McCarthy as Blane McDonough", "an investor couple", "Erica Rivera", "the dermis", "the cavities and surfaces of blood vessels and organs throughout the body", "England, Wales, Ireland and the Isle of Man", "toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "11 : 15 p.m.", "Sylvester Stallone", "Patricia Field", "the Director of National Intelligence", "November 25, 2002", "S - shaped", "Marley & Me", "State Bar of Arizona", "31 October 1851", "summer", "The Romantics", "W. Edwards Deming", "Danny and Pam", "The Comanche / k\u0259\u02c8m\u00e6nt\u0283i\u02d0 / ( Comanche : N\u0289m\u0289n\u0289\u0289 ) are a Native American nation from the Great Plains", "1992", "presidential representative democratic republic", "Eddie Murphy as Prince Akeem Joffer", "Vincent Price", "Steve Zahn as `` Bad Ape ''", "Olivia d'Abo", "O'Meara", "Kimberlin Brown", "writ of certiorari", "the largest financial inflows to developing countries", "The Blind Boys of Alabama, Tom Waits, The Neville Brothers, DoMaJe, and Steve Earle", "Reveille", "The Constitution of India", "StubHub Center", "circa 1439", "plate tectonics", "Himadri Station", "Rajendra Prasad", "1997", "March 11, 2016", "a psychologist", "the Irish Setter Puppies", "attie McDaniel", "Tian Tan Buddha", "Bonkyll Castle", "Danny Elfman", "The Iraqi official said the area was sealed off, so they did not know casualty figures.", "Australian Environment Minister Peter Garrett", "Turkey", "piracy", "lifejackets", "a serve", "Josh"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6400755840064426}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.4615384615384615, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.7058823529411764, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.5, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.8, 0.5, 1.0, 1.0, 1.0, 0.06451612903225808, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-9521", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-8950", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-4605", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-6490", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-1340", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-5839", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-101", "mrqa_searchqa-validation-1397"], "SR": 0.53125, "CSR": 0.5194746376811594, "EFR": 0.9666666666666667, "Overall": 0.7025407608695652}, {"timecode": 69, "before_eval_results": {"predictions": ["Syracuse University", "The Fault in Our Stars", "Stadio Olimpico in Rome, Italy", "traditional music", "water", "May 4, 2004", "Northumbrian", "2001", "American Horror Story", "Salisbury", "University of Nevada, Las Vegas (UNLV)", "1-800-273-8255", "November of that year", "Alfond Stadium", "compact car", "Jos\u00e9 Bispo Clementino dos Santos", "Laurie Metcalf", "Kingdom of Dalmatia", "Russell T Davies", "Darci Kistler", "Oliver Parker", "he turned 51, he died of cancer", "Snowball II", "Oakland", "FCI Danbury", "Division of Barton", "Bob Day", "The Blue Ridge Parkway", "1875", "London", "Shut Up", "James Lofton", "Big Bad Wolf", "Dennis Hull,", "books, films", "Iranian-American", "You're Next", "Bay of Fundy", "2009", "Umberto II", "2016", "War Is the Answer", "1891", "Supergirl", "DI Humphrey Goodman", "Cinderella", "1,382", "left-arm", "Syracuse", "Buckingham Palace", "Rhode Island", "Anglican", "powers in the Eastern Bloc ( the Soviet Union and its satellite states )", "( 1972 -- 81 )", "barba", "Rodgers and Hammerstein", "Ramadan", "the Dalai Lama's", "70,000 or so", "1994", "Lord Peter Wimsey", "sulfuric acid", "neon", "Sean Maddox"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6900602002164502}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9090909090909091, 1.0, 0.4, 0.0, 0.4, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5619", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3504", "mrqa_hotpotqa-validation-4315", "mrqa_hotpotqa-validation-3703", "mrqa_hotpotqa-validation-4352", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-4473", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-278", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5942", "mrqa_triviaqa-validation-2405", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-1720", "mrqa_searchqa-validation-4425", "mrqa_triviaqa-validation-7105"], "SR": 0.59375, "CSR": 0.5205357142857143, "EFR": 1.0, "Overall": 0.7094196428571429}, {"timecode": 70, "UKR": 0.75390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-139", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4646", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5611", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-875", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3330", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-9056", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1527", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1998", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3067", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3083", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10176", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11005", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13441", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-13909", "mrqa_searchqa-validation-14094", "mrqa_searchqa-validation-14204", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14752", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16534", "mrqa_searchqa-validation-16733", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-5457", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7045", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-7326", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-7903", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8807", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-944", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-10284", "mrqa_squad-validation-10352", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1498", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2123", "mrqa_squad-validation-215", "mrqa_squad-validation-2197", "mrqa_squad-validation-2248", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3464", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3750", "mrqa_squad-validation-3813", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-3904", "mrqa_squad-validation-4096", "mrqa_squad-validation-4469", "mrqa_squad-validation-457", "mrqa_squad-validation-4669", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-4949", "mrqa_squad-validation-5042", "mrqa_squad-validation-5325", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-5869", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-6636", "mrqa_squad-validation-682", "mrqa_squad-validation-6838", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-76", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-7932", "mrqa_squad-validation-8028", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8500", "mrqa_squad-validation-8529", "mrqa_squad-validation-8719", "mrqa_squad-validation-8754", "mrqa_squad-validation-8966", "mrqa_squad-validation-9165", "mrqa_squad-validation-9306", "mrqa_squad-validation-9531", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4953", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5928", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6784", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6791", "mrqa_triviaqa-validation-6879", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-934"], "OKR": 0.8125, "KG": 0.45390625, "before_eval_results": {"predictions": ["November 17, 2017", "Melissa Disney", "Andy Kim", "writ of certiorari", "Edgar Lungu", "December 25", "Marcus Atilius Regulus", "Ahmad Givens", "You are a puzzle", "Los Angeles", "a crust of mashed potato", "Johann Karl Nestler", "October 1, 2015", "New Zealand", "Los Angeles", "pit road speed", "laurel wreath", "1 atm", "1923", "Narin Niruttinanon", "Michael Schumacher", "dispense summary justice", "John Quincy Adams", "Ray Charles", "Days of Our Lives", "1 ) 2013", "one or two layers of cells", "Upstate New York", "Henry Purcell", "5,534", "Lord Irwin", "three", "Roman Reigns", "Efren Manalang Reyes", "the second Persian invasion of Greece", "a type of appropriations legislation", "2004", "Fix You", "Ed Roland", "Jos\u00e9 Mart\u00ed", "16 August 1975", "when the forward reaction proceeds at the same rate as the reverse reaction", "Sumitra", "The management team", "Ravi River", "1837", "Pangaea", "Sally Field", "a cylinder of glass or plastic that runs along the fiber's length", "Dr. Sachchidananda Sinha", "Barbara Windsor", "limestone", "The Hague", "The World is Not Enough", "Her mother Woizero Aselefech Wolde Hanna", "Leona Lewis", "Hong Kong Disneyland", "\"peregruzka\"", "an average of 25 percent", "President Barack Obama,", "heldon", "Galileo Galilei", "Aristophanes", "Amy Bishop Anderson,"], "metric_results": {"EM": 0.625, "QA-F1": 0.6949792960662526}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 0.17391304347826086, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428572, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-8326", "mrqa_hotpotqa-validation-2588", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2361", "mrqa_searchqa-validation-14422", "mrqa_searchqa-validation-10531", "mrqa_newsqa-validation-2288"], "SR": 0.625, "CSR": 0.5220070422535211, "EFR": 0.9583333333333334, "Overall": 0.700130575117371}, {"timecode": 71, "before_eval_results": {"predictions": ["Jackie", "Chester", "superhuman abilities", "76,416", "Rawhide", "Anderson Silva", "University of Southern California Trojans", "Orfeo ed Euridice", "Martin Scorsese", "Citgo", "Fukuoka Airport", "Southern Progress Corporation", "Laura Dern", "British Conservative Party", "Mickey's PhilharMagic", "45", "1241", "Hamburger Sport-Verein e.V.", "Kolkata", "Liga MX", "\"Queen City\"", "Dz\u016bkija Alytus", "Texas", "six", "Rob Reiner", "The R-8 Human Rhythm Composer", "WAMC", "67,575", "Hong Kong Mak\u00e9l\u00e9l\u00e9", "Ribosomes", "Sacramento Kings", "1945", "Fairfax County", "Syracuse University", "John Mills", "Martin \"Marty\" McCann", "Blue Ridge Parkway", "the Maldives", "rayon", "Ella Jane", "2000", "0.500", "William Bradley", "Coronation Street", "Duke", "Richard Masur", "anabolic\u2013androgenic steroids", "diving duck", "House of Hohenstaufen", "local South Australian and Australian produced content", "EBSCO Information Services", "Karen Taylor", "as a pH indicator, a color marker, and a dye", "Thomas Edison", "The town is served by Woodbridge railway station on the Ipswich-Lowestoft East Suffolk Line", "runic", "cycling", "Brett", "58 people", "Security officer Stephen Johns reportedly opened the door for the man police say was", "osprey", "Fyodor Dostoevsky", "Turtle Wax", "Dangjin"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6899677579365079}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, false, true, true, false, true, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.5, 0.25, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1395", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-1220", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-2938", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-1000", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-1545", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4357", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-7849", "mrqa_triviaqa-validation-4244", "mrqa_newsqa-validation-3956", "mrqa_newsqa-validation-2439", "mrqa_searchqa-validation-14348"], "SR": 0.578125, "CSR": 0.5227864583333333, "EFR": 1.0, "Overall": 0.7086197916666667}, {"timecode": 72, "before_eval_results": {"predictions": ["direct scattering and inverse scattering", "International Society for the Study of the Origin of Life", "Australian Supercars Championship", "The Jacksonville Jaguars", "\"Crossed: Badlands\"", "the U.S. Navy", "\"Gesellschafter\"", "B507", "Laurie Metcalf", "Nine-card Brag", "\"Godspell\"", "freshman", "Free Range Films", "Mondays", "\"Beauty and the Beast\"", "Jehovah", "fourth President of Pakistan", "Tamara Ecclestone Rutland", "Newport", "Tomorrowland", "1985", "David Michael Bautista Jr.", "1002", "1", "John Meston", "late 19th and early 20th centuries", "Terrence Alexander Jones", "1967", "Casey Bond", "actress", "Christian Kern", "naomi Elaine Campbell", "The dyers of Lincoln", "2013\u201314", "Ghostbusters Spooktacular", "an American financier", "the Maldives", "Them", "its air-cushioned sole", "4,972", "Brittany Anne Snow", "Perth, Western Australia", "Harry F. Sinclair", "Interstate 22", "William McKinley", "Denmark", "Tunisian", "lieutenant general", "Indian state", "Sir Philip Anthony Hopkins", "Raimond Gaita", "In 2015, 13.5 %", "1980", "Isaiah Amir Mustafa", "Michigan", "Jon Stewart", "Australia", "Fourteen", "The National Association of Broadcasters", "July", "Michelangelo", "airplanes", "the Confederate Memorial", "buying"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6760365936147186}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, false, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, false, false], "QA-F1": [0.8333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.1818181818181818, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 0.375, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-706", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-4830", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-294", "mrqa_hotpotqa-validation-1612", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-723", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-5582", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-1007", "mrqa_triviaqa-validation-982", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-270", "mrqa_searchqa-validation-16142", "mrqa_searchqa-validation-5340"], "SR": 0.5625, "CSR": 0.5233304794520548, "EFR": 1.0, "Overall": 0.7087285958904109}, {"timecode": 73, "before_eval_results": {"predictions": ["SAS Fr\u00f6sundavik", "2011", "Indianapolis, Indiana", "a lauded intellectual", "1970", "the 1745 rebellion of Charles Edward Stuart", "96", "a Christian evangelist", "West African descendants", "Rigoletto", "James Stenbeck", "December 19, 1998", "Philip Quast", "November 20, 1942", "Argentine cuisine", "22 September 2015", "David Walliams", "Waylon Smithers", "2006", "Scotland", "Lakshmibai", "Umberto II", "England", "beer and soft drinks", "five", "C. H. Greenblatt", "The first stories appeared in ancient Assyria, in which the goddess Atargatis transformed herself into a mermaid out of shame for accidentally killing her human lover", "1535", "There Is Only the Fight... : An Analysis of the Alinsky Model", "England", "Lord Gort", "Laurel, Mississippi", "The pronghorn", "Wayne Conley", "churros", "1802", "Key West", "Pablo Escobar", "Dave Bautista", "Iftikhar Ali Khan", "October 25, 1881", "Bob Dylan", "Newcastle Brown Ale", "Harrison Ford", "PBS", "Law Adam", "1943", "wineries", "The Hork-Bajir Chronicles", "Jeffrey Meldrum", "quantum mechanics", "early - to - mid fourth century", "The drive serves as the main road through the gated community of Pebble Beach", "$2.187 billion", "In the more peaceful times following the 1745 Jacobite rebellion, the British Fisheries Society was charged with establishing fishing stations to exploit the local stocks, and hence create employment", "Jeffrey Archer", "Oliver Harmon Jones", "Galveston, Texas,", "celebrities", "are concerned that the legislation will foster racial profiling, arguing that most police officers don't have enough training to look past race while investigating a person's legal status.", "the Ohio", "the beaver", "W. Somerset Maugham", "bullfighting"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6569793328017012}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, true, true, true, true], "QA-F1": [0.5, 0.0, 1.0, 0.4444444444444445, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.08333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.10526315789473685, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-3153", "mrqa_hotpotqa-validation-1619", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-1423", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-3360", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-4446", "mrqa_hotpotqa-validation-5747", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-3363", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-5343", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-1116", "mrqa_naturalquestions-validation-3363", "mrqa_triviaqa-validation-535", "mrqa_triviaqa-validation-7264", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-3584"], "SR": 0.59375, "CSR": 0.5242820945945945, "EFR": 1.0, "Overall": 0.708918918918919}, {"timecode": 74, "before_eval_results": {"predictions": ["1946", "Bury, Greater Manchester", "Nye County", "1946 and 1947", "two", "a governor", "40 Acres and a Mule Filmworks", "The Panther", "al-Qaeda", "China", "India", "Skyscraper", "17 October 2006", "April 1, 1949", "Big Machine Records", "Brady Haran", "Jesus", "Heywood \"Woody\" Allen", "tree", "Elliot Fletcher", "Joe McCoy and Memphis Minnie", "Martha Wainwright", "Christopher McCulloch (also known as \"Jackson Publick\")", "Strange Interlude", "1989 until 1994", "Tallahassee City Commission", "the Royal Air Force", "Mika Pauli H\u00e4kkinen", "World Music Awards", "1998", "Peter 'Drago' Sell, (born August 5, 1982) is an American mixed martial artist specializing in Brazilian Jiu Jitsu", "\"Barney Miller\"", "oyote Ugly", "\"Odorama\", whereby viewers could smell what they saw on screen through scratch and sniff cards", "A Song of Ice and Fire", "Andy Roddick", "Bigfoot (also known as Sasquatch) is a cryptid which supposedly is a simian-like creature", "Kim Jong-hyun", "College Football Scoreboard", "A Boltzmann machine", "January 1930", "12", "\"Pour le M\u00e9rite\"", "Franklin, Indiana", "The virus is zoonotic,", "October 6, 1931", "New Boston Air Force Station", "Miracle", "Elizabeth River", "Metrolink", "\"Complex\" magazine", "the oneness of the body, the church, through what Christians have in common, what they have communion in", "Covington, Kentucky", "Elizabeth Dean Lail", "doubles", "Newcastle Falcons", "Rihanna", "Sarah,", "order after demonstrators rose up across Greece Monday in a third day of rioting over Saturday's killing of", "a nuclear weapon", "Pirates of the Burning Sea", "In 1997, the City of Bridgeport, Connecticut (pop. 140,000) received a Best Practices award from the US Conference of Mayors", "A gag is usually an item or device designed to prevent speech, often as a restraint device to stop the subject from calling for help and keep its wearer quiet", "pluripotent"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6830212466931216}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false], "QA-F1": [0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.8571428571428571, 1.0, 0.4, 0.8, 0.0, 1.0, 0.3, 0.6666666666666666, 0.5, 0.125, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.4, 0.1111111111111111, 0.07407407407407407, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-4499", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-958", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-463", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-3336", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-5475", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-5628", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-2579", "mrqa_triviaqa-validation-2322", "mrqa_newsqa-validation-122", "mrqa_searchqa-validation-5810", "mrqa_searchqa-validation-14546", "mrqa_searchqa-validation-13104", "mrqa_searchqa-validation-12635"], "SR": 0.546875, "CSR": 0.5245833333333334, "EFR": 1.0, "Overall": 0.7089791666666667}, {"timecode": 75, "before_eval_results": {"predictions": ["France", "Shanghai", "Paul Bunyan", "Peter Davison", "Pandora", "Turton Tower", "Alaska", "Cowboy Builders", "the DeLorean Motor Company", "VH-71 Kestrel", "Lundy Island", "Car Del Mar", "David Hockney", "spark-ignition", "Janis Joplin", "long type", "Humphrey Bogart", "Christopher Robin", "Antoine Lavoisier", "1960", "the Caribbean Sea", "King County Executive", "St Brendan Dan", "U.S. ambassador to the United Nations", "Adam Smith", "the Cuban missile crisis", "lime", "Larry Bird, Charles Barkley, Patrick Ewing, Mugsy Bogues, Vlade Divac, Larry Johnson and Alonzo Mourning", "littoral seaweed", "Earth", "Declaration of Independence", "decorat", "Hot Chocolate", "Jim Peters", "Armageddon", "Victoria", "Duck Soup", "Berlin", "Neuna", "salmon", "the Wild Bunch", "Flight 815", "Project Gutenberg", "Bloodaxe", "California", "Nissan", "Isar", "Live and Let Die", "Salvador Allende", "The Green Mile", "Poland", "around 1600 BC", "Sean O' Neal", "May 18, 2018", "Dissection", "Baden-W\u00fcrttemberg", "March", "$60 billion", "Mary Phagan,", "$500,000", "vitamin A", "Azkaban", "conga drums", "Vibe"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6230368589743589}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30769230769230765, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2230", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-444", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-2875", "mrqa_triviaqa-validation-191", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-225", "mrqa_triviaqa-validation-4242", "mrqa_triviaqa-validation-4612", "mrqa_triviaqa-validation-85", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-3944", "mrqa_triviaqa-validation-3556", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-7549", "mrqa_hotpotqa-validation-65", "mrqa_newsqa-validation-1977"], "SR": 0.578125, "CSR": 0.5252878289473684, "EFR": 0.9629629629629629, "Overall": 0.7017126583820663}, {"timecode": 76, "before_eval_results": {"predictions": ["the murders of his father and brother", "a \"happy ending\" to the case.", "head injury.", "Bryant Purvis,", "1983", "Chaffetz", "Purvis,", "jazz", "$30 million,", "1994,", "his album \"Tha Carter III\"", "peanuts, nuts, shellfish and fish", "the U.S. program to assassinate terrorists in Iraq.", "April 2010.", "The man ran out of bullets and blew himself up.", "October 19,", "Addis Ababa,", "a president who understands the world today, the future we seek and the change we need.", "five", "St. Louis, Missouri.", "if Gadhafi suffered the wound in crossfire or at close-range", "off Somalia's coast.", "the Indian army and separatist militants in Indian-administered", "The Charlie Daniels Band", "Egypt", "planning processes are urgently needed", "Saturday.", "Obama", "\"a whole new treasure trove of fossils\"", "and renewable energy at home everyday,\"", "insect stings,", "Kenneth Cole", "a one-shot victory in the Bob Hope Classic on the final hole", "Natalie Cole's", "\"The people kill him with the blocks, because the people are angry. They are not hungry, they are angry,\"", "Hundreds", "Current TV", "Colorado prosecutor", "100% of its byproducts", "partially submerged in a stream in Sharks River Park in Monmouth County", "Galveston, Texas, to Veracruz, Mexico,", "nine-wicket", "70,000", "the Nazi war crimes suspect who had been ordered deported to Germany,", "his former caddy,", "three", "fatally shooting a limo driver on February 14, 2002.", "named his company Polo because \"it was the sport of kings.", "\"gotten the balance right\" on Myanmar, the military junta-ruled Asian nation formerly known as Burma, by starting a dialogue while maintaining sanctions, \"China is a different matter.\"", "the administration's progress", "Patrick McGoohan,", "Hathi Jr", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "Bacon", "raw hides", "Verdi and his librettist Antonio Somma", "fort boyard", "1,521", "Australian", "Edward R. Murrow", "Duke Ellington", "l Duncan", "lt. colonel", "neo-Nazi"], "metric_results": {"EM": 0.5625, "QA-F1": 0.684641086437362}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, true, false, false, false, true, false, false, false, true, false, true, true, true, false, false, false, true, false, true, false, false, false, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.9565217391304348, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 0.8, 1.0, 0.47619047619047616, 1.0, 1.0, 1.0, 1.0, 0.7999999999999999, 0.5714285714285715, 1.0, 0.1111111111111111, 0.5333333333333333, 0.0, 1.0, 0.6666666666666666, 0.0, 0.23076923076923078, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2382", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-2385", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-4012", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2061", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-6091", "mrqa_hotpotqa-validation-4863", "mrqa_searchqa-validation-7423", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-7657"], "SR": 0.5625, "CSR": 0.5257711038961039, "EFR": 1.0, "Overall": 0.7092167207792208}, {"timecode": 77, "before_eval_results": {"predictions": ["Terry the Tomboy", "Gatwick Airport", "Abdul Razzak Yaqoob", "Robert Matthew Hurley", "William Shakespeare", "Syracuse University", "Australian", "\"Peshwa\" (Prime Minister)", "Tennessee", "Lowe's Companies, Inc.", "\"Traumnovelle\" (\"Dream Story\")", "Hindi", "\"Big Mamie\"", "first", "Kate Millett", "the Millennium Olympic Games/Games of the New Millennium", "Epicurus", "Centre of Excellence", "Columbine", "381.6 days", "Richard Strauss", "West Cheshire Association Football League", "1994", "Yorgos Lanthimos", "Roscoe Lee Browne", "\"Black Abbots\"", "chocolate-colored", "more than 265 million", "\"The Brothers Karamazov\"", "jumh\u016briyyat as-S\u016bd\u0101n", "Telugu and Tamil", "Australian", "Albert Park", "\"Two Pi\u00f1a Coladas\"", "Stu Henderson", "Charles and Thomas Guard", "1919", "Elise Stefanik", "Lower Manhattan, New York City", "King of the Polish-Lithuanian Commonwealth", "pop music and popular culture", "Almeda Mall", "Ronald Ryan", "robot Overlords", "Doctor of Philosophy", "Anne of Green Gables", "Madeleine L' Engle", "February 5, 2015", "Nathan Bedford Forrest", "the liberation of German-occupied northwestern Europe from Nazi control,", "capital crimes or capital offences", "Tigris and Euphrates rivers", "between 1765 and 1783", "September 2000", "gilda", "Werner Heisenberg", "giuseppeppeppe", "(U. S. Assistant Secretary of State for African Affairs Jendayi Frazer)", "change course", "in a Johannesburg church that has become a de facto transit camp,", "poses", "jab in a poke", "October", "Transportation Security Administration"], "metric_results": {"EM": 0.609375, "QA-F1": 0.67578125}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, false, false, false, true, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-808", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-2340", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-266", "mrqa_hotpotqa-validation-2550", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-2306", "mrqa_hotpotqa-validation-5420", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5297", "mrqa_hotpotqa-validation-3130", "mrqa_hotpotqa-validation-5569", "mrqa_hotpotqa-validation-4061", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-2009", "mrqa_newsqa-validation-3946", "mrqa_newsqa-validation-2651", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-2751"], "SR": 0.609375, "CSR": 0.5268429487179487, "EFR": 1.0, "Overall": 0.7094310897435898}, {"timecode": 78, "before_eval_results": {"predictions": ["an albatross", "Gianlorenzo Bernini", "Tobago", "Denzel Washington", "a prologue", "Ben-Hur: A Tale of the Christ", "national Crime Syndicate", "a prism schism", "Bucharest", "Tennessee", "Dick Wolf", "a mokorotlo", "Helena Bonham Carter", "Cincinnati", "Friday", "Miss Havisham", "Thames", "Daniel Webster", "combust", "New Jersey", "Tarsus", "gold", "grain", "Eli Whitney", "Breckenridge Ski Resort", "the \"Dongfanghong-I\"", "bishops", "Esperanto", "the Hundred Years' War", "Mending Wall", "Twelfth Night", "Veronica", "Polyphemus", "Special Boat Teams", "bananas or avocados", "Today Show", "Sally Field", "earmarks", "Turin", "a confessor", "a deviated septum", "the Golden Girls", "polo", "Wikipedia", "the Maritimes", "1773", "Richard Daley", "silicon", "a bee", "the best man", "Missouri Waltz", "The Romantics", "Danny Veltri", "denigrating incumbent Democrat Martin Van Buren", "a low voice or whisper", "byward", "Wildcats", "Knowlton School of Architecture", "Ronald Wilson Reagan", "the interview", "U.S.", "183", "Jacob Zuma", "1987"], "metric_results": {"EM": 0.625, "QA-F1": 0.6677083333333333}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14500", "mrqa_searchqa-validation-1713", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-12301", "mrqa_searchqa-validation-5575", "mrqa_searchqa-validation-14621", "mrqa_searchqa-validation-8526", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-7331", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-12526", "mrqa_searchqa-validation-7961", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-15622", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-3743", "mrqa_searchqa-validation-5831", "mrqa_naturalquestions-validation-4552", "mrqa_triviaqa-validation-3185", "mrqa_triviaqa-validation-5498", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-5573", "mrqa_newsqa-validation-93"], "SR": 0.625, "CSR": 0.5280854430379747, "EFR": 1.0, "Overall": 0.709679588607595}, {"timecode": 79, "before_eval_results": {"predictions": ["Marrix", "reddish tint", "Cahaba", "fish", "six", "Unicorn", "David Seville", "Whiskas", "John Baker", "50p", "salt", "salt tower", "Mujib", "a scarlet tanager", "Jack Lemmon", "high jump", "Kipps: The Story of A Simple Soul", "rial", "Venezuela", "Margam Castle", "President Gerald Ford", "Ray Illingworth", "golf", "1", "Hugh Hefner", "Florentius", "Kofi Annan", "Marina Piccola (the little harbour), the Belvedere of Tragara (a high panoramic promenade lined with villas)", "right", "George Eliot", "Richard II", "salt tower", "mountain", "near field communication", "Mayflower", "philistine", "South Africa", "Topeka", "Brazil", "George Osborne", "Ever Whasing Circles", "Florence", "Sicily", "Peter Gabriel", "Space Oddity", "the Smiths", "ruse", "Jeffery Deaver", "\u00c9dith Piaf", "Quintero", "salt-G\u00e9mila Mills", "Asuka", "in Canada south of the Arctic", "Teri Hatcher", "Ballarat Bitter", "Mark Dacascos", "\"the backside.\"", "110 mph,", "\"We have several hundred people working for us in Indianapolis [alone].\"", "to work together to stabilize Somalia and cooperate in security and military operations.", "Mike Rowe", "an enigma", "Edinburgh", "an \"electronic identification to which a communication may be sent,"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5052703373015872}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, true, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-5320", "mrqa_triviaqa-validation-888", "mrqa_triviaqa-validation-2346", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-1090", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-3033", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-7071", "mrqa_triviaqa-validation-1601", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-1678", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-7729", "mrqa_triviaqa-validation-5529", "mrqa_triviaqa-validation-5747", "mrqa_triviaqa-validation-2277", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-492", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-6818", "mrqa_naturalquestions-validation-1872", "mrqa_hotpotqa-validation-507", "mrqa_hotpotqa-validation-5375", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2234", "mrqa_searchqa-validation-328"], "SR": 0.453125, "CSR": 0.5271484375, "EFR": 1.0, "Overall": 0.7094921875000001}, {"timecode": 80, "UKR": 0.70703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1619", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2267", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3360", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3967", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4646", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5343", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-681", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3330", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6754", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1527", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1974", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1998", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3083", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-10176", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-10493", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13104", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-14094", "mrqa_searchqa-validation-14204", "mrqa_searchqa-validation-14348", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16534", "mrqa_searchqa-validation-16733", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-16891", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-5457", "mrqa_searchqa-validation-5831", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7045", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8807", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-944", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2248", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3750", "mrqa_squad-validation-3813", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-4469", "mrqa_squad-validation-4669", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-5042", "mrqa_squad-validation-5325", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-5869", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-682", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-7932", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8500", "mrqa_squad-validation-8529", "mrqa_squad-validation-8719", "mrqa_squad-validation-8754", "mrqa_squad-validation-8966", "mrqa_squad-validation-9306", "mrqa_squad-validation-9531", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5167", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5928", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6791", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7717", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-934"], "OKR": 0.8125, "KG": 0.46171875, "before_eval_results": {"predictions": ["eight", "habsburg monarchy", "Levi", "Lucille Ball", "hart", "liqueur", "montana", "Hungary", "Greek", "Groucho Marx", "old Ironsides", "collage", "To Kill a Mockingbird", "fish", "cleveland", "1966", "benedictine", "Billy Fury", "Wednesday's", "giacomo Meyerbeer", "whist", "George Clooney", "Baltimore", "A4", "Mussolini", "George Osborne", "Margaret Thatcher", "set of 'Time Crash'", "livestock", "Jacob", "come dancing", "Chicago", "Hague", "kyiawar", "Inigo Montoya", "nixon", "Temple of the Dog", "Aleister Crowley", "Elton John", "armada", "setts", "batsman", "seth", "of", "Saddam Hussein", "Tombstone", "liriope", "Indonesia", "lenford Christie", "Swiss", "Daedalus", "in Hittite and Mesopotamian laws and treaties", "amylopectin", "David Ben - Gurion", "DTM", "Tetrahydrogestrinone (THG)", "7pm", "Ma Khin Khin Leh,", "prisoners at the South Dakota State Penitentiary", "Shenzhen in southern China.", "chest", "jedoublen", "big", "on the inner edge of the Nebula Arm"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5433779761904761}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, false, true, false, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.47619047619047616]}}, "before_error_ids": ["mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-6040", "mrqa_triviaqa-validation-5921", "mrqa_triviaqa-validation-2957", "mrqa_triviaqa-validation-7182", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-5116", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-6871", "mrqa_triviaqa-validation-5580", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7475", "mrqa_triviaqa-validation-5378", "mrqa_triviaqa-validation-4563", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-2903", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-2928", "mrqa_triviaqa-validation-5168", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-7616", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-6791", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9726", "mrqa_hotpotqa-validation-3117", "mrqa_newsqa-validation-2488", "mrqa_searchqa-validation-4839", "mrqa_searchqa-validation-4136", "mrqa_searchqa-validation-6635", "mrqa_naturalquestions-validation-808"], "SR": 0.484375, "CSR": 0.5266203703703703, "EFR": 0.9696969696969697, "Overall": 0.695513468013468}, {"timecode": 81, "before_eval_results": {"predictions": ["Cleckheaton", "the earthquake", "pommel horse", "four", "Margaret Court", "Arthur", "people", "cannons", "Columba", "jimmy", "power outage", "snapdragons", "weather", "\u201cFor Gallantry", "Zachary Taylor", "1951", "a paddle-steamer", "venice", "Cambridge", "meerkat", "Saturn", "Liverpool", "a carb", "Clark Gable", "bexhill", "thailand", "east of the Mississippi River", "juice", "vanilla", "Patrick Henry", "Apocalypse Now", "'Lord Nelson'", "Let It Snow", "turkish king", "cruisin", "edwina Currie", "e. T. A. Hoffmann (1776-1822) wrote the original story on which the ballet is based.", "turkish controller", "Edinburgh", "Charlotte Corday", "algiers", "turkish mrs2", "golf", "yellow", "meindert Hobbema", "Trenton", "the hose", "sea otter", "cardamom", "giuseppe", "julius", "243 days", "the Sunni Muslim family", "Terry Reid", "leg injury", "1960", "David Yates", "Mohamed Mohamud Qeyre", "15-year-old", "News of the World", "vote", "a crossword", "pemmican", "Reform"], "metric_results": {"EM": 0.4375, "QA-F1": 0.4889399509803921}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0588235294117647, 0.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-5905", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-2392", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-6399", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-2942", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-1944", "mrqa_triviaqa-validation-4479", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-7747", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-7053", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-2837", "mrqa_triviaqa-validation-262", "mrqa_triviaqa-validation-5145", "mrqa_triviaqa-validation-7204", "mrqa_triviaqa-validation-5820", "mrqa_triviaqa-validation-704", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-522", "mrqa_hotpotqa-validation-3778", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-1282", "mrqa_searchqa-validation-8180", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-14428"], "SR": 0.4375, "CSR": 0.5255335365853658, "EFR": 1.0, "Overall": 0.7013567073170732}, {"timecode": 82, "before_eval_results": {"predictions": ["a38", "the chord", "thailand", "fibroblast", "Yardbirds", "brazil", "12th", "queen Elizabeth II", "Bruce Alexander", "niece", "new manhunt 2", "Carson City", "\"Big Mac is just an iconic product for us, and it is a true classic.", "jewelled Easter eggs", "prawns", "i second that emotion", "the wren", "Northwestern University", "Love Never Dies", "lunar new year", "island", "Frank Saul", "126 mph", "David Davis", "Eric Coates", "\u00c9dith Piaf", "Arthur", "anteros", "rugby", "elaine", "antelope", "the Breakfast Club", "chestnut", "b Baldrick", "John Donne", "lomond", "Salt Lake City", "vermilion", "i second that emotion", "the Battle of Ulm", "venus", "the Compact Pussycat", "tintin", "Mercury", "round-shouldered", "Celsius", "Phil Spector", "Switzerland", "Hans Lippershey", "come on Down", "Yann Martel", "foreign investors", "the Gilbert building", "Panzerkampfwagen VIII Maus ( `` Mouse '' ) was a German World War II super-heavy tank", "two", "The Sleaford and North Hykeham by-election", "Four Weddings and a Funeral", "his mother, Katherine Jackson, his three children and undisclosed charities.", "last survivor of the Titanic, 97-year-old Millvina Dean, is auctioning off her remaining mementos of the doomed ship to pay nursing home bills.", "1979", "haifa", "Phil Mickelson", "the Crimean War", "Larry Ellison,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6188787355118}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 0.18181818181818182, 0.06451612903225806, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-2577", "mrqa_triviaqa-validation-7696", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-2425", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-7334", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-4127", "mrqa_triviaqa-validation-2124", "mrqa_triviaqa-validation-7414", "mrqa_triviaqa-validation-1455", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-4859", "mrqa_triviaqa-validation-5389", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-7246", "mrqa_hotpotqa-validation-3037", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-1594", "mrqa_searchqa-validation-3536"], "SR": 0.5625, "CSR": 0.5259789156626506, "EFR": 1.0, "Overall": 0.7014457831325303}, {"timecode": 83, "before_eval_results": {"predictions": ["lymph node", "Swedish", "Toyota", "March 19", "Julie Andrews Edwards", "mad fury", "fungi and fungal diseases", "verona", "Phil Spector", "Leonore", "the Sex Pistols", "minder", "Pisces", "pickled peppers", "yMCA", "wind", "no Fear", "a paperfolder", "Diana Ross", "Lenin", "nettle leaves", "cartoons", "Astronaut", "riyal", "pennsylvania Montana", "Albert Einstein", "One Thousand and One", "John of Gaunt", "sewing machine", "Bristol Aeroplane Company", "tenor saxophonist", "Tutankhamun", "colony", "fedora", "Indus Valley", "Helen Gurley Brown", "Ted Heath", "krakatoa", "pinocchio", "pennsylvania state", "Napoleon", "edilio de Cavalieri", "a suction cup (for the windshield), to install onto the dashboard area of the car", "norway", "rue", "Daft As A Brush", "handley Page", "jimmy hargreaves", "Sarah Vaughan", "Mr Loophole", "the jackpot", "the Shrek franchise", "Watson and Crick", "the appendicular skeleton", "25 November 2015", "Jean Erdman", "Netherlands", "9 a.m.", "U.S. Holocaust Memorial Museum", "The nation's new \"first dog\" has heightened interest in its breed -- Portuguese water dog", "Harvard", "Parris Island", "watts", "pennsylvania state"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6078125}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4062", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-2688", "mrqa_triviaqa-validation-6348", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-987", "mrqa_triviaqa-validation-4471", "mrqa_triviaqa-validation-4917", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-7495", "mrqa_triviaqa-validation-6094", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-971", "mrqa_triviaqa-validation-6241", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-667", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-3468", "mrqa_hotpotqa-validation-413", "mrqa_newsqa-validation-144", "mrqa_searchqa-validation-3232", "mrqa_searchqa-validation-9567", "mrqa_searchqa-validation-13331"], "SR": 0.5625, "CSR": 0.5264136904761905, "EFR": 1.0, "Overall": 0.7015327380952382}, {"timecode": 84, "before_eval_results": {"predictions": ["David Ben - Gurion", "is said to be unattainable", "Karina Smirnoff", "the public", "the eighth episode of Arrow's second season", "Oklahoma native Major General Clarence L. Tinker", "2002", "2012", "the words spoken to Adam and Eve after their sin, reminds worshippers of their sinfulness and mortality and thus, implicitly, of their need to repent in time", "John Roberts", "Kelly Reno", "butane", "Parker's pregnancy at the time of filming", "2004", "state or other organizational body that controls the factors of production", "Ben Willis", "The Enchantress", "the true horrors of human history derive not from orc and Dark Lords, but from our", "Lou Rawls", "the tongue", "bristee", "the March of Dimes", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "The management team", "Paris", "The Parlement de Bretagne", "Matt Flinders", "during prenatal development", "25 -- 30 \u00b0 C / km ( 28 -- 34 \u00b0 F / mi )", "a drug that might be used in surgery for its amnesic properties is", "Ferm\u00edn Francisco", "three", "Coriolis effect", "94 by 50 feet", "The enthalpy of fusion of a substance, also known as ( latent ) heat of fusion", "Washington metropolitan area", "Christina Gregorio", "Ben", "ice giants", "2014", "1920s", "in different parts of the globe", "Japan -- Korea Treaty of 1905", "cylinder of glass or plastic that runs along the fiber's length", "The Intolerable Acts", "Jonno Davies", "Internal epithelia", "Bob Dylan", "Latitude exerts little influence on the Venezuelan climate, but the altitude changes it dramatically, particularly the temperature, reaching values very different according to the presence of different thermal floors", "Buddhism", "a statistical advantage", "Sikhism", "ronald reagan", "John Mortimer", "Florida", "niece", "the second", "Kitty Kelley", "Strategic Arms Reduction Treaty", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "calvert", "pasha", "oregon", "Medellin"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5997446895424836}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.33333333333333337, 1.0, 1.0, 0.7692307692307692, 0.7058823529411764, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 0.6666666666666667, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.4, 1.0, 0.8, 0.9523809523809523, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.15384615384615385, 0.5, 0.8571428571428572, 1.0, 0.0, 1.0, 1.0, 0.07692307692307693, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2890", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-251", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-5921", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-2684", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-4145", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-4442", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-8937", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-9019", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-4382", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-5555", "mrqa_newsqa-validation-4006", "mrqa_newsqa-validation-2355", "mrqa_searchqa-validation-13409", "mrqa_searchqa-validation-2322"], "SR": 0.4375, "CSR": 0.5253676470588236, "EFR": 0.9166666666666666, "Overall": 0.6846568627450981}, {"timecode": 85, "before_eval_results": {"predictions": ["July 10, 2017", "7 August 2021", "increased productivity, trade, and secular economic trends", "North Dakota ( 21.5 % )", "Tom Selleck", "Nicolas Anelka", "18 - season", "flour and water", "Mahatma Gandhi", "J. Presper Eckert", "Missouri River", "Fa Ze Rug", "Orangeville, Ontario, Canada", "May 2016", "up to 100,000", "Kiss", "one season", "a reserve unit", "October 2012", "the winter solstice", "gesture in which the head is tilted in alternating up and down arcs along the sagittal plane", "July 2, 1928", "push the food down the esophagus", "Jackie Robinson", "The Lightning Thief", "TLC - All That", "230 million kilometres ( 143,000,000 mi )", "September 8, 2017", "William the Conqueror", "Abid Ali Neemuchwala", "Sri Lanka Podujana Peramuna, led by former president Mahinda Rajapaksa", "in the city of Chicago", "The User State Migration Tool ( USMT )", "1807", "the season - five premiere episode `` Second Opinion ''", "the Ming dynasty", "Saint Peter ( the keeper of the keys to the kingdom '' )", "1773", "September 25, 1987", "20 years from the filing date", "a homodimer of 37 - kDa subunits", "Billie Jean King", "Internal epithelia", "March 2, 2016", "detritus", "Ravi River", "Michael Biehn ( uncredited )", "12 to 36 months old", "Lulu", "in and around an unnamed village", "on a bronze plaque and mounted inside the pedestal's lower level", "boxelder", "acetone", "Fields", "21 August 1986", "Taylor Swift", "2006", "\"The precipitation will briefly transition back to light snow or flurries Saturday before ending Saturday afternoon,\"", "Zimbabwean", "South Africa", "Labour", "Charles Boyer", "Last of the Mohicans", "bomb"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6349344614739836}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, true, false, true, true, false], "QA-F1": [0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.35294117647058826, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.125, 0.5, 1.0, 1.0, 1.0, 0.5, 0.9090909090909091, 1.0, 1.0, 1.0, 0.5714285714285715, 0.4, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.42857142857142855, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8862", "mrqa_naturalquestions-validation-5647", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-1359", "mrqa_naturalquestions-validation-5413", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-1368", "mrqa_naturalquestions-validation-1789", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-91", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-1008", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-3095", "mrqa_naturalquestions-validation-4633", "mrqa_triviaqa-validation-1705", "mrqa_triviaqa-validation-5074", "mrqa_hotpotqa-validation-3641", "mrqa_newsqa-validation-1019", "mrqa_searchqa-validation-5288", "mrqa_searchqa-validation-13973"], "SR": 0.515625, "CSR": 0.5252543604651163, "EFR": 0.9032258064516129, "Overall": 0.6819460333833459}, {"timecode": 86, "before_eval_results": {"predictions": ["Georgia Groome", "Glen W. Dickson", "Louis Hynes", "Pure Java driver", "Kenneth Cook", "Russia", "Babur", "July 4, 1776", "Africa", "a blighted ovum or anembryonic gestation", "left hand ring finger", "through the Torres Strait", "the Season 6 premiere, `` can't wait ''", "3D modeling", "statute or the Constitution itself", "July -- October 2012", "Massachusetts", "Sebastian Vettel", "Ace", "August 18, 1998", "Brazil", "to form a higher alkane", "1957", "the Ramones", "Phoenix neighborhood of Ahwatukee", "Jurchen Aisin Gioro clan", "China", "Jacques Cousteau", "New Mexico", "Nepal", "538", "Ismailism", "the Congress", "Road / Track ( no `` and '' )", "March 8, 2018", "Gary Player", "1997", "Holden Nowell", "pathology", "332", "if the occurrence of one does not affect the probability of occurrence of the other", "mouth ( at the bottom )", "2018", "mitosis", "The Maidstone Studios in Maidstone, Kent", "the Outfield", "Pakistan", "Claire Rhiannon Holt", "photodiode", "1858", "2010", "George III", "Barbara Good", "Donington", "Kolkata", "The Handmaid's Tale", "fourth", "Dolgorsuren Dagvadorj,", "Herman Cain,", "World Trade Center", "a hormone", "Jane Curtin", "longhorne", "Tom Ellis"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6257688492063492}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, false, false, true, false, true, true, false, false], "QA-F1": [0.5714285714285715, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8333333333333333, 1.0, 0.0, 0.4000000000000001, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-1584", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6727", "mrqa_triviaqa-validation-5546", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-3150", "mrqa_searchqa-validation-15707", "mrqa_hotpotqa-validation-4281"], "SR": 0.53125, "CSR": 0.525323275862069, "EFR": 0.9666666666666667, "Overall": 0.6946479885057472}, {"timecode": 87, "before_eval_results": {"predictions": ["Norway", "le belle femme skunk", "Romeo and Juliet", "The Golden Girls", "Niger", "Artemis", "by increasing the number of arcs", "Spanish", "driving Miss Daisy", "malta", "Robert Galbraith", "Dubai", "Gloucestershire", "Double Trouble", "Switzerland", "President Obama", "the Observer", "Sitka, Alaska", "Captain Mark Phillips", "Brian Clough", "Peter Paul Rubens", "Pembrokeshire Coast National Park", "blood", "May Skinner", "javelin throw", "Port Moresby", "the carousel", "Maxwell", "non-Orthodox synagogues", "Cambridge", "airplane", "Richard Curtis,", "Keswick", "Louis XIV", "horseshoes", "David Copperfield", "the Monkees", "Gatcombe Park", "india", "Charlotte Marie Pomeline Casiraghi", "Michael Phelps", "Vietnam", "Mumbai", "mumps", "stop motion effects", "Sebastian Flyte", "Apollo 11", "sea spray", "Hindi", "Madrid", "Paul Bunyan", "the Reverse - Flash", "the nose", "the summer of 1990 and continued until 1992", "1986", "Central African Republic", "1967", "this week, the first of 1,500 Marines will be part of the initial wave of President Obama's", "1,500", "came forward Monday \"for the other women who couldn't or wouldn't.\"", "cholesterol", "volcanoes", "eight", "Baldwin"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6541973039215687}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3998", "mrqa_triviaqa-validation-994", "mrqa_triviaqa-validation-4700", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-1987", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-1508", "mrqa_triviaqa-validation-7209", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-6967", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-6617", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-1925", "mrqa_triviaqa-validation-4680", "mrqa_triviaqa-validation-7042", "mrqa_triviaqa-validation-3602", "mrqa_triviaqa-validation-225", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-8386", "mrqa_hotpotqa-validation-3558", "mrqa_newsqa-validation-1861", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-1657"], "SR": 0.59375, "CSR": 0.5261008522727273, "EFR": 0.9615384615384616, "Overall": 0.6937778627622377}, {"timecode": 88, "before_eval_results": {"predictions": ["Cool Runnings", "five times", "an explosion and a fire", "video games", "elders", "Andrew Davis", "Drowning Pool", "Alps of northern Italy's Lombardy", "7", "the season nine episode \"Realty Bites\"", "2007", "New York Rangers", "capital crimes or capital offences", "10-metre platform event", "The Big Bang Theory", "3,000", "beer and soft drinks", "Shohola Falls", "The Kennedy Center", "Saoirse Ronan", "H. R. Haldeman", "Netherlands", "Miss Universe 2010 Ximena Navarrete", "John R. Leonetti", "Fat Albert", "Oklahoma State", "stoneware", "Prudence Jane Goward", "Ra.One", "Walcha", "Beyond the Clouds", "Manhattan Project", "Apatosaurus", "Currer Bell", "Anne Perry", "New Jersey", "Argentina", "Province of Syracuse", "David Irving", "Florida", "219", "Charice", "Harlem", "John F. MacArthur", "Eric Edward Whitacre", "George Balanchine", "Laura Dern", "Pont de Grenelle", "Guns N' Roses", "the second", "the youngest publicly documented people to be identified as transgender", "Craig T. Nelson", "1928", "October 30, 2017", "Cuba", "bird", "oregon", "a book.", "South Africa", "Russia", "Dalai Lama", "canine", "Jane", "Yemen"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6149739583333333}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.6, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.8, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.375, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-2164", "mrqa_hotpotqa-validation-1405", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1771", "mrqa_hotpotqa-validation-568", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-2211", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-3796", "mrqa_hotpotqa-validation-5231", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-1870", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-1135", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-5824", "mrqa_newsqa-validation-1389", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-7264"], "SR": 0.53125, "CSR": 0.5261587078651686, "EFR": 0.9666666666666667, "Overall": 0.6948150749063671}, {"timecode": 89, "before_eval_results": {"predictions": ["2012", "848", "Skipton Castle", "Consigliere", "Love the Way You Lie", "the Beatles", "shortstop", "Francis Egerton", "Tamworth", "Taylor Swift's single \"Back to December\"", "Eric Whitacre", "online role-playing video game", "Meryl Streep", "Karl Kraus", "water", "Orchard County", "he turned 51, he died of cancer", "1947", "2004", "1903", "over 170", "2013", "Boulder High School", "Tian Tan Buddha", "Rocky Mountain Institute", "Macau, China", "\"O\" theatre", "Stephanie Plum", "Memphis", "National Collegiate Athletic Association", "Humphrey the Bear", "Eve Hewson", "Leon Marcus Uris", "2 July 2009", "Eielson Air Force Base", "1,462", "University of New South Wales", "God of Israel", "three times", "Budget Rent a Car", "Dallas", "Terrence Jones", "Republican", "2015", "\u00c6thelstan", "black", "half a million acres", "Vikram, Jyothika and Reemma Sen", "\"The Bob Edwards Show\"", "Bill Miner", "50JJB Sports Fitness Clubs", "Tigris and Euphrates", "$2.187 billion", "the egg", "Goldie Hawn", "Home Guard", "Captain Haddock", "North Korean officials", "$40 and a Bread of bread.", "enjoys a cold shower in his home in New Zealand.", "Mike Wallace", "a volcano", "nocturnal mammal", "Capitol Hill"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7542782738095237}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.6666666666666665, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.8000000000000002, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-3058", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-1074", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-1172", "mrqa_hotpotqa-validation-4317", "mrqa_hotpotqa-validation-4715", "mrqa_hotpotqa-validation-3763", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2833", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-5603", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-6931", "mrqa_triviaqa-validation-5948", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-4758"], "SR": 0.640625, "CSR": 0.5274305555555556, "EFR": 1.0, "Overall": 0.7017361111111111}, {"timecode": 90, "UKR": 0.72265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1336", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1547", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2267", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3779", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3812", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-681", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2684", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2798", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5647", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6754", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7190", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7516", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8546", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1974", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13104", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13297", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-13830", "mrqa_searchqa-validation-14348", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-5831", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6635", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3813", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-4469", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-5042", "mrqa_squad-validation-5325", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-682", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8529", "mrqa_squad-validation-8754", "mrqa_squad-validation-9306", "mrqa_squad-validation-9531", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1636", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-2575", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3399", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4234", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4585", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5167", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5837", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7564", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7640", "mrqa_triviaqa-validation-7717", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-920"], "OKR": 0.8203125, "KG": 0.46796875, "before_eval_results": {"predictions": ["Delaware", "The Shirehorses", "Detroit", "Johnny cage", "Bob Hill", "Pennsylvania State University", "Princess Aisha bint Hussein", "1951", "Apalachee", "River Clyde", "aging issues", "Indooroopilly Shopping Centre", "Chow Tai Fook Enterprises", "Rhode Island School of Design", "Bill Walton", "Vanessa Hudgens", "1983", "Andrew Preston", "Eddie Gottlieb Trophy", "Rhodesia", "Ed O'Neill", "My Backyard", "Lommel", "Bharat Ratna", "Aberdeenshire", "three-part", "1969", "Stalybridge Celtic Football Club", "tenant management", "Vincent Anthony Guaraldi", "Pulitzer Prize for drama", "ten", "2002", "Neon City", "Province of New York", "1932 and 1934", "Hopi", "American", "pop music and popular culture", "Australian", "SKUM", "Rain Man", "Wake Atoll", "Carlos Santana", "1942", "2027 Fairmount Avenue", "historic buildings, arts, and published works", "a great man", "The Indianapolis Times", "143,007", "King James I", "committed suicide", "1970", "nucleus", "lung", "discus", "spain", "2,500", "police dogs", "\"Chinese aggression, from aboard the USNS Impeccable,\"", "Sierra Leone", "Dracula", "Jumpmasters", "11th year in a row."], "metric_results": {"EM": 0.640625, "QA-F1": 0.7446428571428572}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, false, true, true, false, true, false, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-699", "mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-4514", "mrqa_hotpotqa-validation-3311", "mrqa_hotpotqa-validation-1048", "mrqa_hotpotqa-validation-5498", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-2599", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-1925", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-5537", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-5403", "mrqa_triviaqa-validation-5991", "mrqa_newsqa-validation-414", "mrqa_newsqa-validation-3310", "mrqa_searchqa-validation-6970", "mrqa_searchqa-validation-10274"], "SR": 0.640625, "CSR": 0.5286744505494505, "EFR": 1.0, "Overall": 0.7079223901098901}, {"timecode": 91, "before_eval_results": {"predictions": ["Atlanta", "The Producers", "E.M. Forster", "James Joyce", "Jamestown", "Helen Hayes", "a cupronickel coin", "Philip II", "a Great Dane", "Vermont", "Moses", "the Moors", "Margaret Mitchell", "Henrik Ibsen", "a cruller", "a ravel", "Hungary", "Sugar Smacks", "Groundhog Day", "New Balance", "a Siberian Husky", "Animal Crackers", "nitrogen", "a platypus", "a Supreme Court justice", "a bicycle", "The Magic Mountain", "Fiji", "Manassas", "Jacqueline Kennedy Onassis", "Forbes", "Death Row Records", "Jimmy Hoffa", "a brain", "a rabbit", "a Car Dealer Fraud", "Wiener Journal", "a rigid system of legal segregation", "Browning", "a compound interest", "molly ringwald", "a screen", "the Marine Corps", "Ho Chi Minh", "a supernatural kiss", "Arkansas", "The Stone of Destiny", "remoulade", "Florida", "Night Fever", "Latin", "Jane Addams", "Thomas Chisholm", "Norman Pritchard", "tin tin scorsese", "spider", "Istanbul", "The Battle of the Rosebud", "40 Acres", "1851", "26", "$22 million", "Sixteen", "Ricky Nelson"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6781520562770562}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14853", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-8040", "mrqa_searchqa-validation-12465", "mrqa_searchqa-validation-4712", "mrqa_searchqa-validation-8297", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-8663", "mrqa_searchqa-validation-524", "mrqa_searchqa-validation-13732", "mrqa_searchqa-validation-8137", "mrqa_searchqa-validation-10072", "mrqa_searchqa-validation-6520", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-9931", "mrqa_searchqa-validation-15184", "mrqa_searchqa-validation-12969", "mrqa_searchqa-validation-13793", "mrqa_searchqa-validation-11111", "mrqa_searchqa-validation-3990", "mrqa_searchqa-validation-13167", "mrqa_naturalquestions-validation-2176", "mrqa_triviaqa-validation-6215", "mrqa_hotpotqa-validation-2520"], "SR": 0.578125, "CSR": 0.5292119565217391, "EFR": 0.9629629629629629, "Overall": 0.7006224838969404}, {"timecode": 92, "before_eval_results": {"predictions": ["a bullion", "A Chorus Line", "Pamplona", "French toast", "Pop-Tarts", "Aesop", "Shawnee", "postscript", "Babe Ruth", "brothels", "the Orinoco", "John Bunyan", "Sicilian pizza", "Punjabi", "insulin", "Jimmy Hoffa", "canyon", "Newton's", "Cincinnati", "Alexander Hamilton", "Timberland", "Bronchoconstriction", "the Perseid", "Phil Cavilleri", "the wall", "a baggage porter", "Fess Parker", "Michelangelo", "I Wanna Be Your Man", "grease", "Kathleen Kennedy Townsend", "Henry Cavendish", "New York", "euphoria", "Don Quixote", "Charlie and the Chocolate Factory", "baboon", "Last Summer", "yerushalayim", "genes", "Around the", "Bionic Woman", "Ben", "Wynona Judd", "pardis", "knight", "HIV", "the Smothers Brothers", "Henry Hudson", "the sidecar", "Ford", "The management team", "the United States", "2017", "table salt", "Toonami's Rising Sun", "Caitlin", "AMC Entertainment Holdings, Inc.", "the sketch comedy series \"The Carol Burnett Show\"", "Montreal, Quebec", "Matthew Fisher", "$150 billion", "Tennessee.", "Johannes Gutenberg"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6654513888888889}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, false, false, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.2222222222222222, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11157", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-6725", "mrqa_searchqa-validation-16940", "mrqa_searchqa-validation-1094", "mrqa_searchqa-validation-16819", "mrqa_searchqa-validation-13243", "mrqa_searchqa-validation-7168", "mrqa_searchqa-validation-7802", "mrqa_searchqa-validation-14776", "mrqa_searchqa-validation-9414", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-8996", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-3896", "mrqa_searchqa-validation-9483", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-8750", "mrqa_naturalquestions-validation-305", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-1759", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-3758", "mrqa_hotpotqa-validation-582"], "SR": 0.59375, "CSR": 0.5299059139784946, "EFR": 1.0, "Overall": 0.7081686827956989}, {"timecode": 93, "before_eval_results": {"predictions": ["the three-Cornered Hat", "7-Eleven", "Oakland", "Australia", "the Grail Knights", "the Hippopotamus", "Grant & Sherman", "carbon", "Britain", "the metacarpals", "a carriage", "the catacombs", "Bahrain", "the Cohans", "Jean Harlow", "the yardarm", "a fish", "29", "the mistletoe", "the council", "Islam", "Tijuana", "Cleopatra", "the Irish", "the Capitol", "Bauhaus", "Homeland Security", "the Jumper", "a batrachus", "paradise", "Baltic Sea", "thunderous Saddles", "Dom u Dobreho", "While You Were Out", "6", "red", "Love Story", "the Ramen", "Truman", "St. Louis", "the Confederate Flag", "the chili rellenos", "Elizabeth Edward", "Stephen", "Sgt. Pepper", "red", "the Amistad", "Prince", "the galaxy", "a noun", "Tycho Brahe", "displacement", "Department of Health and Human Services, Office of Inspector General, as of 2000 there were more than 6,000 entities issuing birth certificates", "February 6, 2005", "congregation", "Cambridge", "Charles Darwin", "Napoleonic Wars", "The Sun", "24", "ice jam", "jobs", "removal of his diamond-studded braces.", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6557849702380952}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, true, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 0.6666666666666666, 0.25, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.125, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1189", "mrqa_searchqa-validation-7905", "mrqa_searchqa-validation-11754", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-2718", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-13462", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-3622", "mrqa_searchqa-validation-13212", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-9980", "mrqa_searchqa-validation-10682", "mrqa_searchqa-validation-12543", "mrqa_searchqa-validation-7097", "mrqa_searchqa-validation-15694", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-4568", "mrqa_searchqa-validation-6841", "mrqa_searchqa-validation-5137", "mrqa_naturalquestions-validation-6993", "mrqa_triviaqa-validation-4180", "mrqa_hotpotqa-validation-5555", "mrqa_hotpotqa-validation-2759", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-4042"], "SR": 0.578125, "CSR": 0.5304188829787234, "EFR": 1.0, "Overall": 0.7082712765957446}, {"timecode": 94, "before_eval_results": {"predictions": ["Southend Pier", "9 February 2018", "the 7th century at Rendlesham in East Anglia", "a party of six members of France's Legislative Assembly", "1971 motion picture Willy Wonka & the Chocolate Factory", "Jesse McCartney", "1902", "Splodgenessabounds", "February 27, 2007", "Toot - Toot -- A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "Chuck Noland", "his teenage role as the title character on the Disney Channel television series The Famous Jett Jackson ( 1998 -- 2001 )", "1939", "60 by West All - Stars", "around 1872", "the plane crash", "Majo to Hyakkihei 2", "the coronary arteries", "Pittsburgh", "the beginning of the American colonies", "James Ray", "Los Angeles", "the Outback", "regulate the employment and working conditions of civil servants, oversee hiring and promotions, and promote the values of the public service", "Detective Abigail Baker", "January 2018", "a contemporary drama in a rural setting", "Cody Fern", "homicidal thoughts of a troubled youth", "August 5, 1937", "San Francisco", "the Stock Market Crash of 1929", "The Magician", "The Royalettes", ". java", "`` Killer Within ''", "Romeo", "1963, when Eddie Mays was electrocuted at Sing Sing Prison", "Roman Reigns", "1901", "1972", "Michael Rooker", "1977", "Richard Stallman", "Keeley Clare Julia Hawes", "Abid Ali Neemuchwala", "Jay Baruchel", "March 26, 1973", "July 25, 2017", "1922", "Eddie Van Halen", "sporades", "Some Like It Hot", "bird", "Bhushan Patel", "Dar es Salaam", "fifth-largest", "\"As an African-American, I have held fund-raisers for Obama but was unhappy with criticisms of the former first lady by Obama's campaign,\"", "Iran's parliament speaker", "Sri Lanka", "love", "Mathew Brady", "the Lord of the Rings", "Greenham Common"], "metric_results": {"EM": 0.5625, "QA-F1": 0.660266855257664}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.6, 0.3333333333333333, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.19999999999999998, 0.0, 0.0, 0.25, 0.4, 1.0, 0.0, 1.0, 0.972972972972973, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-9098", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-5936", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-9836", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8837", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-1911", "mrqa_triviaqa-validation-2910", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-3557", "mrqa_newsqa-validation-2831", "mrqa_searchqa-validation-9281"], "SR": 0.5625, "CSR": 0.5307565789473685, "EFR": 0.9285714285714286, "Overall": 0.6940531015037594}, {"timecode": 95, "before_eval_results": {"predictions": ["Jet Republic", "Frank Ricci,", "fining the computer chip giant a record  $1.45 billion", "drug cartels", "Carlotta Walls LaNier", "five", "Ben Roethlisberger", "Alan Graham", "a Muslim background,", "Asashoryu", "slumdog Millionaire", "U Win Tin,", "$17,000", "In a statement on its Web site, Desire Petroleum said \"oil may be present in thin intervals but that reservoir quality is poor.\"", "the pirates", "10 municipal police officers", "Bangladesh", "Rima Fakih", "Mikkel Kessler", "Friday,", "would slow economic growth with higher taxes.", "Longo-Ciprelli", "suppress the memories and to live as normal a life as possible;", "Hayden", "Malcolm X", "Dangjin", "\"mentally deranged person", "central business district", "The patient, who prefers to be anonymous,", "theft in Switzerland", "only one", "The noose incident occurred two weeks after Black History Month", "opposition parties", "via YouTube", "Cambodian territory", "2.5 million", "against using injectable vitamin supplements", "\"I sort of had a fascination with John Dillinger when I was about 10, 11 years old, for some reason,\"", "If one part hurts, the entire body feels the pain.", "Abhisit Vejjajiva", "the iPods", "Yemen", "228", "More than 15,000", "Anatole Nsengiyumva,", "Operation Pipeline Express.", "roughly $5.5 billion to build.", "$106,482,500", "Long troop deployments in Iraq, above, and Afghanistan", "prostate cancer,", "bartering", "William Wyler", "1608", "Paul", "kachhi", "temperature", "fractal geometry", "Paper", "Lee Seung-gi", "The Lord of the Rings", "Harry Potter and the Chamber of Secrets", "diphthong", "lily", "nuclear weapons"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6395614495798319}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7428571428571429, 0.11764705882352941, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.07692307692307693, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.5714285714285715, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3913", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-742", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-2797", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-408", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-1044", "mrqa_newsqa-validation-2891", "mrqa_naturalquestions-validation-7728", "mrqa_triviaqa-validation-1304", "mrqa_triviaqa-validation-2184", "mrqa_hotpotqa-validation-533", "mrqa_hotpotqa-validation-3247", "mrqa_searchqa-validation-6473"], "SR": 0.53125, "CSR": 0.53076171875, "EFR": 0.9666666666666667, "Overall": 0.7016731770833333}, {"timecode": 96, "before_eval_results": {"predictions": ["\"one of the most magnificent expressions of freedom and free enterprise in history\"", "off Somalia's coast.", "2009", "club managers,", "5,600", "a construction site in the heart of Los Angeles.", "central Cairo,", "Free theater tickets", "delivers a big speech", "Basilan", "Izzat Ibrahim", "Fareed Zakaria", "Gov. Mark Sanford", "56", "$17,000", "ClimateCare,", "Japanese officials", "fake his own death", "The Olympic torch and flame are the symbols of the values and ideals which lie at the heart of the Olympic Games,\"", "Police", "one", "The FARC", "knowingly exposed the soldiers to a cancer-causing toxic chemical.", "in a campus library,", "July", "the two remaining crew members from the helicopter,", "Darrel Mohler", "\"GoldenEye\"", "$1.5 million.", "\"People have lost their homes, their jobs, their hope,\"", "$81,8702", "Larry King", "HPV (human papillomavirus)", "\"The strike means all buses, subways and trolleys in Philadelphia and on the Frontier line in Bucks and Montgomery counties stopped running at 3 a.m.", "Body Tap,", "The Southern Baptist Convention", "Stephen Tyrone Johns", "Math teacher Mawise Gumba", "\"The first line of law and order", "Facebook", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "South African police", "Susan Atkins", "off the coast of Dubai", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "\"falling space debris,\"", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "Silicon Valley.", "137", "\"Slumdog Millionaire\"", "Lillo Brancato Jr.", "Maria works in a bridal shop with Anita, the girlfriend of her brother, Bernardo", "Sylvester Stallone", "a star", "butterflies", "vanilla", "France", "1241 until his death in 1250", "October", "Modbury", "Jacob Marley", "Weissmuller", "Pin the Tail on the Donkey", "Mount Hood"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7435823714497241}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.1739130434782609, 1.0, 1.0, 0.0, 0.9333333333333333, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.07142857142857142, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 0.0, 1.0, 1.0, 0.7368421052631579, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-99", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-752", "mrqa_newsqa-validation-1163", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-4039", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-3570", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5305", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-5864", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-6959"], "SR": 0.640625, "CSR": 0.5318943298969072, "EFR": 0.9565217391304348, "Overall": 0.6998707138054684}, {"timecode": 97, "before_eval_results": {"predictions": ["Michael V. Gazzo", "winnie mae", "Manchester United", "Pocahontas", "Geoffrey Chaucer", "Stubbs", "Jordan", "John Donne", "West Virginia", "dogs", "watchmaking", "Chad", "ballet", "the elephants House", "Cornwall", "the Blackstone River", "green", "Fred Trueman", "Chapters I\u2013II", "Athens", "Yemen", "Loch Morar", "leprosy", "Manhunt 2", "your phone", "piano", "a gun having a regular size of bore", "a rough Collie, also known as a long-haired Collie.", "Karachi", "the best", "Uriah", "George Fox", "bat", "secretary or scribe", "France", "Melissa Duck", "haddock", "a number", "Ross MacManus", "music (to be performed) in a fiery manner", "dry rot", "a cuckoo", "Northumberland", "6", "The Graduate", "3", "early 1900s", "a goose", "Northern Ireland", "Valentino Rossi", "Pat Houston", "McFerrin, Robin Williams, and Bill Irwin", "1273.6 cm", "John von Neumann", "Florida Panthers", "giuseppe Verdi", "67,575", "July", "sailing", "Mary Procidano", "Eva Maria Kiesler", "Quinn", "the Komodo Dragon", "goalkeeper"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6329365079365079}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.2857142857142857, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.1111111111111111, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-2917", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-5438", "mrqa_triviaqa-validation-2342", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-3406", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-7539", "mrqa_triviaqa-validation-7413", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-1142", "mrqa_triviaqa-validation-2595", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-3485", "mrqa_triviaqa-validation-1585", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-7458", "mrqa_hotpotqa-validation-3953", "mrqa_searchqa-validation-15102"], "SR": 0.578125, "CSR": 0.5323660714285714, "EFR": 0.9629629629629629, "Overall": 0.7012533068783069}, {"timecode": 98, "before_eval_results": {"predictions": ["Kite Runner", "Yves Saint Laurent", "Letitia Baldrige", "a shepherd", "Chopin", "curly", "glass", "Bolshoi Ballet", "Mending Wall", "Nathan Lane", "Cheaper by the Dozen", "king Ferdinand", "Marlon Brando", "Sagamore Hill", "Peru", "Copenhagen", "Arctic Ocean", "(Sir Henry) Hudson", "Blofeld", "elle", "Richard Cory", "Franois Truffaut", "barbie", "one", "Chlorine", "(Bill) Clinton", "Hanoi", "Byzantine Empire", "Mali", "flava Flav", "delmonico's", "(Macedonian)", "Hawaii", "Jimmy Hoffa", "elma", "Cincinnati", "bulldog", "gin", "John Paul Jones", "walk the plank", "Three Amigos", "Halloween", "George II", "Stonehenge", "Grease", "Nevada", "a tick", "pH", "William Halsey", "Bangkok", "lungs", "DeWayne Warren", "Neela Montgomery", "New England Patriots", "Philippines", "micky dolenz", "brashy", "Oreiad", "400 MW", "high court of admiralty", "held", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "the conversion", "\"godfather\""], "metric_results": {"EM": 0.578125, "QA-F1": 0.6479166666666667}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15220", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-2008", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-15451", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-13423", "mrqa_searchqa-validation-14990", "mrqa_searchqa-validation-9525", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-11126", "mrqa_searchqa-validation-7874", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-7997", "mrqa_searchqa-validation-9594", "mrqa_searchqa-validation-14239", "mrqa_searchqa-validation-7141", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-5047", "mrqa_triviaqa-validation-6849", "mrqa_hotpotqa-validation-4103", "mrqa_hotpotqa-validation-3975", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1424", "mrqa_hotpotqa-validation-4241"], "SR": 0.578125, "CSR": 0.5328282828282829, "EFR": 1.0, "Overall": 0.7087531565656565}, {"timecode": 99, "UKR": 0.69921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1336", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1547", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2267", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3779", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3812", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-681", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2005", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2798", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-425", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5647", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6754", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7190", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7516", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-9157", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9836", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1974", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2797", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3152", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-1021", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11412", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13104", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13167", "mrqa_searchqa-validation-13297", "mrqa_searchqa-validation-13374", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-13830", "mrqa_searchqa-validation-14348", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14826", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15694", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-1654", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-2620", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-5423", "mrqa_searchqa-validation-5635", "mrqa_searchqa-validation-5831", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6635", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7267", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8297", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9807", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-4469", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-5042", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-682", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8529", "mrqa_squad-validation-8754", "mrqa_squad-validation-9306", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1636", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-218", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-2391", "mrqa_triviaqa-validation-2575", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3173", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3399", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4234", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4585", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5167", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5837", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-7564", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7640", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-920"], "OKR": 0.833984375, "KG": 0.50546875, "before_eval_results": {"predictions": ["Gunpei Yokoi", "2022", "Thomas Mundy Peterson", "$2 million", "client that was first developed and popularized by the Israeli company Mirabilis", "first Sunday after Easter", "acid rain", "The kid then blabs it out when Miley is around, leading Oliver to show her the picture of Jake with another girl", "1943", "Coriolis effect", "ranking used in combat sports", "Owen Wilson and Jennifer Aniston", "1966", "the differential", "arm", "England and Wales", "Nicole Gale Anderson", "Massachusetts", "during the sixteenth series Dr Dalton is killed in an explosion", "Bob Peterson", "In the episode `` Kobol's Last\\'", "March 1602", "Ethiopia", "third", "the Hongwu Emperor", "Bart Millard", "Rationing Stamps and Cards", "The management team", "Nick Kroll", "Longliners", "Waylon Jennings", "FUE harvesting method", "a minority report", "Massachusetts", "U + 2234", "king and parliament", "2017 / 18", "2001", "cadmium", "Jason Momoa", "Phillip Paley", "Tin Woodman", "July 21, 1861", "Bonnie Aarons", "Andrew Michael Harrison", "nucleus", "drizzle, rain, sleet, snow, graupel and hail", "Internal epithelia", "American League ( AL ) champion Cleveland Indians", "a multilayer", "Graub\u00fcnden", "Coco Chanel", "George W. Bush", "Cersei Lannister", "Tatton Park", "Selina D'Arcy", "1964", "Ashley \"A.J.\" Jewell,", "42 years old", "step up.\"", "Argentina", "(Charlie) McCarthy", "Shakespeare", "HBO World Championship Boxing"], "metric_results": {"EM": 0.578125, "QA-F1": 0.655550178987679}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.14285714285714288, 0.0, 1.0, 0.23076923076923075, 1.0, 1.0, 0.3846153846153846, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.25, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.25, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-1632", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-5283", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-6564", "mrqa_triviaqa-validation-1348", "mrqa_triviaqa-validation-1340", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-792"], "SR": 0.578125, "CSR": 0.5332812499999999, "EFR": 0.9629629629629629, "Overall": 0.7069832175925925}]}