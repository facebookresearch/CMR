{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semanticdebugger\n",
    "from semanticdebugger.debug_algs.cl_utils import _keep_first_answer\n",
    "from semanticdebugger.debug_algs.distant_supervision.ds_utils import create_training_stream\n",
    "from semanticdebugger.debug_algs.index_based.index_manager import RandomMemoryManger\n",
    "from semanticdebugger.debug_algs.index_based.index_utils import get_bart_dual_representation\n",
    "from semanticdebugger.models import run_bart\n",
    "from semanticdebugger.models.utils import set_seeds, trim_batch\n",
    "import torch\n",
    "\n",
    "from scipy.stats.stats import describe\n",
    "from semanticdebugger.debug_algs.cl_simple_alg import ContinualFinetuning\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from semanticdebugger.debug_algs import run_lifelong_finetune\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "# torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "torch.cuda.set_device(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/13/2021 22:07:04 - INFO - semanticdebugger.debug_algs.run_lifelong_finetune - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='simple_cl', current_thread_id=None, data_stream_json_path='bug_data/mrqa_naturalquestions_dev.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, index_rank_method='most_similar', indexing_args_path='exp_results/supervision_data/1012_dm_simple.train_args.json', indexing_method='bart_index', inference_query_size=1, init_memory_cache_path='bug_data/memory_key_cache.pkl', learning_rate=1e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=-1, memory_key_encoder='facebook/bart-base', memory_path='', memory_store_rate=1.0, mir_abalation_args='none', num_adapt_epochs=1, num_beams=4, num_threads_eval=0, num_train_epochs=3.0, overtime_ckpt_dir=None, pass_pool_jsonl_path='bug_data/mrqa_naturalquestions_dev.sampled_pass.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='nq_dev', replay_candidate_size=8, replay_frequency=1, replay_size=8, replay_stream_json_path='bug_data/mrqa_naturalquestions_dev.replay_stream.test.json', result_file='bug_data/results.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=0, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_mir=False, use_replay_mix=False, use_sampled_upstream=False, weight_decay=0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/13/2021 22:07:04 - INFO - semanticdebugger.debug_algs.run_lifelong_finetune - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='simple_cl', current_thread_id=None, data_stream_json_path='bug_data/mrqa_naturalquestions_dev.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, index_rank_method='most_similar', indexing_args_path='exp_results/supervision_data/1012_dm_simple.train_args.json', indexing_method='bart_index', inference_query_size=1, init_memory_cache_path='bug_data/memory_key_cache.pkl', learning_rate=1e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=-1, memory_key_encoder='facebook/bart-base', memory_path='', memory_store_rate=1.0, mir_abalation_args='none', num_adapt_epochs=1, num_beams=4, num_threads_eval=0, num_train_epochs=3.0, overtime_ckpt_dir=None, pass_pool_jsonl_path='bug_data/mrqa_naturalquestions_dev.sampled_pass.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='nq_dev', replay_candidate_size=8, replay_frequency=1, replay_size=8, replay_stream_json_path='bug_data/mrqa_naturalquestions_dev.replay_stream.test.json', result_file='bug_data/results.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=0, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_mir=False, use_replay_mix=False, use_sampled_upstream=False, weight_decay=0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/13/2021 22:07:04 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "10/13/2021 22:07:04 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/13/2021 22:07:04 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "10/13/2021 22:07:04 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "parser = run_lifelong_finetune.get_cli_parser()\n",
    "args = parser.parse_args(\"\") \n",
    "\n",
    "cl, data_args, base_model_args, debugger_args, logger = run_lifelong_finetune.setup_args(\n",
    "        args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/home/yuchenlin/SemanticDebugger/semanticdebugger/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/13/2021 23:12:19 - INFO - semanticdebugger.debug_algs.run_lifelong_finetune - Loading checkpoint from /private/home/yuchenlin/SemanticDebugger/out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(base_model_path='/private/home/yuchenlin/SemanticDebugger/out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', model_type='facebook/bart-base')\n",
      "10/13/2021 23:12:19 - INFO - semanticdebugger.debug_algs.run_lifelong_finetune - Loading checkpoint from /private/home/yuchenlin/SemanticDebugger/out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/13/2021 23:12:20 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb\n",
      "10/13/2021 23:12:20 - INFO - transformers.configuration_utils - Model config BartConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\",\n",
      "    \"BartForConditionalGeneration\",\n",
      "    \"BartForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "10/13/2021 23:12:20 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/13/2021 23:12:20 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb\n",
      "10/13/2021 23:12:20 - INFO - transformers.configuration_utils - Model config BartConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\",\n",
      "    \"BartForConditionalGeneration\",\n",
      "    \"BartForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "10/13/2021 23:12:20 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/13/2021 23:12:24 - INFO - semanticdebugger.debug_algs.run_lifelong_finetune - Loading checkpoint from /private/home/yuchenlin/SemanticDebugger/out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!\n",
      "10/13/2021 23:12:24 - INFO - semanticdebugger.debug_algs.run_lifelong_finetune - Moving to the GPUs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/13/2021 23:12:24 - INFO - semanticdebugger.debug_algs.run_lifelong_finetune - Loading checkpoint from /private/home/yuchenlin/SemanticDebugger/out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!\n",
      "10/13/2021 23:12:24 - INFO - semanticdebugger.debug_algs.run_lifelong_finetune - Moving to the GPUs.\n"
     ]
    }
   ],
   "source": [
    "base_model_args.base_model_path = os.path.join(\"/private/home/yuchenlin/SemanticDebugger/\", base_model_args.base_model_path)\n",
    "print(base_model_args)\n",
    "cl.load_base_model(base_model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 27.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_reps.size()=torch.Size([29, 6, 768])\n",
      "input_masks.size()=torch.Size([6, 29])\n",
      "torch.Size([29, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "from scipy import spatial\n",
    "from transformers.modeling_bart import _prepare_bart_decoder_inputs\n",
    "\n",
    "def masked_mean(reps, masks):\n",
    "    masks = masks.view(reps.size()[0], reps.size()[1], 1)\n",
    "    # print(f\"masks.size()={masks.size()}\")\n",
    "    masked_reps = reps * masks\n",
    "    # print(f\"masked_reps.size()={masked_reps.size()}\")\n",
    "    # print(masked_reps[:, -3, :])\n",
    "    # print(masked_reps[:, -2, :])\n",
    "    # print(masked_reps[:, -1, :])\n",
    "\n",
    "    # print(f\"masks.sum(dim=1)={masks.sum(dim=1)}\")\n",
    "    masked_reps_sum = masked_reps.sum(dim=1)\n",
    "    # print(f\"masked_reps_sum.size()={masked_reps_sum.size()}\")\n",
    "    length_reps = masks.sum(dim=1).view(masked_reps_sum.size()[0], 1)\n",
    "    # print(f\"length_reps.size()={length_reps.size()}\")\n",
    "    \n",
    "    mean_reps = masked_reps_sum / length_reps\n",
    "    # print(mean_reps.size())\n",
    "    return mean_reps\n",
    "\n",
    "def get_bart_dual_representation_v1(cl_trainer, bart_model, tokenizer, data_args, examples, return_all_hidden=False):\n",
    "    examples_with_single_ans = _keep_first_answer(examples)\n",
    "    data_manager, _ = cl_trainer.get_dataloader(data_args,\n",
    "                                                    examples_with_single_ans,\n",
    "                                                    mode=\"train\",\n",
    "                                                    is_training=False)\n",
    "    all_vectors = []\n",
    "    bart_model = bart_model if cl_trainer.n_gpu == 1 else bart_model.module\n",
    "    bart_model.eval()\n",
    "    all_hiddens = {\"input_reps\":[], \"input_masks\": [], \"output_reps\": [] , \"output_masks\": []}\n",
    "    for batch in tqdm(data_manager.dataloader):\n",
    "        # self.logger.info(f\"len(batch)={len(batch)}\")\n",
    "        if cl_trainer.use_cuda:\n",
    "            # print(type(batch[0]), batch[0])\n",
    "            batch = [b.to(torch.device(\"cuda\")) for b in batch]\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "        batch[0], batch[1] = trim_batch(\n",
    "            batch[0], pad_token_id, batch[1])\n",
    "        batch[2], batch[3] = trim_batch(\n",
    "            batch[2], pad_token_id, batch[3])\n",
    "\n",
    "        # Encode the input text with BART-encoder\n",
    "        input_ids = batch[0]\n",
    "        input_attention_mask = batch[1]\n",
    "        encoder_outputs = bart_model.model.encoder(\n",
    "            input_ids, input_attention_mask)\n",
    "        x = encoder_outputs[0] \n",
    "        # x = x[:, 0, :]\n",
    "        x = masked_mean(x, input_attention_mask)\n",
    "        input_vectors = x.detach().cpu().numpy()\n",
    "\n",
    "        # self.logger.info(f\"input_vectors.shape = {input_vectors.shape}\")\n",
    "\n",
    "        # Encode the output text with BART-decoder\n",
    "\n",
    "        output_ids = batch[2]\n",
    "        output_attention_mask = batch[3]\n",
    "\n",
    "        decoder_input_ids, decoder_padding_mask, causal_mask = _prepare_bart_decoder_inputs(\n",
    "            bart_model.model.config,\n",
    "            input_ids,\n",
    "            decoder_input_ids=output_ids,\n",
    "            decoder_padding_mask=output_attention_mask,\n",
    "            causal_mask_dtype=bart_model.model.shared.weight.dtype,\n",
    "        )\n",
    "        decoder_outputs = bart_model.model.decoder(\n",
    "            decoder_input_ids,\n",
    "            encoder_outputs[0],\n",
    "            input_attention_mask,\n",
    "            decoder_padding_mask,\n",
    "            decoder_causal_mask=causal_mask,\n",
    "            decoder_cached_states=None,\n",
    "            use_cache=False\n",
    "        )\n",
    "        y = decoder_outputs[0]\n",
    "        # y = y[:, 0, :]\n",
    "        y = masked_mean(y, output_attention_mask)\n",
    "        output_vectors = y.detach().cpu().numpy()\n",
    "\n",
    "        # self.logger.info(f\"output_vectors.shape = {output_vectors.shape}\")\n",
    "\n",
    "        # concatenate the vectors\n",
    "        vectors = np.concatenate([input_vectors, output_vectors], axis=1)\n",
    "        if return_all_hidden:\n",
    "            all_hiddens[\"input_reps\"] += list(encoder_outputs[0].detach().cpu().numpy())\n",
    "            all_hiddens[\"output_reps\"] += list(decoder_outputs[0].detach().cpu().numpy())\n",
    "\n",
    "            all_hiddens[\"input_masks\"] += list(input_attention_mask.detach().cpu().numpy())\n",
    "            all_hiddens[\"output_masks\"] += list(output_attention_mask.detach().cpu().numpy())\n",
    "            \n",
    "            \n",
    "        # self.logger.info(f\"vectors.shape = {vectors.shape}\")\n",
    "        all_vectors += list(vectors)\n",
    "        \n",
    "        del batch\n",
    "        del encoder_outputs\n",
    "        del decoder_outputs\n",
    "    if return_all_hidden:    \n",
    "        return all_hiddens\n",
    "    else:\n",
    "        return all_vectors\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "examples = []\n",
    "examples += [(\"I like my bird.\", \"asd\", \"test_0\")]\n",
    "examples += [(\"I love my bird.\", \"asd\", \"test_1\")]\n",
    "examples += [(\"I love my dog.\", \"asd\", \"test_2\")]\n",
    "examples += [(\"I have a dream!\", \"asd\", \"test_3\")]\n",
    "examples += [(\"asdf as sdafasd\", \"asd\", \"test_4\")]\n",
    "examples += [(\"asdf as sdafasd dddd aaaaaa zcxv asdf  asdfgasdfasd \", \"asd\", \"test_5\")]\n",
    "\n",
    "vectors = get_bart_dual_representation_v1(cl, cl.base_model, cl.tokenizer, data_args, examples)\n",
    "\n",
    "all_hiddens = get_bart_dual_representation_v1(cl, cl.base_model, cl.tokenizer, data_args, examples, return_all_hidden=True)\n",
    "  \n",
    "input_reps = all_hiddens[\"input_reps\"]\n",
    "output_reps = all_hiddens[\"output_reps\"]\n",
    "input_masks = all_hiddens[\"input_masks\"]\n",
    "output_masks = all_hiddens[\"output_masks\"]\n",
    "\n",
    "encoder_layer = torch.nn.TransformerEncoderLayer(d_model=768, nhead=8)\n",
    "transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "input_reps = torch.Tensor(input_reps)\n",
    "input_masks = torch.BoolTensor(input_masks)\n",
    "input_reps = input_reps.permute(1,0,2)\n",
    "# input_masks = input_masks.permute(1,0)\n",
    "# input_masks = input_masks.view(input_masks.size()[0], input_masks.size()[1], 1)\n",
    "print(f\"input_reps.size()={input_reps.size()}\")\n",
    "print(f\"input_masks.size()={input_masks.size()}\")\n",
    "\n",
    "# src = torch.rand(10, 32, 512)\n",
    "out = transformer_encoder(input_reps, src_key_padding_mask=input_masks)\n",
    "final_out = out[:,0,:]\n",
    "print(out.size())\n",
    "\n",
    "# # using the vectors \n",
    "\n",
    "# vectors = np.array(vectors)\n",
    "\n",
    "# def calculate_cosine_distance(a, b):\n",
    "#     cosine_distance = float(spatial.distance.cosine(a, b))\n",
    "#     return cosine_distance\n",
    "\n",
    "\n",
    "# def calculate_cosine_similarity(a, b, span=\"full\"):\n",
    "#     length = len(a) \n",
    "#     if span == \"left\":\n",
    "#         a = a[:length//2]\n",
    "#         b = b[:length//2]\n",
    "#     elif span == \"right\":\n",
    "#         a = a[length//2:]\n",
    "#         b = b[length//2:]\n",
    "#     cosine_similarity = 1 - calculate_cosine_distance(a, b)\n",
    "#     return cosine_similarity\n",
    "# print(vectors)\n",
    "# print(calculate_cosine_similarity(vectors[0], vectors[1]))\n",
    "# print(calculate_cosine_similarity(vectors[0], vectors[2]))\n",
    "# print(calculate_cosine_similarity(vectors[0], vectors[3]))\n",
    "# print(calculate_cosine_similarity(vectors[0], vectors[4])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d0fe79caf7a23c945e060d06040de0c70eb13e64884a9d29892620cc57fdafd0"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('bartqa': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
