11/09/2021 19:02:53 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:02:53 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:02:53 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:02:53 - INFO - __main__ - dataset_size=3430, num_shards=8, local_shard_id=6
11/09/2021 19:02:53 - INFO - __main__ - dataset_size=3430, num_shards=8, local_shard_id=1
11/09/2021 19:02:53 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:02:53 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:02:53 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:02:53 - INFO - __main__ - dataset_size=3430, num_shards=8, local_shard_id=7
11/09/2021 19:02:53 - INFO - __main__ - dataset_size=3430, num_shards=8, local_shard_id=3
11/09/2021 19:02:53 - INFO - __main__ - dataset_size=3430, num_shards=8, local_shard_id=0
11/09/2021 19:02:53 - INFO - __main__ - dataset_size=3430, num_shards=8, local_shard_id=5
11/09/2021 19:02:53 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:02:53 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:02:53 - INFO - __main__ - dataset_size=3430, num_shards=8, local_shard_id=2
11/09/2021 19:02:53 - INFO - __main__ - dataset_size=3430, num_shards=8, local_shard_id=4
11/09/2021 19:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:02:53 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:02:53 - INFO - __main__ - Start tokenizing ... 429 instances
11/09/2021 19:02:53 - INFO - __main__ - Printing 3 examples
11/09/2021 19:02:53 - INFO - __main__ - Premise: The SODIS method uses a combination of UV light and increased temperature (solar thermal) for disinfecting water using only sunlight and plastic PET bottles. </s> Hypothesis: When the temperature of water is increased after being used in cooling, it is thermal pollution. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['neutral']
11/09/2021 19:02:53 - INFO - __main__ - Premise: Solutions are homogenous mixtures of two or more components. </s> Hypothesis: Solution is the term for a homogeneous mixture of two or more substances. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['entailment']
11/09/2021 19:02:53 - INFO - __main__ - Premise: Students also study the central nervous system, skeletal and muscular systems, internal organs, and the germ theory of disease. </s> Hypothesis: Together, the muscular and skeletal organs are known as the musculoskeletal system. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['neutral']
11/09/2021 19:02:53 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:02:53 - INFO - __main__ - Start tokenizing ... 428 instances
11/09/2021 19:02:53 - INFO - __main__ - Printing 3 examples
11/09/2021 19:02:53 - INFO - __main__ - Premise: They are learning that machines make work easier and that many machines are a series of simple machines. </s> Hypothesis: A complex machine is described by: it is made of more than one simple machine. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['neutral']
11/09/2021 19:02:53 - INFO - __main__ - Premise: That means that the force of gravity downwards must be balanced by the magnetic force upwards. </s> Hypothesis: Gravity and magnetism are both common forces. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['neutral']
11/09/2021 19:02:53 - INFO - __main__ - Premise: Environmental Aspects of Geothermal Energy Use Geothermal energy does not require fuel-burning to produce heat or electricity. </s> Hypothesis: Heat is produced when paper is burned. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['neutral']
11/09/2021 19:02:53 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:02:53 - INFO - __main__ - Start tokenizing ... 429 instances
11/09/2021 19:02:53 - INFO - __main__ - Printing 3 examples
11/09/2021 19:02:53 - INFO - __main__ - Premise: Light enters the eye through the pupil, and the lens focuses the light on the retina. </s> Hypothesis: The pupil of the eye allows light to enter. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['entailment']
11/09/2021 19:02:53 - INFO - __main__ - Premise: Human pedigrees Before we consider human Mendelian inheritance it is convenient to consider the symbols used to draw pedigrees. </s> Hypothesis: Pedigree is utilized to analyze simple mendelian inheritance. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['neutral']
11/09/2021 19:02:53 - INFO - __main__ - Premise: Birds live and breed in most terrestrial habitats and on all seven continents, reaching their southern extreme in the snow petrel's breeding colonies up to 440 kilometres (270Â mi) inland in Antarctica . </s> Hypothesis: Birds live and breed on seven of the continents. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['entailment']
11/09/2021 19:02:53 - INFO - __main__ - Start tokenizing ... 429 instances
11/09/2021 19:02:53 - INFO - __main__ - Printing 3 examples
11/09/2021 19:02:53 - INFO - __main__ - Premise: An introduction to atoms and elements, compounds, atomic structure and bonding, the molecule and chemical reactions. </s> Hypothesis: Replace another in a molecule happens to atoms during a substitution reaction. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['neutral']
11/09/2021 19:02:53 - INFO - __main__ - Premise: Wavelength The distance between two consecutive points on a sinusoidal wave that are in phase; </s> Hypothesis: Wavelength is the distance between two corresponding points of adjacent waves called. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['entailment']
11/09/2021 19:02:53 - INFO - __main__ - Premise: humans normally have 23 pairs of chromosomes. </s> Hypothesis: Humans typically have 23 pairs pairs of chromosomes. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['entailment']
11/09/2021 19:02:53 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:02:53 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:02:53 - INFO - __main__ - Start tokenizing ... 429 instances
11/09/2021 19:02:53 - INFO - __main__ - Printing 3 examples
11/09/2021 19:02:53 - INFO - __main__ - Premise: For a human, a mutation in a single gene containing, e.g., 1000 bases, might take thousands of generations. </s> Hypothesis: A single human body cell typically contains thousands of genes. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['neutral']
11/09/2021 19:02:53 - INFO - __main__ - Premise: The sun lies at the heart of the solar system, where it is by far the largest object. </s> Hypothesis: Only the sun in our solar system give off their own light. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['neutral']
11/09/2021 19:02:53 - INFO - __main__ - Premise: The Solar System is the Sun and all the objects in orbit around it. </s> Hypothesis: Only the sun in our solar system give off their own light. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['neutral']
11/09/2021 19:02:53 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:02:53 - INFO - __main__ - Start tokenizing ... 428 instances
11/09/2021 19:02:53 - INFO - __main__ - Printing 3 examples
11/09/2021 19:02:53 - INFO - __main__ - Premise: All food chains begin with sunlight. </s> Hypothesis: Sunlight is the main source of energy for all of the organisms in most food chains. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['entailment']
11/09/2021 19:02:53 - INFO - __main__ - Premise: of the seeds remained dormant and viable. </s> Hypothesis: Seeds that remain inactive until the right conditions of light, water, and soil are present are called dormant | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['neutral']
11/09/2021 19:02:53 - INFO - __main__ - Premise: If the Earth were four times its current distance from the Sun, the time it would take to complete one orbit of the Sun would be 8 times longer. </s> Hypothesis: It takes about one year for earth to orbit the sun. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['neutral']
11/09/2021 19:02:53 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:02:53 - INFO - __main__ - Start tokenizing ... 429 instances
11/09/2021 19:02:53 - INFO - __main__ - Printing 3 examples
11/09/2021 19:02:53 - INFO - __main__ - Premise: This type of neutron star is known as a gamma-ray pulsar. </s> Hypothesis: Radioactive atoms, nuclear explosions, and stars produce gamma rays | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['neutral']
11/09/2021 19:02:53 - INFO - __main__ - Premise: They are evergreen with branches forming horizontal layers. </s> Hypothesis: When sediments settle out of water, they form horizontal layers. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['neutral']
11/09/2021 19:02:53 - INFO - __main__ - Start tokenizing ... 429 instances
11/09/2021 19:02:53 - INFO - __main__ - Premise: Concave lens A concave lens is thinner in the middle than at its edges and causes light rays to diverge (spread apart). </s> Hypothesis: A concave lens is thicker at the edges than it is in the middle. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - Printing 3 examples
11/09/2021 19:02:53 - INFO - __main__ - ['entailment']
11/09/2021 19:02:53 - INFO - __main__ - Premise: The 3 long chain polyunsaturated fatty acid, DHA is one of the most unsaturated fatty acids in the human body and is highly enriched in membrane lipids of rod photoreceptors. </s> Hypothesis: Unsaturated fatty acids have bent chains. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['neutral']
11/09/2021 19:02:53 - INFO - __main__ - Premise: Earthquakes are measured on a variety of scales that attempt to describe their magnitude and intensity. </s> Hypothesis: Seismologists originally measured the intensity of an earthquake with the mercalli scale. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['neutral']
11/09/2021 19:02:53 - INFO - __main__ - Premise: For example, HABs in Spain and California might be linked to a physical ocean process called upwelling (when deep ocean water comes up to the surface near the coast of a continent). </s> Hypothesis: Upwelling is the term for when deep ocean water rises to the surface. | Options: entailment, neutral, contradiction 
11/09/2021 19:02:53 - INFO - __main__ - ['entailment']
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:02:54 - INFO - __main__ - Loaded 428 examples from dev data
11/09/2021 19:02:54 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:02:54 - INFO - __main__ - Loaded 429 examples from dev data
11/09/2021 19:02:54 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:02:54 - INFO - __main__ - Loaded 428 examples from dev data
11/09/2021 19:02:54 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:02:54 - INFO - __main__ - Loaded 429 examples from dev data
11/09/2021 19:02:54 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:02:54 - INFO - __main__ - Loaded 429 examples from dev data
11/09/2021 19:02:54 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:02:54 - INFO - __main__ - Loaded 429 examples from dev data
11/09/2021 19:02:54 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:02:54 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:02:54 - INFO - __main__ - Loaded 429 examples from dev data
11/09/2021 19:02:54 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:02:54 - INFO - __main__ - Loaded 429 examples from dev data
11/09/2021 19:02:54 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:02:54 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:02:54 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:02:54 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:02:54 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:02:55 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:02:55 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:02:55 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:02:55 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:02:55 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:02:55 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:02:55 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:02:55 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:02:55 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:02:55 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:02:55 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:02:55 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:02:55 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:02:55 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:02:55 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:02:55 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:02:55 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:02:55 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:02:55 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:02:55 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:03:00 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:03:00 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:03:02 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:03:02 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:03:03 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:03:03 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:03:03 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:03:03 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:03:06 - INFO - __main__ - Starting inference ...
11/09/2021 19:03:07 - INFO - __main__ - Starting inference ...
11/09/2021 19:03:07 - INFO - __main__ - Starting inference ...
11/09/2021 19:03:08 - INFO - __main__ - Starting inference ...
11/09/2021 19:03:08 - INFO - __main__ - Starting inference ...
11/09/2021 19:03:08 - INFO - __main__ - Starting inference ...
11/09/2021 19:03:08 - INFO - __main__ - Starting inference ...
11/09/2021 19:03:08 - INFO - __main__ - Starting inference ...
11/09/2021 19:03:17 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:03:19 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:03:22 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:03:23 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:03:23 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:03:25 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:03:25 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:03:25 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:56:33 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='nli', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=512, max_output_length=10, model='facebook/bart-base', num_beams=3, output_dir='out/snli_bart-base_1109_upstream_model', predict_batch_size=64, predict_checkpoint='out/snli_bart-base_1109_upstream_model/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:56:33 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='nli', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=512, max_output_length=10, model='facebook/bart-base', num_beams=3, output_dir='out/snli_bart-base_1109_upstream_model', predict_batch_size=64, predict_checkpoint='out/snli_bart-base_1109_upstream_model/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:56:33 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='nli', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=512, max_output_length=10, model='facebook/bart-base', num_beams=3, output_dir='out/snli_bart-base_1109_upstream_model', predict_batch_size=64, predict_checkpoint='out/snli_bart-base_1109_upstream_model/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:56:33 - INFO - __main__ - dataset_size=3430, num_shards=8, local_shard_id=3
11/09/2021 19:56:33 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='nli', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=512, max_output_length=10, model='facebook/bart-base', num_beams=3, output_dir='out/snli_bart-base_1109_upstream_model', predict_batch_size=64, predict_checkpoint='out/snli_bart-base_1109_upstream_model/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:56:33 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='nli', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=512, max_output_length=10, model='facebook/bart-base', num_beams=3, output_dir='out/snli_bart-base_1109_upstream_model', predict_batch_size=64, predict_checkpoint='out/snli_bart-base_1109_upstream_model/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:56:33 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='nli', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=512, max_output_length=10, model='facebook/bart-base', num_beams=3, output_dir='out/snli_bart-base_1109_upstream_model', predict_batch_size=64, predict_checkpoint='out/snli_bart-base_1109_upstream_model/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:56:33 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='nli', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=512, max_output_length=10, model='facebook/bart-base', num_beams=3, output_dir='out/snli_bart-base_1109_upstream_model', predict_batch_size=64, predict_checkpoint='out/snli_bart-base_1109_upstream_model/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:56:33 - INFO - __main__ - dataset_size=3430, num_shards=8, local_shard_id=5
11/09/2021 19:56:33 - INFO - __main__ - dataset_size=3430, num_shards=8, local_shard_id=2
11/09/2021 19:56:33 - INFO - __main__ - dataset_size=3430, num_shards=8, local_shard_id=7
11/09/2021 19:56:33 - INFO - __main__ - dataset_size=3430, num_shards=8, local_shard_id=1
11/09/2021 19:56:33 - INFO - __main__ - dataset_size=3430, num_shards=8, local_shard_id=4
11/09/2021 19:56:33 - INFO - __main__ - dataset_size=3430, num_shards=8, local_shard_id=6
11/09/2021 19:56:33 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='nli', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=512, max_output_length=10, model='facebook/bart-base', num_beams=3, output_dir='out/snli_bart-base_1109_upstream_model', predict_batch_size=64, predict_checkpoint='out/snli_bart-base_1109_upstream_model/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:56:33 - INFO - __main__ - dataset_size=3430, num_shards=8, local_shard_id=0
11/09/2021 19:56:34 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:56:34 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:56:34 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:56:34 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:56:34 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:56:34 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:56:34 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:56:34 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:56:34 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:56:34 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:56:34 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:56:34 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:56:34 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:56:34 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:56:34 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:56:34 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:56:34 - INFO - __main__ - Start tokenizing ... 429 instances
11/09/2021 19:56:34 - INFO - __main__ - Printing 3 examples
11/09/2021 19:56:34 - INFO - __main__ - Premise: Light enters the eye through the pupil, and the lens focuses the light on the retina. </s> Hypothesis: The pupil of the eye allows light to enter. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['entailment']
11/09/2021 19:56:34 - INFO - __main__ - Premise: Human pedigrees Before we consider human Mendelian inheritance it is convenient to consider the symbols used to draw pedigrees. </s> Hypothesis: Pedigree is utilized to analyze simple mendelian inheritance. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['neutral']
11/09/2021 19:56:34 - INFO - __main__ - Premise: Birds live and breed in most terrestrial habitats and on all seven continents, reaching their southern extreme in the snow petrel's breeding colonies up to 440 kilometres (270Â mi) inland in Antarctica . </s> Hypothesis: Birds live and breed on seven of the continents. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['entailment']
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:56:34 - INFO - __main__ - Start tokenizing ... 429 instances
11/09/2021 19:56:34 - INFO - __main__ - Printing 3 examples
11/09/2021 19:56:34 - INFO - __main__ - Premise: The SODIS method uses a combination of UV light and increased temperature (solar thermal) for disinfecting water using only sunlight and plastic PET bottles. </s> Hypothesis: When the temperature of water is increased after being used in cooling, it is thermal pollution. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['neutral']
11/09/2021 19:56:34 - INFO - __main__ - Premise: Solutions are homogenous mixtures of two or more components. </s> Hypothesis: Solution is the term for a homogeneous mixture of two or more substances. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['entailment']
11/09/2021 19:56:34 - INFO - __main__ - Premise: Students also study the central nervous system, skeletal and muscular systems, internal organs, and the germ theory of disease. </s> Hypothesis: Together, the muscular and skeletal organs are known as the musculoskeletal system. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['neutral']
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:56:34 - INFO - __main__ - Start tokenizing ... 429 instances
11/09/2021 19:56:34 - INFO - __main__ - Printing 3 examples
11/09/2021 19:56:34 - INFO - __main__ - Premise: For a human, a mutation in a single gene containing, e.g., 1000 bases, might take thousands of generations. </s> Hypothesis: A single human body cell typically contains thousands of genes. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['neutral']
11/09/2021 19:56:34 - INFO - __main__ - Premise: The sun lies at the heart of the solar system, where it is by far the largest object. </s> Hypothesis: Only the sun in our solar system give off their own light. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['neutral']
11/09/2021 19:56:34 - INFO - __main__ - Premise: The Solar System is the Sun and all the objects in orbit around it. </s> Hypothesis: Only the sun in our solar system give off their own light. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - Start tokenizing ... 429 instances
11/09/2021 19:56:34 - INFO - __main__ - ['neutral']
11/09/2021 19:56:34 - INFO - __main__ - Printing 3 examples
11/09/2021 19:56:34 - INFO - __main__ - Premise: The 3 long chain polyunsaturated fatty acid, DHA is one of the most unsaturated fatty acids in the human body and is highly enriched in membrane lipids of rod photoreceptors. </s> Hypothesis: Unsaturated fatty acids have bent chains. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['neutral']
11/09/2021 19:56:34 - INFO - __main__ - Premise: Earthquakes are measured on a variety of scales that attempt to describe their magnitude and intensity. </s> Hypothesis: Seismologists originally measured the intensity of an earthquake with the mercalli scale. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['neutral']
11/09/2021 19:56:34 - INFO - __main__ - Premise: For example, HABs in Spain and California might be linked to a physical ocean process called upwelling (when deep ocean water comes up to the surface near the coast of a continent). </s> Hypothesis: Upwelling is the term for when deep ocean water rises to the surface. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['entailment']
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:56:34 - INFO - __main__ - Start tokenizing ... 428 instances
11/09/2021 19:56:34 - INFO - __main__ - Printing 3 examples
11/09/2021 19:56:34 - INFO - __main__ - Premise: All food chains begin with sunlight. </s> Hypothesis: Sunlight is the main source of energy for all of the organisms in most food chains. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['entailment']
11/09/2021 19:56:34 - INFO - __main__ - Premise: of the seeds remained dormant and viable. </s> Hypothesis: Seeds that remain inactive until the right conditions of light, water, and soil are present are called dormant | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['neutral']
11/09/2021 19:56:34 - INFO - __main__ - Premise: If the Earth were four times its current distance from the Sun, the time it would take to complete one orbit of the Sun would be 8 times longer. </s> Hypothesis: It takes about one year for earth to orbit the sun. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['neutral']
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:56:34 - INFO - __main__ - Start tokenizing ... 429 instances
11/09/2021 19:56:34 - INFO - __main__ - Printing 3 examples
11/09/2021 19:56:34 - INFO - __main__ - Premise: This type of neutron star is known as a gamma-ray pulsar. </s> Hypothesis: Radioactive atoms, nuclear explosions, and stars produce gamma rays | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['neutral']
11/09/2021 19:56:34 - INFO - __main__ - Premise: They are evergreen with branches forming horizontal layers. </s> Hypothesis: When sediments settle out of water, they form horizontal layers. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['neutral']
11/09/2021 19:56:34 - INFO - __main__ - Premise: Concave lens A concave lens is thinner in the middle than at its edges and causes light rays to diverge (spread apart). </s> Hypothesis: A concave lens is thicker at the edges than it is in the middle. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['entailment']
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:56:34 - INFO - __main__ - Start tokenizing ... 429 instances
11/09/2021 19:56:34 - INFO - __main__ - Printing 3 examples
11/09/2021 19:56:34 - INFO - __main__ - Premise: An introduction to atoms and elements, compounds, atomic structure and bonding, the molecule and chemical reactions. </s> Hypothesis: Replace another in a molecule happens to atoms during a substitution reaction. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['neutral']
11/09/2021 19:56:34 - INFO - __main__ - Premise: Wavelength The distance between two consecutive points on a sinusoidal wave that are in phase; </s> Hypothesis: Wavelength is the distance between two corresponding points of adjacent waves called. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['entailment']
11/09/2021 19:56:34 - INFO - __main__ - Premise: humans normally have 23 pairs of chromosomes. </s> Hypothesis: Humans typically have 23 pairs pairs of chromosomes. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['entailment']
11/09/2021 19:56:34 - INFO - __main__ - Start tokenizing ... 428 instances
11/09/2021 19:56:34 - INFO - __main__ - Printing 3 examples
11/09/2021 19:56:34 - INFO - __main__ - Premise: They are learning that machines make work easier and that many machines are a series of simple machines. </s> Hypothesis: A complex machine is described by: it is made of more than one simple machine. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['neutral']
11/09/2021 19:56:34 - INFO - __main__ - Premise: That means that the force of gravity downwards must be balanced by the magnetic force upwards. </s> Hypothesis: Gravity and magnetism are both common forces. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['neutral']
11/09/2021 19:56:34 - INFO - __main__ - Premise: Environmental Aspects of Geothermal Energy Use Geothermal energy does not require fuel-burning to produce heat or electricity. </s> Hypothesis: Heat is produced when paper is burned. | Options: entailment, neutral, contradiction 
11/09/2021 19:56:34 - INFO - __main__ - ['neutral']
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:56:34 - INFO - __main__ - Loaded 429 examples from dev data
11/09/2021 19:56:34 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt ....
11/09/2021 19:56:34 - INFO - __main__ - Loaded 429 examples from dev data
11/09/2021 19:56:34 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt ....
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:56:34 - INFO - __main__ - Loaded 428 examples from dev data
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:56:34 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt ....
11/09/2021 19:56:34 - INFO - __main__ - Loaded 429 examples from dev data
11/09/2021 19:56:34 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt ....
11/09/2021 19:56:34 - INFO - __main__ - Loaded 428 examples from dev data
11/09/2021 19:56:34 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt ....
11/09/2021 19:56:34 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:56:34 - INFO - __main__ - Loaded 429 examples from dev data
11/09/2021 19:56:34 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt ....
11/09/2021 19:56:35 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:56:35 - INFO - __main__ - Loaded 429 examples from dev data
11/09/2021 19:56:35 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt ....
11/09/2021 19:56:35 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:56:35 - INFO - __main__ - Loaded 429 examples from dev data
11/09/2021 19:56:35 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt ....
11/09/2021 19:56:35 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:56:35 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:56:35 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:56:35 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:56:35 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:56:35 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:56:35 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:56:35 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:56:35 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:56:35 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:56:35 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:56:35 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:56:35 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:56:35 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:56:35 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:56:35 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:56:36 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:56:36 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:56:36 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:56:36 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:56:36 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:56:36 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:56:36 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:56:36 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:56:40 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt .... Done!
11/09/2021 19:56:41 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt .... Done!
11/09/2021 19:56:42 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt .... Done!
11/09/2021 19:56:43 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt .... Done!
11/09/2021 19:56:43 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt .... Done!
11/09/2021 19:56:43 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt .... Done!
11/09/2021 19:56:43 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt .... Done!
11/09/2021 19:56:43 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt .... Done!
11/09/2021 19:56:46 - INFO - __main__ - Starting inference ...
11/09/2021 19:56:47 - INFO - __main__ - Starting inference ...
11/09/2021 19:56:47 - INFO - __main__ - Starting inference ...
11/09/2021 19:56:48 - INFO - __main__ - Starting inference ...
11/09/2021 19:56:49 - INFO - __main__ - Starting inference ...
11/09/2021 19:56:49 - INFO - __main__ - Starting inference ...
11/09/2021 19:56:49 - INFO - __main__ - Starting inference ...
11/09/2021 19:56:49 - INFO - __main__ - Starting inference ...
11/09/2021 19:56:52 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:56:52 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:56:55 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:56:56 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:56:56 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:56:56 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:56:56 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:56:56 - INFO - __main__ - Starting inference ... Done
