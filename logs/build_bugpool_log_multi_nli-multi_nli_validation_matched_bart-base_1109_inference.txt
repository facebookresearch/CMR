11/09/2021 18:59:17 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 18:59:17 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 18:59:17 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 18:59:17 - INFO - __main__ - dataset_size=9815, num_shards=8, local_shard_id=1
11/09/2021 18:59:17 - INFO - __main__ - dataset_size=9815, num_shards=8, local_shard_id=4
11/09/2021 18:59:17 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 18:59:17 - INFO - __main__ - dataset_size=9815, num_shards=8, local_shard_id=3
11/09/2021 18:59:17 - INFO - __main__ - dataset_size=9815, num_shards=8, local_shard_id=6
11/09/2021 18:59:17 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 18:59:17 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 18:59:17 - INFO - __main__ - dataset_size=9815, num_shards=8, local_shard_id=5
11/09/2021 18:59:17 - INFO - __main__ - dataset_size=9815, num_shards=8, local_shard_id=2
11/09/2021 18:59:17 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 18:59:17 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 18:59:17 - INFO - __main__ - dataset_size=9815, num_shards=8, local_shard_id=0
11/09/2021 18:59:17 - INFO - __main__ - dataset_size=9815, num_shards=8, local_shard_id=7
11/09/2021 18:59:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 18:59:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 18:59:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 18:59:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 18:59:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 18:59:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 18:59:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 18:59:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 18:59:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 18:59:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 18:59:18 - INFO - __main__ - Start tokenizing ... 1227 instances
11/09/2021 18:59:18 - INFO - __main__ - Printing 3 examples
11/09/2021 18:59:18 - INFO - __main__ - Premise: He fled in his car when cops arrived and led them on a chase that ended in the massive crash. </s> Hypothesis: He crashed his car because the tires burst. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['neutral']
11/09/2021 18:59:18 - INFO - __main__ - Premise: Yes, undoubtedly the hand of Mr. Brown! Mr. Carter paused. </s> Hypothesis: Carter was excited. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['neutral']
11/09/2021 18:59:18 - INFO - __main__ - Premise: He reverted to his former point of view. </s> Hypothesis: He went back to his previous thoughts about violence. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['neutral']
11/09/2021 18:59:18 - INFO - __main__ - Tokenizing Input ...
11/09/2021 18:59:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 18:59:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 18:59:18 - INFO - __main__ - Start tokenizing ... 1227 instances
11/09/2021 18:59:18 - INFO - __main__ - Printing 3 examples
11/09/2021 18:59:18 - INFO - __main__ - Premise: Today, nothing remains except the foundations. </s> Hypothesis: The rest was destroyed centuries ago. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['neutral']
11/09/2021 18:59:18 - INFO - __main__ - Premise: LSC set a deadline of October 1, 1998, for submission of state planning reports. </s> Hypothesis: LSC set a deadline to submit state reports to make their job easier | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['neutral']
11/09/2021 18:59:18 - INFO - __main__ - Premise: oh  that's accommodating </s> Hypothesis: That is disruptive. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['contradiction']
11/09/2021 18:59:18 - INFO - __main__ - Tokenizing Input ...
11/09/2021 18:59:18 - INFO - __main__ - Start tokenizing ... 1227 instances
11/09/2021 18:59:18 - INFO - __main__ - Printing 3 examples
11/09/2021 18:59:18 - INFO - __main__ - Premise: Horwitz makes us see that the pinched circumstances of their lives are not so different from the conditions of their ancestors, dirt-poor yeoman farmers who seldom saw, much less owned, a slave. </s> Hypothesis: Horwitz says that they are as unhappy as their ancestors. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['neutral']
11/09/2021 18:59:18 - INFO - __main__ - Premise: Sure, the man yells back, you're in a hot air balloon about 30 feet above this field. </s> Hypothesis: "Sure." The man yells "youre in a hot air balloon about 30 feet high above the field  | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['entailment']
11/09/2021 18:59:18 - INFO - __main__ - Premise: In 2001, LSC continued to play an active role in encouraging and supporting states' technology plans. </s> Hypothesis:  In 2001, LSC continued to play an inactive role in not supporting states' technology plans. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['contradiction']
11/09/2021 18:59:18 - INFO - __main__ - Tokenizing Input ...
11/09/2021 18:59:18 - INFO - __main__ - Start tokenizing ... 1227 instances
11/09/2021 18:59:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 18:59:18 - INFO - __main__ - Printing 3 examples
11/09/2021 18:59:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 18:59:18 - INFO - __main__ - Premise: To see the desert at its best, go out at dawn and at sunset. </s> Hypothesis: Optimal times to really experience what the desert has to offer are at sunrise and sunset. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['entailment']
11/09/2021 18:59:18 - INFO - __main__ - Premise: Albino Alligator (Miramax). </s> Hypothesis: Alligators are all born with identical color sequences. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['contradiction']
11/09/2021 18:59:18 - INFO - __main__ - Premise: A museum inside the building gives intriguing insight into the life and heyday of the their rich costumes, their scimitars, and rifles inlaid with bright jewels and silver and a horrible bludgeon with a double serrated edge. </s> Hypothesis: The museum inside of the building features stuffed monkeys.  | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['contradiction']
11/09/2021 18:59:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 18:59:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 18:59:18 - INFO - __main__ - Tokenizing Input ...
11/09/2021 18:59:18 - INFO - __main__ - Start tokenizing ... 1227 instances
11/09/2021 18:59:18 - INFO - __main__ - Printing 3 examples
11/09/2021 18:59:18 - INFO - __main__ - Premise: 'Have you Mr. Whittington's address in town?  </s> Hypothesis: Is the address for Mr. Whittington located in town or in the country? | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['neutral']
11/09/2021 18:59:18 - INFO - __main__ - Premise: In the 1980s, a pragmatic socialist coalition government with the Christian Democrats brought a few years of unusual stability. </s> Hypothesis: The Christian Democrats caused great instability in the 1980s. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['contradiction']
11/09/2021 18:59:18 - INFO - __main__ - Premise: Like Arabs and Jews, Diamond warns, Koreans and Japanese are joined by blood yet locked in traditional enmity. </s> Hypothesis: Koreans and Japanese have tension between them because of a war long ago. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['neutral']
11/09/2021 18:59:18 - INFO - __main__ - Tokenizing Input ...
11/09/2021 18:59:18 - INFO - __main__ - Start tokenizing ... 1227 instances
11/09/2021 18:59:18 - INFO - __main__ - Printing 3 examples
11/09/2021 18:59:18 - INFO - __main__ - Premise: IDPA's OIG's mission is to prevent, detect, and eliminate fraud, waste, abuse, and misconduct in various payment programs. </s> Hypothesis: IDPA's OIG's mission is to take care of the forests. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['contradiction']
11/09/2021 18:59:18 - INFO - __main__ - Premise: uh uh yeah that well um the older you get the more convenience you try to bring with you i guess so i'm up to dragging the trailer around which is my next step is going to be probably Winnebago i hope if i only can afford one but that </s> Hypothesis: The older you get, the more you want conveniences. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['entailment']
11/09/2021 18:59:18 - INFO - __main__ - Premise: After criticizing the GOP openly for weeks, Buchanan announced that he would seek the Reform presidential nomination, which would bring him $12 million in federal funds. </s> Hypothesis: Buchanan was not given the nomination because of his public comments. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['neutral']
11/09/2021 18:59:18 - INFO - __main__ - Tokenizing Input ...
11/09/2021 18:59:18 - INFO - __main__ - Start tokenizing ... 1227 instances
11/09/2021 18:59:18 - INFO - __main__ - Printing 3 examples
11/09/2021 18:59:18 - INFO - __main__ - Premise: The new rights are nice enough </s> Hypothesis: Everyone really likes the newest benefits  | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['neutral']
11/09/2021 18:59:18 - INFO - __main__ - Premise: This site includes a list of all award winners and a searchable database of Government Executive articles. </s> Hypothesis: The Government Executive articles housed on the website are not able to be searched. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['contradiction']
11/09/2021 18:59:18 - INFO - __main__ - Premise: uh i don't know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him </s> Hypothesis: I like him for the most part, but would still enjoy seeing someone beat him. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['entailment']
11/09/2021 18:59:18 - INFO - __main__ - Tokenizing Input ...
11/09/2021 18:59:18 - INFO - __main__ - Start tokenizing ... 1226 instances
11/09/2021 18:59:18 - INFO - __main__ - Printing 3 examples
11/09/2021 18:59:18 - INFO - __main__ - Premise: away from the children </s> Hypothesis: Close to the kids | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['contradiction']
11/09/2021 18:59:18 - INFO - __main__ - Premise: The collection and indeed the building itself is not huge or overbearing, allowing visitors to relax and enjoy the art perhaps more than is possible in such massive galleries as the Louvre or Rijksmuseum. </s> Hypothesis: The Louvre and the Rijksmuseum are incredibly small galleries in comparison. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['contradiction']
11/09/2021 18:59:18 - INFO - __main__ - Premise: A lot of people rely on their local government for protection. </s> Hypothesis: The government provides protection to minority groups. | Options: entailment, neutral, contradiction 
11/09/2021 18:59:18 - INFO - __main__ - ['neutral']
11/09/2021 18:59:18 - INFO - __main__ - Tokenizing Input ...
11/09/2021 18:59:18 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 18:59:18 - INFO - __main__ - Tokenizing Output ...
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 18:59:19 - INFO - __main__ - Loaded 1227 examples from dev data
11/09/2021 18:59:19 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Output ...
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 18:59:19 - INFO - __main__ - Loaded 1227 examples from dev data
11/09/2021 18:59:19 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Output ...
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Output ...
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Output ...
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Output ...
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 18:59:19 - INFO - __main__ - Loaded 1227 examples from dev data
11/09/2021 18:59:19 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Output ...
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 18:59:19 - INFO - __main__ - Loaded 1227 examples from dev data
11/09/2021 18:59:19 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 18:59:19 - INFO - __main__ - Loaded 1227 examples from dev data
11/09/2021 18:59:19 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 18:59:19 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 18:59:19 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 18:59:19 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 18:59:19 - INFO - __main__ - Loaded 1227 examples from dev data
11/09/2021 18:59:19 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 18:59:19 - INFO - __main__ - Tokenizing Output ...
11/09/2021 18:59:20 - INFO - __main__ - Loaded 1227 examples from dev data
11/09/2021 18:59:20 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 18:59:20 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 18:59:20 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 18:59:20 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 18:59:20 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 18:59:20 - INFO - __main__ - Loaded 1226 examples from dev data
11/09/2021 18:59:20 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 18:59:20 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 18:59:20 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 18:59:20 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 18:59:20 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 18:59:20 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 18:59:20 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 18:59:20 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 18:59:20 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 18:59:20 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 18:59:20 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 18:59:20 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 18:59:20 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 18:59:20 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 18:59:20 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 18:59:21 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 18:59:21 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 18:59:21 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 18:59:21 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 18:59:25 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 18:59:26 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 18:59:27 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 18:59:27 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 18:59:28 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 18:59:28 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 18:59:28 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 18:59:28 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 18:59:31 - INFO - __main__ - Starting inference ...
11/09/2021 18:59:31 - INFO - __main__ - Starting inference ...
11/09/2021 18:59:33 - INFO - __main__ - Starting inference ...
11/09/2021 18:59:33 - INFO - __main__ - Starting inference ...
11/09/2021 18:59:33 - INFO - __main__ - Starting inference ...
11/09/2021 18:59:33 - INFO - __main__ - Starting inference ...
11/09/2021 18:59:33 - INFO - __main__ - Starting inference ...
11/09/2021 18:59:33 - INFO - __main__ - Starting inference ...
11/09/2021 19:00:04 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:00:05 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:00:18 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:00:18 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:00:21 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:00:22 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:00:24 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:00:27 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:54:28 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='nli', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=512, max_output_length=10, model='facebook/bart-base', num_beams=3, output_dir='out/snli_bart-base_1109_upstream_model', predict_batch_size=64, predict_checkpoint='out/snli_bart-base_1109_upstream_model/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:54:28 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='nli', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=512, max_output_length=10, model='facebook/bart-base', num_beams=3, output_dir='out/snli_bart-base_1109_upstream_model', predict_batch_size=64, predict_checkpoint='out/snli_bart-base_1109_upstream_model/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:54:28 - INFO - __main__ - dataset_size=9815, num_shards=8, local_shard_id=7
11/09/2021 19:54:28 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='nli', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=512, max_output_length=10, model='facebook/bart-base', num_beams=3, output_dir='out/snli_bart-base_1109_upstream_model', predict_batch_size=64, predict_checkpoint='out/snli_bart-base_1109_upstream_model/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:54:28 - INFO - __main__ - dataset_size=9815, num_shards=8, local_shard_id=5
11/09/2021 19:54:28 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='nli', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=512, max_output_length=10, model='facebook/bart-base', num_beams=3, output_dir='out/snli_bart-base_1109_upstream_model', predict_batch_size=64, predict_checkpoint='out/snli_bart-base_1109_upstream_model/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:54:28 - INFO - __main__ - dataset_size=9815, num_shards=8, local_shard_id=6
11/09/2021 19:54:28 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='nli', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=512, max_output_length=10, model='facebook/bart-base', num_beams=3, output_dir='out/snli_bart-base_1109_upstream_model', predict_batch_size=64, predict_checkpoint='out/snli_bart-base_1109_upstream_model/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:54:28 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='nli', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=512, max_output_length=10, model='facebook/bart-base', num_beams=3, output_dir='out/snli_bart-base_1109_upstream_model', predict_batch_size=64, predict_checkpoint='out/snli_bart-base_1109_upstream_model/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:54:28 - INFO - __main__ - dataset_size=9815, num_shards=8, local_shard_id=1
11/09/2021 19:54:28 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='nli', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=512, max_output_length=10, model='facebook/bart-base', num_beams=3, output_dir='out/snli_bart-base_1109_upstream_model', predict_batch_size=64, predict_checkpoint='out/snli_bart-base_1109_upstream_model/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:54:28 - INFO - __main__ - dataset_size=9815, num_shards=8, local_shard_id=2
11/09/2021 19:54:28 - INFO - __main__ - dataset_size=9815, num_shards=8, local_shard_id=3
11/09/2021 19:54:28 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='nli', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=512, max_output_length=10, model='facebook/bart-base', num_beams=3, output_dir='out/snli_bart-base_1109_upstream_model', predict_batch_size=64, predict_checkpoint='out/snli_bart-base_1109_upstream_model/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:54:28 - INFO - __main__ - dataset_size=9815, num_shards=8, local_shard_id=0
11/09/2021 19:54:28 - INFO - __main__ - dataset_size=9815, num_shards=8, local_shard_id=4
11/09/2021 19:54:28 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:54:28 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:54:28 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:54:28 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:54:28 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:54:28 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:54:28 - INFO - __main__ - Start tokenizing ... 1226 instances
11/09/2021 19:54:28 - INFO - __main__ - Printing 3 examples
11/09/2021 19:54:28 - INFO - __main__ - Premise: away from the children </s> Hypothesis: Close to the kids | Options: entailment, neutral, contradiction 
11/09/2021 19:54:28 - INFO - __main__ - ['contradiction']
11/09/2021 19:54:28 - INFO - __main__ - Premise: The collection and indeed the building itself is not huge or overbearing, allowing visitors to relax and enjoy the art perhaps more than is possible in such massive galleries as the Louvre or Rijksmuseum. </s> Hypothesis: The Louvre and the Rijksmuseum are incredibly small galleries in comparison. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:28 - INFO - __main__ - ['contradiction']
11/09/2021 19:54:28 - INFO - __main__ - Premise: A lot of people rely on their local government for protection. </s> Hypothesis: The government provides protection to minority groups. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:28 - INFO - __main__ - ['neutral']
11/09/2021 19:54:28 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:54:28 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:54:28 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:54:28 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:54:28 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:54:28 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:54:28 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:54:28 - INFO - __main__ - Start tokenizing ... 1227 instances
11/09/2021 19:54:28 - INFO - __main__ - Printing 3 examples
11/09/2021 19:54:28 - INFO - __main__ - Premise: Horwitz makes us see that the pinched circumstances of their lives are not so different from the conditions of their ancestors, dirt-poor yeoman farmers who seldom saw, much less owned, a slave. </s> Hypothesis: Horwitz says that they are as unhappy as their ancestors. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:28 - INFO - __main__ - ['neutral']
11/09/2021 19:54:28 - INFO - __main__ - Premise: Sure, the man yells back, you're in a hot air balloon about 30 feet above this field. </s> Hypothesis: "Sure." The man yells "youre in a hot air balloon about 30 feet high above the field  | Options: entailment, neutral, contradiction 
11/09/2021 19:54:28 - INFO - __main__ - ['entailment']
11/09/2021 19:54:28 - INFO - __main__ - Premise: In 2001, LSC continued to play an active role in encouraging and supporting states' technology plans. </s> Hypothesis:  In 2001, LSC continued to play an inactive role in not supporting states' technology plans. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:28 - INFO - __main__ - ['contradiction']
11/09/2021 19:54:28 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:54:29 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:54:29 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:54:29 - INFO - __main__ - Start tokenizing ... 1227 instances
11/09/2021 19:54:29 - INFO - __main__ - Printing 3 examples
11/09/2021 19:54:29 - INFO - __main__ - Premise: 'Have you Mr. Whittington's address in town?  </s> Hypothesis: Is the address for Mr. Whittington located in town or in the country? | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['neutral']
11/09/2021 19:54:29 - INFO - __main__ - Premise: In the 1980s, a pragmatic socialist coalition government with the Christian Democrats brought a few years of unusual stability. </s> Hypothesis: The Christian Democrats caused great instability in the 1980s. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['contradiction']
11/09/2021 19:54:29 - INFO - __main__ - Premise: Like Arabs and Jews, Diamond warns, Koreans and Japanese are joined by blood yet locked in traditional enmity. </s> Hypothesis: Koreans and Japanese have tension between them because of a war long ago. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['neutral']
11/09/2021 19:54:29 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:54:29 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:54:29 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:54:29 - INFO - __main__ - Start tokenizing ... 1227 instances
11/09/2021 19:54:29 - INFO - __main__ - Printing 3 examples
11/09/2021 19:54:29 - INFO - __main__ - Premise: IDPA's OIG's mission is to prevent, detect, and eliminate fraud, waste, abuse, and misconduct in various payment programs. </s> Hypothesis: IDPA's OIG's mission is to take care of the forests. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['contradiction']
11/09/2021 19:54:29 - INFO - __main__ - Premise: uh uh yeah that well um the older you get the more convenience you try to bring with you i guess so i'm up to dragging the trailer around which is my next step is going to be probably Winnebago i hope if i only can afford one but that </s> Hypothesis: The older you get, the more you want conveniences. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['entailment']
11/09/2021 19:54:29 - INFO - __main__ - Premise: After criticizing the GOP openly for weeks, Buchanan announced that he would seek the Reform presidential nomination, which would bring him $12 million in federal funds. </s> Hypothesis: Buchanan was not given the nomination because of his public comments. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['neutral']
11/09/2021 19:54:29 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:54:29 - INFO - __main__ - Start tokenizing ... 1227 instances
11/09/2021 19:54:29 - INFO - __main__ - Printing 3 examples
11/09/2021 19:54:29 - INFO - __main__ - Premise: He fled in his car when cops arrived and led them on a chase that ended in the massive crash. </s> Hypothesis: He crashed his car because the tires burst. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['neutral']
11/09/2021 19:54:29 - INFO - __main__ - Premise: Yes, undoubtedly the hand of Mr. Brown! Mr. Carter paused. </s> Hypothesis: Carter was excited. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['neutral']
11/09/2021 19:54:29 - INFO - __main__ - Premise: He reverted to his former point of view. </s> Hypothesis: He went back to his previous thoughts about violence. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['neutral']
11/09/2021 19:54:29 - INFO - __main__ - Start tokenizing ... 1227 instances
11/09/2021 19:54:29 - INFO - __main__ - Printing 3 examples
11/09/2021 19:54:29 - INFO - __main__ - Premise: To see the desert at its best, go out at dawn and at sunset. </s> Hypothesis: Optimal times to really experience what the desert has to offer are at sunrise and sunset. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['entailment']
11/09/2021 19:54:29 - INFO - __main__ - Premise: Albino Alligator (Miramax). </s> Hypothesis: Alligators are all born with identical color sequences. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['contradiction']
11/09/2021 19:54:29 - INFO - __main__ - Premise: A museum inside the building gives intriguing insight into the life and heyday of the their rich costumes, their scimitars, and rifles inlaid with bright jewels and silver and a horrible bludgeon with a double serrated edge. </s> Hypothesis: The museum inside of the building features stuffed monkeys.  | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['contradiction']
11/09/2021 19:54:29 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:54:29 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:54:29 - INFO - __main__ - Start tokenizing ... 1227 instances
11/09/2021 19:54:29 - INFO - __main__ - Printing 3 examples
11/09/2021 19:54:29 - INFO - __main__ - Premise: The new rights are nice enough </s> Hypothesis: Everyone really likes the newest benefits  | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['neutral']
11/09/2021 19:54:29 - INFO - __main__ - Premise: This site includes a list of all award winners and a searchable database of Government Executive articles. </s> Hypothesis: The Government Executive articles housed on the website are not able to be searched. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['contradiction']
11/09/2021 19:54:29 - INFO - __main__ - Premise: uh i don't know i i have mixed emotions about him uh sometimes i like him but at the same times i love to see somebody beat him </s> Hypothesis: I like him for the most part, but would still enjoy seeing someone beat him. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['entailment']
11/09/2021 19:54:29 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:54:29 - INFO - __main__ - Start tokenizing ... 1227 instances
11/09/2021 19:54:29 - INFO - __main__ - Printing 3 examples
11/09/2021 19:54:29 - INFO - __main__ - Premise: Today, nothing remains except the foundations. </s> Hypothesis: The rest was destroyed centuries ago. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['neutral']
11/09/2021 19:54:29 - INFO - __main__ - Premise: LSC set a deadline of October 1, 1998, for submission of state planning reports. </s> Hypothesis: LSC set a deadline to submit state reports to make their job easier | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['neutral']
11/09/2021 19:54:29 - INFO - __main__ - Premise: oh  that's accommodating </s> Hypothesis: That is disruptive. | Options: entailment, neutral, contradiction 
11/09/2021 19:54:29 - INFO - __main__ - ['contradiction']
11/09/2021 19:54:29 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:54:29 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:54:29 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:54:29 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:54:29 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:54:29 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:54:29 - INFO - __main__ - Loaded 1226 examples from dev data
11/09/2021 19:54:29 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt ....
11/09/2021 19:54:29 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:54:29 - INFO - __main__ - Loaded 1227 examples from dev data
11/09/2021 19:54:29 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt ....
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:54:30 - INFO - __main__ - Loaded 1227 examples from dev data
11/09/2021 19:54:30 - INFO - __main__ - Loaded 1227 examples from dev data
11/09/2021 19:54:30 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:54:30 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt ....
11/09/2021 19:54:30 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:54:30 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt ....
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:54:30 - INFO - __main__ - Loaded 1227 examples from dev data
11/09/2021 19:54:30 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt ....
11/09/2021 19:54:30 - INFO - __main__ - Loaded 1227 examples from dev data
11/09/2021 19:54:30 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt ....
11/09/2021 19:54:30 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:54:30 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:54:30 - INFO - __main__ - Loaded 1227 examples from dev data
11/09/2021 19:54:30 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:54:30 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt ....
11/09/2021 19:54:30 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:54:30 - INFO - __main__ - Loaded 1227 examples from dev data
11/09/2021 19:54:30 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt ....
11/09/2021 19:54:30 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:54:31 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:54:31 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:54:31 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:54:31 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:54:31 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:54:31 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:54:31 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:54:31 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:54:31 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:54:31 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:54:31 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:54:31 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:54:31 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:54:31 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:54:31 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:54:31 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:54:31 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:54:31 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:54:36 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt .... Done!
11/09/2021 19:54:37 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt .... Done!
11/09/2021 19:54:37 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt .... Done!
11/09/2021 19:54:38 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt .... Done!
11/09/2021 19:54:38 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt .... Done!
11/09/2021 19:54:39 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt .... Done!
11/09/2021 19:54:39 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt .... Done!
11/09/2021 19:54:39 - INFO - __main__ - Loading checkpoint from out/snli_bart-base_1109_upstream_model/best-model.pt .... Done!
11/09/2021 19:54:42 - INFO - __main__ - Starting inference ...
11/09/2021 19:54:42 - INFO - __main__ - Starting inference ...
11/09/2021 19:54:42 - INFO - __main__ - Starting inference ...
11/09/2021 19:54:43 - INFO - __main__ - Starting inference ...
11/09/2021 19:54:43 - INFO - __main__ - Starting inference ...
11/09/2021 19:54:44 - INFO - __main__ - Starting inference ...
11/09/2021 19:54:44 - INFO - __main__ - Starting inference ...
11/09/2021 19:54:44 - INFO - __main__ - Starting inference ...
11/09/2021 19:54:58 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:54:59 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:55:03 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:55:04 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:55:05 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:55:05 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:55:06 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:55:06 - INFO - __main__ - Starting inference ... Done
