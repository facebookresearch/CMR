09/23/2021 13:06:29 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:06:30 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:06:30 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:06:34 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:06:36 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:06:39 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:06:39 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:06:39 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:06:42 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:06:48 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:06:48 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:06:48 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:06:48 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:06:48 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:06:48 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:06:50 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:06:50 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:06:50 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:06:50 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:06:50 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:06:50 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:06:50 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:06:50 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:06:55 - INFO - __main__ - Finished.
09/23/2021 13:06:55 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:06:55 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:06:55 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:06:55 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:06:55 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:06:57 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:06:57 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:06:57 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:07:01 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:07:01 - INFO - __main__ - Found 32 errors.
09/23/2021 13:07:01 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:07:07 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:07:07 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:07:10 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:07:10 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:07:10 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:07:12 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:07:12 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:07:12 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:07:12 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:07:12 - INFO - __main__ - Finished.
09/23/2021 13:07:12 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:07:12 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:07:12 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:07:15 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:07:15 - INFO - __main__ - Found 26 errors.
09/23/2021 13:07:15 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:07:15 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:07:15 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:07:16 - INFO - __main__ - interference_scores=[(('Context: The rainforest contains several species that can pose a hazard. Among the largest predatory creatures are the black caiman, jaguar, cougar, and anaconda. In the river, electric eels can produce an electric shock that can stun or kill, while piranha are known to bite and injure humans. Various species of poison dart frogs secrete lipophilic alkaloid toxins through their flesh. There are also numerous parasites and disease vectors. Vampire bats dwell in the rainforest and can spread the rabies virus. Malaria, yellow fever and Dengue fever can also be contracted in the Amazon region. | Question: What fish living in the Amazon river is known to bit humans?', ['piranha'], 'mrqa_squad-validation-4185'), tensor(1.5536)), (('Context: That Was the Week That WasThat Was the Week That Was, informally TWTWTW or TW3, was a satirical television comedy programme on BBC Television in 1962 and 1963. It was devised, produced and directed by Ned Sherrin and presented by David Frost. An American version by the same name aired on NBC from 1964 to 1965, also featuring Frost.  The programme is considered a significant element of the satire boom in the UK in the early 1960s. It broke ground in comedy through lampooning the establishment and political figures. Its broadcast coincided with coverage of the politically charged Profumo affair and John Profumo, the politician at the centre of the affair, became a target for derision. TW3 was first broadcast on Saturday, 24 November 1962.  Cast and writers  Cast members included cartoonist Timothy Birdsall, political commentator Bernard Levin, and actors Lance Percival, who sang topical calypsos, many improvised to suggestions from the audience, Kenneth Cope, Roy Kinnear, Willie Rushton, Al Mancini, Robert Lang,  David Kernan and Millicent Martin. The last two were also singers and the programme opened with a song\xa0– That Was The Week That Was\xa0– sung by Martin to Ron Grainer\'s theme tune and enumerating topics in the news. Frankie Howerd also guested with stand-up comedy.   Script-writers included John Albery, John Antrobus, John Betjeman, John Bird, Graham Chapman, John Cleese, Peter Cook, Roald Dahl, Richard Ingrams, Lyndon Irving, Gerald Kaufman, Frank Muir, David Nobbs, Denis Norden, Bill Oddie, Dennis Potter, Eric Sykes, Kenneth Tynan, and Keith Waterhouse.   Programme  The programme opened with a song ("That was the week that was, It\'s over, let it go ...") sung by Millicent Martin, referring to news of the week just gone. Lance Percival sang a topical calypso each week. Satirical targets, such as Prime Minister Harold Macmillan and Home Secretary Henry Brooke were lampooned in sketches, debates and monologues. Other targets were the monarchy, Britain\'s declining status as a global power, racism (particularly in the American South and South Africa under Apartheid), sexual and social hypocrisy, the class system, and the BBC itself. Well-remembered sketches include a \'consumers\' guide to religion\', which discussed relative merits of faiths in the manner of a Which? magazine report.  On Saturday, 20 October 1962 the award of Nobel prizes to John Kendrew and Max Perutz, and to Francis Crick, James D. Watson, and Maurice Wilkins was satirised in a short sketch with the prizes referred to as the Alfred Nobel Peace Pools; in this sketch Watson was called "Little J.D. Watson" and "Who\'d have thought he\'d ever get the Nobel Prize? Makes you think, doesn\'t it". The germ of the joke was that Watson was only 25 when he helped discover DNA; much younger than the others.  TW3 was broadcast on Saturday night and attracted an audience of 12 million. It often under- or overran as cast and crew worked through material as they saw fit. At the beginning of the second season in the autumn of 1963, in an attempt to assert control over the programme, the BBC scheduled repeats of The Third Man television series after the end of TW3. Frost suggested a means of sabotaging this tactic to Sherrin, and he agreed. For three weeks, Frost read out the plot of The Third Man, until the repeats were abandoned following the direct intervention of the BBCs Director General Hugh Greene.   Frost often ended a satirical attack with the remark "But seriously, he\'s doing a grand job".  At the end of each episode, Frost usually signed off with: "That was the week, that was." At the end of the final programme he announced: "That was That Was The Week That Was...that was."  Kennedy tribute  For the edition on Saturday, 23 November 1963, the day after the assassination of United States President John F. Kennedy, TW3 produced a shortened 20-minute programme with no satire, reflecting on the loss, including a contribution from Dame | Question: Who was the deviser, producer and director \'That Was The Week That Was\'?', ['ned sherrin'], 'mrqa_triviaqa-validation-4054'), tensor(0.9726)), (('Context: The Super Bowl 50 Host Committee has vowed to be "the most giving Super Bowl ever", and will dedicate 25 percent of all money it raises for philanthropic causes in the Bay Area. The committee created the 50 fund as its philanthropic initiative and focuses on providing grants to aid with youth development, community investment and sustainable environments. | Question: What was the name of the fund setup to help with investing in the community?', ['50 fund', 'the 50 fund'], 'mrqa_squad-validation-392'), tensor(0.7536)), (('Context: Great Train Robbery Anniversary - Heart Beds, Bucks ...Great Train Robbery Anniversary - Heart Beds, Bucks & Herts News  Great Train Robbery Anniversary  Great Train Robbery Anniversary  See pictures of the bridge near Leighton Buzzard where The Great Train Robbery took place fifty years ago.  See all photos (15) in this gallery  By John Stratford (@Heart_JohnS), 8th August 2013, 10:50  Fifty years after The Great Train Robbery, an event\'s taken place to praise police who searched for the men who robbed a mail train in Buckinghamshire.  You may also like  On 8 August 1963, a gang of robbers, masterminded by Bruce Reynolds, stopped the Glasgow-Euston overnight mail train as it passed close to Cheddington, near Leighton Buzzard.  On board were huge numbers of used bank notes.  Twelve of the robbers were jailed for a total of more than 300 years but more than one broke out of prison, including notorious criminal Ronnie Biggs, who spent over 30 years on the run before he finally returned to Britain in 2001 to face arrest.  Reynolds returned in 1968, five years after the crime, and was captured in Torquay and jailed for 25 years.  Two Buckinghamshire Constabulary police officers who were involved in the investigation attended a police commemoration event alongside serving Thames Valley Police officers at Eynsham Hall in Witney, Oxfordshire, on Wednesday 7 August 2013.  Keith Milner was a detective at Aylesbury at the time of the robbery, while John Woolley was a PC and discovered Leatherslade Farm near Brill, Buckinghamshire, where the men hid after committing the crime.  Biggs insisted in July 2013 that he was proud to have been part of the gang.  The famous fugitive, who celebrates his 84th birthday on 8 August 2013, escaped from prison in 1965 and spent 36 years on the run before finally being arrested and jailed in 2001.  Released from prison on compassionate grounds in 2009 due to ill health he is still alive, being cared for in a north London nursing home, and he has few regrets about the crime that made him a household name.  Biggs, who cannot speak and communicates through a spelling board, said: "If you want to ask me if I have any regrets about being one of the train robbers, my answer is, \'No!\'.  "I will go further: I am proud to have been one of them. I am equally happy to be described as the \'tea-boy\' or \'The Brain\'.  "I was there that August night and that is what counts. I am one of the few witnesses - living or dead - to what was \'The Crime of the Century\'.\'\'  But although he is proud to have been involved in the headline-grabbing crime, he admitted he does have some regrets.  "It is regrettable, as I have said many times, that the train driver was injured,\'\' he said. "And he was not the only victim.  "The people who paid the heaviest price for the Great Train Robbery are the families. The families of everyone involved in the Great Train Robbery, and from both sides of the track.  "All have paid a price for our collective involvement in the robbery. A very heavy price, in the case of my family.  "For that, I do have my regrets.\'\'  Ronnie\'s son Michael, who was born while Ronnie was on the run, spoke exclusively to Heart and said he isn\'t proud of his dad being a criminal, but he is proud of the man his dad is.  "I believe if you commit a crime, you have to do the time," he said, "but the time has to be proportional with the crime that you\'ve committed.  "The sad bit was that he went into the train robbery because he needed £500 to put a deposit on his house. About a week before the robbery, he won the £500 in a bet on the horses but there was no turning back then."  Listen to Michael Biggs | Question: What criminal offence took place in Cheddington, Buckinghamshire in August (8th) 1963?', ['great train robbery'], 'mrqa_triviaqa-validation-7369'), tensor(0.6209)), (('Context: An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses . The same principle is used for funicular railways with two connected railway cars on inclined tracks , and for the elevators on the Eiffel Tower which counterbalance each other . Ski lifts are another example , where the gondolas move on a closed ( continuous ) pulley system up and down the mountain . The ski lift is similar to the counter-weighted elevator , but with a constraining force provided by the cable in the vertical dimension thereby achieving work in both the horizontal and vertical dimensions . Boat lifts are another type of counter-weighted elevator system approximating an Atwood machine . | Question: what is a real world application of an atwood machine ?', ['An elevator with a counterbalance'], 'mrqa_naturalquestions-validation-2730'), tensor(0.3404)), (('Context: Cor, Blimey!  Cor, Blimey!  is a 2000 TV film that follows the relationship between "Carry On" film actors Sid James (played by Geoffrey Hutchings) and Barbara Windsor (played by Samantha Spiro).   Samantha Spiro  Samantha Spiro (born 20 June 1968) is a double Olivier Award-winning English actress.  She is best known for portraying Barbara Windsor in the stage play "Cleo, Camping, Emmanuelle and Dick" and the television film "Cor, Blimey! ", DI Vivien Friend in "", and Melessa Tarly in the HBO series "Game of Thrones". | Question: Who plays opposite the double Olivier Award-winning English actress in the 2000 TV film following the relationship between "Carry On" film actors Sid James and Barbara Windsor?', ['Geoffrey Hutchings'], 'mrqa_hotpotqa-validation-2970'), tensor(0.2778)), (("Context: May 14 : The Corps of Discovery departs from Camp Dubois at 4 p.m. , marking the beginning of the voyage to the Pacific coast .   May 16 : The Corps of Discovery arrives at St. Charles , Missouri .   May 21 : Departure from St. Charles at 3 : 30 p.m.   May 24 : Pass Boones Settlement . Home of famous woodsman L. Willenborg .   May 25 : The expedition passes the small village of La Charrette on the Missouri River . Charles Floyd writes in his journal that this is `` the last settlement of whites on this river '' .   June 1 : The expedition reaches the Osage River .   June 12 : Lewis and Clark meet three trappers in two pirogues . One of the men was Pierre Dorion , Jr. -- who knew George Rogers Clark . Lewis and Clark persuade Dorion to return to Sioux camp to act as interpreter .   June 26 : The expedition arrives at Kaw Point where the Kansas River drains into the Missouri River basin .   June 28 -- 29 : First trial in new territory . Pvt . John Collins is on guard duty and breaks into the supplies and gets drunk . Collins invites Pvt . Hugh Hall to drink also . Collins receives 100 lashes , Hall receives 50 lashes .   July 4 : Marking Independence Day , the expedition names Independence Creek located near Atchison , Kansas .   July 11 -- 12 : Second trial in new territory . Pvt . Alexander Hamilton Willard is on guard duty . Is charged with lying down and sleeping at his post whilst a sentinel . Punishable by death . He receives 100 lashes for four straight days .   July 21 : Reaches the Platte River , 640 miles from St. Louis . Entering Sioux Territory .   August 1 : Captain William Clark 's 34th birthday .   August 3 : The Corps of Discovery holds the first official council between representatives of the United States and the Oto and Missouri tribes at Council Bluffs , Iowa . They hand out peace medals , 15 - star flags and other gifts , parade men and show off technology .   August 4 : Moses Reed said he was returning to a previous camp to retrieve a knife but deserted to St. Louis .   August 18 : George Drouillard returns to camp with Reed and Otos ' Chief Little Thief . Reed is sentenced to run the gauntlet ( 500 lashes ) and is discharged from the permanent party .   August 18 : Captain Meriwether Lewis 's 30th birthday .   August 20 : Sergeant Charles Floyd dies . He dies from bilious chorlick ( ruptured appendix ) . He is the only member lost during the expedition .   August 23 : Pvt . Joseph Field kills first bison .   August 26 : Pvt . Patrick Gass is elected to sergeant . First election in new territory west of Mississippi River . George Shannon is selected to get the horses back from native Americans .   August 30 : A friendly council with the Yankton Sioux held . According to a legend , Lewis wraps a newborn baby in a United States flag and declares him `` an American '' .   September 4 : Reach the mouth of the Niobrara River .   September 7 : The expedition drives a prairie dog out of its den ( by pouring water into it ) to send back to Jefferson .   September 14 : Hunters kill and describe prairie goat ( antelope ) .   September 25 -- 29 : A band of Lakota Sioux demand one of the boats as a toll for moving further upriver . Meet with Teton Sioux . Close order drill , air gun demo , gifts of medals , military coat , hats , tobacco . Hard to communicate language problems . Invite chiefs on board keelboat , give each \u200b ⁄ glass whiskey , acted drunk wanted more . Two armed confrontations with Sioux . Some of the chiefs sleep on boat , move up river to another village , meet in lodge , hold scalp dance .   October 8 -- 11 : Pass Grand River home of the Arikara people , 2,000 + . Joseph Gravelins trader , lived with Arikara for 13 yrs . Pierre Antoine Tabeau lived in another village was from Quebec .   October 13 : Pvt . John Newman tried for | Question: on which river did the exploration of the louisiana purchase begin ?", ['the Missouri River'], 'mrqa_naturalquestions-validation-8948'), tensor(0.1775)), (('Context: Rumpole of the Bailey - TV.comRumpole of the Bailey - Show News, Reviews, Recaps and Photos - TV.com  Rumpole of the Bailey  EDIT  Welcome to the Rumpole of the Bailey guide at TV.com.  Horace Rumpole (played by the late Leo McKern) is an untidy, ageing London barrister with one glass eye who defends in criminal cases. His clients rarely cut elegant figures. He is fond of red wine, poetry, and fair dealing, and is not looked on as a great success in life by his wife, Hilda (\'She Who Must Be Obeyed\'). Rumpole has had a few triumphs, and the Penge Bungalow murders are often on his mind... Rumpole shares Chambers at Number 3, Equity Court, with a mixed group of barrister colleagues, including Guthrie Featherstone (Peter Bowles) and Phyllida Trant (Patricia Hodge). He also takes pupils - notably Fiona Allways (played by Rosalyn Landor) and Liz Probert. The creator and writer of the series, Sir John Mortimer, received an Edgar Allan Poe Award for crime and mystery for Rumpole.  Horace Rumpole - Thrilling DetectiveHorace Rumpole  Horace Rumpole  Created by John Mortimer (1923-2009 )  "Crime doesn\'t pay, but it\'s a living."  Down those mean streets and meaner chambers a man must waddle...  Lord knows, he\'s not a private eye, but God, I wish he were. Like Philip Marlowe , Rumpole is ""a relatively poor man... a common man or he could not go among common people. He has a sense of character, or he would not know his job. He will take no man\'s money dishonestly and no man\'s insolence without a due and dispassionate revenge. He is a lonely man and his pride is that you will treat him as a proud man or be very sorry you ever saw him. He talks as the man of his age talks -- that is, with a rude wit, a lively sense of the grotesque, a disgust for sham, and a contempt for pettiness."  So, may I submit for your consideration, Your Honour....  That great defender of most muddled and sinful humanity...  With his jowls a-quiver...  His fondness for Wordsworth, Chateau Thames Embankment and hopeless cases...  His cheroot-puffing and claret-quaffing...  His food-bespeckled robe and raggedy wig...  And his beloved and tattered copy of The Oxford Book of English Verse clutched to his bosom...  For his oratorical outbursts...  His always entertaining jabs at the soft underbelly of hypocrisy, pomposity and upper class twits...  And for standing up for truth, justice, honour and the Golden Thread of Justice...  May I submit for inclusion, Your Honour, this most British of all lawyers...  This proud, this defiant Old Bailey Hack...  With his best gal, Hilda, She Who Must Be Obeyed, standing, NOT amused, by his side...  The one, the only...  HORACE RUMPOLE.  May there always be an England, and may there always be Horace Rumpole to see that justice be done. And thev pompous may squirm.  Your Honour, I rest my case.  ****  Rumpole is, of course John Mortimer\'s rotund, defiant British criminal lawyer who, as brilliantly brought to life by the late, great Australian actor Leo McKern, the star of Rumpole of the Bailey, the popular British courtroom comedy/drama that originally aired on Thames Television in 1978, and soon became popular on both sides of the Atlantic, appearring on the American Public Broadcasting Service as part of its Mystery! series.  Mortimer wrote each and every episode of the television series, and their subsequent novelizations. The show ran, off and on, for seventeen years, an incredible run, and inspired not just the short stories, but novels and two radio series. It was only in 1995, with the publication of the Rumpole and the Angel of Death collection, that Mortimer began writing original Rumpole stories (ie: not adapted from his own TV scripts). Sice then, two | Question: Who created Rumpole of the Bailey?', ['john mortimer', 'john mortimer qc'], 'mrqa_triviaqa-validation-1551'), tensor(0.1726)), (('Context: Walking in the Air  "Walking in the Air" is a song written by Howard Blake for the 1982 animated film of Raymond Briggs\' 1978 children\'s book "The Snowman".  The song forms the centrepiece of "The Snowman", which has become a seasonal favorite on British and Finnish television.  The story relates the fleeting adventures of a young boy and a snowman who has come to life.  In the second part of the story, the boy and the snowman fly to the North Pole.  "Walking in the Air" is the theme for the journey.  They attend a party of snowmen, at which the boy seems to be the only human until they meet Father Christmas with his reindeer, and the boy is given a scarf with a snowman pattern.  In the film, the song was performed by St Paul\'s Cathedral choirboy Peter Auty, and reissued in 1985 (on Stiff Records) and 1987.   Walking in the Air  "Walking in the Air" is a song written by Howard Blake for the 1982 animated film of Raymond Briggs\' 1978 children\'s book "The Snowman".  The song forms the centrepiece of "The Snowman", which has become a seasonal favorite on British and Finnish television.  The story relates the fleeting adventures of a young boy and a snowman who has come to life.  In the second part of the story, the boy and the snowman fly to the North Pole.  "Walking in the Air" is the theme for the journey.  They attend a party of snowmen, at which the boy seems to be the only human until they meet Father Christmas with his reindeer, and the boy is given a scarf with a snowman pattern.  In the film, the song was performed by St Paul\'s Cathedral choirboy Peter Auty, and reissued in 1985 (on Stiff Records) and 1987.   Peter Auty  Peter Robert Auty (born 4 November 1969) is an English operatic tenor who has worked with most of the major opera companies in Britain and a number of companies in continental Europe. | Question: An English operatic tenor performed "Walking in the Air" for which 1982 animated film?', ['The Snowman'], 'mrqa_hotpotqa-validation-1968'), tensor(0.1651)), (('Context: Br\'er RabbitBr\'er Rabbit, also spelled Bre\'r Rabbit or Brer Rabbit or Bruh Rabbit, is a central figure as Uncle Remus tells stories of the Southern United States. Br\'er Rabbit is a trickster who succeeds by his wits rather than by brawn, provoking authority figures and bending social mores as he sees fit. The Walt Disney Company later adapted this character for its 1946 animated motion picture Song of the South.  Tar-Baby story   In one tale, Br\'er Fox constructs a doll out of a lump of tar and dresses it with some clothes. When Br\'er Rabbit comes along he addresses the Tar-Baby amiably, but receives no response. Br\'er Rabbit becomes offended by what he perceives as the Tar-Baby\'s lack of manners, punches it, and in doing so becomes stuck. The more Br\'er Rabbit punches and kicks the tar "baby" out of rage, the more he gets stuck. When Br\'er Fox reveals himself, the helpless but cunning Br\'er Rabbit pleads, "please, Br\'er Fox, don\'t fling me in dat brier-patch," prompting Fox to do exactly that. As rabbits are at home in thickets, the resourceful Br\'er Rabbit uses the thorns and briers to escape. The story was originally published in Harper\'s Weekly by Robert Roosevelt; years later Joel Chandler Harris included his version of the tale in his Uncle Remus stories.  African origins   The Br\'er Rabbit stories can be traced back to trickster figures in Africa, particularly the hare that figures prominently in the storytelling traditions in West, Central, and Southern Africa. These tales continue to be part of the traditional folklore of numerous peoples throughout those regions. In the Akan traditions of West Africa, the trickster is usually the spider Anansi, though the plots in his tales are often identical with those of stories of Br\'er Rabbit.  However, Anansi does encounter a tricky rabbit called "Adanko" (Asante-Twi to mean "Hare") in some stories. The Jamaican character with the same name "Brer Rabbit", is an adaptation of the Ananse stories of the Akan people.   Some scholars have suggested that in his American incarnation, Br\'er Rabbit represented the enslaved Africans who used their wits to overcome adversity and to exact revenge on their adversaries, the White slave-owners.  Though not always successful, the efforts of Br\'er Rabbit made him a folk hero. However, the trickster is a multi-dimensional character. While he can be a hero, his amoral nature and his lack of any positive restraint can make him into a villain as well.   For both Africans and African Americans, the animal trickster represents an extreme form of behavior that people may be forced to adopt in extreme circumstances in order to survive. The trickster is not to be admired in every situation. He is an example of what to do, but also an example of what not to do. The trickster\'s behavior can be summed up in the common African proverb: "It\'s trouble that makes the monkey chew on hot peppers." In other words, sometimes people must use extreme measures in extreme circumstances.  Several elements in the Brer Rabbit Tar Baby story (e.g., rabbit needing to be taught a lesson, punches and head-butting the rabbit does, the stuck rabbit being swung around and around) are reminiscent of those found in a Zimbabwe-Botswana folktale.   Folklorists in the late 19th century first documented evidence that the American versions of the stories originated among enslaved West Africans based on connections between Br\'er Rabbit and Leuk, a rabbit trickster in Senegalese folklore.  The stories of Br\'er Rabbit were written down by Robert Roosevelt, an uncle of US President Theodore Roosevelt. Theodore Roosevelt wrote in his autobiography about his aunt from the State of Georgia, that "She knew all the \'Br\'er Rabbit\' stories, and I was brought up on them. One of my uncles, Robert Roosevelt, was much struck with them, and took them down from her dictation, publishing them in Harper\'s, where they fell flat. This was a good many years before a genius arose who, in \'Uncle Remus\', made the stories | Question: Who gets Brer Rabbit in a terrible tangle in the stories by Joel Chandler Harris?', ['Tar Baby', 'Tar-Baby', 'tar baby', 'tar "baby'], 'mrqa_triviaqa-validation-1616'), tensor(0.0408)), (('Context: Bodhi Natural Health Products | Bodhi Natural ProductsBodhi Natural Health Products | Bodhi Natural Products  Stockists  Welcome  “It is my wish that by using these products you can enjoy life to the fullest by having good health, hopefully relieve suffering, and of course protecting our beautiful planet by using only natural substances. All Bodhi products come from nature, and are organic”\xa0 Julie Herbison  Bodhi originated as Body Mind Balancing which started off as my business as a Natural Health Practitioner. Over time my products became more and more popular so the name Bodhi came about to keep it short and simple. Bodhi comes from the Bodhi Tree which is a type of fig tree. The Bodhi tree has played an important part in human history featuring strongly in religious history and mythology in various parts of the world.  The ficus religiosa, Bodhi tree, has large heart shaped leaves. It is one of the most sacred trees in India, Sri Lanka and Nepal, where it is venerated by both Hindus and Buddhists. The tree under which Siddhartha Gautama gained enlightenment over 2,600 years ago still grows today in North East India.  In Hindu religion, Vishnu is said to have been born under a Bodhi tree and is often depicted sitting on its heart-shaped leaves. The Bodhi tree is often planted in the grounds of temples and, of all the sacred trees of India, it is the most widely worshiped.  The Bodhi tree in Sri Lanka is located in Anuradhapura and is said to be the oldest tree in the world with a known planting date. This fig tree is said to have grown from a branch taken from the original Bodhi tree in India under which Buddha gained enlightenment.  For Homeopathic and Body Mind consultations please call  Julie Herbison  Bodhi Tree, Bodh Gaya - Sacred SitesBodhi Tree, Bodh Gaya  Bodhi Tree, Bodh Gaya  Buddhist Monks at Bodhi Tree (The site of Buddha\'s enlightenment) \xa0 \xa0\xa0  Bodh Gaya, located 100 km (62 mi) south of Patna in the Indian state of Bihar, is the most venerated sacred place in Buddhism. It is the place where Prince Siddhartha Guatama, while meditating beneath the Bodhi Tree, attained enlightenment and became the Buddha.  Traditional accounts say that, in the early years of the 4th century BC, Siddhartha Gautama saw the suffering of the world and wanted to be free from it. As a young man, following the ancient traditions of Hinduism, he sought out spiritual teachers. Inquiring of their knowledge, he diligently practiced various yogas and meditations. Seven years passed, the last three in extreme asceticism, yet still he had not achieved his goal of enlightenment.  Impression of Buddha feet, Bodh Gaya \xa0\xa0\xa0  Siddhartha then journeyed toward the ancient sacred forests of Uruvela (modern Gaya in Bihar, in north India) with the intention of finally and completely realizing the infinite. Guided by visionary dreams and following in the footsteps of the Buddhas of three previous ages, Krakucchanda, Kanakamuni and Kasyapa (who had each attained enlightenment at the site) Siddhartha sat beneath the Bodhi Tree. Touching the earth, thereby calling it to witness the countless lifetimes of virtue that had led him to this place of enlightenment, he resolved not to rise again until enlightenment was attained.  "Here on this seat my body may shrivel up, my skin, my bones, my flesh may dissolve, but my body will not move from this seat until I have attained Enlightenment, so difficult to obtain in the course of great periods of time".  As Siddhartha sat in deep meditation beneath the Bodhi Tree, Mara, the Dark Lord of Death, came to distract him from his endeavor. When the earth shook, confirming the truth of Gautama\'s words, Mara unleashed his army of demons. In the epic battle that ensued, Siddhartha\'s wisdom broke through Mara’s illusions. The power of his compassion transformed the demons\' weapons into flowers and Mara and all his forces fled. Three days and nights passed and Siddhartha’s intention was realized. He became the Buddha, meaning the ‘Enlightened One’.  The Mahabodhi Temple, Bodh Gaya, India \xa0 \xa0\xa0  The Buddha then spent the | Question: Who is said to have gained enlightenment sitting under the Bodhi Tree?', ['buddha', 'enlightened one'], 'mrqa_triviaqa-validation-3915'), tensor(0.0325)), (("Context: It is an allusion to Samuel Taylor Coleridge 's poem The Rime of the Ancient Mariner ( 1798 ) . In the poem , an albatross starts to follow a ship -- being followed by an albatross was generally considered a sign of good luck . However , the titular mariner shoots the albatross with a crossbow , which is regarded as an act that will curse the ship ( which indeed suffers terrible mishaps ) . Even when they are too thirsty to speak , the ship 's crew let the mariner know through their glances that they blame his action for the curse . The albatross is then literally hung around the mariner 's neck by the crew to symbolize his guilt in killing the bird . Thus , the albatross can be both an omen of good or bad luck , as well as a metaphor for a burden to be carried as penance . | Question: the rime of the ancient mariner albatross symbolism ?", ['a metaphor for a burden to be carried as penance', 'omen of good or bad luck'], 'mrqa_naturalquestions-validation-7017'), tensor(0.0136)), (("Context: Steve Carell as Felonious Gru , the former villain turned Anti-Villain League agent , Margo , Edith , and Agnes ' adoptive father , and Lucy 's husband .   Carell also voices Dru , Gru 's long - lost twin brother and the girls ' adoptive uncle .     Kristen Wiig as Lucy Wilde , an Anti-Villain League agent , Gru 's wife and the girls ' adoptive mother .   Trey Parker as Balthazar Bratt , a supervillain and former child star who grows up to become obsessed with the character he played in the ' 80s and is bent on world domination .   Miranda Cosgrove as Margo , Gru and Lucy 's oldest adoptive daughter .   Dana Gaier as Edith , Gru and Lucy 's middle adoptive daughter .   Nev Scharrel as Agnes , Gru and Lucy 's youngest adoptive daughter . She was previously voiced by Elsie Fisher in the first two films .   Pierre Coffin as Mel and the Minions .   Coffin also voices a Museum Director .     Steve Coogan as Silas Ramsbottom , the director of the Anti-Villain League .   Coogan also voices Fritz , the butler of Dru .     Julie Andrews as Marlena Gru , Gru and Dru 's mother .   Jenny Slate as Valerie Da Vinci , a member of the Anti-Villain League who becomes the brand new AVL director .   Andy Nyman as Clive , a robot who is Bratt 's sidekick .   Adrian Ciscato as Niko , a boy from Freedonia who falls in love with Margo .   John Cygan ( in his final film role ) as an Additional Voice . | Question: who is the cast of despicable me 3 ?", ['Nev Scharrel', 'Andy Nyman', 'Adrian Ciscato', 'Pierre Coffin', 'Steve Coogan', 'Trey Parker', 'Julie Andrews', 'Kristen Wiig', 'John Cygan', 'Jenny Slate', 'Dana Gaier', 'Steve Carell', 'Miranda Cosgrove'], 'mrqa_naturalquestions-validation-3490'), tensor(0.0087)), (('Context: Sunday in the Park with George - Music Theatre InternationalSunday In The Park... | Music Theatre International  Music Theatre International  Request Licenses & Perusals, Pay Invoices  Your Web Profile  We are working quickly to merge our two logins.  Region  Sunday In The Park With George  Original Broadway Version (1984)  Inspired by George Seurat\'s famous painting, this poetic masterpiece explores the challenges in understanding life and art.  Inspired by the painting A Sunday Afternoon on the Island of La Grande Jatte by Georges Seurat, Sunday In The Park With George, Stephen Sondheim and James Lapine\'s stunning masterpiece merges past and present into beautiful, poignant truths about life, love and the creation of art. One of the most acclaimed musicals of our time, this moving study of the enigmatic painter Georges Seurat won a Pulitzer Prize and was nominated for an astounding 10 Tony Awards including Best Musical.  The days leading up to the completion of his most famous painting, A Sunday Afternoon on the Island of La Grande Jatte, Georges Seurat is struggling to make meaningful art and maintaining a relationship with his lover Dot. Amid the scorn of the artistic community, Seurat\'s artistic ability thrives while his love diminishes. A century later, Seurat\'s descendant - named George and also an artist - finds himself burnt out and in search of what artistic path to follow, but he finds the answer to his future in the past.  An ensemble of strong singing actors performs this challenging and heartbreaking work about our need to connect to the past, present and future. Sunday In The Park With George features two coveted starring roles made famous by the Broadway performances of Mandy Patinkin and Bernadette Peters. The show may be staged simply as in its original workshop production or with the grandeur of Seurat\'s masterpiece.  "Sunday in the Park with George" - About.com Education"Sunday in the Park with George"  "Sunday in the Park with George"  "Sunday in the Park with George"  From Canvas to Characters  By Rosalind Flynn  Updated February 27, 2016.  Anyone who reads or plans to attend a performance of Sunday in the Park With George by Stephen Sondheim and James Lapine should first spend some time looking at an image of the this painting by Georges Seurat :\xa0 Its original French title is Un dimanche après-midi à l\'Île de la Grande Jatte – English translation: “ Sunday Afternoon on the Island of la Grande Jatte .” (Click on the painting title to view the online image.)  The first act of Sunday in the Park With George occurs with Seurat the artist creating and perfecting his painting. The creative musical theatre artists who were inspired by this painting drew the other characters in Act One from figures in the painting.  Looking Closely at Art  It would not be surprising to learn that Sondheim and Lapine used their own version of Visual Thinking Strategies to examine this painting that is 10 feet wide and 6 and a half feet tall. Visual Thinking Strategies provide ways to look closely at works of art by asking observers to respond to these three questions:  continue reading below our video  10 Best Universities in the United States  1. What’s going on in this picture?  2. What do you see that\xa0makes you say that?  3. What more can we find?  In fact, it would be excellent to begin a study of this play by engaging students in doing precisely that – looking at an image of “Sunday Afternoon on the Island of la Grande Jatte” and asking them to respond to those questions.  What’s Going On? According to Sondheim and Lapine  The painting’s setting is the island of la Grande Jatte, which is located northwest of Paris in the Seine River. In 1884 (when Seurat began working on his painting), the island was a pastoral place where people could go to get away from city life. The musical’s creative artists pulled their characters from the canvas. Here’s a Who’s Who:  The most prominent figure is the woman in profile on the bottom right hand side. Sondheim and Lapine imagined her as Seurat’s mistress and they | Question: The musical \'Sunday In The Park With George\' was inspired by a painting by which artist?', ['georges seurat', 'seurat'], 'mrqa_triviaqa-validation-5937'), tensor(-0.0026)), (('Context: 67th Tony Awards  The 67th Annual Tony Awards were held June 9, 2013, to recognize achievement in Broadway productions during the 2012–13 season.  The ceremony returned to Radio City Music Hall in New York City, after two years at Beacon Theatre, and was broadcast live on CBS television.  Neil Patrick Harris hosted for the third consecutive year, his fourth time as host.  Awards in four of the eight acting categories, (Best Actress in a Play, Best Actor in a Musical, Best Actress in a Musical, Best Featured Actor in a Play) were given to African-American performers.  Furthermore, it is the second time in Tony history that both directing prizes went to women.  Garry Hynes and Julie Taymor had previously won in 1998. " Kinky Boots" had a season best 13 nominations and 6 awards.  Cyndi Lauper, who wrote the score for "Kinky Boots", is the first solo female winner for Best Original Score.   67th Tony Awards  The 67th Annual Tony Awards were held June 9, 2013, to recognize achievement in Broadway productions during the 2012–13 season.  The ceremony returned to Radio City Music Hall in New York City, after two years at Beacon Theatre, and was broadcast live on CBS television.  Neil Patrick Harris hosted for the third consecutive year, his fourth time as host.  Awards in four of the eight acting categories, (Best Actress in a Play, Best Actor in a Musical, Best Actress in a Musical, Best Featured Actor in a Play) were given to African-American performers.  Furthermore, it is the second time in Tony history that both directing prizes went to women.  Garry Hynes and Julie Taymor had previously won in 1998. " Kinky Boots" had a season best 13 nominations and 6 awards.  Cyndi Lauper, who wrote the score for "Kinky Boots", is the first solo female winner for Best Original Score.   Julie Taymor  Julie Taymor (born December 15, 1952) is an American director of theater, opera and film.  She is best known for directing the stage musical "The Lion King", for which she became the first woman to win the Tony Award for directing a musical, in addition to a Tony Award for Original Costume Design.   Julie Taymor  Julie Taymor (born December 15, 1952) is an American director of theater, opera and film.  She is best known for directing the stage musical "The Lion King", for which she became the first woman to win the Tony Award for directing a musical, in addition to a Tony Award for Original Costume Design. | Question: In what year did the director of The Lion King win the Tony Awards?', ['1998'], 'mrqa_hotpotqa-validation-3241'), tensor(-0.0075)), (('Context: <Ol>  A new ruler unites China , founds a new dynasty , and gains the Mandate of Heaven .   China , under the new dynasty , achieves prosperity .   The population increases .   Corruption becomes rampant in the imperial court , and the empire begins to enter decline and instability .   A natural disaster wipes out farm land . The disaster normally would not have been a problem ; however , together with the Corruption and overpopulation , it causes famine .   The famine causes the population to rebel and a civil war ensues .   The ruler loses the Mandate of Heaven .   The population decreases because of the violence .   China goes through a warring states period .   One state emerges victorious .   The state starts a new empire .   The empire gains the Mandate of Heaven .  </Ol> | Question: in the dynastic cycle what is the right to rule called ?', ['the Mandate of Heaven'], 'mrqa_naturalquestions-validation-6157'), tensor(-0.0171)), (('Context: Although partly functional , weather vanes are generally decorative , often featuring the traditional cockerel design with letters indicating the points of the compass . Other common motifs include ships , arrows and horses . Not all weather vanes have pointers . When the wind is sufficiently strong , the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing . | Question: where does the arrow of a wind vane point ?', ['the direction from which the wind is blowing'], 'mrqa_naturalquestions-validation-9688'), tensor(-0.0222)), (('Context: Concentrated O2 will allow combustion to proceed rapidly and energetically. Steel pipes and storage vessels used to store and transmit both gaseous and liquid oxygen will act as a fuel; and therefore the design and manufacture of O2 systems requires special training to ensure that ignition sources are minimized. The fire that killed the Apollo 1 crew in a launch pad test spread so rapidly because the capsule was pressurized with pure O2 but at slightly more than atmospheric pressure, instead of the 1⁄3 normal pressure that would be used in a mission.[k] | Question: ______ In both liquid and gas form can fastly result in an exlposion. ?', ['oxygen'], 'mrqa_squad-validation-3478'), tensor(-0.0396)), (('Context: It has been claimed that the transmission of the first episode was delayed by ten minutes due to extended news coverage of the assassination of US President John F. Kennedy the previous day; whereas in fact it went out after a delay of eighty seconds. The BBC believed that many viewers had missed this introduction to a new series due to the coverage of the assassination, as well as a series of power blackouts across the country, and they broadcast it again on 30 November 1963, just before episode two. | Question: What other event made the BBC concerned that viewers had not seen the premier of Doctor Who?', ['a series of power blackouts across the country', 'power blackouts'], 'mrqa_squad-validation-7746'), tensor(-0.0543)), (('Context: Evolution of the adaptive immune system occurred in an ancestor of the jawed vertebrates. Many of the classical molecules of the adaptive immune system (e.g., immunoglobulins and T cell receptors) exist only in jawed vertebrates. However, a distinct lymphocyte-derived molecule has been discovered in primitive jawless vertebrates, such as the lamprey and hagfish. These animals possess a large array of molecules called Variable lymphocyte receptors (VLRs) that, like the antigen receptors of jawed vertebrates, are produced from only a small number (one or two) of genes. These molecules are believed to bind pathogenic antigens in a similar way to antibodies, and with the same degree of specificity. | Question: Evolution of what part of the immune system occurred in the evolutionary ancestor of jawed vertebrates?', ['adaptive', 'adaptive immune system', 'the adaptive immune system'], 'mrqa_squad-validation-6677'), tensor(-0.0643)), (('Context: Amphitrite in Greek Mythology | MythographyAmphitrite in Greek Mythology | Mythography  Amphitrite in Greek Mythology  Home | Greek Gods | Free Spirits | Amphitrite  In Greek mythology, Amphitrite was a sea goddess. Sources state that she was the daughter of Nereus and Doris, which makes Amphitrite one of the fifty sea goddesses (or nymphs) who are known collectively as the Nereids. The Greek poet Hesiod lists the names of these sea nymphs in his Theogony:  “To Nereus and Doris of the lovely hair,  the daughter of Okeanos, the stream surrounding the earth,  a host of godly daughters was born in the barren sea:  Proto, Eukrante, Amphitrite, and Sao…”  (Hesiod, Theogony, lines 240-43)  According to the legend, the Olympian Poseidon (who was himself a god of the sea) saw Amphitrite one day and fell in love with the graceful and beautiful goddess. However, the sea nymph initially resisted Poseidon’s advances. But Poseidon was not easily discouraged. He sent his sea creatures to find Amphitrite. A dolphin succeeded in locating the Nereid, and it was this creature - the dolphin - who persuaded her to consider Poseidon’s proposal. Eventually, Amphitrite accepted Poseidon, and the two gods were married.  Amphitrite and Poseidon were together the parents of several children, including the sea god Triton. Here’s Hesiod again:  “From the union of rumbling Poseidon and Amphitrite  came the great Triton, whose might is far-flung,  an awesome god dwelling in a golden house that lies  at the sea’s bottom, near his cherished mother and lordly father.”  (Hesiod, Theogony, lines 930 ff. )  Amphitrite Goddess of the Sea - Greek Gods and GoddessesAmphitrite Goddess of the Sea  [ ? ]Subscribe To This Site  Amphitrite Goddess of the Sea  In Greek mythology, Amphitrite goddess of the Sea was one of the fifty Nereids, that is a daughter of Nereus and Doris - that\'s what Hesiod writes in his Theogony (but Apollodorus says her parents were Oceanus and Thetys). Her Roman equivalent was Salacia, god Neptune\'s wife.  Her name means "The third one who encircles (everything)" (because the ancient Greeks thought the whole world was encircled by a river-god, Oceanus ).  She spent most of the time singing and dancing with her sisters. One day Poseidon, god of the sea , saw her near the island of Naxos and fell in love with her. He kidnapped her and made her his wife.  In another story, Amphitrite, being a shy girl, fled away and hid somewhere in the Atlantic Ocean. Poseidon wanted desperately to find her, so he sent all the marine creatures to look for her. Delphinus, a dolphin, managed to find her and convinced her to go back to Poseidon and marry him. That\'s what made Amphitrite goddess of the sea. (By the way, the dolphin who found her was turned into a constellation by the grateful Poseidon ).  Amphitrite goddess of the sea has no special attributions (but sometimes she can calm the waters), she is just the mistress of the sea (and sometimes she is considered the mother of the marine creatures, but they existed even before she became queen of the sea). Some even say that she is just a personification of the sea.  She also appeared briefly in the story of Theseus. King Minos asked the hero to prove that he really was Poseidon\'s son. A ring was thrown into the sea and Theseus had to bring it back. The sea creatures found the ring and gave it to the hero, while Amphitrite goddess of the sea received him in her underwater palace and gave him a golden crown, which he took to Ariadne as a wedding gift. (As you can see, Amphitrite was the ideal wife, as she was never jealous of her husband\'s love affairs.)  Amphitrite and Poseidon had a son, Triton (a fish-man or a he-mermaid) and a daughter, Rhode, and maybe another daughter, Benthesicyme.  In art | Question: In Greek mythology, Amphitrite, queen of the sea, was the wife of which god?', ['poseidon'], 'mrqa_triviaqa-validation-671'), tensor(-0.0919)), (("Context: Doctor Who has appeared on stage numerous times. In the early 1970s, Trevor Martin played the role in Doctor Who and the Daleks in the Seven Keys to Doomsday. In the late 1980s, Jon Pertwee and Colin Baker both played the Doctor at different times during the run of a play titled Doctor Who – The Ultimate Adventure. For two performances, while Pertwee was ill, David Banks (better known for playing Cybermen) played the Doctor. Other original plays have been staged as amateur productions, with other actors playing the Doctor, while Terry Nation wrote The Curse of the Daleks, a stage play mounted in the late 1960s, but without the Doctor. | Question: What was the name of the Doctor Who play from the 1980's?", ['Doctor Who – The Ultimate Adventure'], 'mrqa_squad-validation-7821'), tensor(-0.1457)), (('Context: Newton came to realize that the effects of gravity might be observed in different ways at larger distances. In particular, Newton determined that the acceleration of the Moon around the Earth could be ascribed to the same force of gravity if the acceleration due to gravity decreased as an inverse square law. Further, Newton realized that the acceleration due to gravity is proportional to the mass of the attracting body. Combining these ideas gives a formula that relates the mass () and the radius () of the Earth to the gravitational acceleration: | Question: How might gravity effects be observed differently according to Newton?', ['at larger distances', 'at larger distances.'], 'mrqa_squad-validation-10322'), tensor(-0.1767)), (('Context: Important production centres today are the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ) , the Yakima ( Washington ) and Willamette ( Oregon ) valleys , and western Canyon County , Idaho ( including the communities of Parma , Wilder , Greenleaf , and Notus ) . The principal production centres in the UK are in Kent ( which produces Kent Goldings hops ) , Herefordshire , and Worcestershire . Essentially all of the harvested hops are used in beer making . | Question: where do they grow hops in the us ?', ['Washington', 'Oregon', 'Idaho'], 'mrqa_naturalquestions-validation-2248'), tensor(-0.1814)), (('Context: Genghis Khan, the title is spelled in variety of ways in different languages such as Mongolian Chinggis Khaan, English Chinghiz, Chinghis, and Chingiz, Chinese: 成吉思汗; pinyin: Chéngjísī Hán, Turkic: Cengiz Han, Çingiz Xan, Çingiz Han, Chingizxon, Çıñğız Xan, Chengez Khan, Chinggis Khan, Chinggis Xaan, Chingis Khan, Jenghis Khan, Chinggis Qan, Djingis Kahn, Russian: Чингисхан (Čingiskhan) or Чингиз-хан (Čingiz-khan), etc. Temüjin is written in Chinese as simplified Chinese: 铁木真; traditional Chinese: 鐵木眞; pinyin: Tiěmùzhēn. | Question: What are alternate English spelling of Genghis?', ['Chinghiz, Chinghis, and Chingiz'], 'mrqa_squad-validation-6303'), tensor(-0.1964)), (("Context: Petra Kvitová career statistics  This is a list of the main career statistics of Czech professional tennis player Petra Kvitová.  To date, Kvitová has won 20 singles titles including two Grand Slam singles titles at the Wimbledon Championships, one WTA Tour Championships singles title, two WTA Premier Mandatory singles titles and four WTA Premier 5 singles titles.  She was also the bronze medallist at the 2016 Rio Olympics, a semifinalist at the 2010 Wimbledon Championships, 2012 Australian Open and 2012 French Open and a quarterfinalist at the 2011 Australian Open, 2012 Wimbledon Championships, 2013 Wimbledon Championships, 2015 US Open and 2017 US Open.  Kvitová reached her career-high ranking of world no. 2 on 31 October 2011.   2017 US Open (tennis)  The 2017 US Open was the 137th edition of tennis' US Open and the fourth and final Grand Slam event of the year.  It was held on outdoor hard courts at the USTA Billie Jean King National Tennis Center in New York City.  Experimental rules featured in qualifying for the main draw as well as in the junior, wheelchair and exhibition events. | Question: What edition of tennis' US Open was the 2017 US Open when Petra Kvitova was a quarterfinalist?", ['137th'], 'mrqa_hotpotqa-validation-409'), tensor(-0.2538)), (("Context: The White Coat Ceremony ( WCC ) is a relatively new ritual in some medical ( MD , DO ) , dental , optometry , audiology , chiropractic , dietetic , occupational therapy , physical therapy , podiatric , pharmacy , physician assistant , pathologists ' assistant , nursing , naturopathic and veterinary schools that marks the student 's transition from the study of preclinical to clinical health sciences . At some schools , where students begin meeting patients early in their education , the white coat ceremony is held before the first year begins . It is an example of a matriculation . | Question: when do you get your white coat in pharmacy school ?", ["the student 's transition from the study of preclinical to clinical health sciences"], 'mrqa_naturalquestions-validation-5465'), tensor(-0.2744)), (('Context: TurbineA turbine (from the Latin turbo, a vortex, related to the Greek , tyrbē, meaning "turbulence"),   is a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work. A turbine is a turbomachine with at least one moving part called a rotor assembly, which is a shaft or drum with blades attached. Moving fluid acts on the blades so that they move and impart rotational energy to the rotor. Early turbine examples are windmills and waterwheels.  Gas, steam, and water turbines have a casing around the blades that contains and controls the working fluid. Credit for invention of the steam turbine is given both to the British engineer Sir Charles Parsons (1854–1931), for invention of the reaction turbine and to Swedish engineer Gustaf de Laval (1845–1913), for invention of the impulse turbine. Modern steam turbines frequently employ both reaction and impulse in the same unit, typically varying the degree of reaction and impulse from the blade root to its periphery.  The word "turbine" was coined in 1822 by the French mining engineer Claude Burdin from the Latin turbo, or vortex, in a memo, "Des turbines hydrauliques ou machines rotatoires à grande vitesse", which he submitted to the Académie royale des sciences in Paris.  Benoit Fourneyron, a former student of Claude Burdin, built the first practical water turbine.   Operation theory   A working fluid contains potential energy (pressure head) and kinetic energy (velocity head). The fluid may be compressible or incompressible. Several physical principles are employed by turbines to collect this energy:  Impulse turbines change the direction of flow of a high velocity fluid or gas jet. The resulting impulse spins the turbine and leaves the fluid flow with diminished kinetic energy. There is no pressure change of the fluid or gas in the turbine blades (the moving blades), as in the case of a steam or gas turbine, all the pressure drop takes place in the stationary blades (the nozzles). Before reaching the turbine, the fluid\'s pressure head is changed to velocity head by accelerating the fluid with a nozzle.  Pelton wheels and de Laval turbines use this process exclusively. Impulse turbines do not require a pressure casement around the rotor since the fluid jet is created by the nozzle prior to reaching the blades on the rotor. Newton\'s second law describes the transfer of energy for impulse turbines.  Reaction turbines develop torque by reacting to the gas or fluid\'s pressure or mass.  The pressure of the gas or fluid changes as it passes through the turbine rotor blades. A pressure casement is needed to contain the working fluid as it acts on the turbine stage(s) or the turbine must be fully immersed in the fluid flow (such as with wind turbines). The casing contains and directs the working fluid and, for water turbines, maintains the suction imparted by the draft tube. Francis turbines and most steam turbines use this concept. For compressible working fluids, multiple turbine stages are usually used to harness the expanding gas efficiently. Newton\'s third law describes the transfer of energy for reaction turbines.  In the case of steam turbines, such as would be used for marine applications or for land-based electricity generation, a Parsons type reaction turbine would require approximately double the number of blade rows as a de Laval type impulse turbine, for the same degree of thermal energy conversion. Whilst this makes the Parsons turbine much longer and heavier, the overall efficiency of a reaction turbine is slightly higher than the equivalent impulse turbine for the same thermal energy conversion.  In practice, modern turbine designs use both reaction and impulse concepts to varying degrees whenever possible. Wind turbines use an airfoil to generate a reaction lift from the moving fluid and impart it to the rotor. Wind turbines also gain some energy from the impulse of the wind, by deflecting it at an angle. Turbines with multiple stages may utilize either reaction or impulse blading at high pressure. Steam turbines were traditionally more impulse but continue to move towards reaction designs similar to those used in gas turbines. At low pressure the operating fluid medium expands in volume for small reductions in | Question: What main category of machines \'produce continuous power from a wheel/rotor, usually with vanes, revolved by fast flowing fluid\'?', ['turbine engines', 'reaction turbine', 'turbines', 'turbine', 'turbine blades', 'impulse turbine'], 'mrqa_triviaqa-validation-2376'), tensor(-0.3587)), (('Context: The Soulages collection of Italian and French Renaissance objects was acquired between 1859 and 1865, and includes several cassone. The John Jones Collection of French 18th-century art and furnishings was left to the museum in 1882, then valued at £250,000. One of the most important pieces in this collection is a marquetry commode by the ébéniste Jean Henri Riesener dated c1780. Other signed pieces of furniture in the collection include a bureau by Jean-François Oeben, a pair of pedestals with inlaid brass work by André Charles Boulle, a commode by Bernard Vanrisamburgh and a work-table by Martin Carlin. Other 18th-century ébénistes represented in the Museum collection include Adam Weisweiler, David Roentgen, Gilles Joubert & Pierre Langlois. In 1901, Sir George Donaldson donated several pieces of art Nouveau furniture to the museum, which he had acquired the previous year at the Paris Exposition Universelle. This was criticized at the time, with the result that the museum ceased to collect contemporary items and did not do so again until the 1960s. In 1986 the Lady Abingdon collection of French Empire furniture was bequeathed by Mrs T. R. P. Hole. | Question: What items comprise the John Jones Collection?', ['art and furnishings', 'French 18th-century art and furnishings'], 'mrqa_squad-validation-5622'), tensor(-0.6079)), (('Context: Pi Day is an annual celebration of the mathematical constant π ( pi ) . Pi Day is observed on March 14 ( 3 / 14 in the month / day format ) since 3 , 1 , and 4 are the first three significant digits of π . In 2009 , the United States House of Representatives supported the designation of Pi Day . | Question: how long have we been celebrating pi day ?', ['2009'], 'mrqa_naturalquestions-validation-3028'), tensor(-0.7106)), (('Context: What is a Mulligan in Golf? - About.com SportsWhat is a Mulligan in Golf?  What Is a \'Mulligan\' in Golf?  What Is a \'Mulligan\' in Golf?  This golfer looks like he needed a mulligan.\xa0 John Cumming/Photodisc/Getty Images  By Brent Kelley  Updated February 05, 2016.  A mulligan, most simply put, is a "do-over" in golf. Hit a bad shot? Take a mulligan and replay that stroke . Drop a ball on the spot from which you just played, and re-play. The first (bad) shot is not counted.  Are Mulligans \'Legal\'?  No. There is never a time, when playing under the\xa0 Rules of Golf , that a mulligan is "legal." Mulligans are not allowed under the rules.  But Mulligans Are Very Popular  But just because mulligans aren\'t "legal" doesn\'t mean their use isn\'t common and popular among recreational golfers. Many amateurs and recreational golfers - players just out to have a fun round with buddies, as opposed to serious golfers looking for competition - are lax in observing the rules anyway.  So mulligans are most often employed during friendly rounds by golf buddies; or during charity or playday tournaments where mulligans are sometimes sold. If mulligans are for sale at a charity tournament, that means the golfer can buy, say, three mulligans for a set price each.  continue reading below our video  What Are Mulligans?  The sale of mulligans is sometimes used as an additional fund-raiser at charitable events.  Common Ways of Using Mulligans  Do all golfers use mulligans in the same way? No - whatever a group of golfers agrees upon is what counts (unless you are using mulligans in something like a charity tournament or association outing setting - then do what the organizers tell you).  How many mulligans you get to use during a round, what types of shots you can use them for, and so on, are things that differ from golfer to golfer. So if you\'re playing with a new group and mulligans are in effect, you need to clarify what is allowed.  Here are some common ways that mulligans are used:  Some golfers limit the use of mulligans to the first tee only, or to the first and 10th tees only.  Some golfers use one mulligan per nine holes, but anywhere on each nine.  It\'s most common for mulligans to be used only off the tee , i.e., you can only use a mulligan to replay a drive. However, some groups allow mulligans from the fairway , too.  Less common is allowing mulligans from the rough or out of hazards, but some golfers even do that.  It is rarer still - rarely seen, in fact - for mulligans to be used on the putting green .  And some groups allow mulligans from just about anywhere on the golf course, but set a limit - say, three mulligans per round, or nine, or 18.  Clearly, there are many different ways that golfers use mulligans. If you have a regular group of friends you play with, and your group allows mulligans, you probably long ago settled into your own "rules" about using them.  Note that mulligans are common in the United States, but rarely seen in the UK, for example. As with all localized golf customs, formats and betting games , when playing with golfers you don\'t know clear up the ground rules before play starts to avoid possible confusion later.  And again, please note: If you are playing in a tournament, handicap round or other setting in which the Rules of Golf are strictly followed, you cannot play mulligans.  Why Is It Called a \'Mulligan\'?  Good question! And the fact is, nobody knows for sure how a do-over in golf came to be called a mulligan. There are multiple theories, however, and we go over some of them in our FAQ on the topic:  Why are mulligans called that?  Other Terms/Uses  Mulligans | Question: In which sport may a \'Mulligan\' be awarded?', ['golf', 'golfer'], 'mrqa_triviaqa-validation-5972'), tensor(-1.0603)), (("Context: Prague: Six things you must do in the Czech capital city ...Prague: Six things you must do in the Czech capital city | Daily Mail Online  Prague: Six things you must do in the Czech capital city  comments  Prague is one of Europe's architectural gems, with more knockout picture-postcard views than some entire countries. Unsurprisingly, the Czech capital can also get busy as thousands of tourists jostle for space.  Here is a selection of things to do during a visit to the city - from taking in its best-loved sights to stopping for a drink at a beautifully restored cafe and watching a comic take on a famous Mozart opera.  1... New Jerusalem  Emperor Charles IV planned this intensely beautiful city as the ‘New Jerusalem’ in the 14th Century, and architects maintained the vision until its last golden age in the Thirties. The old centre, with outstanding buildings such as Valdstejn Palace, St James’ Church and St Vitus Cathedral, is still largely intact after 800 years and is now a World Heritage Site.  With nifty footwork you can outwit the crowds. Try walking the famous Charles Bridge before breakfast, and save the striking of the Astronomical Clock in the Old Town Square until the evening when it’s quieter. I also like the peacefulness of Mala Strana, the cafe-lined cobbled streets under Prague Castle. And take a tram to the Jewish Quarter and wander Petrin Hill for wonderful views over the city’s baroque roofs.  Atmospheric: Prague's beautiful Charles Bridge  2... Grand re-opening  How about this for a comeback? The Grand Cafe Orient ( www.grandcafeorient.cz ) opened in 1912 in the House of the Black Madonna, Prague’s architectural take on cubism, the art form Picasso championed. The cafe closed eight years later when cubism fell out of favour but reopened in 2005, faithfully recreated with period furniture and fittings. Try the apple strudel with vanilla ice cream.  Another venue, the Mysak pastry shop, reopened in 2008 inside the Mysak Gallery ( www.gallerymysak.cz ), with original doors, mosaics floors, a marble staircase and wooden alcoves. The signature dish is karamelovy pohar – ice cream topped with caramel, chocolate and walnuts.  RELATED ARTICLES  Share  3...Fighting for freedom  After the euphoria of spring 1968, and Czechoslovakia’s short-lived break for freedom from the Soviet Union, came deep gloom that summer when Russian tanks rolled in to consign this proud nation to 20 more years of oppression.  The Museum of Communism ( www.muzeumkomunismu.cz ) tells the story of those lost years. I came closest to Prague’s tragedy and eventual triumph on the steps of the National Museum in Wenceslas Square. I looked down on the bronze cross at the spot where Jan Palach burnt himself to death in protest in 1969. It's also where people celebrated their new-found freedom after 1989’s so-called Velvet Revolution.  Historic: The Duke of Bohemia is represented by an equestrian statue in Wenceslas Square  4...Recovered treasures  In 1989, William Lobkowicz returned to Prague from exile in Boston to restore the great estate his family had built up over 700 years. It had been confiscated first by the Nazis, then by the communists. Diligent legal teams gradually recovered land, thousands of books, paintings and pieces of furniture. William’s mission culminated in the opening of Lobkowicz Palace in 2007, with its exhibition of the best of the family’s reclaimed treasures. Star exhibits include two views of the River Thames by Canaletto and Haymaking by Pieter Bruegel the Elder.  Another of the city’s new attractions is totally different. The Kafka Museum ( www.kafkamuseum.cz ) celebrates the writer (he was born here) whose name stands for the way the ordinary man is defeated by ‘the system’.  5...Pulling the strings  With his mischievous sense of fun, Mozart would have approved of the National Marionette Theatre’s comic performances of his opera Don Giovanni ( www.mozart.cz ). I particularly loved the puppet cat hissing at an anguished diva in mid aria. Don Giovanni is Prague’s top claim to cultural fame – in 1787, Mozart staged its world premiere at the exquisite opera | Question: By what name do Czechs know their capital city, Prague?", ['prague'], 'mrqa_triviaqa-validation-6351'), tensor(-1.1107))]
09/23/2021 13:07:16 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-4185', 'mrqa_triviaqa-validation-4054', 'mrqa_squad-validation-392', 'mrqa_triviaqa-validation-7369', 'mrqa_naturalquestions-validation-2730', 'mrqa_hotpotqa-validation-2970', 'mrqa_naturalquestions-validation-8948', 'mrqa_triviaqa-validation-1551', 'mrqa_hotpotqa-validation-1968', 'mrqa_triviaqa-validation-1616', 'mrqa_triviaqa-validation-3915', 'mrqa_naturalquestions-validation-7017', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-5937', 'mrqa_hotpotqa-validation-3241', 'mrqa_naturalquestions-validation-6157', 'mrqa_naturalquestions-validation-9688', 'mrqa_squad-validation-3478', 'mrqa_squad-validation-7746', 'mrqa_squad-validation-6677', 'mrqa_triviaqa-validation-671', 'mrqa_squad-validation-7821', 'mrqa_squad-validation-10322', 'mrqa_naturalquestions-validation-2248', 'mrqa_squad-validation-6303', 'mrqa_hotpotqa-validation-409', 'mrqa_naturalquestions-validation-5465', 'mrqa_triviaqa-validation-2376', 'mrqa_squad-validation-5622', 'mrqa_naturalquestions-validation-3028', 'mrqa_triviaqa-validation-5972', 'mrqa_triviaqa-validation-6351']
09/23/2021 13:07:16 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:07:16 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 1
09/23/2021 13:07:28 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:07:28 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 1
09/23/2021 13:07:31 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:07:31 - INFO - __main__ - Instant Fixing Rate: 0.9615384615384616
09/23/2021 13:07:31 - INFO - __main__ - Instant Retention Rate: 0.33333333277777777
09/23/2021 13:07:33 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_002.pt.
09/23/2021 13:07:33 - INFO - __main__ - Saving the current error examples (len=26) to the memory.
09/23/2021 13:07:33 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:07:33 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:07:33 - INFO - __main__ - Finished.
09/23/2021 13:07:33 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:07:33 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:07:33 - INFO - __main__ - Evaluating to get errors .... Timecode: 2
09/23/2021 13:07:36 - INFO - __main__ - Before Error Fixing: 0.0625
09/23/2021 13:07:36 - INFO - __main__ - Found 30 errors.
09/23/2021 13:07:36 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:07:36 - INFO - __main__ - Current memory size: 58.
09/23/2021 13:07:36 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=58;
09/23/2021 13:07:39 - INFO - __main__ - interference_scores=[(('Context: TurbineA turbine (from the Latin turbo, a vortex, related to the Greek , tyrbē, meaning "turbulence"),   is a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work. A turbine is a turbomachine with at least one moving part called a rotor assembly, which is a shaft or drum with blades attached. Moving fluid acts on the blades so that they move and impart rotational energy to the rotor. Early turbine examples are windmills and waterwheels.  Gas, steam, and water turbines have a casing around the blades that contains and controls the working fluid. Credit for invention of the steam turbine is given both to the British engineer Sir Charles Parsons (1854–1931), for invention of the reaction turbine and to Swedish engineer Gustaf de Laval (1845–1913), for invention of the impulse turbine. Modern steam turbines frequently employ both reaction and impulse in the same unit, typically varying the degree of reaction and impulse from the blade root to its periphery.  The word "turbine" was coined in 1822 by the French mining engineer Claude Burdin from the Latin turbo, or vortex, in a memo, "Des turbines hydrauliques ou machines rotatoires à grande vitesse", which he submitted to the Académie royale des sciences in Paris.  Benoit Fourneyron, a former student of Claude Burdin, built the first practical water turbine.   Operation theory   A working fluid contains potential energy (pressure head) and kinetic energy (velocity head). The fluid may be compressible or incompressible. Several physical principles are employed by turbines to collect this energy:  Impulse turbines change the direction of flow of a high velocity fluid or gas jet. The resulting impulse spins the turbine and leaves the fluid flow with diminished kinetic energy. There is no pressure change of the fluid or gas in the turbine blades (the moving blades), as in the case of a steam or gas turbine, all the pressure drop takes place in the stationary blades (the nozzles). Before reaching the turbine, the fluid\'s pressure head is changed to velocity head by accelerating the fluid with a nozzle.  Pelton wheels and de Laval turbines use this process exclusively. Impulse turbines do not require a pressure casement around the rotor since the fluid jet is created by the nozzle prior to reaching the blades on the rotor. Newton\'s second law describes the transfer of energy for impulse turbines.  Reaction turbines develop torque by reacting to the gas or fluid\'s pressure or mass.  The pressure of the gas or fluid changes as it passes through the turbine rotor blades. A pressure casement is needed to contain the working fluid as it acts on the turbine stage(s) or the turbine must be fully immersed in the fluid flow (such as with wind turbines). The casing contains and directs the working fluid and, for water turbines, maintains the suction imparted by the draft tube. Francis turbines and most steam turbines use this concept. For compressible working fluids, multiple turbine stages are usually used to harness the expanding gas efficiently. Newton\'s third law describes the transfer of energy for reaction turbines.  In the case of steam turbines, such as would be used for marine applications or for land-based electricity generation, a Parsons type reaction turbine would require approximately double the number of blade rows as a de Laval type impulse turbine, for the same degree of thermal energy conversion. Whilst this makes the Parsons turbine much longer and heavier, the overall efficiency of a reaction turbine is slightly higher than the equivalent impulse turbine for the same thermal energy conversion.  In practice, modern turbine designs use both reaction and impulse concepts to varying degrees whenever possible. Wind turbines use an airfoil to generate a reaction lift from the moving fluid and impart it to the rotor. Wind turbines also gain some energy from the impulse of the wind, by deflecting it at an angle. Turbines with multiple stages may utilize either reaction or impulse blading at high pressure. Steam turbines were traditionally more impulse but continue to move towards reaction designs similar to those used in gas turbines. At low pressure the operating fluid medium expands in volume for small reductions in | Question: What main category of machines \'produce continuous power from a wheel/rotor, usually with vanes, revolved by fast flowing fluid\'?', ['turbine engines', 'reaction turbine', 'turbines', 'turbine', 'turbine blades', 'impulse turbine'], 'mrqa_triviaqa-validation-2376'), tensor(1.5011)), (('Context: Important production centres today are the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ) , the Yakima ( Washington ) and Willamette ( Oregon ) valleys , and western Canyon County , Idaho ( including the communities of Parma , Wilder , Greenleaf , and Notus ) . The principal production centres in the UK are in Kent ( which produces Kent Goldings hops ) , Herefordshire , and Worcestershire . Essentially all of the harvested hops are used in beer making . | Question: where do they grow hops in the us ?', ['Washington', 'Oregon', 'Idaho'], 'mrqa_naturalquestions-validation-2248'), tensor(1.0201)), (('Context: Walking in the Air  "Walking in the Air" is a song written by Howard Blake for the 1982 animated film of Raymond Briggs\' 1978 children\'s book "The Snowman".  The song forms the centrepiece of "The Snowman", which has become a seasonal favorite on British and Finnish television.  The story relates the fleeting adventures of a young boy and a snowman who has come to life.  In the second part of the story, the boy and the snowman fly to the North Pole.  "Walking in the Air" is the theme for the journey.  They attend a party of snowmen, at which the boy seems to be the only human until they meet Father Christmas with his reindeer, and the boy is given a scarf with a snowman pattern.  In the film, the song was performed by St Paul\'s Cathedral choirboy Peter Auty, and reissued in 1985 (on Stiff Records) and 1987.   Walking in the Air  "Walking in the Air" is a song written by Howard Blake for the 1982 animated film of Raymond Briggs\' 1978 children\'s book "The Snowman".  The song forms the centrepiece of "The Snowman", which has become a seasonal favorite on British and Finnish television.  The story relates the fleeting adventures of a young boy and a snowman who has come to life.  In the second part of the story, the boy and the snowman fly to the North Pole.  "Walking in the Air" is the theme for the journey.  They attend a party of snowmen, at which the boy seems to be the only human until they meet Father Christmas with his reindeer, and the boy is given a scarf with a snowman pattern.  In the film, the song was performed by St Paul\'s Cathedral choirboy Peter Auty, and reissued in 1985 (on Stiff Records) and 1987.   Peter Auty  Peter Robert Auty (born 4 November 1969) is an English operatic tenor who has worked with most of the major opera companies in Britain and a number of companies in continental Europe. | Question: An English operatic tenor performed "Walking in the Air" for which 1982 animated film?', ['The Snowman'], 'mrqa_hotpotqa-validation-1968'), tensor(0.7486)), (("Context: BLACK DOG: Mitchell - now it's snobgate | Daily Mail OnlineBLACK DOG: Mitchell - now it's snobgate | Daily Mail Online  BLACK DOG: Mitchell - now it's snobgate  comments  As revealed in these columns before his downfall in the 'Plebgate' scandal, Tory MP Andrew Mitchell was known as 'Thrasher' in his days as head boy of Rugby School. Dog is now intrigued to learn he had a second nickname: 'Mitchell-snob'.  A fellow Old Rugbeian observes drily: 'It had the advantage that you could drop the hyphen and insert an apostrophe between the 's' and 'n' of snob.'  Tory MP Andrew Mitchell was known as 'Thrasher' in his days as head boy of Rugby School  When Mitchell ran Tory pal David Davis's Conservative leadership campaign in 2005, political broadcaster Michael Cockerell asked Mitchell: 'What do you say to those who argue Davis is Iain Duncan Smith with hair?'\xa0  Adopting a pantomime-villain glower, Mitchell hissed: 'Tell me their names and I'll sue.' How he must wish he had only joked about suing The Sun over PC Toby Rowland.  Hot 'babe' Pepi gets hooked on Vegas\xa0  Ex MoD official Pepi Simpson sashayed in to hear a husky American accent drawl in her direction: 'That's what I call a babe'  A chance visit to a high rollers' bar in a Las Vegas hotel while on holiday with a girlfriend had unintended consequences for Pepi Simpson, sporty wife of Tory MP Keith Simpson. Ex-MoD official Pepi, pictured in the bar, dolled up after a visit to the crimper, sashayed in to hear a husky American accent drawl in her direction: 'That's what I call a babe!' No one told her it was a hookers' pick-up joint. Pepi replied tartly: 'Not bad for 61.'  Hats off to gutsy Andrew Marr, who has thrown away his walking stick nearly two years after a debilitating stroke. In between hosting his Sunday morning TV show and gruelling physiotherapy sessions that set him on the road to recovery, Marr has just completed a second novel and is working on a poetry project. Dog would like to know what he has for breakfast.  On hearing that Alex Salmond was among guests at the Spectator magazine Parliamentary Awards lunch at The Savoy, veteran Tory Norman Tebbit mused he had always wanted to meet the now ex-SNP leader. When a fellow guest asked why, out came 'Chingford Skinhead' Tebbit's razor: 'I've always wanted to sprinkle salt over him to see if he dissolves into slime like the slugs in my garden.'  Theresa May reveals all\xa0  The Spectator's attempt to keep guests in suspense over the winner of the Parliamentarian of the Year award was ruined when Theresa May opened proceedings with a jokey video presentation and pressed the wrong button on her remote control.\xa0  Up flashed the winner's name: Edward VII lookalike and former Commons Clerk Sir Robert Rogers, who was forced to quit after falling out with Speaker John Bercow.  It seems John Bercow has paid a heavy price for falling out with Sir Robert. David Cameron rubbed his nose in it by giving Sir Robert a peerage – and there's worse to come. Another of Bercow's tormentors, former Speaker Betty Boothroyd, is tipped to be at Sir Robert's side as one of his two 'sponsors' when he takes his seat in the Upper House.\xa0  \xa0  The cuddly side of the Chingford Skinhead - Daily Mail OnlineNorman Tebbit's as savage as ever except about wife and the dog who inspired new book | Daily Mail Online  The cuddly side of the Chingford Skinhead: Cameron? Silly. Blair? Offensive. Cherie? Urggh. Norman Tebbit's as savage as ever - except about the wife he adores and the dog who inspired his startling new book  Norman Tebbit has published a children's book about a dog called Ben  The former Minister on Maria Miller: 'It offends against common sense'  On Farage: 'A clever operator. He had touched on | Question: Which ex-politician, now aged 80, had the nickname 'The Chingford Skinhead' ?", ['norman tebbit'], 'mrqa_triviaqa-validation-1924'), tensor(0.5982)), (('Context: Rumpole of the Bailey - TV.comRumpole of the Bailey - Show News, Reviews, Recaps and Photos - TV.com  Rumpole of the Bailey  EDIT  Welcome to the Rumpole of the Bailey guide at TV.com.  Horace Rumpole (played by the late Leo McKern) is an untidy, ageing London barrister with one glass eye who defends in criminal cases. His clients rarely cut elegant figures. He is fond of red wine, poetry, and fair dealing, and is not looked on as a great success in life by his wife, Hilda (\'She Who Must Be Obeyed\'). Rumpole has had a few triumphs, and the Penge Bungalow murders are often on his mind... Rumpole shares Chambers at Number 3, Equity Court, with a mixed group of barrister colleagues, including Guthrie Featherstone (Peter Bowles) and Phyllida Trant (Patricia Hodge). He also takes pupils - notably Fiona Allways (played by Rosalyn Landor) and Liz Probert. The creator and writer of the series, Sir John Mortimer, received an Edgar Allan Poe Award for crime and mystery for Rumpole.  Horace Rumpole - Thrilling DetectiveHorace Rumpole  Horace Rumpole  Created by John Mortimer (1923-2009 )  "Crime doesn\'t pay, but it\'s a living."  Down those mean streets and meaner chambers a man must waddle...  Lord knows, he\'s not a private eye, but God, I wish he were. Like Philip Marlowe , Rumpole is ""a relatively poor man... a common man or he could not go among common people. He has a sense of character, or he would not know his job. He will take no man\'s money dishonestly and no man\'s insolence without a due and dispassionate revenge. He is a lonely man and his pride is that you will treat him as a proud man or be very sorry you ever saw him. He talks as the man of his age talks -- that is, with a rude wit, a lively sense of the grotesque, a disgust for sham, and a contempt for pettiness."  So, may I submit for your consideration, Your Honour....  That great defender of most muddled and sinful humanity...  With his jowls a-quiver...  His fondness for Wordsworth, Chateau Thames Embankment and hopeless cases...  His cheroot-puffing and claret-quaffing...  His food-bespeckled robe and raggedy wig...  And his beloved and tattered copy of The Oxford Book of English Verse clutched to his bosom...  For his oratorical outbursts...  His always entertaining jabs at the soft underbelly of hypocrisy, pomposity and upper class twits...  And for standing up for truth, justice, honour and the Golden Thread of Justice...  May I submit for inclusion, Your Honour, this most British of all lawyers...  This proud, this defiant Old Bailey Hack...  With his best gal, Hilda, She Who Must Be Obeyed, standing, NOT amused, by his side...  The one, the only...  HORACE RUMPOLE.  May there always be an England, and may there always be Horace Rumpole to see that justice be done. And thev pompous may squirm.  Your Honour, I rest my case.  ****  Rumpole is, of course John Mortimer\'s rotund, defiant British criminal lawyer who, as brilliantly brought to life by the late, great Australian actor Leo McKern, the star of Rumpole of the Bailey, the popular British courtroom comedy/drama that originally aired on Thames Television in 1978, and soon became popular on both sides of the Atlantic, appearring on the American Public Broadcasting Service as part of its Mystery! series.  Mortimer wrote each and every episode of the television series, and their subsequent novelizations. The show ran, off and on, for seventeen years, an incredible run, and inspired not just the short stories, but novels and two radio series. It was only in 1995, with the publication of the Rumpole and the Angel of Death collection, that Mortimer began writing original Rumpole stories (ie: not adapted from his own TV scripts). Sice then, two | Question: Who created Rumpole of the Bailey?', ['john mortimer', 'john mortimer qc'], 'mrqa_triviaqa-validation-1551'), tensor(0.5328)), (('Context: An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses . The same principle is used for funicular railways with two connected railway cars on inclined tracks , and for the elevators on the Eiffel Tower which counterbalance each other . Ski lifts are another example , where the gondolas move on a closed ( continuous ) pulley system up and down the mountain . The ski lift is similar to the counter-weighted elevator , but with a constraining force provided by the cable in the vertical dimension thereby achieving work in both the horizontal and vertical dimensions . Boat lifts are another type of counter-weighted elevator system approximating an Atwood machine . | Question: what is a real world application of an atwood machine ?', ['An elevator with a counterbalance'], 'mrqa_naturalquestions-validation-2730'), tensor(0.3998)), (('Context: Chun Woo-hee  Chun Woo-hee (born April 20, 1987) is a South Korean actress.  She made her acting debut in 2004, but first drew attention with her supporting role as a rebellious teenager in the 2011 box-office hit "Sunny".  In 2014, Chun received domestic and international critical acclaim for her first leading role as the title character in "Han Gong-ju", a coming-of-age indie about a traumatized young woman trying to move on with her life after a tragedy.  Her other notable films include "The Beauty Inside" (2015), "Love, Lies" (2016) and "The Wailing" (2016).   Chun Woo-hee  Chun Woo-hee (born April 20, 1987) is a South Korean actress.  She made her acting debut in 2004, but first drew attention with her supporting role as a rebellious teenager in the 2011 box-office hit "Sunny".  In 2014, Chun received domestic and international critical acclaim for her first leading role as the title character in "Han Gong-ju", a coming-of-age indie about a traumatized young woman trying to move on with her life after a tragedy.  Her other notable films include "The Beauty Inside" (2015), "Love, Lies" (2016) and "The Wailing" (2016).   The Wailing (film)  The Wailing () is a 2016 South Korean horror film directed by Na Hong-jin about a policeman who investigates a series of mysterious killings and illnesses.  It was a commercial success. | Question: Chun Woo-hee\'s notable films include a South Korean horror film about  a policeman who investigates a series of mysterious killings and what?', ['illnesses'], 'mrqa_hotpotqa-validation-5325'), tensor(0.3969)), (('Context: David Perdue\'s Charles Dickens Page - David CopperfieldDavid Perdue\'s Charles Dickens Page - David Copperfield  David Copperfield  FAQ & Email  Dickens began writing an autobiography        in the late 1840s which he shared with his friend and future biographer, John Forster . Dickens found the        writing too painful and burned what he had written. He opted instead to        work his story into the fictional account of David Copperfield.  In the novel Dickens\' painful memories of being taken from school to work        at Warren\'s Blacking Factory while his father is in prison for debt are told through David\'s account        of Murdstone and Grinby\'s warehouse. The financial troubles of the Micawbers ,        with whom David was boarding at        the time, mirror Dickens\' parents, John and Elizabeth Dickens,        financial difficulties.  When David is asked by Mrs Micawber to take some of their treasured possessions to the pawn shop to help meet        their obligations, Dickens is recalling painful memories of having to pawn        off the very books he read and treasured as a child to ease his family\'s        financial woes.  On Dickens\' death Forster wrote The        Life of Charles Dickens , which is still the definitive biography        of Dickens, although many of the more negative aspects of Dickens life are        glossed over or missing altogether. Forster\'s biography included the autobiographical        fragment Dickens had given him. This was the first the public knew of Dickens\'        difficult childhood that had so heavily shaped his early work.  Oops...  Dickens originally introduced the        character of the dwarf, Miss Mowcher ,        as an aid to Steerforth\'s plan        to elope with Emily . Mrs. Jane Seymour        Hill, Dickens\' wife Catherine\'s chiropodist, recognized herself as the original for this character and threatened        a lawsuit . Dickens changed the character, in later monthly installments        of the novel, to an honest friend who abhors Steerforth\'s actions. She later        assists in the capture of Littimer .  Like Dickens, David teaches himself        shorthand and becomes a parliamentary reporter. David laments on the difficulties        encountered mastering this art:  "I bought an approved scheme of the noble art and mystery of stenography        (which cost me ten and sixpence); and plunged into a sea of perplexity that        brought me, in a few weeks, to the confines of distraction. The changes        that were rung upon dots, which in such a position meant such a thing, and        in such another position something else, entirely different; the wonderful        vagaries that were played by circles; the unaccountable consequences that        resulted from marks like flies\' legs; the tremendous effects of a curve        in a wrong place; not only troubled my waking hours, but reappeared before        me in my sleep."  Dickens hints at his feelings for politics when David says of his parliamentary        reporting:  "Britannia, that unfortunate female, is always before me, like a trussed        fowl: skewered through and through with office-pens, and bound hand and        foot with red tape. I am sufficiently behind the scenes to know the worth        of political life. I am quite an Infidel about it, and shall never be converted."  During the writing of David Copperfield        Dickens was actively involved in the day-to-day operation of Urania        Cottage , a home for homeless women, which he administered on behalf        of his friend, philanthropist Angela        Burdett Coutts . The home helped to separate homeless, and "fallen",        women from previous lifestyles, educate them in the execution of household        duties and self-discipline, and then help them emigrate to Australia to        begin new lives.  In David Copperfield, Dickens has several of the major characters emigrate        to Australia: the Micawbers , Mr.        Peggotty , Emily , Martha, and Mr.        Mell . Each of these characters are successful in beginning a new life        in the English colony.  Did Dickens visit the birthplace of David Copperfield? Visit the modern Blundeston where the debate rages and David Copperfield    lives on.  Dickens lampoons "the separate system" used at the | Question: What colour hair did Charles Dickens\' character David Copperfield have?', ['red'], 'mrqa_triviaqa-validation-1437'), tensor(0.2218)), (('Context: What is a Mulligan in Golf? - About.com SportsWhat is a Mulligan in Golf?  What Is a \'Mulligan\' in Golf?  What Is a \'Mulligan\' in Golf?  This golfer looks like he needed a mulligan.\xa0 John Cumming/Photodisc/Getty Images  By Brent Kelley  Updated February 05, 2016.  A mulligan, most simply put, is a "do-over" in golf. Hit a bad shot? Take a mulligan and replay that stroke . Drop a ball on the spot from which you just played, and re-play. The first (bad) shot is not counted.  Are Mulligans \'Legal\'?  No. There is never a time, when playing under the\xa0 Rules of Golf , that a mulligan is "legal." Mulligans are not allowed under the rules.  But Mulligans Are Very Popular  But just because mulligans aren\'t "legal" doesn\'t mean their use isn\'t common and popular among recreational golfers. Many amateurs and recreational golfers - players just out to have a fun round with buddies, as opposed to serious golfers looking for competition - are lax in observing the rules anyway.  So mulligans are most often employed during friendly rounds by golf buddies; or during charity or playday tournaments where mulligans are sometimes sold. If mulligans are for sale at a charity tournament, that means the golfer can buy, say, three mulligans for a set price each.  continue reading below our video  What Are Mulligans?  The sale of mulligans is sometimes used as an additional fund-raiser at charitable events.  Common Ways of Using Mulligans  Do all golfers use mulligans in the same way? No - whatever a group of golfers agrees upon is what counts (unless you are using mulligans in something like a charity tournament or association outing setting - then do what the organizers tell you).  How many mulligans you get to use during a round, what types of shots you can use them for, and so on, are things that differ from golfer to golfer. So if you\'re playing with a new group and mulligans are in effect, you need to clarify what is allowed.  Here are some common ways that mulligans are used:  Some golfers limit the use of mulligans to the first tee only, or to the first and 10th tees only.  Some golfers use one mulligan per nine holes, but anywhere on each nine.  It\'s most common for mulligans to be used only off the tee , i.e., you can only use a mulligan to replay a drive. However, some groups allow mulligans from the fairway , too.  Less common is allowing mulligans from the rough or out of hazards, but some golfers even do that.  It is rarer still - rarely seen, in fact - for mulligans to be used on the putting green .  And some groups allow mulligans from just about anywhere on the golf course, but set a limit - say, three mulligans per round, or nine, or 18.  Clearly, there are many different ways that golfers use mulligans. If you have a regular group of friends you play with, and your group allows mulligans, you probably long ago settled into your own "rules" about using them.  Note that mulligans are common in the United States, but rarely seen in the UK, for example. As with all localized golf customs, formats and betting games , when playing with golfers you don\'t know clear up the ground rules before play starts to avoid possible confusion later.  And again, please note: If you are playing in a tournament, handicap round or other setting in which the Rules of Golf are strictly followed, you cannot play mulligans.  Why Is It Called a \'Mulligan\'?  Good question! And the fact is, nobody knows for sure how a do-over in golf came to be called a mulligan. There are multiple theories, however, and we go over some of them in our FAQ on the topic:  Why are mulligans called that?  Other Terms/Uses  Mulligans | Question: In which sport may a \'Mulligan\' be awarded?', ['golf', 'golfer'], 'mrqa_triviaqa-validation-5972'), tensor(0.1673)), (("Context: He produced artificial lightning, with discharges consisting of millions of volts and up to 135 feet long. Thunder from the released energy was heard 15 miles away in Cripple Creek, Colorado. People walking along the street observed sparks jumping between their feet and the ground. Sparks sprang from water line taps when touched. Light bulbs within 100 feet of the lab glowed even when turned off. Horses in a livery stable bolted from their stalls after receiving shocks through their metal shoes. Butterflies were electrified, swirling in circles with blue halos of St. Elmo's fire around their wings. | Question: What happened to nearby light bulbs?", ['glowed even when turned off', 'glowed'], 'mrqa_squad-validation-1516'), tensor(0.1053)), (("Context: Prague: Six things you must do in the Czech capital city ...Prague: Six things you must do in the Czech capital city | Daily Mail Online  Prague: Six things you must do in the Czech capital city  comments  Prague is one of Europe's architectural gems, with more knockout picture-postcard views than some entire countries. Unsurprisingly, the Czech capital can also get busy as thousands of tourists jostle for space.  Here is a selection of things to do during a visit to the city - from taking in its best-loved sights to stopping for a drink at a beautifully restored cafe and watching a comic take on a famous Mozart opera.  1... New Jerusalem  Emperor Charles IV planned this intensely beautiful city as the ‘New Jerusalem’ in the 14th Century, and architects maintained the vision until its last golden age in the Thirties. The old centre, with outstanding buildings such as Valdstejn Palace, St James’ Church and St Vitus Cathedral, is still largely intact after 800 years and is now a World Heritage Site.  With nifty footwork you can outwit the crowds. Try walking the famous Charles Bridge before breakfast, and save the striking of the Astronomical Clock in the Old Town Square until the evening when it’s quieter. I also like the peacefulness of Mala Strana, the cafe-lined cobbled streets under Prague Castle. And take a tram to the Jewish Quarter and wander Petrin Hill for wonderful views over the city’s baroque roofs.  Atmospheric: Prague's beautiful Charles Bridge  2... Grand re-opening  How about this for a comeback? The Grand Cafe Orient ( www.grandcafeorient.cz ) opened in 1912 in the House of the Black Madonna, Prague’s architectural take on cubism, the art form Picasso championed. The cafe closed eight years later when cubism fell out of favour but reopened in 2005, faithfully recreated with period furniture and fittings. Try the apple strudel with vanilla ice cream.  Another venue, the Mysak pastry shop, reopened in 2008 inside the Mysak Gallery ( www.gallerymysak.cz ), with original doors, mosaics floors, a marble staircase and wooden alcoves. The signature dish is karamelovy pohar – ice cream topped with caramel, chocolate and walnuts.  RELATED ARTICLES  Share  3...Fighting for freedom  After the euphoria of spring 1968, and Czechoslovakia’s short-lived break for freedom from the Soviet Union, came deep gloom that summer when Russian tanks rolled in to consign this proud nation to 20 more years of oppression.  The Museum of Communism ( www.muzeumkomunismu.cz ) tells the story of those lost years. I came closest to Prague’s tragedy and eventual triumph on the steps of the National Museum in Wenceslas Square. I looked down on the bronze cross at the spot where Jan Palach burnt himself to death in protest in 1969. It's also where people celebrated their new-found freedom after 1989’s so-called Velvet Revolution.  Historic: The Duke of Bohemia is represented by an equestrian statue in Wenceslas Square  4...Recovered treasures  In 1989, William Lobkowicz returned to Prague from exile in Boston to restore the great estate his family had built up over 700 years. It had been confiscated first by the Nazis, then by the communists. Diligent legal teams gradually recovered land, thousands of books, paintings and pieces of furniture. William’s mission culminated in the opening of Lobkowicz Palace in 2007, with its exhibition of the best of the family’s reclaimed treasures. Star exhibits include two views of the River Thames by Canaletto and Haymaking by Pieter Bruegel the Elder.  Another of the city’s new attractions is totally different. The Kafka Museum ( www.kafkamuseum.cz ) celebrates the writer (he was born here) whose name stands for the way the ordinary man is defeated by ‘the system’.  5...Pulling the strings  With his mischievous sense of fun, Mozart would have approved of the National Marionette Theatre’s comic performances of his opera Don Giovanni ( www.mozart.cz ). I particularly loved the puppet cat hissing at an anguished diva in mid aria. Don Giovanni is Prague’s top claim to cultural fame – in 1787, Mozart staged its world premiere at the exquisite opera | Question: By what name do Czechs know their capital city, Prague?", ['prague'], 'mrqa_triviaqa-validation-6351'), tensor(0.0827)), (('Context: The Soulages collection of Italian and French Renaissance objects was acquired between 1859 and 1865, and includes several cassone. The John Jones Collection of French 18th-century art and furnishings was left to the museum in 1882, then valued at £250,000. One of the most important pieces in this collection is a marquetry commode by the ébéniste Jean Henri Riesener dated c1780. Other signed pieces of furniture in the collection include a bureau by Jean-François Oeben, a pair of pedestals with inlaid brass work by André Charles Boulle, a commode by Bernard Vanrisamburgh and a work-table by Martin Carlin. Other 18th-century ébénistes represented in the Museum collection include Adam Weisweiler, David Roentgen, Gilles Joubert & Pierre Langlois. In 1901, Sir George Donaldson donated several pieces of art Nouveau furniture to the museum, which he had acquired the previous year at the Paris Exposition Universelle. This was criticized at the time, with the result that the museum ceased to collect contemporary items and did not do so again until the 1960s. In 1986 the Lady Abingdon collection of French Empire furniture was bequeathed by Mrs T. R. P. Hole. | Question: What items comprise the John Jones Collection?', ['art and furnishings', 'French 18th-century art and furnishings'], 'mrqa_squad-validation-5622'), tensor(0.0590)), (('Context: 67th Tony Awards  The 67th Annual Tony Awards were held June 9, 2013, to recognize achievement in Broadway productions during the 2012–13 season.  The ceremony returned to Radio City Music Hall in New York City, after two years at Beacon Theatre, and was broadcast live on CBS television.  Neil Patrick Harris hosted for the third consecutive year, his fourth time as host.  Awards in four of the eight acting categories, (Best Actress in a Play, Best Actor in a Musical, Best Actress in a Musical, Best Featured Actor in a Play) were given to African-American performers.  Furthermore, it is the second time in Tony history that both directing prizes went to women.  Garry Hynes and Julie Taymor had previously won in 1998. " Kinky Boots" had a season best 13 nominations and 6 awards.  Cyndi Lauper, who wrote the score for "Kinky Boots", is the first solo female winner for Best Original Score.   67th Tony Awards  The 67th Annual Tony Awards were held June 9, 2013, to recognize achievement in Broadway productions during the 2012–13 season.  The ceremony returned to Radio City Music Hall in New York City, after two years at Beacon Theatre, and was broadcast live on CBS television.  Neil Patrick Harris hosted for the third consecutive year, his fourth time as host.  Awards in four of the eight acting categories, (Best Actress in a Play, Best Actor in a Musical, Best Actress in a Musical, Best Featured Actor in a Play) were given to African-American performers.  Furthermore, it is the second time in Tony history that both directing prizes went to women.  Garry Hynes and Julie Taymor had previously won in 1998. " Kinky Boots" had a season best 13 nominations and 6 awards.  Cyndi Lauper, who wrote the score for "Kinky Boots", is the first solo female winner for Best Original Score.   Julie Taymor  Julie Taymor (born December 15, 1952) is an American director of theater, opera and film.  She is best known for directing the stage musical "The Lion King", for which she became the first woman to win the Tony Award for directing a musical, in addition to a Tony Award for Original Costume Design.   Julie Taymor  Julie Taymor (born December 15, 1952) is an American director of theater, opera and film.  She is best known for directing the stage musical "The Lion King", for which she became the first woman to win the Tony Award for directing a musical, in addition to a Tony Award for Original Costume Design. | Question: In what year did the director of The Lion King win the Tony Awards?', ['1998'], 'mrqa_hotpotqa-validation-3241'), tensor(0.0563)), (("Context: Rolls-Royce Welland  The Rolls-Royce RB.23 Welland was Britain's first production jet engine.  It entered production in 1943 for the Gloster Meteor.  The name Welland is taken from the River Welland, in keeping with the Rolls-Royce policy of naming early jet engines after rivers based on the idea of continuous flow, air through the engine and water in a river.   River Welland  The River Welland is a lowland river in the east of England, some 65 mi long.  It drains part of the Midlands eastwards to The Wash.  The river rises in the Hothorpe Hills, at Sibbertoft in Northamptonshire, then flows generally northeast to Market Harborough, Stamford and Spalding, to reach The Wash near Fosdyke.  It is a major waterway across the part of the Fens called South Holland, and is one of the Fenland rivers which were laid out with washlands.  There are two channels between widely spaced embankments with the intention that flood waters would have space in which to spread while the tide in the estuary prevented free egress.  However, after the floods of 1947, new works such as the Coronation Channel were constructed to control flooding in Spalding and the washes are no longer used solely as pasture, but may be used for arable farming. | Question: Rolls-Royce Welland's name is taken from this river that is low long ?", ['65 mi'], 'mrqa_hotpotqa-validation-1201'), tensor(0.0554)), (('Context: <Ol>  A new ruler unites China , founds a new dynasty , and gains the Mandate of Heaven .   China , under the new dynasty , achieves prosperity .   The population increases .   Corruption becomes rampant in the imperial court , and the empire begins to enter decline and instability .   A natural disaster wipes out farm land . The disaster normally would not have been a problem ; however , together with the Corruption and overpopulation , it causes famine .   The famine causes the population to rebel and a civil war ensues .   The ruler loses the Mandate of Heaven .   The population decreases because of the violence .   China goes through a warring states period .   One state emerges victorious .   The state starts a new empire .   The empire gains the Mandate of Heaven .  </Ol> | Question: in the dynastic cycle what is the right to rule called ?', ['the Mandate of Heaven'], 'mrqa_naturalquestions-validation-6157'), tensor(0.0542)), (('Context: Origin of Does Exactly What it Says on the Tin | RonsealOrigin of Does Exactly What it Says on the Tin | Ronseal  The Ronseal Phrase - Does Exactly What it Says on the Tin  \xa0  People use the phrase every day, it has come to represent a product or policy that is open and honest; it is used when something quite simply ‘Does Exactly What it Says on the Tin .’  Originating back to 1994 the popular phrase ‘Does exactly what is says on the tin’ was developed when Ronseal employed London based advertising agency HHCL to create a campaign that without trying too hard would de-mystify our products. Dave Shelton, co-creator of the line with Liz Whinston, explains the thought process behind the original campaign adverts ,  “We started to write a commercial that featured a straightforward guy who said lines like: "If you\'ve got wood to stain and you want it to dry quickly, you need Ronseal Quick Drying Woodstain."  We\'d soon knocked out several scripts but we needed a line… "Does exactly what it says on the tin" was a great way of summing this up.  After the initial campaign, sales shot up and Ronseal became [a] brand leader.”  ‘Does exactly what it says on the tin’ is now more than an advertising slogan; it has become part of our everyday vernacular. The line has made it into the Oxford Dictionary of Idioms, been featured in the song ‘What it Says On The Tin’ by Katie Melua, and has even been used by Prime Minister David Cameron and on multiple occasions to summarise his preferred approach to politics.  The phrase has come to represent a product or policy that is open, honest and delivers against its promise.  The phrase is now used internationally and it’s not surprising it has become the third most known slogan of all time.  \xa0  \xa0  [1] The phrase is a registered trademark of Ronseal’s owner, the Sherwin-Williams Company , across the European Community for products including paints, varnishes and wood preservatives (E3085826)  [3] Source Creative Review February 2012  \xa0  The Ronseal phrase: It does exactly what it says on the tinThe Ronseal phrase: It does exactly what it says on the tin - BBC News  BBC News  The Ronseal phrase: It does exactly what it says on the tin  8 January 2013  Read more about sharing.  Close share panel  Prime Minister David Cameron used a famous advertising slogan to sum up the state of the coalition government at its halfway point, saying "it is a Ronseal deal - it does what it says on the tin". This phrase has entered the British vernacular, says its co-creator Dave Shelton.  David Cameron has said it before. In 2004, he said: "People are crying out for a kind of Ronseal politics - they want it to do what it says on the tin."  People use the phrase every day. On YouTube you can listen to Katie Melua\'s What It Says On The Tin . Google the line and you get millions of results.  In 1994, when my advertising partner Liz Whiston and I came up with the line "Does exactly what it says on the tin" for Ronseal, we never dreamed how it would enter the language.  About the author  Dave Shelton and Liz Whiston co-founded Bordello Advertising.  They have been working in advertising for more than 20 years.  Back then Ronseal was just another varnish fighting for market share against a lot of fancier lifestyle brands. Anyone wandering into a DIY superstore was faced with a wall of different solutions to whatever woodcare job they might be trying to tackle.  We had presented a number of different campaigns, all of which had been rejected by Ged Shields, the then marketing director of Ronseal for "trying too hard".  Shields wanted to de-mystify the product.  We decided what was needed wasn\'t puns or art.  Instead we would call a spade a spade.  We started to write a commercial that featured a | Question: Which product is advertised on TV with the slogan \'It does exactly what it says on the tin\'?', ['ronseal'], 'mrqa_triviaqa-validation-5026'), tensor(0.0495)), (("Context: May 14 : The Corps of Discovery departs from Camp Dubois at 4 p.m. , marking the beginning of the voyage to the Pacific coast .   May 16 : The Corps of Discovery arrives at St. Charles , Missouri .   May 21 : Departure from St. Charles at 3 : 30 p.m.   May 24 : Pass Boones Settlement . Home of famous woodsman L. Willenborg .   May 25 : The expedition passes the small village of La Charrette on the Missouri River . Charles Floyd writes in his journal that this is `` the last settlement of whites on this river '' .   June 1 : The expedition reaches the Osage River .   June 12 : Lewis and Clark meet three trappers in two pirogues . One of the men was Pierre Dorion , Jr. -- who knew George Rogers Clark . Lewis and Clark persuade Dorion to return to Sioux camp to act as interpreter .   June 26 : The expedition arrives at Kaw Point where the Kansas River drains into the Missouri River basin .   June 28 -- 29 : First trial in new territory . Pvt . John Collins is on guard duty and breaks into the supplies and gets drunk . Collins invites Pvt . Hugh Hall to drink also . Collins receives 100 lashes , Hall receives 50 lashes .   July 4 : Marking Independence Day , the expedition names Independence Creek located near Atchison , Kansas .   July 11 -- 12 : Second trial in new territory . Pvt . Alexander Hamilton Willard is on guard duty . Is charged with lying down and sleeping at his post whilst a sentinel . Punishable by death . He receives 100 lashes for four straight days .   July 21 : Reaches the Platte River , 640 miles from St. Louis . Entering Sioux Territory .   August 1 : Captain William Clark 's 34th birthday .   August 3 : The Corps of Discovery holds the first official council between representatives of the United States and the Oto and Missouri tribes at Council Bluffs , Iowa . They hand out peace medals , 15 - star flags and other gifts , parade men and show off technology .   August 4 : Moses Reed said he was returning to a previous camp to retrieve a knife but deserted to St. Louis .   August 18 : George Drouillard returns to camp with Reed and Otos ' Chief Little Thief . Reed is sentenced to run the gauntlet ( 500 lashes ) and is discharged from the permanent party .   August 18 : Captain Meriwether Lewis 's 30th birthday .   August 20 : Sergeant Charles Floyd dies . He dies from bilious chorlick ( ruptured appendix ) . He is the only member lost during the expedition .   August 23 : Pvt . Joseph Field kills first bison .   August 26 : Pvt . Patrick Gass is elected to sergeant . First election in new territory west of Mississippi River . George Shannon is selected to get the horses back from native Americans .   August 30 : A friendly council with the Yankton Sioux held . According to a legend , Lewis wraps a newborn baby in a United States flag and declares him `` an American '' .   September 4 : Reach the mouth of the Niobrara River .   September 7 : The expedition drives a prairie dog out of its den ( by pouring water into it ) to send back to Jefferson .   September 14 : Hunters kill and describe prairie goat ( antelope ) .   September 25 -- 29 : A band of Lakota Sioux demand one of the boats as a toll for moving further upriver . Meet with Teton Sioux . Close order drill , air gun demo , gifts of medals , military coat , hats , tobacco . Hard to communicate language problems . Invite chiefs on board keelboat , give each \u200b ⁄ glass whiskey , acted drunk wanted more . Two armed confrontations with Sioux . Some of the chiefs sleep on boat , move up river to another village , meet in lodge , hold scalp dance .   October 8 -- 11 : Pass Grand River home of the Arikara people , 2,000 + . Joseph Gravelins trader , lived with Arikara for 13 yrs . Pierre Antoine Tabeau lived in another village was from Quebec .   October 13 : Pvt . John Newman tried for | Question: on which river did the exploration of the louisiana purchase begin ?", ['the Missouri River'], 'mrqa_naturalquestions-validation-8948'), tensor(0.0435)), (('Context: Alex the Dog  Alex the Dog was the advertising mascot for Stroh\'s beer in the 1980s and precursor to Budweiser\'s Spuds MacKenzie.  At the peak of his career, Alex appeared in parades, on "Good Morning America", and the "Today" show.  He even inspired a series of toys, posters, cologne, shampoo and hand lotion.  Hip-hop artist Tone Loc referenced Alex the Dog in his song "Funky Cold Medina".   Spuds MacKenzie  Spuds MacKenzie is a fictional dog character created for use in an extensive advertising campaign marketing Bud Light beer in the late 1980s.  The Spuds MacKenzie mascot and campaign were created by a 23-year-old art director, Jon Moore.  At the time he was working at Needham, Harper & Steers, a Chicago, Illinois, advertising agency.  The dog first showed up in a Bud Light Super Bowl XXI ad in 1987.  During the height of his popularity, large amounts of Spuds merchandise was available, such as plush toys and t-shirts. | Question: When Budweiser created a dog mascot to promote Bud Light in the 1980s, which rival brewer (with its own character Alex the Dog) were they competing with?', ["Stroh's"], 'mrqa_hotpotqa-validation-5802'), tensor(0.0398)), (('Context: The axons of the tract cells cross over ( decussate ) to the other side of the spinal cord via the anterior white commissure , and to the anterolateral corner of the spinal cord ( hence the spinothalamic tract being part of the anterolateral system ) . Decussation usually occurs 1 - 2 spinal nerve segments above the point of entry . The axons travel up the length of the spinal cord into the brainstem , specifically the rostral ventromedial medulla . | Question: where does decussation occur in the spinothalamic pathway ?', ['usually occurs 1 - 2 spinal nerve segments above the point of entry'], 'mrqa_naturalquestions-validation-7511'), tensor(0.0389)), (("Context: It is an allusion to Samuel Taylor Coleridge 's poem The Rime of the Ancient Mariner ( 1798 ) . In the poem , an albatross starts to follow a ship -- being followed by an albatross was generally considered a sign of good luck . However , the titular mariner shoots the albatross with a crossbow , which is regarded as an act that will curse the ship ( which indeed suffers terrible mishaps ) . Even when they are too thirsty to speak , the ship 's crew let the mariner know through their glances that they blame his action for the curse . The albatross is then literally hung around the mariner 's neck by the crew to symbolize his guilt in killing the bird . Thus , the albatross can be both an omen of good or bad luck , as well as a metaphor for a burden to be carried as penance . | Question: the rime of the ancient mariner albatross symbolism ?", ['a metaphor for a burden to be carried as penance', 'omen of good or bad luck'], 'mrqa_naturalquestions-validation-7017'), tensor(0.0361)), (("Context: Steve Carell as Felonious Gru , the former villain turned Anti-Villain League agent , Margo , Edith , and Agnes ' adoptive father , and Lucy 's husband .   Carell also voices Dru , Gru 's long - lost twin brother and the girls ' adoptive uncle .     Kristen Wiig as Lucy Wilde , an Anti-Villain League agent , Gru 's wife and the girls ' adoptive mother .   Trey Parker as Balthazar Bratt , a supervillain and former child star who grows up to become obsessed with the character he played in the ' 80s and is bent on world domination .   Miranda Cosgrove as Margo , Gru and Lucy 's oldest adoptive daughter .   Dana Gaier as Edith , Gru and Lucy 's middle adoptive daughter .   Nev Scharrel as Agnes , Gru and Lucy 's youngest adoptive daughter . She was previously voiced by Elsie Fisher in the first two films .   Pierre Coffin as Mel and the Minions .   Coffin also voices a Museum Director .     Steve Coogan as Silas Ramsbottom , the director of the Anti-Villain League .   Coogan also voices Fritz , the butler of Dru .     Julie Andrews as Marlena Gru , Gru and Dru 's mother .   Jenny Slate as Valerie Da Vinci , a member of the Anti-Villain League who becomes the brand new AVL director .   Andy Nyman as Clive , a robot who is Bratt 's sidekick .   Adrian Ciscato as Niko , a boy from Freedonia who falls in love with Margo .   John Cygan ( in his final film role ) as an Additional Voice . | Question: who is the cast of despicable me 3 ?", ['Nev Scharrel', 'Andy Nyman', 'Adrian Ciscato', 'Pierre Coffin', 'Steve Coogan', 'Trey Parker', 'Julie Andrews', 'Kristen Wiig', 'John Cygan', 'Jenny Slate', 'Dana Gaier', 'Steve Carell', 'Miranda Cosgrove'], 'mrqa_naturalquestions-validation-3490'), tensor(0.0295)), (('Context: Great Train Robbery Anniversary - Heart Beds, Bucks ...Great Train Robbery Anniversary - Heart Beds, Bucks & Herts News  Great Train Robbery Anniversary  Great Train Robbery Anniversary  See pictures of the bridge near Leighton Buzzard where The Great Train Robbery took place fifty years ago.  See all photos (15) in this gallery  By John Stratford (@Heart_JohnS), 8th August 2013, 10:50  Fifty years after The Great Train Robbery, an event\'s taken place to praise police who searched for the men who robbed a mail train in Buckinghamshire.  You may also like  On 8 August 1963, a gang of robbers, masterminded by Bruce Reynolds, stopped the Glasgow-Euston overnight mail train as it passed close to Cheddington, near Leighton Buzzard.  On board were huge numbers of used bank notes.  Twelve of the robbers were jailed for a total of more than 300 years but more than one broke out of prison, including notorious criminal Ronnie Biggs, who spent over 30 years on the run before he finally returned to Britain in 2001 to face arrest.  Reynolds returned in 1968, five years after the crime, and was captured in Torquay and jailed for 25 years.  Two Buckinghamshire Constabulary police officers who were involved in the investigation attended a police commemoration event alongside serving Thames Valley Police officers at Eynsham Hall in Witney, Oxfordshire, on Wednesday 7 August 2013.  Keith Milner was a detective at Aylesbury at the time of the robbery, while John Woolley was a PC and discovered Leatherslade Farm near Brill, Buckinghamshire, where the men hid after committing the crime.  Biggs insisted in July 2013 that he was proud to have been part of the gang.  The famous fugitive, who celebrates his 84th birthday on 8 August 2013, escaped from prison in 1965 and spent 36 years on the run before finally being arrested and jailed in 2001.  Released from prison on compassionate grounds in 2009 due to ill health he is still alive, being cared for in a north London nursing home, and he has few regrets about the crime that made him a household name.  Biggs, who cannot speak and communicates through a spelling board, said: "If you want to ask me if I have any regrets about being one of the train robbers, my answer is, \'No!\'.  "I will go further: I am proud to have been one of them. I am equally happy to be described as the \'tea-boy\' or \'The Brain\'.  "I was there that August night and that is what counts. I am one of the few witnesses - living or dead - to what was \'The Crime of the Century\'.\'\'  But although he is proud to have been involved in the headline-grabbing crime, he admitted he does have some regrets.  "It is regrettable, as I have said many times, that the train driver was injured,\'\' he said. "And he was not the only victim.  "The people who paid the heaviest price for the Great Train Robbery are the families. The families of everyone involved in the Great Train Robbery, and from both sides of the track.  "All have paid a price for our collective involvement in the robbery. A very heavy price, in the case of my family.  "For that, I do have my regrets.\'\'  Ronnie\'s son Michael, who was born while Ronnie was on the run, spoke exclusively to Heart and said he isn\'t proud of his dad being a criminal, but he is proud of the man his dad is.  "I believe if you commit a crime, you have to do the time," he said, "but the time has to be proportional with the crime that you\'ve committed.  "The sad bit was that he went into the train robbery because he needed £500 to put a deposit on his house. About a week before the robbery, he won the £500 in a bet on the horses but there was no turning back then."  Listen to Michael Biggs | Question: What criminal offence took place in Cheddington, Buckinghamshire in August (8th) 1963?', ['great train robbery'], 'mrqa_triviaqa-validation-7369'), tensor(0.0199)), (('Context: Bodhi Natural Health Products | Bodhi Natural ProductsBodhi Natural Health Products | Bodhi Natural Products  Stockists  Welcome  “It is my wish that by using these products you can enjoy life to the fullest by having good health, hopefully relieve suffering, and of course protecting our beautiful planet by using only natural substances. All Bodhi products come from nature, and are organic”\xa0 Julie Herbison  Bodhi originated as Body Mind Balancing which started off as my business as a Natural Health Practitioner. Over time my products became more and more popular so the name Bodhi came about to keep it short and simple. Bodhi comes from the Bodhi Tree which is a type of fig tree. The Bodhi tree has played an important part in human history featuring strongly in religious history and mythology in various parts of the world.  The ficus religiosa, Bodhi tree, has large heart shaped leaves. It is one of the most sacred trees in India, Sri Lanka and Nepal, where it is venerated by both Hindus and Buddhists. The tree under which Siddhartha Gautama gained enlightenment over 2,600 years ago still grows today in North East India.  In Hindu religion, Vishnu is said to have been born under a Bodhi tree and is often depicted sitting on its heart-shaped leaves. The Bodhi tree is often planted in the grounds of temples and, of all the sacred trees of India, it is the most widely worshiped.  The Bodhi tree in Sri Lanka is located in Anuradhapura and is said to be the oldest tree in the world with a known planting date. This fig tree is said to have grown from a branch taken from the original Bodhi tree in India under which Buddha gained enlightenment.  For Homeopathic and Body Mind consultations please call  Julie Herbison  Bodhi Tree, Bodh Gaya - Sacred SitesBodhi Tree, Bodh Gaya  Bodhi Tree, Bodh Gaya  Buddhist Monks at Bodhi Tree (The site of Buddha\'s enlightenment) \xa0 \xa0\xa0  Bodh Gaya, located 100 km (62 mi) south of Patna in the Indian state of Bihar, is the most venerated sacred place in Buddhism. It is the place where Prince Siddhartha Guatama, while meditating beneath the Bodhi Tree, attained enlightenment and became the Buddha.  Traditional accounts say that, in the early years of the 4th century BC, Siddhartha Gautama saw the suffering of the world and wanted to be free from it. As a young man, following the ancient traditions of Hinduism, he sought out spiritual teachers. Inquiring of their knowledge, he diligently practiced various yogas and meditations. Seven years passed, the last three in extreme asceticism, yet still he had not achieved his goal of enlightenment.  Impression of Buddha feet, Bodh Gaya \xa0\xa0\xa0  Siddhartha then journeyed toward the ancient sacred forests of Uruvela (modern Gaya in Bihar, in north India) with the intention of finally and completely realizing the infinite. Guided by visionary dreams and following in the footsteps of the Buddhas of three previous ages, Krakucchanda, Kanakamuni and Kasyapa (who had each attained enlightenment at the site) Siddhartha sat beneath the Bodhi Tree. Touching the earth, thereby calling it to witness the countless lifetimes of virtue that had led him to this place of enlightenment, he resolved not to rise again until enlightenment was attained.  "Here on this seat my body may shrivel up, my skin, my bones, my flesh may dissolve, but my body will not move from this seat until I have attained Enlightenment, so difficult to obtain in the course of great periods of time".  As Siddhartha sat in deep meditation beneath the Bodhi Tree, Mara, the Dark Lord of Death, came to distract him from his endeavor. When the earth shook, confirming the truth of Gautama\'s words, Mara unleashed his army of demons. In the epic battle that ensued, Siddhartha\'s wisdom broke through Mara’s illusions. The power of his compassion transformed the demons\' weapons into flowers and Mara and all his forces fled. Three days and nights passed and Siddhartha’s intention was realized. He became the Buddha, meaning the ‘Enlightened One’.  The Mahabodhi Temple, Bodh Gaya, India \xa0 \xa0\xa0  The Buddha then spent the | Question: Who is said to have gained enlightenment sitting under the Bodhi Tree?', ['buddha', 'enlightened one'], 'mrqa_triviaqa-validation-3915'), tensor(0.0177)), (('Context: The Super Bowl 50 Host Committee has vowed to be "the most giving Super Bowl ever", and will dedicate 25 percent of all money it raises for philanthropic causes in the Bay Area. The committee created the 50 fund as its philanthropic initiative and focuses on providing grants to aid with youth development, community investment and sustainable environments. | Question: What was the name of the fund setup to help with investing in the community?', ['50 fund', 'the 50 fund'], 'mrqa_squad-validation-392'), tensor(0.0177)), (('Context: It has been claimed that the transmission of the first episode was delayed by ten minutes due to extended news coverage of the assassination of US President John F. Kennedy the previous day; whereas in fact it went out after a delay of eighty seconds. The BBC believed that many viewers had missed this introduction to a new series due to the coverage of the assassination, as well as a series of power blackouts across the country, and they broadcast it again on 30 November 1963, just before episode two. | Question: What other event made the BBC concerned that viewers had not seen the premier of Doctor Who?', ['a series of power blackouts across the country', 'power blackouts'], 'mrqa_squad-validation-7746'), tensor(0.0104)), (("Context: Petra Kvitová career statistics  This is a list of the main career statistics of Czech professional tennis player Petra Kvitová.  To date, Kvitová has won 20 singles titles including two Grand Slam singles titles at the Wimbledon Championships, one WTA Tour Championships singles title, two WTA Premier Mandatory singles titles and four WTA Premier 5 singles titles.  She was also the bronze medallist at the 2016 Rio Olympics, a semifinalist at the 2010 Wimbledon Championships, 2012 Australian Open and 2012 French Open and a quarterfinalist at the 2011 Australian Open, 2012 Wimbledon Championships, 2013 Wimbledon Championships, 2015 US Open and 2017 US Open.  Kvitová reached her career-high ranking of world no. 2 on 31 October 2011.   2017 US Open (tennis)  The 2017 US Open was the 137th edition of tennis' US Open and the fourth and final Grand Slam event of the year.  It was held on outdoor hard courts at the USTA Billie Jean King National Tennis Center in New York City.  Experimental rules featured in qualifying for the main draw as well as in the junior, wheelchair and exhibition events. | Question: What edition of tennis' US Open was the 2017 US Open when Petra Kvitova was a quarterfinalist?", ['137th'], 'mrqa_hotpotqa-validation-409'), tensor(0.0099)), (("Context: The comptroller ( who is also auditor general and head of the National Audit Office ) controls both the Consolidated Fund and the National Loans Fund . The full official title of the role is Comptroller General of the Receipt and Issue of Her Majesty 's Exchequer . | Question: who controls the consolidated fund of the state ?", ['The comptroller ( who is also auditor general and head of the National Audit Office )'], 'mrqa_naturalquestions-validation-1364'), tensor(-0.0002)), (('Context: Concentrated O2 will allow combustion to proceed rapidly and energetically. Steel pipes and storage vessels used to store and transmit both gaseous and liquid oxygen will act as a fuel; and therefore the design and manufacture of O2 systems requires special training to ensure that ignition sources are minimized. The fire that killed the Apollo 1 crew in a launch pad test spread so rapidly because the capsule was pressurized with pure O2 but at slightly more than atmospheric pressure, instead of the 1⁄3 normal pressure that would be used in a mission.[k] | Question: ______ In both liquid and gas form can fastly result in an exlposion. ?', ['oxygen'], 'mrqa_squad-validation-3478'), tensor(-0.0089)), (('Context: Although partly functional , weather vanes are generally decorative , often featuring the traditional cockerel design with letters indicating the points of the compass . Other common motifs include ships , arrows and horses . Not all weather vanes have pointers . When the wind is sufficiently strong , the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing . | Question: where does the arrow of a wind vane point ?', ['the direction from which the wind is blowing'], 'mrqa_naturalquestions-validation-9688'), tensor(-0.0135)), (('Context: Pi Day is an annual celebration of the mathematical constant π ( pi ) . Pi Day is observed on March 14 ( 3 / 14 in the month / day format ) since 3 , 1 , and 4 are the first three significant digits of π . In 2009 , the United States House of Representatives supported the designation of Pi Day . | Question: how long have we been celebrating pi day ?', ['2009'], 'mrqa_naturalquestions-validation-3028'), tensor(-0.0138)), (('Context: Honda Ballade  The Honda Ballade is a subcompact automobile built by Honda of Japan.  It began as a four-door higher equipment content version of the Civic in 1980.  The Ballade was developed at the same time the Honda Vigor appeared, which was a higher content Honda Accord.  The Ballade was sold exclusively in Japan at "Honda Verno" dealerships alongside the Vigor, Prelude, CR-X, and Quint.  In the UK it was launched at the same time as the very similar Triumph Acclaim with which it shared a Honda built engine.   Honda Ballade  The Honda Ballade is a subcompact automobile built by Honda of Japan.  It began as a four-door higher equipment content version of the Civic in 1980.  The Ballade was developed at the same time the Honda Vigor appeared, which was a higher content Honda Accord.  The Ballade was sold exclusively in Japan at "Honda Verno" dealerships alongside the Vigor, Prelude, CR-X, and Quint.  In the UK it was launched at the same time as the very similar Triumph Acclaim with which it shared a Honda built engine.   Honda CR-X  The Honda CR-X, originally launched as the Honda Ballade Sports CR-X in Japan, is a front-wheel-drive sport compact car manufactured by Honda between 1983 and 1991.  It was replaced by the Honda CR-X del Sol for the 1992 model year.  Although there are many supposed definitions for the acronym CR-X, the most widely accepted are "Civic rally cross", and "Civic renaissance model X".   Honda CR-X  The Honda CR-X, originally launched as the Honda Ballade Sports CR-X in Japan, is a front-wheel-drive sport compact car manufactured by Honda between 1983 and 1991.  It was replaced by the Honda CR-X del Sol for the 1992 model year.  Although there are many supposed definitions for the acronym CR-X, the most widely accepted are "Civic rally cross", and "Civic renaissance model X". | Question: The "civic rally cross" was sold alongside what other model sold exclusively in Japan at Honda Verno dealerships?', ['Honda Ballade'], 'mrqa_hotpotqa-validation-400'), tensor(-0.0174)), (('Context: Mary O\'Connell  Mary O\'Connell (better known as Sister Anthony, S.C.) (1814 – December 8, 1897) was an Irish immigrant to the United States, who became a Roman Catholic Religious Sister.  A Sister of Charity of Cincinnati, she served with distinction as a nurse on the front lines of the American Civil War.  Her work with the wounded and in health care in general caused her to be known as "the angel of the battlefield" and "the Florence Nightingale of America."  Her portrait hangs in the Smithsonian Institution in Washington, DC.   Mary O\'Connell  Mary O\'Connell (better known as Sister Anthony, S.C.) (1814 – December 8, 1897) was an Irish immigrant to the United States, who became a Roman Catholic Religious Sister.  A Sister of Charity of Cincinnati, she served with distinction as a nurse on the front lines of the American Civil War.  Her work with the wounded and in health care in general caused her to be known as "the angel of the battlefield" and "the Florence Nightingale of America."  Her portrait hangs in the Smithsonian Institution in Washington, DC.   Florence Nightingale  Florence Nightingale, ( ; 12 May 1820 – 13 August 1910) was an English social reformer and statistician, and the founder of modern nursing. | Question: What portrait hangs in the Smithsonian Institute along with what is known as the founder of nursing?', ['Sister Anthony, S.C.'], 'mrqa_hotpotqa-validation-1626'), tensor(-0.0227)), (('Context: The Iroquois was originally designated HU - 1 , hence the Huey nickname , which has remained in common use , despite the official redesignation to UH - 1 in 1962 . The UH - 1 first saw service in combat operations during the Vietnam War , with around 7,000 helicopters deployed . The Bell 204 and 205 are Iroquois versions developed for the civil market . | Question: where did the name huey helicopter come from ?', ['originally designated HU - 1'], 'mrqa_naturalquestions-validation-5144'), tensor(-0.0228)), (('Context: Pangaea or Pangea ( / pænˈdʒiːə / ) was a supercontinent that existed during the late Paleozoic and early Mesozoic eras . It assembled from earlier continental units approximately 335 million years ago , and it began to break apart about 175 million years ago . In contrast to the present Earth and its distribution of continental mass , much of Pangaea was in the southern hemisphere and surrounded by a superocean , Panthalassa . Pangaea was the most recent supercontinent to have existed and the first to be reconstructed by geologists . | Question: what land mass was north america a part of about 300 million years ago ?', ['Pangaea or Pangea'], 'mrqa_naturalquestions-validation-9386'), tensor(-0.0241)), (('Context: Luther\'s other major works on the Jews were his 60,000-word treatise Von den Juden und Ihren Lügen (On the Jews and Their Lies), and Vom Schem Hamphoras und vom Geschlecht Christi (On the Holy Name and the Lineage of Christ), both published in 1543, three years before his death. Luther argued that the Jews were no longer the chosen people but "the devil\'s people", and referred to them with violent, vile language. Citing Deuteronomy 13, wherein Moses commands the killing of idolaters and the burning of their cities and property as an offering to God, Luther called for a "scharfe Barmherzigkeit" ("sharp mercy") against the Jews "to see whether we might save at least a few from the glowing flames." Luther advocated setting synagogues on fire, destroying Jewish prayerbooks, forbidding rabbis from preaching, seizing Jews\' property and money, and smashing up their homes, so that these "envenomed worms" would be forced into labour or expelled "for all time". In Robert Michael\'s view, Luther\'s words "We are at fault in not slaying them" amounted to a sanction for murder. "God\'s anger with them is so intense," Luther concluded, "that gentle mercy will only tend to make them worse, while sharp mercy will reform them but little. Therefore, in any case, away with them!" | Question: How near to his death was the work published?', ['three years before', 'three years before his death'], 'mrqa_squad-validation-2629'), tensor(-0.0247)), (('Context: Amphitrite in Greek Mythology | MythographyAmphitrite in Greek Mythology | Mythography  Amphitrite in Greek Mythology  Home | Greek Gods | Free Spirits | Amphitrite  In Greek mythology, Amphitrite was a sea goddess. Sources state that she was the daughter of Nereus and Doris, which makes Amphitrite one of the fifty sea goddesses (or nymphs) who are known collectively as the Nereids. The Greek poet Hesiod lists the names of these sea nymphs in his Theogony:  “To Nereus and Doris of the lovely hair,  the daughter of Okeanos, the stream surrounding the earth,  a host of godly daughters was born in the barren sea:  Proto, Eukrante, Amphitrite, and Sao…”  (Hesiod, Theogony, lines 240-43)  According to the legend, the Olympian Poseidon (who was himself a god of the sea) saw Amphitrite one day and fell in love with the graceful and beautiful goddess. However, the sea nymph initially resisted Poseidon’s advances. But Poseidon was not easily discouraged. He sent his sea creatures to find Amphitrite. A dolphin succeeded in locating the Nereid, and it was this creature - the dolphin - who persuaded her to consider Poseidon’s proposal. Eventually, Amphitrite accepted Poseidon, and the two gods were married.  Amphitrite and Poseidon were together the parents of several children, including the sea god Triton. Here’s Hesiod again:  “From the union of rumbling Poseidon and Amphitrite  came the great Triton, whose might is far-flung,  an awesome god dwelling in a golden house that lies  at the sea’s bottom, near his cherished mother and lordly father.”  (Hesiod, Theogony, lines 930 ff. )  Amphitrite Goddess of the Sea - Greek Gods and GoddessesAmphitrite Goddess of the Sea  [ ? ]Subscribe To This Site  Amphitrite Goddess of the Sea  In Greek mythology, Amphitrite goddess of the Sea was one of the fifty Nereids, that is a daughter of Nereus and Doris - that\'s what Hesiod writes in his Theogony (but Apollodorus says her parents were Oceanus and Thetys). Her Roman equivalent was Salacia, god Neptune\'s wife.  Her name means "The third one who encircles (everything)" (because the ancient Greeks thought the whole world was encircled by a river-god, Oceanus ).  She spent most of the time singing and dancing with her sisters. One day Poseidon, god of the sea , saw her near the island of Naxos and fell in love with her. He kidnapped her and made her his wife.  In another story, Amphitrite, being a shy girl, fled away and hid somewhere in the Atlantic Ocean. Poseidon wanted desperately to find her, so he sent all the marine creatures to look for her. Delphinus, a dolphin, managed to find her and convinced her to go back to Poseidon and marry him. That\'s what made Amphitrite goddess of the sea. (By the way, the dolphin who found her was turned into a constellation by the grateful Poseidon ).  Amphitrite goddess of the sea has no special attributions (but sometimes she can calm the waters), she is just the mistress of the sea (and sometimes she is considered the mother of the marine creatures, but they existed even before she became queen of the sea). Some even say that she is just a personification of the sea.  She also appeared briefly in the story of Theseus. King Minos asked the hero to prove that he really was Poseidon\'s son. A ring was thrown into the sea and Theseus had to bring it back. The sea creatures found the ring and gave it to the hero, while Amphitrite goddess of the sea received him in her underwater palace and gave him a golden crown, which he took to Ariadne as a wedding gift. (As you can see, Amphitrite was the ideal wife, as she was never jealous of her husband\'s love affairs.)  Amphitrite and Poseidon had a son, Triton (a fish-man or a he-mermaid) and a daughter, Rhode, and maybe another daughter, Benthesicyme.  In art | Question: In Greek mythology, Amphitrite, queen of the sea, was the wife of which god?', ['poseidon'], 'mrqa_triviaqa-validation-671'), tensor(-0.0282)), (('Context: Br\'er RabbitBr\'er Rabbit, also spelled Bre\'r Rabbit or Brer Rabbit or Bruh Rabbit, is a central figure as Uncle Remus tells stories of the Southern United States. Br\'er Rabbit is a trickster who succeeds by his wits rather than by brawn, provoking authority figures and bending social mores as he sees fit. The Walt Disney Company later adapted this character for its 1946 animated motion picture Song of the South.  Tar-Baby story   In one tale, Br\'er Fox constructs a doll out of a lump of tar and dresses it with some clothes. When Br\'er Rabbit comes along he addresses the Tar-Baby amiably, but receives no response. Br\'er Rabbit becomes offended by what he perceives as the Tar-Baby\'s lack of manners, punches it, and in doing so becomes stuck. The more Br\'er Rabbit punches and kicks the tar "baby" out of rage, the more he gets stuck. When Br\'er Fox reveals himself, the helpless but cunning Br\'er Rabbit pleads, "please, Br\'er Fox, don\'t fling me in dat brier-patch," prompting Fox to do exactly that. As rabbits are at home in thickets, the resourceful Br\'er Rabbit uses the thorns and briers to escape. The story was originally published in Harper\'s Weekly by Robert Roosevelt; years later Joel Chandler Harris included his version of the tale in his Uncle Remus stories.  African origins   The Br\'er Rabbit stories can be traced back to trickster figures in Africa, particularly the hare that figures prominently in the storytelling traditions in West, Central, and Southern Africa. These tales continue to be part of the traditional folklore of numerous peoples throughout those regions. In the Akan traditions of West Africa, the trickster is usually the spider Anansi, though the plots in his tales are often identical with those of stories of Br\'er Rabbit.  However, Anansi does encounter a tricky rabbit called "Adanko" (Asante-Twi to mean "Hare") in some stories. The Jamaican character with the same name "Brer Rabbit", is an adaptation of the Ananse stories of the Akan people.   Some scholars have suggested that in his American incarnation, Br\'er Rabbit represented the enslaved Africans who used their wits to overcome adversity and to exact revenge on their adversaries, the White slave-owners.  Though not always successful, the efforts of Br\'er Rabbit made him a folk hero. However, the trickster is a multi-dimensional character. While he can be a hero, his amoral nature and his lack of any positive restraint can make him into a villain as well.   For both Africans and African Americans, the animal trickster represents an extreme form of behavior that people may be forced to adopt in extreme circumstances in order to survive. The trickster is not to be admired in every situation. He is an example of what to do, but also an example of what not to do. The trickster\'s behavior can be summed up in the common African proverb: "It\'s trouble that makes the monkey chew on hot peppers." In other words, sometimes people must use extreme measures in extreme circumstances.  Several elements in the Brer Rabbit Tar Baby story (e.g., rabbit needing to be taught a lesson, punches and head-butting the rabbit does, the stuck rabbit being swung around and around) are reminiscent of those found in a Zimbabwe-Botswana folktale.   Folklorists in the late 19th century first documented evidence that the American versions of the stories originated among enslaved West Africans based on connections between Br\'er Rabbit and Leuk, a rabbit trickster in Senegalese folklore.  The stories of Br\'er Rabbit were written down by Robert Roosevelt, an uncle of US President Theodore Roosevelt. Theodore Roosevelt wrote in his autobiography about his aunt from the State of Georgia, that "She knew all the \'Br\'er Rabbit\' stories, and I was brought up on them. One of my uncles, Robert Roosevelt, was much struck with them, and took them down from her dictation, publishing them in Harper\'s, where they fell flat. This was a good many years before a genius arose who, in \'Uncle Remus\', made the stories | Question: Who gets Brer Rabbit in a terrible tangle in the stories by Joel Chandler Harris?', ['Tar Baby', 'Tar-Baby', 'tar baby', 'tar "baby'], 'mrqa_triviaqa-validation-1616'), tensor(-0.0295)), (('Context: Virginia Wade - International Tennis Hall of FameVirginia Wade  Virginia Wade  7-time major champion, 6-time finalist  Career Titles  Member of the British Wightman Cup Team 1965-1985  Member of the winning team 1968, 1974, 1975, 1978  Fed Cup  Member of the British Federation Cup Team 1967-1970, 1972-1983  Overall Record: 66-33  Virginia Wade WTA Profile  Citizenship: GBR Born: July 10, 1945               in Bournemouth, England Played: Right-handed  Virginia Wade chose the perfect time to become the first British female to win the Wimbledon Ladies Singles Championship in 8 years and the last to win since.  In July 1977, the summer of the monarch’s Silver Jubilee, Wade won her third major title at the All England Club, with nicely coincided with Wimbledon’s centenary year. Queen Elizabeth II, who made no qualms about not being a tennis fan, was making only her second ever appearance at Centre Court when Wade met Betty Stove of the Netherlands for the championship. “If she’s [Queen Elizabeth II] going to be there, I am going to be there too,” Wade said.  Wade, then 31, and singles champion at the US Open in 1968 and at the Australian Open in 1972, wore a beautiful pink cardigan as she arrived on court. After dropping the first set 4-6, her whole demeanor turned a fiery red in winning the final two sets convincingly, 6-3, 6-1. Wade, who was playing in her 17th\xa0of an all-time record 26 Wimbledons, upset No. 1 seed Chris Evert, 6-2, 4-6, 6-1, in the semifinals to reach her one and only final. Once she settled into a groove after the first set, her precise groundstrokes controlled the match. “Winning Wimbledon was the thing that made my career worthwhile,” Wade told the Guardian in 1977. Those in attendance witnessed Queen Elizabeth II present Wade with championship trophy and then sang a rendition of “For She’s a Jolly Good Fellow” to celebrate the victory.  With her victory in London, Wade held the distinction of being the last Brit to win Wimbledon until Andy Murray in 2013. Her winnings were $20,499, compared to today’s multi-million dollar purse. In an interview with the\xa0Independent\xa0in 2007, Wade joked that her Wimbledon title came right on schedule. “Well, Angela Mortimer had won in 1961 and Ann Jones in 1969, so when I won in 1977 we all thought it happened every eight years, but maybe we were just anomalies, because there was Sue Barker and Joe Durie, but then the [British] players just petered out.”  Wade was a lithe 5-foot-8 steady stroke machine who had a beautiful all-court game built on a smooth slice backhand and a forehand that she could hit with topspin or flat. Wade was a thinker between the lines; her shots were patient and calculated. She was adept at controlling tempo and wouldn’t be forced into foolish shots. She would pause a minimum of five seconds before releasing her serve after getting into the ready position and hopped into position to move laterally or take a short ball and attack the net.  In her lengthy 26-year career, Wade won 55 singles titles, eighth on the all-time list. She favored playing at Wimbledon and the US Open the most of any of the four majors. As a 23-year old in 1968, she won the inaugural US Open, doing so as the No. 6 seed with a stunning and unexpected 6-4, 6-2 victory over No. 1 seed Billie Jean King. It was Wade’s fifth trip to the US major and although she would play the event another 15 times, her best finishes afterward were the semifinals in 1969, 1970, and 1975. The 1968 victory earned her $6,000. Wade only traveled to the Australian Open five times, but in 1972 stung another No. 1 seed when defeating crowd favorite and native Evonne Goolagong, who was in the midst of playing in seven consecutive Australian finals, | Question: Prior to Viginia Wade in 1977 who was the last British player to win a tennis Grand Slam title?', ['sue barker'], 'mrqa_triviaqa-validation-5406'), tensor(-0.0359)), (('Context: Kim Dong-wook  Kim Dong-wook (born July 29, 1983) is a South Korean actor.  After appearing in student short films and several minor parts, Kim became a star through his supporting role in the popular TV series "Coffee Prince" (2007), followed by box office hit "Take Off" (2009).  He then starred in "Happy Killers" (2010) and "Romantic Heaven" (2011), but it was his acclaimed performance as an obsessed and tormented king in 2012 period drama "The Concubine" that brought Kim the best reviews of his career yet.   The Concubine (film)  The Concubine (; lit.  "Royal Concubine: Concubine to the King") is a 2012 South Korean historical film directed by Kim Dae-seung.  Set in the Joseon Dynasty, it centers around Hwa-yeon (Jo Yeo-jeong), who becomes a royal concubine against her will, Kwon-yoo (Kim Min-joon), a man torn between love and revenge, and Prince Sung-won (Kim Dong-wook), who has his heart set on Hwa-yeon despite the countless women available to him.  These three characters form a love triangle which is ruled by dangerous passion.  The struggle to survive within the tight-spaced boundaries of the palace is intense, and only those who are strong enough to overcome the hell-like milieu can survive. | Question: What role did Kim Dong-wook play in the 2012 South Korean historical film directed by Kim Dae-seung?', ['an obsessed and tormented king'], 'mrqa_hotpotqa-validation-3971'), tensor(-0.0376)), (("Context: A piñata ( / pɪnˈjɑːtə / , US pronunciation / pɪnˈjɑːdɑː / , Spanish pronunciation : ( piˈɲata ) ( listen ) ) is a container often made of papier - mâché , pottery , or cloth ; it is decorated , and filled with small toys or candy , or both , and then broken as part of a ceremony or celebration . Piñatas are commonly associated with Mexico . The idea of breaking a container filled with treats came to Europe in the 14th century , where the name , from the Italian pignatta , was introduced . The Spanish brought the European tradition to Mexico , although there were similar traditions in Mesoamerica , such as the Aztecs ' honoring the birthday of the god Huitzilopochtli in mid December . According to local records , the Mexican piñata tradition began in the town of Acolman , just north of Mexico City , where piñatas were introduced for catechism purposes as well as to co-opt the Huitzilopochtli ceremony . Today , the piñata is still part of Mexican culture , the cultures of other countries in Latin America , as well as the United States , but it has mostly lost its religious character . | Question: where did the tradition of the pinata come from ?", ['treats'], 'mrqa_naturalquestions-validation-10680'), tensor(-0.0404)), (("Context: The United States two - dollar bill ( $2 ) is a current denomination of U.S. currency . The third U.S. President ( 1801 -- 09 ) , Thomas Jefferson , is featured on the obverse of the note . The reverse features an engraving of the painting The Declaration of Independence by John Trumbull . Throughout the $2 bill 's pre-1929 life as a large - sized note , it was issued as a United States Note , National Bank Note , silver certificate , Treasury or `` Coin '' Note and Federal Reserve Bank Note . When U.S. currency was changed to its current size , the $2 bill was issued only as a United States Note . Production went on until 1966 , when the series was discontinued . Ten years passed before the $2 bill was reissued as a Federal Reserve Note with a new reverse design . Two - dollar bills are seldom seen in circulation as a result of banking policies with businesses which has resulted in low production numbers due to lack of demand . This comparative scarcity in circulation , coupled with a lack of public knowledge that the bill is still in production and circulation , has also inspired urban legends and occasionally has created problems for people trying to use the bill to make purchases . | Question: when were 2 dollar bills stopped being made ?", ['current denomination of U.S. currency'], 'mrqa_naturalquestions-validation-2753'), tensor(-0.0526)), (('Context: Formula One drivers from Mexico  There have been six Formula One drivers from Mexico who have taken part in races since the championship began in 1950.  Pedro Rodríguez is the most successful Mexican driver being the only one to have won a grand prix.  Sergio Pérez, the only other Mexican to finish on the podium, currently races with Sahara Force India F1 Team .   Formula One drivers from Mexico  There have been six Formula One drivers from Mexico who have taken part in races since the championship began in 1950.  Pedro Rodríguez is the most successful Mexican driver being the only one to have won a grand prix.  Sergio Pérez, the only other Mexican to finish on the podium, currently races with Sahara Force India F1 Team .   Sergio Pérez  Sergio Pérez Mendoza (    ; born 26 January 1990) also known as "Checo" Pérez, is a Mexican racing driver, currently driving for Force India. | Question: Which other Mexican Formula One race car driver has held the podium besides the Force India driver born in 1990?', ['Pedro Rodríguez'], 'mrqa_hotpotqa-validation-14'), tensor(-0.0644)), (('Context: In anglophone academic works, theories regarding imperialism are often based on the British experience. The term "Imperialism" was originally introduced into English in its present sense in the late 1870s by opponents of the allegedly aggressive and ostentatious imperial policies of British prime Minister Benjamin Disraeli. It was shortly appropriated by supporters of "imperialism" such as Joseph Chamberlain. For some, imperialism designated a policy of idealism and philanthropy; others alleged that it was characterized by political self-interest, and a growing number associated it with capitalist greed. Liberal John A. Hobson and Marxist Vladimir Lenin added a more theoretical macroeconomic connotation to the term. Lenin in particular exerted substantial influence over later Marxist conceptions of imperialism with his work Imperialism, the Highest Stage of Capitalism. In his writings Lenin portrayed Imperialism as a natural extension of capitalism that arose from need for capitalist economies to constantly expand investment, material resources and manpower in such a way that necessitated colonial expansion. This conception of imperialism as a structural feature of capitalism is echoed by later Marxist theoreticians. Many theoreticians on the left have followed in emphasizing the structural or systemic character of "imperialism". Such writers have expanded the time period associated with the term so that it now designates neither a policy, nor a short space of decades in the late 19th century, but a world system extending over a period of centuries, often going back to Christopher Columbus and, in some accounts, to the Crusades. As the application of the term has expanded, its meaning has shifted along five distinct but often parallel axes: the moral, the economic, the systemic, the cultural, and the temporal. Those changes reflect - among other shifts in sensibility - a growing unease, even squeamishness, with the fact of power, specifically, Western power. | Question: What was the idealized value of imperialism?', ['idealism and philanthropy', 'philanthropy'], 'mrqa_squad-validation-10015'), tensor(-0.0803)), (("Context: New York StadiumThe New York Stadium, known as the AESSEAL New York Stadium for sponsorship purposes, is a football stadium in Rotherham, South Yorkshire, England. Opened in July 2012, it is the home ground of Rotherham United.  History  Rotherham United announced their intention to construct a new community stadium when they moved away from Millmoor to the Don Valley Stadium in May 2008 after a dispute with the ground owner Ken Booth.  In January 2010 the club purchased the former site of the Guest and Chrimes Foundry to be used for the new stadium.  Outline planning permission for the stadium was granted in November 2010, and the first images were sketched shortly after.   The name of the stadium was announced as the 'New York Stadium' on 19 December 2011, chosen ahead of 'The Foundry' and 'The Waterfront Stadium'. The reason for the name is that the area of land that the stadium lies upon is called New York, and it was thought that it would be better to name the stadium after history and/or where the stadium is situated, like nearby stadiums Bramall Lane and Hillsborough. Chairman Tony Stewart also hopes that the name could bring investment from New York City or further afield, as the New York Yankees chairman had recently said that he wanted to invest in an English football team.   Construction started in June 2011 and the stadium was officially opened by Prince Edward, Duke of Kent on 12 March 2012.  The first game played at the stadium was a pre-season match between Rotherham and Barnsley, held on 21 July 2012.  The Millers won 2–1; the first goal in the stadium was scored by Jacob Mellis of Barnsley, and David Noble scored Rotherham's first goal in their new home. The New York Stadium made its league debut on 18 August 2012, in which Rotherham beat Burton Albion 3–0,  Daniel Nardiello scoring the first competitive goal in the ground.  On 16 April 2014, the stadium held an England under-18s game for the first time. The resulting match finished with England beating Germany 2–1. Over 9,000 fans attended the game.   The naming rights to the stadium were announced as having been bought by local company AESSEAL, in a press conference on 21 November 2014. Club chairman Tony Stewart said the deal was worth six figures annually, as a result of the deal. It was also suggested as being the biggest sponsorship deal of the club's history.   Design  The stadium has a 12,000 all-seated capacity, with the option to be able to increase the stadium's capacity if needed.  It cost approximately £20 million to construct.  The stadium includes The 1925 Club, a corporate hospitality suite.  Local businesses such as Norton Finance  and Premier Hytemp  were some of the first members.  At the beginning  of the 2014–15 season, a large video screen was installed in the north west corner of the stadium.  Stands  ;North Stand  The North Stand, known as the KCM Recycling Stand for sponsorship reasons, and often referred to as the New Tivoli, is the kop stand of the stadium. The KCM Recycling Stand holds 2000 home fans, and has the lettering of the club's initials—RUFC—in white across it. The stand sits behind one of the goals, opposite the away end.  ;West Stand  The West Stand, known as the Eric Twigg Pukka Pies Stand for sponsorship reasons, is the main stand of the stadium. It features the executive 1925 Lounge, and also the stand where the players walk under when entering the field of play. It holds 4000 home fans.  ;East Stand  The East Stand, known as the Ben Bennett Stand, is the family stand of the stadium. It holds 4000 home fans, as well as two built-in balcony-type structures for disabled people.  ;South Stand  The South Stand, known as the Morrison Stand, is a 2000 seated away stand. It sits behind a goal, with the family stand to the right, main stand to the left, and the kop directly opposite.  Records  *Record | Question: Which League 2 football team play home games at the New York Stadium?", ['rotherham united'], 'mrqa_triviaqa-validation-4856'), tensor(-0.0899)), (('Context: Genghis Khan, the title is spelled in variety of ways in different languages such as Mongolian Chinggis Khaan, English Chinghiz, Chinghis, and Chingiz, Chinese: 成吉思汗; pinyin: Chéngjísī Hán, Turkic: Cengiz Han, Çingiz Xan, Çingiz Han, Chingizxon, Çıñğız Xan, Chengez Khan, Chinggis Khan, Chinggis Xaan, Chingis Khan, Jenghis Khan, Chinggis Qan, Djingis Kahn, Russian: Чингисхан (Čingiskhan) or Чингиз-хан (Čingiz-khan), etc. Temüjin is written in Chinese as simplified Chinese: 铁木真; traditional Chinese: 鐵木眞; pinyin: Tiěmùzhēn. | Question: What are alternate English spelling of Genghis?', ['Chinghiz, Chinghis, and Chingiz'], 'mrqa_squad-validation-6303'), tensor(-0.0943)), (('Context: The rainforest contains several species that can pose a hazard. Among the largest predatory creatures are the black caiman, jaguar, cougar, and anaconda. In the river, electric eels can produce an electric shock that can stun or kill, while piranha are known to bite and injure humans. Various species of poison dart frogs secrete lipophilic alkaloid toxins through their flesh. There are also numerous parasites and disease vectors. Vampire bats dwell in the rainforest and can spread the rabies virus. Malaria, yellow fever and Dengue fever can also be contracted in the Amazon region. | Question: What fish living in the Amazon river is known to bit humans?', ['piranha'], 'mrqa_squad-validation-4185'), tensor(-0.0961)), (('Context: That Was the Week That WasThat Was the Week That Was, informally TWTWTW or TW3, was a satirical television comedy programme on BBC Television in 1962 and 1963. It was devised, produced and directed by Ned Sherrin and presented by David Frost. An American version by the same name aired on NBC from 1964 to 1965, also featuring Frost.  The programme is considered a significant element of the satire boom in the UK in the early 1960s. It broke ground in comedy through lampooning the establishment and political figures. Its broadcast coincided with coverage of the politically charged Profumo affair and John Profumo, the politician at the centre of the affair, became a target for derision. TW3 was first broadcast on Saturday, 24 November 1962.  Cast and writers  Cast members included cartoonist Timothy Birdsall, political commentator Bernard Levin, and actors Lance Percival, who sang topical calypsos, many improvised to suggestions from the audience, Kenneth Cope, Roy Kinnear, Willie Rushton, Al Mancini, Robert Lang,  David Kernan and Millicent Martin. The last two were also singers and the programme opened with a song\xa0– That Was The Week That Was\xa0– sung by Martin to Ron Grainer\'s theme tune and enumerating topics in the news. Frankie Howerd also guested with stand-up comedy.   Script-writers included John Albery, John Antrobus, John Betjeman, John Bird, Graham Chapman, John Cleese, Peter Cook, Roald Dahl, Richard Ingrams, Lyndon Irving, Gerald Kaufman, Frank Muir, David Nobbs, Denis Norden, Bill Oddie, Dennis Potter, Eric Sykes, Kenneth Tynan, and Keith Waterhouse.   Programme  The programme opened with a song ("That was the week that was, It\'s over, let it go ...") sung by Millicent Martin, referring to news of the week just gone. Lance Percival sang a topical calypso each week. Satirical targets, such as Prime Minister Harold Macmillan and Home Secretary Henry Brooke were lampooned in sketches, debates and monologues. Other targets were the monarchy, Britain\'s declining status as a global power, racism (particularly in the American South and South Africa under Apartheid), sexual and social hypocrisy, the class system, and the BBC itself. Well-remembered sketches include a \'consumers\' guide to religion\', which discussed relative merits of faiths in the manner of a Which? magazine report.  On Saturday, 20 October 1962 the award of Nobel prizes to John Kendrew and Max Perutz, and to Francis Crick, James D. Watson, and Maurice Wilkins was satirised in a short sketch with the prizes referred to as the Alfred Nobel Peace Pools; in this sketch Watson was called "Little J.D. Watson" and "Who\'d have thought he\'d ever get the Nobel Prize? Makes you think, doesn\'t it". The germ of the joke was that Watson was only 25 when he helped discover DNA; much younger than the others.  TW3 was broadcast on Saturday night and attracted an audience of 12 million. It often under- or overran as cast and crew worked through material as they saw fit. At the beginning of the second season in the autumn of 1963, in an attempt to assert control over the programme, the BBC scheduled repeats of The Third Man television series after the end of TW3. Frost suggested a means of sabotaging this tactic to Sherrin, and he agreed. For three weeks, Frost read out the plot of The Third Man, until the repeats were abandoned following the direct intervention of the BBCs Director General Hugh Greene.   Frost often ended a satirical attack with the remark "But seriously, he\'s doing a grand job".  At the end of each episode, Frost usually signed off with: "That was the week, that was." At the end of the final programme he announced: "That was That Was The Week That Was...that was."  Kennedy tribute  For the edition on Saturday, 23 November 1963, the day after the assassination of United States President John F. Kennedy, TW3 produced a shortened 20-minute programme with no satire, reflecting on the loss, including a contribution from Dame | Question: Who was the deviser, producer and director \'That Was The Week That Was\'?', ['ned sherrin'], 'mrqa_triviaqa-validation-4054'), tensor(-0.1427)), (('Context: Cor, Blimey!  Cor, Blimey!  is a 2000 TV film that follows the relationship between "Carry On" film actors Sid James (played by Geoffrey Hutchings) and Barbara Windsor (played by Samantha Spiro).   Samantha Spiro  Samantha Spiro (born 20 June 1968) is a double Olivier Award-winning English actress.  She is best known for portraying Barbara Windsor in the stage play "Cleo, Camping, Emmanuelle and Dick" and the television film "Cor, Blimey! ", DI Vivien Friend in "", and Melessa Tarly in the HBO series "Game of Thrones". | Question: Who plays opposite the double Olivier Award-winning English actress in the 2000 TV film following the relationship between "Carry On" film actors Sid James and Barbara Windsor?', ['Geoffrey Hutchings'], 'mrqa_hotpotqa-validation-2970'), tensor(-0.1835)), (('Context: Evolution of the adaptive immune system occurred in an ancestor of the jawed vertebrates. Many of the classical molecules of the adaptive immune system (e.g., immunoglobulins and T cell receptors) exist only in jawed vertebrates. However, a distinct lymphocyte-derived molecule has been discovered in primitive jawless vertebrates, such as the lamprey and hagfish. These animals possess a large array of molecules called Variable lymphocyte receptors (VLRs) that, like the antigen receptors of jawed vertebrates, are produced from only a small number (one or two) of genes. These molecules are believed to bind pathogenic antigens in a similar way to antibodies, and with the same degree of specificity. | Question: Evolution of what part of the immune system occurred in the evolutionary ancestor of jawed vertebrates?', ['adaptive', 'adaptive immune system', 'the adaptive immune system'], 'mrqa_squad-validation-6677'), tensor(-0.2543)), (("Context: What we now call gravity was not identified as a universal force until the work of Isaac Newton. Before Newton, the tendency for objects to fall towards the Earth was not understood to be related to the motions of celestial objects. Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object. Today, this acceleration due to gravity towards the surface of the Earth is usually designated as  and has a magnitude of about 9.81 meters per second squared (this measurement is taken from sea level and may vary depending on location), and points toward the center of the Earth. This observation means that the force of gravity on an object at the Earth's surface is directly proportional to the object's mass. Thus an object that has a mass of  will experience a force: | Question: Who came up with the concept that falling objects fell at the same speed regardless of weight?", ['Galileo'], 'mrqa_squad-validation-10410'), tensor(-0.3367)), (('Context: Sunday in the Park with George - Music Theatre InternationalSunday In The Park... | Music Theatre International  Music Theatre International  Request Licenses & Perusals, Pay Invoices  Your Web Profile  We are working quickly to merge our two logins.  Region  Sunday In The Park With George  Original Broadway Version (1984)  Inspired by George Seurat\'s famous painting, this poetic masterpiece explores the challenges in understanding life and art.  Inspired by the painting A Sunday Afternoon on the Island of La Grande Jatte by Georges Seurat, Sunday In The Park With George, Stephen Sondheim and James Lapine\'s stunning masterpiece merges past and present into beautiful, poignant truths about life, love and the creation of art. One of the most acclaimed musicals of our time, this moving study of the enigmatic painter Georges Seurat won a Pulitzer Prize and was nominated for an astounding 10 Tony Awards including Best Musical.  The days leading up to the completion of his most famous painting, A Sunday Afternoon on the Island of La Grande Jatte, Georges Seurat is struggling to make meaningful art and maintaining a relationship with his lover Dot. Amid the scorn of the artistic community, Seurat\'s artistic ability thrives while his love diminishes. A century later, Seurat\'s descendant - named George and also an artist - finds himself burnt out and in search of what artistic path to follow, but he finds the answer to his future in the past.  An ensemble of strong singing actors performs this challenging and heartbreaking work about our need to connect to the past, present and future. Sunday In The Park With George features two coveted starring roles made famous by the Broadway performances of Mandy Patinkin and Bernadette Peters. The show may be staged simply as in its original workshop production or with the grandeur of Seurat\'s masterpiece.  "Sunday in the Park with George" - About.com Education"Sunday in the Park with George"  "Sunday in the Park with George"  "Sunday in the Park with George"  From Canvas to Characters  By Rosalind Flynn  Updated February 27, 2016.  Anyone who reads or plans to attend a performance of Sunday in the Park With George by Stephen Sondheim and James Lapine should first spend some time looking at an image of the this painting by Georges Seurat :\xa0 Its original French title is Un dimanche après-midi à l\'Île de la Grande Jatte – English translation: “ Sunday Afternoon on the Island of la Grande Jatte .” (Click on the painting title to view the online image.)  The first act of Sunday in the Park With George occurs with Seurat the artist creating and perfecting his painting. The creative musical theatre artists who were inspired by this painting drew the other characters in Act One from figures in the painting.  Looking Closely at Art  It would not be surprising to learn that Sondheim and Lapine used their own version of Visual Thinking Strategies to examine this painting that is 10 feet wide and 6 and a half feet tall. Visual Thinking Strategies provide ways to look closely at works of art by asking observers to respond to these three questions:  continue reading below our video  10 Best Universities in the United States  1. What’s going on in this picture?  2. What do you see that\xa0makes you say that?  3. What more can we find?  In fact, it would be excellent to begin a study of this play by engaging students in doing precisely that – looking at an image of “Sunday Afternoon on the Island of la Grande Jatte” and asking them to respond to those questions.  What’s Going On? According to Sondheim and Lapine  The painting’s setting is the island of la Grande Jatte, which is located northwest of Paris in the Seine River. In 1884 (when Seurat began working on his painting), the island was a pastoral place where people could go to get away from city life. The musical’s creative artists pulled their characters from the canvas. Here’s a Who’s Who:  The most prominent figure is the woman in profile on the bottom right hand side. Sondheim and Lapine imagined her as Seurat’s mistress and they | Question: The musical \'Sunday In The Park With George\' was inspired by a painting by which artist?', ['georges seurat', 'seurat'], 'mrqa_triviaqa-validation-5937'), tensor(-0.3411)), (('Context: Newton came to realize that the effects of gravity might be observed in different ways at larger distances. In particular, Newton determined that the acceleration of the Moon around the Earth could be ascribed to the same force of gravity if the acceleration due to gravity decreased as an inverse square law. Further, Newton realized that the acceleration due to gravity is proportional to the mass of the attracting body. Combining these ideas gives a formula that relates the mass () and the radius () of the Earth to the gravitational acceleration: | Question: How might gravity effects be observed differently according to Newton?', ['at larger distances', 'at larger distances.'], 'mrqa_squad-validation-10322'), tensor(-0.3848)), (('Context: Quran (Complete) - YouTubeQuran (Complete)  - YouTube  Quran (Complete)  38,356 views  Last updated on Jun 30, 2014  The Qur\'an (English pronunciation: /kɒˈrɑːn/ kor-AHN; Arabic: القرآن\u200e al-qur\'ān, IPA: [qurˈʔaːn], literally "the recitation") is the religious text of Islam,[ also sometimes transliterated as Quran, Kuran, Koran, Qur\'ān, Coran or al-Qur\'ān. It is widely regarded as the finest piece of literature in the Arabic language. Muslims hold that the Qur\'an is the verbal divine guidance and moral direction for mankind. Muslims also consider the original Arabic verbal text to be the final revelation of God the Final Testament  Quran: 78. Surat An Naba\' (The Tidings ) with ... - YouTubeQuran: 78. Surat An Naba\' (The Tidings ) with English Audio Translation and Transliteration HD - YouTube  Quran: 78. Surat An Naba\' (The Tidings ) with English Audio Translation and Transliteration HD  Want to watch this again later?  Sign in to add this video to a playlist.  Need to report the video?  Sign in to report inappropriate content.  The interactive transcript could not be loaded.  Loading...  Rating is available when the video has been rented.  This feature is not available right now. Please try again later.  Published on May 14, 2015  Please support the channel by subscribing, liking and commenting, may Allah reward you with Godness, Ameen https://www.youtube.com/user/kuranreader Islam, Prophet, prophets, Muhammad, Mohamed, Quran, Kuran, Kuranreader, Surat, Chapter, Ayah, Ayat, Sheikh, imam, Emotional, Truth, Emotional, relaxing, Soothing, Recitation, reading, Religion, Religious Text, Amazing , Beautful, Quran English Translation, Transliteration,beautiful quran recitation, quran, koran,kuran reader,quran reader,really beautiful,heart touchig ,relaxing,soothing,heart trembling,good voice, al minshawi, tajweed,emotional,tearful, quran download,surat,surah,most beautiful,sudais, abd albasit,mishari,quran reading ,learn arabic,learn quran,, قراءة, تلاوة , القارىء , مكتوب , مكتوبة, ترجمة , انجليزية,مرتل , مرتل, ,  Quran,Al Quran,the quran,The Holy Quran,Holy Quran,Quran Translation,Al Quran Translation,The Holy Quran Translation,Holy Quran Translation,Quran Bangla,Bangla Quran,Al Quran Bangla,Bangla Al Quran,Quran Bangla Translation,Bangla Quran Translation,Al Quran Bangla Translation,Koran,coran,Surah,Translatio\xadn,Islam,Abdul Rahman Al-Sudais,Imam,quran recitation,urdu quran,full quran,beautiful quran,quran beautiful recitation,urdu quran translation,quran karim,quran with urdu,tilawat,bacaan quran,bacaan al quran,merdu al quran,full al quran,surah al quran,suara merdu,ayat al quran,alquran,tilawah,tilawah al quran,quran recitation beautiful,beautiful quran,al quran recitation,best quran recitation,best quran,the quran recitation,full quran,quran recitation full,kicked the quran,the quran miracles,quran miracles,burning the quran,quran burning,miracles of quran,azab,burn the quran,the quran recitation,the quran full,full quran recitation,sudais full quran,mishary full quran,full quran download,full quran tilawat,beautiful quran recitation,quran recitation,beautiful recitation,most beautiful quran,best quran recitation,quran surah baqarah,quran surah yaseen,quran surah yasin,tilawat quran,quran surah rahman,quran surah kahf,quran surah rehman,quran surah fatiha,quran surah mulk,al quran karim,quran mp3,quran karim mp3,quran karim full,quran karim soudais,quran karim sudais,quran sudais,quran karim urdu,koran karim,coran karim,le coran,coran recitation,al coran,sourate coran,sourate,el coran,soudais,soudais coran,coran francais,coran islam,coran en francais,ecouter le coran,le coran arabe,le coran sourate,coran sourate,recite le coran,apprendre le coran, | Question: The Qur\'an (or Quran, Kuran, Koran, Coran or al-Qur\'an - literally \'the recitation\') is the religious text of which religion?', ['islam'], 'mrqa_triviaqa-validation-2210'), tensor(-0.3867)), (("Context: Doctor Who has appeared on stage numerous times. In the early 1970s, Trevor Martin played the role in Doctor Who and the Daleks in the Seven Keys to Doomsday. In the late 1980s, Jon Pertwee and Colin Baker both played the Doctor at different times during the run of a play titled Doctor Who – The Ultimate Adventure. For two performances, while Pertwee was ill, David Banks (better known for playing Cybermen) played the Doctor. Other original plays have been staged as amateur productions, with other actors playing the Doctor, while Terry Nation wrote The Curse of the Daleks, a stage play mounted in the late 1960s, but without the Doctor. | Question: What was the name of the Doctor Who play from the 1980's?", ['Doctor Who – The Ultimate Adventure'], 'mrqa_squad-validation-7821'), tensor(-0.6166)), (("Context: The White Coat Ceremony ( WCC ) is a relatively new ritual in some medical ( MD , DO ) , dental , optometry , audiology , chiropractic , dietetic , occupational therapy , physical therapy , podiatric , pharmacy , physician assistant , pathologists ' assistant , nursing , naturopathic and veterinary schools that marks the student 's transition from the study of preclinical to clinical health sciences . At some schools , where students begin meeting patients early in their education , the white coat ceremony is held before the first year begins . It is an example of a matriculation . | Question: when do you get your white coat in pharmacy school ?", ["the student 's transition from the study of preclinical to clinical health sciences"], 'mrqa_naturalquestions-validation-5465'), tensor(-0.6917)), (('Context: Catherine Zeta Jones in "The Darling Buds of May" 1991 ...Catherine Zeta Jones in "The Darling Buds of May" 1991 - YouTube  Catherine Zeta Jones in "The Darling Buds of May" 1991  Want to watch this again later?  Sign in to add this video to a playlist.  Need to report the video?  Sign in to report inappropriate content.  The interactive transcript could not be loaded.  Loading...  Rating is available when the video has been rented.  This feature is not available right now. Please try again later.  Uploaded on Mar 31, 2007  Some clips of a young Catherine Zeta Jones as Mariette, in the tv series, "The Darling Buds of May".  "The Darling Buds of May" was a British television series first broadcast in 1991. It is set in an idyllic rural 1950s Kent, among a large, boisterous family. The show was very popular, and launched the acting career of Catherine Zeta Jones.  Category  Mariette - The Darling Buds of May (UK) Characters - ShareTVMariette - The Darling Buds of May (UK) Characters - ShareTV  Catherine Zeta-Jones (born September 25, 1969, in Swansea, West Glamorgan, Wales, UK, the daughter of ...  Character Bio  Mariette Larkin, the eldest Larkin child. Her name was created by  combining \'Marie\' and \'Antoinette\'. In the first episode, she is shown  to be the family beauty and slightly wild. However, she quickly marries  Charley and settles down with him for the rest of the series. She  appears to have inherited Pop\'s business sense, and at the end of the  series she and Charley buy and manage a local brewery. Her wedding  ceremony reveals her middle name as Jane.  Episode Screenshots  Profile: Catherine Zeta Jones - BBC NewsProfile: Catherine Zeta Jones - BBC News  BBC News  Close share panel  Image caption Catherine Zeta Jones became a household name after appearing in The Darling Buds of May  The showbusiness journey of Catherine Zeta Jones, a permanent fixture on the A-list for decades, has taken her from her home in Wales to the Hollywood hills.  Along the way, the 41-year-old has picked up an Oscar, a Tony, a CBE, countless plaudits and a smattering of bad reviews.  She married Hollywood actor Michael Douglas in a lavish New York ceremony in 2000, however the couple have now decided to take some time apart to "evaluate and work on their marriage".  Few could have predicted that a girl born to a sweet factory owner and a seamstress would go on to become one of the most famous actresses in the world.  At the age of 10, the Welsh star won a national talent contest singing a Shirley Bassey song before landing roles in various West End productions.  Image caption Zeta Jones started acting at a very young age  She got her big break at the age of 17, following a promotion from second understudy to the lead role in the hit musical 42nd Street.  But it was Zeta Jones\'s first major TV role, in the 1991 comedy drama The Darling Buds of May, that made her a household name.  Set in rural Kent in the 1950s, viewers fell in love with the Larkins, played by Sir David Jason and Pam Ferris.  Zeta Jones played their fresh-faced sweetheart daughter Mariette.  Squeaky-clean perception  Off camera, her personal life began to hit the headlines more than her acting ability.  Her relationship with Blue Peter presenter John Leslie was well documented. She also became involved with Simply Red star Mick Hucknall and Soldier Soldier actor Angus MacFadyen.  After leaving The Darling Buds of May, Zeta Jones turned her attention to film, but admitted finding it hard to shake off the public\'s squeaky-clean perception of her.  The thing that really upset me was the idea that I was this gold-digger  Catherine Zeta Jones answers the critics about her marriage to Michael Douglas  "I am afraid I have this image which is far | Question: What was the name of Catherine Zeta Jones character in The Darling Buds of May ?', ['mariette'], 'mrqa_triviaqa-validation-7018'), tensor(-0.7154)), (('Context: Nuclear power in space is the use of nuclear power in outer space , typically either small fission systems or radioactive decay for electricity or heat . Another use is for scientific observation , as in a Mössbauer spectrometer . One common type is a radioisotope thermoelectric generator , which has been used on many space probes and on manned lunar missions , and another is small fission reactors for Earth observation satellites such as the TOPAZ nuclear reactor . A radioisotope heater unit provides heat from radioactive decay of a material and can potentially produce heat for decades . | Question: how have scientists used nuclear power as a fuel for spacecraft ?', ['either small fission systems or radioactive decay for electricity or heat'], 'mrqa_naturalquestions-validation-3828'), tensor(-0.8124)), (('Context: George Cross | British medal | Britannica.comGeorge Cross | British medal | Britannica.com  British medal  Academy Award  George Cross, a British civilian and military decoration, instituted in 1940 by King George VI for “acts of the greatest heroism or of the most conspicuous courage in circumstances of extreme danger.” The award, which can be conferred posthumously, is usually given to civilians, although it can be bestowed on military personnel for acts for which military decorations are not usually awarded. The George Cross superseded the Medal of the Order of the British Empire for Gallantry (commonly known as the Empire Gallantry Medal).  George Cross medal engraved on a tombstone.  Acmthompson  The island of Malta received the George Cross in recognition of its inhabitants’ gallantry in World War II. Recipients of this award may add G.C. after their names; the cross ranks second only to the Victoria Cross (the highest British military decoration). The cross is silver, with one side depicting St. George slaying the dragon and with the inscription “For Gallantry;” the other side gives the recipient’s name and the date of the award.  The George Medal, instituted at the same time as the George Cross, is analogous to it but is awarded for services not quite so outstanding as those which merit the George Cross. Recipients of this medal can add G.M. after their names. The medal is silver; one side has the effigy of the reigning British monarch, and the other side has St. George and the dragon with the inscription “The George Medal.”  Learn More in these related articles:  World War 2 Awards.com - George CrossWorld War 2 Awards.com - George Cross  George Cross  Number of awards in the database: 139  Total awarded for WW II:    139  Recipients A-Z  The George Cross -or GC- was instituted June 24th, 1941, by King George VI. This decision was actuated by an air raid on London in 1940 during which the Royal Palace was hit. After this incident, King George became emotionally more involved with his people and he realised what they had to go through during the raids, admiring even more their hardships and heroism. As a result, the George Cross and George Medal were largely designed by him.  The George Cross is the second highest British decoration and is awarded for "acts of the greatest heroism and bravery in circumstances of extreme danger". It was initially awarded to all citizens of the Commonwealth and was later extended to military personnel for actions that did not usually merit a military decoration. Recipients are entitled to add the letters GC after their names. Bars will be awarded for subsequent acts of bravery and the decoration can be awarded posthumously.  The GC is the successor to the Medal of the Order of the British Empire for Gallantry and its recipients were obliged to return it to the Central Chancellery of the Orders of Knighthood to have it replaced by a George Cross. In 1971, recipients of the Albert Medal could also exchange this for a George Cross.  The badge is made of silver. On the front is an image of St. George fighting the dragon surrounded by the inscription \'For Gallantry\', on the reverse are engraved the name of the recipient and the date of the award. Each arm of the cross bears the cypher of the reigning Monarch, during the war GRI or GRVI, after the war EIIR. The cross hangs from a silver bar adorned with laurel leaves by a silver ring. The ribbon is 1.75" (38 mm) wide and coloured blue with a miniature GC in its centre.  The George Cross was conferred upon the Island of Malta for the display of bravery of its citizens during the war. Four times a George Cross was conferred upon a woman, three of which posthumously. Three women were awarded the GC for resistance activities in occupied  territories during WW 2. One of them was Noor Inayat Khan. This Russian born woman was dropped over France July 16th, 1943, to assist the French resistance movement in their activities. Some three months later, she was betrayed and taken prisoner by the Gestapo (German Secret Police). Despite numerous interrogations she | Question: What is the inscription on the George Cross ?', ['for gallantry'], 'mrqa_triviaqa-validation-2096'), tensor(-1.5120))]
09/23/2021 13:07:39 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-2376', 'mrqa_naturalquestions-validation-2248', 'mrqa_hotpotqa-validation-1968', 'mrqa_triviaqa-validation-1924', 'mrqa_triviaqa-validation-1551', 'mrqa_naturalquestions-validation-2730', 'mrqa_hotpotqa-validation-5325', 'mrqa_triviaqa-validation-1437', 'mrqa_triviaqa-validation-5972', 'mrqa_squad-validation-1516', 'mrqa_triviaqa-validation-6351', 'mrqa_squad-validation-5622', 'mrqa_hotpotqa-validation-3241', 'mrqa_hotpotqa-validation-1201', 'mrqa_naturalquestions-validation-6157', 'mrqa_triviaqa-validation-5026', 'mrqa_naturalquestions-validation-8948', 'mrqa_hotpotqa-validation-5802', 'mrqa_naturalquestions-validation-7511', 'mrqa_naturalquestions-validation-7017', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-7369', 'mrqa_triviaqa-validation-3915', 'mrqa_squad-validation-392', 'mrqa_squad-validation-7746', 'mrqa_hotpotqa-validation-409', 'mrqa_naturalquestions-validation-1364', 'mrqa_squad-validation-3478', 'mrqa_naturalquestions-validation-9688', 'mrqa_naturalquestions-validation-3028', 'mrqa_hotpotqa-validation-400', 'mrqa_hotpotqa-validation-1626']
09/23/2021 13:07:39 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:07:39 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=62) .... Timecode: 2
09/23/2021 13:07:51 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:07:51 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 2
09/23/2021 13:07:54 - INFO - __main__ - After Error Fixing: 0.96875
09/23/2021 13:07:54 - INFO - __main__ - Instant Fixing Rate: 0.9666666666666667
09/23/2021 13:07:54 - INFO - __main__ - Instant Retention Rate: 0.999999995
09/23/2021 13:07:56 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_003.pt.
09/23/2021 13:07:56 - INFO - __main__ - Saving the current error examples (len=30) to the memory.
09/23/2021 13:07:56 - INFO - __main__ - Current memory size: 58.
09/23/2021 13:07:56 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:07:56 - INFO - __main__ - Finished.
09/23/2021 13:07:56 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:07:56 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:07:56 - INFO - __main__ - Evaluating to get errors .... Timecode: 3
09/23/2021 13:07:58 - INFO - __main__ - Before Error Fixing: 0.0625
09/23/2021 13:07:58 - INFO - __main__ - Found 30 errors.
09/23/2021 13:07:58 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:07:58 - INFO - __main__ - Current memory size: 88.
09/23/2021 13:07:58 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=88;
09/23/2021 13:08:02 - INFO - __main__ - interference_scores=[(("Context: An acetate / ˈæsɪteɪt / is a salt formed by the combination of acetic acid with an alkaline , earthy , or metallic base . `` Acetate '' also describes the conjugate base or ion ( specifically , the negatively charged ion called an anion ) typically found in aqueous solution and written with the chemical formula C H O . The neutral molecules formed by the combination of the acetate ion and a positive ion ( called a cation ) are also commonly called `` acetates '' ( hence , acetate of lead , acetate of aluminum , etc . ) . The simplest of these is hydrogen acetate ( called acetic acid ) with corresponding salts , esters , and the polyatomic anion CH CO , or CH COO . | Question: what is the name for the ch3coo - ion ?", ['polyatomic anion'], 'mrqa_naturalquestions-validation-1202'), tensor(2.7141)), (("Context: BLACK DOG: Mitchell - now it's snobgate | Daily Mail OnlineBLACK DOG: Mitchell - now it's snobgate | Daily Mail Online  BLACK DOG: Mitchell - now it's snobgate  comments  As revealed in these columns before his downfall in the 'Plebgate' scandal, Tory MP Andrew Mitchell was known as 'Thrasher' in his days as head boy of Rugby School. Dog is now intrigued to learn he had a second nickname: 'Mitchell-snob'.  A fellow Old Rugbeian observes drily: 'It had the advantage that you could drop the hyphen and insert an apostrophe between the 's' and 'n' of snob.'  Tory MP Andrew Mitchell was known as 'Thrasher' in his days as head boy of Rugby School  When Mitchell ran Tory pal David Davis's Conservative leadership campaign in 2005, political broadcaster Michael Cockerell asked Mitchell: 'What do you say to those who argue Davis is Iain Duncan Smith with hair?'\xa0  Adopting a pantomime-villain glower, Mitchell hissed: 'Tell me their names and I'll sue.' How he must wish he had only joked about suing The Sun over PC Toby Rowland.  Hot 'babe' Pepi gets hooked on Vegas\xa0  Ex MoD official Pepi Simpson sashayed in to hear a husky American accent drawl in her direction: 'That's what I call a babe'  A chance visit to a high rollers' bar in a Las Vegas hotel while on holiday with a girlfriend had unintended consequences for Pepi Simpson, sporty wife of Tory MP Keith Simpson. Ex-MoD official Pepi, pictured in the bar, dolled up after a visit to the crimper, sashayed in to hear a husky American accent drawl in her direction: 'That's what I call a babe!' No one told her it was a hookers' pick-up joint. Pepi replied tartly: 'Not bad for 61.'  Hats off to gutsy Andrew Marr, who has thrown away his walking stick nearly two years after a debilitating stroke. In between hosting his Sunday morning TV show and gruelling physiotherapy sessions that set him on the road to recovery, Marr has just completed a second novel and is working on a poetry project. Dog would like to know what he has for breakfast.  On hearing that Alex Salmond was among guests at the Spectator magazine Parliamentary Awards lunch at The Savoy, veteran Tory Norman Tebbit mused he had always wanted to meet the now ex-SNP leader. When a fellow guest asked why, out came 'Chingford Skinhead' Tebbit's razor: 'I've always wanted to sprinkle salt over him to see if he dissolves into slime like the slugs in my garden.'  Theresa May reveals all\xa0  The Spectator's attempt to keep guests in suspense over the winner of the Parliamentarian of the Year award was ruined when Theresa May opened proceedings with a jokey video presentation and pressed the wrong button on her remote control.\xa0  Up flashed the winner's name: Edward VII lookalike and former Commons Clerk Sir Robert Rogers, who was forced to quit after falling out with Speaker John Bercow.  It seems John Bercow has paid a heavy price for falling out with Sir Robert. David Cameron rubbed his nose in it by giving Sir Robert a peerage – and there's worse to come. Another of Bercow's tormentors, former Speaker Betty Boothroyd, is tipped to be at Sir Robert's side as one of his two 'sponsors' when he takes his seat in the Upper House.\xa0  \xa0  The cuddly side of the Chingford Skinhead - Daily Mail OnlineNorman Tebbit's as savage as ever except about wife and the dog who inspired new book | Daily Mail Online  The cuddly side of the Chingford Skinhead: Cameron? Silly. Blair? Offensive. Cherie? Urggh. Norman Tebbit's as savage as ever - except about the wife he adores and the dog who inspired his startling new book  Norman Tebbit has published a children's book about a dog called Ben  The former Minister on Maria Miller: 'It offends against common sense'  On Farage: 'A clever operator. He had touched on | Question: Which ex-politician, now aged 80, had the nickname 'The Chingford Skinhead' ?", ['norman tebbit'], 'mrqa_triviaqa-validation-1924'), tensor(1.5899)), (("Context: Kenya is active in several sports, among them cricket, rallying, football, rugby union and boxing. The country is known chiefly for its dominance in middle-distance and long-distance athletics, having consistently produced Olympic and Commonwealth Games champions in various distance events, especially in 800 m, 1,500 m, 3,000 m steeplechase, 5,000 m, 10,000 m and the marathon. Kenyan athletes (particularly Kalenjin) continue to dominate the world of distance running, although competition from Morocco and Ethiopia has reduced this supremacy. Kenya's best-known athletes included the four-time women's Boston Marathon winner and two-time world champion Catherine Ndereba, 800m world record holder David Rudisha, former Marathon world record-holder Paul Tergat, and John Ngugi. | Question: What sports are Kenyans active in?", ['cricket, rallying, football, rugby union and boxing'], 'mrqa_squad-validation-8542'), tensor(0.9561)), (('Context: The history of agriculture records the domestication of plants and animals and the development and dissemination of techniques for raising them productively . Agriculture began independently in different parts of the globe , and included a diverse range of taxa . At least eleven separate regions of the Old and New World were involved as independent centers of origin . | Question: where did the cultivation of agriculture first arise ?', ['eleven separate regions of the Old and New World'], 'mrqa_naturalquestions-validation-8119'), tensor(0.7349)), (('Context: Important production centres today are the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ) , the Yakima ( Washington ) and Willamette ( Oregon ) valleys , and western Canyon County , Idaho ( including the communities of Parma , Wilder , Greenleaf , and Notus ) . The principal production centres in the UK are in Kent ( which produces Kent Goldings hops ) , Herefordshire , and Worcestershire . Essentially all of the harvested hops are used in beer making . | Question: where do they grow hops in the us ?', ['Washington', 'Oregon', 'Idaho'], 'mrqa_naturalquestions-validation-2248'), tensor(0.6656)), (('Context: Amphitrite in Greek Mythology | MythographyAmphitrite in Greek Mythology | Mythography  Amphitrite in Greek Mythology  Home | Greek Gods | Free Spirits | Amphitrite  In Greek mythology, Amphitrite was a sea goddess. Sources state that she was the daughter of Nereus and Doris, which makes Amphitrite one of the fifty sea goddesses (or nymphs) who are known collectively as the Nereids. The Greek poet Hesiod lists the names of these sea nymphs in his Theogony:  “To Nereus and Doris of the lovely hair,  the daughter of Okeanos, the stream surrounding the earth,  a host of godly daughters was born in the barren sea:  Proto, Eukrante, Amphitrite, and Sao…”  (Hesiod, Theogony, lines 240-43)  According to the legend, the Olympian Poseidon (who was himself a god of the sea) saw Amphitrite one day and fell in love with the graceful and beautiful goddess. However, the sea nymph initially resisted Poseidon’s advances. But Poseidon was not easily discouraged. He sent his sea creatures to find Amphitrite. A dolphin succeeded in locating the Nereid, and it was this creature - the dolphin - who persuaded her to consider Poseidon’s proposal. Eventually, Amphitrite accepted Poseidon, and the two gods were married.  Amphitrite and Poseidon were together the parents of several children, including the sea god Triton. Here’s Hesiod again:  “From the union of rumbling Poseidon and Amphitrite  came the great Triton, whose might is far-flung,  an awesome god dwelling in a golden house that lies  at the sea’s bottom, near his cherished mother and lordly father.”  (Hesiod, Theogony, lines 930 ff. )  Amphitrite Goddess of the Sea - Greek Gods and GoddessesAmphitrite Goddess of the Sea  [ ? ]Subscribe To This Site  Amphitrite Goddess of the Sea  In Greek mythology, Amphitrite goddess of the Sea was one of the fifty Nereids, that is a daughter of Nereus and Doris - that\'s what Hesiod writes in his Theogony (but Apollodorus says her parents were Oceanus and Thetys). Her Roman equivalent was Salacia, god Neptune\'s wife.  Her name means "The third one who encircles (everything)" (because the ancient Greeks thought the whole world was encircled by a river-god, Oceanus ).  She spent most of the time singing and dancing with her sisters. One day Poseidon, god of the sea , saw her near the island of Naxos and fell in love with her. He kidnapped her and made her his wife.  In another story, Amphitrite, being a shy girl, fled away and hid somewhere in the Atlantic Ocean. Poseidon wanted desperately to find her, so he sent all the marine creatures to look for her. Delphinus, a dolphin, managed to find her and convinced her to go back to Poseidon and marry him. That\'s what made Amphitrite goddess of the sea. (By the way, the dolphin who found her was turned into a constellation by the grateful Poseidon ).  Amphitrite goddess of the sea has no special attributions (but sometimes she can calm the waters), she is just the mistress of the sea (and sometimes she is considered the mother of the marine creatures, but they existed even before she became queen of the sea). Some even say that she is just a personification of the sea.  She also appeared briefly in the story of Theseus. King Minos asked the hero to prove that he really was Poseidon\'s son. A ring was thrown into the sea and Theseus had to bring it back. The sea creatures found the ring and gave it to the hero, while Amphitrite goddess of the sea received him in her underwater palace and gave him a golden crown, which he took to Ariadne as a wedding gift. (As you can see, Amphitrite was the ideal wife, as she was never jealous of her husband\'s love affairs.)  Amphitrite and Poseidon had a son, Triton (a fish-man or a he-mermaid) and a daughter, Rhode, and maybe another daughter, Benthesicyme.  In art | Question: In Greek mythology, Amphitrite, queen of the sea, was the wife of which god?', ['poseidon'], 'mrqa_triviaqa-validation-671'), tensor(0.6266)), (('Context: Walking in the Air  "Walking in the Air" is a song written by Howard Blake for the 1982 animated film of Raymond Briggs\' 1978 children\'s book "The Snowman".  The song forms the centrepiece of "The Snowman", which has become a seasonal favorite on British and Finnish television.  The story relates the fleeting adventures of a young boy and a snowman who has come to life.  In the second part of the story, the boy and the snowman fly to the North Pole.  "Walking in the Air" is the theme for the journey.  They attend a party of snowmen, at which the boy seems to be the only human until they meet Father Christmas with his reindeer, and the boy is given a scarf with a snowman pattern.  In the film, the song was performed by St Paul\'s Cathedral choirboy Peter Auty, and reissued in 1985 (on Stiff Records) and 1987.   Walking in the Air  "Walking in the Air" is a song written by Howard Blake for the 1982 animated film of Raymond Briggs\' 1978 children\'s book "The Snowman".  The song forms the centrepiece of "The Snowman", which has become a seasonal favorite on British and Finnish television.  The story relates the fleeting adventures of a young boy and a snowman who has come to life.  In the second part of the story, the boy and the snowman fly to the North Pole.  "Walking in the Air" is the theme for the journey.  They attend a party of snowmen, at which the boy seems to be the only human until they meet Father Christmas with his reindeer, and the boy is given a scarf with a snowman pattern.  In the film, the song was performed by St Paul\'s Cathedral choirboy Peter Auty, and reissued in 1985 (on Stiff Records) and 1987.   Peter Auty  Peter Robert Auty (born 4 November 1969) is an English operatic tenor who has worked with most of the major opera companies in Britain and a number of companies in continental Europe. | Question: An English operatic tenor performed "Walking in the Air" for which 1982 animated film?', ['The Snowman'], 'mrqa_hotpotqa-validation-1968'), tensor(0.6155)), (('Context: Mary O\'Connell  Mary O\'Connell (better known as Sister Anthony, S.C.) (1814 – December 8, 1897) was an Irish immigrant to the United States, who became a Roman Catholic Religious Sister.  A Sister of Charity of Cincinnati, she served with distinction as a nurse on the front lines of the American Civil War.  Her work with the wounded and in health care in general caused her to be known as "the angel of the battlefield" and "the Florence Nightingale of America."  Her portrait hangs in the Smithsonian Institution in Washington, DC.   Mary O\'Connell  Mary O\'Connell (better known as Sister Anthony, S.C.) (1814 – December 8, 1897) was an Irish immigrant to the United States, who became a Roman Catholic Religious Sister.  A Sister of Charity of Cincinnati, she served with distinction as a nurse on the front lines of the American Civil War.  Her work with the wounded and in health care in general caused her to be known as "the angel of the battlefield" and "the Florence Nightingale of America."  Her portrait hangs in the Smithsonian Institution in Washington, DC.   Florence Nightingale  Florence Nightingale, ( ; 12 May 1820 – 13 August 1910) was an English social reformer and statistician, and the founder of modern nursing. | Question: What portrait hangs in the Smithsonian Institute along with what is known as the founder of nursing?', ['Sister Anthony, S.C.'], 'mrqa_hotpotqa-validation-1626'), tensor(0.5635)), (('Context: That Was the Week That WasThat Was the Week That Was, informally TWTWTW or TW3, was a satirical television comedy programme on BBC Television in 1962 and 1963. It was devised, produced and directed by Ned Sherrin and presented by David Frost. An American version by the same name aired on NBC from 1964 to 1965, also featuring Frost.  The programme is considered a significant element of the satire boom in the UK in the early 1960s. It broke ground in comedy through lampooning the establishment and political figures. Its broadcast coincided with coverage of the politically charged Profumo affair and John Profumo, the politician at the centre of the affair, became a target for derision. TW3 was first broadcast on Saturday, 24 November 1962.  Cast and writers  Cast members included cartoonist Timothy Birdsall, political commentator Bernard Levin, and actors Lance Percival, who sang topical calypsos, many improvised to suggestions from the audience, Kenneth Cope, Roy Kinnear, Willie Rushton, Al Mancini, Robert Lang,  David Kernan and Millicent Martin. The last two were also singers and the programme opened with a song\xa0– That Was The Week That Was\xa0– sung by Martin to Ron Grainer\'s theme tune and enumerating topics in the news. Frankie Howerd also guested with stand-up comedy.   Script-writers included John Albery, John Antrobus, John Betjeman, John Bird, Graham Chapman, John Cleese, Peter Cook, Roald Dahl, Richard Ingrams, Lyndon Irving, Gerald Kaufman, Frank Muir, David Nobbs, Denis Norden, Bill Oddie, Dennis Potter, Eric Sykes, Kenneth Tynan, and Keith Waterhouse.   Programme  The programme opened with a song ("That was the week that was, It\'s over, let it go ...") sung by Millicent Martin, referring to news of the week just gone. Lance Percival sang a topical calypso each week. Satirical targets, such as Prime Minister Harold Macmillan and Home Secretary Henry Brooke were lampooned in sketches, debates and monologues. Other targets were the monarchy, Britain\'s declining status as a global power, racism (particularly in the American South and South Africa under Apartheid), sexual and social hypocrisy, the class system, and the BBC itself. Well-remembered sketches include a \'consumers\' guide to religion\', which discussed relative merits of faiths in the manner of a Which? magazine report.  On Saturday, 20 October 1962 the award of Nobel prizes to John Kendrew and Max Perutz, and to Francis Crick, James D. Watson, and Maurice Wilkins was satirised in a short sketch with the prizes referred to as the Alfred Nobel Peace Pools; in this sketch Watson was called "Little J.D. Watson" and "Who\'d have thought he\'d ever get the Nobel Prize? Makes you think, doesn\'t it". The germ of the joke was that Watson was only 25 when he helped discover DNA; much younger than the others.  TW3 was broadcast on Saturday night and attracted an audience of 12 million. It often under- or overran as cast and crew worked through material as they saw fit. At the beginning of the second season in the autumn of 1963, in an attempt to assert control over the programme, the BBC scheduled repeats of The Third Man television series after the end of TW3. Frost suggested a means of sabotaging this tactic to Sherrin, and he agreed. For three weeks, Frost read out the plot of The Third Man, until the repeats were abandoned following the direct intervention of the BBCs Director General Hugh Greene.   Frost often ended a satirical attack with the remark "But seriously, he\'s doing a grand job".  At the end of each episode, Frost usually signed off with: "That was the week, that was." At the end of the final programme he announced: "That was That Was The Week That Was...that was."  Kennedy tribute  For the edition on Saturday, 23 November 1963, the day after the assassination of United States President John F. Kennedy, TW3 produced a shortened 20-minute programme with no satire, reflecting on the loss, including a contribution from Dame | Question: Who was the deviser, producer and director \'That Was The Week That Was\'?', ['ned sherrin'], 'mrqa_triviaqa-validation-4054'), tensor(0.5346)), (('Context: Formula One drivers from Mexico  There have been six Formula One drivers from Mexico who have taken part in races since the championship began in 1950.  Pedro Rodríguez is the most successful Mexican driver being the only one to have won a grand prix.  Sergio Pérez, the only other Mexican to finish on the podium, currently races with Sahara Force India F1 Team .   Formula One drivers from Mexico  There have been six Formula One drivers from Mexico who have taken part in races since the championship began in 1950.  Pedro Rodríguez is the most successful Mexican driver being the only one to have won a grand prix.  Sergio Pérez, the only other Mexican to finish on the podium, currently races with Sahara Force India F1 Team .   Sergio Pérez  Sergio Pérez Mendoza (    ; born 26 January 1990) also known as "Checo" Pérez, is a Mexican racing driver, currently driving for Force India. | Question: Which other Mexican Formula One race car driver has held the podium besides the Force India driver born in 1990?', ['Pedro Rodríguez'], 'mrqa_hotpotqa-validation-14'), tensor(0.4072)), (('Context: Gothard Wilhelm Butler  Gothard Wilhelm Butler (German: "Gotthard Wilhelm von Buttlar" , c. 1600 – January 18, 1660) was a Polish-Lithuanian nobleman and politician of Scottish origin, born in Kuldīga (Goldingen).  He was Grand treasurer of the Crown, the Crown court chamberlain and a captain of the guard of King John II Casimir Vasa and erderman of Prienai, Parnu and Bolesław.   John II Casimir Vasa  John II Casimir (Polish: "Jan II Kazimierz Waza" ; German: "Johann II.  Kasimir Wasa" ; Lithuanian: "Jonas Kazimieras Vaza" ; 22 March 1609 – 16 December 1672) was King of Poland and Grand Duke of Lithuania during the era of the Polish–Lithuanian Commonwealth, Duke of Opole in Upper Silesia, and titular King of Sweden 1648–1660.  In Poland, he is known and commonly referred as Jan Kazimierz.  His parents were Sigismund III Vasa (1566–1632) and Constance of Austria (1588–1631).  His older brother, and predecessor on the throne, was Władysław IV Vasa. | Question: By what name is the King that Gothard Wilhelm Butler was captain of the guard for known in Poland?', ['Jan Kazimierz'], 'mrqa_hotpotqa-validation-3632'), tensor(0.4039)), (("Context: The United States two - dollar bill ( $2 ) is a current denomination of U.S. currency . The third U.S. President ( 1801 -- 09 ) , Thomas Jefferson , is featured on the obverse of the note . The reverse features an engraving of the painting The Declaration of Independence by John Trumbull . Throughout the $2 bill 's pre-1929 life as a large - sized note , it was issued as a United States Note , National Bank Note , silver certificate , Treasury or `` Coin '' Note and Federal Reserve Bank Note . When U.S. currency was changed to its current size , the $2 bill was issued only as a United States Note . Production went on until 1966 , when the series was discontinued . Ten years passed before the $2 bill was reissued as a Federal Reserve Note with a new reverse design . Two - dollar bills are seldom seen in circulation as a result of banking policies with businesses which has resulted in low production numbers due to lack of demand . This comparative scarcity in circulation , coupled with a lack of public knowledge that the bill is still in production and circulation , has also inspired urban legends and occasionally has created problems for people trying to use the bill to make purchases . | Question: when were 2 dollar bills stopped being made ?", ['current denomination of U.S. currency'], 'mrqa_naturalquestions-validation-2753'), tensor(0.3924)), (('Context: <Ol>  A new ruler unites China , founds a new dynasty , and gains the Mandate of Heaven .   China , under the new dynasty , achieves prosperity .   The population increases .   Corruption becomes rampant in the imperial court , and the empire begins to enter decline and instability .   A natural disaster wipes out farm land . The disaster normally would not have been a problem ; however , together with the Corruption and overpopulation , it causes famine .   The famine causes the population to rebel and a civil war ensues .   The ruler loses the Mandate of Heaven .   The population decreases because of the violence .   China goes through a warring states period .   One state emerges victorious .   The state starts a new empire .   The empire gains the Mandate of Heaven .  </Ol> | Question: in the dynastic cycle what is the right to rule called ?', ['the Mandate of Heaven'], 'mrqa_naturalquestions-validation-6157'), tensor(0.3857)), (('Context: Although partly functional , weather vanes are generally decorative , often featuring the traditional cockerel design with letters indicating the points of the compass . Other common motifs include ships , arrows and horses . Not all weather vanes have pointers . When the wind is sufficiently strong , the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing . | Question: where does the arrow of a wind vane point ?', ['the direction from which the wind is blowing'], 'mrqa_naturalquestions-validation-9688'), tensor(0.3224)), (('Context: Hey Ya!  "Hey Ya!"  is a song written and produced by André 3000 for his 2003 album "The Love Below", part of the hip hop duo OutKast\'s double album "Speakerboxxx/The Love Below".  "Hey Ya!"  takes influence from funk, rap and rock music.  Its music video features a live performance by a band, all eight of whose members are played by André 3000, that mimics the Beatles\' 1964 performance on "The Ed Sullivan Show".  The song received praise from contemporary music critics, and won the award for Best Urban/Alternative Performance at the 46th Grammy Awards.  His version of the song has also appeared on the soundtrack of   Hey Ya!  "Hey Ya!"  is a song written and produced by André 3000 for his 2003 album "The Love Below", part of the hip hop duo OutKast\'s double album "Speakerboxxx/The Love Below".  "Hey Ya!"  takes influence from funk, rap and rock music.  Its music video features a live performance by a band, all eight of whose members are played by André 3000, that mimics the Beatles\' 1964 performance on "The Ed Sullivan Show".  The song received praise from contemporary music critics, and won the award for Best Urban/Alternative Performance at the 46th Grammy Awards.  His version of the song has also appeared on the soundtrack of   The Ed Sullivan Show  The Ed Sullivan Show is an American TV variety show that ran on CBS from Sunday June 20, 1948, to Sunday June 6, 1971, and was hosted by New York entertainment columnist Ed Sullivan.  It was replaced in September 1971 by the "CBS Sunday Night Movie". | Question: Hey Ya is a song performed on The Ed Sullivan Show on CBS by which hip hop artist in 2003?', ['is a song written and produced by André 3000'], 'mrqa_hotpotqa-validation-2679'), tensor(0.2571)), (('Context: The Elizabeth Files » Mary Queen of Scots Marries Lord DarnleyThe Elizabeth Files » Mary Queen of Scots Marries Lord Darnley  Mary Queen of Scots Marries Lord Darnley  Posted By  claire on July 29, 2010  Mary Queen of Scots  On this day in history, the 29th July 1565, Elizabeth I’s nemesis, Mary Queen of Scots, married Henry Stuart, Lord Darnley, at Holyrood Palace (the Palace of Holyroodhouse), Edinburgh.  Let’s celebrate their union by giving some facts about the happy couple:-  The Bride – Mary Queen of Scots  Birth: Mary, Queen of Scots, also known as Mary Stuart, Mary Stewart and Marie Stuart, was born on the 8th December 1542 at Linlithgow Palace, Scotland.  Parents: Mary’s parents were James V of Scotland (son of James IV and Margaret Tudor) and Mary of Guise. She was James’s only child to survive and he actually died six days after her birth.  Queen of Scotland After her father’s death on the 14th December, Mary became Queen of Scotland and was crowned at Stirling in September 1543. As Mary was just an infant, the country was ruled by regents on her behalf.  Betrothals: On the 1st July 1543, at the age of 7 months, the Treaty of Greenwich, between England and Scotland, promised Mary in marriage to Henry VIII’s son, Prince Edward. When the pro-Catholic and pro-French Cardinal Beaton became powerful in Scotland and joined forces with the Earl of Arran, the treaty was rejected by the Scottish Parliament and trouble began between England and Scotland. The French came to the aid of Scotland and on the 7th July 1548 a marriage treaty between France and Scotland promised the 5 year old Mary in marriage to Henry II’s son, François, the Dauphin.  Upbringing: From the age of 5 until the age of 19 Mary lived in France. There she received an excellent education, learning French, Italian, Spanish, Greek, Latin and Scots, as well as music, needlework and poetry.  Return to Scotland: Francois (Francis II of France) died in 1560 and his mother, Catherine de’ Medici, took control of the country, as regent for her other son, Charles IX. Mary returned to Scotland on the 19th August 1561.  Marriages: 24th April 1558 to the Dauphin François at Notre Dame. he died in 1560. 29th July 1565 to Henry Stuart, Lord Darnley, he was assassinated in 1567. 15th May 1567 to James Hepburn, Lord Bothwell, he died in 1578.  Mary and her son James  Issue: James VI of Scotland (James I of England) by Lord Darnley, born on 19th June 1566. Mary miscarried twins (fathered by Bothwell) in July 1567.  Titles: Queen of Scotland from 14th December 1542 until 24th July 1567 when she was forced to abdicate.  At the death of Mary I of England in 1558, Henry II of France declared that François and Mary were King and Queen of England and Mary started bearing the royal arms of England.  On the death of her father-in-law, Henry II of France, on the 10th July 1559 Mary became Queen Consort of France, a position she held until 5th December 1560.  Appearance: Mary, Queen of Scots, was considered a beauty. She was tall (around 5′ 11) with auburn hair, hazel eyes and a heart-shaped face.  Scandal: In March 1566, Mary’s husband, Lord Darnley, and some friends murdered Mary’s private secretary, David Rizzio, in front of his pregnant wife. He was jealous of Mary’s friendship with Rizzio. Darnley became a real problem for Mary and the Scottish Lords and in February 1567 Darnley was killed in an explosion at Kirk O’Field. It is thought that James Hepburn, Lord Bothwell, supplied the gunpowder but he was acquitted of murder in April 1567. The famous “Casket Letters” implicated Mary in the murder of her husband, but these are generally believed to have been forgeries.  On the 24th April 1567, Mary was kidnapped by Bothwell (it is | Question: What was the name of the letters that in 1566 implicated Mary Queen of Scots in the murder of her second husband, Lord Darnley?', ['casket letters'], 'mrqa_triviaqa-validation-1935'), tensor(0.2279)), (("Context: 2016 Blancpain GT Series Sprint Cup  The 2016 Blancpain GT Series Sprint Cup was the fourth season following on from the demise of the SRO Group's FIA GT1 World Championship (an auto racing series for grand tourer cars), the third with the designation of Blancpain Sprint Series or Blancpain GT Series Sprint Cup.  After developing their partnership, Blancpain and the SRO decided that 2016 would see both the Sprint and Endurance Series further integrated into the Blancpain GT Series, putting the emphasis on the prestigious overall drivers' and manufacturers' titles causing the Sprint Series name to change from Blancpain Sprint Series to Blancpain GT Series Sprint Cup.   FIA GT1 World Championship  The FIA GT1 World Championship was a world championship sports car racing series developed by the SRO Group and regulated by the Fédération Internationale de l'Automobile (FIA), held from 2010 to 2012.  It featured multiple grand tourer race cars based on production road cars and conforming with the GT1 (2010–2011) and GT3 (2012) regulations competing in one-hour races on multiple continents.  All cars were performance balanced with weight and restrictor adjustments to artificially equalise their performance.  Championships were awarded each season for drivers and teams. | Question: What years was the auto racing series held that was followed on by the 2016 Blancpain GT Series Sprint Cup?", ['2010 to 2012'], 'mrqa_hotpotqa-validation-1376'), tensor(0.1974)), (('Context: The Soulages collection of Italian and French Renaissance objects was acquired between 1859 and 1865, and includes several cassone. The John Jones Collection of French 18th-century art and furnishings was left to the museum in 1882, then valued at £250,000. One of the most important pieces in this collection is a marquetry commode by the ébéniste Jean Henri Riesener dated c1780. Other signed pieces of furniture in the collection include a bureau by Jean-François Oeben, a pair of pedestals with inlaid brass work by André Charles Boulle, a commode by Bernard Vanrisamburgh and a work-table by Martin Carlin. Other 18th-century ébénistes represented in the Museum collection include Adam Weisweiler, David Roentgen, Gilles Joubert & Pierre Langlois. In 1901, Sir George Donaldson donated several pieces of art Nouveau furniture to the museum, which he had acquired the previous year at the Paris Exposition Universelle. This was criticized at the time, with the result that the museum ceased to collect contemporary items and did not do so again until the 1960s. In 1986 the Lady Abingdon collection of French Empire furniture was bequeathed by Mrs T. R. P. Hole. | Question: What items comprise the John Jones Collection?', ['art and furnishings', 'French 18th-century art and furnishings'], 'mrqa_squad-validation-5622'), tensor(0.1636)), (("Context: The comptroller ( who is also auditor general and head of the National Audit Office ) controls both the Consolidated Fund and the National Loans Fund . The full official title of the role is Comptroller General of the Receipt and Issue of Her Majesty 's Exchequer . | Question: who controls the consolidated fund of the state ?", ['The comptroller ( who is also auditor general and head of the National Audit Office )'], 'mrqa_naturalquestions-validation-1364'), tensor(0.1602)), (("Context: The Cold War was a state of geopolitical tension after World War II between powers in the Eastern Bloc ( the Soviet Union and its satellite states ) and powers in the Western Bloc ( the United States , its NATO allies and others ) . Historians do not fully agree on the dates , but a common timeframe is the period between 1947 , the year the Truman Doctrine , a U.S. foreign policy pledging to aid nations threatened by Soviet expansionism , was announced , and either 1989 , when communism fell in Eastern Europe , or 1991 , when the Soviet Union collapsed . The term `` cold '' is used because there was no large - scale fighting directly between the two sides , but they each supported major regional wars known as proxy wars . | Question: who were the major countries involved in the cold war ?", ['the Soviet Union', 'the United States'], 'mrqa_naturalquestions-validation-5180'), tensor(0.1312)), (("Context: Rolls-Royce Welland  The Rolls-Royce RB.23 Welland was Britain's first production jet engine.  It entered production in 1943 for the Gloster Meteor.  The name Welland is taken from the River Welland, in keeping with the Rolls-Royce policy of naming early jet engines after rivers based on the idea of continuous flow, air through the engine and water in a river.   River Welland  The River Welland is a lowland river in the east of England, some 65 mi long.  It drains part of the Midlands eastwards to The Wash.  The river rises in the Hothorpe Hills, at Sibbertoft in Northamptonshire, then flows generally northeast to Market Harborough, Stamford and Spalding, to reach The Wash near Fosdyke.  It is a major waterway across the part of the Fens called South Holland, and is one of the Fenland rivers which were laid out with washlands.  There are two channels between widely spaced embankments with the intention that flood waters would have space in which to spread while the tide in the estuary prevented free egress.  However, after the floods of 1947, new works such as the Coronation Channel were constructed to control flooding in Spalding and the washes are no longer used solely as pasture, but may be used for arable farming. | Question: Rolls-Royce Welland's name is taken from this river that is low long ?", ['65 mi'], 'mrqa_hotpotqa-validation-1201'), tensor(0.1270)), (('Context: Individual Huguenots settled at the Cape of Good Hope from as early as 1671 with the arrival of François Villion (Viljoen). The first Huguenot to arrive at the Cape of Good Hope was however Maria de la Queillerie, wife of commander Jan van Riebeeck (and daughter of a Walloon church minister), who arrived on 6 April 1652 to establish a settlement at what is today Cape Town. The couple left for the Far East ten years later. On 31 December 1687 the first organised group of Huguenots set sail from the Netherlands to the Dutch East India Company post at the Cape of Good Hope. The largest portion of the Huguenots to settle in the Cape arrived between 1688 and 1689 in seven ships as part of the organised migration, but quite a few arrived as late as 1700; thereafter, the numbers declined and only small groups arrived at a time. | Question: The number of new Huguenot colonists declined after what year?', ['1700'], 'mrqa_squad-validation-3113'), tensor(0.1254)), (("Context: Doctor Who has appeared on stage numerous times. In the early 1970s, Trevor Martin played the role in Doctor Who and the Daleks in the Seven Keys to Doomsday. In the late 1980s, Jon Pertwee and Colin Baker both played the Doctor at different times during the run of a play titled Doctor Who – The Ultimate Adventure. For two performances, while Pertwee was ill, David Banks (better known for playing Cybermen) played the Doctor. Other original plays have been staged as amateur productions, with other actors playing the Doctor, while Terry Nation wrote The Curse of the Daleks, a stage play mounted in the late 1960s, but without the Doctor. | Question: What was the name of the Doctor Who play from the 1980's?", ['Doctor Who – The Ultimate Adventure'], 'mrqa_squad-validation-7821'), tensor(0.1000)), (("Context: Steve Carell as Felonious Gru , the former villain turned Anti-Villain League agent , Margo , Edith , and Agnes ' adoptive father , and Lucy 's husband .   Carell also voices Dru , Gru 's long - lost twin brother and the girls ' adoptive uncle .     Kristen Wiig as Lucy Wilde , an Anti-Villain League agent , Gru 's wife and the girls ' adoptive mother .   Trey Parker as Balthazar Bratt , a supervillain and former child star who grows up to become obsessed with the character he played in the ' 80s and is bent on world domination .   Miranda Cosgrove as Margo , Gru and Lucy 's oldest adoptive daughter .   Dana Gaier as Edith , Gru and Lucy 's middle adoptive daughter .   Nev Scharrel as Agnes , Gru and Lucy 's youngest adoptive daughter . She was previously voiced by Elsie Fisher in the first two films .   Pierre Coffin as Mel and the Minions .   Coffin also voices a Museum Director .     Steve Coogan as Silas Ramsbottom , the director of the Anti-Villain League .   Coogan also voices Fritz , the butler of Dru .     Julie Andrews as Marlena Gru , Gru and Dru 's mother .   Jenny Slate as Valerie Da Vinci , a member of the Anti-Villain League who becomes the brand new AVL director .   Andy Nyman as Clive , a robot who is Bratt 's sidekick .   Adrian Ciscato as Niko , a boy from Freedonia who falls in love with Margo .   John Cygan ( in his final film role ) as an Additional Voice . | Question: who is the cast of despicable me 3 ?", ['Nev Scharrel', 'Andy Nyman', 'Adrian Ciscato', 'Pierre Coffin', 'Steve Coogan', 'Trey Parker', 'Julie Andrews', 'Kristen Wiig', 'John Cygan', 'Jenny Slate', 'Dana Gaier', 'Steve Carell', 'Miranda Cosgrove'], 'mrqa_naturalquestions-validation-3490'), tensor(0.0969)), (("Context: Prague: Six things you must do in the Czech capital city ...Prague: Six things you must do in the Czech capital city | Daily Mail Online  Prague: Six things you must do in the Czech capital city  comments  Prague is one of Europe's architectural gems, with more knockout picture-postcard views than some entire countries. Unsurprisingly, the Czech capital can also get busy as thousands of tourists jostle for space.  Here is a selection of things to do during a visit to the city - from taking in its best-loved sights to stopping for a drink at a beautifully restored cafe and watching a comic take on a famous Mozart opera.  1... New Jerusalem  Emperor Charles IV planned this intensely beautiful city as the ‘New Jerusalem’ in the 14th Century, and architects maintained the vision until its last golden age in the Thirties. The old centre, with outstanding buildings such as Valdstejn Palace, St James’ Church and St Vitus Cathedral, is still largely intact after 800 years and is now a World Heritage Site.  With nifty footwork you can outwit the crowds. Try walking the famous Charles Bridge before breakfast, and save the striking of the Astronomical Clock in the Old Town Square until the evening when it’s quieter. I also like the peacefulness of Mala Strana, the cafe-lined cobbled streets under Prague Castle. And take a tram to the Jewish Quarter and wander Petrin Hill for wonderful views over the city’s baroque roofs.  Atmospheric: Prague's beautiful Charles Bridge  2... Grand re-opening  How about this for a comeback? The Grand Cafe Orient ( www.grandcafeorient.cz ) opened in 1912 in the House of the Black Madonna, Prague’s architectural take on cubism, the art form Picasso championed. The cafe closed eight years later when cubism fell out of favour but reopened in 2005, faithfully recreated with period furniture and fittings. Try the apple strudel with vanilla ice cream.  Another venue, the Mysak pastry shop, reopened in 2008 inside the Mysak Gallery ( www.gallerymysak.cz ), with original doors, mosaics floors, a marble staircase and wooden alcoves. The signature dish is karamelovy pohar – ice cream topped with caramel, chocolate and walnuts.  RELATED ARTICLES  Share  3...Fighting for freedom  After the euphoria of spring 1968, and Czechoslovakia’s short-lived break for freedom from the Soviet Union, came deep gloom that summer when Russian tanks rolled in to consign this proud nation to 20 more years of oppression.  The Museum of Communism ( www.muzeumkomunismu.cz ) tells the story of those lost years. I came closest to Prague’s tragedy and eventual triumph on the steps of the National Museum in Wenceslas Square. I looked down on the bronze cross at the spot where Jan Palach burnt himself to death in protest in 1969. It's also where people celebrated their new-found freedom after 1989’s so-called Velvet Revolution.  Historic: The Duke of Bohemia is represented by an equestrian statue in Wenceslas Square  4...Recovered treasures  In 1989, William Lobkowicz returned to Prague from exile in Boston to restore the great estate his family had built up over 700 years. It had been confiscated first by the Nazis, then by the communists. Diligent legal teams gradually recovered land, thousands of books, paintings and pieces of furniture. William’s mission culminated in the opening of Lobkowicz Palace in 2007, with its exhibition of the best of the family’s reclaimed treasures. Star exhibits include two views of the River Thames by Canaletto and Haymaking by Pieter Bruegel the Elder.  Another of the city’s new attractions is totally different. The Kafka Museum ( www.kafkamuseum.cz ) celebrates the writer (he was born here) whose name stands for the way the ordinary man is defeated by ‘the system’.  5...Pulling the strings  With his mischievous sense of fun, Mozart would have approved of the National Marionette Theatre’s comic performances of his opera Don Giovanni ( www.mozart.cz ). I particularly loved the puppet cat hissing at an anguished diva in mid aria. Don Giovanni is Prague’s top claim to cultural fame – in 1787, Mozart staged its world premiere at the exquisite opera | Question: By what name do Czechs know their capital city, Prague?", ['prague'], 'mrqa_triviaqa-validation-6351'), tensor(0.0858)), (('Context: Chun Woo-hee  Chun Woo-hee (born April 20, 1987) is a South Korean actress.  She made her acting debut in 2004, but first drew attention with her supporting role as a rebellious teenager in the 2011 box-office hit "Sunny".  In 2014, Chun received domestic and international critical acclaim for her first leading role as the title character in "Han Gong-ju", a coming-of-age indie about a traumatized young woman trying to move on with her life after a tragedy.  Her other notable films include "The Beauty Inside" (2015), "Love, Lies" (2016) and "The Wailing" (2016).   Chun Woo-hee  Chun Woo-hee (born April 20, 1987) is a South Korean actress.  She made her acting debut in 2004, but first drew attention with her supporting role as a rebellious teenager in the 2011 box-office hit "Sunny".  In 2014, Chun received domestic and international critical acclaim for her first leading role as the title character in "Han Gong-ju", a coming-of-age indie about a traumatized young woman trying to move on with her life after a tragedy.  Her other notable films include "The Beauty Inside" (2015), "Love, Lies" (2016) and "The Wailing" (2016).   The Wailing (film)  The Wailing () is a 2016 South Korean horror film directed by Na Hong-jin about a policeman who investigates a series of mysterious killings and illnesses.  It was a commercial success. | Question: Chun Woo-hee\'s notable films include a South Korean horror film about  a policeman who investigates a series of mysterious killings and what?', ['illnesses'], 'mrqa_hotpotqa-validation-5325'), tensor(0.0708)), (('Context: The Iroquois was originally designated HU - 1 , hence the Huey nickname , which has remained in common use , despite the official redesignation to UH - 1 in 1962 . The UH - 1 first saw service in combat operations during the Vietnam War , with around 7,000 helicopters deployed . The Bell 204 and 205 are Iroquois versions developed for the civil market . | Question: where did the name huey helicopter come from ?', ['originally designated HU - 1'], 'mrqa_naturalquestions-validation-5144'), tensor(0.0636)), (('Context: Many residents of metropolitan regions work within the central urban area , and choose to live in satellite communities called suburbs and commute to work via automobile or mass transit . Others have taken advantage of technological advances to work from their homes . These processes often occur in more economically developed countries , especially in the United States , which is believed to be the first country in which the majority of the population lives in the suburbs , rather than in the cities or in rural areas . Proponents of containing urban sprawl argue that sprawl leads to urban decay and a concentration of lower income residents in the inner city . | Question: suburbanization of more developed countries is mostly due to ?', ['Many residents of metropolitan regions work within the central urban area , and choose to live in satellite communities called suburbs and commute to work'], 'mrqa_naturalquestions-validation-8448'), tensor(0.0634)), (("Context: What we now call gravity was not identified as a universal force until the work of Isaac Newton. Before Newton, the tendency for objects to fall towards the Earth was not understood to be related to the motions of celestial objects. Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object. Today, this acceleration due to gravity towards the surface of the Earth is usually designated as  and has a magnitude of about 9.81 meters per second squared (this measurement is taken from sea level and may vary depending on location), and points toward the center of the Earth. This observation means that the force of gravity on an object at the Earth's surface is directly proportional to the object's mass. Thus an object that has a mass of  will experience a force: | Question: Who came up with the concept that falling objects fell at the same speed regardless of weight?", ['Galileo'], 'mrqa_squad-validation-10410'), tensor(0.0581)), (('Context: Virginia Wade - International Tennis Hall of FameVirginia Wade  Virginia Wade  7-time major champion, 6-time finalist  Career Titles  Member of the British Wightman Cup Team 1965-1985  Member of the winning team 1968, 1974, 1975, 1978  Fed Cup  Member of the British Federation Cup Team 1967-1970, 1972-1983  Overall Record: 66-33  Virginia Wade WTA Profile  Citizenship: GBR Born: July 10, 1945               in Bournemouth, England Played: Right-handed  Virginia Wade chose the perfect time to become the first British female to win the Wimbledon Ladies Singles Championship in 8 years and the last to win since.  In July 1977, the summer of the monarch’s Silver Jubilee, Wade won her third major title at the All England Club, with nicely coincided with Wimbledon’s centenary year. Queen Elizabeth II, who made no qualms about not being a tennis fan, was making only her second ever appearance at Centre Court when Wade met Betty Stove of the Netherlands for the championship. “If she’s [Queen Elizabeth II] going to be there, I am going to be there too,” Wade said.  Wade, then 31, and singles champion at the US Open in 1968 and at the Australian Open in 1972, wore a beautiful pink cardigan as she arrived on court. After dropping the first set 4-6, her whole demeanor turned a fiery red in winning the final two sets convincingly, 6-3, 6-1. Wade, who was playing in her 17th\xa0of an all-time record 26 Wimbledons, upset No. 1 seed Chris Evert, 6-2, 4-6, 6-1, in the semifinals to reach her one and only final. Once she settled into a groove after the first set, her precise groundstrokes controlled the match. “Winning Wimbledon was the thing that made my career worthwhile,” Wade told the Guardian in 1977. Those in attendance witnessed Queen Elizabeth II present Wade with championship trophy and then sang a rendition of “For She’s a Jolly Good Fellow” to celebrate the victory.  With her victory in London, Wade held the distinction of being the last Brit to win Wimbledon until Andy Murray in 2013. Her winnings were $20,499, compared to today’s multi-million dollar purse. In an interview with the\xa0Independent\xa0in 2007, Wade joked that her Wimbledon title came right on schedule. “Well, Angela Mortimer had won in 1961 and Ann Jones in 1969, so when I won in 1977 we all thought it happened every eight years, but maybe we were just anomalies, because there was Sue Barker and Joe Durie, but then the [British] players just petered out.”  Wade was a lithe 5-foot-8 steady stroke machine who had a beautiful all-court game built on a smooth slice backhand and a forehand that she could hit with topspin or flat. Wade was a thinker between the lines; her shots were patient and calculated. She was adept at controlling tempo and wouldn’t be forced into foolish shots. She would pause a minimum of five seconds before releasing her serve after getting into the ready position and hopped into position to move laterally or take a short ball and attack the net.  In her lengthy 26-year career, Wade won 55 singles titles, eighth on the all-time list. She favored playing at Wimbledon and the US Open the most of any of the four majors. As a 23-year old in 1968, she won the inaugural US Open, doing so as the No. 6 seed with a stunning and unexpected 6-4, 6-2 victory over No. 1 seed Billie Jean King. It was Wade’s fifth trip to the US major and although she would play the event another 15 times, her best finishes afterward were the semifinals in 1969, 1970, and 1975. The 1968 victory earned her $6,000. Wade only traveled to the Australian Open five times, but in 1972 stung another No. 1 seed when defeating crowd favorite and native Evonne Goolagong, who was in the midst of playing in seven consecutive Australian finals, | Question: Prior to Viginia Wade in 1977 who was the last British player to win a tennis Grand Slam title?', ['sue barker'], 'mrqa_triviaqa-validation-5406'), tensor(0.0565)), (('Context: Rumpole of the Bailey - TV.comRumpole of the Bailey - Show News, Reviews, Recaps and Photos - TV.com  Rumpole of the Bailey  EDIT  Welcome to the Rumpole of the Bailey guide at TV.com.  Horace Rumpole (played by the late Leo McKern) is an untidy, ageing London barrister with one glass eye who defends in criminal cases. His clients rarely cut elegant figures. He is fond of red wine, poetry, and fair dealing, and is not looked on as a great success in life by his wife, Hilda (\'She Who Must Be Obeyed\'). Rumpole has had a few triumphs, and the Penge Bungalow murders are often on his mind... Rumpole shares Chambers at Number 3, Equity Court, with a mixed group of barrister colleagues, including Guthrie Featherstone (Peter Bowles) and Phyllida Trant (Patricia Hodge). He also takes pupils - notably Fiona Allways (played by Rosalyn Landor) and Liz Probert. The creator and writer of the series, Sir John Mortimer, received an Edgar Allan Poe Award for crime and mystery for Rumpole.  Horace Rumpole - Thrilling DetectiveHorace Rumpole  Horace Rumpole  Created by John Mortimer (1923-2009 )  "Crime doesn\'t pay, but it\'s a living."  Down those mean streets and meaner chambers a man must waddle...  Lord knows, he\'s not a private eye, but God, I wish he were. Like Philip Marlowe , Rumpole is ""a relatively poor man... a common man or he could not go among common people. He has a sense of character, or he would not know his job. He will take no man\'s money dishonestly and no man\'s insolence without a due and dispassionate revenge. He is a lonely man and his pride is that you will treat him as a proud man or be very sorry you ever saw him. He talks as the man of his age talks -- that is, with a rude wit, a lively sense of the grotesque, a disgust for sham, and a contempt for pettiness."  So, may I submit for your consideration, Your Honour....  That great defender of most muddled and sinful humanity...  With his jowls a-quiver...  His fondness for Wordsworth, Chateau Thames Embankment and hopeless cases...  His cheroot-puffing and claret-quaffing...  His food-bespeckled robe and raggedy wig...  And his beloved and tattered copy of The Oxford Book of English Verse clutched to his bosom...  For his oratorical outbursts...  His always entertaining jabs at the soft underbelly of hypocrisy, pomposity and upper class twits...  And for standing up for truth, justice, honour and the Golden Thread of Justice...  May I submit for inclusion, Your Honour, this most British of all lawyers...  This proud, this defiant Old Bailey Hack...  With his best gal, Hilda, She Who Must Be Obeyed, standing, NOT amused, by his side...  The one, the only...  HORACE RUMPOLE.  May there always be an England, and may there always be Horace Rumpole to see that justice be done. And thev pompous may squirm.  Your Honour, I rest my case.  ****  Rumpole is, of course John Mortimer\'s rotund, defiant British criminal lawyer who, as brilliantly brought to life by the late, great Australian actor Leo McKern, the star of Rumpole of the Bailey, the popular British courtroom comedy/drama that originally aired on Thames Television in 1978, and soon became popular on both sides of the Atlantic, appearring on the American Public Broadcasting Service as part of its Mystery! series.  Mortimer wrote each and every episode of the television series, and their subsequent novelizations. The show ran, off and on, for seventeen years, an incredible run, and inspired not just the short stories, but novels and two radio series. It was only in 1995, with the publication of the Rumpole and the Angel of Death collection, that Mortimer began writing original Rumpole stories (ie: not adapted from his own TV scripts). Sice then, two | Question: Who created Rumpole of the Bailey?', ['john mortimer', 'john mortimer qc'], 'mrqa_triviaqa-validation-1551'), tensor(0.0540)), (("Context: `` Everywhere '' is a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 ) , written by Fleetwood Mac member Christine McVie , who also performs lead vocals on the song . `` Everywhere '' was released as the fourth single from Tango in the Night on November 28 , 1987 in the United States , where it reached number 14 on the Billboard Hot 100 chart and number - one on the Adult Contemporary chart , remaining there for three weeks . `` Everywhere '' was released in the United Kingdom on 21 March 1988 and reached number four . It also reached number 45 in Australia . | Question: i want to be with you everywhere song ?", ['Everywhere'], 'mrqa_naturalquestions-validation-114'), tensor(0.0492)), (('Context: Quran (Complete) - YouTubeQuran (Complete)  - YouTube  Quran (Complete)  38,356 views  Last updated on Jun 30, 2014  The Qur\'an (English pronunciation: /kɒˈrɑːn/ kor-AHN; Arabic: القرآن\u200e al-qur\'ān, IPA: [qurˈʔaːn], literally "the recitation") is the religious text of Islam,[ also sometimes transliterated as Quran, Kuran, Koran, Qur\'ān, Coran or al-Qur\'ān. It is widely regarded as the finest piece of literature in the Arabic language. Muslims hold that the Qur\'an is the verbal divine guidance and moral direction for mankind. Muslims also consider the original Arabic verbal text to be the final revelation of God the Final Testament  Quran: 78. Surat An Naba\' (The Tidings ) with ... - YouTubeQuran: 78. Surat An Naba\' (The Tidings ) with English Audio Translation and Transliteration HD - YouTube  Quran: 78. Surat An Naba\' (The Tidings ) with English Audio Translation and Transliteration HD  Want to watch this again later?  Sign in to add this video to a playlist.  Need to report the video?  Sign in to report inappropriate content.  The interactive transcript could not be loaded.  Loading...  Rating is available when the video has been rented.  This feature is not available right now. Please try again later.  Published on May 14, 2015  Please support the channel by subscribing, liking and commenting, may Allah reward you with Godness, Ameen https://www.youtube.com/user/kuranreader Islam, Prophet, prophets, Muhammad, Mohamed, Quran, Kuran, Kuranreader, Surat, Chapter, Ayah, Ayat, Sheikh, imam, Emotional, Truth, Emotional, relaxing, Soothing, Recitation, reading, Religion, Religious Text, Amazing , Beautful, Quran English Translation, Transliteration,beautiful quran recitation, quran, koran,kuran reader,quran reader,really beautiful,heart touchig ,relaxing,soothing,heart trembling,good voice, al minshawi, tajweed,emotional,tearful, quran download,surat,surah,most beautiful,sudais, abd albasit,mishari,quran reading ,learn arabic,learn quran,, قراءة, تلاوة , القارىء , مكتوب , مكتوبة, ترجمة , انجليزية,مرتل , مرتل, ,  Quran,Al Quran,the quran,The Holy Quran,Holy Quran,Quran Translation,Al Quran Translation,The Holy Quran Translation,Holy Quran Translation,Quran Bangla,Bangla Quran,Al Quran Bangla,Bangla Al Quran,Quran Bangla Translation,Bangla Quran Translation,Al Quran Bangla Translation,Koran,coran,Surah,Translatio\xadn,Islam,Abdul Rahman Al-Sudais,Imam,quran recitation,urdu quran,full quran,beautiful quran,quran beautiful recitation,urdu quran translation,quran karim,quran with urdu,tilawat,bacaan quran,bacaan al quran,merdu al quran,full al quran,surah al quran,suara merdu,ayat al quran,alquran,tilawah,tilawah al quran,quran recitation beautiful,beautiful quran,al quran recitation,best quran recitation,best quran,the quran recitation,full quran,quran recitation full,kicked the quran,the quran miracles,quran miracles,burning the quran,quran burning,miracles of quran,azab,burn the quran,the quran recitation,the quran full,full quran recitation,sudais full quran,mishary full quran,full quran download,full quran tilawat,beautiful quran recitation,quran recitation,beautiful recitation,most beautiful quran,best quran recitation,quran surah baqarah,quran surah yaseen,quran surah yasin,tilawat quran,quran surah rahman,quran surah kahf,quran surah rehman,quran surah fatiha,quran surah mulk,al quran karim,quran mp3,quran karim mp3,quran karim full,quran karim soudais,quran karim sudais,quran sudais,quran karim urdu,koran karim,coran karim,le coran,coran recitation,al coran,sourate coran,sourate,el coran,soudais,soudais coran,coran francais,coran islam,coran en francais,ecouter le coran,le coran arabe,le coran sourate,coran sourate,recite le coran,apprendre le coran, | Question: The Qur\'an (or Quran, Kuran, Koran, Coran or al-Qur\'an - literally \'the recitation\') is the religious text of which religion?', ['islam'], 'mrqa_triviaqa-validation-2210'), tensor(0.0384)), (('Context: The axons of the tract cells cross over ( decussate ) to the other side of the spinal cord via the anterior white commissure , and to the anterolateral corner of the spinal cord ( hence the spinothalamic tract being part of the anterolateral system ) . Decussation usually occurs 1 - 2 spinal nerve segments above the point of entry . The axons travel up the length of the spinal cord into the brainstem , specifically the rostral ventromedial medulla . | Question: where does decussation occur in the spinothalamic pathway ?', ['usually occurs 1 - 2 spinal nerve segments above the point of entry'], 'mrqa_naturalquestions-validation-7511'), tensor(0.0373)), (('Context: Concentrated O2 will allow combustion to proceed rapidly and energetically. Steel pipes and storage vessels used to store and transmit both gaseous and liquid oxygen will act as a fuel; and therefore the design and manufacture of O2 systems requires special training to ensure that ignition sources are minimized. The fire that killed the Apollo 1 crew in a launch pad test spread so rapidly because the capsule was pressurized with pure O2 but at slightly more than atmospheric pressure, instead of the 1⁄3 normal pressure that would be used in a mission.[k] | Question: ______ In both liquid and gas form can fastly result in an exlposion. ?', ['oxygen'], 'mrqa_squad-validation-3478'), tensor(0.0340)), (('Context: Origin of Does Exactly What it Says on the Tin | RonsealOrigin of Does Exactly What it Says on the Tin | Ronseal  The Ronseal Phrase - Does Exactly What it Says on the Tin  \xa0  People use the phrase every day, it has come to represent a product or policy that is open and honest; it is used when something quite simply ‘Does Exactly What it Says on the Tin .’  Originating back to 1994 the popular phrase ‘Does exactly what is says on the tin’ was developed when Ronseal employed London based advertising agency HHCL to create a campaign that without trying too hard would de-mystify our products. Dave Shelton, co-creator of the line with Liz Whinston, explains the thought process behind the original campaign adverts ,  “We started to write a commercial that featured a straightforward guy who said lines like: "If you\'ve got wood to stain and you want it to dry quickly, you need Ronseal Quick Drying Woodstain."  We\'d soon knocked out several scripts but we needed a line… "Does exactly what it says on the tin" was a great way of summing this up.  After the initial campaign, sales shot up and Ronseal became [a] brand leader.”  ‘Does exactly what it says on the tin’ is now more than an advertising slogan; it has become part of our everyday vernacular. The line has made it into the Oxford Dictionary of Idioms, been featured in the song ‘What it Says On The Tin’ by Katie Melua, and has even been used by Prime Minister David Cameron and on multiple occasions to summarise his preferred approach to politics.  The phrase has come to represent a product or policy that is open, honest and delivers against its promise.  The phrase is now used internationally and it’s not surprising it has become the third most known slogan of all time.  \xa0  \xa0  [1] The phrase is a registered trademark of Ronseal’s owner, the Sherwin-Williams Company , across the European Community for products including paints, varnishes and wood preservatives (E3085826)  [3] Source Creative Review February 2012  \xa0  The Ronseal phrase: It does exactly what it says on the tinThe Ronseal phrase: It does exactly what it says on the tin - BBC News  BBC News  The Ronseal phrase: It does exactly what it says on the tin  8 January 2013  Read more about sharing.  Close share panel  Prime Minister David Cameron used a famous advertising slogan to sum up the state of the coalition government at its halfway point, saying "it is a Ronseal deal - it does what it says on the tin". This phrase has entered the British vernacular, says its co-creator Dave Shelton.  David Cameron has said it before. In 2004, he said: "People are crying out for a kind of Ronseal politics - they want it to do what it says on the tin."  People use the phrase every day. On YouTube you can listen to Katie Melua\'s What It Says On The Tin . Google the line and you get millions of results.  In 1994, when my advertising partner Liz Whiston and I came up with the line "Does exactly what it says on the tin" for Ronseal, we never dreamed how it would enter the language.  About the author  Dave Shelton and Liz Whiston co-founded Bordello Advertising.  They have been working in advertising for more than 20 years.  Back then Ronseal was just another varnish fighting for market share against a lot of fancier lifestyle brands. Anyone wandering into a DIY superstore was faced with a wall of different solutions to whatever woodcare job they might be trying to tackle.  We had presented a number of different campaigns, all of which had been rejected by Ged Shields, the then marketing director of Ronseal for "trying too hard".  Shields wanted to de-mystify the product.  We decided what was needed wasn\'t puns or art.  Instead we would call a spade a spade.  We started to write a commercial that featured a | Question: Which product is advertised on TV with the slogan \'It does exactly what it says on the tin\'?', ['ronseal'], 'mrqa_triviaqa-validation-5026'), tensor(0.0320)), (('Context: For the third straight season, the number one seeds from both conferences met in the Super Bowl. The Carolina Panthers became one of only ten teams to have completed a regular season with only one loss, and one of only six teams to have acquired a 15–1 record, while the Denver Broncos became one of four teams to have made eight appearances in the Super Bowl. The Broncos made their second Super Bowl appearance in three years, having reached Super Bowl XLVIII, while the Panthers made their second Super Bowl appearance in franchise history, their other appearance being Super Bowl XXXVIII. Coincidentally, both teams were coached by John Fox in their last Super Bowl appearance prior to Super Bowl 50. | Question: How many teams up to Super Bowl 50 have been to the championship game eight times?', ['four'], 'mrqa_squad-validation-194'), tensor(0.0299)), (('Context: Honda Ballade  The Honda Ballade is a subcompact automobile built by Honda of Japan.  It began as a four-door higher equipment content version of the Civic in 1980.  The Ballade was developed at the same time the Honda Vigor appeared, which was a higher content Honda Accord.  The Ballade was sold exclusively in Japan at "Honda Verno" dealerships alongside the Vigor, Prelude, CR-X, and Quint.  In the UK it was launched at the same time as the very similar Triumph Acclaim with which it shared a Honda built engine.   Honda Ballade  The Honda Ballade is a subcompact automobile built by Honda of Japan.  It began as a four-door higher equipment content version of the Civic in 1980.  The Ballade was developed at the same time the Honda Vigor appeared, which was a higher content Honda Accord.  The Ballade was sold exclusively in Japan at "Honda Verno" dealerships alongside the Vigor, Prelude, CR-X, and Quint.  In the UK it was launched at the same time as the very similar Triumph Acclaim with which it shared a Honda built engine.   Honda CR-X  The Honda CR-X, originally launched as the Honda Ballade Sports CR-X in Japan, is a front-wheel-drive sport compact car manufactured by Honda between 1983 and 1991.  It was replaced by the Honda CR-X del Sol for the 1992 model year.  Although there are many supposed definitions for the acronym CR-X, the most widely accepted are "Civic rally cross", and "Civic renaissance model X".   Honda CR-X  The Honda CR-X, originally launched as the Honda Ballade Sports CR-X in Japan, is a front-wheel-drive sport compact car manufactured by Honda between 1983 and 1991.  It was replaced by the Honda CR-X del Sol for the 1992 model year.  Although there are many supposed definitions for the acronym CR-X, the most widely accepted are "Civic rally cross", and "Civic renaissance model X". | Question: The "civic rally cross" was sold alongside what other model sold exclusively in Japan at Honda Verno dealerships?', ['Honda Ballade'], 'mrqa_hotpotqa-validation-400'), tensor(0.0269)), (("Context: He produced artificial lightning, with discharges consisting of millions of volts and up to 135 feet long. Thunder from the released energy was heard 15 miles away in Cripple Creek, Colorado. People walking along the street observed sparks jumping between their feet and the ground. Sparks sprang from water line taps when touched. Light bulbs within 100 feet of the lab glowed even when turned off. Horses in a livery stable bolted from their stalls after receiving shocks through their metal shoes. Butterflies were electrified, swirling in circles with blue halos of St. Elmo's fire around their wings. | Question: What happened to nearby light bulbs?", ['glowed even when turned off', 'glowed'], 'mrqa_squad-validation-1516'), tensor(0.0233)), (("Context: Petra Kvitová career statistics  This is a list of the main career statistics of Czech professional tennis player Petra Kvitová.  To date, Kvitová has won 20 singles titles including two Grand Slam singles titles at the Wimbledon Championships, one WTA Tour Championships singles title, two WTA Premier Mandatory singles titles and four WTA Premier 5 singles titles.  She was also the bronze medallist at the 2016 Rio Olympics, a semifinalist at the 2010 Wimbledon Championships, 2012 Australian Open and 2012 French Open and a quarterfinalist at the 2011 Australian Open, 2012 Wimbledon Championships, 2013 Wimbledon Championships, 2015 US Open and 2017 US Open.  Kvitová reached her career-high ranking of world no. 2 on 31 October 2011.   2017 US Open (tennis)  The 2017 US Open was the 137th edition of tennis' US Open and the fourth and final Grand Slam event of the year.  It was held on outdoor hard courts at the USTA Billie Jean King National Tennis Center in New York City.  Experimental rules featured in qualifying for the main draw as well as in the junior, wheelchair and exhibition events. | Question: What edition of tennis' US Open was the 2017 US Open when Petra Kvitova was a quarterfinalist?", ['137th'], 'mrqa_hotpotqa-validation-409'), tensor(0.0200)), (("Context: The White Coat Ceremony ( WCC ) is a relatively new ritual in some medical ( MD , DO ) , dental , optometry , audiology , chiropractic , dietetic , occupational therapy , physical therapy , podiatric , pharmacy , physician assistant , pathologists ' assistant , nursing , naturopathic and veterinary schools that marks the student 's transition from the study of preclinical to clinical health sciences . At some schools , where students begin meeting patients early in their education , the white coat ceremony is held before the first year begins . It is an example of a matriculation . | Question: when do you get your white coat in pharmacy school ?", ["the student 's transition from the study of preclinical to clinical health sciences"], 'mrqa_naturalquestions-validation-5465'), tensor(0.0195)), (("Context: BBC - Radio 3 - Elgar/Enigma VariationsBBC - Radio 3 - Elgar/Enigma Variations  13 * * * (Romanza)  14 Finale: E. D. U.  The first performance of Elgar’s ‘Enigma’ Variations took place at St James’s Hall London on 19 June 1899, conducted by Hans Richter. It was Elgar’s most ambitious orchestral work to date and a further performance in Düsseldorf in 1901 went on to establish him as a composer of international importance: Richard Strauss declared that ‘here for the first time is an English composer who has something to say’.  Like most overnight successes, it was the result of years of hard work. Elgar was 42 years old when he completed the Variations and, despite bitter disappointments and frustrations, had steadily built up a reputation, first provincially, then at a national level. In particular, a series of cantatas of increasing size had revealed Elgar’s brilliant orchestration and growing mastery of large forms. Now that mastery was demonstrated on a symphonic scale through the time-honoured form of Theme and Variations, inviting comparisons with the greatest classical masters. But it was Elgar’s uniquely personal approach to the form that gave the ‘Enigma’ Variations its initial novelty and lasting appeal.  Elgar himself recalled how the work came to be conceived on the evening of 21 October 1898:  After a long day’s fiddle teaching in Malvern, I came home very tired. Dinner being over, my dear wife said to me, ‘Edward, you look like a good cigar,’ and having lighted it, I sat down at the piano. In a little while, soothed and feeling rested, I began to play, and suddenly my wife interrupted by saying, ‘Edward, that’s a good tune.’ I awoke from the dream: ‘Eh! tune, what tune!’ and she said, ‘Play it again, I like that tune.’ I played and strummed, and played, and then she exclaimed, ‘That’s the tune.’ And that tune is the theme of the Variations.  Many years later Elgar’s daughter Carice recounted the same incident in a BBC broadcast:  My father was at the piano, smoking his pipe, and when I went to bed I heard him playing what I thought were pretty tunes. My mother told me he was inventing music about his friends, and he turned to her and said, ‘Who’s that like?’ My mother replied, ‘I can’t quite say, but it’s exactly the way WMB goes out of the room.’  The grand scheme was established at the outset: 13 variations, 13 musical sketches of ‘my friends pictured within’, as the dedication eventually ran, and a final 14th variation representing the composer himself.  What of the ‘Enigma’ of the title? Before the first performance, Elgar said:  The ‘Enigma’ I will not explain – its ‘dark saying’ must be left unguessed, and I warn you that the apparent connection between the Variations and the Theme is often of the slightest texture; further, through and over the whole set another and larger theme ‘goes’, but is not played . . . So the principal Theme never appears, even as in some late dramas – eg Maeterlinck’s L’Intruse and Les sept Princesses – the chief character is never on stage.  More ink has probably been spilt over these sentences than on any other Elgarian topic, each of the dozens of proposed solutions adding yet another layer of mystery to an already ambiguous pronouncement. What is clear is that the name ‘Enigma’ applies only to the theme itself and not to the whole work. Writing in 1911 Elgar revealed that this work, commenced in a spirit of humour & continued in deep seriousness, contains sketches of the composer’s friends. It may be understood that these personages comment or reflect on the original theme & each one attempts a solution of the Enigma, for so the theme is called. The sketches are not ‘portraits’, but each variation contains a distinct idea founded on some particular personality or perhaps on some incident known | Question: Which musical work of 1898 features a section called 'Nimrod'?", ['Enigma Variations', 'Enigma’ Variations', 'enigma variations'], 'mrqa_triviaqa-validation-4729'), tensor(0.0190)), (('Context: Cor, Blimey!  Cor, Blimey!  is a 2000 TV film that follows the relationship between "Carry On" film actors Sid James (played by Geoffrey Hutchings) and Barbara Windsor (played by Samantha Spiro).   Samantha Spiro  Samantha Spiro (born 20 June 1968) is a double Olivier Award-winning English actress.  She is best known for portraying Barbara Windsor in the stage play "Cleo, Camping, Emmanuelle and Dick" and the television film "Cor, Blimey! ", DI Vivien Friend in "", and Melessa Tarly in the HBO series "Game of Thrones". | Question: Who plays opposite the double Olivier Award-winning English actress in the 2000 TV film following the relationship between "Carry On" film actors Sid James and Barbara Windsor?', ['Geoffrey Hutchings'], 'mrqa_hotpotqa-validation-2970'), tensor(0.0176)), (("Context: For a phylum with relatively few species, ctenophores have a wide range of body plans. Coastal species need to be tough enough to withstand waves and swirling sediment particles, while some oceanic species are so fragile that it is very difficult to capture them intact for study. In addition oceanic species do not preserve well, and are known mainly from photographs and from observers' notes. Hence most attention has until recently concentrated on three coastal genera – Pleurobrachia, Beroe and Mnemiopsis. At least two textbooks base their descriptions of ctenophores on the cydippid Pleurobrachia. | Question: Which group of ctenophore are are hardest to study?", ['oceanic species'], 'mrqa_squad-validation-4456'), tensor(0.0129)), (('Context: Greenland ( / ˈɡriːnlənd / ; Greenlandic : Kalaallit Nunaat , pronounced ( kalaːɬit nunaːt ) ; Danish : Grønland , pronounced ( ˈɡʁɶnˌlanˀ ) ) is an autonomous constituent country within the Kingdom of Denmark between the Arctic and Atlantic Oceans , east of the Canadian Arctic Archipelago . Though physiographically a part of the continent of North America , Greenland has been politically and culturally associated with Europe ( specifically Norway and Denmark , the colonial powers , as well as the nearby island of Iceland ) for more than a millennium . The majority of its residents are Inuit , whose ancestors began migrating from the Canadian mainland in the 13th century , gradually settling across the island . | Question: is greenland part of europe or north america ?', ['physiographically a part of the continent of North America'], 'mrqa_naturalquestions-validation-5502'), tensor(0.0125)), (("Context: A piñata ( / pɪnˈjɑːtə / , US pronunciation / pɪnˈjɑːdɑː / , Spanish pronunciation : ( piˈɲata ) ( listen ) ) is a container often made of papier - mâché , pottery , or cloth ; it is decorated , and filled with small toys or candy , or both , and then broken as part of a ceremony or celebration . Piñatas are commonly associated with Mexico . The idea of breaking a container filled with treats came to Europe in the 14th century , where the name , from the Italian pignatta , was introduced . The Spanish brought the European tradition to Mexico , although there were similar traditions in Mesoamerica , such as the Aztecs ' honoring the birthday of the god Huitzilopochtli in mid December . According to local records , the Mexican piñata tradition began in the town of Acolman , just north of Mexico City , where piñatas were introduced for catechism purposes as well as to co-opt the Huitzilopochtli ceremony . Today , the piñata is still part of Mexican culture , the cultures of other countries in Latin America , as well as the United States , but it has mostly lost its religious character . | Question: where did the tradition of the pinata come from ?", ['treats'], 'mrqa_naturalquestions-validation-10680'), tensor(0.0102)), (("Context: <Ol>  `` The Last Note of Freedom '' - David Coverdale   `` Deal for Life '' - John Waite   `` Break Through the Barrier '' - Tina Turner   `` Hearts in Trouble '' - Chicago   `` Trail of Broken Hearts '' - Cher   `` Knockin ' on Heaven 's Door '' - Guns N ' Roses   `` You Gotta Love Someone '' - Elton John   `` Show Me Heaven '' - Maria McKee   `` Thunderbox '' - Apollo Smile   `` Long Live the Night '' - Joan Jett & The Blackhearts   `` Gimme Some Lovin ' '' - Terry Reid ( Spencer Davis Group version appears in movie )  </Ol> | Question: who sings gim me some lovin in days of thunder ?", ['Spencer Davis Group'], 'mrqa_naturalquestions-validation-539'), tensor(0.0092)), (('Context: Kim Dong-wook  Kim Dong-wook (born July 29, 1983) is a South Korean actor.  After appearing in student short films and several minor parts, Kim became a star through his supporting role in the popular TV series "Coffee Prince" (2007), followed by box office hit "Take Off" (2009).  He then starred in "Happy Killers" (2010) and "Romantic Heaven" (2011), but it was his acclaimed performance as an obsessed and tormented king in 2012 period drama "The Concubine" that brought Kim the best reviews of his career yet.   The Concubine (film)  The Concubine (; lit.  "Royal Concubine: Concubine to the King") is a 2012 South Korean historical film directed by Kim Dae-seung.  Set in the Joseon Dynasty, it centers around Hwa-yeon (Jo Yeo-jeong), who becomes a royal concubine against her will, Kwon-yoo (Kim Min-joon), a man torn between love and revenge, and Prince Sung-won (Kim Dong-wook), who has his heart set on Hwa-yeon despite the countless women available to him.  These three characters form a love triangle which is ruled by dangerous passion.  The struggle to survive within the tight-spaced boundaries of the palace is intense, and only those who are strong enough to overcome the hell-like milieu can survive. | Question: What role did Kim Dong-wook play in the 2012 South Korean historical film directed by Kim Dae-seung?', ['an obsessed and tormented king'], 'mrqa_hotpotqa-validation-3971'), tensor(0.0083)), (('Context: Pi Day is an annual celebration of the mathematical constant π ( pi ) . Pi Day is observed on March 14 ( 3 / 14 in the month / day format ) since 3 , 1 , and 4 are the first three significant digits of π . In 2009 , the United States House of Representatives supported the designation of Pi Day . | Question: how long have we been celebrating pi day ?', ['2009'], 'mrqa_naturalquestions-validation-3028'), tensor(0.0020)), (('Context: Historically , computer language syntax was restricted to the ASCII character set ; in the absence of the × character , U + 002A * Asterisk became the de facto standard notation of the multiplication operator in computing . This selection is still reflected in the standard numeric keypad , where the arithmetic operations of addition , subtraction , multiplication , and division are represented by the + , - , * , and / keys , respectively . | Question: what is the multiplication sign on the computer ?', ['*'], 'mrqa_naturalquestions-validation-10364'), tensor(0.0004)), (('Context: What is a Mulligan in Golf? - About.com SportsWhat is a Mulligan in Golf?  What Is a \'Mulligan\' in Golf?  What Is a \'Mulligan\' in Golf?  This golfer looks like he needed a mulligan.\xa0 John Cumming/Photodisc/Getty Images  By Brent Kelley  Updated February 05, 2016.  A mulligan, most simply put, is a "do-over" in golf. Hit a bad shot? Take a mulligan and replay that stroke . Drop a ball on the spot from which you just played, and re-play. The first (bad) shot is not counted.  Are Mulligans \'Legal\'?  No. There is never a time, when playing under the\xa0 Rules of Golf , that a mulligan is "legal." Mulligans are not allowed under the rules.  But Mulligans Are Very Popular  But just because mulligans aren\'t "legal" doesn\'t mean their use isn\'t common and popular among recreational golfers. Many amateurs and recreational golfers - players just out to have a fun round with buddies, as opposed to serious golfers looking for competition - are lax in observing the rules anyway.  So mulligans are most often employed during friendly rounds by golf buddies; or during charity or playday tournaments where mulligans are sometimes sold. If mulligans are for sale at a charity tournament, that means the golfer can buy, say, three mulligans for a set price each.  continue reading below our video  What Are Mulligans?  The sale of mulligans is sometimes used as an additional fund-raiser at charitable events.  Common Ways of Using Mulligans  Do all golfers use mulligans in the same way? No - whatever a group of golfers agrees upon is what counts (unless you are using mulligans in something like a charity tournament or association outing setting - then do what the organizers tell you).  How many mulligans you get to use during a round, what types of shots you can use them for, and so on, are things that differ from golfer to golfer. So if you\'re playing with a new group and mulligans are in effect, you need to clarify what is allowed.  Here are some common ways that mulligans are used:  Some golfers limit the use of mulligans to the first tee only, or to the first and 10th tees only.  Some golfers use one mulligan per nine holes, but anywhere on each nine.  It\'s most common for mulligans to be used only off the tee , i.e., you can only use a mulligan to replay a drive. However, some groups allow mulligans from the fairway , too.  Less common is allowing mulligans from the rough or out of hazards, but some golfers even do that.  It is rarer still - rarely seen, in fact - for mulligans to be used on the putting green .  And some groups allow mulligans from just about anywhere on the golf course, but set a limit - say, three mulligans per round, or nine, or 18.  Clearly, there are many different ways that golfers use mulligans. If you have a regular group of friends you play with, and your group allows mulligans, you probably long ago settled into your own "rules" about using them.  Note that mulligans are common in the United States, but rarely seen in the UK, for example. As with all localized golf customs, formats and betting games , when playing with golfers you don\'t know clear up the ground rules before play starts to avoid possible confusion later.  And again, please note: If you are playing in a tournament, handicap round or other setting in which the Rules of Golf are strictly followed, you cannot play mulligans.  Why Is It Called a \'Mulligan\'?  Good question! And the fact is, nobody knows for sure how a do-over in golf came to be called a mulligan. There are multiple theories, however, and we go over some of them in our FAQ on the topic:  Why are mulligans called that?  Other Terms/Uses  Mulligans | Question: In which sport may a \'Mulligan\' be awarded?', ['golf', 'golfer'], 'mrqa_triviaqa-validation-5972'), tensor(-0.0022)), (('Context: Bodhi Natural Health Products | Bodhi Natural ProductsBodhi Natural Health Products | Bodhi Natural Products  Stockists  Welcome  “It is my wish that by using these products you can enjoy life to the fullest by having good health, hopefully relieve suffering, and of course protecting our beautiful planet by using only natural substances. All Bodhi products come from nature, and are organic”\xa0 Julie Herbison  Bodhi originated as Body Mind Balancing which started off as my business as a Natural Health Practitioner. Over time my products became more and more popular so the name Bodhi came about to keep it short and simple. Bodhi comes from the Bodhi Tree which is a type of fig tree. The Bodhi tree has played an important part in human history featuring strongly in religious history and mythology in various parts of the world.  The ficus religiosa, Bodhi tree, has large heart shaped leaves. It is one of the most sacred trees in India, Sri Lanka and Nepal, where it is venerated by both Hindus and Buddhists. The tree under which Siddhartha Gautama gained enlightenment over 2,600 years ago still grows today in North East India.  In Hindu religion, Vishnu is said to have been born under a Bodhi tree and is often depicted sitting on its heart-shaped leaves. The Bodhi tree is often planted in the grounds of temples and, of all the sacred trees of India, it is the most widely worshiped.  The Bodhi tree in Sri Lanka is located in Anuradhapura and is said to be the oldest tree in the world with a known planting date. This fig tree is said to have grown from a branch taken from the original Bodhi tree in India under which Buddha gained enlightenment.  For Homeopathic and Body Mind consultations please call  Julie Herbison  Bodhi Tree, Bodh Gaya - Sacred SitesBodhi Tree, Bodh Gaya  Bodhi Tree, Bodh Gaya  Buddhist Monks at Bodhi Tree (The site of Buddha\'s enlightenment) \xa0 \xa0\xa0  Bodh Gaya, located 100 km (62 mi) south of Patna in the Indian state of Bihar, is the most venerated sacred place in Buddhism. It is the place where Prince Siddhartha Guatama, while meditating beneath the Bodhi Tree, attained enlightenment and became the Buddha.  Traditional accounts say that, in the early years of the 4th century BC, Siddhartha Gautama saw the suffering of the world and wanted to be free from it. As a young man, following the ancient traditions of Hinduism, he sought out spiritual teachers. Inquiring of their knowledge, he diligently practiced various yogas and meditations. Seven years passed, the last three in extreme asceticism, yet still he had not achieved his goal of enlightenment.  Impression of Buddha feet, Bodh Gaya \xa0\xa0\xa0  Siddhartha then journeyed toward the ancient sacred forests of Uruvela (modern Gaya in Bihar, in north India) with the intention of finally and completely realizing the infinite. Guided by visionary dreams and following in the footsteps of the Buddhas of three previous ages, Krakucchanda, Kanakamuni and Kasyapa (who had each attained enlightenment at the site) Siddhartha sat beneath the Bodhi Tree. Touching the earth, thereby calling it to witness the countless lifetimes of virtue that had led him to this place of enlightenment, he resolved not to rise again until enlightenment was attained.  "Here on this seat my body may shrivel up, my skin, my bones, my flesh may dissolve, but my body will not move from this seat until I have attained Enlightenment, so difficult to obtain in the course of great periods of time".  As Siddhartha sat in deep meditation beneath the Bodhi Tree, Mara, the Dark Lord of Death, came to distract him from his endeavor. When the earth shook, confirming the truth of Gautama\'s words, Mara unleashed his army of demons. In the epic battle that ensued, Siddhartha\'s wisdom broke through Mara’s illusions. The power of his compassion transformed the demons\' weapons into flowers and Mara and all his forces fled. Three days and nights passed and Siddhartha’s intention was realized. He became the Buddha, meaning the ‘Enlightened One’.  The Mahabodhi Temple, Bodh Gaya, India \xa0 \xa0\xa0  The Buddha then spent the | Question: Who is said to have gained enlightenment sitting under the Bodhi Tree?', ['buddha', 'enlightened one'], 'mrqa_triviaqa-validation-3915'), tensor(-0.0071)), (('Context: BBC Wales - Arts - Children - Ivor The EngineBBC Wales  - Arts - Children  - Ivor The Engine  Ivor The Engine  Ivor The Engine  Last updated: 11 February 2009  "Not very long ago, in the top left-hand corner of Wales, there was a railway.  It wasn\'t a very long railway or a very important railway, but it was called The Merioneth and Llantisilly Rail Traction Company Limited, and it was all there was.  "And in a shed, in a siding at the end of the railway, lives the Locomotive of the Merioneth and Llantisilly Rail Traction Company Limited, which was a long name for a little engine so his friends just called him Ivor."  Ivor The Engine was a children\'s animation created by Oliver Postgate and Peter Firmin\'s Smallfilms television company. It was made in black and white for Associated Rediffusion and first broadcast in 1959, but was remade in colour for the BBC in 1975.  It told the story of a sometimes disobedient small green locomotive who worked for the Merioneth and Llantisilly Rail Traction Company Limited, although he dreamed of singing with the Grumbley and District Choral Society. In time he became first bass of the choir and transported the members from place to place.  His driver was Edwin Jones, known as Jones the Steam, who enjoyed fishing and daydreaming. Ivor\'s other friends included stationmaster Dai Station, portly choirmaster Evans the Song, fellow chorister Idris the Dragon, and fairground owner Morgan the Roundabout. Mr Morgan gave Ivor some pipes from his steam calliope to allow him to sing in the choir.  Behind the scenes  Ivor The Engine was inspired by Welshman Denzyl Ellis, a former railwayman whom Postgate met shortly after World War Two. Ellis, a former fireman with the Royal Scot train, described how locomotives came to life in the mornings after the engine fires had been lit.  The programme was Smallfilms\' first production. Although set in Wales, Ivor The Engine was actually made in Firmin\'s home in Blean near Canterbury, Kent, using stop-motion animation techniques. Cardboard cutouts painted with watercolours were used for the characters and backgrounds.  The programmes were written and narrated by Oliver Postgate and drawn and painted by Peter Firmin. Voices were performed by Oliver Postgate, Anthony Jackson and Olwen Griffiths - the only Welsh person involved in the production.  The distinctive puffing sound made as Ivor moved was voiced by Postgate, and the show\'s music was composed by Vernon Elliott. Ivor\'s \'voice\' was mostly sounded by a bassoon.  Postgate described himself as having been "intoxicated by the work of Dylan Thomas , and used to carry Under Milk Wood around in my pocket". Wales was a logical setting for the tale, and became romanticised in Postgate\'s fictionalised world.  "Wales is where you have little railways going along the tops of hills, which is much less boring that hurtling up the slumbering Midlands plain in the middle of the night," he told science fiction enthusiast Clive Banks , "so we decided it would be nice to set it in Wales."  "Ivor The Engine is entirely bogus as far as Wales is concerned - it\'s built entirely on a picture of Wales given by Dylan Thomas! Then, literally in the bath, I came to realise what the story was: the engine wanted to sing in the choir, which is obviously what a Welsh engine would want, so from then on it fell into place.  Postgate and Firmin created a map of the top left corner of north Wales where Ivor lived. It included viaducts, bridges, tunnels, towns, a mine and gasworks, and was strictly adhered to by the show\'s creators.  The episodes  The original series comprised six episodes, and told the story of Ivor getting his pipes and joining the choir. They were followed by two further series of 13 episodes, all of which were in black and white. Each lasted 10 minutes.  The animations were shown regularly by Associated Rediffusion until the company folded in 1968. In 1975 Smallfilms were given back the rights to the stories and began remaking the second | Question: In the children’s television series ‘Ivor the Engine’ what is the name of the dragon?', ['idris'], 'mrqa_triviaqa-validation-893'), tensor(-0.0086)), (('Context: The IPCC does not carry out research nor does it monitor climate related data. Lead authors of IPCC reports assess the available information about climate change based on published sources. According to IPCC guidelines, authors should give priority to peer-reviewed sources. Authors may refer to non-peer-reviewed sources (the "grey literature"), provided that they are of sufficient quality. Examples of non-peer-reviewed sources include model results, reports from government agencies and non-governmental organizations, and industry journals. Each subsequent IPCC report notes areas where the science has improved since the previous report and also notes areas where further research is required. | Question: What is \'grey literature\'?', ['non-peer-reviewed sources'], 'mrqa_squad-validation-8513'), tensor(-0.0104)), (('Context: It has been claimed that the transmission of the first episode was delayed by ten minutes due to extended news coverage of the assassination of US President John F. Kennedy the previous day; whereas in fact it went out after a delay of eighty seconds. The BBC believed that many viewers had missed this introduction to a new series due to the coverage of the assassination, as well as a series of power blackouts across the country, and they broadcast it again on 30 November 1963, just before episode two. | Question: What other event made the BBC concerned that viewers had not seen the premier of Doctor Who?', ['a series of power blackouts across the country', 'power blackouts'], 'mrqa_squad-validation-7746'), tensor(-0.0132)), (('Context: Sunday in the Park with George - Music Theatre InternationalSunday In The Park... | Music Theatre International  Music Theatre International  Request Licenses & Perusals, Pay Invoices  Your Web Profile  We are working quickly to merge our two logins.  Region  Sunday In The Park With George  Original Broadway Version (1984)  Inspired by George Seurat\'s famous painting, this poetic masterpiece explores the challenges in understanding life and art.  Inspired by the painting A Sunday Afternoon on the Island of La Grande Jatte by Georges Seurat, Sunday In The Park With George, Stephen Sondheim and James Lapine\'s stunning masterpiece merges past and present into beautiful, poignant truths about life, love and the creation of art. One of the most acclaimed musicals of our time, this moving study of the enigmatic painter Georges Seurat won a Pulitzer Prize and was nominated for an astounding 10 Tony Awards including Best Musical.  The days leading up to the completion of his most famous painting, A Sunday Afternoon on the Island of La Grande Jatte, Georges Seurat is struggling to make meaningful art and maintaining a relationship with his lover Dot. Amid the scorn of the artistic community, Seurat\'s artistic ability thrives while his love diminishes. A century later, Seurat\'s descendant - named George and also an artist - finds himself burnt out and in search of what artistic path to follow, but he finds the answer to his future in the past.  An ensemble of strong singing actors performs this challenging and heartbreaking work about our need to connect to the past, present and future. Sunday In The Park With George features two coveted starring roles made famous by the Broadway performances of Mandy Patinkin and Bernadette Peters. The show may be staged simply as in its original workshop production or with the grandeur of Seurat\'s masterpiece.  "Sunday in the Park with George" - About.com Education"Sunday in the Park with George"  "Sunday in the Park with George"  "Sunday in the Park with George"  From Canvas to Characters  By Rosalind Flynn  Updated February 27, 2016.  Anyone who reads or plans to attend a performance of Sunday in the Park With George by Stephen Sondheim and James Lapine should first spend some time looking at an image of the this painting by Georges Seurat :\xa0 Its original French title is Un dimanche après-midi à l\'Île de la Grande Jatte – English translation: “ Sunday Afternoon on the Island of la Grande Jatte .” (Click on the painting title to view the online image.)  The first act of Sunday in the Park With George occurs with Seurat the artist creating and perfecting his painting. The creative musical theatre artists who were inspired by this painting drew the other characters in Act One from figures in the painting.  Looking Closely at Art  It would not be surprising to learn that Sondheim and Lapine used their own version of Visual Thinking Strategies to examine this painting that is 10 feet wide and 6 and a half feet tall. Visual Thinking Strategies provide ways to look closely at works of art by asking observers to respond to these three questions:  continue reading below our video  10 Best Universities in the United States  1. What’s going on in this picture?  2. What do you see that\xa0makes you say that?  3. What more can we find?  In fact, it would be excellent to begin a study of this play by engaging students in doing precisely that – looking at an image of “Sunday Afternoon on the Island of la Grande Jatte” and asking them to respond to those questions.  What’s Going On? According to Sondheim and Lapine  The painting’s setting is the island of la Grande Jatte, which is located northwest of Paris in the Seine River. In 1884 (when Seurat began working on his painting), the island was a pastoral place where people could go to get away from city life. The musical’s creative artists pulled their characters from the canvas. Here’s a Who’s Who:  The most prominent figure is the woman in profile on the bottom right hand side. Sondheim and Lapine imagined her as Seurat’s mistress and they | Question: The musical \'Sunday In The Park With George\' was inspired by a painting by which artist?', ['georges seurat', 'seurat'], 'mrqa_triviaqa-validation-5937'), tensor(-0.0138)), (('Context: Often rules apply to all goods neutrally, but may have a greater practical effect on imports than domestic products. For such "indirect" discriminatory (or "indistinctly applicable") measures the Court of Justice has developed more justifications: either those in article 36, or additional "mandatory" or "overriding" requirements such as consumer protection, improving labour standards, protecting the environment, press diversity, fairness in commerce, and more: the categories are not closed. In the most famous case Rewe-Zentral AG v Bundesmonopol für Branntwein, the Court of Justice found that a German law requiring all spirits and liqueurs (not just imported ones) to have a minimum alcohol content of 25 per cent was contrary to TFEU article 34, because it had a greater negative effect on imports. German liqueurs were over 25 per cent alcohol, but Cassis de Dijon, which Rewe-Zentrale AG wished to import from France, only had 15 to 20 per cent alcohol. The Court of Justice rejected the German government\'s arguments that the measure proportionately protected public health under TFEU article 36, because stronger beverages were available and adequate labelling would be enough for consumers to understand what they bought. This rule primarily applies to requirements about a product\'s content or packaging. In Walter Rau Lebensmittelwerke v De Smedt PVBA the Court of Justice found that a Belgian law requiring all margarine to be in cube shaped packages infringed article 34, and was not justified by the pursuit of consumer protection. The argument that Belgians would believe it was butter if it was not cube shaped was disproportionate: it would "considerably exceed the requirements of the object in view" and labelling would protect consumers "just as effectively". In a 2003 case, Commission v Italy Italian law required that cocoa products that included other vegetable fats could not be labelled as "chocolate". It had to be "chocolate substitute". All Italian chocolate was made from cocoa butter alone, but British, Danish and Irish manufacturers used other vegetable fats. They claimed the law infringed article 34. The Court of Justice held that a low content of vegetable fat did not justify a "chocolate substitute" label. This was derogatory in the consumers\' eyes. A ‘neutral and objective statement’ was enough to protect consumers. If member states place considerable obstacles on the use of a product, this can also infringe article 34. So, in a 2009 case, Commission v Italy, the Court of Justice held that an Italian law prohibiting motorcycles or mopeds pulling trailers infringed article 34. Again, the law applied neutrally to everyone, but disproportionately affected importers, because Italian companies did not make trailers. This was not a product requirement, but the Court reasoned that the prohibition would deter people from buying it: it would have "a considerable influence on the behaviour of consumers" that "affects the access of that product to the market". It would require justification under article 36, or as a mandatory requirement. | Question: Which year was the case Commission v Italy that dealt with cocoa products?', ['2003'], 'mrqa_squad-validation-4253'), tensor(-0.0160)), (('Context: George Cross | British medal | Britannica.comGeorge Cross | British medal | Britannica.com  British medal  Academy Award  George Cross, a British civilian and military decoration, instituted in 1940 by King George VI for “acts of the greatest heroism or of the most conspicuous courage in circumstances of extreme danger.” The award, which can be conferred posthumously, is usually given to civilians, although it can be bestowed on military personnel for acts for which military decorations are not usually awarded. The George Cross superseded the Medal of the Order of the British Empire for Gallantry (commonly known as the Empire Gallantry Medal).  George Cross medal engraved on a tombstone.  Acmthompson  The island of Malta received the George Cross in recognition of its inhabitants’ gallantry in World War II. Recipients of this award may add G.C. after their names; the cross ranks second only to the Victoria Cross (the highest British military decoration). The cross is silver, with one side depicting St. George slaying the dragon and with the inscription “For Gallantry;” the other side gives the recipient’s name and the date of the award.  The George Medal, instituted at the same time as the George Cross, is analogous to it but is awarded for services not quite so outstanding as those which merit the George Cross. Recipients of this medal can add G.M. after their names. The medal is silver; one side has the effigy of the reigning British monarch, and the other side has St. George and the dragon with the inscription “The George Medal.”  Learn More in these related articles:  World War 2 Awards.com - George CrossWorld War 2 Awards.com - George Cross  George Cross  Number of awards in the database: 139  Total awarded for WW II:    139  Recipients A-Z  The George Cross -or GC- was instituted June 24th, 1941, by King George VI. This decision was actuated by an air raid on London in 1940 during which the Royal Palace was hit. After this incident, King George became emotionally more involved with his people and he realised what they had to go through during the raids, admiring even more their hardships and heroism. As a result, the George Cross and George Medal were largely designed by him.  The George Cross is the second highest British decoration and is awarded for "acts of the greatest heroism and bravery in circumstances of extreme danger". It was initially awarded to all citizens of the Commonwealth and was later extended to military personnel for actions that did not usually merit a military decoration. Recipients are entitled to add the letters GC after their names. Bars will be awarded for subsequent acts of bravery and the decoration can be awarded posthumously.  The GC is the successor to the Medal of the Order of the British Empire for Gallantry and its recipients were obliged to return it to the Central Chancellery of the Orders of Knighthood to have it replaced by a George Cross. In 1971, recipients of the Albert Medal could also exchange this for a George Cross.  The badge is made of silver. On the front is an image of St. George fighting the dragon surrounded by the inscription \'For Gallantry\', on the reverse are engraved the name of the recipient and the date of the award. Each arm of the cross bears the cypher of the reigning Monarch, during the war GRI or GRVI, after the war EIIR. The cross hangs from a silver bar adorned with laurel leaves by a silver ring. The ribbon is 1.75" (38 mm) wide and coloured blue with a miniature GC in its centre.  The George Cross was conferred upon the Island of Malta for the display of bravery of its citizens during the war. Four times a George Cross was conferred upon a woman, three of which posthumously. Three women were awarded the GC for resistance activities in occupied  territories during WW 2. One of them was Noor Inayat Khan. This Russian born woman was dropped over France July 16th, 1943, to assist the French resistance movement in their activities. Some three months later, she was betrayed and taken prisoner by the Gestapo (German Secret Police). Despite numerous interrogations she | Question: What is the inscription on the George Cross ?', ['for gallantry'], 'mrqa_triviaqa-validation-2096'), tensor(-0.0254)), (('Context: Afonwen  Afonwen (] ; Welsh: "Afon-wen" ) is a town in Flintshire, Wales.  It is situated just under four miles from the A55 North Wales Expressway and on the A541 Mold-Denbigh road.  At the 2001 Census, the population of Afonwen was included into the civil parish of Caerwys and was 1,319, with a total ward population of 2,496.   A55 road  The A55, also known as the North Wales Expressway (Welsh: "Gwibffordd Gogledd Cymru") and the Chester to Bangor Trunk Road, is a major road in Britain.  Its entire length is a dual carriageway primary route, with the exception of the point where it crosses the Britannia Bridge over the Menai Strait and several short sections where there are gaps in between the two carriageways.  All junctions are grade separated except for two roundabouts — one east of Penmaenmawr and one in Llanfairfechan.  The road originally ran from Chester to Bangor but was extended parallel to the A5 across Anglesey to just outside Holyhead Docks in 2001.  The road improvements have been part funded with European money, under the Trans-European Networks programme, as the route is designated part of Euroroute E22 (Holyhead - Leeds - Amsterdam - Hamburg - Malmö - Riga - Moscow - Perm - Ekaterinburg - Ishim). | Question: Afonwen is situated just under four miles from what road also known as the North Wales Expressway ?', ['A55'], 'mrqa_hotpotqa-validation-1888'), tensor(-0.0265)), (('Context: Sophomore - definition of sophomore by The Free DictionarySophomore - definition of sophomore by The Free Dictionary  Sophomore - definition of sophomore by The Free Dictionary  http://www.thefreedictionary.com/sophomore  Related to sophomore: Sophomore Album  soph·o·more  a. A second-year student in a US college.  b. A tenth-grade student in a US high school.  2. A person in the second year of carrying out an endeavor.  3. A three-year-old racehorse, usually in its second year of racing.  adj.  1. Of or relating to the second year of an endeavor, especially of attending a school or college.  2. Being the second in a series: a singer\'s sophomore album.  [Alteration (probably influenced by Greek sophos, wise, and mōros, stupid) of sophumer, from obsolete sophom, sophism, dialectic exercise, variant of sophism .]  sophomore  (Education) chiefly US and Canadian a second-year student at a secondary (high) school or college  adj  (of a book, recording, etc, by an artist) second: her sophomore album.  [C17: perhaps from earlier sophumer, from sophum, variant of sophism + -er1]  soph•o•more  (ˈsɒf əˌmɔr, -ˌmoʊr; ˈsɒf mɔr, -moʊr)  n.  a student in the second year at a high school, college, or university.  [1645–55; earlier sophumer, perhaps =sophum sophism + -er 1]  sophomore  A student in the second year of a college course, or a high-school student in the tenth grade.  ThesaurusAntonymsRelated WordsSynonymsLegend:  lowerclassman , underclassman - an undergraduate who is not yet a senior  Adj.  1.  sophomore - used of the second year in United States high school or college; "the sophomore class"; "his sophomore year"  intermediate - lying between two extremes in time or space or state; "going from sitting to standing without intermediate pushes with the hands"; "intermediate stages in a process"; "intermediate stops on the route"; "an intermediate range plane"  Translations  [ˈsɒfəmɔːʳ] N (US) → estudiante mf de segundo año GRADE  sophomore  n (US) → étudiant (e) m/f de deuxième année  modif  sophomore year → deuxième année f  Want to thank TFD for its existence? Tell a friend about us , add a link to this page, or visit the webmaster\'s page for free fun content .  Link to this page:  View in context  Perhaps the president of a corps notices that one of the membership who is no longer an exempt--that is a freshman-- has remained a sophomore some little time without volunteering to fight; some day, the president, instead of calling for volunteers, will APPOINT this sophomore to measure swords with a student of another corps; he is free to decline--everybody says so--there is no compulsion.  View in context  Then we\'ll be able to look as bored and sophisticated as any Sophomore of them all.  The "freshettes" stood about in detached groups of two or three, looking askance at each other; the "freshies," wiser in their day and generation, had banded themselves together on the big staircase of the entrance hall, where they were shouting out glees with all the vigor of youthful lungs, as a species of defiance to their traditional enemies, the Sophomores, a few of whom were prowling loftily about, looking properly disdainful of the "unlicked cubs" on the stairs.  against six Sophomores and a Freshman from the Gladiatorial College!  when he had slain all the sophomores and was dallying with the  Sophomore dictionary definition | sophomore definedSophomore dictionary definition | sophomore defined  Adjective  (not comparable)  (US) The second in a series , especially, the second of an artist\'s albums or the second of four years in a high school (tenth grade) or university.  The band\'s sophomore album built upon the success of their debut release, catapulting them to megastardom.  Noun  (plural sophomores | Question: A sophomore is a student in which year of a US college?', ['second'], 'mrqa_triviaqa-validation-2136'), tensor(-0.0269)), (('Context: Catherine Zeta Jones in "The Darling Buds of May" 1991 ...Catherine Zeta Jones in "The Darling Buds of May" 1991 - YouTube  Catherine Zeta Jones in "The Darling Buds of May" 1991  Want to watch this again later?  Sign in to add this video to a playlist.  Need to report the video?  Sign in to report inappropriate content.  The interactive transcript could not be loaded.  Loading...  Rating is available when the video has been rented.  This feature is not available right now. Please try again later.  Uploaded on Mar 31, 2007  Some clips of a young Catherine Zeta Jones as Mariette, in the tv series, "The Darling Buds of May".  "The Darling Buds of May" was a British television series first broadcast in 1991. It is set in an idyllic rural 1950s Kent, among a large, boisterous family. The show was very popular, and launched the acting career of Catherine Zeta Jones.  Category  Mariette - The Darling Buds of May (UK) Characters - ShareTVMariette - The Darling Buds of May (UK) Characters - ShareTV  Catherine Zeta-Jones (born September 25, 1969, in Swansea, West Glamorgan, Wales, UK, the daughter of ...  Character Bio  Mariette Larkin, the eldest Larkin child. Her name was created by  combining \'Marie\' and \'Antoinette\'. In the first episode, she is shown  to be the family beauty and slightly wild. However, she quickly marries  Charley and settles down with him for the rest of the series. She  appears to have inherited Pop\'s business sense, and at the end of the  series she and Charley buy and manage a local brewery. Her wedding  ceremony reveals her middle name as Jane.  Episode Screenshots  Profile: Catherine Zeta Jones - BBC NewsProfile: Catherine Zeta Jones - BBC News  BBC News  Close share panel  Image caption Catherine Zeta Jones became a household name after appearing in The Darling Buds of May  The showbusiness journey of Catherine Zeta Jones, a permanent fixture on the A-list for decades, has taken her from her home in Wales to the Hollywood hills.  Along the way, the 41-year-old has picked up an Oscar, a Tony, a CBE, countless plaudits and a smattering of bad reviews.  She married Hollywood actor Michael Douglas in a lavish New York ceremony in 2000, however the couple have now decided to take some time apart to "evaluate and work on their marriage".  Few could have predicted that a girl born to a sweet factory owner and a seamstress would go on to become one of the most famous actresses in the world.  At the age of 10, the Welsh star won a national talent contest singing a Shirley Bassey song before landing roles in various West End productions.  Image caption Zeta Jones started acting at a very young age  She got her big break at the age of 17, following a promotion from second understudy to the lead role in the hit musical 42nd Street.  But it was Zeta Jones\'s first major TV role, in the 1991 comedy drama The Darling Buds of May, that made her a household name.  Set in rural Kent in the 1950s, viewers fell in love with the Larkins, played by Sir David Jason and Pam Ferris.  Zeta Jones played their fresh-faced sweetheart daughter Mariette.  Squeaky-clean perception  Off camera, her personal life began to hit the headlines more than her acting ability.  Her relationship with Blue Peter presenter John Leslie was well documented. She also became involved with Simply Red star Mick Hucknall and Soldier Soldier actor Angus MacFadyen.  After leaving The Darling Buds of May, Zeta Jones turned her attention to film, but admitted finding it hard to shake off the public\'s squeaky-clean perception of her.  The thing that really upset me was the idea that I was this gold-digger  Catherine Zeta Jones answers the critics about her marriage to Michael Douglas  "I am afraid I have this image which is far | Question: What was the name of Catherine Zeta Jones character in The Darling Buds of May ?', ['mariette'], 'mrqa_triviaqa-validation-7018'), tensor(-0.0459)), (('Context: In the early years the College trained many Puritan ministers.[citation needed] (A 1643 publication said the school\'s purpose was "to advance learning and perpetuate it to posterity, dreading to leave an illiterate ministry to the churches when our present ministers shall lie in the dust".) It offered a classic curriculum on the English university model—\u200b\u200bmany leaders in the colony had attended the University of Cambridge—\u200b\u200bbut conformed Puritanism. It was never affiliated with any particular denomination, but many of its earliest graduates went on to become clergymen in Congregational and Unitarian churches. | Question: Was the school officially associated with any denomination?', ['It was never affiliated with any particular denomination', 'never'], 'mrqa_squad-validation-7149'), tensor(-0.0475)), (('Context: An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses . The same principle is used for funicular railways with two connected railway cars on inclined tracks , and for the elevators on the Eiffel Tower which counterbalance each other . Ski lifts are another example , where the gondolas move on a closed ( continuous ) pulley system up and down the mountain . The ski lift is similar to the counter-weighted elevator , but with a constraining force provided by the cable in the vertical dimension thereby achieving work in both the horizontal and vertical dimensions . Boat lifts are another type of counter-weighted elevator system approximating an Atwood machine . | Question: what is a real world application of an atwood machine ?', ['An elevator with a counterbalance'], 'mrqa_naturalquestions-validation-2730'), tensor(-0.0637)), (('Context: In anglophone academic works, theories regarding imperialism are often based on the British experience. The term "Imperialism" was originally introduced into English in its present sense in the late 1870s by opponents of the allegedly aggressive and ostentatious imperial policies of British prime Minister Benjamin Disraeli. It was shortly appropriated by supporters of "imperialism" such as Joseph Chamberlain. For some, imperialism designated a policy of idealism and philanthropy; others alleged that it was characterized by political self-interest, and a growing number associated it with capitalist greed. Liberal John A. Hobson and Marxist Vladimir Lenin added a more theoretical macroeconomic connotation to the term. Lenin in particular exerted substantial influence over later Marxist conceptions of imperialism with his work Imperialism, the Highest Stage of Capitalism. In his writings Lenin portrayed Imperialism as a natural extension of capitalism that arose from need for capitalist economies to constantly expand investment, material resources and manpower in such a way that necessitated colonial expansion. This conception of imperialism as a structural feature of capitalism is echoed by later Marxist theoreticians. Many theoreticians on the left have followed in emphasizing the structural or systemic character of "imperialism". Such writers have expanded the time period associated with the term so that it now designates neither a policy, nor a short space of decades in the late 19th century, but a world system extending over a period of centuries, often going back to Christopher Columbus and, in some accounts, to the Crusades. As the application of the term has expanded, its meaning has shifted along five distinct but often parallel axes: the moral, the economic, the systemic, the cultural, and the temporal. Those changes reflect - among other shifts in sensibility - a growing unease, even squeamishness, with the fact of power, specifically, Western power. | Question: What was the idealized value of imperialism?', ['idealism and philanthropy', 'philanthropy'], 'mrqa_squad-validation-10015'), tensor(-0.0706)), (("Context: May 14 : The Corps of Discovery departs from Camp Dubois at 4 p.m. , marking the beginning of the voyage to the Pacific coast .   May 16 : The Corps of Discovery arrives at St. Charles , Missouri .   May 21 : Departure from St. Charles at 3 : 30 p.m.   May 24 : Pass Boones Settlement . Home of famous woodsman L. Willenborg .   May 25 : The expedition passes the small village of La Charrette on the Missouri River . Charles Floyd writes in his journal that this is `` the last settlement of whites on this river '' .   June 1 : The expedition reaches the Osage River .   June 12 : Lewis and Clark meet three trappers in two pirogues . One of the men was Pierre Dorion , Jr. -- who knew George Rogers Clark . Lewis and Clark persuade Dorion to return to Sioux camp to act as interpreter .   June 26 : The expedition arrives at Kaw Point where the Kansas River drains into the Missouri River basin .   June 28 -- 29 : First trial in new territory . Pvt . John Collins is on guard duty and breaks into the supplies and gets drunk . Collins invites Pvt . Hugh Hall to drink also . Collins receives 100 lashes , Hall receives 50 lashes .   July 4 : Marking Independence Day , the expedition names Independence Creek located near Atchison , Kansas .   July 11 -- 12 : Second trial in new territory . Pvt . Alexander Hamilton Willard is on guard duty . Is charged with lying down and sleeping at his post whilst a sentinel . Punishable by death . He receives 100 lashes for four straight days .   July 21 : Reaches the Platte River , 640 miles from St. Louis . Entering Sioux Territory .   August 1 : Captain William Clark 's 34th birthday .   August 3 : The Corps of Discovery holds the first official council between representatives of the United States and the Oto and Missouri tribes at Council Bluffs , Iowa . They hand out peace medals , 15 - star flags and other gifts , parade men and show off technology .   August 4 : Moses Reed said he was returning to a previous camp to retrieve a knife but deserted to St. Louis .   August 18 : George Drouillard returns to camp with Reed and Otos ' Chief Little Thief . Reed is sentenced to run the gauntlet ( 500 lashes ) and is discharged from the permanent party .   August 18 : Captain Meriwether Lewis 's 30th birthday .   August 20 : Sergeant Charles Floyd dies . He dies from bilious chorlick ( ruptured appendix ) . He is the only member lost during the expedition .   August 23 : Pvt . Joseph Field kills first bison .   August 26 : Pvt . Patrick Gass is elected to sergeant . First election in new territory west of Mississippi River . George Shannon is selected to get the horses back from native Americans .   August 30 : A friendly council with the Yankton Sioux held . According to a legend , Lewis wraps a newborn baby in a United States flag and declares him `` an American '' .   September 4 : Reach the mouth of the Niobrara River .   September 7 : The expedition drives a prairie dog out of its den ( by pouring water into it ) to send back to Jefferson .   September 14 : Hunters kill and describe prairie goat ( antelope ) .   September 25 -- 29 : A band of Lakota Sioux demand one of the boats as a toll for moving further upriver . Meet with Teton Sioux . Close order drill , air gun demo , gifts of medals , military coat , hats , tobacco . Hard to communicate language problems . Invite chiefs on board keelboat , give each \u200b ⁄ glass whiskey , acted drunk wanted more . Two armed confrontations with Sioux . Some of the chiefs sleep on boat , move up river to another village , meet in lodge , hold scalp dance .   October 8 -- 11 : Pass Grand River home of the Arikara people , 2,000 + . Joseph Gravelins trader , lived with Arikara for 13 yrs . Pierre Antoine Tabeau lived in another village was from Quebec .   October 13 : Pvt . John Newman tried for | Question: on which river did the exploration of the louisiana purchase begin ?", ['the Missouri River'], 'mrqa_naturalquestions-validation-8948'), tensor(-0.0745)), (('Context: Br\'er RabbitBr\'er Rabbit, also spelled Bre\'r Rabbit or Brer Rabbit or Bruh Rabbit, is a central figure as Uncle Remus tells stories of the Southern United States. Br\'er Rabbit is a trickster who succeeds by his wits rather than by brawn, provoking authority figures and bending social mores as he sees fit. The Walt Disney Company later adapted this character for its 1946 animated motion picture Song of the South.  Tar-Baby story   In one tale, Br\'er Fox constructs a doll out of a lump of tar and dresses it with some clothes. When Br\'er Rabbit comes along he addresses the Tar-Baby amiably, but receives no response. Br\'er Rabbit becomes offended by what he perceives as the Tar-Baby\'s lack of manners, punches it, and in doing so becomes stuck. The more Br\'er Rabbit punches and kicks the tar "baby" out of rage, the more he gets stuck. When Br\'er Fox reveals himself, the helpless but cunning Br\'er Rabbit pleads, "please, Br\'er Fox, don\'t fling me in dat brier-patch," prompting Fox to do exactly that. As rabbits are at home in thickets, the resourceful Br\'er Rabbit uses the thorns and briers to escape. The story was originally published in Harper\'s Weekly by Robert Roosevelt; years later Joel Chandler Harris included his version of the tale in his Uncle Remus stories.  African origins   The Br\'er Rabbit stories can be traced back to trickster figures in Africa, particularly the hare that figures prominently in the storytelling traditions in West, Central, and Southern Africa. These tales continue to be part of the traditional folklore of numerous peoples throughout those regions. In the Akan traditions of West Africa, the trickster is usually the spider Anansi, though the plots in his tales are often identical with those of stories of Br\'er Rabbit.  However, Anansi does encounter a tricky rabbit called "Adanko" (Asante-Twi to mean "Hare") in some stories. The Jamaican character with the same name "Brer Rabbit", is an adaptation of the Ananse stories of the Akan people.   Some scholars have suggested that in his American incarnation, Br\'er Rabbit represented the enslaved Africans who used their wits to overcome adversity and to exact revenge on their adversaries, the White slave-owners.  Though not always successful, the efforts of Br\'er Rabbit made him a folk hero. However, the trickster is a multi-dimensional character. While he can be a hero, his amoral nature and his lack of any positive restraint can make him into a villain as well.   For both Africans and African Americans, the animal trickster represents an extreme form of behavior that people may be forced to adopt in extreme circumstances in order to survive. The trickster is not to be admired in every situation. He is an example of what to do, but also an example of what not to do. The trickster\'s behavior can be summed up in the common African proverb: "It\'s trouble that makes the monkey chew on hot peppers." In other words, sometimes people must use extreme measures in extreme circumstances.  Several elements in the Brer Rabbit Tar Baby story (e.g., rabbit needing to be taught a lesson, punches and head-butting the rabbit does, the stuck rabbit being swung around and around) are reminiscent of those found in a Zimbabwe-Botswana folktale.   Folklorists in the late 19th century first documented evidence that the American versions of the stories originated among enslaved West Africans based on connections between Br\'er Rabbit and Leuk, a rabbit trickster in Senegalese folklore.  The stories of Br\'er Rabbit were written down by Robert Roosevelt, an uncle of US President Theodore Roosevelt. Theodore Roosevelt wrote in his autobiography about his aunt from the State of Georgia, that "She knew all the \'Br\'er Rabbit\' stories, and I was brought up on them. One of my uncles, Robert Roosevelt, was much struck with them, and took them down from her dictation, publishing them in Harper\'s, where they fell flat. This was a good many years before a genius arose who, in \'Uncle Remus\', made the stories | Question: Who gets Brer Rabbit in a terrible tangle in the stories by Joel Chandler Harris?', ['Tar Baby', 'Tar-Baby', 'tar baby', 'tar "baby'], 'mrqa_triviaqa-validation-1616'), tensor(-0.0912)), (("Context: One of the best - known 18th Dynasty pharaohs is Amenhotep IV , who changed his name to Akhenaten in honor of the Aten , a representation of the Egyptian god , Ra . His exclusive worship of the Aten is often interpreted as history 's first instance of monotheism . Akhenaten 's wife , Nefertiti , contributed a great deal to his new take on the Egyptian religion . Nefertiti was bold enough to perform rituals to Aten . Akhenaten 's religious fervor is cited as the reason why he and his wife were subsequently written out of Egyptian history . Under his reign , in the 14th century BC , Egyptian art flourished under a distinctive style . ( See Amarna Period . ) | Question: what was the most important new religious figure of the new kingdom of ancient egypt ?", ['the Aten , a representation of the Egyptian god , Ra'], 'mrqa_naturalquestions-validation-6896'), tensor(-0.1036)), (('Context: The rainforest contains several species that can pose a hazard. Among the largest predatory creatures are the black caiman, jaguar, cougar, and anaconda. In the river, electric eels can produce an electric shock that can stun or kill, while piranha are known to bite and injure humans. Various species of poison dart frogs secrete lipophilic alkaloid toxins through their flesh. There are also numerous parasites and disease vectors. Vampire bats dwell in the rainforest and can spread the rabies virus. Malaria, yellow fever and Dengue fever can also be contracted in the Amazon region. | Question: What fish living in the Amazon river is known to bit humans?', ['piranha'], 'mrqa_squad-validation-4185'), tensor(-0.1287)), (('Context: David Perdue\'s Charles Dickens Page - David CopperfieldDavid Perdue\'s Charles Dickens Page - David Copperfield  David Copperfield  FAQ & Email  Dickens began writing an autobiography        in the late 1840s which he shared with his friend and future biographer, John Forster . Dickens found the        writing too painful and burned what he had written. He opted instead to        work his story into the fictional account of David Copperfield.  In the novel Dickens\' painful memories of being taken from school to work        at Warren\'s Blacking Factory while his father is in prison for debt are told through David\'s account        of Murdstone and Grinby\'s warehouse. The financial troubles of the Micawbers ,        with whom David was boarding at        the time, mirror Dickens\' parents, John and Elizabeth Dickens,        financial difficulties.  When David is asked by Mrs Micawber to take some of their treasured possessions to the pawn shop to help meet        their obligations, Dickens is recalling painful memories of having to pawn        off the very books he read and treasured as a child to ease his family\'s        financial woes.  On Dickens\' death Forster wrote The        Life of Charles Dickens , which is still the definitive biography        of Dickens, although many of the more negative aspects of Dickens life are        glossed over or missing altogether. Forster\'s biography included the autobiographical        fragment Dickens had given him. This was the first the public knew of Dickens\'        difficult childhood that had so heavily shaped his early work.  Oops...  Dickens originally introduced the        character of the dwarf, Miss Mowcher ,        as an aid to Steerforth\'s plan        to elope with Emily . Mrs. Jane Seymour        Hill, Dickens\' wife Catherine\'s chiropodist, recognized herself as the original for this character and threatened        a lawsuit . Dickens changed the character, in later monthly installments        of the novel, to an honest friend who abhors Steerforth\'s actions. She later        assists in the capture of Littimer .  Like Dickens, David teaches himself        shorthand and becomes a parliamentary reporter. David laments on the difficulties        encountered mastering this art:  "I bought an approved scheme of the noble art and mystery of stenography        (which cost me ten and sixpence); and plunged into a sea of perplexity that        brought me, in a few weeks, to the confines of distraction. The changes        that were rung upon dots, which in such a position meant such a thing, and        in such another position something else, entirely different; the wonderful        vagaries that were played by circles; the unaccountable consequences that        resulted from marks like flies\' legs; the tremendous effects of a curve        in a wrong place; not only troubled my waking hours, but reappeared before        me in my sleep."  Dickens hints at his feelings for politics when David says of his parliamentary        reporting:  "Britannia, that unfortunate female, is always before me, like a trussed        fowl: skewered through and through with office-pens, and bound hand and        foot with red tape. I am sufficiently behind the scenes to know the worth        of political life. I am quite an Infidel about it, and shall never be converted."  During the writing of David Copperfield        Dickens was actively involved in the day-to-day operation of Urania        Cottage , a home for homeless women, which he administered on behalf        of his friend, philanthropist Angela        Burdett Coutts . The home helped to separate homeless, and "fallen",        women from previous lifestyles, educate them in the execution of household        duties and self-discipline, and then help them emigrate to Australia to        begin new lives.  In David Copperfield, Dickens has several of the major characters emigrate        to Australia: the Micawbers , Mr.        Peggotty , Emily , Martha, and Mr.        Mell . Each of these characters are successful in beginning a new life        in the English colony.  Did Dickens visit the birthplace of David Copperfield? Visit the modern Blundeston where the debate rages and David Copperfield    lives on.  Dickens lampoons "the separate system" used at the | Question: What colour hair did Charles Dickens\' character David Copperfield have?', ['red'], 'mrqa_triviaqa-validation-1437'), tensor(-0.1333)), (('Context: The Super Bowl 50 Host Committee has vowed to be "the most giving Super Bowl ever", and will dedicate 25 percent of all money it raises for philanthropic causes in the Bay Area. The committee created the 50 fund as its philanthropic initiative and focuses on providing grants to aid with youth development, community investment and sustainable environments. | Question: What was the name of the fund setup to help with investing in the community?', ['50 fund', 'the 50 fund'], 'mrqa_squad-validation-392'), tensor(-0.1531)), (('Context: Nuclear power in space is the use of nuclear power in outer space , typically either small fission systems or radioactive decay for electricity or heat . Another use is for scientific observation , as in a Mössbauer spectrometer . One common type is a radioisotope thermoelectric generator , which has been used on many space probes and on manned lunar missions , and another is small fission reactors for Earth observation satellites such as the TOPAZ nuclear reactor . A radioisotope heater unit provides heat from radioactive decay of a material and can potentially produce heat for decades . | Question: how have scientists used nuclear power as a fuel for spacecraft ?', ['either small fission systems or radioactive decay for electricity or heat'], 'mrqa_naturalquestions-validation-3828'), tensor(-0.1608)), (('Context: MS Kronprins Harald  Several motor ships have borne the name Kronprins Harald, after Harald V of Norway:   Harald V of Norway  Harald V (] ; born 21 February 1937) is the King of Norway, having ascended the throne following the death of his father on 17 January 1991. | Question: What kind of ships have been named after the King of Norway who ascended the throne in 1991?', ['motor ships'], 'mrqa_hotpotqa-validation-5699'), tensor(-0.1669)), (('Context: TurbineA turbine (from the Latin turbo, a vortex, related to the Greek , tyrbē, meaning "turbulence"),   is a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work. A turbine is a turbomachine with at least one moving part called a rotor assembly, which is a shaft or drum with blades attached. Moving fluid acts on the blades so that they move and impart rotational energy to the rotor. Early turbine examples are windmills and waterwheels.  Gas, steam, and water turbines have a casing around the blades that contains and controls the working fluid. Credit for invention of the steam turbine is given both to the British engineer Sir Charles Parsons (1854–1931), for invention of the reaction turbine and to Swedish engineer Gustaf de Laval (1845–1913), for invention of the impulse turbine. Modern steam turbines frequently employ both reaction and impulse in the same unit, typically varying the degree of reaction and impulse from the blade root to its periphery.  The word "turbine" was coined in 1822 by the French mining engineer Claude Burdin from the Latin turbo, or vortex, in a memo, "Des turbines hydrauliques ou machines rotatoires à grande vitesse", which he submitted to the Académie royale des sciences in Paris.  Benoit Fourneyron, a former student of Claude Burdin, built the first practical water turbine.   Operation theory   A working fluid contains potential energy (pressure head) and kinetic energy (velocity head). The fluid may be compressible or incompressible. Several physical principles are employed by turbines to collect this energy:  Impulse turbines change the direction of flow of a high velocity fluid or gas jet. The resulting impulse spins the turbine and leaves the fluid flow with diminished kinetic energy. There is no pressure change of the fluid or gas in the turbine blades (the moving blades), as in the case of a steam or gas turbine, all the pressure drop takes place in the stationary blades (the nozzles). Before reaching the turbine, the fluid\'s pressure head is changed to velocity head by accelerating the fluid with a nozzle.  Pelton wheels and de Laval turbines use this process exclusively. Impulse turbines do not require a pressure casement around the rotor since the fluid jet is created by the nozzle prior to reaching the blades on the rotor. Newton\'s second law describes the transfer of energy for impulse turbines.  Reaction turbines develop torque by reacting to the gas or fluid\'s pressure or mass.  The pressure of the gas or fluid changes as it passes through the turbine rotor blades. A pressure casement is needed to contain the working fluid as it acts on the turbine stage(s) or the turbine must be fully immersed in the fluid flow (such as with wind turbines). The casing contains and directs the working fluid and, for water turbines, maintains the suction imparted by the draft tube. Francis turbines and most steam turbines use this concept. For compressible working fluids, multiple turbine stages are usually used to harness the expanding gas efficiently. Newton\'s third law describes the transfer of energy for reaction turbines.  In the case of steam turbines, such as would be used for marine applications or for land-based electricity generation, a Parsons type reaction turbine would require approximately double the number of blade rows as a de Laval type impulse turbine, for the same degree of thermal energy conversion. Whilst this makes the Parsons turbine much longer and heavier, the overall efficiency of a reaction turbine is slightly higher than the equivalent impulse turbine for the same thermal energy conversion.  In practice, modern turbine designs use both reaction and impulse concepts to varying degrees whenever possible. Wind turbines use an airfoil to generate a reaction lift from the moving fluid and impart it to the rotor. Wind turbines also gain some energy from the impulse of the wind, by deflecting it at an angle. Turbines with multiple stages may utilize either reaction or impulse blading at high pressure. Steam turbines were traditionally more impulse but continue to move towards reaction designs similar to those used in gas turbines. At low pressure the operating fluid medium expands in volume for small reductions in | Question: What main category of machines \'produce continuous power from a wheel/rotor, usually with vanes, revolved by fast flowing fluid\'?', ['turbine engines', 'reaction turbine', 'turbines', 'turbine', 'turbine blades', 'impulse turbine'], 'mrqa_triviaqa-validation-2376'), tensor(-0.1762)), (("Context: New York StadiumThe New York Stadium, known as the AESSEAL New York Stadium for sponsorship purposes, is a football stadium in Rotherham, South Yorkshire, England. Opened in July 2012, it is the home ground of Rotherham United.  History  Rotherham United announced their intention to construct a new community stadium when they moved away from Millmoor to the Don Valley Stadium in May 2008 after a dispute with the ground owner Ken Booth.  In January 2010 the club purchased the former site of the Guest and Chrimes Foundry to be used for the new stadium.  Outline planning permission for the stadium was granted in November 2010, and the first images were sketched shortly after.   The name of the stadium was announced as the 'New York Stadium' on 19 December 2011, chosen ahead of 'The Foundry' and 'The Waterfront Stadium'. The reason for the name is that the area of land that the stadium lies upon is called New York, and it was thought that it would be better to name the stadium after history and/or where the stadium is situated, like nearby stadiums Bramall Lane and Hillsborough. Chairman Tony Stewart also hopes that the name could bring investment from New York City or further afield, as the New York Yankees chairman had recently said that he wanted to invest in an English football team.   Construction started in June 2011 and the stadium was officially opened by Prince Edward, Duke of Kent on 12 March 2012.  The first game played at the stadium was a pre-season match between Rotherham and Barnsley, held on 21 July 2012.  The Millers won 2–1; the first goal in the stadium was scored by Jacob Mellis of Barnsley, and David Noble scored Rotherham's first goal in their new home. The New York Stadium made its league debut on 18 August 2012, in which Rotherham beat Burton Albion 3–0,  Daniel Nardiello scoring the first competitive goal in the ground.  On 16 April 2014, the stadium held an England under-18s game for the first time. The resulting match finished with England beating Germany 2–1. Over 9,000 fans attended the game.   The naming rights to the stadium were announced as having been bought by local company AESSEAL, in a press conference on 21 November 2014. Club chairman Tony Stewart said the deal was worth six figures annually, as a result of the deal. It was also suggested as being the biggest sponsorship deal of the club's history.   Design  The stadium has a 12,000 all-seated capacity, with the option to be able to increase the stadium's capacity if needed.  It cost approximately £20 million to construct.  The stadium includes The 1925 Club, a corporate hospitality suite.  Local businesses such as Norton Finance  and Premier Hytemp  were some of the first members.  At the beginning  of the 2014–15 season, a large video screen was installed in the north west corner of the stadium.  Stands  ;North Stand  The North Stand, known as the KCM Recycling Stand for sponsorship reasons, and often referred to as the New Tivoli, is the kop stand of the stadium. The KCM Recycling Stand holds 2000 home fans, and has the lettering of the club's initials—RUFC—in white across it. The stand sits behind one of the goals, opposite the away end.  ;West Stand  The West Stand, known as the Eric Twigg Pukka Pies Stand for sponsorship reasons, is the main stand of the stadium. It features the executive 1925 Lounge, and also the stand where the players walk under when entering the field of play. It holds 4000 home fans.  ;East Stand  The East Stand, known as the Ben Bennett Stand, is the family stand of the stadium. It holds 4000 home fans, as well as two built-in balcony-type structures for disabled people.  ;South Stand  The South Stand, known as the Morrison Stand, is a 2000 seated away stand. It sits behind a goal, with the family stand to the right, main stand to the left, and the kop directly opposite.  Records  *Record | Question: Which League 2 football team play home games at the New York Stadium?", ['rotherham united'], 'mrqa_triviaqa-validation-4856'), tensor(-0.1852)), (('Context: The Roosevelt Corollary was an addition to the Monroe Doctrine articulated by President Theodore Roosevelt in his State of the Union address in 1904 after the Venezuela Crisis of 1902 -- 03 . The corollary states that the United States will intervene in conflicts between European countries and Latin American countries to enforce legitimate claims of the European powers , rather than having the Europeans press their claims directly . | Question: who warned europe to stay out of the americas ?', ['Roosevelt Corollary', 'Monroe Doctrine'], 'mrqa_naturalquestions-validation-2501'), tensor(-0.2685)), (("Context: It is an allusion to Samuel Taylor Coleridge 's poem The Rime of the Ancient Mariner ( 1798 ) . In the poem , an albatross starts to follow a ship -- being followed by an albatross was generally considered a sign of good luck . However , the titular mariner shoots the albatross with a crossbow , which is regarded as an act that will curse the ship ( which indeed suffers terrible mishaps ) . Even when they are too thirsty to speak , the ship 's crew let the mariner know through their glances that they blame his action for the curse . The albatross is then literally hung around the mariner 's neck by the crew to symbolize his guilt in killing the bird . Thus , the albatross can be both an omen of good or bad luck , as well as a metaphor for a burden to be carried as penance . | Question: the rime of the ancient mariner albatross symbolism ?", ['a metaphor for a burden to be carried as penance', 'omen of good or bad luck'], 'mrqa_naturalquestions-validation-7017'), tensor(-0.2817)), (('Context: Great Train Robbery Anniversary - Heart Beds, Bucks ...Great Train Robbery Anniversary - Heart Beds, Bucks & Herts News  Great Train Robbery Anniversary  Great Train Robbery Anniversary  See pictures of the bridge near Leighton Buzzard where The Great Train Robbery took place fifty years ago.  See all photos (15) in this gallery  By John Stratford (@Heart_JohnS), 8th August 2013, 10:50  Fifty years after The Great Train Robbery, an event\'s taken place to praise police who searched for the men who robbed a mail train in Buckinghamshire.  You may also like  On 8 August 1963, a gang of robbers, masterminded by Bruce Reynolds, stopped the Glasgow-Euston overnight mail train as it passed close to Cheddington, near Leighton Buzzard.  On board were huge numbers of used bank notes.  Twelve of the robbers were jailed for a total of more than 300 years but more than one broke out of prison, including notorious criminal Ronnie Biggs, who spent over 30 years on the run before he finally returned to Britain in 2001 to face arrest.  Reynolds returned in 1968, five years after the crime, and was captured in Torquay and jailed for 25 years.  Two Buckinghamshire Constabulary police officers who were involved in the investigation attended a police commemoration event alongside serving Thames Valley Police officers at Eynsham Hall in Witney, Oxfordshire, on Wednesday 7 August 2013.  Keith Milner was a detective at Aylesbury at the time of the robbery, while John Woolley was a PC and discovered Leatherslade Farm near Brill, Buckinghamshire, where the men hid after committing the crime.  Biggs insisted in July 2013 that he was proud to have been part of the gang.  The famous fugitive, who celebrates his 84th birthday on 8 August 2013, escaped from prison in 1965 and spent 36 years on the run before finally being arrested and jailed in 2001.  Released from prison on compassionate grounds in 2009 due to ill health he is still alive, being cared for in a north London nursing home, and he has few regrets about the crime that made him a household name.  Biggs, who cannot speak and communicates through a spelling board, said: "If you want to ask me if I have any regrets about being one of the train robbers, my answer is, \'No!\'.  "I will go further: I am proud to have been one of them. I am equally happy to be described as the \'tea-boy\' or \'The Brain\'.  "I was there that August night and that is what counts. I am one of the few witnesses - living or dead - to what was \'The Crime of the Century\'.\'\'  But although he is proud to have been involved in the headline-grabbing crime, he admitted he does have some regrets.  "It is regrettable, as I have said many times, that the train driver was injured,\'\' he said. "And he was not the only victim.  "The people who paid the heaviest price for the Great Train Robbery are the families. The families of everyone involved in the Great Train Robbery, and from both sides of the track.  "All have paid a price for our collective involvement in the robbery. A very heavy price, in the case of my family.  "For that, I do have my regrets.\'\'  Ronnie\'s son Michael, who was born while Ronnie was on the run, spoke exclusively to Heart and said he isn\'t proud of his dad being a criminal, but he is proud of the man his dad is.  "I believe if you commit a crime, you have to do the time," he said, "but the time has to be proportional with the crime that you\'ve committed.  "The sad bit was that he went into the train robbery because he needed £500 to put a deposit on his house. About a week before the robbery, he won the £500 in a bet on the horses but there was no turning back then."  Listen to Michael Biggs | Question: What criminal offence took place in Cheddington, Buckinghamshire in August (8th) 1963?', ['great train robbery'], 'mrqa_triviaqa-validation-7369'), tensor(-0.3341)), (('Context: Luther\'s other major works on the Jews were his 60,000-word treatise Von den Juden und Ihren Lügen (On the Jews and Their Lies), and Vom Schem Hamphoras und vom Geschlecht Christi (On the Holy Name and the Lineage of Christ), both published in 1543, three years before his death. Luther argued that the Jews were no longer the chosen people but "the devil\'s people", and referred to them with violent, vile language. Citing Deuteronomy 13, wherein Moses commands the killing of idolaters and the burning of their cities and property as an offering to God, Luther called for a "scharfe Barmherzigkeit" ("sharp mercy") against the Jews "to see whether we might save at least a few from the glowing flames." Luther advocated setting synagogues on fire, destroying Jewish prayerbooks, forbidding rabbis from preaching, seizing Jews\' property and money, and smashing up their homes, so that these "envenomed worms" would be forced into labour or expelled "for all time". In Robert Michael\'s view, Luther\'s words "We are at fault in not slaying them" amounted to a sanction for murder. "God\'s anger with them is so intense," Luther concluded, "that gentle mercy will only tend to make them worse, while sharp mercy will reform them but little. Therefore, in any case, away with them!" | Question: How near to his death was the work published?', ['three years before', 'three years before his death'], 'mrqa_squad-validation-2629'), tensor(-0.3709)), (('Context: Alex the Dog  Alex the Dog was the advertising mascot for Stroh\'s beer in the 1980s and precursor to Budweiser\'s Spuds MacKenzie.  At the peak of his career, Alex appeared in parades, on "Good Morning America", and the "Today" show.  He even inspired a series of toys, posters, cologne, shampoo and hand lotion.  Hip-hop artist Tone Loc referenced Alex the Dog in his song "Funky Cold Medina".   Spuds MacKenzie  Spuds MacKenzie is a fictional dog character created for use in an extensive advertising campaign marketing Bud Light beer in the late 1980s.  The Spuds MacKenzie mascot and campaign were created by a 23-year-old art director, Jon Moore.  At the time he was working at Needham, Harper & Steers, a Chicago, Illinois, advertising agency.  The dog first showed up in a Bud Light Super Bowl XXI ad in 1987.  During the height of his popularity, large amounts of Spuds merchandise was available, such as plush toys and t-shirts. | Question: When Budweiser created a dog mascot to promote Bud Light in the 1980s, which rival brewer (with its own character Alex the Dog) were they competing with?', ["Stroh's"], 'mrqa_hotpotqa-validation-5802'), tensor(-0.3826)), (('Context: 67th Tony Awards  The 67th Annual Tony Awards were held June 9, 2013, to recognize achievement in Broadway productions during the 2012–13 season.  The ceremony returned to Radio City Music Hall in New York City, after two years at Beacon Theatre, and was broadcast live on CBS television.  Neil Patrick Harris hosted for the third consecutive year, his fourth time as host.  Awards in four of the eight acting categories, (Best Actress in a Play, Best Actor in a Musical, Best Actress in a Musical, Best Featured Actor in a Play) were given to African-American performers.  Furthermore, it is the second time in Tony history that both directing prizes went to women.  Garry Hynes and Julie Taymor had previously won in 1998. " Kinky Boots" had a season best 13 nominations and 6 awards.  Cyndi Lauper, who wrote the score for "Kinky Boots", is the first solo female winner for Best Original Score.   67th Tony Awards  The 67th Annual Tony Awards were held June 9, 2013, to recognize achievement in Broadway productions during the 2012–13 season.  The ceremony returned to Radio City Music Hall in New York City, after two years at Beacon Theatre, and was broadcast live on CBS television.  Neil Patrick Harris hosted for the third consecutive year, his fourth time as host.  Awards in four of the eight acting categories, (Best Actress in a Play, Best Actor in a Musical, Best Actress in a Musical, Best Featured Actor in a Play) were given to African-American performers.  Furthermore, it is the second time in Tony history that both directing prizes went to women.  Garry Hynes and Julie Taymor had previously won in 1998. " Kinky Boots" had a season best 13 nominations and 6 awards.  Cyndi Lauper, who wrote the score for "Kinky Boots", is the first solo female winner for Best Original Score.   Julie Taymor  Julie Taymor (born December 15, 1952) is an American director of theater, opera and film.  She is best known for directing the stage musical "The Lion King", for which she became the first woman to win the Tony Award for directing a musical, in addition to a Tony Award for Original Costume Design.   Julie Taymor  Julie Taymor (born December 15, 1952) is an American director of theater, opera and film.  She is best known for directing the stage musical "The Lion King", for which she became the first woman to win the Tony Award for directing a musical, in addition to a Tony Award for Original Costume Design. | Question: In what year did the director of The Lion King win the Tony Awards?', ['1998'], 'mrqa_hotpotqa-validation-3241'), tensor(-0.3829)), (('Context: Pangaea or Pangea ( / pænˈdʒiːə / ) was a supercontinent that existed during the late Paleozoic and early Mesozoic eras . It assembled from earlier continental units approximately 335 million years ago , and it began to break apart about 175 million years ago . In contrast to the present Earth and its distribution of continental mass , much of Pangaea was in the southern hemisphere and surrounded by a superocean , Panthalassa . Pangaea was the most recent supercontinent to have existed and the first to be reconstructed by geologists . | Question: what land mass was north america a part of about 300 million years ago ?', ['Pangaea or Pangea'], 'mrqa_naturalquestions-validation-9386'), tensor(-0.4138)), (('Context: Newton came to realize that the effects of gravity might be observed in different ways at larger distances. In particular, Newton determined that the acceleration of the Moon around the Earth could be ascribed to the same force of gravity if the acceleration due to gravity decreased as an inverse square law. Further, Newton realized that the acceleration due to gravity is proportional to the mass of the attracting body. Combining these ideas gives a formula that relates the mass () and the radius () of the Earth to the gravitational acceleration: | Question: How might gravity effects be observed differently according to Newton?', ['at larger distances', 'at larger distances.'], 'mrqa_squad-validation-10322'), tensor(-0.5170)), (('Context: Shaqtin\' a Fool  Shaqtin\' a Fool is a weekly segment from the television show "Inside the NBA", the postgame show of "NBA on TNT" following the conclusion of National Basketball Association (NBA) games airing on cable TV channel TNT.  It first aired during the 2011–12 NBA season, when retired NBA All-Star Shaquille O\'Neal voiced it upon joining the show and was created by Turner Sports producer Mike Goldfarb.  It highlights humorous and uncommon basketball plays that have occurred during NBA games in the past week.  O\'Neal is the host and presenter, while the other analysts in studio react and provide commentary.  Most often, those have been fellow "Inside" regulars Ernie Johnson, Kenny Smith, and Charles Barkley, but other "Inside" hosts have also participated, including Chris Webber, Grant Hill, Steve Smith and Matt Winer.   Matt Winer  Matthew Ward Winer (born January 10, 1969) is an American television personality who is currently working for Turner Sports.  Winer also was known for working eight years at ESPN.   Matt Winer  Matthew Ward Winer (born January 10, 1969) is an American television personality who is currently working for Turner Sports.  Winer also was known for working eight years at ESPN. | Question: Shaqtin\' a Fool has included which television personality who worked for 8 yearst at ESPN?', ['Matthew Ward Winer'], 'mrqa_hotpotqa-validation-4367'), tensor(-0.5699)), (('Context: The Internet protocol suite ( TCP / IP ) was developed by Robert E. Kahn and Vint Cerf in the 1970s and became the standard networking protocol on the ARPANET , incorporating concepts from the French CYCLADES project directed by Louis Pouzin . In the early 1980s the NSF funded the establishment for national supercomputing centers at several universities , and provided interconnectivity in 1986 with the NSFNET project , which also created network access to the supercomputer sites in the United States from research and education organizations . Commercial Internet service providers ( ISPs ) began to emerge in the very late 1980s . The ARPANET was decommissioned in 1990 . Limited private connections to parts of the Internet by officially commercial entities emerged in several American cities by late 1989 and 1990 , and the NSFNET was decommissioned in 1995 , removing the last restrictions on the use of the Internet to carry commercial traffic . | Question: when was the internet introduced to the public ?', ['in the very late 1980s'], 'mrqa_naturalquestions-validation-683'), tensor(-0.5898)), (('Context: Evolution of the adaptive immune system occurred in an ancestor of the jawed vertebrates. Many of the classical molecules of the adaptive immune system (e.g., immunoglobulins and T cell receptors) exist only in jawed vertebrates. However, a distinct lymphocyte-derived molecule has been discovered in primitive jawless vertebrates, such as the lamprey and hagfish. These animals possess a large array of molecules called Variable lymphocyte receptors (VLRs) that, like the antigen receptors of jawed vertebrates, are produced from only a small number (one or two) of genes. These molecules are believed to bind pathogenic antigens in a similar way to antibodies, and with the same degree of specificity. | Question: Evolution of what part of the immune system occurred in the evolutionary ancestor of jawed vertebrates?', ['adaptive', 'adaptive immune system', 'the adaptive immune system'], 'mrqa_squad-validation-6677'), tensor(-0.7086)), (('Context: During the mid-Eocene, it is believed that the drainage basin of the Amazon was split along the middle of the continent by the Purus Arch. Water on the eastern side flowed toward the Atlantic, while to the west water flowed toward the Pacific across the Amazonas Basin. As the Andes Mountains rose, however, a large basin was created that enclosed a lake; now known as the Solimões Basin. Within the last 5–10 million years, this accumulating water broke through the Purus Arch, joining the easterly flow toward the Atlantic. | Question: Where did water to the west of the Amazon drainage basin flow towards?', ['Pacific', 'the Pacific'], 'mrqa_squad-validation-4283'), tensor(-0.7612)), (('Context: Genghis Khan, the title is spelled in variety of ways in different languages such as Mongolian Chinggis Khaan, English Chinghiz, Chinghis, and Chingiz, Chinese: 成吉思汗; pinyin: Chéngjísī Hán, Turkic: Cengiz Han, Çingiz Xan, Çingiz Han, Chingizxon, Çıñğız Xan, Chengez Khan, Chinggis Khan, Chinggis Xaan, Chingis Khan, Jenghis Khan, Chinggis Qan, Djingis Kahn, Russian: Чингисхан (Čingiskhan) or Чингиз-хан (Čingiz-khan), etc. Temüjin is written in Chinese as simplified Chinese: 铁木真; traditional Chinese: 鐵木眞; pinyin: Tiěmùzhēn. | Question: What are alternate English spelling of Genghis?', ['Chinghiz, Chinghis, and Chingiz'], 'mrqa_squad-validation-6303'), tensor(-1.3192)), (('Context: CarmenCarmen (;) is an opera in four acts by French composer Georges Bizet. The libretto was written by Henri Meilhac and Ludovic Halévy, based on a novella of the same title by Prosper Mérimée. The opera was first performed at the Opéra-Comique in Paris on 3 March 1875, where  its breaking of conventions   shocked and scandalized its first audiences. Bizet died suddenly after the 33rd performance, unaware  that the work would achieve   international acclaim within the following ten years.  Carmen has since become one of the most popular and frequently performed operas in the classical canon;  the "Habanera" from act 1 and the "Toreador Song" from act 2 are among the best known of all operatic arias.  The opera is written in the genre of opéra comique with musical numbers separated by dialogue. It is set in southern Spain and tells the story of the downfall of Don José, a naïve soldier who is seduced by the wiles of the fiery gypsy Carmen. José abandons his childhood sweetheart and deserts from his military duties, yet loses Carmen\'s love to the glamorous toreador Escamillo, after which José kills her in a jealous rage. The depictions of proletarian life, immorality, and lawlessness, and the tragic death of the main character on stage, broke new ground in French opera and were highly controversial.  After the premiere, most reviews were critical, and the French public was generally indifferent. Carmen initially gained its reputation through a series of productions outside France, and was not revived in Paris until 1883; thereafter it rapidly acquired popularity at home and abroad. Later commentators have asserted that Carmen forms the bridge between the tradition of opéra comique and the realism or verismo that characterised late 19th-century Italian opera.  The music of Carmen has since been widely acclaimed for brilliance of melody, harmony, atmosphere, and orchestration, and for the skill with which Bizet musically represented the emotions and suffering of his characters. After the composer\'s death, the score was subject to significant amendment, including the introduction of recitative in place of the original dialogue; there is no standard edition of the opera, and different views exist as to what versions best express Bizet\'s intentions. The opera has been recorded many times since the first acoustical recording in 1908, and the story has been the subject of many screen and stage adaptations.  Background  In the Paris of the 1860s, despite being a Prix de Rome laureate, Bizet struggled to get his stage works performed. The capital\'s two main state-funded opera houses—the Opéra and the Opéra-Comique—followed conservative repertoires that restricted opportunities for young native talent.  Bizet\'s professional relationship with Léon Carvalho, manager of the independent Théâtre Lyrique company, enabled him to bring to the stage two full-scale operas, Les pêcheurs de perles (1863) and La jolie fille de Perth (1867), but neither enjoyed much public success.    When artistic life in Paris resumed after the Franco-Prussian War of 1870–71, Bizet found wider opportunities for the performance of his works; his one-act opera Djamileh opened at the Opéra-Comique in May 1872. Although this failed and was withdrawn after 11 performances,  it led to a further commission from the theatre, this time for a full-length opera for which Henri Meilhac and Ludovic Halévy would provide the libretto. Halévy, who had written the text for Bizet\'s student opera Le docteur Miracle (1856), was a cousin of Bizet\'s wife, Geneviève;  he and Meilhac had a solid reputation as the librettists of many of Jacques Offenbach\'s operettas.   Bizet was delighted with the Opéra-Comique commission, and expressed to his friend Edmund Galabert his satisfaction in "the absolute certainty of having found my path".Dean 1965, p. 100 The subject of the projected work was a matter of discussion between composer, librettists and the Opéra-Comique management; Adolphe de Leuven, on behalf of the theatre, made several suggestions that were politely rejected. It was Bizet who first proposed an adaptation of Prosper Mérimée\'s novella Carmen.  Mérimée\'s story is a blend of travelogue and adventure yarn, | Question: Who wrote the opera Carmen?', ['bizet'], 'mrqa_triviaqa-validation-2722'), tensor(-1.6620))]
09/23/2021 13:08:02 - INFO - __main__ - retrieved candidates ids = ['mrqa_naturalquestions-validation-1202', 'mrqa_triviaqa-validation-1924', 'mrqa_squad-validation-8542', 'mrqa_naturalquestions-validation-8119', 'mrqa_naturalquestions-validation-2248', 'mrqa_triviaqa-validation-671', 'mrqa_hotpotqa-validation-1968', 'mrqa_hotpotqa-validation-1626', 'mrqa_triviaqa-validation-4054', 'mrqa_hotpotqa-validation-14', 'mrqa_hotpotqa-validation-3632', 'mrqa_naturalquestions-validation-2753', 'mrqa_naturalquestions-validation-6157', 'mrqa_naturalquestions-validation-9688', 'mrqa_hotpotqa-validation-2679', 'mrqa_triviaqa-validation-1935', 'mrqa_hotpotqa-validation-1376', 'mrqa_squad-validation-5622', 'mrqa_naturalquestions-validation-1364', 'mrqa_naturalquestions-validation-5180', 'mrqa_hotpotqa-validation-1201', 'mrqa_squad-validation-3113', 'mrqa_squad-validation-7821', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-6351', 'mrqa_hotpotqa-validation-5325', 'mrqa_naturalquestions-validation-5144', 'mrqa_naturalquestions-validation-8448', 'mrqa_squad-validation-10410', 'mrqa_triviaqa-validation-5406', 'mrqa_triviaqa-validation-1551', 'mrqa_naturalquestions-validation-114']
09/23/2021 13:08:02 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:08:02 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=62) .... Timecode: 3
09/23/2021 13:08:15 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:08:15 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 3
09/23/2021 13:08:17 - INFO - __main__ - After Error Fixing: 0.9375
09/23/2021 13:08:17 - INFO - __main__ - Instant Fixing Rate: 0.9666666666666667
09/23/2021 13:08:17 - INFO - __main__ - Instant Retention Rate: 0.4999999975
09/23/2021 13:08:19 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_004.pt.
09/23/2021 13:08:19 - INFO - __main__ - Saving the current error examples (len=30) to the memory.
09/23/2021 13:08:19 - INFO - __main__ - Current memory size: 88.
09/23/2021 13:08:19 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:08:19 - INFO - __main__ - Finished.
09/23/2021 13:08:19 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:08:19 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:08:19 - INFO - __main__ - Evaluating to get errors .... Timecode: 4
09/23/2021 13:08:23 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:08:23 - INFO - __main__ - Found 26 errors.
09/23/2021 13:08:23 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:08:23 - INFO - __main__ - Current memory size: 118.
09/23/2021 13:08:23 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=118;
09/23/2021 13:08:29 - INFO - __main__ - interference_scores=[(('Context: There are many similarities and differences among teachers around the world. In almost all countries teachers are educated in a university or college. Governments may require certification by a recognized body before they can teach in a school. In many countries, elementary school education certificate is earned after completion of high school. The high school student follows an education specialty track, obtain the prerequisite "student-teaching" time, and receive a special diploma to begin teaching after graduation. In addition to certification, many educational institutions especially within the US, require that prospective teachers pass a background check and psychiatric evaluation to be able to teach in classroom. This is not always the case with adult further learning institutions but is fast becoming the norm in many countries as security concerns grow. | Question: In what country is a background check required?', ['US'], 'mrqa_squad-validation-2191'), tensor(2.7998)), (("Context: Interior Angles of Polygons - Maths ResourcesInterior Angles of Polygons  Interior Angles of Polygons  An Interior Angle is an angle inside a shape  Triangles  The Interior Angles of a Triangle add up to 180°  Let's try a triangle:  It works for this triangle  Now tilt a line by 10°:  80° + 70° + 30° = 180°  One angle went up by 10°,  and the other went down by 10°  Quadrilaterals (Squares, etc)  (A Quadrilateral has 4 straight sides)  Let's try a square:  90° + 90° + 90° + 90° = 360°  A Square adds up to 360°  Now tilt a line by 10°:  80° + 100° + 90° + 90° = 360°  It still adds up to 360°  The Interior Angles of a Quadrilateral add up to 360°  Because there are 2 triangles in a square ...  The interior angles in a triangle add up to 180° ...  ... and for the square they add up to 360° ...  ... because the square can be made from two triangles!  Pentagon  A pentagon has 5 sides, and can be  made from three triangles, so you know what ...  ... its interior angles add up to 3 × 180° = 540°  And when  it is regular (all angles the same), then each angle is 540° / 5 = 108°  (Exercise: make sure each triangle here adds up to 180°, and check that the pentagon's interior angles add up to 540°)  The Interior Angles of a Pentagon add up to 540°  The General Rule  Each time we add a side (triangle to quadrilateral, quadrilateral to pentagon, etc), we add another 180° to the total:  \xa0  Parallelograms and Trapezoids - Free Math HelpParallelograms and Trapezoids - Free Math Help  Related Pages  What is a Parallelogram?  A parallelogram is a quadrilateral in which both pairs of opposite sides are parallel.  Special relationships exist between the measures of consecutive angles, opposite angles and opposite sides of a parallelogram. A square is the most straightforward parallelogram, because it has 2 sets of parallel sides. Naturally, all of its angles and sides match in length or measure.  Facts About a Parallelogram  (1) The degree measure of the four angles of a parallelogram add up to 360 degrees. Remember that all quadrilaterals (4 sided figures) have angles which add up to 360 degrees. Here's a sample:  Then: a + b + c + d = 360 degrees  (2) The degree measure of any two consecutive angles add up to 180 degrees.                 In parallelogram ABCD:  angle a + angle b = 180 degrees  angle b + angle c = 180 degrees  angle c + angle d = 180 degrees  angle a + angle d = 180 degrees  (3) Opposite angles have the same measure in terms of degrees.  In parallelogram ABCD:  (1) Opposite sides are parallel:  side AD || side BC  NOTE: The symbol || means parallel.  (2) Opposite sides have the same lengths:  side AD = side BC  side AB = side CD  Diagonals of a Parallelogram  The diagonals of a parallelogram divide the parallelogram into two side-by-side triangles.  As shown in the picture below, diagonal AC forms equal alternate interior angles with each pair of parallel sides.  We can also see that there are two triangles in the picture below.  Triangle 1 is congruent to triangle 2 by ASA (Angle-Side-Angle) Method.  Where did the two triangles come from?  They were formed by diagonal AC.  I should also note that diagonals of a parallelogram bisect each other as shown in the picture below.  AE = EC  where E is the midpoint of BOTH diagonals.  Example:  In parallelogram WXYZ, the measure of angle X = 4a - 40 and the measure of angle Z | Question: The internal angles of a quadrilateral add up to how many degrees?", ['360'], 'mrqa_triviaqa-validation-5362'), tensor(1.6188)), (('Context: Why do Americans call it a period and the British a full stop?Why do Americans call it a period and the British a full stop? - Topic  \xa0\xa0Why do Americans call it a period and the British a full stop?  Go  Why do Americans call it a period and the British a full stop?  Cheers, Susan  IP  \xa0  I really don\'t know for sure, Susan, but my hunch (from what I know about our history) is that we call it a period just to be different from the British. That doesn\'t mean that the word was just "invented" by Americans. Actually, the word period used to mean the punctuation mark placed to signal the end of a sentence goes back to the early 16th century, if I remember right. So there is justification for calling it a period rather than a full stop.  After the Americans won their independence from Great Britain, the famous dictionary compiler, Noah Webster, determined that we should try to do things as differently from the British as possible to separate ourselves psychologically as much as we could. In that vein he changed lots of spellings. For example colour/color; gaol/jail; Geoffrey/Jeffery; tyre/tire). I wouldn\'t be surprised if that\'s why we now say period and the British say full stop.  (By the way, I\'ve always thought it silly to say full stop. A stop is a stop. How can you have a "full" stop? Can you have a "partial" stop as well? Would that be a comma? In that case, I\'d call it a "momentary stop," not a "partial" stop!  )  We even went so far as to change our table manners. When the British cut some food on their plates, they hold the fork in the left hand and the knife in the right hand. Then they pick the cut food up with the fork, still held in the left hand, and bring it to their mouths. Americans, in contrast, cut the food the same way, but then put down the knife and take the fork with their right hands. Then they bring the cut food to their mouths.  I know I\'ve gotten off the topic, but I thought you might find this interesting. There was a lot of psychology going on back in the early years of our independence from Great Britain, Susan!  This message has been edited. Last edited by: <Richard, Moderator>,  I would like to join in.  Thanks a lot for such informative reply.  Since the Americans want to be different from the British, I wonder why they i.e the Americans have spoken their language at first. They could have spoken Spanish, for example.  PS. You, the Americans try to be different form the British in grammar, vocabulary, spelling and pronunciation. And we, the learners of English take full credit to cope with such differences! That\'s not fair, is it!  This message has been edited. Last edited by: Izzy loves you all ,  Izzy loves you all  \xa0  I don\'t agree that Americans always want to be different from the British.  It was true in the past that, in declaring ourselves independent, out nation\'s founders tended to divorce ourselves from everything British. For example, at the beginning of our nation, we wanted to avoid any kind of royalty in a government, so great efforts were taken to make sure that a president could not be a king. And, people strove to make many concepts in language and writing different, too.  However, at some times now, and in some circles, things British are very much admired and copied.  People who love the British are called Anglophiles, and we have quite a few of them in the United States.  Still, if some Americans adopt a British accent, and call the letter Z "zed," and an elevator "a lift," the rest of us kind of smile and think the speaker is a | Question: We call it a ‘full stop’ what do Americans call it?', ['period'], 'mrqa_triviaqa-validation-1792'), tensor(1.6098)), (("Context: The comptroller ( who is also auditor general and head of the National Audit Office ) controls both the Consolidated Fund and the National Loans Fund . The full official title of the role is Comptroller General of the Receipt and Issue of Her Majesty 's Exchequer . | Question: who controls the consolidated fund of the state ?", ['The comptroller ( who is also auditor general and head of the National Audit Office )'], 'mrqa_naturalquestions-validation-1364'), tensor(1.1639)), (("Context: Kenya is active in several sports, among them cricket, rallying, football, rugby union and boxing. The country is known chiefly for its dominance in middle-distance and long-distance athletics, having consistently produced Olympic and Commonwealth Games champions in various distance events, especially in 800 m, 1,500 m, 3,000 m steeplechase, 5,000 m, 10,000 m and the marathon. Kenyan athletes (particularly Kalenjin) continue to dominate the world of distance running, although competition from Morocco and Ethiopia has reduced this supremacy. Kenya's best-known athletes included the four-time women's Boston Marathon winner and two-time world champion Catherine Ndereba, 800m world record holder David Rudisha, former Marathon world record-holder Paul Tergat, and John Ngugi. | Question: What sports are Kenyans active in?", ['cricket, rallying, football, rugby union and boxing'], 'mrqa_squad-validation-8542'), tensor(1.0468)), (('Context: Greenland ( / ˈɡriːnlənd / ; Greenlandic : Kalaallit Nunaat , pronounced ( kalaːɬit nunaːt ) ; Danish : Grønland , pronounced ( ˈɡʁɶnˌlanˀ ) ) is an autonomous constituent country within the Kingdom of Denmark between the Arctic and Atlantic Oceans , east of the Canadian Arctic Archipelago . Though physiographically a part of the continent of North America , Greenland has been politically and culturally associated with Europe ( specifically Norway and Denmark , the colonial powers , as well as the nearby island of Iceland ) for more than a millennium . The majority of its residents are Inuit , whose ancestors began migrating from the Canadian mainland in the 13th century , gradually settling across the island . | Question: is greenland part of europe or north america ?', ['physiographically a part of the continent of North America'], 'mrqa_naturalquestions-validation-5502'), tensor(1.0057)), (("Context: The United States two - dollar bill ( $2 ) is a current denomination of U.S. currency . The third U.S. President ( 1801 -- 09 ) , Thomas Jefferson , is featured on the obverse of the note . The reverse features an engraving of the painting The Declaration of Independence by John Trumbull . Throughout the $2 bill 's pre-1929 life as a large - sized note , it was issued as a United States Note , National Bank Note , silver certificate , Treasury or `` Coin '' Note and Federal Reserve Bank Note . When U.S. currency was changed to its current size , the $2 bill was issued only as a United States Note . Production went on until 1966 , when the series was discontinued . Ten years passed before the $2 bill was reissued as a Federal Reserve Note with a new reverse design . Two - dollar bills are seldom seen in circulation as a result of banking policies with businesses which has resulted in low production numbers due to lack of demand . This comparative scarcity in circulation , coupled with a lack of public knowledge that the bill is still in production and circulation , has also inspired urban legends and occasionally has created problems for people trying to use the bill to make purchases . | Question: when were 2 dollar bills stopped being made ?", ['current denomination of U.S. currency'], 'mrqa_naturalquestions-validation-2753'), tensor(0.7729)), (("Context: One of the best - known 18th Dynasty pharaohs is Amenhotep IV , who changed his name to Akhenaten in honor of the Aten , a representation of the Egyptian god , Ra . His exclusive worship of the Aten is often interpreted as history 's first instance of monotheism . Akhenaten 's wife , Nefertiti , contributed a great deal to his new take on the Egyptian religion . Nefertiti was bold enough to perform rituals to Aten . Akhenaten 's religious fervor is cited as the reason why he and his wife were subsequently written out of Egyptian history . Under his reign , in the 14th century BC , Egyptian art flourished under a distinctive style . ( See Amarna Period . ) | Question: what was the most important new religious figure of the new kingdom of ancient egypt ?", ['the Aten , a representation of the Egyptian god , Ra'], 'mrqa_naturalquestions-validation-6896'), tensor(0.6705)), (('Context: The industry called Nollywood - Vanguard NewsThe industry called Nollywood - Vanguard News  Vanguard News  A Nigerian newspaper and Online version of the Vanguard, a daily publication in Nigeria covering Niger delta, general national news, politics, business, energy, sports, entertainment, fashion,lifestyle human interest stories, etc  Search for:  Home » Special Report » The industry called Nollywood  The industry called Nollywood  9:58 pmIn Special Report by adekunle Comments  By McPhilips Nwachukwu  IN the beginning: The story of Nigeria’s Nollywood is like Ben Okri’s “river” in his novel, The Famished Road, “which becomes a road and branches out to the world.”  Like a river, which starts small from a tiny tributary, the industry began as a small effort by some persons with energetic entrepreneurial spirit, who wanted to do small budget films for commercial purposes; and suddenly branched out like an ocean into one huge market that today intersects the world of business interests of actors and actresses, script writers, production crews, distributors and regulatory bodies, even in the daunting face of threatening copyright infringement challenges.  Berthing in Nigeria in 1992, this entertainment industry popularly known as Nollywood is acclaimed by industry players to rank as the third largest movie industry in the world after India’s Bollywood and America’s Nollywood. According to movie scholars and critics, Nollywood industry, which despite its relative young emergence in the global movie market has successfully become an enviable artistic cultural vehicle with which African narrative is communicated to the world, ironically emerged as a child of necessity to fill the gap created by the total collapse of the country’s theatre and stage culture.  According to industry sources, the failure of government in providing adequate security surveillance over life and property in the late 80s’and early 90s’resulted in the very negative manner at which the public viewed public shows, especially as it had to do with cinema, stage and theatrical performances with disenchantment.  Video filmartistes at a film shooting site  Also added to this ugly development “was the fact that there were inadequate television stations across the country with enough entertainment contents to satisfy the yearning needs of the viewing public, a development that led\xa0content providers to seek alternative.”  “Nollywood was a child of necessity” says Professor Ahmed Yerima, theatre artiste and former Artistic Director of The National Troupe of Nigeria. He said: “There were very few television stations and we got to a point where the TV stations that existed needed you, the producer, to get sponsorship for your productions.”  Movie-theatre going culture  But for Obby Patrick Ebewo, Nigerian born and American based film scholar, “The collapse of movie-theatre going culture in the 1980s caused by incessant harassment of innocent citizen by criminals, the country’s economic downturn and various problems affecting celluloid film production, gave rise to video film.”  The implication of this new development, according to Yerima, was that fast thinking and business minded entrepreneurs cashed into the opportunity provided by this yearning need; and aided with the arrival of VHS tapes embarked on a business trial and experimentation that was to transform the continent’s entertainment turf.  Prior to this boom, efforts had been made by some dramatists to evolve a film culture in Nigeria. It is on record that as early as the 1960s’ theatre artistes in the mould of Hubert Ogunde, had recorded his plays on celluloid. Other artistes like Moses Adejumo and Duro Ladipo had equally attempted to reduce the cinematic art of film to a more popular medium for social commentary.  In an effort to trace the emergence of Nollywood and the entire film culture in the country, Ebewo argues that “the current video film industry in Nigeria owes a huge debt to the pioneering efforts of Nigerian theatre, particularly practitioners of the Yoruba Travelling theatre, who branched off from mainstream theatre to experiment with celluloid.”  However, the Nollywood brand today owes its projection and commercial resourcefulness to Kenneth Nnebue, who according to market consensus, pioneered the raving popular media brand with his movie, Living in Bondage, in 1992. According to unconfirmed sources, since inception, “the industry rakes gross estimate of | Question: "What country\'s film industry is called ""Nollywood""?" ?', ['nigeria'], 'mrqa_triviaqa-validation-3901'), tensor(0.5823)), (('Context: The Life and Crimes of Bonnie Parker and Clyde BarrowThe Life and Crimes of Bonnie Parker and Clyde Barrow  By Jennifer Rosenberg  Who Were\xa0Bonnie and Clyde?  It was during the Great Depression that Bonnie Parker and Clyde Barrow went on their two-year crime spree (1932-1934). The general attitude in the United States\xa0was against government and Bonnie and Clyde used that to their advantage. With an image closer to Robin Hood rather than mass murderers, Bonnie and Clyde captured the imagination of the nation.  Dates: Bonnie Parker (October 1, 1910 -- May 23, 1934); Clyde Barrow (March 24, 1909 -- May 23, 1934)  Also Known As: Bonnie Elizabeth Parker, Clyde Chestnut Barrow, The Barrow Gang  Overview of Bonnie and Clyde  In some ways it was easy to romanticize Bonnie and Clyde. They were a young couple in love who were out on the open road, running from the "big, bad law" who were "out to get them." Clyde\'s impressive driving skill got the gang out of many close calls, while Bonnie\'s poetry won the hearts of many. (Clyde loved Fords so much, he even wrote a letter to Henry Ford himself!)  continue reading below our video  Profile of Bonnie and Clyde  Although Bonnie and Clyde had killed people, they were equally known for kidnapping policemen who had caught up to them and then driving them around for hours only to release them, unharmed, hundreds of miles away. The two seemed like they were on an adventure, having fun while easily side-stepping the law.  As with any image, the truth behind Bonnie and Clyde was far from their portrayal in the newspapers. Bonnie and Clyde were responsible for 13 murders, some of whom were innocent people, killed during one of Clyde\'s many bungled robberies. Bonnie and Clyde lived out of their car, stealing new cars as often as possible, and lived off the money they stole from small grocery stores and gas stations.  While Bonnie and Clyde sometimes robbed banks, they never managed to walk away with very much money. Bonnie and Clyde were desperate criminals, constantly fearing what they were sure was to come -- dying in a hail of bullets from a police ambush.  Background of Bonnie  Bonnie Parker was born on October 1, 1910 in Rowena, Texas as the second of three children to Henry and Emma Parker. The family lived somewhat comfortably off Henry Parker\'s job as a bricklayer, but when he died unexpectedly in 1914, Emma Parker moved the family in with her mother in the small town of Cement City, Texas (now part of Dallas).  From all accounts, Bonnie Parker was beautiful. She stood 4\' 11" and weighed a mere 90 pounds. She did well in school and loved to write poetry. ( Two poems that she wrote while on the run helped make her famous.)  Bored with her average life, Bonnie dropped out of school at age 16 and married Roy Thornton. The marriage wasn\'t a happy one and Roy began to spend a lot of time away from home by 1927. Two years later, Roy was caught for robbery and sentenced to five years in prison. They never divorced.  While Roy was away, Bonnie worked as a waitress; however, she was out of a job just as the Great Depression was really getting started at the end of 1929.  Background of Clyde  Clyde Barrow was born on March 24, 1909 in Telico, Texas as the sixth of eight children to Henry and Cummie Barrow. Clyde\'s parents were tenant farmers , often not making enough money to feed their children. During the rough times, Clyde was frequently sent to live with other relatives.  When Clyde was 12-years old, his parents gave up tenant farming and moved to West Dallas where Henry opened up a gas station.  At that time, West Dallas was a very rough neighborhood and Clyde fit right in. Clyde and his older brother, Marvin Ivan "Buck" Barrow, were often in trouble with the law for they were frequently stealing things | Question: In which year in the 1930\'s were Bonnie and Clyde killed?', ['1934'], 'mrqa_triviaqa-validation-6683'), tensor(0.5186)), (('Context: Newton came to realize that the effects of gravity might be observed in different ways at larger distances. In particular, Newton determined that the acceleration of the Moon around the Earth could be ascribed to the same force of gravity if the acceleration due to gravity decreased as an inverse square law. Further, Newton realized that the acceleration due to gravity is proportional to the mass of the attracting body. Combining these ideas gives a formula that relates the mass () and the radius () of the Earth to the gravitational acceleration: | Question: How might gravity effects be observed differently according to Newton?', ['at larger distances', 'at larger distances.'], 'mrqa_squad-validation-10322'), tensor(0.5103)), (("Context: Prague: Six things you must do in the Czech capital city ...Prague: Six things you must do in the Czech capital city | Daily Mail Online  Prague: Six things you must do in the Czech capital city  comments  Prague is one of Europe's architectural gems, with more knockout picture-postcard views than some entire countries. Unsurprisingly, the Czech capital can also get busy as thousands of tourists jostle for space.  Here is a selection of things to do during a visit to the city - from taking in its best-loved sights to stopping for a drink at a beautifully restored cafe and watching a comic take on a famous Mozart opera.  1... New Jerusalem  Emperor Charles IV planned this intensely beautiful city as the ‘New Jerusalem’ in the 14th Century, and architects maintained the vision until its last golden age in the Thirties. The old centre, with outstanding buildings such as Valdstejn Palace, St James’ Church and St Vitus Cathedral, is still largely intact after 800 years and is now a World Heritage Site.  With nifty footwork you can outwit the crowds. Try walking the famous Charles Bridge before breakfast, and save the striking of the Astronomical Clock in the Old Town Square until the evening when it’s quieter. I also like the peacefulness of Mala Strana, the cafe-lined cobbled streets under Prague Castle. And take a tram to the Jewish Quarter and wander Petrin Hill for wonderful views over the city’s baroque roofs.  Atmospheric: Prague's beautiful Charles Bridge  2... Grand re-opening  How about this for a comeback? The Grand Cafe Orient ( www.grandcafeorient.cz ) opened in 1912 in the House of the Black Madonna, Prague’s architectural take on cubism, the art form Picasso championed. The cafe closed eight years later when cubism fell out of favour but reopened in 2005, faithfully recreated with period furniture and fittings. Try the apple strudel with vanilla ice cream.  Another venue, the Mysak pastry shop, reopened in 2008 inside the Mysak Gallery ( www.gallerymysak.cz ), with original doors, mosaics floors, a marble staircase and wooden alcoves. The signature dish is karamelovy pohar – ice cream topped with caramel, chocolate and walnuts.  RELATED ARTICLES  Share  3...Fighting for freedom  After the euphoria of spring 1968, and Czechoslovakia’s short-lived break for freedom from the Soviet Union, came deep gloom that summer when Russian tanks rolled in to consign this proud nation to 20 more years of oppression.  The Museum of Communism ( www.muzeumkomunismu.cz ) tells the story of those lost years. I came closest to Prague’s tragedy and eventual triumph on the steps of the National Museum in Wenceslas Square. I looked down on the bronze cross at the spot where Jan Palach burnt himself to death in protest in 1969. It's also where people celebrated their new-found freedom after 1989’s so-called Velvet Revolution.  Historic: The Duke of Bohemia is represented by an equestrian statue in Wenceslas Square  4...Recovered treasures  In 1989, William Lobkowicz returned to Prague from exile in Boston to restore the great estate his family had built up over 700 years. It had been confiscated first by the Nazis, then by the communists. Diligent legal teams gradually recovered land, thousands of books, paintings and pieces of furniture. William’s mission culminated in the opening of Lobkowicz Palace in 2007, with its exhibition of the best of the family’s reclaimed treasures. Star exhibits include two views of the River Thames by Canaletto and Haymaking by Pieter Bruegel the Elder.  Another of the city’s new attractions is totally different. The Kafka Museum ( www.kafkamuseum.cz ) celebrates the writer (he was born here) whose name stands for the way the ordinary man is defeated by ‘the system’.  5...Pulling the strings  With his mischievous sense of fun, Mozart would have approved of the National Marionette Theatre’s comic performances of his opera Don Giovanni ( www.mozart.cz ). I particularly loved the puppet cat hissing at an anguished diva in mid aria. Don Giovanni is Prague’s top claim to cultural fame – in 1787, Mozart staged its world premiere at the exquisite opera | Question: By what name do Czechs know their capital city, Prague?", ['prague'], 'mrqa_triviaqa-validation-6351'), tensor(0.4252)), (('Context: It has been claimed that the transmission of the first episode was delayed by ten minutes due to extended news coverage of the assassination of US President John F. Kennedy the previous day; whereas in fact it went out after a delay of eighty seconds. The BBC believed that many viewers had missed this introduction to a new series due to the coverage of the assassination, as well as a series of power blackouts across the country, and they broadcast it again on 30 November 1963, just before episode two. | Question: What other event made the BBC concerned that viewers had not seen the premier of Doctor Who?', ['a series of power blackouts across the country', 'power blackouts'], 'mrqa_squad-validation-7746'), tensor(0.4058)), (("Context: New York StadiumThe New York Stadium, known as the AESSEAL New York Stadium for sponsorship purposes, is a football stadium in Rotherham, South Yorkshire, England. Opened in July 2012, it is the home ground of Rotherham United.  History  Rotherham United announced their intention to construct a new community stadium when they moved away from Millmoor to the Don Valley Stadium in May 2008 after a dispute with the ground owner Ken Booth.  In January 2010 the club purchased the former site of the Guest and Chrimes Foundry to be used for the new stadium.  Outline planning permission for the stadium was granted in November 2010, and the first images were sketched shortly after.   The name of the stadium was announced as the 'New York Stadium' on 19 December 2011, chosen ahead of 'The Foundry' and 'The Waterfront Stadium'. The reason for the name is that the area of land that the stadium lies upon is called New York, and it was thought that it would be better to name the stadium after history and/or where the stadium is situated, like nearby stadiums Bramall Lane and Hillsborough. Chairman Tony Stewart also hopes that the name could bring investment from New York City or further afield, as the New York Yankees chairman had recently said that he wanted to invest in an English football team.   Construction started in June 2011 and the stadium was officially opened by Prince Edward, Duke of Kent on 12 March 2012.  The first game played at the stadium was a pre-season match between Rotherham and Barnsley, held on 21 July 2012.  The Millers won 2–1; the first goal in the stadium was scored by Jacob Mellis of Barnsley, and David Noble scored Rotherham's first goal in their new home. The New York Stadium made its league debut on 18 August 2012, in which Rotherham beat Burton Albion 3–0,  Daniel Nardiello scoring the first competitive goal in the ground.  On 16 April 2014, the stadium held an England under-18s game for the first time. The resulting match finished with England beating Germany 2–1. Over 9,000 fans attended the game.   The naming rights to the stadium were announced as having been bought by local company AESSEAL, in a press conference on 21 November 2014. Club chairman Tony Stewart said the deal was worth six figures annually, as a result of the deal. It was also suggested as being the biggest sponsorship deal of the club's history.   Design  The stadium has a 12,000 all-seated capacity, with the option to be able to increase the stadium's capacity if needed.  It cost approximately £20 million to construct.  The stadium includes The 1925 Club, a corporate hospitality suite.  Local businesses such as Norton Finance  and Premier Hytemp  were some of the first members.  At the beginning  of the 2014–15 season, a large video screen was installed in the north west corner of the stadium.  Stands  ;North Stand  The North Stand, known as the KCM Recycling Stand for sponsorship reasons, and often referred to as the New Tivoli, is the kop stand of the stadium. The KCM Recycling Stand holds 2000 home fans, and has the lettering of the club's initials—RUFC—in white across it. The stand sits behind one of the goals, opposite the away end.  ;West Stand  The West Stand, known as the Eric Twigg Pukka Pies Stand for sponsorship reasons, is the main stand of the stadium. It features the executive 1925 Lounge, and also the stand where the players walk under when entering the field of play. It holds 4000 home fans.  ;East Stand  The East Stand, known as the Ben Bennett Stand, is the family stand of the stadium. It holds 4000 home fans, as well as two built-in balcony-type structures for disabled people.  ;South Stand  The South Stand, known as the Morrison Stand, is a 2000 seated away stand. It sits behind a goal, with the family stand to the right, main stand to the left, and the kop directly opposite.  Records  *Record | Question: Which League 2 football team play home games at the New York Stadium?", ['rotherham united'], 'mrqa_triviaqa-validation-4856'), tensor(0.3627)), (('Context: Mary O\'Connell  Mary O\'Connell (better known as Sister Anthony, S.C.) (1814 – December 8, 1897) was an Irish immigrant to the United States, who became a Roman Catholic Religious Sister.  A Sister of Charity of Cincinnati, she served with distinction as a nurse on the front lines of the American Civil War.  Her work with the wounded and in health care in general caused her to be known as "the angel of the battlefield" and "the Florence Nightingale of America."  Her portrait hangs in the Smithsonian Institution in Washington, DC.   Mary O\'Connell  Mary O\'Connell (better known as Sister Anthony, S.C.) (1814 – December 8, 1897) was an Irish immigrant to the United States, who became a Roman Catholic Religious Sister.  A Sister of Charity of Cincinnati, she served with distinction as a nurse on the front lines of the American Civil War.  Her work with the wounded and in health care in general caused her to be known as "the angel of the battlefield" and "the Florence Nightingale of America."  Her portrait hangs in the Smithsonian Institution in Washington, DC.   Florence Nightingale  Florence Nightingale, ( ; 12 May 1820 – 13 August 1910) was an English social reformer and statistician, and the founder of modern nursing. | Question: What portrait hangs in the Smithsonian Institute along with what is known as the founder of nursing?', ['Sister Anthony, S.C.'], 'mrqa_hotpotqa-validation-1626'), tensor(0.2656)), (('Context: In anglophone academic works, theories regarding imperialism are often based on the British experience. The term "Imperialism" was originally introduced into English in its present sense in the late 1870s by opponents of the allegedly aggressive and ostentatious imperial policies of British prime Minister Benjamin Disraeli. It was shortly appropriated by supporters of "imperialism" such as Joseph Chamberlain. For some, imperialism designated a policy of idealism and philanthropy; others alleged that it was characterized by political self-interest, and a growing number associated it with capitalist greed. Liberal John A. Hobson and Marxist Vladimir Lenin added a more theoretical macroeconomic connotation to the term. Lenin in particular exerted substantial influence over later Marxist conceptions of imperialism with his work Imperialism, the Highest Stage of Capitalism. In his writings Lenin portrayed Imperialism as a natural extension of capitalism that arose from need for capitalist economies to constantly expand investment, material resources and manpower in such a way that necessitated colonial expansion. This conception of imperialism as a structural feature of capitalism is echoed by later Marxist theoreticians. Many theoreticians on the left have followed in emphasizing the structural or systemic character of "imperialism". Such writers have expanded the time period associated with the term so that it now designates neither a policy, nor a short space of decades in the late 19th century, but a world system extending over a period of centuries, often going back to Christopher Columbus and, in some accounts, to the Crusades. As the application of the term has expanded, its meaning has shifted along five distinct but often parallel axes: the moral, the economic, the systemic, the cultural, and the temporal. Those changes reflect - among other shifts in sensibility - a growing unease, even squeamishness, with the fact of power, specifically, Western power. | Question: What was the idealized value of imperialism?', ['idealism and philanthropy', 'philanthropy'], 'mrqa_squad-validation-10015'), tensor(0.1551)), (('Context: Gothard Wilhelm Butler  Gothard Wilhelm Butler (German: "Gotthard Wilhelm von Buttlar" , c. 1600 – January 18, 1660) was a Polish-Lithuanian nobleman and politician of Scottish origin, born in Kuldīga (Goldingen).  He was Grand treasurer of the Crown, the Crown court chamberlain and a captain of the guard of King John II Casimir Vasa and erderman of Prienai, Parnu and Bolesław.   John II Casimir Vasa  John II Casimir (Polish: "Jan II Kazimierz Waza" ; German: "Johann II.  Kasimir Wasa" ; Lithuanian: "Jonas Kazimieras Vaza" ; 22 March 1609 – 16 December 1672) was King of Poland and Grand Duke of Lithuania during the era of the Polish–Lithuanian Commonwealth, Duke of Opole in Upper Silesia, and titular King of Sweden 1648–1660.  In Poland, he is known and commonly referred as Jan Kazimierz.  His parents were Sigismund III Vasa (1566–1632) and Constance of Austria (1588–1631).  His older brother, and predecessor on the throne, was Władysław IV Vasa. | Question: By what name is the King that Gothard Wilhelm Butler was captain of the guard for known in Poland?', ['Jan Kazimierz'], 'mrqa_hotpotqa-validation-3632'), tensor(0.1449)), (('Context: Genghis Khan, the title is spelled in variety of ways in different languages such as Mongolian Chinggis Khaan, English Chinghiz, Chinghis, and Chingiz, Chinese: 成吉思汗; pinyin: Chéngjísī Hán, Turkic: Cengiz Han, Çingiz Xan, Çingiz Han, Chingizxon, Çıñğız Xan, Chengez Khan, Chinggis Khan, Chinggis Xaan, Chingis Khan, Jenghis Khan, Chinggis Qan, Djingis Kahn, Russian: Чингисхан (Čingiskhan) or Чингиз-хан (Čingiz-khan), etc. Temüjin is written in Chinese as simplified Chinese: 铁木真; traditional Chinese: 鐵木眞; pinyin: Tiěmùzhēn. | Question: What are alternate English spelling of Genghis?', ['Chinghiz, Chinghis, and Chingiz'], 'mrqa_squad-validation-6303'), tensor(0.1445)), (("Context: An acetate / ˈæsɪteɪt / is a salt formed by the combination of acetic acid with an alkaline , earthy , or metallic base . `` Acetate '' also describes the conjugate base or ion ( specifically , the negatively charged ion called an anion ) typically found in aqueous solution and written with the chemical formula C H O . The neutral molecules formed by the combination of the acetate ion and a positive ion ( called a cation ) are also commonly called `` acetates '' ( hence , acetate of lead , acetate of aluminum , etc . ) . The simplest of these is hydrogen acetate ( called acetic acid ) with corresponding salts , esters , and the polyatomic anion CH CO , or CH COO . | Question: what is the name for the ch3coo - ion ?", ['polyatomic anion'], 'mrqa_naturalquestions-validation-1202'), tensor(0.1361)), (('Context: In Hinduism the spiritual teacher is known as a guru, and, in many traditions of Hinduism - especially those common in the West - the emphasis on spiritual mentorship is extremely high, with gurus often exercising a great deal of control over the lives of their disciples. | Question: In what area is it common for spiritual mentorship to be extremely high?', ['the West'], 'mrqa_squad-validation-2069'), tensor(0.1323)), (('Context: The Super Bowl 50 Host Committee has vowed to be "the most giving Super Bowl ever", and will dedicate 25 percent of all money it raises for philanthropic causes in the Bay Area. The committee created the 50 fund as its philanthropic initiative and focuses on providing grants to aid with youth development, community investment and sustainable environments. | Question: What was the name of the fund setup to help with investing in the community?', ['50 fund', 'the 50 fund'], 'mrqa_squad-validation-392'), tensor(0.1293)), (("Context: In the early twentieth century , biologists thought that proteins carried genetic information . This was based on the belief that proteins were more complex than DNA . Phoebus Levene 's influential `` tetranucleotide hypothesis '' , which incorrectly proposed that DNA was a repeating set of identical nucleotides , supported this conclusion . The results of the Avery -- MacLeod -- McCarty experiment , published in 1944 , suggested that DNA was the genetic material , but there was still some hesitation within the general scientific community to accept this , which set the stage for the Hershey -- Chase experiment . | Question: what was the key factor that allowed hershey and chase ?", ['The results of the Avery -- MacLeod -- McCarty experiment'], 'mrqa_naturalquestions-validation-5818'), tensor(0.1183)), (('Context: Quran (Complete) - YouTubeQuran (Complete)  - YouTube  Quran (Complete)  38,356 views  Last updated on Jun 30, 2014  The Qur\'an (English pronunciation: /kɒˈrɑːn/ kor-AHN; Arabic: القرآن\u200e al-qur\'ān, IPA: [qurˈʔaːn], literally "the recitation") is the religious text of Islam,[ also sometimes transliterated as Quran, Kuran, Koran, Qur\'ān, Coran or al-Qur\'ān. It is widely regarded as the finest piece of literature in the Arabic language. Muslims hold that the Qur\'an is the verbal divine guidance and moral direction for mankind. Muslims also consider the original Arabic verbal text to be the final revelation of God the Final Testament  Quran: 78. Surat An Naba\' (The Tidings ) with ... - YouTubeQuran: 78. Surat An Naba\' (The Tidings ) with English Audio Translation and Transliteration HD - YouTube  Quran: 78. Surat An Naba\' (The Tidings ) with English Audio Translation and Transliteration HD  Want to watch this again later?  Sign in to add this video to a playlist.  Need to report the video?  Sign in to report inappropriate content.  The interactive transcript could not be loaded.  Loading...  Rating is available when the video has been rented.  This feature is not available right now. Please try again later.  Published on May 14, 2015  Please support the channel by subscribing, liking and commenting, may Allah reward you with Godness, Ameen https://www.youtube.com/user/kuranreader Islam, Prophet, prophets, Muhammad, Mohamed, Quran, Kuran, Kuranreader, Surat, Chapter, Ayah, Ayat, Sheikh, imam, Emotional, Truth, Emotional, relaxing, Soothing, Recitation, reading, Religion, Religious Text, Amazing , Beautful, Quran English Translation, Transliteration,beautiful quran recitation, quran, koran,kuran reader,quran reader,really beautiful,heart touchig ,relaxing,soothing,heart trembling,good voice, al minshawi, tajweed,emotional,tearful, quran download,surat,surah,most beautiful,sudais, abd albasit,mishari,quran reading ,learn arabic,learn quran,, قراءة, تلاوة , القارىء , مكتوب , مكتوبة, ترجمة , انجليزية,مرتل , مرتل, ,  Quran,Al Quran,the quran,The Holy Quran,Holy Quran,Quran Translation,Al Quran Translation,The Holy Quran Translation,Holy Quran Translation,Quran Bangla,Bangla Quran,Al Quran Bangla,Bangla Al Quran,Quran Bangla Translation,Bangla Quran Translation,Al Quran Bangla Translation,Koran,coran,Surah,Translatio\xadn,Islam,Abdul Rahman Al-Sudais,Imam,quran recitation,urdu quran,full quran,beautiful quran,quran beautiful recitation,urdu quran translation,quran karim,quran with urdu,tilawat,bacaan quran,bacaan al quran,merdu al quran,full al quran,surah al quran,suara merdu,ayat al quran,alquran,tilawah,tilawah al quran,quran recitation beautiful,beautiful quran,al quran recitation,best quran recitation,best quran,the quran recitation,full quran,quran recitation full,kicked the quran,the quran miracles,quran miracles,burning the quran,quran burning,miracles of quran,azab,burn the quran,the quran recitation,the quran full,full quran recitation,sudais full quran,mishary full quran,full quran download,full quran tilawat,beautiful quran recitation,quran recitation,beautiful recitation,most beautiful quran,best quran recitation,quran surah baqarah,quran surah yaseen,quran surah yasin,tilawat quran,quran surah rahman,quran surah kahf,quran surah rehman,quran surah fatiha,quran surah mulk,al quran karim,quran mp3,quran karim mp3,quran karim full,quran karim soudais,quran karim sudais,quran sudais,quran karim urdu,koran karim,coran karim,le coran,coran recitation,al coran,sourate coran,sourate,el coran,soudais,soudais coran,coran francais,coran islam,coran en francais,ecouter le coran,le coran arabe,le coran sourate,coran sourate,recite le coran,apprendre le coran, | Question: The Qur\'an (or Quran, Kuran, Koran, Coran or al-Qur\'an - literally \'the recitation\') is the religious text of which religion?', ['islam'], 'mrqa_triviaqa-validation-2210'), tensor(0.1177)), (('Context: Often rules apply to all goods neutrally, but may have a greater practical effect on imports than domestic products. For such "indirect" discriminatory (or "indistinctly applicable") measures the Court of Justice has developed more justifications: either those in article 36, or additional "mandatory" or "overriding" requirements such as consumer protection, improving labour standards, protecting the environment, press diversity, fairness in commerce, and more: the categories are not closed. In the most famous case Rewe-Zentral AG v Bundesmonopol für Branntwein, the Court of Justice found that a German law requiring all spirits and liqueurs (not just imported ones) to have a minimum alcohol content of 25 per cent was contrary to TFEU article 34, because it had a greater negative effect on imports. German liqueurs were over 25 per cent alcohol, but Cassis de Dijon, which Rewe-Zentrale AG wished to import from France, only had 15 to 20 per cent alcohol. The Court of Justice rejected the German government\'s arguments that the measure proportionately protected public health under TFEU article 36, because stronger beverages were available and adequate labelling would be enough for consumers to understand what they bought. This rule primarily applies to requirements about a product\'s content or packaging. In Walter Rau Lebensmittelwerke v De Smedt PVBA the Court of Justice found that a Belgian law requiring all margarine to be in cube shaped packages infringed article 34, and was not justified by the pursuit of consumer protection. The argument that Belgians would believe it was butter if it was not cube shaped was disproportionate: it would "considerably exceed the requirements of the object in view" and labelling would protect consumers "just as effectively". In a 2003 case, Commission v Italy Italian law required that cocoa products that included other vegetable fats could not be labelled as "chocolate". It had to be "chocolate substitute". All Italian chocolate was made from cocoa butter alone, but British, Danish and Irish manufacturers used other vegetable fats. They claimed the law infringed article 34. The Court of Justice held that a low content of vegetable fat did not justify a "chocolate substitute" label. This was derogatory in the consumers\' eyes. A ‘neutral and objective statement’ was enough to protect consumers. If member states place considerable obstacles on the use of a product, this can also infringe article 34. So, in a 2009 case, Commission v Italy, the Court of Justice held that an Italian law prohibiting motorcycles or mopeds pulling trailers infringed article 34. Again, the law applied neutrally to everyone, but disproportionately affected importers, because Italian companies did not make trailers. This was not a product requirement, but the Court reasoned that the prohibition would deter people from buying it: it would have "a considerable influence on the behaviour of consumers" that "affects the access of that product to the market". It would require justification under article 36, or as a mandatory requirement. | Question: Which year was the case Commission v Italy that dealt with cocoa products?', ['2003'], 'mrqa_squad-validation-4253'), tensor(0.1125)), (("Context: The Cold War was a state of geopolitical tension after World War II between powers in the Eastern Bloc ( the Soviet Union and its satellite states ) and powers in the Western Bloc ( the United States , its NATO allies and others ) . Historians do not fully agree on the dates , but a common timeframe is the period between 1947 , the year the Truman Doctrine , a U.S. foreign policy pledging to aid nations threatened by Soviet expansionism , was announced , and either 1989 , when communism fell in Eastern Europe , or 1991 , when the Soviet Union collapsed . The term `` cold '' is used because there was no large - scale fighting directly between the two sides , but they each supported major regional wars known as proxy wars . | Question: who were the major countries involved in the cold war ?", ['the Soviet Union', 'the United States'], 'mrqa_naturalquestions-validation-5180'), tensor(0.1014)), (('Context: CarmenCarmen (;) is an opera in four acts by French composer Georges Bizet. The libretto was written by Henri Meilhac and Ludovic Halévy, based on a novella of the same title by Prosper Mérimée. The opera was first performed at the Opéra-Comique in Paris on 3 March 1875, where  its breaking of conventions   shocked and scandalized its first audiences. Bizet died suddenly after the 33rd performance, unaware  that the work would achieve   international acclaim within the following ten years.  Carmen has since become one of the most popular and frequently performed operas in the classical canon;  the "Habanera" from act 1 and the "Toreador Song" from act 2 are among the best known of all operatic arias.  The opera is written in the genre of opéra comique with musical numbers separated by dialogue. It is set in southern Spain and tells the story of the downfall of Don José, a naïve soldier who is seduced by the wiles of the fiery gypsy Carmen. José abandons his childhood sweetheart and deserts from his military duties, yet loses Carmen\'s love to the glamorous toreador Escamillo, after which José kills her in a jealous rage. The depictions of proletarian life, immorality, and lawlessness, and the tragic death of the main character on stage, broke new ground in French opera and were highly controversial.  After the premiere, most reviews were critical, and the French public was generally indifferent. Carmen initially gained its reputation through a series of productions outside France, and was not revived in Paris until 1883; thereafter it rapidly acquired popularity at home and abroad. Later commentators have asserted that Carmen forms the bridge between the tradition of opéra comique and the realism or verismo that characterised late 19th-century Italian opera.  The music of Carmen has since been widely acclaimed for brilliance of melody, harmony, atmosphere, and orchestration, and for the skill with which Bizet musically represented the emotions and suffering of his characters. After the composer\'s death, the score was subject to significant amendment, including the introduction of recitative in place of the original dialogue; there is no standard edition of the opera, and different views exist as to what versions best express Bizet\'s intentions. The opera has been recorded many times since the first acoustical recording in 1908, and the story has been the subject of many screen and stage adaptations.  Background  In the Paris of the 1860s, despite being a Prix de Rome laureate, Bizet struggled to get his stage works performed. The capital\'s two main state-funded opera houses—the Opéra and the Opéra-Comique—followed conservative repertoires that restricted opportunities for young native talent.  Bizet\'s professional relationship with Léon Carvalho, manager of the independent Théâtre Lyrique company, enabled him to bring to the stage two full-scale operas, Les pêcheurs de perles (1863) and La jolie fille de Perth (1867), but neither enjoyed much public success.    When artistic life in Paris resumed after the Franco-Prussian War of 1870–71, Bizet found wider opportunities for the performance of his works; his one-act opera Djamileh opened at the Opéra-Comique in May 1872. Although this failed and was withdrawn after 11 performances,  it led to a further commission from the theatre, this time for a full-length opera for which Henri Meilhac and Ludovic Halévy would provide the libretto. Halévy, who had written the text for Bizet\'s student opera Le docteur Miracle (1856), was a cousin of Bizet\'s wife, Geneviève;  he and Meilhac had a solid reputation as the librettists of many of Jacques Offenbach\'s operettas.   Bizet was delighted with the Opéra-Comique commission, and expressed to his friend Edmund Galabert his satisfaction in "the absolute certainty of having found my path".Dean 1965, p. 100 The subject of the projected work was a matter of discussion between composer, librettists and the Opéra-Comique management; Adolphe de Leuven, on behalf of the theatre, made several suggestions that were politely rejected. It was Bizet who first proposed an adaptation of Prosper Mérimée\'s novella Carmen.  Mérimée\'s story is a blend of travelogue and adventure yarn, | Question: Who wrote the opera Carmen?', ['bizet'], 'mrqa_triviaqa-validation-2722'), tensor(0.0971)), (('Context: Alex the Dog  Alex the Dog was the advertising mascot for Stroh\'s beer in the 1980s and precursor to Budweiser\'s Spuds MacKenzie.  At the peak of his career, Alex appeared in parades, on "Good Morning America", and the "Today" show.  He even inspired a series of toys, posters, cologne, shampoo and hand lotion.  Hip-hop artist Tone Loc referenced Alex the Dog in his song "Funky Cold Medina".   Spuds MacKenzie  Spuds MacKenzie is a fictional dog character created for use in an extensive advertising campaign marketing Bud Light beer in the late 1980s.  The Spuds MacKenzie mascot and campaign were created by a 23-year-old art director, Jon Moore.  At the time he was working at Needham, Harper & Steers, a Chicago, Illinois, advertising agency.  The dog first showed up in a Bud Light Super Bowl XXI ad in 1987.  During the height of his popularity, large amounts of Spuds merchandise was available, such as plush toys and t-shirts. | Question: When Budweiser created a dog mascot to promote Bud Light in the 1980s, which rival brewer (with its own character Alex the Dog) were they competing with?', ["Stroh's"], 'mrqa_hotpotqa-validation-5802'), tensor(0.0964)), (("Context: Famous Jockeys - Racing-InsiderDiscover Who Are The Most Successful Jockeys of All Time  William Hill  Famous Jockeys  Fred Archer (11 January 1857 –8 November 1886), was an English flat race jockey of the Victorian era. Nicknamed “The Tin Man”, and described as “as the best all-round jockey that the turf has ever seen”, Archer was the holder of several records which lasted deep into the 20th century. He was Champion Jockey for 13 consecutive years until 1886, claiming 2,748 victories from 8,064 starts. He won a total of 21 classic races, including Epson Derby five times. He committed suicide at the age of 29 after the loss of his wife during childbirth.  Sir Gordon Richards (5 May 1904 – 10 November 1986) was one of the England’s finest jockeys, often considered the world’s greatest ever jockey. He is still the only jockey to have been knighted. Gordon Richard was the British flat racing Champion Jockey 26 times. He amassed a total of 4,870 winners, and a record 12 consecutive winners ridden. In 1999, the Racing Post (leading racing newspaper in Britain) placed him at number 1 in their list of the top 50 jockeys of 20th century.  Lester Piggott (born 5 November 1935) – is a retired professional jockey and one of the most successful English flat racing jockeys of all time. Nicknamed “The Long Fellow” to his height, Lester Piggott introduced a new style of race-riding that was adopted all over the world and enabled him to become Champion Jockey eleven times. Piggott boasts 4,493 career wins including 30 British classics.  William Lee “Bill” Shoemaker (August 19, 1931 – October 12, 2003) is one of the all-time legends who have graced the sport. Referred to as “The Shoe”, he held the world record of number of professional jockey victories for 29 years. Shoemaker used his small size (1.50 m) to his advantage riding 8,833 winners. A high school dropout, Shoemaker went to ride a total of 40,350 races, and win the United States Champion Jockey by earnings , a record 10 times. He was inducted into the National Museum of Racing and Hall of Fame in 1958.  Russell Avery Braze (born 7 August 1958) is the record holder of the most race wins in North American horse racing history . He’s victory counter stopped at the astonishing 12,007 number. Baze won 400 or more races per year for four consecutive years and got rewarded with a special Eclipse Award in 1995. Since then he has on over 400 races a year seven additional times; an achievement that no other jockey has accomplished more than three times. Russell Baze was inducted into the National Museum of Racing and Hall of Fame in 1999. He also received the prestigious George Woolf Memorial Jockey Award in 2002.  Julieann Louise “Julie” Krone (born July 24, 1963) is one of the most successful female jockeys in North American horse racing history. Julie became the first female to win a Triple Crown Race when she claimed the Belmont Stakes, riding on Colonial Affair. She sustained number of severe injuries while racing, but always came back stronger. Because of her never give up attitude, Krone was named by USA Today as one of the 10 Toughest Athletes and was honored with the Wilma Rudolph Courage Award by the Women’s Sports Foundation. In 2000 she became the first woman inducted into the National Museum of Racing and Hall of Fame.  Racing: Dettori determined not to be fearful of the Derby ...Racing: Dettori determined not to be fearful of the Derby | The Independent  Sport  Racing: Dettori determined not to be fearful of the Derby  Winning the premier Classic for the first time at Epsom on Saturday would help erase a very bad memory for an Italian jockey.  Sunday 30 May 1999 23:02 BST  Click to follow  The Independent Online  IN THE build-up to the Derby they used to ask what Lester Piggott was going to ride. This week the question is being posed of the man who has taken over from the Long Fellow as Britain's dominant jockey.  In the next | Question: Which horse racing jockey was sometimes referred to as the long fellow?", ['lester piggott'], 'mrqa_triviaqa-validation-365'), tensor(0.0806)), (("Context: 2016 Blancpain GT Series Sprint Cup  The 2016 Blancpain GT Series Sprint Cup was the fourth season following on from the demise of the SRO Group's FIA GT1 World Championship (an auto racing series for grand tourer cars), the third with the designation of Blancpain Sprint Series or Blancpain GT Series Sprint Cup.  After developing their partnership, Blancpain and the SRO decided that 2016 would see both the Sprint and Endurance Series further integrated into the Blancpain GT Series, putting the emphasis on the prestigious overall drivers' and manufacturers' titles causing the Sprint Series name to change from Blancpain Sprint Series to Blancpain GT Series Sprint Cup.   FIA GT1 World Championship  The FIA GT1 World Championship was a world championship sports car racing series developed by the SRO Group and regulated by the Fédération Internationale de l'Automobile (FIA), held from 2010 to 2012.  It featured multiple grand tourer race cars based on production road cars and conforming with the GT1 (2010–2011) and GT3 (2012) regulations competing in one-hour races on multiple continents.  All cars were performance balanced with weight and restrictor adjustments to artificially equalise their performance.  Championships were awarded each season for drivers and teams. | Question: What years was the auto racing series held that was followed on by the 2016 Blancpain GT Series Sprint Cup?", ['2010 to 2012'], 'mrqa_hotpotqa-validation-1376'), tensor(0.0747)), (('Context: Fox\'s Glacier MintsFox\'s Glacier Mints are the leading, branded boiled mint in the UK. They have been manufactured by Fox\'s Confectionery in Leicester since 1918. The mints were developed by Eric Fox, one of the original founders of Fox\'s Confectionery. Since 1922 the mints have been sold with the Peppy the polar bear icon. Peppy is typically depicted as though standing on one of the mints. Glacier Mints resemble miniature blocks of ice and are clear and translucent. Companion products are Fox\'s Glacier Fruits and Fox\'s Glacier Dark.  There is no connection with Fox Glacier in New Zealand.  On Impulse - Packaging TodayOn Impulse - Packaging Today  On Impulse  4 December 2002  Competition in the confectionery market has reached boiling point and is forcing the big brands to provide added value to their products and the packaging in which they come. Today, the impulse purchase sector and promotions play a far more significant role in developing brand loyalty. Rodney Abbott  reports  Do you remember those happy carefree childhood days when you spent your hard-saved pennies or shillings on sweets at the corner shop? I do and I am going back over half a century – which is why I refer to pennies and shillings. Even then the shopkeeper\'s display was sufficiently expansive to keep my mind occupied for several minutes before I could make up my mind exactly what I wanted to buy.  Today, supermarkets and specialist confectioners, even confectionists tobaconistsand news agents, offer a mind boggling array of products that makes it almost impossible to finalise a choice without undertaking an inventory. The battle for brand supremacy is fierce and the confectionery supplier has to find increasingly innovative and cost-effective pack and marketing concepts to move products off the shelf.  According to market analysts Nielsen, the sugar confectionery market [mints, fruits, gums, traditional sweets etc.] is worth £1036M. The mint market alone is worth £146M and the fruits market is worth a massive £378M. Both of those markets have been growing at 1% and 4%, respectively, over the last year.  I sought timely refuge at one of the most British of confectionery suppliers – Leicester-based Fox\'s – which is so proud of its 105-year old heritage.  Mind you, the company has seen some changes in it\'s history, having been taken over by Rowntree, Nestlé and, more recently, Northern Foods which has just acquired Paynes – renown for its Poppets and Just Brazils.  Since the arrival of Northern Foods, the company has not let the grass grow under its feet.  Since the start of the year, it has put Fox\'s back on TV and undertaken various on pack promotions. Last month, it relaunched the brand in a bid to take it forward.  I asked brand manager Emma Gilbert to explain the reasoning behind the relaunch, which was preceded by careful market research with small but focused consumer groups through packaging designers Siebert Head.  "Loyal consumers – adults aged 35 and above – allied Fox\'s with Glacier Mints, a strong brand name. While they recognised the mint as a quality product, they didn\'t think that the former packaging reflected that quality and heritage of the brand.  "In fact, the research disclosed that the three basic elements of the branding were almost disjointed, all fighting for the consumer\'s attention. Above the image of Peppy the famous polar bear was the brand Fox\'s. Below Peppy were the words Glacier Mints. While the image of Peppy was regarded as important, even more key was the brand name \'Fox\'s\'."  Fox\'s Glacier Fruits were less well known than the mint, even though the brand has been around since the middle 50s and consumers were saying that they wanted added value from the fruits. "The product just didn\'t stand out from other products on the market," admitted Emma.  Armed with this information Fox\'s added real fruit juice, took out artificial colours and followed market trends to provide products with added benefits.  Since the majority of consumers said that their favourite sweets were either strawberry or blackcurrant flavoured, Fox\'s has also created a new product | Question: What animal is traditionally seen in the branding of Fox\'s Glacier Mints?', ['polar bear'], 'mrqa_triviaqa-validation-2327'), tensor(0.0746)), (('Context: The rainforest contains several species that can pose a hazard. Among the largest predatory creatures are the black caiman, jaguar, cougar, and anaconda. In the river, electric eels can produce an electric shock that can stun or kill, while piranha are known to bite and injure humans. Various species of poison dart frogs secrete lipophilic alkaloid toxins through their flesh. There are also numerous parasites and disease vectors. Vampire bats dwell in the rainforest and can spread the rabies virus. Malaria, yellow fever and Dengue fever can also be contracted in the Amazon region. | Question: What fish living in the Amazon river is known to bit humans?', ['piranha'], 'mrqa_squad-validation-4185'), tensor(0.0721)), (("Context: A piñata ( / pɪnˈjɑːtə / , US pronunciation / pɪnˈjɑːdɑː / , Spanish pronunciation : ( piˈɲata ) ( listen ) ) is a container often made of papier - mâché , pottery , or cloth ; it is decorated , and filled with small toys or candy , or both , and then broken as part of a ceremony or celebration . Piñatas are commonly associated with Mexico . The idea of breaking a container filled with treats came to Europe in the 14th century , where the name , from the Italian pignatta , was introduced . The Spanish brought the European tradition to Mexico , although there were similar traditions in Mesoamerica , such as the Aztecs ' honoring the birthday of the god Huitzilopochtli in mid December . According to local records , the Mexican piñata tradition began in the town of Acolman , just north of Mexico City , where piñatas were introduced for catechism purposes as well as to co-opt the Huitzilopochtli ceremony . Today , the piñata is still part of Mexican culture , the cultures of other countries in Latin America , as well as the United States , but it has mostly lost its religious character . | Question: where did the tradition of the pinata come from ?", ['treats'], 'mrqa_naturalquestions-validation-10680'), tensor(0.0709)), (('Context: Kim Dong-wook  Kim Dong-wook (born July 29, 1983) is a South Korean actor.  After appearing in student short films and several minor parts, Kim became a star through his supporting role in the popular TV series "Coffee Prince" (2007), followed by box office hit "Take Off" (2009).  He then starred in "Happy Killers" (2010) and "Romantic Heaven" (2011), but it was his acclaimed performance as an obsessed and tormented king in 2012 period drama "The Concubine" that brought Kim the best reviews of his career yet.   The Concubine (film)  The Concubine (; lit.  "Royal Concubine: Concubine to the King") is a 2012 South Korean historical film directed by Kim Dae-seung.  Set in the Joseon Dynasty, it centers around Hwa-yeon (Jo Yeo-jeong), who becomes a royal concubine against her will, Kwon-yoo (Kim Min-joon), a man torn between love and revenge, and Prince Sung-won (Kim Dong-wook), who has his heart set on Hwa-yeon despite the countless women available to him.  These three characters form a love triangle which is ruled by dangerous passion.  The struggle to survive within the tight-spaced boundaries of the palace is intense, and only those who are strong enough to overcome the hell-like milieu can survive. | Question: What role did Kim Dong-wook play in the 2012 South Korean historical film directed by Kim Dae-seung?', ['an obsessed and tormented king'], 'mrqa_hotpotqa-validation-3971'), tensor(0.0688)), (("Context: To fix carbon dioxide into sugar molecules in the process of photosynthesis, chloroplasts use an enzyme called rubisco. Rubisco has a problem—it has trouble distinguishing between carbon dioxide and oxygen, so at high oxygen concentrations, rubisco starts accidentally adding oxygen to sugar precursors. This has the end result of ATP energy being wasted and CO2 being released, all with no sugar being produced. This is a big problem, since O2 is produced by the initial light reactions of photosynthesis, causing issues down the line in the Calvin cycle which uses rubisco. | Question: What effect does rubisco's flaw have?", ['at high oxygen concentrations, rubisco starts accidentally adding oxygen to sugar precursors'], 'mrqa_squad-validation-8832'), tensor(0.0596)), (('Context: Many residents of metropolitan regions work within the central urban area , and choose to live in satellite communities called suburbs and commute to work via automobile or mass transit . Others have taken advantage of technological advances to work from their homes . These processes often occur in more economically developed countries , especially in the United States , which is believed to be the first country in which the majority of the population lives in the suburbs , rather than in the cities or in rural areas . Proponents of containing urban sprawl argue that sprawl leads to urban decay and a concentration of lower income residents in the inner city . | Question: suburbanization of more developed countries is mostly due to ?', ['Many residents of metropolitan regions work within the central urban area , and choose to live in satellite communities called suburbs and commute to work'], 'mrqa_naturalquestions-validation-8448'), tensor(0.0576)), (('Context: Its hardware contains similarities to the LG G2 ; it is powered by a 2.26 GHz quad - core Snapdragon 800 processor with 2 GB of RAM , either 16 or 32 GB of internal storage , and a 2300 mAh battery . The Nexus 5 uses a 4.95 - inch ( marketed as 5 - inch ) 445 PPI 1080p IPS display , and includes an 8 - megapixel rear - facing camera with optical image stabilization ( OIS ) , and a 1.3 - megapixel front - facing camera . The Nexus 5 supports LTE networks where available , unlike the Nexus 4 which unofficially supported LTE on AWS Band 4 only with a hidden software option , but was not formally approved or marketed for any LTE use . There are two variants of the Nexus 5 , with varying support for cellular frequency bands ; one is specific to North America ( LG - D820 ) , and the other is designed for the rest of the world ( LG - D821 ) . | Question: what is the processor for google nexus 5 ?', ['a 2.26 GHz quad - core Snapdragon 800 processor'], 'mrqa_naturalquestions-validation-6341'), tensor(0.0565)), (("Context: The term ' motor neuron ' is usually restricted to the lower motor neurons , the efferent nerves that directly innervate muscles . | Question: all the motor neurons that control the skeletal muscles are ?", ['efferent nerves'], 'mrqa_naturalquestions-validation-2571'), tensor(0.0519)), (('Context: During the mid-Eocene, it is believed that the drainage basin of the Amazon was split along the middle of the continent by the Purus Arch. Water on the eastern side flowed toward the Atlantic, while to the west water flowed toward the Pacific across the Amazonas Basin. As the Andes Mountains rose, however, a large basin was created that enclosed a lake; now known as the Solimões Basin. Within the last 5–10 million years, this accumulating water broke through the Purus Arch, joining the easterly flow toward the Atlantic. | Question: Where did water to the west of the Amazon drainage basin flow towards?', ['Pacific', 'the Pacific'], 'mrqa_squad-validation-4283'), tensor(0.0453)), (('Context: The axons of the tract cells cross over ( decussate ) to the other side of the spinal cord via the anterior white commissure , and to the anterolateral corner of the spinal cord ( hence the spinothalamic tract being part of the anterolateral system ) . Decussation usually occurs 1 - 2 spinal nerve segments above the point of entry . The axons travel up the length of the spinal cord into the brainstem , specifically the rostral ventromedial medulla . | Question: where does decussation occur in the spinothalamic pathway ?', ['usually occurs 1 - 2 spinal nerve segments above the point of entry'], 'mrqa_naturalquestions-validation-7511'), tensor(0.0444)), (('Context: Evolution of the adaptive immune system occurred in an ancestor of the jawed vertebrates. Many of the classical molecules of the adaptive immune system (e.g., immunoglobulins and T cell receptors) exist only in jawed vertebrates. However, a distinct lymphocyte-derived molecule has been discovered in primitive jawless vertebrates, such as the lamprey and hagfish. These animals possess a large array of molecules called Variable lymphocyte receptors (VLRs) that, like the antigen receptors of jawed vertebrates, are produced from only a small number (one or two) of genes. These molecules are believed to bind pathogenic antigens in a similar way to antibodies, and with the same degree of specificity. | Question: Evolution of what part of the immune system occurred in the evolutionary ancestor of jawed vertebrates?', ['adaptive', 'adaptive immune system', 'the adaptive immune system'], 'mrqa_squad-validation-6677'), tensor(0.0438)), (("Context: Album Review: Ringo Starr Postcards From ParadiseAlbum Review: Ringo Starr Postcards From Paradise  Album Review: Ringo Starr Postcards From Paradise  Views 32By Kevin Kearns 1 Comment  Shares 0  Facebook Share on facebook Twitter Tweet it Google+ Share on google+ Linkedin Share on linkedin Reddit Read it on Reddit Whatsapp Share on whatsapp Share on Email Mail it  +  The peace-and-love guru is back with his eighteenth studio release. Postcards From Paradise was produced by Ringo and recorded at his home studio in Los Angeles, and, as all his previous solo albums, features a little help from his friends and family. Among the contributors this time are Joe Walsh, Benmont Tench, Dave Stewart, Richard Marx, Peter Frampton, Nathan East, and Glen Ballard. Unlike previous Ringo recordings, however, Postcards includes one song, “Island in the Sun,” written and recorded by Ringo and all the members of his All Starr Band: Steve Lukather, Todd Rundgren, Gregg Rolie, Richard Page, Warren Ham, and Gregg Bissonette, who plays percussion, trumpet and steel drums on the track.  As usual, if you’re expecting a “drummy” album featuring crazy fills and odd time signatures, you’d be better off looking elsewhere. What you will get is Ringo having fun and laying down the formula that he’s perfected for the past fifty years, playing exactly what’s right for the song, no more and no less.  The album kicks off with “Rory and the Hurricanes,” a lyrical trip down memory lane going back to Ringo’s first band. On the title track Ringo and Rundgren pay tribute to the drummer’s next band—yup, the one he changed the world with—by creating the lyrics out of the titles of many of the Beatles’ most famous songs. Ringo, who also plays keyboards on this track, lays down the on-the-money groove we’ve all loved for so long, which is made even tastier by the amazing drum sound heard throughout the entire record. (Kudos to engineer Bruce Sugar.) The track “Touch and Go” will also touch a nerve with Beatle fans—you can practically see him bopping his head as he’s playing, as this track comes the closest on the record to a swinging “mop-top” beat.  On “You Bring the Party Down,” co-written with Toto’s Steve Lukather, Ringo goes back and forth between a reggae-like feel and a driving rock beat, adding well-timed timbale overdubs. (Like his past few releases, Ringo lays heavy on his reggae influence throughout, including on the tracks “Island in the Sun,” “Right Side of the Road,” and “Bridges.”) On “Bamboula,” Ringo lets loose on a syncopated New Orleans–inspired snare/tom rhythm, with more overdubbed percussion, while on the track “Confirmation” he plays a laid-back Motown groove. Then on “Let Love Lead,” he channels a Charlie Watts vibe and throws in a few unexpected upbeat cymbal crashes, and after the lead guitar break kicks into a snare/tom groove with a beautiful buzz roll.  Postcards from Paradise is officially out on March 31. For more on Ringo visit ringostarr.com . (Universal Music) Billy Amendola  Tagged With album reviews Benmont Tench Dave Stewart Glen Ballard Gregg Bissonette Gregg Rolie Joe Walsh Nathan East Peter Frampton Postcards From Paradise Richard Marx Richard Page Ringo Starr Steve Lukather Todd Rundgren Warren Ham  Shares 0  Ringo Starr ill, cancels San Francisco show - AXSRingo Starr ill, cancels San Francisco show - AXS  Ringo Starr ill, cancels San Francisco show  By: Lori Melton AXS Contributor Mar 13, 2015  67 475401 16715476 6 y2015m03d13  44075  Colin Hay YouTube  Former Beatle Ringo Star is canceling tonight’s March 13 show at the SFMasonic Hall in San Francisco, according to a tweet from the venue which reads: “Due to illness, tonight's Ringo Starr & His All Starr Band concert has been postponed &rescheduled for Oct 1.”  Starr and his All-Starr band also canceled their | Question: Whose eighteenth studio album released in March this year is Postcards from Paradise?", ['ringo starr', 'ringo'], 'mrqa_triviaqa-validation-6402'), tensor(0.0438)), (('Context: Pangaea or Pangea ( / pænˈdʒiːə / ) was a supercontinent that existed during the late Paleozoic and early Mesozoic eras . It assembled from earlier continental units approximately 335 million years ago , and it began to break apart about 175 million years ago . In contrast to the present Earth and its distribution of continental mass , much of Pangaea was in the southern hemisphere and surrounded by a superocean , Panthalassa . Pangaea was the most recent supercontinent to have existed and the first to be reconstructed by geologists . | Question: what land mass was north america a part of about 300 million years ago ?', ['Pangaea or Pangea'], 'mrqa_naturalquestions-validation-9386'), tensor(0.0417)), (('Context: Cor, Blimey!  Cor, Blimey!  is a 2000 TV film that follows the relationship between "Carry On" film actors Sid James (played by Geoffrey Hutchings) and Barbara Windsor (played by Samantha Spiro).   Samantha Spiro  Samantha Spiro (born 20 June 1968) is a double Olivier Award-winning English actress.  She is best known for portraying Barbara Windsor in the stage play "Cleo, Camping, Emmanuelle and Dick" and the television film "Cor, Blimey! ", DI Vivien Friend in "", and Melessa Tarly in the HBO series "Game of Thrones". | Question: Who plays opposite the double Olivier Award-winning English actress in the 2000 TV film following the relationship between "Carry On" film actors Sid James and Barbara Windsor?', ['Geoffrey Hutchings'], 'mrqa_hotpotqa-validation-2970'), tensor(0.0387)), (("Context: The White Coat Ceremony ( WCC ) is a relatively new ritual in some medical ( MD , DO ) , dental , optometry , audiology , chiropractic , dietetic , occupational therapy , physical therapy , podiatric , pharmacy , physician assistant , pathologists ' assistant , nursing , naturopathic and veterinary schools that marks the student 's transition from the study of preclinical to clinical health sciences . At some schools , where students begin meeting patients early in their education , the white coat ceremony is held before the first year begins . It is an example of a matriculation . | Question: when do you get your white coat in pharmacy school ?", ["the student 's transition from the study of preclinical to clinical health sciences"], 'mrqa_naturalquestions-validation-5465'), tensor(0.0349)), (('Context: Origin of Does Exactly What it Says on the Tin | RonsealOrigin of Does Exactly What it Says on the Tin | Ronseal  The Ronseal Phrase - Does Exactly What it Says on the Tin  \xa0  People use the phrase every day, it has come to represent a product or policy that is open and honest; it is used when something quite simply ‘Does Exactly What it Says on the Tin .’  Originating back to 1994 the popular phrase ‘Does exactly what is says on the tin’ was developed when Ronseal employed London based advertising agency HHCL to create a campaign that without trying too hard would de-mystify our products. Dave Shelton, co-creator of the line with Liz Whinston, explains the thought process behind the original campaign adverts ,  “We started to write a commercial that featured a straightforward guy who said lines like: "If you\'ve got wood to stain and you want it to dry quickly, you need Ronseal Quick Drying Woodstain."  We\'d soon knocked out several scripts but we needed a line… "Does exactly what it says on the tin" was a great way of summing this up.  After the initial campaign, sales shot up and Ronseal became [a] brand leader.”  ‘Does exactly what it says on the tin’ is now more than an advertising slogan; it has become part of our everyday vernacular. The line has made it into the Oxford Dictionary of Idioms, been featured in the song ‘What it Says On The Tin’ by Katie Melua, and has even been used by Prime Minister David Cameron and on multiple occasions to summarise his preferred approach to politics.  The phrase has come to represent a product or policy that is open, honest and delivers against its promise.  The phrase is now used internationally and it’s not surprising it has become the third most known slogan of all time.  \xa0  \xa0  [1] The phrase is a registered trademark of Ronseal’s owner, the Sherwin-Williams Company , across the European Community for products including paints, varnishes and wood preservatives (E3085826)  [3] Source Creative Review February 2012  \xa0  The Ronseal phrase: It does exactly what it says on the tinThe Ronseal phrase: It does exactly what it says on the tin - BBC News  BBC News  The Ronseal phrase: It does exactly what it says on the tin  8 January 2013  Read more about sharing.  Close share panel  Prime Minister David Cameron used a famous advertising slogan to sum up the state of the coalition government at its halfway point, saying "it is a Ronseal deal - it does what it says on the tin". This phrase has entered the British vernacular, says its co-creator Dave Shelton.  David Cameron has said it before. In 2004, he said: "People are crying out for a kind of Ronseal politics - they want it to do what it says on the tin."  People use the phrase every day. On YouTube you can listen to Katie Melua\'s What It Says On The Tin . Google the line and you get millions of results.  In 1994, when my advertising partner Liz Whiston and I came up with the line "Does exactly what it says on the tin" for Ronseal, we never dreamed how it would enter the language.  About the author  Dave Shelton and Liz Whiston co-founded Bordello Advertising.  They have been working in advertising for more than 20 years.  Back then Ronseal was just another varnish fighting for market share against a lot of fancier lifestyle brands. Anyone wandering into a DIY superstore was faced with a wall of different solutions to whatever woodcare job they might be trying to tackle.  We had presented a number of different campaigns, all of which had been rejected by Ged Shields, the then marketing director of Ronseal for "trying too hard".  Shields wanted to de-mystify the product.  We decided what was needed wasn\'t puns or art.  Instead we would call a spade a spade.  We started to write a commercial that featured a | Question: Which product is advertised on TV with the slogan \'It does exactly what it says on the tin\'?', ['ronseal'], 'mrqa_triviaqa-validation-5026'), tensor(0.0335)), (("Context: What we now call gravity was not identified as a universal force until the work of Isaac Newton. Before Newton, the tendency for objects to fall towards the Earth was not understood to be related to the motions of celestial objects. Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object. Today, this acceleration due to gravity towards the surface of the Earth is usually designated as  and has a magnitude of about 9.81 meters per second squared (this measurement is taken from sea level and may vary depending on location), and points toward the center of the Earth. This observation means that the force of gravity on an object at the Earth's surface is directly proportional to the object's mass. Thus an object that has a mass of  will experience a force: | Question: Who came up with the concept that falling objects fell at the same speed regardless of weight?", ['Galileo'], 'mrqa_squad-validation-10410'), tensor(0.0269)), (("Context: The Qur'an admonishes Muslim women to dress modestly and cover their breasts and genitals but it does n't require covering the head . The Qur'an explicitly states that `` O wives of the Prophet , you are not like anyone among women '' and as such has separate rules specifically for the wives of the Prophet . Wearing hijab , or al - khimar , is considered by some to be obligatory in Islam , as the quote states `` Tell the believing women to put their khimar over their bosoms '' , all the major Arab translations and dictionaries agree that `` al - khimar '' means a veil or scarf that a woman uses to cover her head , and `` over their bosoms '' implies women should cover their breasts , necks etc . | Question: what is the name of the muslim dress ?", ['hijab'], 'mrqa_naturalquestions-validation-868'), tensor(0.0259)), (('Context: Br\'er RabbitBr\'er Rabbit, also spelled Bre\'r Rabbit or Brer Rabbit or Bruh Rabbit, is a central figure as Uncle Remus tells stories of the Southern United States. Br\'er Rabbit is a trickster who succeeds by his wits rather than by brawn, provoking authority figures and bending social mores as he sees fit. The Walt Disney Company later adapted this character for its 1946 animated motion picture Song of the South.  Tar-Baby story   In one tale, Br\'er Fox constructs a doll out of a lump of tar and dresses it with some clothes. When Br\'er Rabbit comes along he addresses the Tar-Baby amiably, but receives no response. Br\'er Rabbit becomes offended by what he perceives as the Tar-Baby\'s lack of manners, punches it, and in doing so becomes stuck. The more Br\'er Rabbit punches and kicks the tar "baby" out of rage, the more he gets stuck. When Br\'er Fox reveals himself, the helpless but cunning Br\'er Rabbit pleads, "please, Br\'er Fox, don\'t fling me in dat brier-patch," prompting Fox to do exactly that. As rabbits are at home in thickets, the resourceful Br\'er Rabbit uses the thorns and briers to escape. The story was originally published in Harper\'s Weekly by Robert Roosevelt; years later Joel Chandler Harris included his version of the tale in his Uncle Remus stories.  African origins   The Br\'er Rabbit stories can be traced back to trickster figures in Africa, particularly the hare that figures prominently in the storytelling traditions in West, Central, and Southern Africa. These tales continue to be part of the traditional folklore of numerous peoples throughout those regions. In the Akan traditions of West Africa, the trickster is usually the spider Anansi, though the plots in his tales are often identical with those of stories of Br\'er Rabbit.  However, Anansi does encounter a tricky rabbit called "Adanko" (Asante-Twi to mean "Hare") in some stories. The Jamaican character with the same name "Brer Rabbit", is an adaptation of the Ananse stories of the Akan people.   Some scholars have suggested that in his American incarnation, Br\'er Rabbit represented the enslaved Africans who used their wits to overcome adversity and to exact revenge on their adversaries, the White slave-owners.  Though not always successful, the efforts of Br\'er Rabbit made him a folk hero. However, the trickster is a multi-dimensional character. While he can be a hero, his amoral nature and his lack of any positive restraint can make him into a villain as well.   For both Africans and African Americans, the animal trickster represents an extreme form of behavior that people may be forced to adopt in extreme circumstances in order to survive. The trickster is not to be admired in every situation. He is an example of what to do, but also an example of what not to do. The trickster\'s behavior can be summed up in the common African proverb: "It\'s trouble that makes the monkey chew on hot peppers." In other words, sometimes people must use extreme measures in extreme circumstances.  Several elements in the Brer Rabbit Tar Baby story (e.g., rabbit needing to be taught a lesson, punches and head-butting the rabbit does, the stuck rabbit being swung around and around) are reminiscent of those found in a Zimbabwe-Botswana folktale.   Folklorists in the late 19th century first documented evidence that the American versions of the stories originated among enslaved West Africans based on connections between Br\'er Rabbit and Leuk, a rabbit trickster in Senegalese folklore.  The stories of Br\'er Rabbit were written down by Robert Roosevelt, an uncle of US President Theodore Roosevelt. Theodore Roosevelt wrote in his autobiography about his aunt from the State of Georgia, that "She knew all the \'Br\'er Rabbit\' stories, and I was brought up on them. One of my uncles, Robert Roosevelt, was much struck with them, and took them down from her dictation, publishing them in Harper\'s, where they fell flat. This was a good many years before a genius arose who, in \'Uncle Remus\', made the stories | Question: Who gets Brer Rabbit in a terrible tangle in the stories by Joel Chandler Harris?', ['Tar Baby', 'Tar-Baby', 'tar baby', 'tar "baby'], 'mrqa_triviaqa-validation-1616'), tensor(0.0245)), (("Context: Petra Kvitová career statistics  This is a list of the main career statistics of Czech professional tennis player Petra Kvitová.  To date, Kvitová has won 20 singles titles including two Grand Slam singles titles at the Wimbledon Championships, one WTA Tour Championships singles title, two WTA Premier Mandatory singles titles and four WTA Premier 5 singles titles.  She was also the bronze medallist at the 2016 Rio Olympics, a semifinalist at the 2010 Wimbledon Championships, 2012 Australian Open and 2012 French Open and a quarterfinalist at the 2011 Australian Open, 2012 Wimbledon Championships, 2013 Wimbledon Championships, 2015 US Open and 2017 US Open.  Kvitová reached her career-high ranking of world no. 2 on 31 October 2011.   2017 US Open (tennis)  The 2017 US Open was the 137th edition of tennis' US Open and the fourth and final Grand Slam event of the year.  It was held on outdoor hard courts at the USTA Billie Jean King National Tennis Center in New York City.  Experimental rules featured in qualifying for the main draw as well as in the junior, wheelchair and exhibition events. | Question: What edition of tennis' US Open was the 2017 US Open when Petra Kvitova was a quarterfinalist?", ['137th'], 'mrqa_hotpotqa-validation-409'), tensor(0.0238)), (('Context: <Ol>  A new ruler unites China , founds a new dynasty , and gains the Mandate of Heaven .   China , under the new dynasty , achieves prosperity .   The population increases .   Corruption becomes rampant in the imperial court , and the empire begins to enter decline and instability .   A natural disaster wipes out farm land . The disaster normally would not have been a problem ; however , together with the Corruption and overpopulation , it causes famine .   The famine causes the population to rebel and a civil war ensues .   The ruler loses the Mandate of Heaven .   The population decreases because of the violence .   China goes through a warring states period .   One state emerges victorious .   The state starts a new empire .   The empire gains the Mandate of Heaven .  </Ol> | Question: in the dynastic cycle what is the right to rule called ?', ['the Mandate of Heaven'], 'mrqa_naturalquestions-validation-6157'), tensor(0.0223)), (('Context: PRI is the standard for providing telecommunication services to enterprises and offices . It is based on T - carrier ( T1 ) transmission in the US , Canada , and Japan , while the E-carrier ( E1 ) is common in Europe and Australia . The T1 line consists of 23 bearer ( B ) channels and one data ( D ) channel for control purposes , for a total bandwidth of 24x64 - kbit / s or 1.544 Mbit / s . The E1 carrier provides 30 B - and two D - channels for a bandwidth of 2.048 Mbit / s . The first timeslot on the E1 is used for synchronization purposes and is not considered to be a B - or D - channel . The D - channel typically uses timeslot 16 on an E1 , while it is timeslot 24 for a T1 . Fewer active bearer channels , sometimes called user channels , may be used in fractional T1 or E1 services . | Question: isdn uses b & d channels . what is d channel use for ?', ['data'], 'mrqa_naturalquestions-validation-9650'), tensor(0.0193)), (('Context: Individual Huguenots settled at the Cape of Good Hope from as early as 1671 with the arrival of François Villion (Viljoen). The first Huguenot to arrive at the Cape of Good Hope was however Maria de la Queillerie, wife of commander Jan van Riebeeck (and daughter of a Walloon church minister), who arrived on 6 April 1652 to establish a settlement at what is today Cape Town. The couple left for the Far East ten years later. On 31 December 1687 the first organised group of Huguenots set sail from the Netherlands to the Dutch East India Company post at the Cape of Good Hope. The largest portion of the Huguenots to settle in the Cape arrived between 1688 and 1689 in seven ships as part of the organised migration, but quite a few arrived as late as 1700; thereafter, the numbers declined and only small groups arrived at a time. | Question: The number of new Huguenot colonists declined after what year?', ['1700'], 'mrqa_squad-validation-3113'), tensor(0.0182)), (('Context: Callability -- Some bonds give the issuer the right to repay the bond before the maturity date on the call dates ; see call option . These bonds are referred to as callable bonds . Most callable bonds allow the issuer to repay the bond at par . With some bonds , the issuer has to pay a premium , the so - called call premium . This is mainly the case for high - yield bonds . These have very strict covenants , restricting the issuer in its operations . To be free from these covenants , the issuer can repay the bonds early , but only at a high cost . | Question: a bond that the issuer has the right to pay off before its maturity date ?', ['callable bonds'], 'mrqa_naturalquestions-validation-2385'), tensor(0.0174)), (('Context: For the third straight season, the number one seeds from both conferences met in the Super Bowl. The Carolina Panthers became one of only ten teams to have completed a regular season with only one loss, and one of only six teams to have acquired a 15–1 record, while the Denver Broncos became one of four teams to have made eight appearances in the Super Bowl. The Broncos made their second Super Bowl appearance in three years, having reached Super Bowl XLVIII, while the Panthers made their second Super Bowl appearance in franchise history, their other appearance being Super Bowl XXXVIII. Coincidentally, both teams were coached by John Fox in their last Super Bowl appearance prior to Super Bowl 50. | Question: How many teams up to Super Bowl 50 have been to the championship game eight times?', ['four'], 'mrqa_squad-validation-194'), tensor(0.0173)), (("Context: `` Everywhere '' is a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 ) , written by Fleetwood Mac member Christine McVie , who also performs lead vocals on the song . `` Everywhere '' was released as the fourth single from Tango in the Night on November 28 , 1987 in the United States , where it reached number 14 on the Billboard Hot 100 chart and number - one on the Adult Contemporary chart , remaining there for three weeks . `` Everywhere '' was released in the United Kingdom on 21 March 1988 and reached number four . It also reached number 45 in Australia . | Question: i want to be with you everywhere song ?", ['Everywhere'], 'mrqa_naturalquestions-validation-114'), tensor(0.0168)), (('Context: Chun Woo-hee  Chun Woo-hee (born April 20, 1987) is a South Korean actress.  She made her acting debut in 2004, but first drew attention with her supporting role as a rebellious teenager in the 2011 box-office hit "Sunny".  In 2014, Chun received domestic and international critical acclaim for her first leading role as the title character in "Han Gong-ju", a coming-of-age indie about a traumatized young woman trying to move on with her life after a tragedy.  Her other notable films include "The Beauty Inside" (2015), "Love, Lies" (2016) and "The Wailing" (2016).   Chun Woo-hee  Chun Woo-hee (born April 20, 1987) is a South Korean actress.  She made her acting debut in 2004, but first drew attention with her supporting role as a rebellious teenager in the 2011 box-office hit "Sunny".  In 2014, Chun received domestic and international critical acclaim for her first leading role as the title character in "Han Gong-ju", a coming-of-age indie about a traumatized young woman trying to move on with her life after a tragedy.  Her other notable films include "The Beauty Inside" (2015), "Love, Lies" (2016) and "The Wailing" (2016).   The Wailing (film)  The Wailing () is a 2016 South Korean horror film directed by Na Hong-jin about a policeman who investigates a series of mysterious killings and illnesses.  It was a commercial success. | Question: Chun Woo-hee\'s notable films include a South Korean horror film about  a policeman who investigates a series of mysterious killings and what?', ['illnesses'], 'mrqa_hotpotqa-validation-5325'), tensor(0.0168)), (("Context: 1932 Allan Cup  The 1932 Allan Cup was won by the Toronto National Sea Fleas.  This team went on to represent Canada at the 1933 World Ice Hockey Championships held in Prague, Czechoslovakia where the team lost the final game to the United States in overtime to capture the silver medal for Canada.   Overtime (ice hockey)  Overtime is a method of extending an ice hockey game when the scores are tied after regulation.  The two main methods of extending the game are the overtime period (commonly referred to as overtime) and the shootout.  Depending upon league rules, the game's winning team may or may not be necessarily determined. | Question: What other method of extending an ice hockey game exists other than the one used in the 1932 Allan Cup?", ['the shootout'], 'mrqa_hotpotqa-validation-2263'), tensor(0.0103)), (('Context: That Was the Week That WasThat Was the Week That Was, informally TWTWTW or TW3, was a satirical television comedy programme on BBC Television in 1962 and 1963. It was devised, produced and directed by Ned Sherrin and presented by David Frost. An American version by the same name aired on NBC from 1964 to 1965, also featuring Frost.  The programme is considered a significant element of the satire boom in the UK in the early 1960s. It broke ground in comedy through lampooning the establishment and political figures. Its broadcast coincided with coverage of the politically charged Profumo affair and John Profumo, the politician at the centre of the affair, became a target for derision. TW3 was first broadcast on Saturday, 24 November 1962.  Cast and writers  Cast members included cartoonist Timothy Birdsall, political commentator Bernard Levin, and actors Lance Percival, who sang topical calypsos, many improvised to suggestions from the audience, Kenneth Cope, Roy Kinnear, Willie Rushton, Al Mancini, Robert Lang,  David Kernan and Millicent Martin. The last two were also singers and the programme opened with a song\xa0– That Was The Week That Was\xa0– sung by Martin to Ron Grainer\'s theme tune and enumerating topics in the news. Frankie Howerd also guested with stand-up comedy.   Script-writers included John Albery, John Antrobus, John Betjeman, John Bird, Graham Chapman, John Cleese, Peter Cook, Roald Dahl, Richard Ingrams, Lyndon Irving, Gerald Kaufman, Frank Muir, David Nobbs, Denis Norden, Bill Oddie, Dennis Potter, Eric Sykes, Kenneth Tynan, and Keith Waterhouse.   Programme  The programme opened with a song ("That was the week that was, It\'s over, let it go ...") sung by Millicent Martin, referring to news of the week just gone. Lance Percival sang a topical calypso each week. Satirical targets, such as Prime Minister Harold Macmillan and Home Secretary Henry Brooke were lampooned in sketches, debates and monologues. Other targets were the monarchy, Britain\'s declining status as a global power, racism (particularly in the American South and South Africa under Apartheid), sexual and social hypocrisy, the class system, and the BBC itself. Well-remembered sketches include a \'consumers\' guide to religion\', which discussed relative merits of faiths in the manner of a Which? magazine report.  On Saturday, 20 October 1962 the award of Nobel prizes to John Kendrew and Max Perutz, and to Francis Crick, James D. Watson, and Maurice Wilkins was satirised in a short sketch with the prizes referred to as the Alfred Nobel Peace Pools; in this sketch Watson was called "Little J.D. Watson" and "Who\'d have thought he\'d ever get the Nobel Prize? Makes you think, doesn\'t it". The germ of the joke was that Watson was only 25 when he helped discover DNA; much younger than the others.  TW3 was broadcast on Saturday night and attracted an audience of 12 million. It often under- or overran as cast and crew worked through material as they saw fit. At the beginning of the second season in the autumn of 1963, in an attempt to assert control over the programme, the BBC scheduled repeats of The Third Man television series after the end of TW3. Frost suggested a means of sabotaging this tactic to Sherrin, and he agreed. For three weeks, Frost read out the plot of The Third Man, until the repeats were abandoned following the direct intervention of the BBCs Director General Hugh Greene.   Frost often ended a satirical attack with the remark "But seriously, he\'s doing a grand job".  At the end of each episode, Frost usually signed off with: "That was the week, that was." At the end of the final programme he announced: "That was That Was The Week That Was...that was."  Kennedy tribute  For the edition on Saturday, 23 November 1963, the day after the assassination of United States President John F. Kennedy, TW3 produced a shortened 20-minute programme with no satire, reflecting on the loss, including a contribution from Dame | Question: Who was the deviser, producer and director \'That Was The Week That Was\'?', ['ned sherrin'], 'mrqa_triviaqa-validation-4054'), tensor(0.0057)), (('Context: The IPCC does not carry out research nor does it monitor climate related data. Lead authors of IPCC reports assess the available information about climate change based on published sources. According to IPCC guidelines, authors should give priority to peer-reviewed sources. Authors may refer to non-peer-reviewed sources (the "grey literature"), provided that they are of sufficient quality. Examples of non-peer-reviewed sources include model results, reports from government agencies and non-governmental organizations, and industry journals. Each subsequent IPCC report notes areas where the science has improved since the previous report and also notes areas where further research is required. | Question: What is \'grey literature\'?', ['non-peer-reviewed sources'], 'mrqa_squad-validation-8513'), tensor(0.0044)), (("Context: BBC - Radio 3 - Elgar/Enigma VariationsBBC - Radio 3 - Elgar/Enigma Variations  13 * * * (Romanza)  14 Finale: E. D. U.  The first performance of Elgar’s ‘Enigma’ Variations took place at St James’s Hall London on 19 June 1899, conducted by Hans Richter. It was Elgar’s most ambitious orchestral work to date and a further performance in Düsseldorf in 1901 went on to establish him as a composer of international importance: Richard Strauss declared that ‘here for the first time is an English composer who has something to say’.  Like most overnight successes, it was the result of years of hard work. Elgar was 42 years old when he completed the Variations and, despite bitter disappointments and frustrations, had steadily built up a reputation, first provincially, then at a national level. In particular, a series of cantatas of increasing size had revealed Elgar’s brilliant orchestration and growing mastery of large forms. Now that mastery was demonstrated on a symphonic scale through the time-honoured form of Theme and Variations, inviting comparisons with the greatest classical masters. But it was Elgar’s uniquely personal approach to the form that gave the ‘Enigma’ Variations its initial novelty and lasting appeal.  Elgar himself recalled how the work came to be conceived on the evening of 21 October 1898:  After a long day’s fiddle teaching in Malvern, I came home very tired. Dinner being over, my dear wife said to me, ‘Edward, you look like a good cigar,’ and having lighted it, I sat down at the piano. In a little while, soothed and feeling rested, I began to play, and suddenly my wife interrupted by saying, ‘Edward, that’s a good tune.’ I awoke from the dream: ‘Eh! tune, what tune!’ and she said, ‘Play it again, I like that tune.’ I played and strummed, and played, and then she exclaimed, ‘That’s the tune.’ And that tune is the theme of the Variations.  Many years later Elgar’s daughter Carice recounted the same incident in a BBC broadcast:  My father was at the piano, smoking his pipe, and when I went to bed I heard him playing what I thought were pretty tunes. My mother told me he was inventing music about his friends, and he turned to her and said, ‘Who’s that like?’ My mother replied, ‘I can’t quite say, but it’s exactly the way WMB goes out of the room.’  The grand scheme was established at the outset: 13 variations, 13 musical sketches of ‘my friends pictured within’, as the dedication eventually ran, and a final 14th variation representing the composer himself.  What of the ‘Enigma’ of the title? Before the first performance, Elgar said:  The ‘Enigma’ I will not explain – its ‘dark saying’ must be left unguessed, and I warn you that the apparent connection between the Variations and the Theme is often of the slightest texture; further, through and over the whole set another and larger theme ‘goes’, but is not played . . . So the principal Theme never appears, even as in some late dramas – eg Maeterlinck’s L’Intruse and Les sept Princesses – the chief character is never on stage.  More ink has probably been spilt over these sentences than on any other Elgarian topic, each of the dozens of proposed solutions adding yet another layer of mystery to an already ambiguous pronouncement. What is clear is that the name ‘Enigma’ applies only to the theme itself and not to the whole work. Writing in 1911 Elgar revealed that this work, commenced in a spirit of humour & continued in deep seriousness, contains sketches of the composer’s friends. It may be understood that these personages comment or reflect on the original theme & each one attempts a solution of the Enigma, for so the theme is called. The sketches are not ‘portraits’, but each variation contains a distinct idea founded on some particular personality or perhaps on some incident known | Question: Which musical work of 1898 features a section called 'Nimrod'?", ['Enigma Variations', 'Enigma’ Variations', 'enigma variations'], 'mrqa_triviaqa-validation-4729'), tensor(-0.0005)), (("Context: The first Christmas cards were commissioned by Sir Henry Cole and illustrated by John Callcott Horsley in London on 1st May 1843 . The central picture showed three generations of a family raising a toast to the card 's recipient : on either side were scenes of charity , with food and clothing being given to the poor . Allegedly the image of the family drinking wine together proved controversial , but the idea was shrewd : Cole had helped introduce the Penny Post three years earlier . Two batches totaling 2,050 cards were printed and sold that year for a shilling each . | Question: who commissioned the first christmas card in 1943 ?", ['commissioned by Sir Henry Cole and illustrated by John Callcott Horsley in London on 1st May 1843'], 'mrqa_naturalquestions-validation-3189'), tensor(-0.0034)), (("Context: The show has received recognition as one of Britain's finest television programmes, winning the 2006 British Academy Television Award for Best Drama Series and five consecutive (2005–2010) awards at the National Television Awards during Russell T Davies' tenure as executive producer. In 2011, Matt Smith became the first Doctor to be nominated for a BAFTA Television Award for Best Actor and in 2016, Michelle Gomez became the first female to receive a BAFTA nomination for the series, getting a Best Supporting Actress nomination for her work as Missy. | Question: What years did Doctor Who win five consecutive awards?", ['2005–2010'], 'mrqa_squad-validation-7816'), tensor(-0.0051)), (('Context: Renewable Heat Incentive scandal  The Renewable Heat Incentive scandal (RHI scandal), also referred to as the Cash for Ash scandal, is a political scandal in Northern Ireland that centres on a failed renewable energy incentive scheme that has been reported to potentially cost the public purse almost £500 million.  The plan was overseen by Arlene Foster of the Democratic Unionist Party (DUP), the then-Minister for Enterprise, Trade and Investment, who failed to introduce proper cost controls, allowing the plan to spiral out of control.  The scheme worked by paying applicants to use renewable energy.  The rate paid was more than the cost of the fuel (the same as in the GBRHI scheme) however, meaning applicants were making profits simply by heating their properties.   Renewable Heat Incentive scandal  The Renewable Heat Incentive scandal (RHI scandal), also referred to as the Cash for Ash scandal, is a political scandal in Northern Ireland that centres on a failed renewable energy incentive scheme that has been reported to potentially cost the public purse almost £500 million.  The plan was overseen by Arlene Foster of the Democratic Unionist Party (DUP), the then-Minister for Enterprise, Trade and Investment, who failed to introduce proper cost controls, allowing the plan to spiral out of control.  The scheme worked by paying applicants to use renewable energy.  The rate paid was more than the cost of the fuel (the same as in the GBRHI scheme) however, meaning applicants were making profits simply by heating their properties.   Arlene Foster  Arlene Isabel Foster MLA PC ("née" Kelly; born 3 July 1970) is a Northern Irish politician who has been the leader of the Democratic Unionist Party since December 2015 and the Member of the Northern Ireland Assembly for Fermanagh and South Tyrone since 2003. | Question: Which organization has been led by the overseer of the Renewable Heat Incentive Scandal since 2003?', ['Northern Ireland Assembly for Fermanagh and South Tyrone'], 'mrqa_hotpotqa-validation-2150'), tensor(-0.0057)), (("Context: ET Canada | Blog - Remake Of Tom Hanks' Mermaid Romcom ...Remake Of Tom Hanks’ Mermaid Romcom ‘Splash’ In The\xa0Works | ETCanada.com  Remake Of Tom Hanks’ Mermaid Romcom ‘Splash’ In The\xa0Works  By Rachel West.  6 Jun 2016 1:54 PM  Photo: Keystone Press  Thirty-two years ago, Tom Hanks fell in love with a mermaid in “Splash”;. Now, the 1984 romantic comedy is getting a remake – this time with a twist.  Producer Brain Grazer admitted during an interview with CNBC that a remake of “Splash”; was in the works. While Grazer kept mostly mum on the details, he revealed that this version of the story would be told from the mermaid’s perspective.  RELATED: John Stamos Singing “Under The Sea’ From “The Little Mermaid’ Is The Best Way To Start Your Weekend  The original film saw an unlucky-in-love Tom Hanks fall in love with a beautiful mermaid, played by Daryl Hannah, after she rescues him from drowning. Directed by Ron Howard, the film was a commercial and critical success, earning an Oscar nomination for Best Original Screenplay.  Simply titled “Wet”;, there’s no word on if the film’s original stars will appear in the sequel, but Grazer did share at least one big name could be attached to the project.  “There’s a movie star involved, or going to be involved,”; Grazer said during the interview. “I haven’t announced it.”;  RELATED: Casting Call: Stars Nab A New Role Gallery  “Wet”;”s mermaid star would certainly be at home on land or sea. The movie would stand to capitalize on a trend which sees Hollywood showing some love to mermaids, as of late.  This past weekend, stars gathered together to give a live rendition of the animated Disney classic for “The Little Mermaid Live”; with Rebel Wilson as Ursula and Darren Criss as Eric. Up next, Chloe Grace Moretz will star in a non-musical non-Disney version of “The Little Mermaid”;, which is currently in the pre-production stage.  Movie Review - - SCREEN: 'SPLASH,' A MERMAID'S LOVE ...Movie Review -    - SCREEN: 'SPLASH,' A MERMAID'S LOVE - NYTimes.com  SCREEN: 'SPLASH,' A MERMAID'S LOVE  By JANET MASLIN  Published: March     9, 1984  THOUGH Ron Howard's comedies don't adhere to any familiar formulas - Mr. Howard's funny ''Night Shift'' was about a prostitution ring operating out of a morgue, and his even more disarming new ''Splash'' is about a mermaid in Manhattan - they have a comfortably old-fashioned flavor. ''Splash'' may feature a heroine with fins, but it's mostly a standard love story, albeit one with some delightful new twists. The boy (Tom Hanks) is a lonely bachelor with a single overriding wish: ''to meet a woman, fall in love, get married, have a kid and see him play a tooth in the school play.'' And the girl (Daryl Hannah) at first knows only one word of English: ''Bloomingdale's.''  ''Splash,'' which opens today at the National and other theaters, accomplishes the improbable with some enchanting underwater sequences, scenes that make credible the thought that Daryl Hannah might really be a mermaid. Following a brief prologue in which she meets Allen Bauer, played by Mr. Hanks when both are children, the mermaid reappears to save his life when he falls overboard near Cape Cod. There are hints that Allen may be susceptible to aquatic creatures in the decors of both his office and his apartment, since there are large fish tanks in both places (only rarely does Mr. Howard use such unnecessarily broad strokes). But nothing in his past hints at how hard Allen will fall for the beautiful, innocent mermaid once she sprouts a pair of tawny legs and follows him home.  Much of the humor grows out of the way in which | Question: In which 1984 film did Tom Hanks fall in love with a mermaid?", ['splash'], 'mrqa_triviaqa-validation-7382'), tensor(-0.0062)), (("Context: May 14 : The Corps of Discovery departs from Camp Dubois at 4 p.m. , marking the beginning of the voyage to the Pacific coast .   May 16 : The Corps of Discovery arrives at St. Charles , Missouri .   May 21 : Departure from St. Charles at 3 : 30 p.m.   May 24 : Pass Boones Settlement . Home of famous woodsman L. Willenborg .   May 25 : The expedition passes the small village of La Charrette on the Missouri River . Charles Floyd writes in his journal that this is `` the last settlement of whites on this river '' .   June 1 : The expedition reaches the Osage River .   June 12 : Lewis and Clark meet three trappers in two pirogues . One of the men was Pierre Dorion , Jr. -- who knew George Rogers Clark . Lewis and Clark persuade Dorion to return to Sioux camp to act as interpreter .   June 26 : The expedition arrives at Kaw Point where the Kansas River drains into the Missouri River basin .   June 28 -- 29 : First trial in new territory . Pvt . John Collins is on guard duty and breaks into the supplies and gets drunk . Collins invites Pvt . Hugh Hall to drink also . Collins receives 100 lashes , Hall receives 50 lashes .   July 4 : Marking Independence Day , the expedition names Independence Creek located near Atchison , Kansas .   July 11 -- 12 : Second trial in new territory . Pvt . Alexander Hamilton Willard is on guard duty . Is charged with lying down and sleeping at his post whilst a sentinel . Punishable by death . He receives 100 lashes for four straight days .   July 21 : Reaches the Platte River , 640 miles from St. Louis . Entering Sioux Territory .   August 1 : Captain William Clark 's 34th birthday .   August 3 : The Corps of Discovery holds the first official council between representatives of the United States and the Oto and Missouri tribes at Council Bluffs , Iowa . They hand out peace medals , 15 - star flags and other gifts , parade men and show off technology .   August 4 : Moses Reed said he was returning to a previous camp to retrieve a knife but deserted to St. Louis .   August 18 : George Drouillard returns to camp with Reed and Otos ' Chief Little Thief . Reed is sentenced to run the gauntlet ( 500 lashes ) and is discharged from the permanent party .   August 18 : Captain Meriwether Lewis 's 30th birthday .   August 20 : Sergeant Charles Floyd dies . He dies from bilious chorlick ( ruptured appendix ) . He is the only member lost during the expedition .   August 23 : Pvt . Joseph Field kills first bison .   August 26 : Pvt . Patrick Gass is elected to sergeant . First election in new territory west of Mississippi River . George Shannon is selected to get the horses back from native Americans .   August 30 : A friendly council with the Yankton Sioux held . According to a legend , Lewis wraps a newborn baby in a United States flag and declares him `` an American '' .   September 4 : Reach the mouth of the Niobrara River .   September 7 : The expedition drives a prairie dog out of its den ( by pouring water into it ) to send back to Jefferson .   September 14 : Hunters kill and describe prairie goat ( antelope ) .   September 25 -- 29 : A band of Lakota Sioux demand one of the boats as a toll for moving further upriver . Meet with Teton Sioux . Close order drill , air gun demo , gifts of medals , military coat , hats , tobacco . Hard to communicate language problems . Invite chiefs on board keelboat , give each \u200b ⁄ glass whiskey , acted drunk wanted more . Two armed confrontations with Sioux . Some of the chiefs sleep on boat , move up river to another village , meet in lodge , hold scalp dance .   October 8 -- 11 : Pass Grand River home of the Arikara people , 2,000 + . Joseph Gravelins trader , lived with Arikara for 13 yrs . Pierre Antoine Tabeau lived in another village was from Quebec .   October 13 : Pvt . John Newman tried for | Question: on which river did the exploration of the louisiana purchase begin ?", ['the Missouri River'], 'mrqa_naturalquestions-validation-8948'), tensor(-0.0080)), (('Context: Concentrated O2 will allow combustion to proceed rapidly and energetically. Steel pipes and storage vessels used to store and transmit both gaseous and liquid oxygen will act as a fuel; and therefore the design and manufacture of O2 systems requires special training to ensure that ignition sources are minimized. The fire that killed the Apollo 1 crew in a launch pad test spread so rapidly because the capsule was pressurized with pure O2 but at slightly more than atmospheric pressure, instead of the 1⁄3 normal pressure that would be used in a mission.[k] | Question: ______ In both liquid and gas form can fastly result in an exlposion. ?', ['oxygen'], 'mrqa_squad-validation-3478'), tensor(-0.0161)), (('Context: Formula One drivers from Mexico  There have been six Formula One drivers from Mexico who have taken part in races since the championship began in 1950.  Pedro Rodríguez is the most successful Mexican driver being the only one to have won a grand prix.  Sergio Pérez, the only other Mexican to finish on the podium, currently races with Sahara Force India F1 Team .   Formula One drivers from Mexico  There have been six Formula One drivers from Mexico who have taken part in races since the championship began in 1950.  Pedro Rodríguez is the most successful Mexican driver being the only one to have won a grand prix.  Sergio Pérez, the only other Mexican to finish on the podium, currently races with Sahara Force India F1 Team .   Sergio Pérez  Sergio Pérez Mendoza (    ; born 26 January 1990) also known as "Checo" Pérez, is a Mexican racing driver, currently driving for Force India. | Question: Which other Mexican Formula One race car driver has held the podium besides the Force India driver born in 1990?', ['Pedro Rodríguez'], 'mrqa_hotpotqa-validation-14'), tensor(-0.0173)), (('Context: Great Train Robbery Anniversary - Heart Beds, Bucks ...Great Train Robbery Anniversary - Heart Beds, Bucks & Herts News  Great Train Robbery Anniversary  Great Train Robbery Anniversary  See pictures of the bridge near Leighton Buzzard where The Great Train Robbery took place fifty years ago.  See all photos (15) in this gallery  By John Stratford (@Heart_JohnS), 8th August 2013, 10:50  Fifty years after The Great Train Robbery, an event\'s taken place to praise police who searched for the men who robbed a mail train in Buckinghamshire.  You may also like  On 8 August 1963, a gang of robbers, masterminded by Bruce Reynolds, stopped the Glasgow-Euston overnight mail train as it passed close to Cheddington, near Leighton Buzzard.  On board were huge numbers of used bank notes.  Twelve of the robbers were jailed for a total of more than 300 years but more than one broke out of prison, including notorious criminal Ronnie Biggs, who spent over 30 years on the run before he finally returned to Britain in 2001 to face arrest.  Reynolds returned in 1968, five years after the crime, and was captured in Torquay and jailed for 25 years.  Two Buckinghamshire Constabulary police officers who were involved in the investigation attended a police commemoration event alongside serving Thames Valley Police officers at Eynsham Hall in Witney, Oxfordshire, on Wednesday 7 August 2013.  Keith Milner was a detective at Aylesbury at the time of the robbery, while John Woolley was a PC and discovered Leatherslade Farm near Brill, Buckinghamshire, where the men hid after committing the crime.  Biggs insisted in July 2013 that he was proud to have been part of the gang.  The famous fugitive, who celebrates his 84th birthday on 8 August 2013, escaped from prison in 1965 and spent 36 years on the run before finally being arrested and jailed in 2001.  Released from prison on compassionate grounds in 2009 due to ill health he is still alive, being cared for in a north London nursing home, and he has few regrets about the crime that made him a household name.  Biggs, who cannot speak and communicates through a spelling board, said: "If you want to ask me if I have any regrets about being one of the train robbers, my answer is, \'No!\'.  "I will go further: I am proud to have been one of them. I am equally happy to be described as the \'tea-boy\' or \'The Brain\'.  "I was there that August night and that is what counts. I am one of the few witnesses - living or dead - to what was \'The Crime of the Century\'.\'\'  But although he is proud to have been involved in the headline-grabbing crime, he admitted he does have some regrets.  "It is regrettable, as I have said many times, that the train driver was injured,\'\' he said. "And he was not the only victim.  "The people who paid the heaviest price for the Great Train Robbery are the families. The families of everyone involved in the Great Train Robbery, and from both sides of the track.  "All have paid a price for our collective involvement in the robbery. A very heavy price, in the case of my family.  "For that, I do have my regrets.\'\'  Ronnie\'s son Michael, who was born while Ronnie was on the run, spoke exclusively to Heart and said he isn\'t proud of his dad being a criminal, but he is proud of the man his dad is.  "I believe if you commit a crime, you have to do the time," he said, "but the time has to be proportional with the crime that you\'ve committed.  "The sad bit was that he went into the train robbery because he needed £500 to put a deposit on his house. About a week before the robbery, he won the £500 in a bet on the horses but there was no turning back then."  Listen to Michael Biggs | Question: What criminal offence took place in Cheddington, Buckinghamshire in August (8th) 1963?', ['great train robbery'], 'mrqa_triviaqa-validation-7369'), tensor(-0.0219)), (('Context: Hey Ya!  "Hey Ya!"  is a song written and produced by André 3000 for his 2003 album "The Love Below", part of the hip hop duo OutKast\'s double album "Speakerboxxx/The Love Below".  "Hey Ya!"  takes influence from funk, rap and rock music.  Its music video features a live performance by a band, all eight of whose members are played by André 3000, that mimics the Beatles\' 1964 performance on "The Ed Sullivan Show".  The song received praise from contemporary music critics, and won the award for Best Urban/Alternative Performance at the 46th Grammy Awards.  His version of the song has also appeared on the soundtrack of   Hey Ya!  "Hey Ya!"  is a song written and produced by André 3000 for his 2003 album "The Love Below", part of the hip hop duo OutKast\'s double album "Speakerboxxx/The Love Below".  "Hey Ya!"  takes influence from funk, rap and rock music.  Its music video features a live performance by a band, all eight of whose members are played by André 3000, that mimics the Beatles\' 1964 performance on "The Ed Sullivan Show".  The song received praise from contemporary music critics, and won the award for Best Urban/Alternative Performance at the 46th Grammy Awards.  His version of the song has also appeared on the soundtrack of   The Ed Sullivan Show  The Ed Sullivan Show is an American TV variety show that ran on CBS from Sunday June 20, 1948, to Sunday June 6, 1971, and was hosted by New York entertainment columnist Ed Sullivan.  It was replaced in September 1971 by the "CBS Sunday Night Movie". | Question: Hey Ya is a song performed on The Ed Sullivan Show on CBS by which hip hop artist in 2003?', ['is a song written and produced by André 3000'], 'mrqa_hotpotqa-validation-2679'), tensor(-0.0223)), (('Context: Virginia Wade - International Tennis Hall of FameVirginia Wade  Virginia Wade  7-time major champion, 6-time finalist  Career Titles  Member of the British Wightman Cup Team 1965-1985  Member of the winning team 1968, 1974, 1975, 1978  Fed Cup  Member of the British Federation Cup Team 1967-1970, 1972-1983  Overall Record: 66-33  Virginia Wade WTA Profile  Citizenship: GBR Born: July 10, 1945               in Bournemouth, England Played: Right-handed  Virginia Wade chose the perfect time to become the first British female to win the Wimbledon Ladies Singles Championship in 8 years and the last to win since.  In July 1977, the summer of the monarch’s Silver Jubilee, Wade won her third major title at the All England Club, with nicely coincided with Wimbledon’s centenary year. Queen Elizabeth II, who made no qualms about not being a tennis fan, was making only her second ever appearance at Centre Court when Wade met Betty Stove of the Netherlands for the championship. “If she’s [Queen Elizabeth II] going to be there, I am going to be there too,” Wade said.  Wade, then 31, and singles champion at the US Open in 1968 and at the Australian Open in 1972, wore a beautiful pink cardigan as she arrived on court. After dropping the first set 4-6, her whole demeanor turned a fiery red in winning the final two sets convincingly, 6-3, 6-1. Wade, who was playing in her 17th\xa0of an all-time record 26 Wimbledons, upset No. 1 seed Chris Evert, 6-2, 4-6, 6-1, in the semifinals to reach her one and only final. Once she settled into a groove after the first set, her precise groundstrokes controlled the match. “Winning Wimbledon was the thing that made my career worthwhile,” Wade told the Guardian in 1977. Those in attendance witnessed Queen Elizabeth II present Wade with championship trophy and then sang a rendition of “For She’s a Jolly Good Fellow” to celebrate the victory.  With her victory in London, Wade held the distinction of being the last Brit to win Wimbledon until Andy Murray in 2013. Her winnings were $20,499, compared to today’s multi-million dollar purse. In an interview with the\xa0Independent\xa0in 2007, Wade joked that her Wimbledon title came right on schedule. “Well, Angela Mortimer had won in 1961 and Ann Jones in 1969, so when I won in 1977 we all thought it happened every eight years, but maybe we were just anomalies, because there was Sue Barker and Joe Durie, but then the [British] players just petered out.”  Wade was a lithe 5-foot-8 steady stroke machine who had a beautiful all-court game built on a smooth slice backhand and a forehand that she could hit with topspin or flat. Wade was a thinker between the lines; her shots were patient and calculated. She was adept at controlling tempo and wouldn’t be forced into foolish shots. She would pause a minimum of five seconds before releasing her serve after getting into the ready position and hopped into position to move laterally or take a short ball and attack the net.  In her lengthy 26-year career, Wade won 55 singles titles, eighth on the all-time list. She favored playing at Wimbledon and the US Open the most of any of the four majors. As a 23-year old in 1968, she won the inaugural US Open, doing so as the No. 6 seed with a stunning and unexpected 6-4, 6-2 victory over No. 1 seed Billie Jean King. It was Wade’s fifth trip to the US major and although she would play the event another 15 times, her best finishes afterward were the semifinals in 1969, 1970, and 1975. The 1968 victory earned her $6,000. Wade only traveled to the Australian Open five times, but in 1972 stung another No. 1 seed when defeating crowd favorite and native Evonne Goolagong, who was in the midst of playing in seven consecutive Australian finals, | Question: Prior to Viginia Wade in 1977 who was the last British player to win a tennis Grand Slam title?', ['sue barker'], 'mrqa_triviaqa-validation-5406'), tensor(-0.0258)), (('Context: George Cross | British medal | Britannica.comGeorge Cross | British medal | Britannica.com  British medal  Academy Award  George Cross, a British civilian and military decoration, instituted in 1940 by King George VI for “acts of the greatest heroism or of the most conspicuous courage in circumstances of extreme danger.” The award, which can be conferred posthumously, is usually given to civilians, although it can be bestowed on military personnel for acts for which military decorations are not usually awarded. The George Cross superseded the Medal of the Order of the British Empire for Gallantry (commonly known as the Empire Gallantry Medal).  George Cross medal engraved on a tombstone.  Acmthompson  The island of Malta received the George Cross in recognition of its inhabitants’ gallantry in World War II. Recipients of this award may add G.C. after their names; the cross ranks second only to the Victoria Cross (the highest British military decoration). The cross is silver, with one side depicting St. George slaying the dragon and with the inscription “For Gallantry;” the other side gives the recipient’s name and the date of the award.  The George Medal, instituted at the same time as the George Cross, is analogous to it but is awarded for services not quite so outstanding as those which merit the George Cross. Recipients of this medal can add G.M. after their names. The medal is silver; one side has the effigy of the reigning British monarch, and the other side has St. George and the dragon with the inscription “The George Medal.”  Learn More in these related articles:  World War 2 Awards.com - George CrossWorld War 2 Awards.com - George Cross  George Cross  Number of awards in the database: 139  Total awarded for WW II:    139  Recipients A-Z  The George Cross -or GC- was instituted June 24th, 1941, by King George VI. This decision was actuated by an air raid on London in 1940 during which the Royal Palace was hit. After this incident, King George became emotionally more involved with his people and he realised what they had to go through during the raids, admiring even more their hardships and heroism. As a result, the George Cross and George Medal were largely designed by him.  The George Cross is the second highest British decoration and is awarded for "acts of the greatest heroism and bravery in circumstances of extreme danger". It was initially awarded to all citizens of the Commonwealth and was later extended to military personnel for actions that did not usually merit a military decoration. Recipients are entitled to add the letters GC after their names. Bars will be awarded for subsequent acts of bravery and the decoration can be awarded posthumously.  The GC is the successor to the Medal of the Order of the British Empire for Gallantry and its recipients were obliged to return it to the Central Chancellery of the Orders of Knighthood to have it replaced by a George Cross. In 1971, recipients of the Albert Medal could also exchange this for a George Cross.  The badge is made of silver. On the front is an image of St. George fighting the dragon surrounded by the inscription \'For Gallantry\', on the reverse are engraved the name of the recipient and the date of the award. Each arm of the cross bears the cypher of the reigning Monarch, during the war GRI or GRVI, after the war EIIR. The cross hangs from a silver bar adorned with laurel leaves by a silver ring. The ribbon is 1.75" (38 mm) wide and coloured blue with a miniature GC in its centre.  The George Cross was conferred upon the Island of Malta for the display of bravery of its citizens during the war. Four times a George Cross was conferred upon a woman, three of which posthumously. Three women were awarded the GC for resistance activities in occupied  territories during WW 2. One of them was Noor Inayat Khan. This Russian born woman was dropped over France July 16th, 1943, to assist the French resistance movement in their activities. Some three months later, she was betrayed and taken prisoner by the Gestapo (German Secret Police). Despite numerous interrogations she | Question: What is the inscription on the George Cross ?', ['for gallantry'], 'mrqa_triviaqa-validation-2096'), tensor(-0.0262)), (("Context: He produced artificial lightning, with discharges consisting of millions of volts and up to 135 feet long. Thunder from the released energy was heard 15 miles away in Cripple Creek, Colorado. People walking along the street observed sparks jumping between their feet and the ground. Sparks sprang from water line taps when touched. Light bulbs within 100 feet of the lab glowed even when turned off. Horses in a livery stable bolted from their stalls after receiving shocks through their metal shoes. Butterflies were electrified, swirling in circles with blue halos of St. Elmo's fire around their wings. | Question: What happened to nearby light bulbs?", ['glowed even when turned off', 'glowed'], 'mrqa_squad-validation-1516'), tensor(-0.0264)), (("Context: It is an allusion to Samuel Taylor Coleridge 's poem The Rime of the Ancient Mariner ( 1798 ) . In the poem , an albatross starts to follow a ship -- being followed by an albatross was generally considered a sign of good luck . However , the titular mariner shoots the albatross with a crossbow , which is regarded as an act that will curse the ship ( which indeed suffers terrible mishaps ) . Even when they are too thirsty to speak , the ship 's crew let the mariner know through their glances that they blame his action for the curse . The albatross is then literally hung around the mariner 's neck by the crew to symbolize his guilt in killing the bird . Thus , the albatross can be both an omen of good or bad luck , as well as a metaphor for a burden to be carried as penance . | Question: the rime of the ancient mariner albatross symbolism ?", ['a metaphor for a burden to be carried as penance', 'omen of good or bad luck'], 'mrqa_naturalquestions-validation-7017'), tensor(-0.0294)), (('Context: On 7 January 1943, at the age of 86, Tesla died alone in room 3327 of the New Yorker Hotel. His body was later found by maid Alice Monaghan after she had entered Tesla\'s room, ignoring the "do not disturb" sign that Tesla had placed on his door two days earlier. Assistant medical examiner H.W. Wembly examined the body and ruled that the cause of death had been coronary thrombosis. Tesla\'s remains were taken to the Frank E. Campbell Funeral Home at Madison Ave. and 81st St. A long-time friend and supporter of Tesla, Hugo Gernsback, commissioned a sculptor to create a death mask, now displayed in the Nikola Tesla Museum. | Question: What had been left hanging on the door to Tesla\'s room?', ['"do not disturb" sign', 'do not disturb" sign'], 'mrqa_squad-validation-1539'), tensor(-0.0317)), (('Context: The Roosevelt Corollary was an addition to the Monroe Doctrine articulated by President Theodore Roosevelt in his State of the Union address in 1904 after the Venezuela Crisis of 1902 -- 03 . The corollary states that the United States will intervene in conflicts between European countries and Latin American countries to enforce legitimate claims of the European powers , rather than having the Europeans press their claims directly . | Question: who warned europe to stay out of the americas ?', ['Roosevelt Corollary', 'Monroe Doctrine'], 'mrqa_naturalquestions-validation-2501'), tensor(-0.0318)), (("Context: Rolls-Royce Welland  The Rolls-Royce RB.23 Welland was Britain's first production jet engine.  It entered production in 1943 for the Gloster Meteor.  The name Welland is taken from the River Welland, in keeping with the Rolls-Royce policy of naming early jet engines after rivers based on the idea of continuous flow, air through the engine and water in a river.   River Welland  The River Welland is a lowland river in the east of England, some 65 mi long.  It drains part of the Midlands eastwards to The Wash.  The river rises in the Hothorpe Hills, at Sibbertoft in Northamptonshire, then flows generally northeast to Market Harborough, Stamford and Spalding, to reach The Wash near Fosdyke.  It is a major waterway across the part of the Fens called South Holland, and is one of the Fenland rivers which were laid out with washlands.  There are two channels between widely spaced embankments with the intention that flood waters would have space in which to spread while the tide in the estuary prevented free egress.  However, after the floods of 1947, new works such as the Coronation Channel were constructed to control flooding in Spalding and the washes are no longer used solely as pasture, but may be used for arable farming. | Question: Rolls-Royce Welland's name is taken from this river that is low long ?", ['65 mi'], 'mrqa_hotpotqa-validation-1201'), tensor(-0.0325)), (('Context: The history of agriculture records the domestication of plants and animals and the development and dissemination of techniques for raising them productively . Agriculture began independently in different parts of the globe , and included a diverse range of taxa . At least eleven separate regions of the Old and New World were involved as independent centers of origin . | Question: where did the cultivation of agriculture first arise ?', ['eleven separate regions of the Old and New World'], 'mrqa_naturalquestions-validation-8119'), tensor(-0.0325)), (('Context: TurbineA turbine (from the Latin turbo, a vortex, related to the Greek , tyrbē, meaning "turbulence"),   is a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work. A turbine is a turbomachine with at least one moving part called a rotor assembly, which is a shaft or drum with blades attached. Moving fluid acts on the blades so that they move and impart rotational energy to the rotor. Early turbine examples are windmills and waterwheels.  Gas, steam, and water turbines have a casing around the blades that contains and controls the working fluid. Credit for invention of the steam turbine is given both to the British engineer Sir Charles Parsons (1854–1931), for invention of the reaction turbine and to Swedish engineer Gustaf de Laval (1845–1913), for invention of the impulse turbine. Modern steam turbines frequently employ both reaction and impulse in the same unit, typically varying the degree of reaction and impulse from the blade root to its periphery.  The word "turbine" was coined in 1822 by the French mining engineer Claude Burdin from the Latin turbo, or vortex, in a memo, "Des turbines hydrauliques ou machines rotatoires à grande vitesse", which he submitted to the Académie royale des sciences in Paris.  Benoit Fourneyron, a former student of Claude Burdin, built the first practical water turbine.   Operation theory   A working fluid contains potential energy (pressure head) and kinetic energy (velocity head). The fluid may be compressible or incompressible. Several physical principles are employed by turbines to collect this energy:  Impulse turbines change the direction of flow of a high velocity fluid or gas jet. The resulting impulse spins the turbine and leaves the fluid flow with diminished kinetic energy. There is no pressure change of the fluid or gas in the turbine blades (the moving blades), as in the case of a steam or gas turbine, all the pressure drop takes place in the stationary blades (the nozzles). Before reaching the turbine, the fluid\'s pressure head is changed to velocity head by accelerating the fluid with a nozzle.  Pelton wheels and de Laval turbines use this process exclusively. Impulse turbines do not require a pressure casement around the rotor since the fluid jet is created by the nozzle prior to reaching the blades on the rotor. Newton\'s second law describes the transfer of energy for impulse turbines.  Reaction turbines develop torque by reacting to the gas or fluid\'s pressure or mass.  The pressure of the gas or fluid changes as it passes through the turbine rotor blades. A pressure casement is needed to contain the working fluid as it acts on the turbine stage(s) or the turbine must be fully immersed in the fluid flow (such as with wind turbines). The casing contains and directs the working fluid and, for water turbines, maintains the suction imparted by the draft tube. Francis turbines and most steam turbines use this concept. For compressible working fluids, multiple turbine stages are usually used to harness the expanding gas efficiently. Newton\'s third law describes the transfer of energy for reaction turbines.  In the case of steam turbines, such as would be used for marine applications or for land-based electricity generation, a Parsons type reaction turbine would require approximately double the number of blade rows as a de Laval type impulse turbine, for the same degree of thermal energy conversion. Whilst this makes the Parsons turbine much longer and heavier, the overall efficiency of a reaction turbine is slightly higher than the equivalent impulse turbine for the same thermal energy conversion.  In practice, modern turbine designs use both reaction and impulse concepts to varying degrees whenever possible. Wind turbines use an airfoil to generate a reaction lift from the moving fluid and impart it to the rotor. Wind turbines also gain some energy from the impulse of the wind, by deflecting it at an angle. Turbines with multiple stages may utilize either reaction or impulse blading at high pressure. Steam turbines were traditionally more impulse but continue to move towards reaction designs similar to those used in gas turbines. At low pressure the operating fluid medium expands in volume for small reductions in | Question: What main category of machines \'produce continuous power from a wheel/rotor, usually with vanes, revolved by fast flowing fluid\'?', ['turbine engines', 'reaction turbine', 'turbines', 'turbine', 'turbine blades', 'impulse turbine'], 'mrqa_triviaqa-validation-2376'), tensor(-0.0334)), (('Context: Sunday in the Park with George - Music Theatre InternationalSunday In The Park... | Music Theatre International  Music Theatre International  Request Licenses & Perusals, Pay Invoices  Your Web Profile  We are working quickly to merge our two logins.  Region  Sunday In The Park With George  Original Broadway Version (1984)  Inspired by George Seurat\'s famous painting, this poetic masterpiece explores the challenges in understanding life and art.  Inspired by the painting A Sunday Afternoon on the Island of La Grande Jatte by Georges Seurat, Sunday In The Park With George, Stephen Sondheim and James Lapine\'s stunning masterpiece merges past and present into beautiful, poignant truths about life, love and the creation of art. One of the most acclaimed musicals of our time, this moving study of the enigmatic painter Georges Seurat won a Pulitzer Prize and was nominated for an astounding 10 Tony Awards including Best Musical.  The days leading up to the completion of his most famous painting, A Sunday Afternoon on the Island of La Grande Jatte, Georges Seurat is struggling to make meaningful art and maintaining a relationship with his lover Dot. Amid the scorn of the artistic community, Seurat\'s artistic ability thrives while his love diminishes. A century later, Seurat\'s descendant - named George and also an artist - finds himself burnt out and in search of what artistic path to follow, but he finds the answer to his future in the past.  An ensemble of strong singing actors performs this challenging and heartbreaking work about our need to connect to the past, present and future. Sunday In The Park With George features two coveted starring roles made famous by the Broadway performances of Mandy Patinkin and Bernadette Peters. The show may be staged simply as in its original workshop production or with the grandeur of Seurat\'s masterpiece.  "Sunday in the Park with George" - About.com Education"Sunday in the Park with George"  "Sunday in the Park with George"  "Sunday in the Park with George"  From Canvas to Characters  By Rosalind Flynn  Updated February 27, 2016.  Anyone who reads or plans to attend a performance of Sunday in the Park With George by Stephen Sondheim and James Lapine should first spend some time looking at an image of the this painting by Georges Seurat :\xa0 Its original French title is Un dimanche après-midi à l\'Île de la Grande Jatte – English translation: “ Sunday Afternoon on the Island of la Grande Jatte .” (Click on the painting title to view the online image.)  The first act of Sunday in the Park With George occurs with Seurat the artist creating and perfecting his painting. The creative musical theatre artists who were inspired by this painting drew the other characters in Act One from figures in the painting.  Looking Closely at Art  It would not be surprising to learn that Sondheim and Lapine used their own version of Visual Thinking Strategies to examine this painting that is 10 feet wide and 6 and a half feet tall. Visual Thinking Strategies provide ways to look closely at works of art by asking observers to respond to these three questions:  continue reading below our video  10 Best Universities in the United States  1. What’s going on in this picture?  2. What do you see that\xa0makes you say that?  3. What more can we find?  In fact, it would be excellent to begin a study of this play by engaging students in doing precisely that – looking at an image of “Sunday Afternoon on the Island of la Grande Jatte” and asking them to respond to those questions.  What’s Going On? According to Sondheim and Lapine  The painting’s setting is the island of la Grande Jatte, which is located northwest of Paris in the Seine River. In 1884 (when Seurat began working on his painting), the island was a pastoral place where people could go to get away from city life. The musical’s creative artists pulled their characters from the canvas. Here’s a Who’s Who:  The most prominent figure is the woman in profile on the bottom right hand side. Sondheim and Lapine imagined her as Seurat’s mistress and they | Question: The musical \'Sunday In The Park With George\' was inspired by a painting by which artist?', ['georges seurat', 'seurat'], 'mrqa_triviaqa-validation-5937'), tensor(-0.0336)), (('Context: Least of the Great Powers  The least of the Great Powers is a label used to conceptualize Italy\'s international status.  Italy is part of great power concerts such as the EU trio, the NATO Quint, the G7, the G20 and various International Contact Groups. Italy, one of the UN\'s major funders, is the leading nation of the Uniting for Consensus and serves as one of the states of "chief" importance in providing shipping services, air transport and Industrial development.  Alternative terms used by academics and observers to describe this concept include "intermittent Major power" or "small Great power", asserting that Italy\'s position in the international arena can be described in this way.   G20  The G20 (or G-20 or Group of Twenty) is an international forum for the governments and central bank governors from 20 major economies.  Currently, these are Argentina, Australia, Brazil, Canada, China, France, Germany, India, Indonesia, Italy, Japan, Mexico, Russia, Saudi Arabia, South Africa, South Korea, Turkey, United Kingdom, United States, and the European Union.  Founded in 1999, the G20 aims to discuss policy issues pertaining to the promotion of international financial stability.  It seeks to address issues that go beyond the responsibilities of any one organization.  The G20 heads of government or heads of state have periodically conferred at summits since their initial meeting in 2008, and the group also hosts separate meetings of finance ministers and foreign ministers due to the expansion of its agenda in recent years. | Question: What part of the great power concerts such as the EU trio, the Nato Quint, the G7, and the G20, is Italy associated with?', ['G20'], 'mrqa_hotpotqa-validation-4825'), tensor(-0.0377)), (("Context: Steve Carell as Felonious Gru , the former villain turned Anti-Villain League agent , Margo , Edith , and Agnes ' adoptive father , and Lucy 's husband .   Carell also voices Dru , Gru 's long - lost twin brother and the girls ' adoptive uncle .     Kristen Wiig as Lucy Wilde , an Anti-Villain League agent , Gru 's wife and the girls ' adoptive mother .   Trey Parker as Balthazar Bratt , a supervillain and former child star who grows up to become obsessed with the character he played in the ' 80s and is bent on world domination .   Miranda Cosgrove as Margo , Gru and Lucy 's oldest adoptive daughter .   Dana Gaier as Edith , Gru and Lucy 's middle adoptive daughter .   Nev Scharrel as Agnes , Gru and Lucy 's youngest adoptive daughter . She was previously voiced by Elsie Fisher in the first two films .   Pierre Coffin as Mel and the Minions .   Coffin also voices a Museum Director .     Steve Coogan as Silas Ramsbottom , the director of the Anti-Villain League .   Coogan also voices Fritz , the butler of Dru .     Julie Andrews as Marlena Gru , Gru and Dru 's mother .   Jenny Slate as Valerie Da Vinci , a member of the Anti-Villain League who becomes the brand new AVL director .   Andy Nyman as Clive , a robot who is Bratt 's sidekick .   Adrian Ciscato as Niko , a boy from Freedonia who falls in love with Margo .   John Cygan ( in his final film role ) as an Additional Voice . | Question: who is the cast of despicable me 3 ?", ['Nev Scharrel', 'Andy Nyman', 'Adrian Ciscato', 'Pierre Coffin', 'Steve Coogan', 'Trey Parker', 'Julie Andrews', 'Kristen Wiig', 'John Cygan', 'Jenny Slate', 'Dana Gaier', 'Steve Carell', 'Miranda Cosgrove'], 'mrqa_naturalquestions-validation-3490'), tensor(-0.0410)), (('Context: MS Kronprins Harald  Several motor ships have borne the name Kronprins Harald, after Harald V of Norway:   Harald V of Norway  Harald V (] ; born 21 February 1937) is the King of Norway, having ascended the throne following the death of his father on 17 January 1991. | Question: What kind of ships have been named after the King of Norway who ascended the throne in 1991?', ['motor ships'], 'mrqa_hotpotqa-validation-5699'), tensor(-0.0442)), (('Context: Rumpole of the Bailey - TV.comRumpole of the Bailey - Show News, Reviews, Recaps and Photos - TV.com  Rumpole of the Bailey  EDIT  Welcome to the Rumpole of the Bailey guide at TV.com.  Horace Rumpole (played by the late Leo McKern) is an untidy, ageing London barrister with one glass eye who defends in criminal cases. His clients rarely cut elegant figures. He is fond of red wine, poetry, and fair dealing, and is not looked on as a great success in life by his wife, Hilda (\'She Who Must Be Obeyed\'). Rumpole has had a few triumphs, and the Penge Bungalow murders are often on his mind... Rumpole shares Chambers at Number 3, Equity Court, with a mixed group of barrister colleagues, including Guthrie Featherstone (Peter Bowles) and Phyllida Trant (Patricia Hodge). He also takes pupils - notably Fiona Allways (played by Rosalyn Landor) and Liz Probert. The creator and writer of the series, Sir John Mortimer, received an Edgar Allan Poe Award for crime and mystery for Rumpole.  Horace Rumpole - Thrilling DetectiveHorace Rumpole  Horace Rumpole  Created by John Mortimer (1923-2009 )  "Crime doesn\'t pay, but it\'s a living."  Down those mean streets and meaner chambers a man must waddle...  Lord knows, he\'s not a private eye, but God, I wish he were. Like Philip Marlowe , Rumpole is ""a relatively poor man... a common man or he could not go among common people. He has a sense of character, or he would not know his job. He will take no man\'s money dishonestly and no man\'s insolence without a due and dispassionate revenge. He is a lonely man and his pride is that you will treat him as a proud man or be very sorry you ever saw him. He talks as the man of his age talks -- that is, with a rude wit, a lively sense of the grotesque, a disgust for sham, and a contempt for pettiness."  So, may I submit for your consideration, Your Honour....  That great defender of most muddled and sinful humanity...  With his jowls a-quiver...  His fondness for Wordsworth, Chateau Thames Embankment and hopeless cases...  His cheroot-puffing and claret-quaffing...  His food-bespeckled robe and raggedy wig...  And his beloved and tattered copy of The Oxford Book of English Verse clutched to his bosom...  For his oratorical outbursts...  His always entertaining jabs at the soft underbelly of hypocrisy, pomposity and upper class twits...  And for standing up for truth, justice, honour and the Golden Thread of Justice...  May I submit for inclusion, Your Honour, this most British of all lawyers...  This proud, this defiant Old Bailey Hack...  With his best gal, Hilda, She Who Must Be Obeyed, standing, NOT amused, by his side...  The one, the only...  HORACE RUMPOLE.  May there always be an England, and may there always be Horace Rumpole to see that justice be done. And thev pompous may squirm.  Your Honour, I rest my case.  ****  Rumpole is, of course John Mortimer\'s rotund, defiant British criminal lawyer who, as brilliantly brought to life by the late, great Australian actor Leo McKern, the star of Rumpole of the Bailey, the popular British courtroom comedy/drama that originally aired on Thames Television in 1978, and soon became popular on both sides of the Atlantic, appearring on the American Public Broadcasting Service as part of its Mystery! series.  Mortimer wrote each and every episode of the television series, and their subsequent novelizations. The show ran, off and on, for seventeen years, an incredible run, and inspired not just the short stories, but novels and two radio series. It was only in 1995, with the publication of the Rumpole and the Angel of Death collection, that Mortimer began writing original Rumpole stories (ie: not adapted from his own TV scripts). Sice then, two | Question: Who created Rumpole of the Bailey?', ['john mortimer', 'john mortimer qc'], 'mrqa_triviaqa-validation-1551'), tensor(-0.0463)), (('Context: BBC Wales - Arts - Children - Ivor The EngineBBC Wales  - Arts - Children  - Ivor The Engine  Ivor The Engine  Ivor The Engine  Last updated: 11 February 2009  "Not very long ago, in the top left-hand corner of Wales, there was a railway.  It wasn\'t a very long railway or a very important railway, but it was called The Merioneth and Llantisilly Rail Traction Company Limited, and it was all there was.  "And in a shed, in a siding at the end of the railway, lives the Locomotive of the Merioneth and Llantisilly Rail Traction Company Limited, which was a long name for a little engine so his friends just called him Ivor."  Ivor The Engine was a children\'s animation created by Oliver Postgate and Peter Firmin\'s Smallfilms television company. It was made in black and white for Associated Rediffusion and first broadcast in 1959, but was remade in colour for the BBC in 1975.  It told the story of a sometimes disobedient small green locomotive who worked for the Merioneth and Llantisilly Rail Traction Company Limited, although he dreamed of singing with the Grumbley and District Choral Society. In time he became first bass of the choir and transported the members from place to place.  His driver was Edwin Jones, known as Jones the Steam, who enjoyed fishing and daydreaming. Ivor\'s other friends included stationmaster Dai Station, portly choirmaster Evans the Song, fellow chorister Idris the Dragon, and fairground owner Morgan the Roundabout. Mr Morgan gave Ivor some pipes from his steam calliope to allow him to sing in the choir.  Behind the scenes  Ivor The Engine was inspired by Welshman Denzyl Ellis, a former railwayman whom Postgate met shortly after World War Two. Ellis, a former fireman with the Royal Scot train, described how locomotives came to life in the mornings after the engine fires had been lit.  The programme was Smallfilms\' first production. Although set in Wales, Ivor The Engine was actually made in Firmin\'s home in Blean near Canterbury, Kent, using stop-motion animation techniques. Cardboard cutouts painted with watercolours were used for the characters and backgrounds.  The programmes were written and narrated by Oliver Postgate and drawn and painted by Peter Firmin. Voices were performed by Oliver Postgate, Anthony Jackson and Olwen Griffiths - the only Welsh person involved in the production.  The distinctive puffing sound made as Ivor moved was voiced by Postgate, and the show\'s music was composed by Vernon Elliott. Ivor\'s \'voice\' was mostly sounded by a bassoon.  Postgate described himself as having been "intoxicated by the work of Dylan Thomas , and used to carry Under Milk Wood around in my pocket". Wales was a logical setting for the tale, and became romanticised in Postgate\'s fictionalised world.  "Wales is where you have little railways going along the tops of hills, which is much less boring that hurtling up the slumbering Midlands plain in the middle of the night," he told science fiction enthusiast Clive Banks , "so we decided it would be nice to set it in Wales."  "Ivor The Engine is entirely bogus as far as Wales is concerned - it\'s built entirely on a picture of Wales given by Dylan Thomas! Then, literally in the bath, I came to realise what the story was: the engine wanted to sing in the choir, which is obviously what a Welsh engine would want, so from then on it fell into place.  Postgate and Firmin created a map of the top left corner of north Wales where Ivor lived. It included viaducts, bridges, tunnels, towns, a mine and gasworks, and was strictly adhered to by the show\'s creators.  The episodes  The original series comprised six episodes, and told the story of Ivor getting his pipes and joining the choir. They were followed by two further series of 13 episodes, all of which were in black and white. Each lasted 10 minutes.  The animations were shown regularly by Associated Rediffusion until the company folded in 1968. In 1975 Smallfilms were given back the rights to the stories and began remaking the second | Question: In the children’s television series ‘Ivor the Engine’ what is the name of the dragon?', ['idris'], 'mrqa_triviaqa-validation-893'), tensor(-0.0484)), (('Context: Afonwen  Afonwen (] ; Welsh: "Afon-wen" ) is a town in Flintshire, Wales.  It is situated just under four miles from the A55 North Wales Expressway and on the A541 Mold-Denbigh road.  At the 2001 Census, the population of Afonwen was included into the civil parish of Caerwys and was 1,319, with a total ward population of 2,496.   A55 road  The A55, also known as the North Wales Expressway (Welsh: "Gwibffordd Gogledd Cymru") and the Chester to Bangor Trunk Road, is a major road in Britain.  Its entire length is a dual carriageway primary route, with the exception of the point where it crosses the Britannia Bridge over the Menai Strait and several short sections where there are gaps in between the two carriageways.  All junctions are grade separated except for two roundabouts — one east of Penmaenmawr and one in Llanfairfechan.  The road originally ran from Chester to Bangor but was extended parallel to the A5 across Anglesey to just outside Holyhead Docks in 2001.  The road improvements have been part funded with European money, under the Trans-European Networks programme, as the route is designated part of Euroroute E22 (Holyhead - Leeds - Amsterdam - Hamburg - Malmö - Riga - Moscow - Perm - Ekaterinburg - Ishim). | Question: Afonwen is situated just under four miles from what road also known as the North Wales Expressway ?', ['A55'], 'mrqa_hotpotqa-validation-1888'), tensor(-0.0495)), (('Context: Amphitrite in Greek Mythology | MythographyAmphitrite in Greek Mythology | Mythography  Amphitrite in Greek Mythology  Home | Greek Gods | Free Spirits | Amphitrite  In Greek mythology, Amphitrite was a sea goddess. Sources state that she was the daughter of Nereus and Doris, which makes Amphitrite one of the fifty sea goddesses (or nymphs) who are known collectively as the Nereids. The Greek poet Hesiod lists the names of these sea nymphs in his Theogony:  “To Nereus and Doris of the lovely hair,  the daughter of Okeanos, the stream surrounding the earth,  a host of godly daughters was born in the barren sea:  Proto, Eukrante, Amphitrite, and Sao…”  (Hesiod, Theogony, lines 240-43)  According to the legend, the Olympian Poseidon (who was himself a god of the sea) saw Amphitrite one day and fell in love with the graceful and beautiful goddess. However, the sea nymph initially resisted Poseidon’s advances. But Poseidon was not easily discouraged. He sent his sea creatures to find Amphitrite. A dolphin succeeded in locating the Nereid, and it was this creature - the dolphin - who persuaded her to consider Poseidon’s proposal. Eventually, Amphitrite accepted Poseidon, and the two gods were married.  Amphitrite and Poseidon were together the parents of several children, including the sea god Triton. Here’s Hesiod again:  “From the union of rumbling Poseidon and Amphitrite  came the great Triton, whose might is far-flung,  an awesome god dwelling in a golden house that lies  at the sea’s bottom, near his cherished mother and lordly father.”  (Hesiod, Theogony, lines 930 ff. )  Amphitrite Goddess of the Sea - Greek Gods and GoddessesAmphitrite Goddess of the Sea  [ ? ]Subscribe To This Site  Amphitrite Goddess of the Sea  In Greek mythology, Amphitrite goddess of the Sea was one of the fifty Nereids, that is a daughter of Nereus and Doris - that\'s what Hesiod writes in his Theogony (but Apollodorus says her parents were Oceanus and Thetys). Her Roman equivalent was Salacia, god Neptune\'s wife.  Her name means "The third one who encircles (everything)" (because the ancient Greeks thought the whole world was encircled by a river-god, Oceanus ).  She spent most of the time singing and dancing with her sisters. One day Poseidon, god of the sea , saw her near the island of Naxos and fell in love with her. He kidnapped her and made her his wife.  In another story, Amphitrite, being a shy girl, fled away and hid somewhere in the Atlantic Ocean. Poseidon wanted desperately to find her, so he sent all the marine creatures to look for her. Delphinus, a dolphin, managed to find her and convinced her to go back to Poseidon and marry him. That\'s what made Amphitrite goddess of the sea. (By the way, the dolphin who found her was turned into a constellation by the grateful Poseidon ).  Amphitrite goddess of the sea has no special attributions (but sometimes she can calm the waters), she is just the mistress of the sea (and sometimes she is considered the mother of the marine creatures, but they existed even before she became queen of the sea). Some even say that she is just a personification of the sea.  She also appeared briefly in the story of Theseus. King Minos asked the hero to prove that he really was Poseidon\'s son. A ring was thrown into the sea and Theseus had to bring it back. The sea creatures found the ring and gave it to the hero, while Amphitrite goddess of the sea received him in her underwater palace and gave him a golden crown, which he took to Ariadne as a wedding gift. (As you can see, Amphitrite was the ideal wife, as she was never jealous of her husband\'s love affairs.)  Amphitrite and Poseidon had a son, Triton (a fish-man or a he-mermaid) and a daughter, Rhode, and maybe another daughter, Benthesicyme.  In art | Question: In Greek mythology, Amphitrite, queen of the sea, was the wife of which god?', ['poseidon'], 'mrqa_triviaqa-validation-671'), tensor(-0.0503)), (("Context: BLACK DOG: Mitchell - now it's snobgate | Daily Mail OnlineBLACK DOG: Mitchell - now it's snobgate | Daily Mail Online  BLACK DOG: Mitchell - now it's snobgate  comments  As revealed in these columns before his downfall in the 'Plebgate' scandal, Tory MP Andrew Mitchell was known as 'Thrasher' in his days as head boy of Rugby School. Dog is now intrigued to learn he had a second nickname: 'Mitchell-snob'.  A fellow Old Rugbeian observes drily: 'It had the advantage that you could drop the hyphen and insert an apostrophe between the 's' and 'n' of snob.'  Tory MP Andrew Mitchell was known as 'Thrasher' in his days as head boy of Rugby School  When Mitchell ran Tory pal David Davis's Conservative leadership campaign in 2005, political broadcaster Michael Cockerell asked Mitchell: 'What do you say to those who argue Davis is Iain Duncan Smith with hair?'\xa0  Adopting a pantomime-villain glower, Mitchell hissed: 'Tell me their names and I'll sue.' How he must wish he had only joked about suing The Sun over PC Toby Rowland.  Hot 'babe' Pepi gets hooked on Vegas\xa0  Ex MoD official Pepi Simpson sashayed in to hear a husky American accent drawl in her direction: 'That's what I call a babe'  A chance visit to a high rollers' bar in a Las Vegas hotel while on holiday with a girlfriend had unintended consequences for Pepi Simpson, sporty wife of Tory MP Keith Simpson. Ex-MoD official Pepi, pictured in the bar, dolled up after a visit to the crimper, sashayed in to hear a husky American accent drawl in her direction: 'That's what I call a babe!' No one told her it was a hookers' pick-up joint. Pepi replied tartly: 'Not bad for 61.'  Hats off to gutsy Andrew Marr, who has thrown away his walking stick nearly two years after a debilitating stroke. In between hosting his Sunday morning TV show and gruelling physiotherapy sessions that set him on the road to recovery, Marr has just completed a second novel and is working on a poetry project. Dog would like to know what he has for breakfast.  On hearing that Alex Salmond was among guests at the Spectator magazine Parliamentary Awards lunch at The Savoy, veteran Tory Norman Tebbit mused he had always wanted to meet the now ex-SNP leader. When a fellow guest asked why, out came 'Chingford Skinhead' Tebbit's razor: 'I've always wanted to sprinkle salt over him to see if he dissolves into slime like the slugs in my garden.'  Theresa May reveals all\xa0  The Spectator's attempt to keep guests in suspense over the winner of the Parliamentarian of the Year award was ruined when Theresa May opened proceedings with a jokey video presentation and pressed the wrong button on her remote control.\xa0  Up flashed the winner's name: Edward VII lookalike and former Commons Clerk Sir Robert Rogers, who was forced to quit after falling out with Speaker John Bercow.  It seems John Bercow has paid a heavy price for falling out with Sir Robert. David Cameron rubbed his nose in it by giving Sir Robert a peerage – and there's worse to come. Another of Bercow's tormentors, former Speaker Betty Boothroyd, is tipped to be at Sir Robert's side as one of his two 'sponsors' when he takes his seat in the Upper House.\xa0  \xa0  The cuddly side of the Chingford Skinhead - Daily Mail OnlineNorman Tebbit's as savage as ever except about wife and the dog who inspired new book | Daily Mail Online  The cuddly side of the Chingford Skinhead: Cameron? Silly. Blair? Offensive. Cherie? Urggh. Norman Tebbit's as savage as ever - except about the wife he adores and the dog who inspired his startling new book  Norman Tebbit has published a children's book about a dog called Ben  The former Minister on Maria Miller: 'It offends against common sense'  On Farage: 'A clever operator. He had touched on | Question: Which ex-politician, now aged 80, had the nickname 'The Chingford Skinhead' ?", ['norman tebbit'], 'mrqa_triviaqa-validation-1924'), tensor(-0.0503)), (("Context: Doctor Who has appeared on stage numerous times. In the early 1970s, Trevor Martin played the role in Doctor Who and the Daleks in the Seven Keys to Doomsday. In the late 1980s, Jon Pertwee and Colin Baker both played the Doctor at different times during the run of a play titled Doctor Who – The Ultimate Adventure. For two performances, while Pertwee was ill, David Banks (better known for playing Cybermen) played the Doctor. Other original plays have been staged as amateur productions, with other actors playing the Doctor, while Terry Nation wrote The Curse of the Daleks, a stage play mounted in the late 1960s, but without the Doctor. | Question: What was the name of the Doctor Who play from the 1980's?", ['Doctor Who – The Ultimate Adventure'], 'mrqa_squad-validation-7821'), tensor(-0.0576)), (('Context: David Perdue\'s Charles Dickens Page - David CopperfieldDavid Perdue\'s Charles Dickens Page - David Copperfield  David Copperfield  FAQ & Email  Dickens began writing an autobiography        in the late 1840s which he shared with his friend and future biographer, John Forster . Dickens found the        writing too painful and burned what he had written. He opted instead to        work his story into the fictional account of David Copperfield.  In the novel Dickens\' painful memories of being taken from school to work        at Warren\'s Blacking Factory while his father is in prison for debt are told through David\'s account        of Murdstone and Grinby\'s warehouse. The financial troubles of the Micawbers ,        with whom David was boarding at        the time, mirror Dickens\' parents, John and Elizabeth Dickens,        financial difficulties.  When David is asked by Mrs Micawber to take some of their treasured possessions to the pawn shop to help meet        their obligations, Dickens is recalling painful memories of having to pawn        off the very books he read and treasured as a child to ease his family\'s        financial woes.  On Dickens\' death Forster wrote The        Life of Charles Dickens , which is still the definitive biography        of Dickens, although many of the more negative aspects of Dickens life are        glossed over or missing altogether. Forster\'s biography included the autobiographical        fragment Dickens had given him. This was the first the public knew of Dickens\'        difficult childhood that had so heavily shaped his early work.  Oops...  Dickens originally introduced the        character of the dwarf, Miss Mowcher ,        as an aid to Steerforth\'s plan        to elope with Emily . Mrs. Jane Seymour        Hill, Dickens\' wife Catherine\'s chiropodist, recognized herself as the original for this character and threatened        a lawsuit . Dickens changed the character, in later monthly installments        of the novel, to an honest friend who abhors Steerforth\'s actions. She later        assists in the capture of Littimer .  Like Dickens, David teaches himself        shorthand and becomes a parliamentary reporter. David laments on the difficulties        encountered mastering this art:  "I bought an approved scheme of the noble art and mystery of stenography        (which cost me ten and sixpence); and plunged into a sea of perplexity that        brought me, in a few weeks, to the confines of distraction. The changes        that were rung upon dots, which in such a position meant such a thing, and        in such another position something else, entirely different; the wonderful        vagaries that were played by circles; the unaccountable consequences that        resulted from marks like flies\' legs; the tremendous effects of a curve        in a wrong place; not only troubled my waking hours, but reappeared before        me in my sleep."  Dickens hints at his feelings for politics when David says of his parliamentary        reporting:  "Britannia, that unfortunate female, is always before me, like a trussed        fowl: skewered through and through with office-pens, and bound hand and        foot with red tape. I am sufficiently behind the scenes to know the worth        of political life. I am quite an Infidel about it, and shall never be converted."  During the writing of David Copperfield        Dickens was actively involved in the day-to-day operation of Urania        Cottage , a home for homeless women, which he administered on behalf        of his friend, philanthropist Angela        Burdett Coutts . The home helped to separate homeless, and "fallen",        women from previous lifestyles, educate them in the execution of household        duties and self-discipline, and then help them emigrate to Australia to        begin new lives.  In David Copperfield, Dickens has several of the major characters emigrate        to Australia: the Micawbers , Mr.        Peggotty , Emily , Martha, and Mr.        Mell . Each of these characters are successful in beginning a new life        in the English colony.  Did Dickens visit the birthplace of David Copperfield? Visit the modern Blundeston where the debate rages and David Copperfield    lives on.  Dickens lampoons "the separate system" used at the | Question: What colour hair did Charles Dickens\' character David Copperfield have?', ['red'], 'mrqa_triviaqa-validation-1437'), tensor(-0.0678)), (('Context: Pi Day is an annual celebration of the mathematical constant π ( pi ) . Pi Day is observed on March 14 ( 3 / 14 in the month / day format ) since 3 , 1 , and 4 are the first three significant digits of π . In 2009 , the United States House of Representatives supported the designation of Pi Day . | Question: how long have we been celebrating pi day ?', ['2009'], 'mrqa_naturalquestions-validation-3028'), tensor(-0.0713)), (("Context: <Ol>  `` The Last Note of Freedom '' - David Coverdale   `` Deal for Life '' - John Waite   `` Break Through the Barrier '' - Tina Turner   `` Hearts in Trouble '' - Chicago   `` Trail of Broken Hearts '' - Cher   `` Knockin ' on Heaven 's Door '' - Guns N ' Roses   `` You Gotta Love Someone '' - Elton John   `` Show Me Heaven '' - Maria McKee   `` Thunderbox '' - Apollo Smile   `` Long Live the Night '' - Joan Jett & The Blackhearts   `` Gimme Some Lovin ' '' - Terry Reid ( Spencer Davis Group version appears in movie )  </Ol> | Question: who sings gim me some lovin in days of thunder ?", ['Spencer Davis Group'], 'mrqa_naturalquestions-validation-539'), tensor(-0.0732)), (('Context: Luther\'s other major works on the Jews were his 60,000-word treatise Von den Juden und Ihren Lügen (On the Jews and Their Lies), and Vom Schem Hamphoras und vom Geschlecht Christi (On the Holy Name and the Lineage of Christ), both published in 1543, three years before his death. Luther argued that the Jews were no longer the chosen people but "the devil\'s people", and referred to them with violent, vile language. Citing Deuteronomy 13, wherein Moses commands the killing of idolaters and the burning of their cities and property as an offering to God, Luther called for a "scharfe Barmherzigkeit" ("sharp mercy") against the Jews "to see whether we might save at least a few from the glowing flames." Luther advocated setting synagogues on fire, destroying Jewish prayerbooks, forbidding rabbis from preaching, seizing Jews\' property and money, and smashing up their homes, so that these "envenomed worms" would be forced into labour or expelled "for all time". In Robert Michael\'s view, Luther\'s words "We are at fault in not slaying them" amounted to a sanction for murder. "God\'s anger with them is so intense," Luther concluded, "that gentle mercy will only tend to make them worse, while sharp mercy will reform them but little. Therefore, in any case, away with them!" | Question: How near to his death was the work published?', ['three years before', 'three years before his death'], 'mrqa_squad-validation-2629'), tensor(-0.0810)), (('Context: Catherine Zeta Jones in "The Darling Buds of May" 1991 ...Catherine Zeta Jones in "The Darling Buds of May" 1991 - YouTube  Catherine Zeta Jones in "The Darling Buds of May" 1991  Want to watch this again later?  Sign in to add this video to a playlist.  Need to report the video?  Sign in to report inappropriate content.  The interactive transcript could not be loaded.  Loading...  Rating is available when the video has been rented.  This feature is not available right now. Please try again later.  Uploaded on Mar 31, 2007  Some clips of a young Catherine Zeta Jones as Mariette, in the tv series, "The Darling Buds of May".  "The Darling Buds of May" was a British television series first broadcast in 1991. It is set in an idyllic rural 1950s Kent, among a large, boisterous family. The show was very popular, and launched the acting career of Catherine Zeta Jones.  Category  Mariette - The Darling Buds of May (UK) Characters - ShareTVMariette - The Darling Buds of May (UK) Characters - ShareTV  Catherine Zeta-Jones (born September 25, 1969, in Swansea, West Glamorgan, Wales, UK, the daughter of ...  Character Bio  Mariette Larkin, the eldest Larkin child. Her name was created by  combining \'Marie\' and \'Antoinette\'. In the first episode, she is shown  to be the family beauty and slightly wild. However, she quickly marries  Charley and settles down with him for the rest of the series. She  appears to have inherited Pop\'s business sense, and at the end of the  series she and Charley buy and manage a local brewery. Her wedding  ceremony reveals her middle name as Jane.  Episode Screenshots  Profile: Catherine Zeta Jones - BBC NewsProfile: Catherine Zeta Jones - BBC News  BBC News  Close share panel  Image caption Catherine Zeta Jones became a household name after appearing in The Darling Buds of May  The showbusiness journey of Catherine Zeta Jones, a permanent fixture on the A-list for decades, has taken her from her home in Wales to the Hollywood hills.  Along the way, the 41-year-old has picked up an Oscar, a Tony, a CBE, countless plaudits and a smattering of bad reviews.  She married Hollywood actor Michael Douglas in a lavish New York ceremony in 2000, however the couple have now decided to take some time apart to "evaluate and work on their marriage".  Few could have predicted that a girl born to a sweet factory owner and a seamstress would go on to become one of the most famous actresses in the world.  At the age of 10, the Welsh star won a national talent contest singing a Shirley Bassey song before landing roles in various West End productions.  Image caption Zeta Jones started acting at a very young age  She got her big break at the age of 17, following a promotion from second understudy to the lead role in the hit musical 42nd Street.  But it was Zeta Jones\'s first major TV role, in the 1991 comedy drama The Darling Buds of May, that made her a household name.  Set in rural Kent in the 1950s, viewers fell in love with the Larkins, played by Sir David Jason and Pam Ferris.  Zeta Jones played their fresh-faced sweetheart daughter Mariette.  Squeaky-clean perception  Off camera, her personal life began to hit the headlines more than her acting ability.  Her relationship with Blue Peter presenter John Leslie was well documented. She also became involved with Simply Red star Mick Hucknall and Soldier Soldier actor Angus MacFadyen.  After leaving The Darling Buds of May, Zeta Jones turned her attention to film, but admitted finding it hard to shake off the public\'s squeaky-clean perception of her.  The thing that really upset me was the idea that I was this gold-digger  Catherine Zeta Jones answers the critics about her marriage to Michael Douglas  "I am afraid I have this image which is far | Question: What was the name of Catherine Zeta Jones character in The Darling Buds of May ?', ['mariette'], 'mrqa_triviaqa-validation-7018'), tensor(-0.0845)), (('Context: Laparoscopic Cholecystectomy Surgery - Schwartzapfel ...Laparoscopic Cholecystectomy Surgery - Schwartzapfel Lawyers P.C.  Laparoscopic Cholecystectomy Surgery  Laparoscopic Cholecystectomy Surgery  Cholecystectomy: Removal of the Gallbladder  The gallbladder is a small organ that is part of the human digestive system. It secretes bile, an enzyme that breaks down fats in food. Sometimes the gallbladder cannot work effectively, and instead of breaking down fats it begins to form lumps known as gallstones. When this happens, removal of the gallbladder – known as a cholecystectomy – may be necessary.  Traditional vs. laparoscopic cholecystectomy  Traditional cholecystectomy involved making a major incision and dislodging the liver in order to remove the gall bladder. This procedure was risky and required several days in the hospital as well as more than a month of recovery time.  The modern alternative, laparoscopic cholecystectomy, was developed in 1989. This procedure involves the insertion of a camera in a tube through the navel. Several small incisions are made, and the surgeon, guided by the camera, uses tiny instruments to cut the ducts and arteries that hold the gallbladder inside the body. When performed correctly, laparoscopic cholecystectomy poses less risk to the patient’s liver, reduces hospital stays, and cuts recovery time to less than a month. But laparoscopic cholecystectomy is an intricate and difficult procedure. Accidentally cutting or clipping the wrong duct or artery could have disastrous results for the patient.  Doctors performing laparoscopic cholecystectomy without proper experience  The best way for gallbladder patients to prevent complications from laparoscopic cholecystectomy is to choose a doctor who has performed many procedures. The National Institutes of Health became so concerned about the rate of patient injuries following laparoscopic cholecystectomy that a conference was convened to examine the issue. The conference found that training for laparoscopic cholecystectomy has been so irregular that an alarming number of doctors are not sufficiently competent to perform the procedure safely. Doctors with fewer than 25 procedures under their belts were more likely to identify the wrong duct or artery to cut and cause damage to the patient .  Laparoscopic Cholecystectomy complications can be life-threatening  Because the gallbladder is situated close to many other vital organs (most prominently the liver), it is paramount that the surgeon performs the surgery correctly without cutting, clipping, or puncturing any other organs. When complications occur, they can include:  Jaundice  Infection or inflammation of the bile ducts causing digestive and liver problems  Damage to the liver, the intestines, or other nearby organs  Damage to the common bile duct, causing bile to enter the bloodstream or the abdominal cavity, thereby poisoning the patient  If the hepatic (liver) system is disrupted, or if the common bile duct is damaged, the patient is placed in a life-threatening situation that can swiftly degenerate to liver failure and even death. Emergency surgery is often required to correct complications of a laparoscopic cholecystectomy, and, even when it is successful, the patient may be sentenced to a life with permanent pain and impaired digestive function.  Laparoscopic Cholecystectomy malpractice lawsuits  The consequences of a laparoscopic cholecystectomy gone wrong can be devastating. If you or a loved one has suffered due to a doctor’s negligence, you may have a case for medical malpractice. We can help. Please call our experienced team of medical malpractice lawyers at 1-888-575-6410 , or fill out our online contact form for a free case evaluation. We will fight for you!  Medical Malpractice  Cholecystectomy | Laparoscopic Surgery Mesa AZ | Gilbert ...Cholecystectomy |\xa0Laparoscopic Surgery Mesa AZ | Gilbert | Scottsdale  Mesa , AZ Gilbert, AZ Scottsdale, AZ  Laparoscopic Surgery  Laparoscopy is a minimally invasive surgical procedure used to diagnose and treat problems of the genital and pelvic areas. During this procedure, an endoscope (tube) with a camera on the end is inserted through a tiny incision to allow your doctor to closely examine the organs of the area. Surgical instruments can be inserted through additional incisions to treat any identified problems.  Technological advances have brought computers and laparoscopic instruments to the forefront of surgical approaches. This provides patients with a minimally invasive technique that can be utilized in a wide range of procedures.  A laparoscopic procedure may be performed for a number of | Question: Cholecystectomy is the surgical removal of which small organ of the body?', ['gallbladder', 'gallbladder fundus'], 'mrqa_triviaqa-validation-6800'), tensor(-0.0898)), (('Context: The collection of drawings includes over 10,000 British and 2,000 old master works, including works by: Dürer, Giovanni Benedetto Castiglione, Bernardo Buontalenti, Rembrandt, Antonio Verrio, Paul Sandby, John Russell, Angelica Kauffman, John Flaxman, Hugh Douglas Hamilton, Thomas Rowlandson, William Kilburn, Thomas Girtin, Jean Auguste Dominique Ingres, David Wilkie, John Martin, Samuel Palmer, Sir Edwin Henry Landseer, Lord Frederic Leighton, Sir Samuel Luke Fildes and Aubrey Beardsley. Modern British artists represented in the collection include: Paul Nash, Percy Wyndham Lewis, Eric Gill, Stanley Spencer, John Piper, Graham Sutherland, Lucian Freud and David Hockney. | Question: Approximately how many British drawings are included in the V&A collection?', ['10,000', 'over 10,000'], 'mrqa_squad-validation-5517'), tensor(-0.1036)), (('Context: 67th Tony Awards  The 67th Annual Tony Awards were held June 9, 2013, to recognize achievement in Broadway productions during the 2012–13 season.  The ceremony returned to Radio City Music Hall in New York City, after two years at Beacon Theatre, and was broadcast live on CBS television.  Neil Patrick Harris hosted for the third consecutive year, his fourth time as host.  Awards in four of the eight acting categories, (Best Actress in a Play, Best Actor in a Musical, Best Actress in a Musical, Best Featured Actor in a Play) were given to African-American performers.  Furthermore, it is the second time in Tony history that both directing prizes went to women.  Garry Hynes and Julie Taymor had previously won in 1998. " Kinky Boots" had a season best 13 nominations and 6 awards.  Cyndi Lauper, who wrote the score for "Kinky Boots", is the first solo female winner for Best Original Score.   67th Tony Awards  The 67th Annual Tony Awards were held June 9, 2013, to recognize achievement in Broadway productions during the 2012–13 season.  The ceremony returned to Radio City Music Hall in New York City, after two years at Beacon Theatre, and was broadcast live on CBS television.  Neil Patrick Harris hosted for the third consecutive year, his fourth time as host.  Awards in four of the eight acting categories, (Best Actress in a Play, Best Actor in a Musical, Best Actress in a Musical, Best Featured Actor in a Play) were given to African-American performers.  Furthermore, it is the second time in Tony history that both directing prizes went to women.  Garry Hynes and Julie Taymor had previously won in 1998. " Kinky Boots" had a season best 13 nominations and 6 awards.  Cyndi Lauper, who wrote the score for "Kinky Boots", is the first solo female winner for Best Original Score.   Julie Taymor  Julie Taymor (born December 15, 1952) is an American director of theater, opera and film.  She is best known for directing the stage musical "The Lion King", for which she became the first woman to win the Tony Award for directing a musical, in addition to a Tony Award for Original Costume Design.   Julie Taymor  Julie Taymor (born December 15, 1952) is an American director of theater, opera and film.  She is best known for directing the stage musical "The Lion King", for which she became the first woman to win the Tony Award for directing a musical, in addition to a Tony Award for Original Costume Design. | Question: In what year did the director of The Lion King win the Tony Awards?', ['1998'], 'mrqa_hotpotqa-validation-3241'), tensor(-0.1048)), (('Context: An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses . The same principle is used for funicular railways with two connected railway cars on inclined tracks , and for the elevators on the Eiffel Tower which counterbalance each other . Ski lifts are another example , where the gondolas move on a closed ( continuous ) pulley system up and down the mountain . The ski lift is similar to the counter-weighted elevator , but with a constraining force provided by the cable in the vertical dimension thereby achieving work in both the horizontal and vertical dimensions . Boat lifts are another type of counter-weighted elevator system approximating an Atwood machine . | Question: what is a real world application of an atwood machine ?', ['An elevator with a counterbalance'], 'mrqa_naturalquestions-validation-2730'), tensor(-0.1083)), (('Context: The Elizabeth Files » Mary Queen of Scots Marries Lord DarnleyThe Elizabeth Files » Mary Queen of Scots Marries Lord Darnley  Mary Queen of Scots Marries Lord Darnley  Posted By  claire on July 29, 2010  Mary Queen of Scots  On this day in history, the 29th July 1565, Elizabeth I’s nemesis, Mary Queen of Scots, married Henry Stuart, Lord Darnley, at Holyrood Palace (the Palace of Holyroodhouse), Edinburgh.  Let’s celebrate their union by giving some facts about the happy couple:-  The Bride – Mary Queen of Scots  Birth: Mary, Queen of Scots, also known as Mary Stuart, Mary Stewart and Marie Stuart, was born on the 8th December 1542 at Linlithgow Palace, Scotland.  Parents: Mary’s parents were James V of Scotland (son of James IV and Margaret Tudor) and Mary of Guise. She was James’s only child to survive and he actually died six days after her birth.  Queen of Scotland After her father’s death on the 14th December, Mary became Queen of Scotland and was crowned at Stirling in September 1543. As Mary was just an infant, the country was ruled by regents on her behalf.  Betrothals: On the 1st July 1543, at the age of 7 months, the Treaty of Greenwich, between England and Scotland, promised Mary in marriage to Henry VIII’s son, Prince Edward. When the pro-Catholic and pro-French Cardinal Beaton became powerful in Scotland and joined forces with the Earl of Arran, the treaty was rejected by the Scottish Parliament and trouble began between England and Scotland. The French came to the aid of Scotland and on the 7th July 1548 a marriage treaty between France and Scotland promised the 5 year old Mary in marriage to Henry II’s son, François, the Dauphin.  Upbringing: From the age of 5 until the age of 19 Mary lived in France. There she received an excellent education, learning French, Italian, Spanish, Greek, Latin and Scots, as well as music, needlework and poetry.  Return to Scotland: Francois (Francis II of France) died in 1560 and his mother, Catherine de’ Medici, took control of the country, as regent for her other son, Charles IX. Mary returned to Scotland on the 19th August 1561.  Marriages: 24th April 1558 to the Dauphin François at Notre Dame. he died in 1560. 29th July 1565 to Henry Stuart, Lord Darnley, he was assassinated in 1567. 15th May 1567 to James Hepburn, Lord Bothwell, he died in 1578.  Mary and her son James  Issue: James VI of Scotland (James I of England) by Lord Darnley, born on 19th June 1566. Mary miscarried twins (fathered by Bothwell) in July 1567.  Titles: Queen of Scotland from 14th December 1542 until 24th July 1567 when she was forced to abdicate.  At the death of Mary I of England in 1558, Henry II of France declared that François and Mary were King and Queen of England and Mary started bearing the royal arms of England.  On the death of her father-in-law, Henry II of France, on the 10th July 1559 Mary became Queen Consort of France, a position she held until 5th December 1560.  Appearance: Mary, Queen of Scots, was considered a beauty. She was tall (around 5′ 11) with auburn hair, hazel eyes and a heart-shaped face.  Scandal: In March 1566, Mary’s husband, Lord Darnley, and some friends murdered Mary’s private secretary, David Rizzio, in front of his pregnant wife. He was jealous of Mary’s friendship with Rizzio. Darnley became a real problem for Mary and the Scottish Lords and in February 1567 Darnley was killed in an explosion at Kirk O’Field. It is thought that James Hepburn, Lord Bothwell, supplied the gunpowder but he was acquitted of murder in April 1567. The famous “Casket Letters” implicated Mary in the murder of her husband, but these are generally believed to have been forgeries.  On the 24th April 1567, Mary was kidnapped by Bothwell (it is | Question: What was the name of the letters that in 1566 implicated Mary Queen of Scots in the murder of her second husband, Lord Darnley?', ['casket letters'], 'mrqa_triviaqa-validation-1935'), tensor(-0.1087)), (('Context: The Soulages collection of Italian and French Renaissance objects was acquired between 1859 and 1865, and includes several cassone. The John Jones Collection of French 18th-century art and furnishings was left to the museum in 1882, then valued at £250,000. One of the most important pieces in this collection is a marquetry commode by the ébéniste Jean Henri Riesener dated c1780. Other signed pieces of furniture in the collection include a bureau by Jean-François Oeben, a pair of pedestals with inlaid brass work by André Charles Boulle, a commode by Bernard Vanrisamburgh and a work-table by Martin Carlin. Other 18th-century ébénistes represented in the Museum collection include Adam Weisweiler, David Roentgen, Gilles Joubert & Pierre Langlois. In 1901, Sir George Donaldson donated several pieces of art Nouveau furniture to the museum, which he had acquired the previous year at the Paris Exposition Universelle. This was criticized at the time, with the result that the museum ceased to collect contemporary items and did not do so again until the 1960s. In 1986 the Lady Abingdon collection of French Empire furniture was bequeathed by Mrs T. R. P. Hole. | Question: What items comprise the John Jones Collection?', ['art and furnishings', 'French 18th-century art and furnishings'], 'mrqa_squad-validation-5622'), tensor(-0.1160)), (('Context: Nuclear power in space is the use of nuclear power in outer space , typically either small fission systems or radioactive decay for electricity or heat . Another use is for scientific observation , as in a Mössbauer spectrometer . One common type is a radioisotope thermoelectric generator , which has been used on many space probes and on manned lunar missions , and another is small fission reactors for Earth observation satellites such as the TOPAZ nuclear reactor . A radioisotope heater unit provides heat from radioactive decay of a material and can potentially produce heat for decades . | Question: how have scientists used nuclear power as a fuel for spacecraft ?', ['either small fission systems or radioactive decay for electricity or heat'], 'mrqa_naturalquestions-validation-3828'), tensor(-0.1267)), (('Context: Learn and talk about Family Fortunes, 1980 British ...Learn and talk about Family Fortunes, 1980 British television programme debuts, 1980s British television series, 1990s British television series, 2000s British television series  All Star Family Fortunes  Family Fortunes is a British television game show based on the American game show Family Feud . The programme ran on ITV from 6 January 1980 to 6 December 2002, before being revived by the same channel in 2006 under the title of All Star Family Fortunes . The game involves two families providing answers to \'everyday questions\' that were surveyed by 100 members of the British public before the show (e.g. \'Name something usually done in the dark\') to win cash prizes (and sometimes mystery prizes for giving a correct answer). The top answers to the surveys are displayed on a large electronic board, originally known as "Mr. Babbage", which famously sounds a wrong answer "Eh-uhh" sound effect and its accompanying X to signal the strike, as well as a "ding" for a right answer).  Contents  11 External links  Hosts and presentation[ edit ]  Family Fortunes was first hosted by comedian Bob Monkhouse (1980–83) then by singer and entertainer Max Bygraves (1983–85). After being rested for the whole of 1986, the show returned on 27 June 1987 with Les Dennis as presenter, and had a consistently successful run for the next 15 years. It was then moved out of peak time and became a daily daytime show, hosted by Andy Collins , but it only had a short run in this format before being axed.  In 2006, the series was revived with Vernon Kay as host, and was renamed All Star Family Fortunes , as each team consisted of a celebrity and four family members. The show was also transmitted back to peak time. Prize money goes to a charity of the celebrity\'s choice, and contestants being either celebrity families, or a group of actors famous for playing a fictional family. Several Christmas specials of All Star Family Fortunes have aired as well.  The most iconic aspects of the show are the large computer screen, named "Mr Babbage" by original host Bob Monkhouse and the famous computerised "Eh-uhh" sound used when wrong answers are given. Both were originally designed to appear high-tech but have since become fondly regarded for being quite the opposite (as compared to the original US Feud, which has used a video board since its 1999 revival). The computer screen name "Mr Babbage" was in recognition to the English mathematician, philosopher, inventor, and mechanical engineer who originated the concept of a programmable computer, Charles Babbage .  Format[ edit ]  Two family teams, each with five members, are asked to guess the results of surveys, in which 100 people would be asked open ended questions (e.g. "we asked 100 people to name something associated with the country Wales" or "we asked 100 people to name a breed of dog"). Although rarely acknowledged in the show, the 100 people surveyed are invariably audience members who have volunteered before the show.  Each round begins with a member of each family (in rotation, meaning all players do this at least once) approaching the podium. As the question is read, the first of the two nominees to hit a buzzer gives an answer. If this is not the most popular answer, the other nominee is asked. The family with the more popular answer then chooses whether to "play" the question, or "pass" control to the other family.  The host then passes down the line of the controlling team, asking for an answer from each. After each answer, the board reveals whether this answer featured. If not, the family is assessed a strike, and the family loses control of the board after accumulating three strikes (also referred as striking out) in the round. If a family manages to come up with all the survey answers (most commonly six in the early part of the show, reduced in number after the commercial break) before striking out, they win the amount in pounds of the total | Question: Who presented Family Fortunes in the two years between Bob Monkhouse and Les Dennis?', ['max bygraves'], 'mrqa_triviaqa-validation-287'), tensor(-0.1318)), (('Context: Sophomore - definition of sophomore by The Free DictionarySophomore - definition of sophomore by The Free Dictionary  Sophomore - definition of sophomore by The Free Dictionary  http://www.thefreedictionary.com/sophomore  Related to sophomore: Sophomore Album  soph·o·more  a. A second-year student in a US college.  b. A tenth-grade student in a US high school.  2. A person in the second year of carrying out an endeavor.  3. A three-year-old racehorse, usually in its second year of racing.  adj.  1. Of or relating to the second year of an endeavor, especially of attending a school or college.  2. Being the second in a series: a singer\'s sophomore album.  [Alteration (probably influenced by Greek sophos, wise, and mōros, stupid) of sophumer, from obsolete sophom, sophism, dialectic exercise, variant of sophism .]  sophomore  (Education) chiefly US and Canadian a second-year student at a secondary (high) school or college  adj  (of a book, recording, etc, by an artist) second: her sophomore album.  [C17: perhaps from earlier sophumer, from sophum, variant of sophism + -er1]  soph•o•more  (ˈsɒf əˌmɔr, -ˌmoʊr; ˈsɒf mɔr, -moʊr)  n.  a student in the second year at a high school, college, or university.  [1645–55; earlier sophumer, perhaps =sophum sophism + -er 1]  sophomore  A student in the second year of a college course, or a high-school student in the tenth grade.  ThesaurusAntonymsRelated WordsSynonymsLegend:  lowerclassman , underclassman - an undergraduate who is not yet a senior  Adj.  1.  sophomore - used of the second year in United States high school or college; "the sophomore class"; "his sophomore year"  intermediate - lying between two extremes in time or space or state; "going from sitting to standing without intermediate pushes with the hands"; "intermediate stages in a process"; "intermediate stops on the route"; "an intermediate range plane"  Translations  [ˈsɒfəmɔːʳ] N (US) → estudiante mf de segundo año GRADE  sophomore  n (US) → étudiant (e) m/f de deuxième année  modif  sophomore year → deuxième année f  Want to thank TFD for its existence? Tell a friend about us , add a link to this page, or visit the webmaster\'s page for free fun content .  Link to this page:  View in context  Perhaps the president of a corps notices that one of the membership who is no longer an exempt--that is a freshman-- has remained a sophomore some little time without volunteering to fight; some day, the president, instead of calling for volunteers, will APPOINT this sophomore to measure swords with a student of another corps; he is free to decline--everybody says so--there is no compulsion.  View in context  Then we\'ll be able to look as bored and sophisticated as any Sophomore of them all.  The "freshettes" stood about in detached groups of two or three, looking askance at each other; the "freshies," wiser in their day and generation, had banded themselves together on the big staircase of the entrance hall, where they were shouting out glees with all the vigor of youthful lungs, as a species of defiance to their traditional enemies, the Sophomores, a few of whom were prowling loftily about, looking properly disdainful of the "unlicked cubs" on the stairs.  against six Sophomores and a Freshman from the Gladiatorial College!  when he had slain all the sophomores and was dallying with the  Sophomore dictionary definition | sophomore definedSophomore dictionary definition | sophomore defined  Adjective  (not comparable)  (US) The second in a series , especially, the second of an artist\'s albums or the second of four years in a high school (tenth grade) or university.  The band\'s sophomore album built upon the success of their debut release, catapulting them to megastardom.  Noun  (plural sophomores | Question: A sophomore is a student in which year of a US college?', ['second'], 'mrqa_triviaqa-validation-2136'), tensor(-0.1740)), (('Context: Theatre Notables Sondheim, Ziegfield, Simon & More Amongst ...Theatre Notables Sondheim, Ziegfield, Simon & More Amongst \'New York City 400\'  Theatre Notables Sondheim, Ziegfield, Simon & More Amongst \'New York City 400\'  Tweet Share \xa0\xa0\xa0\xa0  The NYC400 is the first-ever list of New York City\'s ultimate movers and shakers since the City\'s founding-from politics, the arts, business, sports, science, and entertainment.  In commemoration of Henry Hudson\'s epic 1609 voyage into New York Harbor, the Museum is celebrating our City\'s 400th birthday by recognizing the people who have had the greatest impact and influence on the world\'s greatest city.  Our goal in creating the NYC 400 is to help educate the public about New York City\'s fascinating and dramatic history-its heritage of diversity, opportunity and perpetual transformation-by humanizing our amazing common story.  Berenice Abbott (1898-1991)  Photographer best known for her "Changing New York" project for the WPA (1935-1939), which documented New York City\'s evolving built environment.  Alvin Ailey (1931-1989)  Dancer/choreographer who founded Alvin Ailey American Dance Theater in 1958 to bring black dancers into the mainstream. Known for combining elements of ballet, jazz, and modern dance and drawing on elements of black culture.  Horatio Alger (1832-1899)  Writer, philanthropist, and Unitarian minister who supported such charities as the Newsboys\' Lodging House and wrote moralistic novels about street boys who eventually became wealthy through luck and hard work.  Woody Allen (1935- )  Comedian, filmmaker, writer, and musician best known for chronicling New Yorkers\' neuroses and foibles in films such as Annie Hall (1977) and Manhattan (1979).  Othmar H. Ammann (1879-1965)  Engineer of bridges linking Manhattan to the outer boroughs and beyond, including the George Washington Bridge (1931), the Triborough (now Robert F. Kennedy) Bridge (1936), and the Verrazzano Narrows Bridge (1964).  Diane Arbus (1923-1971)  Photographer whose 1960s images of eccentrics and other often ignored people explored issues of identity and appearance. Hubert\'s Dime Museum and Flea Circus, a Times Square freak show, was one of her favorite places to photograph.  Harold Arlen (1905-1986)  Singer, pianist, and composer of jazz and blues songs and ballads for Broadway musicals and shows at Harlem\'s Cotton Club. Best known for composing such songs as "Get Happy" (1929) and "Over The Rainbow" (1939).  Louis Armstrong (1901-1971)  Trumpeter and singer who came to prominence in New York City as a major jazz influence in the 1920s. Known for hits like "What a Wonderful World" (1967) and his "Hot Fives" and "Hot Seven" recordings made between 1925-29.  Chester A. Arthur (1829-1886)  As a New York lawyer interested in civil rights cases, he represented a black woman in a successful suit against the Third Avenue Railway (1855), which helped end segregation on the city\'s passenger railroads. As President, he was the first since George Washington to take the oath of office in NYC.  Brooke Astor (1902-2007)  Socialite and philanthropist who from 1959-97 gave away the corpus of her late husband\'s foundation to New York City institutions, leading others in philanthropy. Known for her visits to each and every one of her grantees.  John Astor (1763-1848)  Immigrant fur trader who amassed a fortune in real estate and who, by the 1840s, was the country\'s wealthiest man. Astor Place and Astoria are named for him.  Brooks Atkinson (1894-1984)  Theater critic for The New York Times from 1925-60 and an influential voice in an era when American drama emerged as a serious art form.  Louis Auchincloss (1917- )  Attorney, essayist, and writer of historic fiction known for closely observed portraits of old, patrician New York society in such novels as Portrait in Brownstone (1962 | Question: American businessman and philanthropist Solomon Guggenheim (1861-1949) established a famous eponymous international network of?', ['museum', 'museums'], 'mrqa_triviaqa-validation-2530'), tensor(-0.1903)), (('Context: Shaqtin\' a Fool  Shaqtin\' a Fool is a weekly segment from the television show "Inside the NBA", the postgame show of "NBA on TNT" following the conclusion of National Basketball Association (NBA) games airing on cable TV channel TNT.  It first aired during the 2011–12 NBA season, when retired NBA All-Star Shaquille O\'Neal voiced it upon joining the show and was created by Turner Sports producer Mike Goldfarb.  It highlights humorous and uncommon basketball plays that have occurred during NBA games in the past week.  O\'Neal is the host and presenter, while the other analysts in studio react and provide commentary.  Most often, those have been fellow "Inside" regulars Ernie Johnson, Kenny Smith, and Charles Barkley, but other "Inside" hosts have also participated, including Chris Webber, Grant Hill, Steve Smith and Matt Winer.   Matt Winer  Matthew Ward Winer (born January 10, 1969) is an American television personality who is currently working for Turner Sports.  Winer also was known for working eight years at ESPN.   Matt Winer  Matthew Ward Winer (born January 10, 1969) is an American television personality who is currently working for Turner Sports.  Winer also was known for working eight years at ESPN. | Question: Shaqtin\' a Fool has included which television personality who worked for 8 yearst at ESPN?', ['Matthew Ward Winer'], 'mrqa_hotpotqa-validation-4367'), tensor(-0.2576)), (('Context: Important production centres today are the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ) , the Yakima ( Washington ) and Willamette ( Oregon ) valleys , and western Canyon County , Idaho ( including the communities of Parma , Wilder , Greenleaf , and Notus ) . The principal production centres in the UK are in Kent ( which produces Kent Goldings hops ) , Herefordshire , and Worcestershire . Essentially all of the harvested hops are used in beer making . | Question: where do they grow hops in the us ?', ['Washington', 'Oregon', 'Idaho'], 'mrqa_naturalquestions-validation-2248'), tensor(-0.2798)), (('Context: Walking in the Air  "Walking in the Air" is a song written by Howard Blake for the 1982 animated film of Raymond Briggs\' 1978 children\'s book "The Snowman".  The song forms the centrepiece of "The Snowman", which has become a seasonal favorite on British and Finnish television.  The story relates the fleeting adventures of a young boy and a snowman who has come to life.  In the second part of the story, the boy and the snowman fly to the North Pole.  "Walking in the Air" is the theme for the journey.  They attend a party of snowmen, at which the boy seems to be the only human until they meet Father Christmas with his reindeer, and the boy is given a scarf with a snowman pattern.  In the film, the song was performed by St Paul\'s Cathedral choirboy Peter Auty, and reissued in 1985 (on Stiff Records) and 1987.   Walking in the Air  "Walking in the Air" is a song written by Howard Blake for the 1982 animated film of Raymond Briggs\' 1978 children\'s book "The Snowman".  The song forms the centrepiece of "The Snowman", which has become a seasonal favorite on British and Finnish television.  The story relates the fleeting adventures of a young boy and a snowman who has come to life.  In the second part of the story, the boy and the snowman fly to the North Pole.  "Walking in the Air" is the theme for the journey.  They attend a party of snowmen, at which the boy seems to be the only human until they meet Father Christmas with his reindeer, and the boy is given a scarf with a snowman pattern.  In the film, the song was performed by St Paul\'s Cathedral choirboy Peter Auty, and reissued in 1985 (on Stiff Records) and 1987.   Peter Auty  Peter Robert Auty (born 4 November 1969) is an English operatic tenor who has worked with most of the major opera companies in Britain and a number of companies in continental Europe. | Question: An English operatic tenor performed "Walking in the Air" for which 1982 animated film?', ['The Snowman'], 'mrqa_hotpotqa-validation-1968'), tensor(-0.3109)), (('Context: Huguenot immigrants did not disperse or settle in different parts of the country, but rather, formed three societies or congregations; one in the city of New York, another 21 miles north of New York in a town which they named New Rochelle, and a third further upstate in New Paltz. The "Huguenot Street Historic District" in New Paltz has been designated a National Historic Landmark site and contains the oldest street in the United States of America. A small group of Huguenots also settled on the south shore of Staten Island along the New York Harbor, for which the current neighborhood of Huguenot was named. | Question: In what borough is there a neighborhood called Huguenot?', ['Staten Island'], 'mrqa_squad-validation-3058'), tensor(-0.4187)), (('Context: Bodhi Natural Health Products | Bodhi Natural ProductsBodhi Natural Health Products | Bodhi Natural Products  Stockists  Welcome  “It is my wish that by using these products you can enjoy life to the fullest by having good health, hopefully relieve suffering, and of course protecting our beautiful planet by using only natural substances. All Bodhi products come from nature, and are organic”\xa0 Julie Herbison  Bodhi originated as Body Mind Balancing which started off as my business as a Natural Health Practitioner. Over time my products became more and more popular so the name Bodhi came about to keep it short and simple. Bodhi comes from the Bodhi Tree which is a type of fig tree. The Bodhi tree has played an important part in human history featuring strongly in religious history and mythology in various parts of the world.  The ficus religiosa, Bodhi tree, has large heart shaped leaves. It is one of the most sacred trees in India, Sri Lanka and Nepal, where it is venerated by both Hindus and Buddhists. The tree under which Siddhartha Gautama gained enlightenment over 2,600 years ago still grows today in North East India.  In Hindu religion, Vishnu is said to have been born under a Bodhi tree and is often depicted sitting on its heart-shaped leaves. The Bodhi tree is often planted in the grounds of temples and, of all the sacred trees of India, it is the most widely worshiped.  The Bodhi tree in Sri Lanka is located in Anuradhapura and is said to be the oldest tree in the world with a known planting date. This fig tree is said to have grown from a branch taken from the original Bodhi tree in India under which Buddha gained enlightenment.  For Homeopathic and Body Mind consultations please call  Julie Herbison  Bodhi Tree, Bodh Gaya - Sacred SitesBodhi Tree, Bodh Gaya  Bodhi Tree, Bodh Gaya  Buddhist Monks at Bodhi Tree (The site of Buddha\'s enlightenment) \xa0 \xa0\xa0  Bodh Gaya, located 100 km (62 mi) south of Patna in the Indian state of Bihar, is the most venerated sacred place in Buddhism. It is the place where Prince Siddhartha Guatama, while meditating beneath the Bodhi Tree, attained enlightenment and became the Buddha.  Traditional accounts say that, in the early years of the 4th century BC, Siddhartha Gautama saw the suffering of the world and wanted to be free from it. As a young man, following the ancient traditions of Hinduism, he sought out spiritual teachers. Inquiring of their knowledge, he diligently practiced various yogas and meditations. Seven years passed, the last three in extreme asceticism, yet still he had not achieved his goal of enlightenment.  Impression of Buddha feet, Bodh Gaya \xa0\xa0\xa0  Siddhartha then journeyed toward the ancient sacred forests of Uruvela (modern Gaya in Bihar, in north India) with the intention of finally and completely realizing the infinite. Guided by visionary dreams and following in the footsteps of the Buddhas of three previous ages, Krakucchanda, Kanakamuni and Kasyapa (who had each attained enlightenment at the site) Siddhartha sat beneath the Bodhi Tree. Touching the earth, thereby calling it to witness the countless lifetimes of virtue that had led him to this place of enlightenment, he resolved not to rise again until enlightenment was attained.  "Here on this seat my body may shrivel up, my skin, my bones, my flesh may dissolve, but my body will not move from this seat until I have attained Enlightenment, so difficult to obtain in the course of great periods of time".  As Siddhartha sat in deep meditation beneath the Bodhi Tree, Mara, the Dark Lord of Death, came to distract him from his endeavor. When the earth shook, confirming the truth of Gautama\'s words, Mara unleashed his army of demons. In the epic battle that ensued, Siddhartha\'s wisdom broke through Mara’s illusions. The power of his compassion transformed the demons\' weapons into flowers and Mara and all his forces fled. Three days and nights passed and Siddhartha’s intention was realized. He became the Buddha, meaning the ‘Enlightened One’.  The Mahabodhi Temple, Bodh Gaya, India \xa0 \xa0\xa0  The Buddha then spent the | Question: Who is said to have gained enlightenment sitting under the Bodhi Tree?', ['buddha', 'enlightened one'], 'mrqa_triviaqa-validation-3915'), tensor(-0.4602)), (('Context: The Iroquois was originally designated HU - 1 , hence the Huey nickname , which has remained in common use , despite the official redesignation to UH - 1 in 1962 . The UH - 1 first saw service in combat operations during the Vietnam War , with around 7,000 helicopters deployed . The Bell 204 and 205 are Iroquois versions developed for the civil market . | Question: where did the name huey helicopter come from ?', ['originally designated HU - 1'], 'mrqa_naturalquestions-validation-5144'), tensor(-0.5648)), (('Context: The Young Ones (TV series)The Young Ones is a British sitcom, broadcast in the United Kingdom from 1982 to 1984 in two six-part series. Shown on BBC2, it featured anarchic, offbeat humour which helped bring alternative comedy to television in the 1980s and made household names of its writers and performers. In 1985, it was shown on MTV, one of the first non-music television shows on the fledgling channel. In a 2004 poll, it ranked at number 31 in the BBC\'s list of Britain\'s Best Sitcoms.  History  The series originated on London\'s comedy club circuit in the early 1980s, where most of the cast had gained popularity at The Comedy Store. Alexei Sayle was the prominent act, drawing attention as the manic, aggressive compere. Adrian Edmondson and Rik Mayall worked as the double act 20th Century Coyote, which later became The Dangerous Brothers. Nigel Planer was in a double act with Peter Richardson called "The Outer Limits".   As The Comedy Store became popular, Sayle, 20th Century Coyote, and The Outer Limits, with French and Saunders and Arnold Brown, set up their own club called The Comic Strip in the Raymond Revuebar club in Soho.  The Comic Strip became one of the most popular comedy venues in London, and came to the attention of Jeremy Isaacs of Channel 4. Peter Richardson then negotiated a deal for six self-contained half-hour films, using the group as comedy actors rather than stand-up performers. In response, the BBC began negotiations with Edmondson, Mayall, Richardson, Planer and Sayle to star in a sitcom in a similar style. Paul Jackson was installed as a producer.  Richardson\'s project, The Comic Strip Presents..., aired on Channel 4\'s opening night on 2 November 1982, with The Young Ones following a week later on BBC2.  The series was written by Mayall, his then-girlfriend Lise Mayer, and Ben Elton (who had attended the University of Manchester with Mayall and Edmondson). Richardson was originally set to play Mike, but clashed with Jackson. He was replaced by Christopher Ryan, the only member of the group who was not a stand-up comedian.  The show was voted number 31 in the BBC\'s Best Sitcom poll in 2004.  Setting  The main characters were four undergraduate students who were sharing a house: aggressive punk Vyvyan (Adrian Edmondson), conceited wannabe anarchist Rick (Rik Mayall), oppressed paranoid hippie Neil (Nigel Planer), and the suave, charming Mike (Christopher Ryan). It also featured Alexei Sayle, who played various members of the Balowski family—most often Jerzei Balowski, the quartet\'s landlord—and occasional independent characters, such as the train driver in "Bambi" and the Mussolini-lookalike Police Chief in "Cash".  The show combined traditional sitcom style with violent slapstick, non-sequitur plot turns, and surrealism. These older styles were mixed with the working and lower-middle class attitudes of the growing 1980s alternative comedy boom, in which all the principal performers except Ryan had been involved. Every episode except one featured a live performance by a band, including Madness, Motörhead, and The Damned.  This was a device used to qualify the series for a larger budget, as "variety" shows attracted higher fees than "comedy".  Synopsis  Stories were set in a squalid house where the students lived during their time at Scumbag College. It can be classified as a comedy of manners.  When it was first broadcast, the show gained attention for its violent slapstick, which Mayall and Edmondson had been using in 20th Century Coyote for some time. The show also featured surreal elements, such as puppets playing talking animals or objects. Confusion was added with lengthy cutaways with no relation to the main plot.  Throughout the series, the fourth wall was frequently broken for comedic effect by all characters at various parts of the show. The wall was usually broken as either a punchline to a joke or to make a plot point more obvious. On several occasions, Alexei Sayle broke both the fourth wall and character | Question: "Who played Vyvyan, a psychotic punk metal medical student with orange-dyed & spiked hair and four metal stars embedded in his forehead, in the UK TV series ""The Young Ones""?" ?', ['adrian edmondson'], 'mrqa_triviaqa-validation-3339'), tensor(-0.6134)), (('Context: Honda Ballade  The Honda Ballade is a subcompact automobile built by Honda of Japan.  It began as a four-door higher equipment content version of the Civic in 1980.  The Ballade was developed at the same time the Honda Vigor appeared, which was a higher content Honda Accord.  The Ballade was sold exclusively in Japan at "Honda Verno" dealerships alongside the Vigor, Prelude, CR-X, and Quint.  In the UK it was launched at the same time as the very similar Triumph Acclaim with which it shared a Honda built engine.   Honda Ballade  The Honda Ballade is a subcompact automobile built by Honda of Japan.  It began as a four-door higher equipment content version of the Civic in 1980.  The Ballade was developed at the same time the Honda Vigor appeared, which was a higher content Honda Accord.  The Ballade was sold exclusively in Japan at "Honda Verno" dealerships alongside the Vigor, Prelude, CR-X, and Quint.  In the UK it was launched at the same time as the very similar Triumph Acclaim with which it shared a Honda built engine.   Honda CR-X  The Honda CR-X, originally launched as the Honda Ballade Sports CR-X in Japan, is a front-wheel-drive sport compact car manufactured by Honda between 1983 and 1991.  It was replaced by the Honda CR-X del Sol for the 1992 model year.  Although there are many supposed definitions for the acronym CR-X, the most widely accepted are "Civic rally cross", and "Civic renaissance model X".   Honda CR-X  The Honda CR-X, originally launched as the Honda Ballade Sports CR-X in Japan, is a front-wheel-drive sport compact car manufactured by Honda between 1983 and 1991.  It was replaced by the Honda CR-X del Sol for the 1992 model year.  Although there are many supposed definitions for the acronym CR-X, the most widely accepted are "Civic rally cross", and "Civic renaissance model X". | Question: The "civic rally cross" was sold alongside what other model sold exclusively in Japan at Honda Verno dealerships?', ['Honda Ballade'], 'mrqa_hotpotqa-validation-400'), tensor(-0.6272)), (('Context: Historically , computer language syntax was restricted to the ASCII character set ; in the absence of the × character , U + 002A * Asterisk became the de facto standard notation of the multiplication operator in computing . This selection is still reflected in the standard numeric keypad , where the arithmetic operations of addition , subtraction , multiplication , and division are represented by the + , - , * , and / keys , respectively . | Question: what is the multiplication sign on the computer ?', ['*'], 'mrqa_naturalquestions-validation-10364'), tensor(-0.6608)), (('Context: In the early years the College trained many Puritan ministers.[citation needed] (A 1643 publication said the school\'s purpose was "to advance learning and perpetuate it to posterity, dreading to leave an illiterate ministry to the churches when our present ministers shall lie in the dust".) It offered a classic curriculum on the English university model—\u200b\u200bmany leaders in the colony had attended the University of Cambridge—\u200b\u200bbut conformed Puritanism. It was never affiliated with any particular denomination, but many of its earliest graduates went on to become clergymen in Congregational and Unitarian churches. | Question: Was the school officially associated with any denomination?', ['It was never affiliated with any particular denomination', 'never'], 'mrqa_squad-validation-7149'), tensor(-0.7518)), (("Context: For a phylum with relatively few species, ctenophores have a wide range of body plans. Coastal species need to be tough enough to withstand waves and swirling sediment particles, while some oceanic species are so fragile that it is very difficult to capture them intact for study. In addition oceanic species do not preserve well, and are known mainly from photographs and from observers' notes. Hence most attention has until recently concentrated on three coastal genera – Pleurobrachia, Beroe and Mnemiopsis. At least two textbooks base their descriptions of ctenophores on the cydippid Pleurobrachia. | Question: Which group of ctenophore are are hardest to study?", ['oceanic species'], 'mrqa_squad-validation-4456'), tensor(-0.8050)), (('Context: What is a Mulligan in Golf? - About.com SportsWhat is a Mulligan in Golf?  What Is a \'Mulligan\' in Golf?  What Is a \'Mulligan\' in Golf?  This golfer looks like he needed a mulligan.\xa0 John Cumming/Photodisc/Getty Images  By Brent Kelley  Updated February 05, 2016.  A mulligan, most simply put, is a "do-over" in golf. Hit a bad shot? Take a mulligan and replay that stroke . Drop a ball on the spot from which you just played, and re-play. The first (bad) shot is not counted.  Are Mulligans \'Legal\'?  No. There is never a time, when playing under the\xa0 Rules of Golf , that a mulligan is "legal." Mulligans are not allowed under the rules.  But Mulligans Are Very Popular  But just because mulligans aren\'t "legal" doesn\'t mean their use isn\'t common and popular among recreational golfers. Many amateurs and recreational golfers - players just out to have a fun round with buddies, as opposed to serious golfers looking for competition - are lax in observing the rules anyway.  So mulligans are most often employed during friendly rounds by golf buddies; or during charity or playday tournaments where mulligans are sometimes sold. If mulligans are for sale at a charity tournament, that means the golfer can buy, say, three mulligans for a set price each.  continue reading below our video  What Are Mulligans?  The sale of mulligans is sometimes used as an additional fund-raiser at charitable events.  Common Ways of Using Mulligans  Do all golfers use mulligans in the same way? No - whatever a group of golfers agrees upon is what counts (unless you are using mulligans in something like a charity tournament or association outing setting - then do what the organizers tell you).  How many mulligans you get to use during a round, what types of shots you can use them for, and so on, are things that differ from golfer to golfer. So if you\'re playing with a new group and mulligans are in effect, you need to clarify what is allowed.  Here are some common ways that mulligans are used:  Some golfers limit the use of mulligans to the first tee only, or to the first and 10th tees only.  Some golfers use one mulligan per nine holes, but anywhere on each nine.  It\'s most common for mulligans to be used only off the tee , i.e., you can only use a mulligan to replay a drive. However, some groups allow mulligans from the fairway , too.  Less common is allowing mulligans from the rough or out of hazards, but some golfers even do that.  It is rarer still - rarely seen, in fact - for mulligans to be used on the putting green .  And some groups allow mulligans from just about anywhere on the golf course, but set a limit - say, three mulligans per round, or nine, or 18.  Clearly, there are many different ways that golfers use mulligans. If you have a regular group of friends you play with, and your group allows mulligans, you probably long ago settled into your own "rules" about using them.  Note that mulligans are common in the United States, but rarely seen in the UK, for example. As with all localized golf customs, formats and betting games , when playing with golfers you don\'t know clear up the ground rules before play starts to avoid possible confusion later.  And again, please note: If you are playing in a tournament, handicap round or other setting in which the Rules of Golf are strictly followed, you cannot play mulligans.  Why Is It Called a \'Mulligan\'?  Good question! And the fact is, nobody knows for sure how a do-over in golf came to be called a mulligan. There are multiple theories, however, and we go over some of them in our FAQ on the topic:  Why are mulligans called that?  Other Terms/Uses  Mulligans | Question: In which sport may a \'Mulligan\' be awarded?', ['golf', 'golfer'], 'mrqa_triviaqa-validation-5972'), tensor(-0.9088)), (('Context: The Games are expected to take place between 27 July and 7 August 2022 . The city was announced as the host at a press conference at the Arena Academy in Birmingham on 21 December 2017 . | Question: when are the next commonwealth games going to be held ?', ['2022'], 'mrqa_naturalquestions-validation-5647'), tensor(-1.0127)), (('Context: The Internet protocol suite ( TCP / IP ) was developed by Robert E. Kahn and Vint Cerf in the 1970s and became the standard networking protocol on the ARPANET , incorporating concepts from the French CYCLADES project directed by Louis Pouzin . In the early 1980s the NSF funded the establishment for national supercomputing centers at several universities , and provided interconnectivity in 1986 with the NSFNET project , which also created network access to the supercomputer sites in the United States from research and education organizations . Commercial Internet service providers ( ISPs ) began to emerge in the very late 1980s . The ARPANET was decommissioned in 1990 . Limited private connections to parts of the Internet by officially commercial entities emerged in several American cities by late 1989 and 1990 , and the NSFNET was decommissioned in 1995 , removing the last restrictions on the use of the Internet to carry commercial traffic . | Question: when was the internet introduced to the public ?', ['in the very late 1980s'], 'mrqa_naturalquestions-validation-683'), tensor(-2.3747)), (('Context: Although partly functional , weather vanes are generally decorative , often featuring the traditional cockerel design with letters indicating the points of the compass . Other common motifs include ships , arrows and horses . Not all weather vanes have pointers . When the wind is sufficiently strong , the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing . | Question: where does the arrow of a wind vane point ?', ['the direction from which the wind is blowing'], 'mrqa_naturalquestions-validation-9688'), tensor(-2.4483))]
09/23/2021 13:08:29 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-2191', 'mrqa_triviaqa-validation-5362', 'mrqa_triviaqa-validation-1792', 'mrqa_naturalquestions-validation-1364', 'mrqa_squad-validation-8542', 'mrqa_naturalquestions-validation-5502', 'mrqa_naturalquestions-validation-2753', 'mrqa_naturalquestions-validation-6896', 'mrqa_triviaqa-validation-3901', 'mrqa_triviaqa-validation-6683', 'mrqa_squad-validation-10322', 'mrqa_triviaqa-validation-6351', 'mrqa_squad-validation-7746', 'mrqa_triviaqa-validation-4856', 'mrqa_hotpotqa-validation-1626', 'mrqa_squad-validation-10015', 'mrqa_hotpotqa-validation-3632', 'mrqa_squad-validation-6303', 'mrqa_naturalquestions-validation-1202', 'mrqa_squad-validation-2069', 'mrqa_squad-validation-392', 'mrqa_naturalquestions-validation-5818', 'mrqa_triviaqa-validation-2210', 'mrqa_squad-validation-4253', 'mrqa_naturalquestions-validation-5180', 'mrqa_triviaqa-validation-2722', 'mrqa_hotpotqa-validation-5802', 'mrqa_triviaqa-validation-365', 'mrqa_hotpotqa-validation-1376', 'mrqa_triviaqa-validation-2327', 'mrqa_squad-validation-4185', 'mrqa_naturalquestions-validation-10680']
09/23/2021 13:08:29 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:08:29 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 4
09/23/2021 13:08:42 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:08:42 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 4
09/23/2021 13:08:44 - INFO - __main__ - After Error Fixing: 0.9375
09/23/2021 13:08:44 - INFO - __main__ - Instant Fixing Rate: 1.0
09/23/2021 13:08:44 - INFO - __main__ - Instant Retention Rate: 0.6666666655555555
09/23/2021 13:08:46 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_005.pt.
09/23/2021 13:08:46 - INFO - __main__ - Saving the current error examples (len=26) to the memory.
09/23/2021 13:08:46 - INFO - __main__ - Current memory size: 118.
09/23/2021 13:08:46 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:08:46 - INFO - __main__ - Finished.
09/23/2021 13:08:46 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:08:46 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:08:46 - INFO - __main__ - Evaluating to get errors .... Timecode: 5
09/23/2021 13:08:49 - INFO - __main__ - Before Error Fixing: 0.125
09/23/2021 13:08:49 - INFO - __main__ - Found 28 errors.
09/23/2021 13:08:49 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:08:49 - INFO - __main__ - Current memory size: 144.
09/23/2021 13:08:49 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=144;
09/23/2021 13:08:56 - INFO - __main__ - interference_scores=[(("Context: He produced artificial lightning, with discharges consisting of millions of volts and up to 135 feet long. Thunder from the released energy was heard 15 miles away in Cripple Creek, Colorado. People walking along the street observed sparks jumping between their feet and the ground. Sparks sprang from water line taps when touched. Light bulbs within 100 feet of the lab glowed even when turned off. Horses in a livery stable bolted from their stalls after receiving shocks through their metal shoes. Butterflies were electrified, swirling in circles with blue halos of St. Elmo's fire around their wings. | Question: What happened to nearby light bulbs?", ['glowed even when turned off', 'glowed'], 'mrqa_squad-validation-1516'), tensor(1.9666)), (('Context: The bulk of Huguenot émigrés relocated to Protestant European nations such as England, Wales, Scotland, Denmark, Sweden, Switzerland, the Dutch Republic, the Electorate of Brandenburg and Electorate of the Palatinate in the Holy Roman Empire, the Duchy of Prussia, the Channel Islands, and Ireland. They also spread beyond Europe to the Dutch Cape Colony in South Africa, the Dutch East Indies, the Caribbean, and several of the English colonies of North America, and Quebec, where they were accepted and allowed to worship freely. | Question: What area in modern-day Canada received Huguenot immigrants?', ['Quebec'], 'mrqa_squad-validation-3021'), tensor(1.3487)), (('Context: MS Kronprins Harald  Several motor ships have borne the name Kronprins Harald, after Harald V of Norway:   Harald V of Norway  Harald V (] ; born 21 February 1937) is the King of Norway, having ascended the throne following the death of his father on 17 January 1991. | Question: What kind of ships have been named after the King of Norway who ascended the throne in 1991?', ['motor ships'], 'mrqa_hotpotqa-validation-5699'), tensor(1.3257)), (('Context: The Games are expected to take place between 27 July and 7 August 2022 . The city was announced as the host at a press conference at the Arena Academy in Birmingham on 21 December 2017 . | Question: when are the next commonwealth games going to be held ?', ['2022'], 'mrqa_naturalquestions-validation-5647'), tensor(1.1612)), (('Context: Luther\'s other major works on the Jews were his 60,000-word treatise Von den Juden und Ihren Lügen (On the Jews and Their Lies), and Vom Schem Hamphoras und vom Geschlecht Christi (On the Holy Name and the Lineage of Christ), both published in 1543, three years before his death. Luther argued that the Jews were no longer the chosen people but "the devil\'s people", and referred to them with violent, vile language. Citing Deuteronomy 13, wherein Moses commands the killing of idolaters and the burning of their cities and property as an offering to God, Luther called for a "scharfe Barmherzigkeit" ("sharp mercy") against the Jews "to see whether we might save at least a few from the glowing flames." Luther advocated setting synagogues on fire, destroying Jewish prayerbooks, forbidding rabbis from preaching, seizing Jews\' property and money, and smashing up their homes, so that these "envenomed worms" would be forced into labour or expelled "for all time". In Robert Michael\'s view, Luther\'s words "We are at fault in not slaying them" amounted to a sanction for murder. "God\'s anger with them is so intense," Luther concluded, "that gentle mercy will only tend to make them worse, while sharp mercy will reform them but little. Therefore, in any case, away with them!" | Question: How near to his death was the work published?', ['three years before', 'three years before his death'], 'mrqa_squad-validation-2629'), tensor(1.0483)), (('Context: CarmenCarmen (;) is an opera in four acts by French composer Georges Bizet. The libretto was written by Henri Meilhac and Ludovic Halévy, based on a novella of the same title by Prosper Mérimée. The opera was first performed at the Opéra-Comique in Paris on 3 March 1875, where  its breaking of conventions   shocked and scandalized its first audiences. Bizet died suddenly after the 33rd performance, unaware  that the work would achieve   international acclaim within the following ten years.  Carmen has since become one of the most popular and frequently performed operas in the classical canon;  the "Habanera" from act 1 and the "Toreador Song" from act 2 are among the best known of all operatic arias.  The opera is written in the genre of opéra comique with musical numbers separated by dialogue. It is set in southern Spain and tells the story of the downfall of Don José, a naïve soldier who is seduced by the wiles of the fiery gypsy Carmen. José abandons his childhood sweetheart and deserts from his military duties, yet loses Carmen\'s love to the glamorous toreador Escamillo, after which José kills her in a jealous rage. The depictions of proletarian life, immorality, and lawlessness, and the tragic death of the main character on stage, broke new ground in French opera and were highly controversial.  After the premiere, most reviews were critical, and the French public was generally indifferent. Carmen initially gained its reputation through a series of productions outside France, and was not revived in Paris until 1883; thereafter it rapidly acquired popularity at home and abroad. Later commentators have asserted that Carmen forms the bridge between the tradition of opéra comique and the realism or verismo that characterised late 19th-century Italian opera.  The music of Carmen has since been widely acclaimed for brilliance of melody, harmony, atmosphere, and orchestration, and for the skill with which Bizet musically represented the emotions and suffering of his characters. After the composer\'s death, the score was subject to significant amendment, including the introduction of recitative in place of the original dialogue; there is no standard edition of the opera, and different views exist as to what versions best express Bizet\'s intentions. The opera has been recorded many times since the first acoustical recording in 1908, and the story has been the subject of many screen and stage adaptations.  Background  In the Paris of the 1860s, despite being a Prix de Rome laureate, Bizet struggled to get his stage works performed. The capital\'s two main state-funded opera houses—the Opéra and the Opéra-Comique—followed conservative repertoires that restricted opportunities for young native talent.  Bizet\'s professional relationship with Léon Carvalho, manager of the independent Théâtre Lyrique company, enabled him to bring to the stage two full-scale operas, Les pêcheurs de perles (1863) and La jolie fille de Perth (1867), but neither enjoyed much public success.    When artistic life in Paris resumed after the Franco-Prussian War of 1870–71, Bizet found wider opportunities for the performance of his works; his one-act opera Djamileh opened at the Opéra-Comique in May 1872. Although this failed and was withdrawn after 11 performances,  it led to a further commission from the theatre, this time for a full-length opera for which Henri Meilhac and Ludovic Halévy would provide the libretto. Halévy, who had written the text for Bizet\'s student opera Le docteur Miracle (1856), was a cousin of Bizet\'s wife, Geneviève;  he and Meilhac had a solid reputation as the librettists of many of Jacques Offenbach\'s operettas.   Bizet was delighted with the Opéra-Comique commission, and expressed to his friend Edmund Galabert his satisfaction in "the absolute certainty of having found my path".Dean 1965, p. 100 The subject of the projected work was a matter of discussion between composer, librettists and the Opéra-Comique management; Adolphe de Leuven, on behalf of the theatre, made several suggestions that were politely rejected. It was Bizet who first proposed an adaptation of Prosper Mérimée\'s novella Carmen.  Mérimée\'s story is a blend of travelogue and adventure yarn, | Question: Who wrote the opera Carmen?', ['bizet'], 'mrqa_triviaqa-validation-2722'), tensor(0.8478)), (("Context: BBC - Radio 3 - Elgar/Enigma VariationsBBC - Radio 3 - Elgar/Enigma Variations  13 * * * (Romanza)  14 Finale: E. D. U.  The first performance of Elgar’s ‘Enigma’ Variations took place at St James’s Hall London on 19 June 1899, conducted by Hans Richter. It was Elgar’s most ambitious orchestral work to date and a further performance in Düsseldorf in 1901 went on to establish him as a composer of international importance: Richard Strauss declared that ‘here for the first time is an English composer who has something to say’.  Like most overnight successes, it was the result of years of hard work. Elgar was 42 years old when he completed the Variations and, despite bitter disappointments and frustrations, had steadily built up a reputation, first provincially, then at a national level. In particular, a series of cantatas of increasing size had revealed Elgar’s brilliant orchestration and growing mastery of large forms. Now that mastery was demonstrated on a symphonic scale through the time-honoured form of Theme and Variations, inviting comparisons with the greatest classical masters. But it was Elgar’s uniquely personal approach to the form that gave the ‘Enigma’ Variations its initial novelty and lasting appeal.  Elgar himself recalled how the work came to be conceived on the evening of 21 October 1898:  After a long day’s fiddle teaching in Malvern, I came home very tired. Dinner being over, my dear wife said to me, ‘Edward, you look like a good cigar,’ and having lighted it, I sat down at the piano. In a little while, soothed and feeling rested, I began to play, and suddenly my wife interrupted by saying, ‘Edward, that’s a good tune.’ I awoke from the dream: ‘Eh! tune, what tune!’ and she said, ‘Play it again, I like that tune.’ I played and strummed, and played, and then she exclaimed, ‘That’s the tune.’ And that tune is the theme of the Variations.  Many years later Elgar’s daughter Carice recounted the same incident in a BBC broadcast:  My father was at the piano, smoking his pipe, and when I went to bed I heard him playing what I thought were pretty tunes. My mother told me he was inventing music about his friends, and he turned to her and said, ‘Who’s that like?’ My mother replied, ‘I can’t quite say, but it’s exactly the way WMB goes out of the room.’  The grand scheme was established at the outset: 13 variations, 13 musical sketches of ‘my friends pictured within’, as the dedication eventually ran, and a final 14th variation representing the composer himself.  What of the ‘Enigma’ of the title? Before the first performance, Elgar said:  The ‘Enigma’ I will not explain – its ‘dark saying’ must be left unguessed, and I warn you that the apparent connection between the Variations and the Theme is often of the slightest texture; further, through and over the whole set another and larger theme ‘goes’, but is not played . . . So the principal Theme never appears, even as in some late dramas – eg Maeterlinck’s L’Intruse and Les sept Princesses – the chief character is never on stage.  More ink has probably been spilt over these sentences than on any other Elgarian topic, each of the dozens of proposed solutions adding yet another layer of mystery to an already ambiguous pronouncement. What is clear is that the name ‘Enigma’ applies only to the theme itself and not to the whole work. Writing in 1911 Elgar revealed that this work, commenced in a spirit of humour & continued in deep seriousness, contains sketches of the composer’s friends. It may be understood that these personages comment or reflect on the original theme & each one attempts a solution of the Enigma, for so the theme is called. The sketches are not ‘portraits’, but each variation contains a distinct idea founded on some particular personality or perhaps on some incident known | Question: Which musical work of 1898 features a section called 'Nimrod'?", ['Enigma Variations', 'Enigma’ Variations', 'enigma variations'], 'mrqa_triviaqa-validation-4729'), tensor(0.8158)), (("Context: After the revelation of Mona Vanderwaal as the first and original `` A '' , she began receiving visits from someone , known as Red Coat , who offered her a partnership and together they built up the `` A-Team '' . The team had many members but disbanded after the season three finale and Big A began working with a single ally . The identity of the second `` A '' , Red Coat , and the leader of the `` A-Team '' was revealed to be CeCe Drake , while her ally that donned the Black Widow and other Red Coat disguise was revealed to be Sara Harvey . Five years later , a new mysterious entity arises and begins using Emojis to communicate but later baptizes themselves as `` A.D. '' , while the Liars refer to the anonymous figure as Uber A. Then , in the Series Finale , `` A.D. '' reveals themselves to be Alex Drake , the twin sister of Spencer . | Question: who is the head a in pretty little liars ?", ['CeCe Drake'], 'mrqa_naturalquestions-validation-2900'), tensor(0.6209)), (('Context: Aaron, Brother of Moses - The Church of Jesus Christ of ...Aaron, Brother of Moses  Aaron, Brother of Moses  See also Aaronic Priesthood ; Moses  In the Old Testament, a son of Amram and Jochebed, of the tribe of Levi ( Ex. 6:16–20 ); the elder brother of Moses ( Ex. 7:7 ).  Was appointed by the Lord to assist Moses in bringing the children of Israel out of Egypt and to be his spokesman: Ex. 4:10–16, 27–31 ;  Who Was Moses? - Biblical Archaeology SocietyWho Was Moses? Was He More than an Exodus Hero? - Biblical Archaeology Society  Who Was Moses? Was He More than an Exodus Hero?  Discovering the Biblical Moses  Read Peter Machinist’s article “The Man Moses” as it originally appeared in Bible Review, April 2000. The article was first republished in Bible History Daily in 2012.—Ed.  \xa0  The Exodus hero Moses. The Biblical Moses, portrayed here as a shepherd in a print by contemporary Israeli artist Mordechai Beck, protectively clasps a sheep in his arms. Photo: Mordechai Beck.  Moses’ story is told in the Book of Exodus , but it starts in Genesis with the story of Abraham and his family with whom God makes a covenant. Generations later the Biblical Moses draws the extended family together in the form of a nation with a structure and code of law, given to him on Mount Sinai . Below, Peter Machinist explores the story of Moses, the Exodus hero, in “The Man Moses.”  Some might say that God himself was the Exodus hero, but in human terms the Biblical Moses takes center stage throughout the whole Pentateuch. Who was Moses? A rather solitary leader, one with his people but set apart, even in his childhood, when he was raised by the pharaoh’s daughter as if he were an Egyptian prince. Set apart also in that he married an alien wife—Midianite or possibly Ethiopian. Even his physical characteristics—a speech defect—set him apart from others and is accommodated by God who arranges a leadership duo with Moses and his priestly brother Aaron . His role was unique—even to receiving the Law and seeing God, as evidenced by Moses’ blinding countenance.  The Biblical Moses also has an unusual death. God says he must die alone on a mountaintop outside the promised land. Who was Moses? We might say he was a man who was a son of Abraham who led the people but was not typical of them.  In “The Man Moses,” Peter Machinist proposes that our Exodus hero is a type of anti-hero, outside the stereotype of a tribal or national leader. He might represent the people of Israel themselves, Biblically portrayed as being outsiders. Further, Moses’ otherness might also serve to turn the spotlight not on himself but on the message he delivers to the people: the Law. Who was Moses—the Biblical Moses? Who was the man chosen to meet God on Sinai and receive the Law on behalf of God’s chosen people?  Below, Peter Machinist explores the character of the Exodus hero—the Biblical Moses—in “The Man Moses.”  \xa0  The Man Moses  by Peter Machinist  “You shall not cross there,” God decrees as Moses gazes across the Jordan. In this 1928 pastel by Lesser Ury, heavenly light illuminates the promised land that Moses has sought almost all his life but will never enter. Rather, Moses dies on Mt. Nebo—a strange and solitary death for a strangely solitary man. The biblical portrayal of Moses as distant and unapproachable, as the only biblical leader to see God “face to face” (Deuteronomy 34:10), presents Moses as representative of the Israelites—a people apart. At the same time, it encourages readers to concentrate more on the law he gave than on the life he lived. Photo: Hans-Joachim Bartsch/Collection Jüdisches Museum Berlin.  The introduction of Moses in the first chapters of Exodus marks a new, a second beginning in the Bible’s account of the history of Israel. The first beginning had been | Question: According to the Old Testament of the Bible who was the brother of Moses?', ['aaron'], 'mrqa_triviaqa-validation-5231'), tensor(0.5705)), (('Context: Nuclear power in space is the use of nuclear power in outer space , typically either small fission systems or radioactive decay for electricity or heat . Another use is for scientific observation , as in a Mössbauer spectrometer . One common type is a radioisotope thermoelectric generator , which has been used on many space probes and on manned lunar missions , and another is small fission reactors for Earth observation satellites such as the TOPAZ nuclear reactor . A radioisotope heater unit provides heat from radioactive decay of a material and can potentially produce heat for decades . | Question: how have scientists used nuclear power as a fuel for spacecraft ?', ['either small fission systems or radioactive decay for electricity or heat'], 'mrqa_naturalquestions-validation-3828'), tensor(0.5651)), (("Context: Kenya is active in several sports, among them cricket, rallying, football, rugby union and boxing. The country is known chiefly for its dominance in middle-distance and long-distance athletics, having consistently produced Olympic and Commonwealth Games champions in various distance events, especially in 800 m, 1,500 m, 3,000 m steeplechase, 5,000 m, 10,000 m and the marathon. Kenyan athletes (particularly Kalenjin) continue to dominate the world of distance running, although competition from Morocco and Ethiopia has reduced this supremacy. Kenya's best-known athletes included the four-time women's Boston Marathon winner and two-time world champion Catherine Ndereba, 800m world record holder David Rudisha, former Marathon world record-holder Paul Tergat, and John Ngugi. | Question: What sports are Kenyans active in?", ['cricket, rallying, football, rugby union and boxing'], 'mrqa_squad-validation-8542'), tensor(0.5481)), (("Context: Interior Angles of Polygons - Maths ResourcesInterior Angles of Polygons  Interior Angles of Polygons  An Interior Angle is an angle inside a shape  Triangles  The Interior Angles of a Triangle add up to 180°  Let's try a triangle:  It works for this triangle  Now tilt a line by 10°:  80° + 70° + 30° = 180°  One angle went up by 10°,  and the other went down by 10°  Quadrilaterals (Squares, etc)  (A Quadrilateral has 4 straight sides)  Let's try a square:  90° + 90° + 90° + 90° = 360°  A Square adds up to 360°  Now tilt a line by 10°:  80° + 100° + 90° + 90° = 360°  It still adds up to 360°  The Interior Angles of a Quadrilateral add up to 360°  Because there are 2 triangles in a square ...  The interior angles in a triangle add up to 180° ...  ... and for the square they add up to 360° ...  ... because the square can be made from two triangles!  Pentagon  A pentagon has 5 sides, and can be  made from three triangles, so you know what ...  ... its interior angles add up to 3 × 180° = 540°  And when  it is regular (all angles the same), then each angle is 540° / 5 = 108°  (Exercise: make sure each triangle here adds up to 180°, and check that the pentagon's interior angles add up to 540°)  The Interior Angles of a Pentagon add up to 540°  The General Rule  Each time we add a side (triangle to quadrilateral, quadrilateral to pentagon, etc), we add another 180° to the total:  \xa0  Parallelograms and Trapezoids - Free Math HelpParallelograms and Trapezoids - Free Math Help  Related Pages  What is a Parallelogram?  A parallelogram is a quadrilateral in which both pairs of opposite sides are parallel.  Special relationships exist between the measures of consecutive angles, opposite angles and opposite sides of a parallelogram. A square is the most straightforward parallelogram, because it has 2 sets of parallel sides. Naturally, all of its angles and sides match in length or measure.  Facts About a Parallelogram  (1) The degree measure of the four angles of a parallelogram add up to 360 degrees. Remember that all quadrilaterals (4 sided figures) have angles which add up to 360 degrees. Here's a sample:  Then: a + b + c + d = 360 degrees  (2) The degree measure of any two consecutive angles add up to 180 degrees.                 In parallelogram ABCD:  angle a + angle b = 180 degrees  angle b + angle c = 180 degrees  angle c + angle d = 180 degrees  angle a + angle d = 180 degrees  (3) Opposite angles have the same measure in terms of degrees.  In parallelogram ABCD:  (1) Opposite sides are parallel:  side AD || side BC  NOTE: The symbol || means parallel.  (2) Opposite sides have the same lengths:  side AD = side BC  side AB = side CD  Diagonals of a Parallelogram  The diagonals of a parallelogram divide the parallelogram into two side-by-side triangles.  As shown in the picture below, diagonal AC forms equal alternate interior angles with each pair of parallel sides.  We can also see that there are two triangles in the picture below.  Triangle 1 is congruent to triangle 2 by ASA (Angle-Side-Angle) Method.  Where did the two triangles come from?  They were formed by diagonal AC.  I should also note that diagonals of a parallelogram bisect each other as shown in the picture below.  AE = EC  where E is the midpoint of BOTH diagonals.  Example:  In parallelogram WXYZ, the measure of angle X = 4a - 40 and the measure of angle Z | Question: The internal angles of a quadrilateral add up to how many degrees?", ['360'], 'mrqa_triviaqa-validation-5362'), tensor(0.5112)), (("Context: `` Everywhere '' is a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 ) , written by Fleetwood Mac member Christine McVie , who also performs lead vocals on the song . `` Everywhere '' was released as the fourth single from Tango in the Night on November 28 , 1987 in the United States , where it reached number 14 on the Billboard Hot 100 chart and number - one on the Adult Contemporary chart , remaining there for three weeks . `` Everywhere '' was released in the United Kingdom on 21 March 1988 and reached number four . It also reached number 45 in Australia . | Question: i want to be with you everywhere song ?", ['Everywhere'], 'mrqa_naturalquestions-validation-114'), tensor(0.5054)), (('Context: Least of the Great Powers  The least of the Great Powers is a label used to conceptualize Italy\'s international status.  Italy is part of great power concerts such as the EU trio, the NATO Quint, the G7, the G20 and various International Contact Groups. Italy, one of the UN\'s major funders, is the leading nation of the Uniting for Consensus and serves as one of the states of "chief" importance in providing shipping services, air transport and Industrial development.  Alternative terms used by academics and observers to describe this concept include "intermittent Major power" or "small Great power", asserting that Italy\'s position in the international arena can be described in this way.   G20  The G20 (or G-20 or Group of Twenty) is an international forum for the governments and central bank governors from 20 major economies.  Currently, these are Argentina, Australia, Brazil, Canada, China, France, Germany, India, Indonesia, Italy, Japan, Mexico, Russia, Saudi Arabia, South Africa, South Korea, Turkey, United Kingdom, United States, and the European Union.  Founded in 1999, the G20 aims to discuss policy issues pertaining to the promotion of international financial stability.  It seeks to address issues that go beyond the responsibilities of any one organization.  The G20 heads of government or heads of state have periodically conferred at summits since their initial meeting in 2008, and the group also hosts separate meetings of finance ministers and foreign ministers due to the expansion of its agenda in recent years. | Question: What part of the great power concerts such as the EU trio, the Nato Quint, the G7, and the G20, is Italy associated with?', ['G20'], 'mrqa_hotpotqa-validation-4825'), tensor(0.4889)), (('Context: Evolution of the adaptive immune system occurred in an ancestor of the jawed vertebrates. Many of the classical molecules of the adaptive immune system (e.g., immunoglobulins and T cell receptors) exist only in jawed vertebrates. However, a distinct lymphocyte-derived molecule has been discovered in primitive jawless vertebrates, such as the lamprey and hagfish. These animals possess a large array of molecules called Variable lymphocyte receptors (VLRs) that, like the antigen receptors of jawed vertebrates, are produced from only a small number (one or two) of genes. These molecules are believed to bind pathogenic antigens in a similar way to antibodies, and with the same degree of specificity. | Question: Evolution of what part of the immune system occurred in the evolutionary ancestor of jawed vertebrates?', ['adaptive', 'adaptive immune system', 'the adaptive immune system'], 'mrqa_squad-validation-6677'), tensor(0.3650)), (('Context: BBC Wales - Arts - Children - Ivor The EngineBBC Wales  - Arts - Children  - Ivor The Engine  Ivor The Engine  Ivor The Engine  Last updated: 11 February 2009  "Not very long ago, in the top left-hand corner of Wales, there was a railway.  It wasn\'t a very long railway or a very important railway, but it was called The Merioneth and Llantisilly Rail Traction Company Limited, and it was all there was.  "And in a shed, in a siding at the end of the railway, lives the Locomotive of the Merioneth and Llantisilly Rail Traction Company Limited, which was a long name for a little engine so his friends just called him Ivor."  Ivor The Engine was a children\'s animation created by Oliver Postgate and Peter Firmin\'s Smallfilms television company. It was made in black and white for Associated Rediffusion and first broadcast in 1959, but was remade in colour for the BBC in 1975.  It told the story of a sometimes disobedient small green locomotive who worked for the Merioneth and Llantisilly Rail Traction Company Limited, although he dreamed of singing with the Grumbley and District Choral Society. In time he became first bass of the choir and transported the members from place to place.  His driver was Edwin Jones, known as Jones the Steam, who enjoyed fishing and daydreaming. Ivor\'s other friends included stationmaster Dai Station, portly choirmaster Evans the Song, fellow chorister Idris the Dragon, and fairground owner Morgan the Roundabout. Mr Morgan gave Ivor some pipes from his steam calliope to allow him to sing in the choir.  Behind the scenes  Ivor The Engine was inspired by Welshman Denzyl Ellis, a former railwayman whom Postgate met shortly after World War Two. Ellis, a former fireman with the Royal Scot train, described how locomotives came to life in the mornings after the engine fires had been lit.  The programme was Smallfilms\' first production. Although set in Wales, Ivor The Engine was actually made in Firmin\'s home in Blean near Canterbury, Kent, using stop-motion animation techniques. Cardboard cutouts painted with watercolours were used for the characters and backgrounds.  The programmes were written and narrated by Oliver Postgate and drawn and painted by Peter Firmin. Voices were performed by Oliver Postgate, Anthony Jackson and Olwen Griffiths - the only Welsh person involved in the production.  The distinctive puffing sound made as Ivor moved was voiced by Postgate, and the show\'s music was composed by Vernon Elliott. Ivor\'s \'voice\' was mostly sounded by a bassoon.  Postgate described himself as having been "intoxicated by the work of Dylan Thomas , and used to carry Under Milk Wood around in my pocket". Wales was a logical setting for the tale, and became romanticised in Postgate\'s fictionalised world.  "Wales is where you have little railways going along the tops of hills, which is much less boring that hurtling up the slumbering Midlands plain in the middle of the night," he told science fiction enthusiast Clive Banks , "so we decided it would be nice to set it in Wales."  "Ivor The Engine is entirely bogus as far as Wales is concerned - it\'s built entirely on a picture of Wales given by Dylan Thomas! Then, literally in the bath, I came to realise what the story was: the engine wanted to sing in the choir, which is obviously what a Welsh engine would want, so from then on it fell into place.  Postgate and Firmin created a map of the top left corner of north Wales where Ivor lived. It included viaducts, bridges, tunnels, towns, a mine and gasworks, and was strictly adhered to by the show\'s creators.  The episodes  The original series comprised six episodes, and told the story of Ivor getting his pipes and joining the choir. They were followed by two further series of 13 episodes, all of which were in black and white. Each lasted 10 minutes.  The animations were shown regularly by Associated Rediffusion until the company folded in 1968. In 1975 Smallfilms were given back the rights to the stories and began remaking the second | Question: In the children’s television series ‘Ivor the Engine’ what is the name of the dragon?', ['idris'], 'mrqa_triviaqa-validation-893'), tensor(0.3561)), (('Context: Greenland ( / ˈɡriːnlənd / ; Greenlandic : Kalaallit Nunaat , pronounced ( kalaːɬit nunaːt ) ; Danish : Grønland , pronounced ( ˈɡʁɶnˌlanˀ ) ) is an autonomous constituent country within the Kingdom of Denmark between the Arctic and Atlantic Oceans , east of the Canadian Arctic Archipelago . Though physiographically a part of the continent of North America , Greenland has been politically and culturally associated with Europe ( specifically Norway and Denmark , the colonial powers , as well as the nearby island of Iceland ) for more than a millennium . The majority of its residents are Inuit , whose ancestors began migrating from the Canadian mainland in the 13th century , gradually settling across the island . | Question: is greenland part of europe or north america ?', ['physiographically a part of the continent of North America'], 'mrqa_naturalquestions-validation-5502'), tensor(0.2521)), (('Context: What is a Mulligan in Golf? - About.com SportsWhat is a Mulligan in Golf?  What Is a \'Mulligan\' in Golf?  What Is a \'Mulligan\' in Golf?  This golfer looks like he needed a mulligan.\xa0 John Cumming/Photodisc/Getty Images  By Brent Kelley  Updated February 05, 2016.  A mulligan, most simply put, is a "do-over" in golf. Hit a bad shot? Take a mulligan and replay that stroke . Drop a ball on the spot from which you just played, and re-play. The first (bad) shot is not counted.  Are Mulligans \'Legal\'?  No. There is never a time, when playing under the\xa0 Rules of Golf , that a mulligan is "legal." Mulligans are not allowed under the rules.  But Mulligans Are Very Popular  But just because mulligans aren\'t "legal" doesn\'t mean their use isn\'t common and popular among recreational golfers. Many amateurs and recreational golfers - players just out to have a fun round with buddies, as opposed to serious golfers looking for competition - are lax in observing the rules anyway.  So mulligans are most often employed during friendly rounds by golf buddies; or during charity or playday tournaments where mulligans are sometimes sold. If mulligans are for sale at a charity tournament, that means the golfer can buy, say, three mulligans for a set price each.  continue reading below our video  What Are Mulligans?  The sale of mulligans is sometimes used as an additional fund-raiser at charitable events.  Common Ways of Using Mulligans  Do all golfers use mulligans in the same way? No - whatever a group of golfers agrees upon is what counts (unless you are using mulligans in something like a charity tournament or association outing setting - then do what the organizers tell you).  How many mulligans you get to use during a round, what types of shots you can use them for, and so on, are things that differ from golfer to golfer. So if you\'re playing with a new group and mulligans are in effect, you need to clarify what is allowed.  Here are some common ways that mulligans are used:  Some golfers limit the use of mulligans to the first tee only, or to the first and 10th tees only.  Some golfers use one mulligan per nine holes, but anywhere on each nine.  It\'s most common for mulligans to be used only off the tee , i.e., you can only use a mulligan to replay a drive. However, some groups allow mulligans from the fairway , too.  Less common is allowing mulligans from the rough or out of hazards, but some golfers even do that.  It is rarer still - rarely seen, in fact - for mulligans to be used on the putting green .  And some groups allow mulligans from just about anywhere on the golf course, but set a limit - say, three mulligans per round, or nine, or 18.  Clearly, there are many different ways that golfers use mulligans. If you have a regular group of friends you play with, and your group allows mulligans, you probably long ago settled into your own "rules" about using them.  Note that mulligans are common in the United States, but rarely seen in the UK, for example. As with all localized golf customs, formats and betting games , when playing with golfers you don\'t know clear up the ground rules before play starts to avoid possible confusion later.  And again, please note: If you are playing in a tournament, handicap round or other setting in which the Rules of Golf are strictly followed, you cannot play mulligans.  Why Is It Called a \'Mulligan\'?  Good question! And the fact is, nobody knows for sure how a do-over in golf came to be called a mulligan. There are multiple theories, however, and we go over some of them in our FAQ on the topic:  Why are mulligans called that?  Other Terms/Uses  Mulligans | Question: In which sport may a \'Mulligan\' be awarded?', ['golf', 'golfer'], 'mrqa_triviaqa-validation-5972'), tensor(0.1991)), (('Context: The history of agriculture records the domestication of plants and animals and the development and dissemination of techniques for raising them productively . Agriculture began independently in different parts of the globe , and included a diverse range of taxa . At least eleven separate regions of the Old and New World were involved as independent centers of origin . | Question: where did the cultivation of agriculture first arise ?', ['eleven separate regions of the Old and New World'], 'mrqa_naturalquestions-validation-8119'), tensor(0.1852)), (("Context: The comptroller ( who is also auditor general and head of the National Audit Office ) controls both the Consolidated Fund and the National Loans Fund . The full official title of the role is Comptroller General of the Receipt and Issue of Her Majesty 's Exchequer . | Question: who controls the consolidated fund of the state ?", ['The comptroller ( who is also auditor general and head of the National Audit Office )'], 'mrqa_naturalquestions-validation-1364'), tensor(0.1330)), (('Context: Why do Americans call it a period and the British a full stop?Why do Americans call it a period and the British a full stop? - Topic  \xa0\xa0Why do Americans call it a period and the British a full stop?  Go  Why do Americans call it a period and the British a full stop?  Cheers, Susan  IP  \xa0  I really don\'t know for sure, Susan, but my hunch (from what I know about our history) is that we call it a period just to be different from the British. That doesn\'t mean that the word was just "invented" by Americans. Actually, the word period used to mean the punctuation mark placed to signal the end of a sentence goes back to the early 16th century, if I remember right. So there is justification for calling it a period rather than a full stop.  After the Americans won their independence from Great Britain, the famous dictionary compiler, Noah Webster, determined that we should try to do things as differently from the British as possible to separate ourselves psychologically as much as we could. In that vein he changed lots of spellings. For example colour/color; gaol/jail; Geoffrey/Jeffery; tyre/tire). I wouldn\'t be surprised if that\'s why we now say period and the British say full stop.  (By the way, I\'ve always thought it silly to say full stop. A stop is a stop. How can you have a "full" stop? Can you have a "partial" stop as well? Would that be a comma? In that case, I\'d call it a "momentary stop," not a "partial" stop!  )  We even went so far as to change our table manners. When the British cut some food on their plates, they hold the fork in the left hand and the knife in the right hand. Then they pick the cut food up with the fork, still held in the left hand, and bring it to their mouths. Americans, in contrast, cut the food the same way, but then put down the knife and take the fork with their right hands. Then they bring the cut food to their mouths.  I know I\'ve gotten off the topic, but I thought you might find this interesting. There was a lot of psychology going on back in the early years of our independence from Great Britain, Susan!  This message has been edited. Last edited by: <Richard, Moderator>,  I would like to join in.  Thanks a lot for such informative reply.  Since the Americans want to be different from the British, I wonder why they i.e the Americans have spoken their language at first. They could have spoken Spanish, for example.  PS. You, the Americans try to be different form the British in grammar, vocabulary, spelling and pronunciation. And we, the learners of English take full credit to cope with such differences! That\'s not fair, is it!  This message has been edited. Last edited by: Izzy loves you all ,  Izzy loves you all  \xa0  I don\'t agree that Americans always want to be different from the British.  It was true in the past that, in declaring ourselves independent, out nation\'s founders tended to divorce ourselves from everything British. For example, at the beginning of our nation, we wanted to avoid any kind of royalty in a government, so great efforts were taken to make sure that a president could not be a king. And, people strove to make many concepts in language and writing different, too.  However, at some times now, and in some circles, things British are very much admired and copied.  People who love the British are called Anglophiles, and we have quite a few of them in the United States.  Still, if some Americans adopt a British accent, and call the letter Z "zed," and an elevator "a lift," the rest of us kind of smile and think the speaker is a | Question: We call it a ‘full stop’ what do Americans call it?', ['period'], 'mrqa_triviaqa-validation-1792'), tensor(0.1181)), (("Context: For a phylum with relatively few species, ctenophores have a wide range of body plans. Coastal species need to be tough enough to withstand waves and swirling sediment particles, while some oceanic species are so fragile that it is very difficult to capture them intact for study. In addition oceanic species do not preserve well, and are known mainly from photographs and from observers' notes. Hence most attention has until recently concentrated on three coastal genera – Pleurobrachia, Beroe and Mnemiopsis. At least two textbooks base their descriptions of ctenophores on the cydippid Pleurobrachia. | Question: Which group of ctenophore are are hardest to study?", ['oceanic species'], 'mrqa_squad-validation-4456'), tensor(0.1104)), (("Context: What we now call gravity was not identified as a universal force until the work of Isaac Newton. Before Newton, the tendency for objects to fall towards the Earth was not understood to be related to the motions of celestial objects. Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object. Today, this acceleration due to gravity towards the surface of the Earth is usually designated as  and has a magnitude of about 9.81 meters per second squared (this measurement is taken from sea level and may vary depending on location), and points toward the center of the Earth. This observation means that the force of gravity on an object at the Earth's surface is directly proportional to the object's mass. Thus an object that has a mass of  will experience a force: | Question: Who came up with the concept that falling objects fell at the same speed regardless of weight?", ['Galileo'], 'mrqa_squad-validation-10410'), tensor(0.1084)), (("Context: The term ' motor neuron ' is usually restricted to the lower motor neurons , the efferent nerves that directly innervate muscles . | Question: all the motor neurons that control the skeletal muscles are ?", ['efferent nerves'], 'mrqa_naturalquestions-validation-2571'), tensor(0.1002)), (('Context: TurbineA turbine (from the Latin turbo, a vortex, related to the Greek , tyrbē, meaning "turbulence"),   is a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work. A turbine is a turbomachine with at least one moving part called a rotor assembly, which is a shaft or drum with blades attached. Moving fluid acts on the blades so that they move and impart rotational energy to the rotor. Early turbine examples are windmills and waterwheels.  Gas, steam, and water turbines have a casing around the blades that contains and controls the working fluid. Credit for invention of the steam turbine is given both to the British engineer Sir Charles Parsons (1854–1931), for invention of the reaction turbine and to Swedish engineer Gustaf de Laval (1845–1913), for invention of the impulse turbine. Modern steam turbines frequently employ both reaction and impulse in the same unit, typically varying the degree of reaction and impulse from the blade root to its periphery.  The word "turbine" was coined in 1822 by the French mining engineer Claude Burdin from the Latin turbo, or vortex, in a memo, "Des turbines hydrauliques ou machines rotatoires à grande vitesse", which he submitted to the Académie royale des sciences in Paris.  Benoit Fourneyron, a former student of Claude Burdin, built the first practical water turbine.   Operation theory   A working fluid contains potential energy (pressure head) and kinetic energy (velocity head). The fluid may be compressible or incompressible. Several physical principles are employed by turbines to collect this energy:  Impulse turbines change the direction of flow of a high velocity fluid or gas jet. The resulting impulse spins the turbine and leaves the fluid flow with diminished kinetic energy. There is no pressure change of the fluid or gas in the turbine blades (the moving blades), as in the case of a steam or gas turbine, all the pressure drop takes place in the stationary blades (the nozzles). Before reaching the turbine, the fluid\'s pressure head is changed to velocity head by accelerating the fluid with a nozzle.  Pelton wheels and de Laval turbines use this process exclusively. Impulse turbines do not require a pressure casement around the rotor since the fluid jet is created by the nozzle prior to reaching the blades on the rotor. Newton\'s second law describes the transfer of energy for impulse turbines.  Reaction turbines develop torque by reacting to the gas or fluid\'s pressure or mass.  The pressure of the gas or fluid changes as it passes through the turbine rotor blades. A pressure casement is needed to contain the working fluid as it acts on the turbine stage(s) or the turbine must be fully immersed in the fluid flow (such as with wind turbines). The casing contains and directs the working fluid and, for water turbines, maintains the suction imparted by the draft tube. Francis turbines and most steam turbines use this concept. For compressible working fluids, multiple turbine stages are usually used to harness the expanding gas efficiently. Newton\'s third law describes the transfer of energy for reaction turbines.  In the case of steam turbines, such as would be used for marine applications or for land-based electricity generation, a Parsons type reaction turbine would require approximately double the number of blade rows as a de Laval type impulse turbine, for the same degree of thermal energy conversion. Whilst this makes the Parsons turbine much longer and heavier, the overall efficiency of a reaction turbine is slightly higher than the equivalent impulse turbine for the same thermal energy conversion.  In practice, modern turbine designs use both reaction and impulse concepts to varying degrees whenever possible. Wind turbines use an airfoil to generate a reaction lift from the moving fluid and impart it to the rotor. Wind turbines also gain some energy from the impulse of the wind, by deflecting it at an angle. Turbines with multiple stages may utilize either reaction or impulse blading at high pressure. Steam turbines were traditionally more impulse but continue to move towards reaction designs similar to those used in gas turbines. At low pressure the operating fluid medium expands in volume for small reductions in | Question: What main category of machines \'produce continuous power from a wheel/rotor, usually with vanes, revolved by fast flowing fluid\'?', ['turbine engines', 'reaction turbine', 'turbines', 'turbine', 'turbine blades', 'impulse turbine'], 'mrqa_triviaqa-validation-2376'), tensor(0.0922)), (('Context: It has been claimed that the transmission of the first episode was delayed by ten minutes due to extended news coverage of the assassination of US President John F. Kennedy the previous day; whereas in fact it went out after a delay of eighty seconds. The BBC believed that many viewers had missed this introduction to a new series due to the coverage of the assassination, as well as a series of power blackouts across the country, and they broadcast it again on 30 November 1963, just before episode two. | Question: What other event made the BBC concerned that viewers had not seen the premier of Doctor Who?', ['a series of power blackouts across the country', 'power blackouts'], 'mrqa_squad-validation-7746'), tensor(0.0887)), (('Context: 67th Tony Awards  The 67th Annual Tony Awards were held June 9, 2013, to recognize achievement in Broadway productions during the 2012–13 season.  The ceremony returned to Radio City Music Hall in New York City, after two years at Beacon Theatre, and was broadcast live on CBS television.  Neil Patrick Harris hosted for the third consecutive year, his fourth time as host.  Awards in four of the eight acting categories, (Best Actress in a Play, Best Actor in a Musical, Best Actress in a Musical, Best Featured Actor in a Play) were given to African-American performers.  Furthermore, it is the second time in Tony history that both directing prizes went to women.  Garry Hynes and Julie Taymor had previously won in 1998. " Kinky Boots" had a season best 13 nominations and 6 awards.  Cyndi Lauper, who wrote the score for "Kinky Boots", is the first solo female winner for Best Original Score.   67th Tony Awards  The 67th Annual Tony Awards were held June 9, 2013, to recognize achievement in Broadway productions during the 2012–13 season.  The ceremony returned to Radio City Music Hall in New York City, after two years at Beacon Theatre, and was broadcast live on CBS television.  Neil Patrick Harris hosted for the third consecutive year, his fourth time as host.  Awards in four of the eight acting categories, (Best Actress in a Play, Best Actor in a Musical, Best Actress in a Musical, Best Featured Actor in a Play) were given to African-American performers.  Furthermore, it is the second time in Tony history that both directing prizes went to women.  Garry Hynes and Julie Taymor had previously won in 1998. " Kinky Boots" had a season best 13 nominations and 6 awards.  Cyndi Lauper, who wrote the score for "Kinky Boots", is the first solo female winner for Best Original Score.   Julie Taymor  Julie Taymor (born December 15, 1952) is an American director of theater, opera and film.  She is best known for directing the stage musical "The Lion King", for which she became the first woman to win the Tony Award for directing a musical, in addition to a Tony Award for Original Costume Design.   Julie Taymor  Julie Taymor (born December 15, 1952) is an American director of theater, opera and film.  She is best known for directing the stage musical "The Lion King", for which she became the first woman to win the Tony Award for directing a musical, in addition to a Tony Award for Original Costume Design. | Question: In what year did the director of The Lion King win the Tony Awards?', ['1998'], 'mrqa_hotpotqa-validation-3241'), tensor(0.0855)), (('Context: The industry called Nollywood - Vanguard NewsThe industry called Nollywood - Vanguard News  Vanguard News  A Nigerian newspaper and Online version of the Vanguard, a daily publication in Nigeria covering Niger delta, general national news, politics, business, energy, sports, entertainment, fashion,lifestyle human interest stories, etc  Search for:  Home » Special Report » The industry called Nollywood  The industry called Nollywood  9:58 pmIn Special Report by adekunle Comments  By McPhilips Nwachukwu  IN the beginning: The story of Nigeria’s Nollywood is like Ben Okri’s “river” in his novel, The Famished Road, “which becomes a road and branches out to the world.”  Like a river, which starts small from a tiny tributary, the industry began as a small effort by some persons with energetic entrepreneurial spirit, who wanted to do small budget films for commercial purposes; and suddenly branched out like an ocean into one huge market that today intersects the world of business interests of actors and actresses, script writers, production crews, distributors and regulatory bodies, even in the daunting face of threatening copyright infringement challenges.  Berthing in Nigeria in 1992, this entertainment industry popularly known as Nollywood is acclaimed by industry players to rank as the third largest movie industry in the world after India’s Bollywood and America’s Nollywood. According to movie scholars and critics, Nollywood industry, which despite its relative young emergence in the global movie market has successfully become an enviable artistic cultural vehicle with which African narrative is communicated to the world, ironically emerged as a child of necessity to fill the gap created by the total collapse of the country’s theatre and stage culture.  According to industry sources, the failure of government in providing adequate security surveillance over life and property in the late 80s’and early 90s’resulted in the very negative manner at which the public viewed public shows, especially as it had to do with cinema, stage and theatrical performances with disenchantment.  Video filmartistes at a film shooting site  Also added to this ugly development “was the fact that there were inadequate television stations across the country with enough entertainment contents to satisfy the yearning needs of the viewing public, a development that led\xa0content providers to seek alternative.”  “Nollywood was a child of necessity” says Professor Ahmed Yerima, theatre artiste and former Artistic Director of The National Troupe of Nigeria. He said: “There were very few television stations and we got to a point where the TV stations that existed needed you, the producer, to get sponsorship for your productions.”  Movie-theatre going culture  But for Obby Patrick Ebewo, Nigerian born and American based film scholar, “The collapse of movie-theatre going culture in the 1980s caused by incessant harassment of innocent citizen by criminals, the country’s economic downturn and various problems affecting celluloid film production, gave rise to video film.”  The implication of this new development, according to Yerima, was that fast thinking and business minded entrepreneurs cashed into the opportunity provided by this yearning need; and aided with the arrival of VHS tapes embarked on a business trial and experimentation that was to transform the continent’s entertainment turf.  Prior to this boom, efforts had been made by some dramatists to evolve a film culture in Nigeria. It is on record that as early as the 1960s’ theatre artistes in the mould of Hubert Ogunde, had recorded his plays on celluloid. Other artistes like Moses Adejumo and Duro Ladipo had equally attempted to reduce the cinematic art of film to a more popular medium for social commentary.  In an effort to trace the emergence of Nollywood and the entire film culture in the country, Ebewo argues that “the current video film industry in Nigeria owes a huge debt to the pioneering efforts of Nigerian theatre, particularly practitioners of the Yoruba Travelling theatre, who branched off from mainstream theatre to experiment with celluloid.”  However, the Nollywood brand today owes its projection and commercial resourcefulness to Kenneth Nnebue, who according to market consensus, pioneered the raving popular media brand with his movie, Living in Bondage, in 1992. According to unconfirmed sources, since inception, “the industry rakes gross estimate of | Question: "What country\'s film industry is called ""Nollywood""?" ?', ['nigeria'], 'mrqa_triviaqa-validation-3901'), tensor(0.0831)), (('Context: On 7 January 1943, at the age of 86, Tesla died alone in room 3327 of the New Yorker Hotel. His body was later found by maid Alice Monaghan after she had entered Tesla\'s room, ignoring the "do not disturb" sign that Tesla had placed on his door two days earlier. Assistant medical examiner H.W. Wembly examined the body and ruled that the cause of death had been coronary thrombosis. Tesla\'s remains were taken to the Frank E. Campbell Funeral Home at Madison Ave. and 81st St. A long-time friend and supporter of Tesla, Hugo Gernsback, commissioned a sculptor to create a death mask, now displayed in the Nikola Tesla Museum. | Question: What had been left hanging on the door to Tesla\'s room?', ['"do not disturb" sign', 'do not disturb" sign'], 'mrqa_squad-validation-1539'), tensor(0.0788)), (('Context: Origin of Does Exactly What it Says on the Tin | RonsealOrigin of Does Exactly What it Says on the Tin | Ronseal  The Ronseal Phrase - Does Exactly What it Says on the Tin  \xa0  People use the phrase every day, it has come to represent a product or policy that is open and honest; it is used when something quite simply ‘Does Exactly What it Says on the Tin .’  Originating back to 1994 the popular phrase ‘Does exactly what is says on the tin’ was developed when Ronseal employed London based advertising agency HHCL to create a campaign that without trying too hard would de-mystify our products. Dave Shelton, co-creator of the line with Liz Whinston, explains the thought process behind the original campaign adverts ,  “We started to write a commercial that featured a straightforward guy who said lines like: "If you\'ve got wood to stain and you want it to dry quickly, you need Ronseal Quick Drying Woodstain."  We\'d soon knocked out several scripts but we needed a line… "Does exactly what it says on the tin" was a great way of summing this up.  After the initial campaign, sales shot up and Ronseal became [a] brand leader.”  ‘Does exactly what it says on the tin’ is now more than an advertising slogan; it has become part of our everyday vernacular. The line has made it into the Oxford Dictionary of Idioms, been featured in the song ‘What it Says On The Tin’ by Katie Melua, and has even been used by Prime Minister David Cameron and on multiple occasions to summarise his preferred approach to politics.  The phrase has come to represent a product or policy that is open, honest and delivers against its promise.  The phrase is now used internationally and it’s not surprising it has become the third most known slogan of all time.  \xa0  \xa0  [1] The phrase is a registered trademark of Ronseal’s owner, the Sherwin-Williams Company , across the European Community for products including paints, varnishes and wood preservatives (E3085826)  [3] Source Creative Review February 2012  \xa0  The Ronseal phrase: It does exactly what it says on the tinThe Ronseal phrase: It does exactly what it says on the tin - BBC News  BBC News  The Ronseal phrase: It does exactly what it says on the tin  8 January 2013  Read more about sharing.  Close share panel  Prime Minister David Cameron used a famous advertising slogan to sum up the state of the coalition government at its halfway point, saying "it is a Ronseal deal - it does what it says on the tin". This phrase has entered the British vernacular, says its co-creator Dave Shelton.  David Cameron has said it before. In 2004, he said: "People are crying out for a kind of Ronseal politics - they want it to do what it says on the tin."  People use the phrase every day. On YouTube you can listen to Katie Melua\'s What It Says On The Tin . Google the line and you get millions of results.  In 1994, when my advertising partner Liz Whiston and I came up with the line "Does exactly what it says on the tin" for Ronseal, we never dreamed how it would enter the language.  About the author  Dave Shelton and Liz Whiston co-founded Bordello Advertising.  They have been working in advertising for more than 20 years.  Back then Ronseal was just another varnish fighting for market share against a lot of fancier lifestyle brands. Anyone wandering into a DIY superstore was faced with a wall of different solutions to whatever woodcare job they might be trying to tackle.  We had presented a number of different campaigns, all of which had been rejected by Ged Shields, the then marketing director of Ronseal for "trying too hard".  Shields wanted to de-mystify the product.  We decided what was needed wasn\'t puns or art.  Instead we would call a spade a spade.  We started to write a commercial that featured a | Question: Which product is advertised on TV with the slogan \'It does exactly what it says on the tin\'?', ['ronseal'], 'mrqa_triviaqa-validation-5026'), tensor(0.0768)), (('Context: In July 1973, as part of its outreach programme to young people, the V&A became the first museum in Britain to present a rock concert. The V&A presented a combined concert/lecture by British progressive folk-rock band Gryphon, who explored the lineage of mediaeval music and instrumentation and related how those contributed to contemporary music 500 years later. This innovative approach to bringing young people to museums was a hallmark of the directorship of Roy Strong and was subsequently emulated by some other British museums. | Question: Which musical group did the V&A present in July 1973 as part of its youth outreach programme?', ['Gryphon'], 'mrqa_squad-validation-5360'), tensor(0.0708)), (('Context: Although partly functional , weather vanes are generally decorative , often featuring the traditional cockerel design with letters indicating the points of the compass . Other common motifs include ships , arrows and horses . Not all weather vanes have pointers . When the wind is sufficiently strong , the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing . | Question: where does the arrow of a wind vane point ?', ['the direction from which the wind is blowing'], 'mrqa_naturalquestions-validation-9688'), tensor(0.0707)), (('Context: Learn and talk about Family Fortunes, 1980 British ...Learn and talk about Family Fortunes, 1980 British television programme debuts, 1980s British television series, 1990s British television series, 2000s British television series  All Star Family Fortunes  Family Fortunes is a British television game show based on the American game show Family Feud . The programme ran on ITV from 6 January 1980 to 6 December 2002, before being revived by the same channel in 2006 under the title of All Star Family Fortunes . The game involves two families providing answers to \'everyday questions\' that were surveyed by 100 members of the British public before the show (e.g. \'Name something usually done in the dark\') to win cash prizes (and sometimes mystery prizes for giving a correct answer). The top answers to the surveys are displayed on a large electronic board, originally known as "Mr. Babbage", which famously sounds a wrong answer "Eh-uhh" sound effect and its accompanying X to signal the strike, as well as a "ding" for a right answer).  Contents  11 External links  Hosts and presentation[ edit ]  Family Fortunes was first hosted by comedian Bob Monkhouse (1980–83) then by singer and entertainer Max Bygraves (1983–85). After being rested for the whole of 1986, the show returned on 27 June 1987 with Les Dennis as presenter, and had a consistently successful run for the next 15 years. It was then moved out of peak time and became a daily daytime show, hosted by Andy Collins , but it only had a short run in this format before being axed.  In 2006, the series was revived with Vernon Kay as host, and was renamed All Star Family Fortunes , as each team consisted of a celebrity and four family members. The show was also transmitted back to peak time. Prize money goes to a charity of the celebrity\'s choice, and contestants being either celebrity families, or a group of actors famous for playing a fictional family. Several Christmas specials of All Star Family Fortunes have aired as well.  The most iconic aspects of the show are the large computer screen, named "Mr Babbage" by original host Bob Monkhouse and the famous computerised "Eh-uhh" sound used when wrong answers are given. Both were originally designed to appear high-tech but have since become fondly regarded for being quite the opposite (as compared to the original US Feud, which has used a video board since its 1999 revival). The computer screen name "Mr Babbage" was in recognition to the English mathematician, philosopher, inventor, and mechanical engineer who originated the concept of a programmable computer, Charles Babbage .  Format[ edit ]  Two family teams, each with five members, are asked to guess the results of surveys, in which 100 people would be asked open ended questions (e.g. "we asked 100 people to name something associated with the country Wales" or "we asked 100 people to name a breed of dog"). Although rarely acknowledged in the show, the 100 people surveyed are invariably audience members who have volunteered before the show.  Each round begins with a member of each family (in rotation, meaning all players do this at least once) approaching the podium. As the question is read, the first of the two nominees to hit a buzzer gives an answer. If this is not the most popular answer, the other nominee is asked. The family with the more popular answer then chooses whether to "play" the question, or "pass" control to the other family.  The host then passes down the line of the controlling team, asking for an answer from each. After each answer, the board reveals whether this answer featured. If not, the family is assessed a strike, and the family loses control of the board after accumulating three strikes (also referred as striking out) in the round. If a family manages to come up with all the survey answers (most commonly six in the early part of the show, reduced in number after the commercial break) before striking out, they win the amount in pounds of the total | Question: Who presented Family Fortunes in the two years between Bob Monkhouse and Les Dennis?', ['max bygraves'], 'mrqa_triviaqa-validation-287'), tensor(0.0703)), (('Context: Alex the Dog  Alex the Dog was the advertising mascot for Stroh\'s beer in the 1980s and precursor to Budweiser\'s Spuds MacKenzie.  At the peak of his career, Alex appeared in parades, on "Good Morning America", and the "Today" show.  He even inspired a series of toys, posters, cologne, shampoo and hand lotion.  Hip-hop artist Tone Loc referenced Alex the Dog in his song "Funky Cold Medina".   Spuds MacKenzie  Spuds MacKenzie is a fictional dog character created for use in an extensive advertising campaign marketing Bud Light beer in the late 1980s.  The Spuds MacKenzie mascot and campaign were created by a 23-year-old art director, Jon Moore.  At the time he was working at Needham, Harper & Steers, a Chicago, Illinois, advertising agency.  The dog first showed up in a Bud Light Super Bowl XXI ad in 1987.  During the height of his popularity, large amounts of Spuds merchandise was available, such as plush toys and t-shirts. | Question: When Budweiser created a dog mascot to promote Bud Light in the 1980s, which rival brewer (with its own character Alex the Dog) were they competing with?', ["Stroh's"], 'mrqa_hotpotqa-validation-5802'), tensor(0.0629)), (("Context: 1932 Allan Cup  The 1932 Allan Cup was won by the Toronto National Sea Fleas.  This team went on to represent Canada at the 1933 World Ice Hockey Championships held in Prague, Czechoslovakia where the team lost the final game to the United States in overtime to capture the silver medal for Canada.   Overtime (ice hockey)  Overtime is a method of extending an ice hockey game when the scores are tied after regulation.  The two main methods of extending the game are the overtime period (commonly referred to as overtime) and the shootout.  Depending upon league rules, the game's winning team may or may not be necessarily determined. | Question: What other method of extending an ice hockey game exists other than the one used in the 1932 Allan Cup?", ['the shootout'], 'mrqa_hotpotqa-validation-2263'), tensor(0.0511)), (('Context: To remedy the causes of the fire, changes were made in the Block II spacecraft and operational procedures, the most important of which were use of a nitrogen/oxygen mixture instead of pure oxygen before and during launch, and removal of flammable cabin and space suit materials. The Block II design already called for replacement of the Block I plug-type hatch cover with a quick-release, outward opening door. NASA discontinued the manned Block I program, using the Block I spacecraft only for unmanned Saturn V flights. Crew members would also exclusively wear modified, fire-resistant Block II space suits, and would be designated by the Block II titles, regardless of whether a LM was present on the flight or not. | Question: What eventually happened to the Block I program after the incident?', ['unmanned Saturn V flights', 'discontinued'], 'mrqa_squad-validation-3971'), tensor(0.0474)), (("Context: Album Review: Ringo Starr Postcards From ParadiseAlbum Review: Ringo Starr Postcards From Paradise  Album Review: Ringo Starr Postcards From Paradise  Views 32By Kevin Kearns 1 Comment  Shares 0  Facebook Share on facebook Twitter Tweet it Google+ Share on google+ Linkedin Share on linkedin Reddit Read it on Reddit Whatsapp Share on whatsapp Share on Email Mail it  +  The peace-and-love guru is back with his eighteenth studio release. Postcards From Paradise was produced by Ringo and recorded at his home studio in Los Angeles, and, as all his previous solo albums, features a little help from his friends and family. Among the contributors this time are Joe Walsh, Benmont Tench, Dave Stewart, Richard Marx, Peter Frampton, Nathan East, and Glen Ballard. Unlike previous Ringo recordings, however, Postcards includes one song, “Island in the Sun,” written and recorded by Ringo and all the members of his All Starr Band: Steve Lukather, Todd Rundgren, Gregg Rolie, Richard Page, Warren Ham, and Gregg Bissonette, who plays percussion, trumpet and steel drums on the track.  As usual, if you’re expecting a “drummy” album featuring crazy fills and odd time signatures, you’d be better off looking elsewhere. What you will get is Ringo having fun and laying down the formula that he’s perfected for the past fifty years, playing exactly what’s right for the song, no more and no less.  The album kicks off with “Rory and the Hurricanes,” a lyrical trip down memory lane going back to Ringo’s first band. On the title track Ringo and Rundgren pay tribute to the drummer’s next band—yup, the one he changed the world with—by creating the lyrics out of the titles of many of the Beatles’ most famous songs. Ringo, who also plays keyboards on this track, lays down the on-the-money groove we’ve all loved for so long, which is made even tastier by the amazing drum sound heard throughout the entire record. (Kudos to engineer Bruce Sugar.) The track “Touch and Go” will also touch a nerve with Beatle fans—you can practically see him bopping his head as he’s playing, as this track comes the closest on the record to a swinging “mop-top” beat.  On “You Bring the Party Down,” co-written with Toto’s Steve Lukather, Ringo goes back and forth between a reggae-like feel and a driving rock beat, adding well-timed timbale overdubs. (Like his past few releases, Ringo lays heavy on his reggae influence throughout, including on the tracks “Island in the Sun,” “Right Side of the Road,” and “Bridges.”) On “Bamboula,” Ringo lets loose on a syncopated New Orleans–inspired snare/tom rhythm, with more overdubbed percussion, while on the track “Confirmation” he plays a laid-back Motown groove. Then on “Let Love Lead,” he channels a Charlie Watts vibe and throws in a few unexpected upbeat cymbal crashes, and after the lead guitar break kicks into a snare/tom groove with a beautiful buzz roll.  Postcards from Paradise is officially out on March 31. For more on Ringo visit ringostarr.com . (Universal Music) Billy Amendola  Tagged With album reviews Benmont Tench Dave Stewart Glen Ballard Gregg Bissonette Gregg Rolie Joe Walsh Nathan East Peter Frampton Postcards From Paradise Richard Marx Richard Page Ringo Starr Steve Lukather Todd Rundgren Warren Ham  Shares 0  Ringo Starr ill, cancels San Francisco show - AXSRingo Starr ill, cancels San Francisco show - AXS  Ringo Starr ill, cancels San Francisco show  By: Lori Melton AXS Contributor Mar 13, 2015  67 475401 16715476 6 y2015m03d13  44075  Colin Hay YouTube  Former Beatle Ringo Star is canceling tonight’s March 13 show at the SFMasonic Hall in San Francisco, according to a tweet from the venue which reads: “Due to illness, tonight's Ringo Starr & His All Starr Band concert has been postponed &rescheduled for Oct 1.”  Starr and his All-Starr band also canceled their | Question: Whose eighteenth studio album released in March this year is Postcards from Paradise?", ['ringo starr', 'ringo'], 'mrqa_triviaqa-validation-6402'), tensor(0.0471)), (('Context: Many residents of metropolitan regions work within the central urban area , and choose to live in satellite communities called suburbs and commute to work via automobile or mass transit . Others have taken advantage of technological advances to work from their homes . These processes often occur in more economically developed countries , especially in the United States , which is believed to be the first country in which the majority of the population lives in the suburbs , rather than in the cities or in rural areas . Proponents of containing urban sprawl argue that sprawl leads to urban decay and a concentration of lower income residents in the inner city . | Question: suburbanization of more developed countries is mostly due to ?', ['Many residents of metropolitan regions work within the central urban area , and choose to live in satellite communities called suburbs and commute to work'], 'mrqa_naturalquestions-validation-8448'), tensor(0.0399)), (('Context: Esther Schapira  Schapira is co-author of "The Act of Alois Brunner", and producer of two award-winning documentaries, "Drei Kugeln und ein totes Kind" ("Three bullets and a dead child") (2002), about the death of Muhammad al-Durrah in Gaza in 2000, and "Der Tag, als Theo van Gogh ermordet wurde" ("The day Theo van Gogh was murdered") (2007), about the killing in 2004 of Dutch filmmaker, Theo van Gogh.  The latter won her and her co-producer, Kamil Taylan, a Prix Europa award.  In 2009, she produced a second documentary about the death of al-Durrah, "Das Kind, Der Tod, und Die Wahrheit" ("The Child, the Death, and the Truth").   Alois Brunner  Alois Brunner (8 April 1912 – 2001 or 2010) was an Austrian "Schutzstaffel" (SS) officer who worked as Adolf Eichmann\'s assistant.  Brunner is held responsible for sending over 100,000 European Jews to ghettos and internment camps in eastern Europe.  He was commander of the Drancy internment camp outside Paris from June 1943 to August 1944, from which nearly 24,000 people were deported. | Question: When was the Austrian "Schutzstaffel" (SS) officer born based on who\'s life Schapira co-authored a book?', ['8 April 1912'], 'mrqa_hotpotqa-validation-3978'), tensor(0.0345)), (('Context: Individual Huguenots settled at the Cape of Good Hope from as early as 1671 with the arrival of François Villion (Viljoen). The first Huguenot to arrive at the Cape of Good Hope was however Maria de la Queillerie, wife of commander Jan van Riebeeck (and daughter of a Walloon church minister), who arrived on 6 April 1652 to establish a settlement at what is today Cape Town. The couple left for the Far East ten years later. On 31 December 1687 the first organised group of Huguenots set sail from the Netherlands to the Dutch East India Company post at the Cape of Good Hope. The largest portion of the Huguenots to settle in the Cape arrived between 1688 and 1689 in seven ships as part of the organised migration, but quite a few arrived as late as 1700; thereafter, the numbers declined and only small groups arrived at a time. | Question: The number of new Huguenot colonists declined after what year?', ['1700'], 'mrqa_squad-validation-3113'), tensor(0.0312)), (('Context: Gothard Wilhelm Butler  Gothard Wilhelm Butler (German: "Gotthard Wilhelm von Buttlar" , c. 1600 – January 18, 1660) was a Polish-Lithuanian nobleman and politician of Scottish origin, born in Kuldīga (Goldingen).  He was Grand treasurer of the Crown, the Crown court chamberlain and a captain of the guard of King John II Casimir Vasa and erderman of Prienai, Parnu and Bolesław.   John II Casimir Vasa  John II Casimir (Polish: "Jan II Kazimierz Waza" ; German: "Johann II.  Kasimir Wasa" ; Lithuanian: "Jonas Kazimieras Vaza" ; 22 March 1609 – 16 December 1672) was King of Poland and Grand Duke of Lithuania during the era of the Polish–Lithuanian Commonwealth, Duke of Opole in Upper Silesia, and titular King of Sweden 1648–1660.  In Poland, he is known and commonly referred as Jan Kazimierz.  His parents were Sigismund III Vasa (1566–1632) and Constance of Austria (1588–1631).  His older brother, and predecessor on the throne, was Władysław IV Vasa. | Question: By what name is the King that Gothard Wilhelm Butler was captain of the guard for known in Poland?', ['Jan Kazimierz'], 'mrqa_hotpotqa-validation-3632'), tensor(0.0286)), (('Context: Amphitrite in Greek Mythology | MythographyAmphitrite in Greek Mythology | Mythography  Amphitrite in Greek Mythology  Home | Greek Gods | Free Spirits | Amphitrite  In Greek mythology, Amphitrite was a sea goddess. Sources state that she was the daughter of Nereus and Doris, which makes Amphitrite one of the fifty sea goddesses (or nymphs) who are known collectively as the Nereids. The Greek poet Hesiod lists the names of these sea nymphs in his Theogony:  “To Nereus and Doris of the lovely hair,  the daughter of Okeanos, the stream surrounding the earth,  a host of godly daughters was born in the barren sea:  Proto, Eukrante, Amphitrite, and Sao…”  (Hesiod, Theogony, lines 240-43)  According to the legend, the Olympian Poseidon (who was himself a god of the sea) saw Amphitrite one day and fell in love with the graceful and beautiful goddess. However, the sea nymph initially resisted Poseidon’s advances. But Poseidon was not easily discouraged. He sent his sea creatures to find Amphitrite. A dolphin succeeded in locating the Nereid, and it was this creature - the dolphin - who persuaded her to consider Poseidon’s proposal. Eventually, Amphitrite accepted Poseidon, and the two gods were married.  Amphitrite and Poseidon were together the parents of several children, including the sea god Triton. Here’s Hesiod again:  “From the union of rumbling Poseidon and Amphitrite  came the great Triton, whose might is far-flung,  an awesome god dwelling in a golden house that lies  at the sea’s bottom, near his cherished mother and lordly father.”  (Hesiod, Theogony, lines 930 ff. )  Amphitrite Goddess of the Sea - Greek Gods and GoddessesAmphitrite Goddess of the Sea  [ ? ]Subscribe To This Site  Amphitrite Goddess of the Sea  In Greek mythology, Amphitrite goddess of the Sea was one of the fifty Nereids, that is a daughter of Nereus and Doris - that\'s what Hesiod writes in his Theogony (but Apollodorus says her parents were Oceanus and Thetys). Her Roman equivalent was Salacia, god Neptune\'s wife.  Her name means "The third one who encircles (everything)" (because the ancient Greeks thought the whole world was encircled by a river-god, Oceanus ).  She spent most of the time singing and dancing with her sisters. One day Poseidon, god of the sea , saw her near the island of Naxos and fell in love with her. He kidnapped her and made her his wife.  In another story, Amphitrite, being a shy girl, fled away and hid somewhere in the Atlantic Ocean. Poseidon wanted desperately to find her, so he sent all the marine creatures to look for her. Delphinus, a dolphin, managed to find her and convinced her to go back to Poseidon and marry him. That\'s what made Amphitrite goddess of the sea. (By the way, the dolphin who found her was turned into a constellation by the grateful Poseidon ).  Amphitrite goddess of the sea has no special attributions (but sometimes she can calm the waters), she is just the mistress of the sea (and sometimes she is considered the mother of the marine creatures, but they existed even before she became queen of the sea). Some even say that she is just a personification of the sea.  She also appeared briefly in the story of Theseus. King Minos asked the hero to prove that he really was Poseidon\'s son. A ring was thrown into the sea and Theseus had to bring it back. The sea creatures found the ring and gave it to the hero, while Amphitrite goddess of the sea received him in her underwater palace and gave him a golden crown, which he took to Ariadne as a wedding gift. (As you can see, Amphitrite was the ideal wife, as she was never jealous of her husband\'s love affairs.)  Amphitrite and Poseidon had a son, Triton (a fish-man or a he-mermaid) and a daughter, Rhode, and maybe another daughter, Benthesicyme.  In art | Question: In Greek mythology, Amphitrite, queen of the sea, was the wife of which god?', ['poseidon'], 'mrqa_triviaqa-validation-671'), tensor(0.0285)), (('Context: The Young Ones (TV series)The Young Ones is a British sitcom, broadcast in the United Kingdom from 1982 to 1984 in two six-part series. Shown on BBC2, it featured anarchic, offbeat humour which helped bring alternative comedy to television in the 1980s and made household names of its writers and performers. In 1985, it was shown on MTV, one of the first non-music television shows on the fledgling channel. In a 2004 poll, it ranked at number 31 in the BBC\'s list of Britain\'s Best Sitcoms.  History  The series originated on London\'s comedy club circuit in the early 1980s, where most of the cast had gained popularity at The Comedy Store. Alexei Sayle was the prominent act, drawing attention as the manic, aggressive compere. Adrian Edmondson and Rik Mayall worked as the double act 20th Century Coyote, which later became The Dangerous Brothers. Nigel Planer was in a double act with Peter Richardson called "The Outer Limits".   As The Comedy Store became popular, Sayle, 20th Century Coyote, and The Outer Limits, with French and Saunders and Arnold Brown, set up their own club called The Comic Strip in the Raymond Revuebar club in Soho.  The Comic Strip became one of the most popular comedy venues in London, and came to the attention of Jeremy Isaacs of Channel 4. Peter Richardson then negotiated a deal for six self-contained half-hour films, using the group as comedy actors rather than stand-up performers. In response, the BBC began negotiations with Edmondson, Mayall, Richardson, Planer and Sayle to star in a sitcom in a similar style. Paul Jackson was installed as a producer.  Richardson\'s project, The Comic Strip Presents..., aired on Channel 4\'s opening night on 2 November 1982, with The Young Ones following a week later on BBC2.  The series was written by Mayall, his then-girlfriend Lise Mayer, and Ben Elton (who had attended the University of Manchester with Mayall and Edmondson). Richardson was originally set to play Mike, but clashed with Jackson. He was replaced by Christopher Ryan, the only member of the group who was not a stand-up comedian.  The show was voted number 31 in the BBC\'s Best Sitcom poll in 2004.  Setting  The main characters were four undergraduate students who were sharing a house: aggressive punk Vyvyan (Adrian Edmondson), conceited wannabe anarchist Rick (Rik Mayall), oppressed paranoid hippie Neil (Nigel Planer), and the suave, charming Mike (Christopher Ryan). It also featured Alexei Sayle, who played various members of the Balowski family—most often Jerzei Balowski, the quartet\'s landlord—and occasional independent characters, such as the train driver in "Bambi" and the Mussolini-lookalike Police Chief in "Cash".  The show combined traditional sitcom style with violent slapstick, non-sequitur plot turns, and surrealism. These older styles were mixed with the working and lower-middle class attitudes of the growing 1980s alternative comedy boom, in which all the principal performers except Ryan had been involved. Every episode except one featured a live performance by a band, including Madness, Motörhead, and The Damned.  This was a device used to qualify the series for a larger budget, as "variety" shows attracted higher fees than "comedy".  Synopsis  Stories were set in a squalid house where the students lived during their time at Scumbag College. It can be classified as a comedy of manners.  When it was first broadcast, the show gained attention for its violent slapstick, which Mayall and Edmondson had been using in 20th Century Coyote for some time. The show also featured surreal elements, such as puppets playing talking animals or objects. Confusion was added with lengthy cutaways with no relation to the main plot.  Throughout the series, the fourth wall was frequently broken for comedic effect by all characters at various parts of the show. The wall was usually broken as either a punchline to a joke or to make a plot point more obvious. On several occasions, Alexei Sayle broke both the fourth wall and character | Question: "Who played Vyvyan, a psychotic punk metal medical student with orange-dyed & spiked hair and four metal stars embedded in his forehead, in the UK TV series ""The Young Ones""?" ?', ['adrian edmondson'], 'mrqa_triviaqa-validation-3339'), tensor(0.0277)), (("Context: <Ol>  `` The Last Note of Freedom '' - David Coverdale   `` Deal for Life '' - John Waite   `` Break Through the Barrier '' - Tina Turner   `` Hearts in Trouble '' - Chicago   `` Trail of Broken Hearts '' - Cher   `` Knockin ' on Heaven 's Door '' - Guns N ' Roses   `` You Gotta Love Someone '' - Elton John   `` Show Me Heaven '' - Maria McKee   `` Thunderbox '' - Apollo Smile   `` Long Live the Night '' - Joan Jett & The Blackhearts   `` Gimme Some Lovin ' '' - Terry Reid ( Spencer Davis Group version appears in movie )  </Ol> | Question: who sings gim me some lovin in days of thunder ?", ['Spencer Davis Group'], 'mrqa_naturalquestions-validation-539'), tensor(0.0268)), (('Context: The rainforest contains several species that can pose a hazard. Among the largest predatory creatures are the black caiman, jaguar, cougar, and anaconda. In the river, electric eels can produce an electric shock that can stun or kill, while piranha are known to bite and injure humans. Various species of poison dart frogs secrete lipophilic alkaloid toxins through their flesh. There are also numerous parasites and disease vectors. Vampire bats dwell in the rainforest and can spread the rabies virus. Malaria, yellow fever and Dengue fever can also be contracted in the Amazon region. | Question: What fish living in the Amazon river is known to bit humans?', ['piranha'], 'mrqa_squad-validation-4185'), tensor(0.0260)), (("Context: The first Christmas cards were commissioned by Sir Henry Cole and illustrated by John Callcott Horsley in London on 1st May 1843 . The central picture showed three generations of a family raising a toast to the card 's recipient : on either side were scenes of charity , with food and clothing being given to the poor . Allegedly the image of the family drinking wine together proved controversial , but the idea was shrewd : Cole had helped introduce the Penny Post three years earlier . Two batches totaling 2,050 cards were printed and sold that year for a shilling each . | Question: who commissioned the first christmas card in 1943 ?", ['commissioned by Sir Henry Cole and illustrated by John Callcott Horsley in London on 1st May 1843'], 'mrqa_naturalquestions-validation-3189'), tensor(0.0255)), (("Context: A steam turbine consists of one or more rotors (rotating discs) mounted on a drive shaft, alternating with a series of stators (static discs) fixed to the turbine casing. The rotors have a propeller-like arrangement of blades at the outer edge. Steam acts upon these blades, producing rotary motion. The stator consists of a similar, but fixed, series of blades that serve to redirect the steam flow onto the next rotor stage. A steam turbine often exhausts into a surface condenser that provides a vacuum. The stages of a steam turbine are typically arranged to extract the maximum potential work from a specific velocity and pressure of steam, giving rise to a series of variably sized high- and low-pressure stages. Turbines are only efficient if they rotate at relatively high speed, therefore they are usually connected to reduction gearing to drive lower speed applications, such as a ship's propeller. In the vast majority of large electric generating stations, turbines are directly connected to generators with no reduction gearing. Typical speeds are 3600 revolutions per minute (RPM) in the USA with 60 Hertz power, 3000 RPM in Europe and other countries with 50 Hertz electric power systems. In nuclear power applications the turbines typically run at half these speeds, 1800 RPM and 1500 RPM. A turbine rotor is also only capable of providing power when rotating in one direction. Therefore, a reversing stage or gearbox is usually required where power is required in the opposite direction.[citation needed] | Question: What is another term for rotors?", ['rotating discs'], 'mrqa_squad-validation-3467'), tensor(0.0250)), (('Context: Bodhi Natural Health Products | Bodhi Natural ProductsBodhi Natural Health Products | Bodhi Natural Products  Stockists  Welcome  “It is my wish that by using these products you can enjoy life to the fullest by having good health, hopefully relieve suffering, and of course protecting our beautiful planet by using only natural substances. All Bodhi products come from nature, and are organic”\xa0 Julie Herbison  Bodhi originated as Body Mind Balancing which started off as my business as a Natural Health Practitioner. Over time my products became more and more popular so the name Bodhi came about to keep it short and simple. Bodhi comes from the Bodhi Tree which is a type of fig tree. The Bodhi tree has played an important part in human history featuring strongly in religious history and mythology in various parts of the world.  The ficus religiosa, Bodhi tree, has large heart shaped leaves. It is one of the most sacred trees in India, Sri Lanka and Nepal, where it is venerated by both Hindus and Buddhists. The tree under which Siddhartha Gautama gained enlightenment over 2,600 years ago still grows today in North East India.  In Hindu religion, Vishnu is said to have been born under a Bodhi tree and is often depicted sitting on its heart-shaped leaves. The Bodhi tree is often planted in the grounds of temples and, of all the sacred trees of India, it is the most widely worshiped.  The Bodhi tree in Sri Lanka is located in Anuradhapura and is said to be the oldest tree in the world with a known planting date. This fig tree is said to have grown from a branch taken from the original Bodhi tree in India under which Buddha gained enlightenment.  For Homeopathic and Body Mind consultations please call  Julie Herbison  Bodhi Tree, Bodh Gaya - Sacred SitesBodhi Tree, Bodh Gaya  Bodhi Tree, Bodh Gaya  Buddhist Monks at Bodhi Tree (The site of Buddha\'s enlightenment) \xa0 \xa0\xa0  Bodh Gaya, located 100 km (62 mi) south of Patna in the Indian state of Bihar, is the most venerated sacred place in Buddhism. It is the place where Prince Siddhartha Guatama, while meditating beneath the Bodhi Tree, attained enlightenment and became the Buddha.  Traditional accounts say that, in the early years of the 4th century BC, Siddhartha Gautama saw the suffering of the world and wanted to be free from it. As a young man, following the ancient traditions of Hinduism, he sought out spiritual teachers. Inquiring of their knowledge, he diligently practiced various yogas and meditations. Seven years passed, the last three in extreme asceticism, yet still he had not achieved his goal of enlightenment.  Impression of Buddha feet, Bodh Gaya \xa0\xa0\xa0  Siddhartha then journeyed toward the ancient sacred forests of Uruvela (modern Gaya in Bihar, in north India) with the intention of finally and completely realizing the infinite. Guided by visionary dreams and following in the footsteps of the Buddhas of three previous ages, Krakucchanda, Kanakamuni and Kasyapa (who had each attained enlightenment at the site) Siddhartha sat beneath the Bodhi Tree. Touching the earth, thereby calling it to witness the countless lifetimes of virtue that had led him to this place of enlightenment, he resolved not to rise again until enlightenment was attained.  "Here on this seat my body may shrivel up, my skin, my bones, my flesh may dissolve, but my body will not move from this seat until I have attained Enlightenment, so difficult to obtain in the course of great periods of time".  As Siddhartha sat in deep meditation beneath the Bodhi Tree, Mara, the Dark Lord of Death, came to distract him from his endeavor. When the earth shook, confirming the truth of Gautama\'s words, Mara unleashed his army of demons. In the epic battle that ensued, Siddhartha\'s wisdom broke through Mara’s illusions. The power of his compassion transformed the demons\' weapons into flowers and Mara and all his forces fled. Three days and nights passed and Siddhartha’s intention was realized. He became the Buddha, meaning the ‘Enlightened One’.  The Mahabodhi Temple, Bodh Gaya, India \xa0 \xa0\xa0  The Buddha then spent the | Question: Who is said to have gained enlightenment sitting under the Bodhi Tree?', ['buddha', 'enlightened one'], 'mrqa_triviaqa-validation-3915'), tensor(0.0250)), (("Context: The Roman Catholic holy day of All Saints ( or All Hallows ) was introduced in the year 609 , but was originally celebrated on 13 May . In 835 , Louis the Pious switched it to 1 November in the Carolingian Empire , at the behest of Pope Gregory IV . However , from the testimony of Pseudo-Bede , it is known that churches in what are now England and Germany were already celebrating All Saints on 1 November at the beginning of the 8th century . Thus , Louis merely made official the custom of celebrating it on 1 November . James Frazer suggests that 1 November was chosen because it was the date of the Celtic festival of the dead ( Samhain ) -- the Celts had influenced their English neighbours , and English missionaries had influenced the Germans . However , Ronald Hutton points out that , according to Óengus of Tallaght ( d . ca . 824 ) , the 7th / 8th century church in Ireland celebrated All Saints on 20 April . He suggests that the 1 November date was a Germanic rather than a Celtic idea . In the 11th century , 2 November became established as All Souls ' Day . This created the three - day observance known as Allhallowtide : All Hallows ' Eve ( 31 October ) , All Hallows ' Day ( 1 November ) , and All Souls ' Day ( 2 November ) . | Question: the christian church used which holiday to take attention away from samhain ?", ['All Saints ( or All Hallows )'], 'mrqa_naturalquestions-validation-8545'), tensor(0.0236)), (("Context: Describing 58-facet Round Brilliant-Cut Diamonds at GIA ...Describing 58-facet Round Brilliant-Cut Diamonds at GIA | Research & News  Duncan Pay  December 20, 2013  Today’s round brilliant-cut diamonds showcase the gem’s sparkling optics. They are the most recent expression of a cutting style that has been around for centuries. Photo by Eric Welch, © GIA, courtesy of Signed Pieces, New York  Introduction  Most consumers in today’s marketplace choose round brilliant-cut diamonds fashioned to meet modern concepts of beauty. These sparkling gems represent the collective talents of generations of skilled cutters over more than six centuries.  As diamond cuts evolved into today’s 57- or 58-facet round brilliants, styles progressed through many stages. By 1750, when the brilliant cut had developed into a style with a circular face-up outline, it had passed through many variations in facet size and proportions. These included table size, crown height, length of the lower half facets, total depth, and culet size.  This illustration shows the names and arrangement of the facets of a standard round  brilliant-cut. Illustration by Peter Johnston, © GIA  This progression has resulted in diamonds from different periods having quite distinct appearances. Over time, table facets have become larger, culets smaller, and star facets and lower half facets longer. If you rock or tilt one of the older-style gems, you’ll see a different face-up pattern of light and dark than you would with a contemporary brilliant-cut.  Transition to Modern Brilliants  Play  This video shows three styles of brilliants with round face-up outlines. From left to right, they are an old European cut, in place by 1750; a “transitional” brilliant from around 1880; and a modern round brilliant, in place since the 1950s. The transitional cut is labeled “circular brilliant” in the video; this is the same term now used by GIA on grading reports.The three animations show how cutting styles developed over time, causing the different face-up patterns associated with each proportion set. Animations by Al Gilbertson, © GIA  Many beautiful older diamonds present a conundrum for diamond-grading laboratories—including GIA's—because their proportions diverge from contemporary conventions. This makes it difficult to judge them against today’s aesthetic. In spite of this, there is a market for their antique beauty. In fact, some consumers cherish their different interplay with light.  Compared to a modern round brilliant cut, this older-style 1.07-ct. brilliant has a much smaller table (38.5 percent), a higher crown, shorter lower halves, and a much larger culet. Digitally modified image by Al Gilbertson, © GIA, courtesy of Michael Goldstein  Compared to a modern brilliant, older gems typically show larger face-up patterns of light and dark—what might be described as a “checkerboard” or “blocky” pattern. A modern brilliant can display a tighter mosaic of light and dark that might be described as “splintery” by those who prefer older cutting styles.  Many older round brilliants were cut between the 1870s and the 1940s. Because some served as transitions between older and more-modern styles, some trade professionals call them “transitional cuts.”  Recent high prices for precious metals have caused many pieces containing older-style diamonds to be broken up, releasing more of these beautiful gems into the market and increasing appreciation for their historic nature.  This article explains GIA’s approach to describing these beautiful older-style brilliant-cut diamonds.  The GIA Cut-Grading System for Unmodified Brilliant-Cut Diamonds  In 2005, GIA introduced a cut-grading system for unmodified round brilliant cut diamonds. The name “unmodified round brilliant” was applied to a symmetrical round cut with a regular array of 58 facets, cut to modern standards. The system was the culmination of years of research that included the results of preference testing among a wide variety of groups, including trade professionals and consumers.  GIA designed the system to assess the design quality and craftsmanship of diamonds cut in the standard round brilliant style when compared to modern preferences. | Question: A modern round brilliant cut has 58 what?", ['facets'], 'mrqa_triviaqa-validation-3808'), tensor(0.0214)), (("Context: One of the best - known 18th Dynasty pharaohs is Amenhotep IV , who changed his name to Akhenaten in honor of the Aten , a representation of the Egyptian god , Ra . His exclusive worship of the Aten is often interpreted as history 's first instance of monotheism . Akhenaten 's wife , Nefertiti , contributed a great deal to his new take on the Egyptian religion . Nefertiti was bold enough to perform rituals to Aten . Akhenaten 's religious fervor is cited as the reason why he and his wife were subsequently written out of Egyptian history . Under his reign , in the 14th century BC , Egyptian art flourished under a distinctive style . ( See Amarna Period . ) | Question: what was the most important new religious figure of the new kingdom of ancient egypt ?", ['the Aten , a representation of the Egyptian god , Ra'], 'mrqa_naturalquestions-validation-6896'), tensor(0.0211)), (('Context: PRI is the standard for providing telecommunication services to enterprises and offices . It is based on T - carrier ( T1 ) transmission in the US , Canada , and Japan , while the E-carrier ( E1 ) is common in Europe and Australia . The T1 line consists of 23 bearer ( B ) channels and one data ( D ) channel for control purposes , for a total bandwidth of 24x64 - kbit / s or 1.544 Mbit / s . The E1 carrier provides 30 B - and two D - channels for a bandwidth of 2.048 Mbit / s . The first timeslot on the E1 is used for synchronization purposes and is not considered to be a B - or D - channel . The D - channel typically uses timeslot 16 on an E1 , while it is timeslot 24 for a T1 . Fewer active bearer channels , sometimes called user channels , may be used in fractional T1 or E1 services . | Question: isdn uses b & d channels . what is d channel use for ?', ['data'], 'mrqa_naturalquestions-validation-9650'), tensor(0.0177)), (('Context: Quran (Complete) - YouTubeQuran (Complete)  - YouTube  Quran (Complete)  38,356 views  Last updated on Jun 30, 2014  The Qur\'an (English pronunciation: /kɒˈrɑːn/ kor-AHN; Arabic: القرآن\u200e al-qur\'ān, IPA: [qurˈʔaːn], literally "the recitation") is the religious text of Islam,[ also sometimes transliterated as Quran, Kuran, Koran, Qur\'ān, Coran or al-Qur\'ān. It is widely regarded as the finest piece of literature in the Arabic language. Muslims hold that the Qur\'an is the verbal divine guidance and moral direction for mankind. Muslims also consider the original Arabic verbal text to be the final revelation of God the Final Testament  Quran: 78. Surat An Naba\' (The Tidings ) with ... - YouTubeQuran: 78. Surat An Naba\' (The Tidings ) with English Audio Translation and Transliteration HD - YouTube  Quran: 78. Surat An Naba\' (The Tidings ) with English Audio Translation and Transliteration HD  Want to watch this again later?  Sign in to add this video to a playlist.  Need to report the video?  Sign in to report inappropriate content.  The interactive transcript could not be loaded.  Loading...  Rating is available when the video has been rented.  This feature is not available right now. Please try again later.  Published on May 14, 2015  Please support the channel by subscribing, liking and commenting, may Allah reward you with Godness, Ameen https://www.youtube.com/user/kuranreader Islam, Prophet, prophets, Muhammad, Mohamed, Quran, Kuran, Kuranreader, Surat, Chapter, Ayah, Ayat, Sheikh, imam, Emotional, Truth, Emotional, relaxing, Soothing, Recitation, reading, Religion, Religious Text, Amazing , Beautful, Quran English Translation, Transliteration,beautiful quran recitation, quran, koran,kuran reader,quran reader,really beautiful,heart touchig ,relaxing,soothing,heart trembling,good voice, al minshawi, tajweed,emotional,tearful, quran download,surat,surah,most beautiful,sudais, abd albasit,mishari,quran reading ,learn arabic,learn quran,, قراءة, تلاوة , القارىء , مكتوب , مكتوبة, ترجمة , انجليزية,مرتل , مرتل, ,  Quran,Al Quran,the quran,The Holy Quran,Holy Quran,Quran Translation,Al Quran Translation,The Holy Quran Translation,Holy Quran Translation,Quran Bangla,Bangla Quran,Al Quran Bangla,Bangla Al Quran,Quran Bangla Translation,Bangla Quran Translation,Al Quran Bangla Translation,Koran,coran,Surah,Translatio\xadn,Islam,Abdul Rahman Al-Sudais,Imam,quran recitation,urdu quran,full quran,beautiful quran,quran beautiful recitation,urdu quran translation,quran karim,quran with urdu,tilawat,bacaan quran,bacaan al quran,merdu al quran,full al quran,surah al quran,suara merdu,ayat al quran,alquran,tilawah,tilawah al quran,quran recitation beautiful,beautiful quran,al quran recitation,best quran recitation,best quran,the quran recitation,full quran,quran recitation full,kicked the quran,the quran miracles,quran miracles,burning the quran,quran burning,miracles of quran,azab,burn the quran,the quran recitation,the quran full,full quran recitation,sudais full quran,mishary full quran,full quran download,full quran tilawat,beautiful quran recitation,quran recitation,beautiful recitation,most beautiful quran,best quran recitation,quran surah baqarah,quran surah yaseen,quran surah yasin,tilawat quran,quran surah rahman,quran surah kahf,quran surah rehman,quran surah fatiha,quran surah mulk,al quran karim,quran mp3,quran karim mp3,quran karim full,quran karim soudais,quran karim sudais,quran sudais,quran karim urdu,koran karim,coran karim,le coran,coran recitation,al coran,sourate coran,sourate,el coran,soudais,soudais coran,coran francais,coran islam,coran en francais,ecouter le coran,le coran arabe,le coran sourate,coran sourate,recite le coran,apprendre le coran, | Question: The Qur\'an (or Quran, Kuran, Koran, Coran or al-Qur\'an - literally \'the recitation\') is the religious text of which religion?', ['islam'], 'mrqa_triviaqa-validation-2210'), tensor(0.0170)), (('Context: Laparoscopic Cholecystectomy Surgery - Schwartzapfel ...Laparoscopic Cholecystectomy Surgery - Schwartzapfel Lawyers P.C.  Laparoscopic Cholecystectomy Surgery  Laparoscopic Cholecystectomy Surgery  Cholecystectomy: Removal of the Gallbladder  The gallbladder is a small organ that is part of the human digestive system. It secretes bile, an enzyme that breaks down fats in food. Sometimes the gallbladder cannot work effectively, and instead of breaking down fats it begins to form lumps known as gallstones. When this happens, removal of the gallbladder – known as a cholecystectomy – may be necessary.  Traditional vs. laparoscopic cholecystectomy  Traditional cholecystectomy involved making a major incision and dislodging the liver in order to remove the gall bladder. This procedure was risky and required several days in the hospital as well as more than a month of recovery time.  The modern alternative, laparoscopic cholecystectomy, was developed in 1989. This procedure involves the insertion of a camera in a tube through the navel. Several small incisions are made, and the surgeon, guided by the camera, uses tiny instruments to cut the ducts and arteries that hold the gallbladder inside the body. When performed correctly, laparoscopic cholecystectomy poses less risk to the patient’s liver, reduces hospital stays, and cuts recovery time to less than a month. But laparoscopic cholecystectomy is an intricate and difficult procedure. Accidentally cutting or clipping the wrong duct or artery could have disastrous results for the patient.  Doctors performing laparoscopic cholecystectomy without proper experience  The best way for gallbladder patients to prevent complications from laparoscopic cholecystectomy is to choose a doctor who has performed many procedures. The National Institutes of Health became so concerned about the rate of patient injuries following laparoscopic cholecystectomy that a conference was convened to examine the issue. The conference found that training for laparoscopic cholecystectomy has been so irregular that an alarming number of doctors are not sufficiently competent to perform the procedure safely. Doctors with fewer than 25 procedures under their belts were more likely to identify the wrong duct or artery to cut and cause damage to the patient .  Laparoscopic Cholecystectomy complications can be life-threatening  Because the gallbladder is situated close to many other vital organs (most prominently the liver), it is paramount that the surgeon performs the surgery correctly without cutting, clipping, or puncturing any other organs. When complications occur, they can include:  Jaundice  Infection or inflammation of the bile ducts causing digestive and liver problems  Damage to the liver, the intestines, or other nearby organs  Damage to the common bile duct, causing bile to enter the bloodstream or the abdominal cavity, thereby poisoning the patient  If the hepatic (liver) system is disrupted, or if the common bile duct is damaged, the patient is placed in a life-threatening situation that can swiftly degenerate to liver failure and even death. Emergency surgery is often required to correct complications of a laparoscopic cholecystectomy, and, even when it is successful, the patient may be sentenced to a life with permanent pain and impaired digestive function.  Laparoscopic Cholecystectomy malpractice lawsuits  The consequences of a laparoscopic cholecystectomy gone wrong can be devastating. If you or a loved one has suffered due to a doctor’s negligence, you may have a case for medical malpractice. We can help. Please call our experienced team of medical malpractice lawyers at 1-888-575-6410 , or fill out our online contact form for a free case evaluation. We will fight for you!  Medical Malpractice  Cholecystectomy | Laparoscopic Surgery Mesa AZ | Gilbert ...Cholecystectomy |\xa0Laparoscopic Surgery Mesa AZ | Gilbert | Scottsdale  Mesa , AZ Gilbert, AZ Scottsdale, AZ  Laparoscopic Surgery  Laparoscopy is a minimally invasive surgical procedure used to diagnose and treat problems of the genital and pelvic areas. During this procedure, an endoscope (tube) with a camera on the end is inserted through a tiny incision to allow your doctor to closely examine the organs of the area. Surgical instruments can be inserted through additional incisions to treat any identified problems.  Technological advances have brought computers and laparoscopic instruments to the forefront of surgical approaches. This provides patients with a minimally invasive technique that can be utilized in a wide range of procedures.  A laparoscopic procedure may be performed for a number of | Question: Cholecystectomy is the surgical removal of which small organ of the body?', ['gallbladder', 'gallbladder fundus'], 'mrqa_triviaqa-validation-6800'), tensor(0.0164)), (('Context: Pangaea or Pangea ( / pænˈdʒiːə / ) was a supercontinent that existed during the late Paleozoic and early Mesozoic eras . It assembled from earlier continental units approximately 335 million years ago , and it began to break apart about 175 million years ago . In contrast to the present Earth and its distribution of continental mass , much of Pangaea was in the southern hemisphere and surrounded by a superocean , Panthalassa . Pangaea was the most recent supercontinent to have existed and the first to be reconstructed by geologists . | Question: what land mass was north america a part of about 300 million years ago ?', ['Pangaea or Pangea'], 'mrqa_naturalquestions-validation-9386'), tensor(0.0158)), (('Context: Concentrated O2 will allow combustion to proceed rapidly and energetically. Steel pipes and storage vessels used to store and transmit both gaseous and liquid oxygen will act as a fuel; and therefore the design and manufacture of O2 systems requires special training to ensure that ignition sources are minimized. The fire that killed the Apollo 1 crew in a launch pad test spread so rapidly because the capsule was pressurized with pure O2 but at slightly more than atmospheric pressure, instead of the 1⁄3 normal pressure that would be used in a mission.[k] | Question: ______ In both liquid and gas form can fastly result in an exlposion. ?', ['oxygen'], 'mrqa_squad-validation-3478'), tensor(0.0144)), (("Context: The show has received recognition as one of Britain's finest television programmes, winning the 2006 British Academy Television Award for Best Drama Series and five consecutive (2005–2010) awards at the National Television Awards during Russell T Davies' tenure as executive producer. In 2011, Matt Smith became the first Doctor to be nominated for a BAFTA Television Award for Best Actor and in 2016, Michelle Gomez became the first female to receive a BAFTA nomination for the series, getting a Best Supporting Actress nomination for her work as Missy. | Question: What years did Doctor Who win five consecutive awards?", ['2005–2010'], 'mrqa_squad-validation-7816'), tensor(0.0139)), (("Context: The Qur'an admonishes Muslim women to dress modestly and cover their breasts and genitals but it does n't require covering the head . The Qur'an explicitly states that `` O wives of the Prophet , you are not like anyone among women '' and as such has separate rules specifically for the wives of the Prophet . Wearing hijab , or al - khimar , is considered by some to be obligatory in Islam , as the quote states `` Tell the believing women to put their khimar over their bosoms '' , all the major Arab translations and dictionaries agree that `` al - khimar '' means a veil or scarf that a woman uses to cover her head , and `` over their bosoms '' implies women should cover their breasts , necks etc . | Question: what is the name of the muslim dress ?", ['hijab'], 'mrqa_naturalquestions-validation-868'), tensor(0.0136)), (('Context: Many important complexity classes can be defined by bounding the time or space used by the algorithm. Some important complexity classes of decision problems defined in this manner are the following: | Question: What function is used by algorithms to define measurements like time or space?', ['bounding'], 'mrqa_squad-validation-1688'), tensor(0.0118)), (('Context: Newton came to realize that the effects of gravity might be observed in different ways at larger distances. In particular, Newton determined that the acceleration of the Moon around the Earth could be ascribed to the same force of gravity if the acceleration due to gravity decreased as an inverse square law. Further, Newton realized that the acceleration due to gravity is proportional to the mass of the attracting body. Combining these ideas gives a formula that relates the mass () and the radius () of the Earth to the gravitational acceleration: | Question: How might gravity effects be observed differently according to Newton?', ['at larger distances', 'at larger distances.'], 'mrqa_squad-validation-10322'), tensor(0.0118)), (('Context: Walking in the Air  "Walking in the Air" is a song written by Howard Blake for the 1982 animated film of Raymond Briggs\' 1978 children\'s book "The Snowman".  The song forms the centrepiece of "The Snowman", which has become a seasonal favorite on British and Finnish television.  The story relates the fleeting adventures of a young boy and a snowman who has come to life.  In the second part of the story, the boy and the snowman fly to the North Pole.  "Walking in the Air" is the theme for the journey.  They attend a party of snowmen, at which the boy seems to be the only human until they meet Father Christmas with his reindeer, and the boy is given a scarf with a snowman pattern.  In the film, the song was performed by St Paul\'s Cathedral choirboy Peter Auty, and reissued in 1985 (on Stiff Records) and 1987.   Walking in the Air  "Walking in the Air" is a song written by Howard Blake for the 1982 animated film of Raymond Briggs\' 1978 children\'s book "The Snowman".  The song forms the centrepiece of "The Snowman", which has become a seasonal favorite on British and Finnish television.  The story relates the fleeting adventures of a young boy and a snowman who has come to life.  In the second part of the story, the boy and the snowman fly to the North Pole.  "Walking in the Air" is the theme for the journey.  They attend a party of snowmen, at which the boy seems to be the only human until they meet Father Christmas with his reindeer, and the boy is given a scarf with a snowman pattern.  In the film, the song was performed by St Paul\'s Cathedral choirboy Peter Auty, and reissued in 1985 (on Stiff Records) and 1987.   Peter Auty  Peter Robert Auty (born 4 November 1969) is an English operatic tenor who has worked with most of the major opera companies in Britain and a number of companies in continental Europe. | Question: An English operatic tenor performed "Walking in the Air" for which 1982 animated film?', ['The Snowman'], 'mrqa_hotpotqa-validation-1968'), tensor(0.0103)), (('Context: Rumpole of the Bailey - TV.comRumpole of the Bailey - Show News, Reviews, Recaps and Photos - TV.com  Rumpole of the Bailey  EDIT  Welcome to the Rumpole of the Bailey guide at TV.com.  Horace Rumpole (played by the late Leo McKern) is an untidy, ageing London barrister with one glass eye who defends in criminal cases. His clients rarely cut elegant figures. He is fond of red wine, poetry, and fair dealing, and is not looked on as a great success in life by his wife, Hilda (\'She Who Must Be Obeyed\'). Rumpole has had a few triumphs, and the Penge Bungalow murders are often on his mind... Rumpole shares Chambers at Number 3, Equity Court, with a mixed group of barrister colleagues, including Guthrie Featherstone (Peter Bowles) and Phyllida Trant (Patricia Hodge). He also takes pupils - notably Fiona Allways (played by Rosalyn Landor) and Liz Probert. The creator and writer of the series, Sir John Mortimer, received an Edgar Allan Poe Award for crime and mystery for Rumpole.  Horace Rumpole - Thrilling DetectiveHorace Rumpole  Horace Rumpole  Created by John Mortimer (1923-2009 )  "Crime doesn\'t pay, but it\'s a living."  Down those mean streets and meaner chambers a man must waddle...  Lord knows, he\'s not a private eye, but God, I wish he were. Like Philip Marlowe , Rumpole is ""a relatively poor man... a common man or he could not go among common people. He has a sense of character, or he would not know his job. He will take no man\'s money dishonestly and no man\'s insolence without a due and dispassionate revenge. He is a lonely man and his pride is that you will treat him as a proud man or be very sorry you ever saw him. He talks as the man of his age talks -- that is, with a rude wit, a lively sense of the grotesque, a disgust for sham, and a contempt for pettiness."  So, may I submit for your consideration, Your Honour....  That great defender of most muddled and sinful humanity...  With his jowls a-quiver...  His fondness for Wordsworth, Chateau Thames Embankment and hopeless cases...  His cheroot-puffing and claret-quaffing...  His food-bespeckled robe and raggedy wig...  And his beloved and tattered copy of The Oxford Book of English Verse clutched to his bosom...  For his oratorical outbursts...  His always entertaining jabs at the soft underbelly of hypocrisy, pomposity and upper class twits...  And for standing up for truth, justice, honour and the Golden Thread of Justice...  May I submit for inclusion, Your Honour, this most British of all lawyers...  This proud, this defiant Old Bailey Hack...  With his best gal, Hilda, She Who Must Be Obeyed, standing, NOT amused, by his side...  The one, the only...  HORACE RUMPOLE.  May there always be an England, and may there always be Horace Rumpole to see that justice be done. And thev pompous may squirm.  Your Honour, I rest my case.  ****  Rumpole is, of course John Mortimer\'s rotund, defiant British criminal lawyer who, as brilliantly brought to life by the late, great Australian actor Leo McKern, the star of Rumpole of the Bailey, the popular British courtroom comedy/drama that originally aired on Thames Television in 1978, and soon became popular on both sides of the Atlantic, appearring on the American Public Broadcasting Service as part of its Mystery! series.  Mortimer wrote each and every episode of the television series, and their subsequent novelizations. The show ran, off and on, for seventeen years, an incredible run, and inspired not just the short stories, but novels and two radio series. It was only in 1995, with the publication of the Rumpole and the Angel of Death collection, that Mortimer began writing original Rumpole stories (ie: not adapted from his own TV scripts). Sice then, two | Question: Who created Rumpole of the Bailey?', ['john mortimer', 'john mortimer qc'], 'mrqa_triviaqa-validation-1551'), tensor(0.0093)), (('Context: Sunday in the Park with George - Music Theatre InternationalSunday In The Park... | Music Theatre International  Music Theatre International  Request Licenses & Perusals, Pay Invoices  Your Web Profile  We are working quickly to merge our two logins.  Region  Sunday In The Park With George  Original Broadway Version (1984)  Inspired by George Seurat\'s famous painting, this poetic masterpiece explores the challenges in understanding life and art.  Inspired by the painting A Sunday Afternoon on the Island of La Grande Jatte by Georges Seurat, Sunday In The Park With George, Stephen Sondheim and James Lapine\'s stunning masterpiece merges past and present into beautiful, poignant truths about life, love and the creation of art. One of the most acclaimed musicals of our time, this moving study of the enigmatic painter Georges Seurat won a Pulitzer Prize and was nominated for an astounding 10 Tony Awards including Best Musical.  The days leading up to the completion of his most famous painting, A Sunday Afternoon on the Island of La Grande Jatte, Georges Seurat is struggling to make meaningful art and maintaining a relationship with his lover Dot. Amid the scorn of the artistic community, Seurat\'s artistic ability thrives while his love diminishes. A century later, Seurat\'s descendant - named George and also an artist - finds himself burnt out and in search of what artistic path to follow, but he finds the answer to his future in the past.  An ensemble of strong singing actors performs this challenging and heartbreaking work about our need to connect to the past, present and future. Sunday In The Park With George features two coveted starring roles made famous by the Broadway performances of Mandy Patinkin and Bernadette Peters. The show may be staged simply as in its original workshop production or with the grandeur of Seurat\'s masterpiece.  "Sunday in the Park with George" - About.com Education"Sunday in the Park with George"  "Sunday in the Park with George"  "Sunday in the Park with George"  From Canvas to Characters  By Rosalind Flynn  Updated February 27, 2016.  Anyone who reads or plans to attend a performance of Sunday in the Park With George by Stephen Sondheim and James Lapine should first spend some time looking at an image of the this painting by Georges Seurat :\xa0 Its original French title is Un dimanche après-midi à l\'Île de la Grande Jatte – English translation: “ Sunday Afternoon on the Island of la Grande Jatte .” (Click on the painting title to view the online image.)  The first act of Sunday in the Park With George occurs with Seurat the artist creating and perfecting his painting. The creative musical theatre artists who were inspired by this painting drew the other characters in Act One from figures in the painting.  Looking Closely at Art  It would not be surprising to learn that Sondheim and Lapine used their own version of Visual Thinking Strategies to examine this painting that is 10 feet wide and 6 and a half feet tall. Visual Thinking Strategies provide ways to look closely at works of art by asking observers to respond to these three questions:  continue reading below our video  10 Best Universities in the United States  1. What’s going on in this picture?  2. What do you see that\xa0makes you say that?  3. What more can we find?  In fact, it would be excellent to begin a study of this play by engaging students in doing precisely that – looking at an image of “Sunday Afternoon on the Island of la Grande Jatte” and asking them to respond to those questions.  What’s Going On? According to Sondheim and Lapine  The painting’s setting is the island of la Grande Jatte, which is located northwest of Paris in the Seine River. In 1884 (when Seurat began working on his painting), the island was a pastoral place where people could go to get away from city life. The musical’s creative artists pulled their characters from the canvas. Here’s a Who’s Who:  The most prominent figure is the woman in profile on the bottom right hand side. Sondheim and Lapine imagined her as Seurat’s mistress and they | Question: The musical \'Sunday In The Park With George\' was inspired by a painting by which artist?', ['georges seurat', 'seurat'], 'mrqa_triviaqa-validation-5937'), tensor(0.0089)), (('Context: There are many similarities and differences among teachers around the world. In almost all countries teachers are educated in a university or college. Governments may require certification by a recognized body before they can teach in a school. In many countries, elementary school education certificate is earned after completion of high school. The high school student follows an education specialty track, obtain the prerequisite "student-teaching" time, and receive a special diploma to begin teaching after graduation. In addition to certification, many educational institutions especially within the US, require that prospective teachers pass a background check and psychiatric evaluation to be able to teach in classroom. This is not always the case with adult further learning institutions but is fast becoming the norm in many countries as security concerns grow. | Question: In what country is a background check required?', ['US'], 'mrqa_squad-validation-2191'), tensor(0.0088)), (('Context: Fox\'s Glacier MintsFox\'s Glacier Mints are the leading, branded boiled mint in the UK. They have been manufactured by Fox\'s Confectionery in Leicester since 1918. The mints were developed by Eric Fox, one of the original founders of Fox\'s Confectionery. Since 1922 the mints have been sold with the Peppy the polar bear icon. Peppy is typically depicted as though standing on one of the mints. Glacier Mints resemble miniature blocks of ice and are clear and translucent. Companion products are Fox\'s Glacier Fruits and Fox\'s Glacier Dark.  There is no connection with Fox Glacier in New Zealand.  On Impulse - Packaging TodayOn Impulse - Packaging Today  On Impulse  4 December 2002  Competition in the confectionery market has reached boiling point and is forcing the big brands to provide added value to their products and the packaging in which they come. Today, the impulse purchase sector and promotions play a far more significant role in developing brand loyalty. Rodney Abbott  reports  Do you remember those happy carefree childhood days when you spent your hard-saved pennies or shillings on sweets at the corner shop? I do and I am going back over half a century – which is why I refer to pennies and shillings. Even then the shopkeeper\'s display was sufficiently expansive to keep my mind occupied for several minutes before I could make up my mind exactly what I wanted to buy.  Today, supermarkets and specialist confectioners, even confectionists tobaconistsand news agents, offer a mind boggling array of products that makes it almost impossible to finalise a choice without undertaking an inventory. The battle for brand supremacy is fierce and the confectionery supplier has to find increasingly innovative and cost-effective pack and marketing concepts to move products off the shelf.  According to market analysts Nielsen, the sugar confectionery market [mints, fruits, gums, traditional sweets etc.] is worth £1036M. The mint market alone is worth £146M and the fruits market is worth a massive £378M. Both of those markets have been growing at 1% and 4%, respectively, over the last year.  I sought timely refuge at one of the most British of confectionery suppliers – Leicester-based Fox\'s – which is so proud of its 105-year old heritage.  Mind you, the company has seen some changes in it\'s history, having been taken over by Rowntree, Nestlé and, more recently, Northern Foods which has just acquired Paynes – renown for its Poppets and Just Brazils.  Since the arrival of Northern Foods, the company has not let the grass grow under its feet.  Since the start of the year, it has put Fox\'s back on TV and undertaken various on pack promotions. Last month, it relaunched the brand in a bid to take it forward.  I asked brand manager Emma Gilbert to explain the reasoning behind the relaunch, which was preceded by careful market research with small but focused consumer groups through packaging designers Siebert Head.  "Loyal consumers – adults aged 35 and above – allied Fox\'s with Glacier Mints, a strong brand name. While they recognised the mint as a quality product, they didn\'t think that the former packaging reflected that quality and heritage of the brand.  "In fact, the research disclosed that the three basic elements of the branding were almost disjointed, all fighting for the consumer\'s attention. Above the image of Peppy the famous polar bear was the brand Fox\'s. Below Peppy were the words Glacier Mints. While the image of Peppy was regarded as important, even more key was the brand name \'Fox\'s\'."  Fox\'s Glacier Fruits were less well known than the mint, even though the brand has been around since the middle 50s and consumers were saying that they wanted added value from the fruits. "The product just didn\'t stand out from other products on the market," admitted Emma.  Armed with this information Fox\'s added real fruit juice, took out artificial colours and followed market trends to provide products with added benefits.  Since the majority of consumers said that their favourite sweets were either strawberry or blackcurrant flavoured, Fox\'s has also created a new product | Question: What animal is traditionally seen in the branding of Fox\'s Glacier Mints?', ['polar bear'], 'mrqa_triviaqa-validation-2327'), tensor(0.0071)), (("Context: Doctor Who has appeared on stage numerous times. In the early 1970s, Trevor Martin played the role in Doctor Who and the Daleks in the Seven Keys to Doomsday. In the late 1980s, Jon Pertwee and Colin Baker both played the Doctor at different times during the run of a play titled Doctor Who – The Ultimate Adventure. For two performances, while Pertwee was ill, David Banks (better known for playing Cybermen) played the Doctor. Other original plays have been staged as amateur productions, with other actors playing the Doctor, while Terry Nation wrote The Curse of the Daleks, a stage play mounted in the late 1960s, but without the Doctor. | Question: What was the name of the Doctor Who play from the 1980's?", ['Doctor Who – The Ultimate Adventure'], 'mrqa_squad-validation-7821'), tensor(0.0055)), (('Context: Great Train Robbery Anniversary - Heart Beds, Bucks ...Great Train Robbery Anniversary - Heart Beds, Bucks & Herts News  Great Train Robbery Anniversary  Great Train Robbery Anniversary  See pictures of the bridge near Leighton Buzzard where The Great Train Robbery took place fifty years ago.  See all photos (15) in this gallery  By John Stratford (@Heart_JohnS), 8th August 2013, 10:50  Fifty years after The Great Train Robbery, an event\'s taken place to praise police who searched for the men who robbed a mail train in Buckinghamshire.  You may also like  On 8 August 1963, a gang of robbers, masterminded by Bruce Reynolds, stopped the Glasgow-Euston overnight mail train as it passed close to Cheddington, near Leighton Buzzard.  On board were huge numbers of used bank notes.  Twelve of the robbers were jailed for a total of more than 300 years but more than one broke out of prison, including notorious criminal Ronnie Biggs, who spent over 30 years on the run before he finally returned to Britain in 2001 to face arrest.  Reynolds returned in 1968, five years after the crime, and was captured in Torquay and jailed for 25 years.  Two Buckinghamshire Constabulary police officers who were involved in the investigation attended a police commemoration event alongside serving Thames Valley Police officers at Eynsham Hall in Witney, Oxfordshire, on Wednesday 7 August 2013.  Keith Milner was a detective at Aylesbury at the time of the robbery, while John Woolley was a PC and discovered Leatherslade Farm near Brill, Buckinghamshire, where the men hid after committing the crime.  Biggs insisted in July 2013 that he was proud to have been part of the gang.  The famous fugitive, who celebrates his 84th birthday on 8 August 2013, escaped from prison in 1965 and spent 36 years on the run before finally being arrested and jailed in 2001.  Released from prison on compassionate grounds in 2009 due to ill health he is still alive, being cared for in a north London nursing home, and he has few regrets about the crime that made him a household name.  Biggs, who cannot speak and communicates through a spelling board, said: "If you want to ask me if I have any regrets about being one of the train robbers, my answer is, \'No!\'.  "I will go further: I am proud to have been one of them. I am equally happy to be described as the \'tea-boy\' or \'The Brain\'.  "I was there that August night and that is what counts. I am one of the few witnesses - living or dead - to what was \'The Crime of the Century\'.\'\'  But although he is proud to have been involved in the headline-grabbing crime, he admitted he does have some regrets.  "It is regrettable, as I have said many times, that the train driver was injured,\'\' he said. "And he was not the only victim.  "The people who paid the heaviest price for the Great Train Robbery are the families. The families of everyone involved in the Great Train Robbery, and from both sides of the track.  "All have paid a price for our collective involvement in the robbery. A very heavy price, in the case of my family.  "For that, I do have my regrets.\'\'  Ronnie\'s son Michael, who was born while Ronnie was on the run, spoke exclusively to Heart and said he isn\'t proud of his dad being a criminal, but he is proud of the man his dad is.  "I believe if you commit a crime, you have to do the time," he said, "but the time has to be proportional with the crime that you\'ve committed.  "The sad bit was that he went into the train robbery because he needed £500 to put a deposit on his house. About a week before the robbery, he won the £500 in a bet on the horses but there was no turning back then."  Listen to Michael Biggs | Question: What criminal offence took place in Cheddington, Buckinghamshire in August (8th) 1963?', ['great train robbery'], 'mrqa_triviaqa-validation-7369'), tensor(0.0017)), (("Context: BLACK DOG: Mitchell - now it's snobgate | Daily Mail OnlineBLACK DOG: Mitchell - now it's snobgate | Daily Mail Online  BLACK DOG: Mitchell - now it's snobgate  comments  As revealed in these columns before his downfall in the 'Plebgate' scandal, Tory MP Andrew Mitchell was known as 'Thrasher' in his days as head boy of Rugby School. Dog is now intrigued to learn he had a second nickname: 'Mitchell-snob'.  A fellow Old Rugbeian observes drily: 'It had the advantage that you could drop the hyphen and insert an apostrophe between the 's' and 'n' of snob.'  Tory MP Andrew Mitchell was known as 'Thrasher' in his days as head boy of Rugby School  When Mitchell ran Tory pal David Davis's Conservative leadership campaign in 2005, political broadcaster Michael Cockerell asked Mitchell: 'What do you say to those who argue Davis is Iain Duncan Smith with hair?'\xa0  Adopting a pantomime-villain glower, Mitchell hissed: 'Tell me their names and I'll sue.' How he must wish he had only joked about suing The Sun over PC Toby Rowland.  Hot 'babe' Pepi gets hooked on Vegas\xa0  Ex MoD official Pepi Simpson sashayed in to hear a husky American accent drawl in her direction: 'That's what I call a babe'  A chance visit to a high rollers' bar in a Las Vegas hotel while on holiday with a girlfriend had unintended consequences for Pepi Simpson, sporty wife of Tory MP Keith Simpson. Ex-MoD official Pepi, pictured in the bar, dolled up after a visit to the crimper, sashayed in to hear a husky American accent drawl in her direction: 'That's what I call a babe!' No one told her it was a hookers' pick-up joint. Pepi replied tartly: 'Not bad for 61.'  Hats off to gutsy Andrew Marr, who has thrown away his walking stick nearly two years after a debilitating stroke. In between hosting his Sunday morning TV show and gruelling physiotherapy sessions that set him on the road to recovery, Marr has just completed a second novel and is working on a poetry project. Dog would like to know what he has for breakfast.  On hearing that Alex Salmond was among guests at the Spectator magazine Parliamentary Awards lunch at The Savoy, veteran Tory Norman Tebbit mused he had always wanted to meet the now ex-SNP leader. When a fellow guest asked why, out came 'Chingford Skinhead' Tebbit's razor: 'I've always wanted to sprinkle salt over him to see if he dissolves into slime like the slugs in my garden.'  Theresa May reveals all\xa0  The Spectator's attempt to keep guests in suspense over the winner of the Parliamentarian of the Year award was ruined when Theresa May opened proceedings with a jokey video presentation and pressed the wrong button on her remote control.\xa0  Up flashed the winner's name: Edward VII lookalike and former Commons Clerk Sir Robert Rogers, who was forced to quit after falling out with Speaker John Bercow.  It seems John Bercow has paid a heavy price for falling out with Sir Robert. David Cameron rubbed his nose in it by giving Sir Robert a peerage – and there's worse to come. Another of Bercow's tormentors, former Speaker Betty Boothroyd, is tipped to be at Sir Robert's side as one of his two 'sponsors' when he takes his seat in the Upper House.\xa0  \xa0  The cuddly side of the Chingford Skinhead - Daily Mail OnlineNorman Tebbit's as savage as ever except about wife and the dog who inspired new book | Daily Mail Online  The cuddly side of the Chingford Skinhead: Cameron? Silly. Blair? Offensive. Cherie? Urggh. Norman Tebbit's as savage as ever - except about the wife he adores and the dog who inspired his startling new book  Norman Tebbit has published a children's book about a dog called Ben  The former Minister on Maria Miller: 'It offends against common sense'  On Farage: 'A clever operator. He had touched on | Question: Which ex-politician, now aged 80, had the nickname 'The Chingford Skinhead' ?", ['norman tebbit'], 'mrqa_triviaqa-validation-1924'), tensor(0.0002)), (("Context: ET Canada | Blog - Remake Of Tom Hanks' Mermaid Romcom ...Remake Of Tom Hanks’ Mermaid Romcom ‘Splash’ In The\xa0Works | ETCanada.com  Remake Of Tom Hanks’ Mermaid Romcom ‘Splash’ In The\xa0Works  By Rachel West.  6 Jun 2016 1:54 PM  Photo: Keystone Press  Thirty-two years ago, Tom Hanks fell in love with a mermaid in “Splash”;. Now, the 1984 romantic comedy is getting a remake – this time with a twist.  Producer Brain Grazer admitted during an interview with CNBC that a remake of “Splash”; was in the works. While Grazer kept mostly mum on the details, he revealed that this version of the story would be told from the mermaid’s perspective.  RELATED: John Stamos Singing “Under The Sea’ From “The Little Mermaid’ Is The Best Way To Start Your Weekend  The original film saw an unlucky-in-love Tom Hanks fall in love with a beautiful mermaid, played by Daryl Hannah, after she rescues him from drowning. Directed by Ron Howard, the film was a commercial and critical success, earning an Oscar nomination for Best Original Screenplay.  Simply titled “Wet”;, there’s no word on if the film’s original stars will appear in the sequel, but Grazer did share at least one big name could be attached to the project.  “There’s a movie star involved, or going to be involved,”; Grazer said during the interview. “I haven’t announced it.”;  RELATED: Casting Call: Stars Nab A New Role Gallery  “Wet”;”s mermaid star would certainly be at home on land or sea. The movie would stand to capitalize on a trend which sees Hollywood showing some love to mermaids, as of late.  This past weekend, stars gathered together to give a live rendition of the animated Disney classic for “The Little Mermaid Live”; with Rebel Wilson as Ursula and Darren Criss as Eric. Up next, Chloe Grace Moretz will star in a non-musical non-Disney version of “The Little Mermaid”;, which is currently in the pre-production stage.  Movie Review - - SCREEN: 'SPLASH,' A MERMAID'S LOVE ...Movie Review -    - SCREEN: 'SPLASH,' A MERMAID'S LOVE - NYTimes.com  SCREEN: 'SPLASH,' A MERMAID'S LOVE  By JANET MASLIN  Published: March     9, 1984  THOUGH Ron Howard's comedies don't adhere to any familiar formulas - Mr. Howard's funny ''Night Shift'' was about a prostitution ring operating out of a morgue, and his even more disarming new ''Splash'' is about a mermaid in Manhattan - they have a comfortably old-fashioned flavor. ''Splash'' may feature a heroine with fins, but it's mostly a standard love story, albeit one with some delightful new twists. The boy (Tom Hanks) is a lonely bachelor with a single overriding wish: ''to meet a woman, fall in love, get married, have a kid and see him play a tooth in the school play.'' And the girl (Daryl Hannah) at first knows only one word of English: ''Bloomingdale's.''  ''Splash,'' which opens today at the National and other theaters, accomplishes the improbable with some enchanting underwater sequences, scenes that make credible the thought that Daryl Hannah might really be a mermaid. Following a brief prologue in which she meets Allen Bauer, played by Mr. Hanks when both are children, the mermaid reappears to save his life when he falls overboard near Cape Cod. There are hints that Allen may be susceptible to aquatic creatures in the decors of both his office and his apartment, since there are large fish tanks in both places (only rarely does Mr. Howard use such unnecessarily broad strokes). But nothing in his past hints at how hard Allen will fall for the beautiful, innocent mermaid once she sprouts a pair of tawny legs and follows him home.  Much of the humor grows out of the way in which | Question: In which 1984 film did Tom Hanks fall in love with a mermaid?", ['splash'], 'mrqa_triviaqa-validation-7382'), tensor(-0.0005)), (("Context: In some rural areas in the United Kingdom, there are dispensing physicians who are allowed to both prescribe and dispense prescription-only medicines to their patients from within their practices. The law requires that the GP practice be located in a designated rural area and that there is also a specified, minimum distance (currently 1.6 kilometres) between a patient's home and the nearest retail pharmacy. This law also exists in Austria for general physicians if the nearest pharmacy is more than 4 kilometers away, or where none is registered in the city. | Question: What is another country that permits physicians to give out drugs from within their practice?", ['Austria'], 'mrqa_squad-validation-6399'), tensor(-0.0007)), (("Context: Famous Jockeys - Racing-InsiderDiscover Who Are The Most Successful Jockeys of All Time  William Hill  Famous Jockeys  Fred Archer (11 January 1857 –8 November 1886), was an English flat race jockey of the Victorian era. Nicknamed “The Tin Man”, and described as “as the best all-round jockey that the turf has ever seen”, Archer was the holder of several records which lasted deep into the 20th century. He was Champion Jockey for 13 consecutive years until 1886, claiming 2,748 victories from 8,064 starts. He won a total of 21 classic races, including Epson Derby five times. He committed suicide at the age of 29 after the loss of his wife during childbirth.  Sir Gordon Richards (5 May 1904 – 10 November 1986) was one of the England’s finest jockeys, often considered the world’s greatest ever jockey. He is still the only jockey to have been knighted. Gordon Richard was the British flat racing Champion Jockey 26 times. He amassed a total of 4,870 winners, and a record 12 consecutive winners ridden. In 1999, the Racing Post (leading racing newspaper in Britain) placed him at number 1 in their list of the top 50 jockeys of 20th century.  Lester Piggott (born 5 November 1935) – is a retired professional jockey and one of the most successful English flat racing jockeys of all time. Nicknamed “The Long Fellow” to his height, Lester Piggott introduced a new style of race-riding that was adopted all over the world and enabled him to become Champion Jockey eleven times. Piggott boasts 4,493 career wins including 30 British classics.  William Lee “Bill” Shoemaker (August 19, 1931 – October 12, 2003) is one of the all-time legends who have graced the sport. Referred to as “The Shoe”, he held the world record of number of professional jockey victories for 29 years. Shoemaker used his small size (1.50 m) to his advantage riding 8,833 winners. A high school dropout, Shoemaker went to ride a total of 40,350 races, and win the United States Champion Jockey by earnings , a record 10 times. He was inducted into the National Museum of Racing and Hall of Fame in 1958.  Russell Avery Braze (born 7 August 1958) is the record holder of the most race wins in North American horse racing history . He’s victory counter stopped at the astonishing 12,007 number. Baze won 400 or more races per year for four consecutive years and got rewarded with a special Eclipse Award in 1995. Since then he has on over 400 races a year seven additional times; an achievement that no other jockey has accomplished more than three times. Russell Baze was inducted into the National Museum of Racing and Hall of Fame in 1999. He also received the prestigious George Woolf Memorial Jockey Award in 2002.  Julieann Louise “Julie” Krone (born July 24, 1963) is one of the most successful female jockeys in North American horse racing history. Julie became the first female to win a Triple Crown Race when she claimed the Belmont Stakes, riding on Colonial Affair. She sustained number of severe injuries while racing, but always came back stronger. Because of her never give up attitude, Krone was named by USA Today as one of the 10 Toughest Athletes and was honored with the Wilma Rudolph Courage Award by the Women’s Sports Foundation. In 2000 she became the first woman inducted into the National Museum of Racing and Hall of Fame.  Racing: Dettori determined not to be fearful of the Derby ...Racing: Dettori determined not to be fearful of the Derby | The Independent  Sport  Racing: Dettori determined not to be fearful of the Derby  Winning the premier Classic for the first time at Epsom on Saturday would help erase a very bad memory for an Italian jockey.  Sunday 30 May 1999 23:02 BST  Click to follow  The Independent Online  IN THE build-up to the Derby they used to ask what Lester Piggott was going to ride. This week the question is being posed of the man who has taken over from the Long Fellow as Britain's dominant jockey.  In the next | Question: Which horse racing jockey was sometimes referred to as the long fellow?", ['lester piggott'], 'mrqa_triviaqa-validation-365'), tensor(-0.0023)), (('Context: Where is Coney Island, NY? / Where is Coney Island, NY ...Where is Coney Island, NY? / Where is Coney Island, NY Located in The World? / Coney Island Map - WorldAtlas.com  Where is Coney Island, NY?  Location of Coney Island on a map.  Coney Island is a city found in New York , The United States Of America . It is located 40.58 latitude and -73.99 longitude and it is situated at elevation 2 meters above sea level.  Coney Island has a population of 60,000 making it the 23rd biggest city in New York. It operates on the EDT time zone, which means that it follows the same time zone as New York City.  Quick facts  Coney Island, Brooklyn | New York City Tourist AttractionsConey Island, Brooklyn  | New York City Tourist Attractions  Coney Island  Coney Island  Probably one of the most popular tourist areas in the entire city of New York is Coney Island. It is a beloved location and every much part of the city’s psyche as other famous symbols, such as the Statue of Liberty.  Coney Island is a peninsula in the southernmost part of Brooklyn, New York. It comprises a neighborhood with approximately 60,000 inhabitants and is well-known for its long stretch of sandy beaches on the Atlantic coastline, as well as its cultural events and amusement parks. The charm and history of this area makes it that no visit to New York would be complete without a stopover at Coney Island.  History  Coney Island was known as Narrioch (“Land without Shadows”) by the Native Americans because the area is flooded by sunlight all day long. When the island – which is now a peninsula – was settled by the Dutch, they called it Konijn Eiland (meaning ‘Rabbit Island’). The name was eventually anglicized to Coney Island.  After the American Civil War, Coney Island became a resort and was accessible by new streetcars and railroads. The area grew rapidly into a popular destination for people seeking pleasure in hotels, beaches, amusement parks and horseracing. It also developed a reputation for itself in some of the shadier areas, such as prostitution and gambling. As transportation methods improved, Coney Island became more popular for New Yorkers seeking a day excursion.  The heydays of Coney Island reached their peak in the early 1900s and then slowly started to decline after the Second World War. New Yorkers started seeking out other, cleaner areas, with a better reputation. Years of neglect followed in this once grand area, however local inhabitants still have a soft spot for Coney Island and its splendid past.  In recent years, Coney Island has enjoyed a revival, especially with the opening of the KeySpan baseball park, which is home to the wildly successful and popular Brooklyn Cyclones baseball team.  In addition, organizations such as Coney Island USA, a not-for-profit corporation founded in 1983, have taken it upon themselves to revitalize the area and attract the tourists and visitors back. Coney Island USA develops a number of different programs such as the much-loved Mermaid Parade and the Circus Side Show. It also runs the Coney Island Museum.  Things to Do and See in Coney Island  The Coney Island Museum  For less than a dollar, visitors can get to see artifacts that bring back the nostalgia of yesteryear Coney Island in this quaint museum. Located on the second floor of an historic building on Surf Avenue with a delightful view of classic amusement park rides, the Coney Island museum interprets and preserves the history of the area through a small but fascinating collection of items. Visitors get to see quirky pieces of the collection, such as a hand-carved figure of the Steeplechase Man and Boardwalk rolling chair.  The museum is open every Saturday and Sunday between 12 and 5 pm, right through the year, with opening hours sometimes extended during the summer months. The museum also serves as a tourist information center where visitors can use the bathrooms and receive knowledgeable answers to the questions about the area.  The Coney Island Circus Sideshow  At its peak, Coney Island offered amazing forms of entertainments, including the classic sideshows | Question: In what city is Coney Island located?', ['new york city'], 'mrqa_triviaqa-validation-3647'), tensor(-0.0037)), (("Context: Becoming Italian Word by Word: How the Months Got Their ...Becoming Italian Word by Word: How the Months Got Their Names in the Italian Language  How the Months Got Their Names in the Italian Language  \xa0  i mesi\xa0  the months\xa0  The original Roman year had ten named months, beginning with Martius, named for Mars, god of war. This was the month when the mighty Roman legions resumed their battles to conquer and rule the known world. It was also the time when farmers began working the fields and preparing for planting.\xa0  The other months were:\xa0  *Aprilis (April), dedicated to the love goddess Aphrodite (Venus)\xa0  *Maius (May), for Maia, the goddess of new plantings (who also gave her name to il maiale, Italian for pig)\xa0  *Junius (June), the month of Juno, the king of gods  *Quintilis (July), the fifth month, renamed luglio in honor of Julius Caesar in 44 B.C.\xa0  *Sextilis (August), the sixth month, which Augustus claimed as his own in 8 B.C.\xa0  *Septem (September), the seventh month\xa0  *Octo (October), the eighth month\xa0  *Novem (November), the ninth month\xa0  *Decem (December), the tenth month  There were two unnamed months in the dead of winter when not much happened. Numa Pompilius, the second king of Rome circa 700 BC, added the two months Januarius (January), dedicated to Janus, the two-faced god of beginnings and endings, and Februarius (February), named after Februalia, a time of sacrifices to atone for sins. He also moved the beginning of the year from Martius to Januarius.  After Februarius there was occasionally an additional month of Intercalaris\xa0(intercalendar). In 46 BC, Julius Caesar reformed the Roman calendar (replacing it with the Julian calendar), changed the number of days in many months, and eliminated Intercalaris. In its stead February gets an extra day during an anno bisestile (leap year).\xa0  You can learn the month sof the year the way Italian school children do: by memorizing this “filastrocca dei mesi dell’anno” (nursery rhyme of the months of the year):\xa0  gennaio freddoloso (January cold)\xa0  febbraio spiritoso (February funny or hilarious--a reference to Carnevale)\xa0  marzo pazzerello (March crazy)\xa0  aprile mite e bello (April mild and beautiful)  maggio sognatore (May the dreamer)  giugno cantatore (June the singer)\xa0  luglio nuotatore (July the swimmer)\xa0  agosto gran signore (August the great signore)\xa0  settembre grappolaio (September brings the grapes)\xa0  ottobre castagnaio (October brings the chestnuts)  novembre triste e stanco (November sad and tired)  dicembre tutto bianco\xa0(December all white)\xa0  Words and Expressions\xa0  arrivare a fine mese -- make it to the end of the month\xa0  Quanti ne abbiamo del mese? -– literally, how much of the month do we have? What day of the month is it?\xa0  Ii mese scorso --last month\xa0  il mese prossimo -- next month\xa0  “30 giorni ha novembre con april, giugno e settembre\xa0  di 28 ce n’è uno, tutti gli altri fan trentuno” –-\xa0  “30 days has November, April, June and September  One has 28 of them; all the others 31.”\xa0  Click here to listen to an Italian ode to the months of the year. Let me know if you'd like a copy of the lyrics (in Italian):  Italian Calendar - Italian Months - Italian DaysItalian Vocabulary: Italian Calendar Months  Italian Vocabulary: Italian Calendar Months  Italian Vocabulary: Italian Calendar Months  Learn the words for January - December  Calendar with view of January.\xa0 SuperStock  By Cher Hale  Updated September 14, 2016.  You want to tell your language partner when you’re going to Italy for a vacation , and that’s when you | Question: Giugno is Italian for which month of the year?", ['june'], 'mrqa_triviaqa-validation-1085'), tensor(-0.0040)), (('Context: Genghis Khan, the title is spelled in variety of ways in different languages such as Mongolian Chinggis Khaan, English Chinghiz, Chinghis, and Chingiz, Chinese: 成吉思汗; pinyin: Chéngjísī Hán, Turkic: Cengiz Han, Çingiz Xan, Çingiz Han, Chingizxon, Çıñğız Xan, Chengez Khan, Chinggis Khan, Chinggis Xaan, Chingis Khan, Jenghis Khan, Chinggis Qan, Djingis Kahn, Russian: Чингисхан (Čingiskhan) or Чингиз-хан (Čingiz-khan), etc. Temüjin is written in Chinese as simplified Chinese: 铁木真; traditional Chinese: 鐵木眞; pinyin: Tiěmùzhēn. | Question: What are alternate English spelling of Genghis?', ['Chinghiz, Chinghis, and Chingiz'], 'mrqa_squad-validation-6303'), tensor(-0.0045)), (('Context: During the mid-Eocene, it is believed that the drainage basin of the Amazon was split along the middle of the continent by the Purus Arch. Water on the eastern side flowed toward the Atlantic, while to the west water flowed toward the Pacific across the Amazonas Basin. As the Andes Mountains rose, however, a large basin was created that enclosed a lake; now known as the Solimões Basin. Within the last 5–10 million years, this accumulating water broke through the Purus Arch, joining the easterly flow toward the Atlantic. | Question: Where did water to the west of the Amazon drainage basin flow towards?', ['Pacific', 'the Pacific'], 'mrqa_squad-validation-4283'), tensor(-0.0079)), (('Context: George Cross | British medal | Britannica.comGeorge Cross | British medal | Britannica.com  British medal  Academy Award  George Cross, a British civilian and military decoration, instituted in 1940 by King George VI for “acts of the greatest heroism or of the most conspicuous courage in circumstances of extreme danger.” The award, which can be conferred posthumously, is usually given to civilians, although it can be bestowed on military personnel for acts for which military decorations are not usually awarded. The George Cross superseded the Medal of the Order of the British Empire for Gallantry (commonly known as the Empire Gallantry Medal).  George Cross medal engraved on a tombstone.  Acmthompson  The island of Malta received the George Cross in recognition of its inhabitants’ gallantry in World War II. Recipients of this award may add G.C. after their names; the cross ranks second only to the Victoria Cross (the highest British military decoration). The cross is silver, with one side depicting St. George slaying the dragon and with the inscription “For Gallantry;” the other side gives the recipient’s name and the date of the award.  The George Medal, instituted at the same time as the George Cross, is analogous to it but is awarded for services not quite so outstanding as those which merit the George Cross. Recipients of this medal can add G.M. after their names. The medal is silver; one side has the effigy of the reigning British monarch, and the other side has St. George and the dragon with the inscription “The George Medal.”  Learn More in these related articles:  World War 2 Awards.com - George CrossWorld War 2 Awards.com - George Cross  George Cross  Number of awards in the database: 139  Total awarded for WW II:    139  Recipients A-Z  The George Cross -or GC- was instituted June 24th, 1941, by King George VI. This decision was actuated by an air raid on London in 1940 during which the Royal Palace was hit. After this incident, King George became emotionally more involved with his people and he realised what they had to go through during the raids, admiring even more their hardships and heroism. As a result, the George Cross and George Medal were largely designed by him.  The George Cross is the second highest British decoration and is awarded for "acts of the greatest heroism and bravery in circumstances of extreme danger". It was initially awarded to all citizens of the Commonwealth and was later extended to military personnel for actions that did not usually merit a military decoration. Recipients are entitled to add the letters GC after their names. Bars will be awarded for subsequent acts of bravery and the decoration can be awarded posthumously.  The GC is the successor to the Medal of the Order of the British Empire for Gallantry and its recipients were obliged to return it to the Central Chancellery of the Orders of Knighthood to have it replaced by a George Cross. In 1971, recipients of the Albert Medal could also exchange this for a George Cross.  The badge is made of silver. On the front is an image of St. George fighting the dragon surrounded by the inscription \'For Gallantry\', on the reverse are engraved the name of the recipient and the date of the award. Each arm of the cross bears the cypher of the reigning Monarch, during the war GRI or GRVI, after the war EIIR. The cross hangs from a silver bar adorned with laurel leaves by a silver ring. The ribbon is 1.75" (38 mm) wide and coloured blue with a miniature GC in its centre.  The George Cross was conferred upon the Island of Malta for the display of bravery of its citizens during the war. Four times a George Cross was conferred upon a woman, three of which posthumously. Three women were awarded the GC for resistance activities in occupied  territories during WW 2. One of them was Noor Inayat Khan. This Russian born woman was dropped over France July 16th, 1943, to assist the French resistance movement in their activities. Some three months later, she was betrayed and taken prisoner by the Gestapo (German Secret Police). Despite numerous interrogations she | Question: What is the inscription on the George Cross ?', ['for gallantry'], 'mrqa_triviaqa-validation-2096'), tensor(-0.0093)), (('Context: Callability -- Some bonds give the issuer the right to repay the bond before the maturity date on the call dates ; see call option . These bonds are referred to as callable bonds . Most callable bonds allow the issuer to repay the bond at par . With some bonds , the issuer has to pay a premium , the so - called call premium . This is mainly the case for high - yield bonds . These have very strict covenants , restricting the issuer in its operations . To be free from these covenants , the issuer can repay the bonds early , but only at a high cost . | Question: a bond that the issuer has the right to pay off before its maturity date ?', ['callable bonds'], 'mrqa_naturalquestions-validation-2385'), tensor(-0.0104)), (("Context: To fix carbon dioxide into sugar molecules in the process of photosynthesis, chloroplasts use an enzyme called rubisco. Rubisco has a problem—it has trouble distinguishing between carbon dioxide and oxygen, so at high oxygen concentrations, rubisco starts accidentally adding oxygen to sugar precursors. This has the end result of ATP energy being wasted and CO2 being released, all with no sugar being produced. This is a big problem, since O2 is produced by the initial light reactions of photosynthesis, causing issues down the line in the Calvin cycle which uses rubisco. | Question: What effect does rubisco's flaw have?", ['at high oxygen concentrations, rubisco starts accidentally adding oxygen to sugar precursors'], 'mrqa_squad-validation-8832'), tensor(-0.0108)), (('Context: Theatre Notables Sondheim, Ziegfield, Simon & More Amongst ...Theatre Notables Sondheim, Ziegfield, Simon & More Amongst \'New York City 400\'  Theatre Notables Sondheim, Ziegfield, Simon & More Amongst \'New York City 400\'  Tweet Share \xa0\xa0\xa0\xa0  The NYC400 is the first-ever list of New York City\'s ultimate movers and shakers since the City\'s founding-from politics, the arts, business, sports, science, and entertainment.  In commemoration of Henry Hudson\'s epic 1609 voyage into New York Harbor, the Museum is celebrating our City\'s 400th birthday by recognizing the people who have had the greatest impact and influence on the world\'s greatest city.  Our goal in creating the NYC 400 is to help educate the public about New York City\'s fascinating and dramatic history-its heritage of diversity, opportunity and perpetual transformation-by humanizing our amazing common story.  Berenice Abbott (1898-1991)  Photographer best known for her "Changing New York" project for the WPA (1935-1939), which documented New York City\'s evolving built environment.  Alvin Ailey (1931-1989)  Dancer/choreographer who founded Alvin Ailey American Dance Theater in 1958 to bring black dancers into the mainstream. Known for combining elements of ballet, jazz, and modern dance and drawing on elements of black culture.  Horatio Alger (1832-1899)  Writer, philanthropist, and Unitarian minister who supported such charities as the Newsboys\' Lodging House and wrote moralistic novels about street boys who eventually became wealthy through luck and hard work.  Woody Allen (1935- )  Comedian, filmmaker, writer, and musician best known for chronicling New Yorkers\' neuroses and foibles in films such as Annie Hall (1977) and Manhattan (1979).  Othmar H. Ammann (1879-1965)  Engineer of bridges linking Manhattan to the outer boroughs and beyond, including the George Washington Bridge (1931), the Triborough (now Robert F. Kennedy) Bridge (1936), and the Verrazzano Narrows Bridge (1964).  Diane Arbus (1923-1971)  Photographer whose 1960s images of eccentrics and other often ignored people explored issues of identity and appearance. Hubert\'s Dime Museum and Flea Circus, a Times Square freak show, was one of her favorite places to photograph.  Harold Arlen (1905-1986)  Singer, pianist, and composer of jazz and blues songs and ballads for Broadway musicals and shows at Harlem\'s Cotton Club. Best known for composing such songs as "Get Happy" (1929) and "Over The Rainbow" (1939).  Louis Armstrong (1901-1971)  Trumpeter and singer who came to prominence in New York City as a major jazz influence in the 1920s. Known for hits like "What a Wonderful World" (1967) and his "Hot Fives" and "Hot Seven" recordings made between 1925-29.  Chester A. Arthur (1829-1886)  As a New York lawyer interested in civil rights cases, he represented a black woman in a successful suit against the Third Avenue Railway (1855), which helped end segregation on the city\'s passenger railroads. As President, he was the first since George Washington to take the oath of office in NYC.  Brooke Astor (1902-2007)  Socialite and philanthropist who from 1959-97 gave away the corpus of her late husband\'s foundation to New York City institutions, leading others in philanthropy. Known for her visits to each and every one of her grantees.  John Astor (1763-1848)  Immigrant fur trader who amassed a fortune in real estate and who, by the 1840s, was the country\'s wealthiest man. Astor Place and Astoria are named for him.  Brooks Atkinson (1894-1984)  Theater critic for The New York Times from 1925-60 and an influential voice in an era when American drama emerged as a serious art form.  Louis Auchincloss (1917- )  Attorney, essayist, and writer of historic fiction known for closely observed portraits of old, patrician New York society in such novels as Portrait in Brownstone (1962 | Question: American businessman and philanthropist Solomon Guggenheim (1861-1949) established a famous eponymous international network of?', ['museum', 'museums'], 'mrqa_triviaqa-validation-2530'), tensor(-0.0124)), (("Context: The White Coat Ceremony ( WCC ) is a relatively new ritual in some medical ( MD , DO ) , dental , optometry , audiology , chiropractic , dietetic , occupational therapy , physical therapy , podiatric , pharmacy , physician assistant , pathologists ' assistant , nursing , naturopathic and veterinary schools that marks the student 's transition from the study of preclinical to clinical health sciences . At some schools , where students begin meeting patients early in their education , the white coat ceremony is held before the first year begins . It is an example of a matriculation . | Question: when do you get your white coat in pharmacy school ?", ["the student 's transition from the study of preclinical to clinical health sciences"], 'mrqa_naturalquestions-validation-5465'), tensor(-0.0127)), (("Context: Prague: Six things you must do in the Czech capital city ...Prague: Six things you must do in the Czech capital city | Daily Mail Online  Prague: Six things you must do in the Czech capital city  comments  Prague is one of Europe's architectural gems, with more knockout picture-postcard views than some entire countries. Unsurprisingly, the Czech capital can also get busy as thousands of tourists jostle for space.  Here is a selection of things to do during a visit to the city - from taking in its best-loved sights to stopping for a drink at a beautifully restored cafe and watching a comic take on a famous Mozart opera.  1... New Jerusalem  Emperor Charles IV planned this intensely beautiful city as the ‘New Jerusalem’ in the 14th Century, and architects maintained the vision until its last golden age in the Thirties. The old centre, with outstanding buildings such as Valdstejn Palace, St James’ Church and St Vitus Cathedral, is still largely intact after 800 years and is now a World Heritage Site.  With nifty footwork you can outwit the crowds. Try walking the famous Charles Bridge before breakfast, and save the striking of the Astronomical Clock in the Old Town Square until the evening when it’s quieter. I also like the peacefulness of Mala Strana, the cafe-lined cobbled streets under Prague Castle. And take a tram to the Jewish Quarter and wander Petrin Hill for wonderful views over the city’s baroque roofs.  Atmospheric: Prague's beautiful Charles Bridge  2... Grand re-opening  How about this for a comeback? The Grand Cafe Orient ( www.grandcafeorient.cz ) opened in 1912 in the House of the Black Madonna, Prague’s architectural take on cubism, the art form Picasso championed. The cafe closed eight years later when cubism fell out of favour but reopened in 2005, faithfully recreated with period furniture and fittings. Try the apple strudel with vanilla ice cream.  Another venue, the Mysak pastry shop, reopened in 2008 inside the Mysak Gallery ( www.gallerymysak.cz ), with original doors, mosaics floors, a marble staircase and wooden alcoves. The signature dish is karamelovy pohar – ice cream topped with caramel, chocolate and walnuts.  RELATED ARTICLES  Share  3...Fighting for freedom  After the euphoria of spring 1968, and Czechoslovakia’s short-lived break for freedom from the Soviet Union, came deep gloom that summer when Russian tanks rolled in to consign this proud nation to 20 more years of oppression.  The Museum of Communism ( www.muzeumkomunismu.cz ) tells the story of those lost years. I came closest to Prague’s tragedy and eventual triumph on the steps of the National Museum in Wenceslas Square. I looked down on the bronze cross at the spot where Jan Palach burnt himself to death in protest in 1969. It's also where people celebrated their new-found freedom after 1989’s so-called Velvet Revolution.  Historic: The Duke of Bohemia is represented by an equestrian statue in Wenceslas Square  4...Recovered treasures  In 1989, William Lobkowicz returned to Prague from exile in Boston to restore the great estate his family had built up over 700 years. It had been confiscated first by the Nazis, then by the communists. Diligent legal teams gradually recovered land, thousands of books, paintings and pieces of furniture. William’s mission culminated in the opening of Lobkowicz Palace in 2007, with its exhibition of the best of the family’s reclaimed treasures. Star exhibits include two views of the River Thames by Canaletto and Haymaking by Pieter Bruegel the Elder.  Another of the city’s new attractions is totally different. The Kafka Museum ( www.kafkamuseum.cz ) celebrates the writer (he was born here) whose name stands for the way the ordinary man is defeated by ‘the system’.  5...Pulling the strings  With his mischievous sense of fun, Mozart would have approved of the National Marionette Theatre’s comic performances of his opera Don Giovanni ( www.mozart.cz ). I particularly loved the puppet cat hissing at an anguished diva in mid aria. Don Giovanni is Prague’s top claim to cultural fame – in 1787, Mozart staged its world premiere at the exquisite opera | Question: By what name do Czechs know their capital city, Prague?", ['prague'], 'mrqa_triviaqa-validation-6351'), tensor(-0.0134)), (("Context: A piñata ( / pɪnˈjɑːtə / , US pronunciation / pɪnˈjɑːdɑː / , Spanish pronunciation : ( piˈɲata ) ( listen ) ) is a container often made of papier - mâché , pottery , or cloth ; it is decorated , and filled with small toys or candy , or both , and then broken as part of a ceremony or celebration . Piñatas are commonly associated with Mexico . The idea of breaking a container filled with treats came to Europe in the 14th century , where the name , from the Italian pignatta , was introduced . The Spanish brought the European tradition to Mexico , although there were similar traditions in Mesoamerica , such as the Aztecs ' honoring the birthday of the god Huitzilopochtli in mid December . According to local records , the Mexican piñata tradition began in the town of Acolman , just north of Mexico City , where piñatas were introduced for catechism purposes as well as to co-opt the Huitzilopochtli ceremony . Today , the piñata is still part of Mexican culture , the cultures of other countries in Latin America , as well as the United States , but it has mostly lost its religious character . | Question: where did the tradition of the pinata come from ?", ['treats'], 'mrqa_naturalquestions-validation-10680'), tensor(-0.0156)), (('Context: The IPCC does not carry out research nor does it monitor climate related data. Lead authors of IPCC reports assess the available information about climate change based on published sources. According to IPCC guidelines, authors should give priority to peer-reviewed sources. Authors may refer to non-peer-reviewed sources (the "grey literature"), provided that they are of sufficient quality. Examples of non-peer-reviewed sources include model results, reports from government agencies and non-governmental organizations, and industry journals. Each subsequent IPCC report notes areas where the science has improved since the previous report and also notes areas where further research is required. | Question: What is \'grey literature\'?', ['non-peer-reviewed sources'], 'mrqa_squad-validation-8513'), tensor(-0.0193)), (('Context: Sophomore - definition of sophomore by The Free DictionarySophomore - definition of sophomore by The Free Dictionary  Sophomore - definition of sophomore by The Free Dictionary  http://www.thefreedictionary.com/sophomore  Related to sophomore: Sophomore Album  soph·o·more  a. A second-year student in a US college.  b. A tenth-grade student in a US high school.  2. A person in the second year of carrying out an endeavor.  3. A three-year-old racehorse, usually in its second year of racing.  adj.  1. Of or relating to the second year of an endeavor, especially of attending a school or college.  2. Being the second in a series: a singer\'s sophomore album.  [Alteration (probably influenced by Greek sophos, wise, and mōros, stupid) of sophumer, from obsolete sophom, sophism, dialectic exercise, variant of sophism .]  sophomore  (Education) chiefly US and Canadian a second-year student at a secondary (high) school or college  adj  (of a book, recording, etc, by an artist) second: her sophomore album.  [C17: perhaps from earlier sophumer, from sophum, variant of sophism + -er1]  soph•o•more  (ˈsɒf əˌmɔr, -ˌmoʊr; ˈsɒf mɔr, -moʊr)  n.  a student in the second year at a high school, college, or university.  [1645–55; earlier sophumer, perhaps =sophum sophism + -er 1]  sophomore  A student in the second year of a college course, or a high-school student in the tenth grade.  ThesaurusAntonymsRelated WordsSynonymsLegend:  lowerclassman , underclassman - an undergraduate who is not yet a senior  Adj.  1.  sophomore - used of the second year in United States high school or college; "the sophomore class"; "his sophomore year"  intermediate - lying between two extremes in time or space or state; "going from sitting to standing without intermediate pushes with the hands"; "intermediate stages in a process"; "intermediate stops on the route"; "an intermediate range plane"  Translations  [ˈsɒfəmɔːʳ] N (US) → estudiante mf de segundo año GRADE  sophomore  n (US) → étudiant (e) m/f de deuxième année  modif  sophomore year → deuxième année f  Want to thank TFD for its existence? Tell a friend about us , add a link to this page, or visit the webmaster\'s page for free fun content .  Link to this page:  View in context  Perhaps the president of a corps notices that one of the membership who is no longer an exempt--that is a freshman-- has remained a sophomore some little time without volunteering to fight; some day, the president, instead of calling for volunteers, will APPOINT this sophomore to measure swords with a student of another corps; he is free to decline--everybody says so--there is no compulsion.  View in context  Then we\'ll be able to look as bored and sophisticated as any Sophomore of them all.  The "freshettes" stood about in detached groups of two or three, looking askance at each other; the "freshies," wiser in their day and generation, had banded themselves together on the big staircase of the entrance hall, where they were shouting out glees with all the vigor of youthful lungs, as a species of defiance to their traditional enemies, the Sophomores, a few of whom were prowling loftily about, looking properly disdainful of the "unlicked cubs" on the stairs.  against six Sophomores and a Freshman from the Gladiatorial College!  when he had slain all the sophomores and was dallying with the  Sophomore dictionary definition | sophomore definedSophomore dictionary definition | sophomore defined  Adjective  (not comparable)  (US) The second in a series , especially, the second of an artist\'s albums or the second of four years in a high school (tenth grade) or university.  The band\'s sophomore album built upon the success of their debut release, catapulting them to megastardom.  Noun  (plural sophomores | Question: A sophomore is a student in which year of a US college?', ['second'], 'mrqa_triviaqa-validation-2136'), tensor(-0.0197)), (('Context: Renewable Heat Incentive scandal  The Renewable Heat Incentive scandal (RHI scandal), also referred to as the Cash for Ash scandal, is a political scandal in Northern Ireland that centres on a failed renewable energy incentive scheme that has been reported to potentially cost the public purse almost £500 million.  The plan was overseen by Arlene Foster of the Democratic Unionist Party (DUP), the then-Minister for Enterprise, Trade and Investment, who failed to introduce proper cost controls, allowing the plan to spiral out of control.  The scheme worked by paying applicants to use renewable energy.  The rate paid was more than the cost of the fuel (the same as in the GBRHI scheme) however, meaning applicants were making profits simply by heating their properties.   Renewable Heat Incentive scandal  The Renewable Heat Incentive scandal (RHI scandal), also referred to as the Cash for Ash scandal, is a political scandal in Northern Ireland that centres on a failed renewable energy incentive scheme that has been reported to potentially cost the public purse almost £500 million.  The plan was overseen by Arlene Foster of the Democratic Unionist Party (DUP), the then-Minister for Enterprise, Trade and Investment, who failed to introduce proper cost controls, allowing the plan to spiral out of control.  The scheme worked by paying applicants to use renewable energy.  The rate paid was more than the cost of the fuel (the same as in the GBRHI scheme) however, meaning applicants were making profits simply by heating their properties.   Arlene Foster  Arlene Isabel Foster MLA PC ("née" Kelly; born 3 July 1970) is a Northern Irish politician who has been the leader of the Democratic Unionist Party since December 2015 and the Member of the Northern Ireland Assembly for Fermanagh and South Tyrone since 2003. | Question: Which organization has been led by the overseer of the Renewable Heat Incentive Scandal since 2003?', ['Northern Ireland Assembly for Fermanagh and South Tyrone'], 'mrqa_hotpotqa-validation-2150'), tensor(-0.0222)), (('Context: <Ol>  A new ruler unites China , founds a new dynasty , and gains the Mandate of Heaven .   China , under the new dynasty , achieves prosperity .   The population increases .   Corruption becomes rampant in the imperial court , and the empire begins to enter decline and instability .   A natural disaster wipes out farm land . The disaster normally would not have been a problem ; however , together with the Corruption and overpopulation , it causes famine .   The famine causes the population to rebel and a civil war ensues .   The ruler loses the Mandate of Heaven .   The population decreases because of the violence .   China goes through a warring states period .   One state emerges victorious .   The state starts a new empire .   The empire gains the Mandate of Heaven .  </Ol> | Question: in the dynastic cycle what is the right to rule called ?', ['the Mandate of Heaven'], 'mrqa_naturalquestions-validation-6157'), tensor(-0.0247)), (('Context: Huguenot immigrants did not disperse or settle in different parts of the country, but rather, formed three societies or congregations; one in the city of New York, another 21 miles north of New York in a town which they named New Rochelle, and a third further upstate in New Paltz. The "Huguenot Street Historic District" in New Paltz has been designated a National Historic Landmark site and contains the oldest street in the United States of America. A small group of Huguenots also settled on the south shore of Staten Island along the New York Harbor, for which the current neighborhood of Huguenot was named. | Question: In what borough is there a neighborhood called Huguenot?', ['Staten Island'], 'mrqa_squad-validation-3058'), tensor(-0.0252)), (('Context: Virginia Wade - International Tennis Hall of FameVirginia Wade  Virginia Wade  7-time major champion, 6-time finalist  Career Titles  Member of the British Wightman Cup Team 1965-1985  Member of the winning team 1968, 1974, 1975, 1978  Fed Cup  Member of the British Federation Cup Team 1967-1970, 1972-1983  Overall Record: 66-33  Virginia Wade WTA Profile  Citizenship: GBR Born: July 10, 1945               in Bournemouth, England Played: Right-handed  Virginia Wade chose the perfect time to become the first British female to win the Wimbledon Ladies Singles Championship in 8 years and the last to win since.  In July 1977, the summer of the monarch’s Silver Jubilee, Wade won her third major title at the All England Club, with nicely coincided with Wimbledon’s centenary year. Queen Elizabeth II, who made no qualms about not being a tennis fan, was making only her second ever appearance at Centre Court when Wade met Betty Stove of the Netherlands for the championship. “If she’s [Queen Elizabeth II] going to be there, I am going to be there too,” Wade said.  Wade, then 31, and singles champion at the US Open in 1968 and at the Australian Open in 1972, wore a beautiful pink cardigan as she arrived on court. After dropping the first set 4-6, her whole demeanor turned a fiery red in winning the final two sets convincingly, 6-3, 6-1. Wade, who was playing in her 17th\xa0of an all-time record 26 Wimbledons, upset No. 1 seed Chris Evert, 6-2, 4-6, 6-1, in the semifinals to reach her one and only final. Once she settled into a groove after the first set, her precise groundstrokes controlled the match. “Winning Wimbledon was the thing that made my career worthwhile,” Wade told the Guardian in 1977. Those in attendance witnessed Queen Elizabeth II present Wade with championship trophy and then sang a rendition of “For She’s a Jolly Good Fellow” to celebrate the victory.  With her victory in London, Wade held the distinction of being the last Brit to win Wimbledon until Andy Murray in 2013. Her winnings were $20,499, compared to today’s multi-million dollar purse. In an interview with the\xa0Independent\xa0in 2007, Wade joked that her Wimbledon title came right on schedule. “Well, Angela Mortimer had won in 1961 and Ann Jones in 1969, so when I won in 1977 we all thought it happened every eight years, but maybe we were just anomalies, because there was Sue Barker and Joe Durie, but then the [British] players just petered out.”  Wade was a lithe 5-foot-8 steady stroke machine who had a beautiful all-court game built on a smooth slice backhand and a forehand that she could hit with topspin or flat. Wade was a thinker between the lines; her shots were patient and calculated. She was adept at controlling tempo and wouldn’t be forced into foolish shots. She would pause a minimum of five seconds before releasing her serve after getting into the ready position and hopped into position to move laterally or take a short ball and attack the net.  In her lengthy 26-year career, Wade won 55 singles titles, eighth on the all-time list. She favored playing at Wimbledon and the US Open the most of any of the four majors. As a 23-year old in 1968, she won the inaugural US Open, doing so as the No. 6 seed with a stunning and unexpected 6-4, 6-2 victory over No. 1 seed Billie Jean King. It was Wade’s fifth trip to the US major and although she would play the event another 15 times, her best finishes afterward were the semifinals in 1969, 1970, and 1975. The 1968 victory earned her $6,000. Wade only traveled to the Australian Open five times, but in 1972 stung another No. 1 seed when defeating crowd favorite and native Evonne Goolagong, who was in the midst of playing in seven consecutive Australian finals, | Question: Prior to Viginia Wade in 1977 who was the last British player to win a tennis Grand Slam title?', ['sue barker'], 'mrqa_triviaqa-validation-5406'), tensor(-0.0252)), (('Context: Historically , computer language syntax was restricted to the ASCII character set ; in the absence of the × character , U + 002A * Asterisk became the de facto standard notation of the multiplication operator in computing . This selection is still reflected in the standard numeric keypad , where the arithmetic operations of addition , subtraction , multiplication , and division are represented by the + , - , * , and / keys , respectively . | Question: what is the multiplication sign on the computer ?', ['*'], 'mrqa_naturalquestions-validation-10364'), tensor(-0.0255)), (('Context: The Elizabeth Files » Mary Queen of Scots Marries Lord DarnleyThe Elizabeth Files » Mary Queen of Scots Marries Lord Darnley  Mary Queen of Scots Marries Lord Darnley  Posted By  claire on July 29, 2010  Mary Queen of Scots  On this day in history, the 29th July 1565, Elizabeth I’s nemesis, Mary Queen of Scots, married Henry Stuart, Lord Darnley, at Holyrood Palace (the Palace of Holyroodhouse), Edinburgh.  Let’s celebrate their union by giving some facts about the happy couple:-  The Bride – Mary Queen of Scots  Birth: Mary, Queen of Scots, also known as Mary Stuart, Mary Stewart and Marie Stuart, was born on the 8th December 1542 at Linlithgow Palace, Scotland.  Parents: Mary’s parents were James V of Scotland (son of James IV and Margaret Tudor) and Mary of Guise. She was James’s only child to survive and he actually died six days after her birth.  Queen of Scotland After her father’s death on the 14th December, Mary became Queen of Scotland and was crowned at Stirling in September 1543. As Mary was just an infant, the country was ruled by regents on her behalf.  Betrothals: On the 1st July 1543, at the age of 7 months, the Treaty of Greenwich, between England and Scotland, promised Mary in marriage to Henry VIII’s son, Prince Edward. When the pro-Catholic and pro-French Cardinal Beaton became powerful in Scotland and joined forces with the Earl of Arran, the treaty was rejected by the Scottish Parliament and trouble began between England and Scotland. The French came to the aid of Scotland and on the 7th July 1548 a marriage treaty between France and Scotland promised the 5 year old Mary in marriage to Henry II’s son, François, the Dauphin.  Upbringing: From the age of 5 until the age of 19 Mary lived in France. There she received an excellent education, learning French, Italian, Spanish, Greek, Latin and Scots, as well as music, needlework and poetry.  Return to Scotland: Francois (Francis II of France) died in 1560 and his mother, Catherine de’ Medici, took control of the country, as regent for her other son, Charles IX. Mary returned to Scotland on the 19th August 1561.  Marriages: 24th April 1558 to the Dauphin François at Notre Dame. he died in 1560. 29th July 1565 to Henry Stuart, Lord Darnley, he was assassinated in 1567. 15th May 1567 to James Hepburn, Lord Bothwell, he died in 1578.  Mary and her son James  Issue: James VI of Scotland (James I of England) by Lord Darnley, born on 19th June 1566. Mary miscarried twins (fathered by Bothwell) in July 1567.  Titles: Queen of Scotland from 14th December 1542 until 24th July 1567 when she was forced to abdicate.  At the death of Mary I of England in 1558, Henry II of France declared that François and Mary were King and Queen of England and Mary started bearing the royal arms of England.  On the death of her father-in-law, Henry II of France, on the 10th July 1559 Mary became Queen Consort of France, a position she held until 5th December 1560.  Appearance: Mary, Queen of Scots, was considered a beauty. She was tall (around 5′ 11) with auburn hair, hazel eyes and a heart-shaped face.  Scandal: In March 1566, Mary’s husband, Lord Darnley, and some friends murdered Mary’s private secretary, David Rizzio, in front of his pregnant wife. He was jealous of Mary’s friendship with Rizzio. Darnley became a real problem for Mary and the Scottish Lords and in February 1567 Darnley was killed in an explosion at Kirk O’Field. It is thought that James Hepburn, Lord Bothwell, supplied the gunpowder but he was acquitted of murder in April 1567. The famous “Casket Letters” implicated Mary in the murder of her husband, but these are generally believed to have been forgeries.  On the 24th April 1567, Mary was kidnapped by Bothwell (it is | Question: What was the name of the letters that in 1566 implicated Mary Queen of Scots in the murder of her second husband, Lord Darnley?', ['casket letters'], 'mrqa_triviaqa-validation-1935'), tensor(-0.0261)), (('Context: The collection of drawings includes over 10,000 British and 2,000 old master works, including works by: Dürer, Giovanni Benedetto Castiglione, Bernardo Buontalenti, Rembrandt, Antonio Verrio, Paul Sandby, John Russell, Angelica Kauffman, John Flaxman, Hugh Douglas Hamilton, Thomas Rowlandson, William Kilburn, Thomas Girtin, Jean Auguste Dominique Ingres, David Wilkie, John Martin, Samuel Palmer, Sir Edwin Henry Landseer, Lord Frederic Leighton, Sir Samuel Luke Fildes and Aubrey Beardsley. Modern British artists represented in the collection include: Paul Nash, Percy Wyndham Lewis, Eric Gill, Stanley Spencer, John Piper, Graham Sutherland, Lucian Freud and David Hockney. | Question: Approximately how many British drawings are included in the V&A collection?', ['10,000', 'over 10,000'], 'mrqa_squad-validation-5517'), tensor(-0.0265)), (('Context: Honda Ballade  The Honda Ballade is a subcompact automobile built by Honda of Japan.  It began as a four-door higher equipment content version of the Civic in 1980.  The Ballade was developed at the same time the Honda Vigor appeared, which was a higher content Honda Accord.  The Ballade was sold exclusively in Japan at "Honda Verno" dealerships alongside the Vigor, Prelude, CR-X, and Quint.  In the UK it was launched at the same time as the very similar Triumph Acclaim with which it shared a Honda built engine.   Honda Ballade  The Honda Ballade is a subcompact automobile built by Honda of Japan.  It began as a four-door higher equipment content version of the Civic in 1980.  The Ballade was developed at the same time the Honda Vigor appeared, which was a higher content Honda Accord.  The Ballade was sold exclusively in Japan at "Honda Verno" dealerships alongside the Vigor, Prelude, CR-X, and Quint.  In the UK it was launched at the same time as the very similar Triumph Acclaim with which it shared a Honda built engine.   Honda CR-X  The Honda CR-X, originally launched as the Honda Ballade Sports CR-X in Japan, is a front-wheel-drive sport compact car manufactured by Honda between 1983 and 1991.  It was replaced by the Honda CR-X del Sol for the 1992 model year.  Although there are many supposed definitions for the acronym CR-X, the most widely accepted are "Civic rally cross", and "Civic renaissance model X".   Honda CR-X  The Honda CR-X, originally launched as the Honda Ballade Sports CR-X in Japan, is a front-wheel-drive sport compact car manufactured by Honda between 1983 and 1991.  It was replaced by the Honda CR-X del Sol for the 1992 model year.  Although there are many supposed definitions for the acronym CR-X, the most widely accepted are "Civic rally cross", and "Civic renaissance model X". | Question: The "civic rally cross" was sold alongside what other model sold exclusively in Japan at Honda Verno dealerships?', ['Honda Ballade'], 'mrqa_hotpotqa-validation-400'), tensor(-0.0273)), (("Context: Professor Eobard Thawne is introduced on The Flash as a scientist from the future who duplicated the reaction that gave Barry Allen / The Flash his powers and became the hero 's archenemy , the Reverse - Flash . Eobard traveled back in time to the year 2000 to kill his nemesis as a child . When the 2024 Flash intervened , the Reverse - Flash murdered Barry 's mother and framed his father instead , thus erasing this Flash from existence while unintentionally severing his own connection to the Speed Force , leaving him stranded in the 21st Century . Knowing that Dr. Harrison Wells would be responsible for creating metahumans in 2020 , Eobard killed the scientist and assumed his identity . He then founded S.T.A.R. Labs and created a particle accelerator to turn Barry into the Flash and regain his own link to the Speed Force . Faking paralysis , `` Dr. Wells '' pushes Barry to run faster in hopes of using the latter 's pure Speed Force connection to travel back to his time . After Eobard 's secret is discovered , he is defeated by the Flash , the Arrow and Firestorm . Eobard then offers Barry a chance to travel back in time to save his mother in exchange for a time sphere to return to the future , but Barry ultimately chooses not to do so and returns to destroy Eobard 's machine . As the Reverse - Flash prepares to kill the Flash , his ancestor Detective Eddie Thawne kills himself and seemingly erases Eobard from existence . A year later , Barry temporarily creates a new reality and restores a time remnant of Eobard to the timeline in the process . | Question: who kills barry 's mom in the flash ?", ['Professor Eobard Thawne'], 'mrqa_naturalquestions-validation-3033'), tensor(-0.0279)), (("Context: The United States two - dollar bill ( $2 ) is a current denomination of U.S. currency . The third U.S. President ( 1801 -- 09 ) , Thomas Jefferson , is featured on the obverse of the note . The reverse features an engraving of the painting The Declaration of Independence by John Trumbull . Throughout the $2 bill 's pre-1929 life as a large - sized note , it was issued as a United States Note , National Bank Note , silver certificate , Treasury or `` Coin '' Note and Federal Reserve Bank Note . When U.S. currency was changed to its current size , the $2 bill was issued only as a United States Note . Production went on until 1966 , when the series was discontinued . Ten years passed before the $2 bill was reissued as a Federal Reserve Note with a new reverse design . Two - dollar bills are seldom seen in circulation as a result of banking policies with businesses which has resulted in low production numbers due to lack of demand . This comparative scarcity in circulation , coupled with a lack of public knowledge that the bill is still in production and circulation , has also inspired urban legends and occasionally has created problems for people trying to use the bill to make purchases . | Question: when were 2 dollar bills stopped being made ?", ['current denomination of U.S. currency'], 'mrqa_naturalquestions-validation-2753'), tensor(-0.0289)), (("Context: Rolls-Royce Welland  The Rolls-Royce RB.23 Welland was Britain's first production jet engine.  It entered production in 1943 for the Gloster Meteor.  The name Welland is taken from the River Welland, in keeping with the Rolls-Royce policy of naming early jet engines after rivers based on the idea of continuous flow, air through the engine and water in a river.   River Welland  The River Welland is a lowland river in the east of England, some 65 mi long.  It drains part of the Midlands eastwards to The Wash.  The river rises in the Hothorpe Hills, at Sibbertoft in Northamptonshire, then flows generally northeast to Market Harborough, Stamford and Spalding, to reach The Wash near Fosdyke.  It is a major waterway across the part of the Fens called South Holland, and is one of the Fenland rivers which were laid out with washlands.  There are two channels between widely spaced embankments with the intention that flood waters would have space in which to spread while the tide in the estuary prevented free egress.  However, after the floods of 1947, new works such as the Coronation Channel were constructed to control flooding in Spalding and the washes are no longer used solely as pasture, but may be used for arable farming. | Question: Rolls-Royce Welland's name is taken from this river that is low long ?", ['65 mi'], 'mrqa_hotpotqa-validation-1201'), tensor(-0.0289)), (('Context: The Life and Crimes of Bonnie Parker and Clyde BarrowThe Life and Crimes of Bonnie Parker and Clyde Barrow  By Jennifer Rosenberg  Who Were\xa0Bonnie and Clyde?  It was during the Great Depression that Bonnie Parker and Clyde Barrow went on their two-year crime spree (1932-1934). The general attitude in the United States\xa0was against government and Bonnie and Clyde used that to their advantage. With an image closer to Robin Hood rather than mass murderers, Bonnie and Clyde captured the imagination of the nation.  Dates: Bonnie Parker (October 1, 1910 -- May 23, 1934); Clyde Barrow (March 24, 1909 -- May 23, 1934)  Also Known As: Bonnie Elizabeth Parker, Clyde Chestnut Barrow, The Barrow Gang  Overview of Bonnie and Clyde  In some ways it was easy to romanticize Bonnie and Clyde. They were a young couple in love who were out on the open road, running from the "big, bad law" who were "out to get them." Clyde\'s impressive driving skill got the gang out of many close calls, while Bonnie\'s poetry won the hearts of many. (Clyde loved Fords so much, he even wrote a letter to Henry Ford himself!)  continue reading below our video  Profile of Bonnie and Clyde  Although Bonnie and Clyde had killed people, they were equally known for kidnapping policemen who had caught up to them and then driving them around for hours only to release them, unharmed, hundreds of miles away. The two seemed like they were on an adventure, having fun while easily side-stepping the law.  As with any image, the truth behind Bonnie and Clyde was far from their portrayal in the newspapers. Bonnie and Clyde were responsible for 13 murders, some of whom were innocent people, killed during one of Clyde\'s many bungled robberies. Bonnie and Clyde lived out of their car, stealing new cars as often as possible, and lived off the money they stole from small grocery stores and gas stations.  While Bonnie and Clyde sometimes robbed banks, they never managed to walk away with very much money. Bonnie and Clyde were desperate criminals, constantly fearing what they were sure was to come -- dying in a hail of bullets from a police ambush.  Background of Bonnie  Bonnie Parker was born on October 1, 1910 in Rowena, Texas as the second of three children to Henry and Emma Parker. The family lived somewhat comfortably off Henry Parker\'s job as a bricklayer, but when he died unexpectedly in 1914, Emma Parker moved the family in with her mother in the small town of Cement City, Texas (now part of Dallas).  From all accounts, Bonnie Parker was beautiful. She stood 4\' 11" and weighed a mere 90 pounds. She did well in school and loved to write poetry. ( Two poems that she wrote while on the run helped make her famous.)  Bored with her average life, Bonnie dropped out of school at age 16 and married Roy Thornton. The marriage wasn\'t a happy one and Roy began to spend a lot of time away from home by 1927. Two years later, Roy was caught for robbery and sentenced to five years in prison. They never divorced.  While Roy was away, Bonnie worked as a waitress; however, she was out of a job just as the Great Depression was really getting started at the end of 1929.  Background of Clyde  Clyde Barrow was born on March 24, 1909 in Telico, Texas as the sixth of eight children to Henry and Cummie Barrow. Clyde\'s parents were tenant farmers , often not making enough money to feed their children. During the rough times, Clyde was frequently sent to live with other relatives.  When Clyde was 12-years old, his parents gave up tenant farming and moved to West Dallas where Henry opened up a gas station.  At that time, West Dallas was a very rough neighborhood and Clyde fit right in. Clyde and his older brother, Marvin Ivan "Buck" Barrow, were often in trouble with the law for they were frequently stealing things | Question: In which year in the 1930\'s were Bonnie and Clyde killed?', ['1934'], 'mrqa_triviaqa-validation-6683'), tensor(-0.0290)), (("Context: 2016 Blancpain GT Series Sprint Cup  The 2016 Blancpain GT Series Sprint Cup was the fourth season following on from the demise of the SRO Group's FIA GT1 World Championship (an auto racing series for grand tourer cars), the third with the designation of Blancpain Sprint Series or Blancpain GT Series Sprint Cup.  After developing their partnership, Blancpain and the SRO decided that 2016 would see both the Sprint and Endurance Series further integrated into the Blancpain GT Series, putting the emphasis on the prestigious overall drivers' and manufacturers' titles causing the Sprint Series name to change from Blancpain Sprint Series to Blancpain GT Series Sprint Cup.   FIA GT1 World Championship  The FIA GT1 World Championship was a world championship sports car racing series developed by the SRO Group and regulated by the Fédération Internationale de l'Automobile (FIA), held from 2010 to 2012.  It featured multiple grand tourer race cars based on production road cars and conforming with the GT1 (2010–2011) and GT3 (2012) regulations competing in one-hour races on multiple continents.  All cars were performance balanced with weight and restrictor adjustments to artificially equalise their performance.  Championships were awarded each season for drivers and teams. | Question: What years was the auto racing series held that was followed on by the 2016 Blancpain GT Series Sprint Cup?", ['2010 to 2012'], 'mrqa_hotpotqa-validation-1376'), tensor(-0.0294)), (('Context: David Perdue\'s Charles Dickens Page - David CopperfieldDavid Perdue\'s Charles Dickens Page - David Copperfield  David Copperfield  FAQ & Email  Dickens began writing an autobiography        in the late 1840s which he shared with his friend and future biographer, John Forster . Dickens found the        writing too painful and burned what he had written. He opted instead to        work his story into the fictional account of David Copperfield.  In the novel Dickens\' painful memories of being taken from school to work        at Warren\'s Blacking Factory while his father is in prison for debt are told through David\'s account        of Murdstone and Grinby\'s warehouse. The financial troubles of the Micawbers ,        with whom David was boarding at        the time, mirror Dickens\' parents, John and Elizabeth Dickens,        financial difficulties.  When David is asked by Mrs Micawber to take some of their treasured possessions to the pawn shop to help meet        their obligations, Dickens is recalling painful memories of having to pawn        off the very books he read and treasured as a child to ease his family\'s        financial woes.  On Dickens\' death Forster wrote The        Life of Charles Dickens , which is still the definitive biography        of Dickens, although many of the more negative aspects of Dickens life are        glossed over or missing altogether. Forster\'s biography included the autobiographical        fragment Dickens had given him. This was the first the public knew of Dickens\'        difficult childhood that had so heavily shaped his early work.  Oops...  Dickens originally introduced the        character of the dwarf, Miss Mowcher ,        as an aid to Steerforth\'s plan        to elope with Emily . Mrs. Jane Seymour        Hill, Dickens\' wife Catherine\'s chiropodist, recognized herself as the original for this character and threatened        a lawsuit . Dickens changed the character, in later monthly installments        of the novel, to an honest friend who abhors Steerforth\'s actions. She later        assists in the capture of Littimer .  Like Dickens, David teaches himself        shorthand and becomes a parliamentary reporter. David laments on the difficulties        encountered mastering this art:  "I bought an approved scheme of the noble art and mystery of stenography        (which cost me ten and sixpence); and plunged into a sea of perplexity that        brought me, in a few weeks, to the confines of distraction. The changes        that were rung upon dots, which in such a position meant such a thing, and        in such another position something else, entirely different; the wonderful        vagaries that were played by circles; the unaccountable consequences that        resulted from marks like flies\' legs; the tremendous effects of a curve        in a wrong place; not only troubled my waking hours, but reappeared before        me in my sleep."  Dickens hints at his feelings for politics when David says of his parliamentary        reporting:  "Britannia, that unfortunate female, is always before me, like a trussed        fowl: skewered through and through with office-pens, and bound hand and        foot with red tape. I am sufficiently behind the scenes to know the worth        of political life. I am quite an Infidel about it, and shall never be converted."  During the writing of David Copperfield        Dickens was actively involved in the day-to-day operation of Urania        Cottage , a home for homeless women, which he administered on behalf        of his friend, philanthropist Angela        Burdett Coutts . The home helped to separate homeless, and "fallen",        women from previous lifestyles, educate them in the execution of household        duties and self-discipline, and then help them emigrate to Australia to        begin new lives.  In David Copperfield, Dickens has several of the major characters emigrate        to Australia: the Micawbers , Mr.        Peggotty , Emily , Martha, and Mr.        Mell . Each of these characters are successful in beginning a new life        in the English colony.  Did Dickens visit the birthplace of David Copperfield? Visit the modern Blundeston where the debate rages and David Copperfield    lives on.  Dickens lampoons "the separate system" used at the | Question: What colour hair did Charles Dickens\' character David Copperfield have?', ['red'], 'mrqa_triviaqa-validation-1437'), tensor(-0.0298)), (('Context: In anglophone academic works, theories regarding imperialism are often based on the British experience. The term "Imperialism" was originally introduced into English in its present sense in the late 1870s by opponents of the allegedly aggressive and ostentatious imperial policies of British prime Minister Benjamin Disraeli. It was shortly appropriated by supporters of "imperialism" such as Joseph Chamberlain. For some, imperialism designated a policy of idealism and philanthropy; others alleged that it was characterized by political self-interest, and a growing number associated it with capitalist greed. Liberal John A. Hobson and Marxist Vladimir Lenin added a more theoretical macroeconomic connotation to the term. Lenin in particular exerted substantial influence over later Marxist conceptions of imperialism with his work Imperialism, the Highest Stage of Capitalism. In his writings Lenin portrayed Imperialism as a natural extension of capitalism that arose from need for capitalist economies to constantly expand investment, material resources and manpower in such a way that necessitated colonial expansion. This conception of imperialism as a structural feature of capitalism is echoed by later Marxist theoreticians. Many theoreticians on the left have followed in emphasizing the structural or systemic character of "imperialism". Such writers have expanded the time period associated with the term so that it now designates neither a policy, nor a short space of decades in the late 19th century, but a world system extending over a period of centuries, often going back to Christopher Columbus and, in some accounts, to the Crusades. As the application of the term has expanded, its meaning has shifted along five distinct but often parallel axes: the moral, the economic, the systemic, the cultural, and the temporal. Those changes reflect - among other shifts in sensibility - a growing unease, even squeamishness, with the fact of power, specifically, Western power. | Question: What was the idealized value of imperialism?', ['idealism and philanthropy', 'philanthropy'], 'mrqa_squad-validation-10015'), tensor(-0.0302)), (('Context: Tetanus Disease | Home | Lockjaw | CDCTetanus Disease | Home | Lockjaw | CDC  ShareCompartir  Tetanus is an infection caused by bacteria called Clostridium tetani. When the bacteria invade the body, they produce a poison (toxin) that causes painful muscle contractions. Another name for tetanus is "lockjaw" because it often causes a person\'s neck and jaw muscles to lock, making it hard to open the mouth or swallow. Vaccines are recommended for infants, children, teens, and adults to prevent tetanus.  Tetanus disease | definition of Tetanus disease by Medical ...Tetanus disease | definition of Tetanus disease by Medical dictionary  Tetanus disease | definition of Tetanus disease by Medical dictionary  http://medical-dictionary.thefreedictionary.com/Tetanus+disease  \xa0  Definition  Tetanus is a rare but often fatal disease that affects the central nervous system by causing painful muscular contractions. It begins when tetanus bacteria enter the body, usually through a wound or cut exposed to contaminated soil. Tetanus is easily preventable through vaccination .  Description  Tetanus is rare in the United States, with nearly all cases occurring in adults who were not vaccinated as children. About 100 cases are reported each year; 63% of these occur in people over the age of 50. The number of tetanus cases in the United States has steadily decreased since the 1940s (500 to 600 cases per year); the number of reported cases has remained at approximately 50 to 100 cases per year since the mid-1970s. In 1999, however, the lowest number of annual cases to date was reported (33, or 0.02 per 100,000).  Tetanus causes convulsive muscle spasms and rigidity that can lead to respiratory paralysis and death . It is sometimes called "lockjaw" because one of the most common symptoms is a stiff jaw, unable to be opened. Sometimes, tetanus affects only the part of the body where the infection began, but in almost all of reported cases, it spreads to the entire body. The incubation period from the time of the injury until the first symptoms appear ranges from two to 50 days. Symptoms usually occur within five to 10 days. When symptoms occur early, the chance of death is increased. Tetanus is not contagious.  Causes and symptoms  Tetanus is caused by a bacteria called Clostridium tetani, whose spores (the dormant form) are found in soil, street dust, and animal (or even human) feces. Tetanus spores germinate in the body, producing a highly poisonous neurotoxin in the blood, spreading to the nervous system. The infection is usually transmitted through deep puncture wounds or cuts or scratches that are not cleaned well. Between 1997 and 1999, approximately 64% of tetanus cases in the United States were associated with such wounds as punctures, lacerations, or abrasions. Many people associate tetanus with rusty nails and other dirty objects, but any wound can be a source. Less common ways of getting tetanus are animal scratches and bites, surgical wounds, dental work, and therapeutic abortion. About 18% of cases reported between 1997 and 1999 were a result of intravenous drug use. Cases have also been reported in people with no known wound or medical condition.  The first symptom of tetanus is often a stiff or "locked" jaw that prevents the patient from opening his/her mouth or swallowing. This is also called trismus and results in a facial expression called a sardonic smile (or risus sardonicus). Stiffness of the neck and other muscles throughout the body and uncontrollable spasms often follow. Sometimes these convulsions are severe enough to cause broken bones. The bacterial toxin (tetanospasmin) affects the nerve endings, causing a continuous stimulation of muscles. Other symptoms include irritability, restlessness, loss of appetite, and drooling. People with tetanus that is localized experience pain and tingling only at the wound site and spasms in nearby muscles.  In the underdeveloped world, neonatal tetanus accounts for about one-half of tetanus deaths and is related to infection of the umbilical stump in a baby born of an unimmunized mother. The Centers for Disease Control and Prevention (CDC) estimate that over 270,000 deaths occur annually worldwide as a result of neonatal tetanus. In contrast, only two cases of neonatal tetanus in the United | Question: Which disease is also called lockjaw?', ['tetanus'], 'mrqa_triviaqa-validation-7253'), tensor(-0.0323)), (('Context: An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses . The same principle is used for funicular railways with two connected railway cars on inclined tracks , and for the elevators on the Eiffel Tower which counterbalance each other . Ski lifts are another example , where the gondolas move on a closed ( continuous ) pulley system up and down the mountain . The ski lift is similar to the counter-weighted elevator , but with a constraining force provided by the cable in the vertical dimension thereby achieving work in both the horizontal and vertical dimensions . Boat lifts are another type of counter-weighted elevator system approximating an Atwood machine . | Question: what is a real world application of an atwood machine ?', ['An elevator with a counterbalance'], 'mrqa_naturalquestions-validation-2730'), tensor(-0.0334)), (('Context: Afonwen  Afonwen (] ; Welsh: "Afon-wen" ) is a town in Flintshire, Wales.  It is situated just under four miles from the A55 North Wales Expressway and on the A541 Mold-Denbigh road.  At the 2001 Census, the population of Afonwen was included into the civil parish of Caerwys and was 1,319, with a total ward population of 2,496.   A55 road  The A55, also known as the North Wales Expressway (Welsh: "Gwibffordd Gogledd Cymru") and the Chester to Bangor Trunk Road, is a major road in Britain.  Its entire length is a dual carriageway primary route, with the exception of the point where it crosses the Britannia Bridge over the Menai Strait and several short sections where there are gaps in between the two carriageways.  All junctions are grade separated except for two roundabouts — one east of Penmaenmawr and one in Llanfairfechan.  The road originally ran from Chester to Bangor but was extended parallel to the A5 across Anglesey to just outside Holyhead Docks in 2001.  The road improvements have been part funded with European money, under the Trans-European Networks programme, as the route is designated part of Euroroute E22 (Holyhead - Leeds - Amsterdam - Hamburg - Malmö - Riga - Moscow - Perm - Ekaterinburg - Ishim). | Question: Afonwen is situated just under four miles from what road also known as the North Wales Expressway ?', ['A55'], 'mrqa_hotpotqa-validation-1888'), tensor(-0.0387)), (('Context: As of December 9 , 2016 , all episodes of Raw are available on demand on the WWE Network . Recent episodes are available for on - demand viewing 30 days after the original air date . | Question: when does monday night raw come on hulu ?', ['30 days after the original air date'], 'mrqa_naturalquestions-validation-5146'), tensor(-0.0414)), (('Context: The Internet protocol suite ( TCP / IP ) was developed by Robert E. Kahn and Vint Cerf in the 1970s and became the standard networking protocol on the ARPANET , incorporating concepts from the French CYCLADES project directed by Louis Pouzin . In the early 1980s the NSF funded the establishment for national supercomputing centers at several universities , and provided interconnectivity in 1986 with the NSFNET project , which also created network access to the supercomputer sites in the United States from research and education organizations . Commercial Internet service providers ( ISPs ) began to emerge in the very late 1980s . The ARPANET was decommissioned in 1990 . Limited private connections to parts of the Internet by officially commercial entities emerged in several American cities by late 1989 and 1990 , and the NSFNET was decommissioned in 1995 , removing the last restrictions on the use of the Internet to carry commercial traffic . | Question: when was the internet introduced to the public ?', ['in the very late 1980s'], 'mrqa_naturalquestions-validation-683'), tensor(-0.0419)), (('Context: Often rules apply to all goods neutrally, but may have a greater practical effect on imports than domestic products. For such "indirect" discriminatory (or "indistinctly applicable") measures the Court of Justice has developed more justifications: either those in article 36, or additional "mandatory" or "overriding" requirements such as consumer protection, improving labour standards, protecting the environment, press diversity, fairness in commerce, and more: the categories are not closed. In the most famous case Rewe-Zentral AG v Bundesmonopol für Branntwein, the Court of Justice found that a German law requiring all spirits and liqueurs (not just imported ones) to have a minimum alcohol content of 25 per cent was contrary to TFEU article 34, because it had a greater negative effect on imports. German liqueurs were over 25 per cent alcohol, but Cassis de Dijon, which Rewe-Zentrale AG wished to import from France, only had 15 to 20 per cent alcohol. The Court of Justice rejected the German government\'s arguments that the measure proportionately protected public health under TFEU article 36, because stronger beverages were available and adequate labelling would be enough for consumers to understand what they bought. This rule primarily applies to requirements about a product\'s content or packaging. In Walter Rau Lebensmittelwerke v De Smedt PVBA the Court of Justice found that a Belgian law requiring all margarine to be in cube shaped packages infringed article 34, and was not justified by the pursuit of consumer protection. The argument that Belgians would believe it was butter if it was not cube shaped was disproportionate: it would "considerably exceed the requirements of the object in view" and labelling would protect consumers "just as effectively". In a 2003 case, Commission v Italy Italian law required that cocoa products that included other vegetable fats could not be labelled as "chocolate". It had to be "chocolate substitute". All Italian chocolate was made from cocoa butter alone, but British, Danish and Irish manufacturers used other vegetable fats. They claimed the law infringed article 34. The Court of Justice held that a low content of vegetable fat did not justify a "chocolate substitute" label. This was derogatory in the consumers\' eyes. A ‘neutral and objective statement’ was enough to protect consumers. If member states place considerable obstacles on the use of a product, this can also infringe article 34. So, in a 2009 case, Commission v Italy, the Court of Justice held that an Italian law prohibiting motorcycles or mopeds pulling trailers infringed article 34. Again, the law applied neutrally to everyone, but disproportionately affected importers, because Italian companies did not make trailers. This was not a product requirement, but the Court reasoned that the prohibition would deter people from buying it: it would have "a considerable influence on the behaviour of consumers" that "affects the access of that product to the market". It would require justification under article 36, or as a mandatory requirement. | Question: Which year was the case Commission v Italy that dealt with cocoa products?', ['2003'], 'mrqa_squad-validation-4253'), tensor(-0.0458)), (("Context: Petra Kvitová career statistics  This is a list of the main career statistics of Czech professional tennis player Petra Kvitová.  To date, Kvitová has won 20 singles titles including two Grand Slam singles titles at the Wimbledon Championships, one WTA Tour Championships singles title, two WTA Premier Mandatory singles titles and four WTA Premier 5 singles titles.  She was also the bronze medallist at the 2016 Rio Olympics, a semifinalist at the 2010 Wimbledon Championships, 2012 Australian Open and 2012 French Open and a quarterfinalist at the 2011 Australian Open, 2012 Wimbledon Championships, 2013 Wimbledon Championships, 2015 US Open and 2017 US Open.  Kvitová reached her career-high ranking of world no. 2 on 31 October 2011.   2017 US Open (tennis)  The 2017 US Open was the 137th edition of tennis' US Open and the fourth and final Grand Slam event of the year.  It was held on outdoor hard courts at the USTA Billie Jean King National Tennis Center in New York City.  Experimental rules featured in qualifying for the main draw as well as in the junior, wheelchair and exhibition events. | Question: What edition of tennis' US Open was the 2017 US Open when Petra Kvitova was a quarterfinalist?", ['137th'], 'mrqa_hotpotqa-validation-409'), tensor(-0.0488)), (('Context: Other components are often present; pumps (such as an injector) to supply water to the boiler during operation, condensers to recirculate the water and recover the latent heat of vaporisation, and superheaters to raise the temperature of the steam above its saturated vapour point, and various mechanisms to increase the draft for fireboxes. When coal is used, a chain or screw stoking mechanism and its drive engine or motor may be included to move the fuel from a supply bin (bunker) to the firebox. See: Mechanical stoker | Question: What is another name for a coal supply bin?', ['bunker'], 'mrqa_squad-validation-3389'), tensor(-0.0496)), (('Context: Chun Woo-hee  Chun Woo-hee (born April 20, 1987) is a South Korean actress.  She made her acting debut in 2004, but first drew attention with her supporting role as a rebellious teenager in the 2011 box-office hit "Sunny".  In 2014, Chun received domestic and international critical acclaim for her first leading role as the title character in "Han Gong-ju", a coming-of-age indie about a traumatized young woman trying to move on with her life after a tragedy.  Her other notable films include "The Beauty Inside" (2015), "Love, Lies" (2016) and "The Wailing" (2016).   Chun Woo-hee  Chun Woo-hee (born April 20, 1987) is a South Korean actress.  She made her acting debut in 2004, but first drew attention with her supporting role as a rebellious teenager in the 2011 box-office hit "Sunny".  In 2014, Chun received domestic and international critical acclaim for her first leading role as the title character in "Han Gong-ju", a coming-of-age indie about a traumatized young woman trying to move on with her life after a tragedy.  Her other notable films include "The Beauty Inside" (2015), "Love, Lies" (2016) and "The Wailing" (2016).   The Wailing (film)  The Wailing () is a 2016 South Korean horror film directed by Na Hong-jin about a policeman who investigates a series of mysterious killings and illnesses.  It was a commercial success. | Question: Chun Woo-hee\'s notable films include a South Korean horror film about  a policeman who investigates a series of mysterious killings and what?', ['illnesses'], 'mrqa_hotpotqa-validation-5325'), tensor(-0.0576)), (('Context: That Was the Week That WasThat Was the Week That Was, informally TWTWTW or TW3, was a satirical television comedy programme on BBC Television in 1962 and 1963. It was devised, produced and directed by Ned Sherrin and presented by David Frost. An American version by the same name aired on NBC from 1964 to 1965, also featuring Frost.  The programme is considered a significant element of the satire boom in the UK in the early 1960s. It broke ground in comedy through lampooning the establishment and political figures. Its broadcast coincided with coverage of the politically charged Profumo affair and John Profumo, the politician at the centre of the affair, became a target for derision. TW3 was first broadcast on Saturday, 24 November 1962.  Cast and writers  Cast members included cartoonist Timothy Birdsall, political commentator Bernard Levin, and actors Lance Percival, who sang topical calypsos, many improvised to suggestions from the audience, Kenneth Cope, Roy Kinnear, Willie Rushton, Al Mancini, Robert Lang,  David Kernan and Millicent Martin. The last two were also singers and the programme opened with a song\xa0– That Was The Week That Was\xa0– sung by Martin to Ron Grainer\'s theme tune and enumerating topics in the news. Frankie Howerd also guested with stand-up comedy.   Script-writers included John Albery, John Antrobus, John Betjeman, John Bird, Graham Chapman, John Cleese, Peter Cook, Roald Dahl, Richard Ingrams, Lyndon Irving, Gerald Kaufman, Frank Muir, David Nobbs, Denis Norden, Bill Oddie, Dennis Potter, Eric Sykes, Kenneth Tynan, and Keith Waterhouse.   Programme  The programme opened with a song ("That was the week that was, It\'s over, let it go ...") sung by Millicent Martin, referring to news of the week just gone. Lance Percival sang a topical calypso each week. Satirical targets, such as Prime Minister Harold Macmillan and Home Secretary Henry Brooke were lampooned in sketches, debates and monologues. Other targets were the monarchy, Britain\'s declining status as a global power, racism (particularly in the American South and South Africa under Apartheid), sexual and social hypocrisy, the class system, and the BBC itself. Well-remembered sketches include a \'consumers\' guide to religion\', which discussed relative merits of faiths in the manner of a Which? magazine report.  On Saturday, 20 October 1962 the award of Nobel prizes to John Kendrew and Max Perutz, and to Francis Crick, James D. Watson, and Maurice Wilkins was satirised in a short sketch with the prizes referred to as the Alfred Nobel Peace Pools; in this sketch Watson was called "Little J.D. Watson" and "Who\'d have thought he\'d ever get the Nobel Prize? Makes you think, doesn\'t it". The germ of the joke was that Watson was only 25 when he helped discover DNA; much younger than the others.  TW3 was broadcast on Saturday night and attracted an audience of 12 million. It often under- or overran as cast and crew worked through material as they saw fit. At the beginning of the second season in the autumn of 1963, in an attempt to assert control over the programme, the BBC scheduled repeats of The Third Man television series after the end of TW3. Frost suggested a means of sabotaging this tactic to Sherrin, and he agreed. For three weeks, Frost read out the plot of The Third Man, until the repeats were abandoned following the direct intervention of the BBCs Director General Hugh Greene.   Frost often ended a satirical attack with the remark "But seriously, he\'s doing a grand job".  At the end of each episode, Frost usually signed off with: "That was the week, that was." At the end of the final programme he announced: "That was That Was The Week That Was...that was."  Kennedy tribute  For the edition on Saturday, 23 November 1963, the day after the assassination of United States President John F. Kennedy, TW3 produced a shortened 20-minute programme with no satire, reflecting on the loss, including a contribution from Dame | Question: Who was the deviser, producer and director \'That Was The Week That Was\'?', ['ned sherrin'], 'mrqa_triviaqa-validation-4054'), tensor(-0.0584)), (('Context: In World War II, it was recognised that the Rhine would present a formidable natural obstacle to the invasion of Germany, by the Western Allies. The Rhine bridge at Arnhem, immortalized in the book, A Bridge Too Far and the film, was a central focus of the battle for Arnhem, during the failed Operation Market Garden of September 1944. The bridges at Nijmegen, over the Waal distributary of the Rhine, were also an objective of Operation Market Garden. In a separate operation, the Ludendorff Bridge, crossing the Rhine at Remagen, became famous, when U.S. forces were able to capture it intact – much to their own surprise – after the Germans failed to demolish it. This also became the subject of a film, The Bridge at Remagen. Seven Days to the River Rhine was a Warsaw Pact war plan for an invasion of Western Europe during the Cold War. | Question: Where is the Rhine Bridge?', ['Arnhem'], 'mrqa_squad-validation-9358'), tensor(-0.0586)), (('Context: The Iroquois was originally designated HU - 1 , hence the Huey nickname , which has remained in common use , despite the official redesignation to UH - 1 in 1962 . The UH - 1 first saw service in combat operations during the Vietnam War , with around 7,000 helicopters deployed . The Bell 204 and 205 are Iroquois versions developed for the civil market . | Question: where did the name huey helicopter come from ?', ['originally designated HU - 1'], 'mrqa_naturalquestions-validation-5144'), tensor(-0.0604)), (('Context: In the early years the College trained many Puritan ministers.[citation needed] (A 1643 publication said the school\'s purpose was "to advance learning and perpetuate it to posterity, dreading to leave an illiterate ministry to the churches when our present ministers shall lie in the dust".) It offered a classic curriculum on the English university model—\u200b\u200bmany leaders in the colony had attended the University of Cambridge—\u200b\u200bbut conformed Puritanism. It was never affiliated with any particular denomination, but many of its earliest graduates went on to become clergymen in Congregational and Unitarian churches. | Question: Was the school officially associated with any denomination?', ['It was never affiliated with any particular denomination', 'never'], 'mrqa_squad-validation-7149'), tensor(-0.0630)), (('Context: The Roosevelt Corollary was an addition to the Monroe Doctrine articulated by President Theodore Roosevelt in his State of the Union address in 1904 after the Venezuela Crisis of 1902 -- 03 . The corollary states that the United States will intervene in conflicts between European countries and Latin American countries to enforce legitimate claims of the European powers , rather than having the Europeans press their claims directly . | Question: who warned europe to stay out of the americas ?', ['Roosevelt Corollary', 'Monroe Doctrine'], 'mrqa_naturalquestions-validation-2501'), tensor(-0.0666)), (('Context: Death of a Soldier  Death of a Soldier is a 1986 Australian film based on the life of American serial killer Eddie Leonski.  The film was shot using locations around Melbourne, Victoria.   Eddie Leonski  Edward Joseph Leonski (December 12, 1917 – November 9, 1942) was an American soldier and serial killer responsible for the strangling murders of three women in Melbourne, Australia.  Leonski was known as the "Brownout Strangler", given Melbourne\'s wartime status of keeping low lighting (not as stringent as a wartime blackout).  His self-confessed motive for the killings was a twisted fascination with female voices, especially when they were singing, and his claim that he killed the women to "get at their voices." | Question: which American soldier and serial killer was Death of a Soldier based on ?', ['Edward Joseph Leonski'], 'mrqa_hotpotqa-validation-1289'), tensor(-0.0713)), (('Context: Mary O\'Connell  Mary O\'Connell (better known as Sister Anthony, S.C.) (1814 – December 8, 1897) was an Irish immigrant to the United States, who became a Roman Catholic Religious Sister.  A Sister of Charity of Cincinnati, she served with distinction as a nurse on the front lines of the American Civil War.  Her work with the wounded and in health care in general caused her to be known as "the angel of the battlefield" and "the Florence Nightingale of America."  Her portrait hangs in the Smithsonian Institution in Washington, DC.   Mary O\'Connell  Mary O\'Connell (better known as Sister Anthony, S.C.) (1814 – December 8, 1897) was an Irish immigrant to the United States, who became a Roman Catholic Religious Sister.  A Sister of Charity of Cincinnati, she served with distinction as a nurse on the front lines of the American Civil War.  Her work with the wounded and in health care in general caused her to be known as "the angel of the battlefield" and "the Florence Nightingale of America."  Her portrait hangs in the Smithsonian Institution in Washington, DC.   Florence Nightingale  Florence Nightingale, ( ; 12 May 1820 – 13 August 1910) was an English social reformer and statistician, and the founder of modern nursing. | Question: What portrait hangs in the Smithsonian Institute along with what is known as the founder of nursing?', ['Sister Anthony, S.C.'], 'mrqa_hotpotqa-validation-1626'), tensor(-0.0714)), (('Context: Catherine Zeta Jones in "The Darling Buds of May" 1991 ...Catherine Zeta Jones in "The Darling Buds of May" 1991 - YouTube  Catherine Zeta Jones in "The Darling Buds of May" 1991  Want to watch this again later?  Sign in to add this video to a playlist.  Need to report the video?  Sign in to report inappropriate content.  The interactive transcript could not be loaded.  Loading...  Rating is available when the video has been rented.  This feature is not available right now. Please try again later.  Uploaded on Mar 31, 2007  Some clips of a young Catherine Zeta Jones as Mariette, in the tv series, "The Darling Buds of May".  "The Darling Buds of May" was a British television series first broadcast in 1991. It is set in an idyllic rural 1950s Kent, among a large, boisterous family. The show was very popular, and launched the acting career of Catherine Zeta Jones.  Category  Mariette - The Darling Buds of May (UK) Characters - ShareTVMariette - The Darling Buds of May (UK) Characters - ShareTV  Catherine Zeta-Jones (born September 25, 1969, in Swansea, West Glamorgan, Wales, UK, the daughter of ...  Character Bio  Mariette Larkin, the eldest Larkin child. Her name was created by  combining \'Marie\' and \'Antoinette\'. In the first episode, she is shown  to be the family beauty and slightly wild. However, she quickly marries  Charley and settles down with him for the rest of the series. She  appears to have inherited Pop\'s business sense, and at the end of the  series she and Charley buy and manage a local brewery. Her wedding  ceremony reveals her middle name as Jane.  Episode Screenshots  Profile: Catherine Zeta Jones - BBC NewsProfile: Catherine Zeta Jones - BBC News  BBC News  Close share panel  Image caption Catherine Zeta Jones became a household name after appearing in The Darling Buds of May  The showbusiness journey of Catherine Zeta Jones, a permanent fixture on the A-list for decades, has taken her from her home in Wales to the Hollywood hills.  Along the way, the 41-year-old has picked up an Oscar, a Tony, a CBE, countless plaudits and a smattering of bad reviews.  She married Hollywood actor Michael Douglas in a lavish New York ceremony in 2000, however the couple have now decided to take some time apart to "evaluate and work on their marriage".  Few could have predicted that a girl born to a sweet factory owner and a seamstress would go on to become one of the most famous actresses in the world.  At the age of 10, the Welsh star won a national talent contest singing a Shirley Bassey song before landing roles in various West End productions.  Image caption Zeta Jones started acting at a very young age  She got her big break at the age of 17, following a promotion from second understudy to the lead role in the hit musical 42nd Street.  But it was Zeta Jones\'s first major TV role, in the 1991 comedy drama The Darling Buds of May, that made her a household name.  Set in rural Kent in the 1950s, viewers fell in love with the Larkins, played by Sir David Jason and Pam Ferris.  Zeta Jones played their fresh-faced sweetheart daughter Mariette.  Squeaky-clean perception  Off camera, her personal life began to hit the headlines more than her acting ability.  Her relationship with Blue Peter presenter John Leslie was well documented. She also became involved with Simply Red star Mick Hucknall and Soldier Soldier actor Angus MacFadyen.  After leaving The Darling Buds of May, Zeta Jones turned her attention to film, but admitted finding it hard to shake off the public\'s squeaky-clean perception of her.  The thing that really upset me was the idea that I was this gold-digger  Catherine Zeta Jones answers the critics about her marriage to Michael Douglas  "I am afraid I have this image which is far | Question: What was the name of Catherine Zeta Jones character in The Darling Buds of May ?', ['mariette'], 'mrqa_triviaqa-validation-7018'), tensor(-0.0769)), (('Context: Cor, Blimey!  Cor, Blimey!  is a 2000 TV film that follows the relationship between "Carry On" film actors Sid James (played by Geoffrey Hutchings) and Barbara Windsor (played by Samantha Spiro).   Samantha Spiro  Samantha Spiro (born 20 June 1968) is a double Olivier Award-winning English actress.  She is best known for portraying Barbara Windsor in the stage play "Cleo, Camping, Emmanuelle and Dick" and the television film "Cor, Blimey! ", DI Vivien Friend in "", and Melessa Tarly in the HBO series "Game of Thrones". | Question: Who plays opposite the double Olivier Award-winning English actress in the 2000 TV film following the relationship between "Carry On" film actors Sid James and Barbara Windsor?', ['Geoffrey Hutchings'], 'mrqa_hotpotqa-validation-2970'), tensor(-0.0821)), (("Context: New York StadiumThe New York Stadium, known as the AESSEAL New York Stadium for sponsorship purposes, is a football stadium in Rotherham, South Yorkshire, England. Opened in July 2012, it is the home ground of Rotherham United.  History  Rotherham United announced their intention to construct a new community stadium when they moved away from Millmoor to the Don Valley Stadium in May 2008 after a dispute with the ground owner Ken Booth.  In January 2010 the club purchased the former site of the Guest and Chrimes Foundry to be used for the new stadium.  Outline planning permission for the stadium was granted in November 2010, and the first images were sketched shortly after.   The name of the stadium was announced as the 'New York Stadium' on 19 December 2011, chosen ahead of 'The Foundry' and 'The Waterfront Stadium'. The reason for the name is that the area of land that the stadium lies upon is called New York, and it was thought that it would be better to name the stadium after history and/or where the stadium is situated, like nearby stadiums Bramall Lane and Hillsborough. Chairman Tony Stewart also hopes that the name could bring investment from New York City or further afield, as the New York Yankees chairman had recently said that he wanted to invest in an English football team.   Construction started in June 2011 and the stadium was officially opened by Prince Edward, Duke of Kent on 12 March 2012.  The first game played at the stadium was a pre-season match between Rotherham and Barnsley, held on 21 July 2012.  The Millers won 2–1; the first goal in the stadium was scored by Jacob Mellis of Barnsley, and David Noble scored Rotherham's first goal in their new home. The New York Stadium made its league debut on 18 August 2012, in which Rotherham beat Burton Albion 3–0,  Daniel Nardiello scoring the first competitive goal in the ground.  On 16 April 2014, the stadium held an England under-18s game for the first time. The resulting match finished with England beating Germany 2–1. Over 9,000 fans attended the game.   The naming rights to the stadium were announced as having been bought by local company AESSEAL, in a press conference on 21 November 2014. Club chairman Tony Stewart said the deal was worth six figures annually, as a result of the deal. It was also suggested as being the biggest sponsorship deal of the club's history.   Design  The stadium has a 12,000 all-seated capacity, with the option to be able to increase the stadium's capacity if needed.  It cost approximately £20 million to construct.  The stadium includes The 1925 Club, a corporate hospitality suite.  Local businesses such as Norton Finance  and Premier Hytemp  were some of the first members.  At the beginning  of the 2014–15 season, a large video screen was installed in the north west corner of the stadium.  Stands  ;North Stand  The North Stand, known as the KCM Recycling Stand for sponsorship reasons, and often referred to as the New Tivoli, is the kop stand of the stadium. The KCM Recycling Stand holds 2000 home fans, and has the lettering of the club's initials—RUFC—in white across it. The stand sits behind one of the goals, opposite the away end.  ;West Stand  The West Stand, known as the Eric Twigg Pukka Pies Stand for sponsorship reasons, is the main stand of the stadium. It features the executive 1925 Lounge, and also the stand where the players walk under when entering the field of play. It holds 4000 home fans.  ;East Stand  The East Stand, known as the Ben Bennett Stand, is the family stand of the stadium. It holds 4000 home fans, as well as two built-in balcony-type structures for disabled people.  ;South Stand  The South Stand, known as the Morrison Stand, is a 2000 seated away stand. It sits behind a goal, with the family stand to the right, main stand to the left, and the kop directly opposite.  Records  *Record | Question: Which League 2 football team play home games at the New York Stadium?", ['rotherham united'], 'mrqa_triviaqa-validation-4856'), tensor(-0.0852)), (('Context: Sweeney Todd - The Demon Barber of Fleet Street.Sweeney Todd - The Demon Barber of Fleet Street.  Home \xa0\xa0\xa0 Ghost Stories \xa0\xa0\xa0 Haunted Pubs \xa0\xa0\xa0 Haunted Houses \xa0\xa0\xa0 Haunted  Graveyards \xa0\xa0\xa0 Historic  Buildings \xa0\xa0\xa0 Jack the  Ripper \xa0\xa0\xa0  THE REAL SWEENEY TODD  Sweeney Todd, "The Demon Barber of Fleet Street" first came to prominence in  the mid 19th century since when he has appeared in books, plays, a musical and  in films. His gruesome story caught the public imagination yet again in 2008  when Johnny Depp played him in Tim Burton\'s film Sweeney Todd. But who was this murderous barber, and when, if ever, did he live?  Before looking at the history of the story of Sweeney Todd one thing has to be made absolutely and unequivocally clear - there never was a barber on Fleet Street by the name of Sweeney Todd.  Yet his story has thrilled, chilled and turned the stomachs of many generations of horror seekers ever since he slashed his way into the public consciousness by way of the Victorian Penny Dreadful periodicals in the middle decade of the 19th century.  In essence his story is a simple one. He belongs to  a bygone age when men\'s home grooming was little more than primitive. Electric and safety razors were luxuries of the future and so any gentleman that required a close, clean shave was forced to entrust himself to a local barber.  Sitting in the barber\'s chair, his head tilted back, his throat exposed as the sharp blade of the barber\'s razor glided back and forth across his skin, a man would be, and indeed might well feel, decidedly exposed  and vulnerable!  Those behind the Sweeney Todd story, like many horror writers and film makers since, used the vulnerability of a familiar, everyday situation and turned it into something that would be guaranteed to illicit gasps of terror from their readers and, no doubt, instil feelings of trepidation in  to generations of men! In short  they created fear out of the familiar!  The Sweeney Todd stories have at their root a simple, blood-drenched scenario. Todd is a successful and prosperous Barber with premises at 185 Fleet Street.  Sitting his unsuspecting clients into his specially constructed barber\'s chair he lathers up their faces and suddenly tips the chair back, pitching his unfortunate customers heel over head through a trap door into the cellar below. If the fall hasn\'t killed them, Todd is compelled to \'polish them off\' with his razor. Having robbed them, he drags their bodies through an underground tunnel to the premises of his lover Mrs. Margery Lovett in nearby Bell Yard.  Here the story takes another twist creating fear from the familiar in an age when people were far more dependent on outside caterers than we are today by the stomach churning device of having Todd\'s victims turned into succulent meat pies for Mrs Lovett\'s much vaunted Meat Pie Shop.  His victim\'s worldly possessions are hidden away in Sweeny Todd\'s shop, whilst any remains that haven\'t gone into a batch of meat pies are secreted in the dank, disused vaults beneath St Dunstan\'s church on Fleet Street.  As time progresses Sweeney Todd grows ever more confident and audacious, but in so doing his insatiable lust for  blood proves his undoing. Thanks to the efforts of a determined magistrate, a group of Bow Street Runners and a pair of lovers, Todd and Lovett are brought to justice and put on trial at the Central Criminal Courts or Old Bailey.  Despite the fact that several books and articles have confidently assured their readers  that Sweeney Todd did exist, there is absolutely no historical figure by that  name and indeed no barber by the name of Sweeney Todd ever found himself on  trial at the Old Bailey charged with murdering his clients and, with the aid of  his mistress and accomplice, using them to create killer recipes.  Indeed, the proceedings of the Old Bailey from 1674 to 1913 are now available to peruse online via a fully searchable database. There is no transcript for a trial of a Sweeney Todd, nor for that matter any similar crimes. A case | Question: The thoroughfare where fictional Sweeny Todd had his barber shop?', ['fleet street'], 'mrqa_triviaqa-validation-1575'), tensor(-0.0913)), (('Context: Its hardware contains similarities to the LG G2 ; it is powered by a 2.26 GHz quad - core Snapdragon 800 processor with 2 GB of RAM , either 16 or 32 GB of internal storage , and a 2300 mAh battery . The Nexus 5 uses a 4.95 - inch ( marketed as 5 - inch ) 445 PPI 1080p IPS display , and includes an 8 - megapixel rear - facing camera with optical image stabilization ( OIS ) , and a 1.3 - megapixel front - facing camera . The Nexus 5 supports LTE networks where available , unlike the Nexus 4 which unofficially supported LTE on AWS Band 4 only with a hidden software option , but was not formally approved or marketed for any LTE use . There are two variants of the Nexus 5 , with varying support for cellular frequency bands ; one is specific to North America ( LG - D820 ) , and the other is designed for the rest of the world ( LG - D821 ) . | Question: what is the processor for google nexus 5 ?', ['a 2.26 GHz quad - core Snapdragon 800 processor'], 'mrqa_naturalquestions-validation-6341'), tensor(-0.0922)), (('Context: Hey Ya!  "Hey Ya!"  is a song written and produced by André 3000 for his 2003 album "The Love Below", part of the hip hop duo OutKast\'s double album "Speakerboxxx/The Love Below".  "Hey Ya!"  takes influence from funk, rap and rock music.  Its music video features a live performance by a band, all eight of whose members are played by André 3000, that mimics the Beatles\' 1964 performance on "The Ed Sullivan Show".  The song received praise from contemporary music critics, and won the award for Best Urban/Alternative Performance at the 46th Grammy Awards.  His version of the song has also appeared on the soundtrack of   Hey Ya!  "Hey Ya!"  is a song written and produced by André 3000 for his 2003 album "The Love Below", part of the hip hop duo OutKast\'s double album "Speakerboxxx/The Love Below".  "Hey Ya!"  takes influence from funk, rap and rock music.  Its music video features a live performance by a band, all eight of whose members are played by André 3000, that mimics the Beatles\' 1964 performance on "The Ed Sullivan Show".  The song received praise from contemporary music critics, and won the award for Best Urban/Alternative Performance at the 46th Grammy Awards.  His version of the song has also appeared on the soundtrack of   The Ed Sullivan Show  The Ed Sullivan Show is an American TV variety show that ran on CBS from Sunday June 20, 1948, to Sunday June 6, 1971, and was hosted by New York entertainment columnist Ed Sullivan.  It was replaced in September 1971 by the "CBS Sunday Night Movie". | Question: Hey Ya is a song performed on The Ed Sullivan Show on CBS by which hip hop artist in 2003?', ['is a song written and produced by André 3000'], 'mrqa_hotpotqa-validation-2679'), tensor(-0.0932)), (('Context: Pi Day is an annual celebration of the mathematical constant π ( pi ) . Pi Day is observed on March 14 ( 3 / 14 in the month / day format ) since 3 , 1 , and 4 are the first three significant digits of π . In 2009 , the United States House of Representatives supported the designation of Pi Day . | Question: how long have we been celebrating pi day ?', ['2009'], 'mrqa_naturalquestions-validation-3028'), tensor(-0.1034)), (("Context: Symphony Splash  Victoria Symphony Splash is an annual event held in Victoria, British Columbia on the Sunday before BC Day.  The event is produced by the Victoria Symphony and consists of the Victoria Symphony playing, live on a barge, in the middle of Victoria's Inner Harbour.  Also included in the event is a very large fireworks display, as well as live cannon fire, during the 1812 Overture.  The concert is led by Victoria Symphony Music Director Tania Miller.  The 2016 event on July 31 will be the 27th Victoria Symphony Splash.   1812 Overture  The Year 1812, festival overture in E♭ major, Op. 49, popularly known as the 1812 Overture, is an overture written in 1880 by Russian composer Pyotr Ilyich Tchaikovsky to commemorate Russia's defence of its motherland against Napoleon's invading Grande Armée in 1812. | Question: Who will conduct the firing cannons in Victoria as part of a musical piece written to celebrate the defeat of Napoleon in Russia?", ['Tania Miller'], 'mrqa_hotpotqa-validation-4734'), tensor(-0.1117)), (('Context: Kim Dong-wook  Kim Dong-wook (born July 29, 1983) is a South Korean actor.  After appearing in student short films and several minor parts, Kim became a star through his supporting role in the popular TV series "Coffee Prince" (2007), followed by box office hit "Take Off" (2009).  He then starred in "Happy Killers" (2010) and "Romantic Heaven" (2011), but it was his acclaimed performance as an obsessed and tormented king in 2012 period drama "The Concubine" that brought Kim the best reviews of his career yet.   The Concubine (film)  The Concubine (; lit.  "Royal Concubine: Concubine to the King") is a 2012 South Korean historical film directed by Kim Dae-seung.  Set in the Joseon Dynasty, it centers around Hwa-yeon (Jo Yeo-jeong), who becomes a royal concubine against her will, Kwon-yoo (Kim Min-joon), a man torn between love and revenge, and Prince Sung-won (Kim Dong-wook), who has his heart set on Hwa-yeon despite the countless women available to him.  These three characters form a love triangle which is ruled by dangerous passion.  The struggle to survive within the tight-spaced boundaries of the palace is intense, and only those who are strong enough to overcome the hell-like milieu can survive. | Question: What role did Kim Dong-wook play in the 2012 South Korean historical film directed by Kim Dae-seung?', ['an obsessed and tormented king'], 'mrqa_hotpotqa-validation-3971'), tensor(-0.1165)), (("Context: It is an allusion to Samuel Taylor Coleridge 's poem The Rime of the Ancient Mariner ( 1798 ) . In the poem , an albatross starts to follow a ship -- being followed by an albatross was generally considered a sign of good luck . However , the titular mariner shoots the albatross with a crossbow , which is regarded as an act that will curse the ship ( which indeed suffers terrible mishaps ) . Even when they are too thirsty to speak , the ship 's crew let the mariner know through their glances that they blame his action for the curse . The albatross is then literally hung around the mariner 's neck by the crew to symbolize his guilt in killing the bird . Thus , the albatross can be both an omen of good or bad luck , as well as a metaphor for a burden to be carried as penance . | Question: the rime of the ancient mariner albatross symbolism ?", ['a metaphor for a burden to be carried as penance', 'omen of good or bad luck'], 'mrqa_naturalquestions-validation-7017'), tensor(-0.1598)), (('Context: Br\'er RabbitBr\'er Rabbit, also spelled Bre\'r Rabbit or Brer Rabbit or Bruh Rabbit, is a central figure as Uncle Remus tells stories of the Southern United States. Br\'er Rabbit is a trickster who succeeds by his wits rather than by brawn, provoking authority figures and bending social mores as he sees fit. The Walt Disney Company later adapted this character for its 1946 animated motion picture Song of the South.  Tar-Baby story   In one tale, Br\'er Fox constructs a doll out of a lump of tar and dresses it with some clothes. When Br\'er Rabbit comes along he addresses the Tar-Baby amiably, but receives no response. Br\'er Rabbit becomes offended by what he perceives as the Tar-Baby\'s lack of manners, punches it, and in doing so becomes stuck. The more Br\'er Rabbit punches and kicks the tar "baby" out of rage, the more he gets stuck. When Br\'er Fox reveals himself, the helpless but cunning Br\'er Rabbit pleads, "please, Br\'er Fox, don\'t fling me in dat brier-patch," prompting Fox to do exactly that. As rabbits are at home in thickets, the resourceful Br\'er Rabbit uses the thorns and briers to escape. The story was originally published in Harper\'s Weekly by Robert Roosevelt; years later Joel Chandler Harris included his version of the tale in his Uncle Remus stories.  African origins   The Br\'er Rabbit stories can be traced back to trickster figures in Africa, particularly the hare that figures prominently in the storytelling traditions in West, Central, and Southern Africa. These tales continue to be part of the traditional folklore of numerous peoples throughout those regions. In the Akan traditions of West Africa, the trickster is usually the spider Anansi, though the plots in his tales are often identical with those of stories of Br\'er Rabbit.  However, Anansi does encounter a tricky rabbit called "Adanko" (Asante-Twi to mean "Hare") in some stories. The Jamaican character with the same name "Brer Rabbit", is an adaptation of the Ananse stories of the Akan people.   Some scholars have suggested that in his American incarnation, Br\'er Rabbit represented the enslaved Africans who used their wits to overcome adversity and to exact revenge on their adversaries, the White slave-owners.  Though not always successful, the efforts of Br\'er Rabbit made him a folk hero. However, the trickster is a multi-dimensional character. While he can be a hero, his amoral nature and his lack of any positive restraint can make him into a villain as well.   For both Africans and African Americans, the animal trickster represents an extreme form of behavior that people may be forced to adopt in extreme circumstances in order to survive. The trickster is not to be admired in every situation. He is an example of what to do, but also an example of what not to do. The trickster\'s behavior can be summed up in the common African proverb: "It\'s trouble that makes the monkey chew on hot peppers." In other words, sometimes people must use extreme measures in extreme circumstances.  Several elements in the Brer Rabbit Tar Baby story (e.g., rabbit needing to be taught a lesson, punches and head-butting the rabbit does, the stuck rabbit being swung around and around) are reminiscent of those found in a Zimbabwe-Botswana folktale.   Folklorists in the late 19th century first documented evidence that the American versions of the stories originated among enslaved West Africans based on connections between Br\'er Rabbit and Leuk, a rabbit trickster in Senegalese folklore.  The stories of Br\'er Rabbit were written down by Robert Roosevelt, an uncle of US President Theodore Roosevelt. Theodore Roosevelt wrote in his autobiography about his aunt from the State of Georgia, that "She knew all the \'Br\'er Rabbit\' stories, and I was brought up on them. One of my uncles, Robert Roosevelt, was much struck with them, and took them down from her dictation, publishing them in Harper\'s, where they fell flat. This was a good many years before a genius arose who, in \'Uncle Remus\', made the stories | Question: Who gets Brer Rabbit in a terrible tangle in the stories by Joel Chandler Harris?', ['Tar Baby', 'Tar-Baby', 'tar baby', 'tar "baby'], 'mrqa_triviaqa-validation-1616'), tensor(-0.1652)), (("Context: Steve Carell as Felonious Gru , the former villain turned Anti-Villain League agent , Margo , Edith , and Agnes ' adoptive father , and Lucy 's husband .   Carell also voices Dru , Gru 's long - lost twin brother and the girls ' adoptive uncle .     Kristen Wiig as Lucy Wilde , an Anti-Villain League agent , Gru 's wife and the girls ' adoptive mother .   Trey Parker as Balthazar Bratt , a supervillain and former child star who grows up to become obsessed with the character he played in the ' 80s and is bent on world domination .   Miranda Cosgrove as Margo , Gru and Lucy 's oldest adoptive daughter .   Dana Gaier as Edith , Gru and Lucy 's middle adoptive daughter .   Nev Scharrel as Agnes , Gru and Lucy 's youngest adoptive daughter . She was previously voiced by Elsie Fisher in the first two films .   Pierre Coffin as Mel and the Minions .   Coffin also voices a Museum Director .     Steve Coogan as Silas Ramsbottom , the director of the Anti-Villain League .   Coogan also voices Fritz , the butler of Dru .     Julie Andrews as Marlena Gru , Gru and Dru 's mother .   Jenny Slate as Valerie Da Vinci , a member of the Anti-Villain League who becomes the brand new AVL director .   Andy Nyman as Clive , a robot who is Bratt 's sidekick .   Adrian Ciscato as Niko , a boy from Freedonia who falls in love with Margo .   John Cygan ( in his final film role ) as an Additional Voice . | Question: who is the cast of despicable me 3 ?", ['Nev Scharrel', 'Andy Nyman', 'Adrian Ciscato', 'Pierre Coffin', 'Steve Coogan', 'Trey Parker', 'Julie Andrews', 'Kristen Wiig', 'John Cygan', 'Jenny Slate', 'Dana Gaier', 'Steve Carell', 'Miranda Cosgrove'], 'mrqa_naturalquestions-validation-3490'), tensor(-0.1921)), (('Context: For the third straight season, the number one seeds from both conferences met in the Super Bowl. The Carolina Panthers became one of only ten teams to have completed a regular season with only one loss, and one of only six teams to have acquired a 15–1 record, while the Denver Broncos became one of four teams to have made eight appearances in the Super Bowl. The Broncos made their second Super Bowl appearance in three years, having reached Super Bowl XLVIII, while the Panthers made their second Super Bowl appearance in franchise history, their other appearance being Super Bowl XXXVIII. Coincidentally, both teams were coached by John Fox in their last Super Bowl appearance prior to Super Bowl 50. | Question: How many teams up to Super Bowl 50 have been to the championship game eight times?', ['four'], 'mrqa_squad-validation-194'), tensor(-0.2802)), (('Context: Welcome | Kansas City RoyalsWelcome | Kansas City Royals  Clubhouse Lobby  Retired Numbers: Three Royals have been honored with retired numbers: 10-Dick Howser (1987), 5-George Brett (1994) and 20-Frank White (1995). Their jerseys and stories greet each visitor in the Royals Hall of Fame Clubhouse Lobby.  Ash Wood Wall: Trace the history of professional baseball in Kansas City from its very beginning in 1884 on the 17 foot tall and 67 foot wide Ash Wood Wall. Images and headlines tell the story of the eight teams to call Kansas City home.  Predecessor Teams Panels: What team was Kansas City\'s first?  What team name sprang from their unique uniform?  Which club had a mule for a mascot and a rabbit as a ball boy?  Who did Hall of Famers Mickey Mantle, Satchel Paige, and Billy Hamilton play for? All those answers and more. More »  Royals Dugout Theatre  Move from the clubhouse, down the tunnel and into the Royals Dugout for a trip through Kansas City baseball history. The legendary Buck O\'Neil guides you through a 15-minute award-winning film, putting you right on the field with the sight and sounds of the game. More »  A New Franchise  Kansas City has had a hometown baseball team every year since 1884 -- except for 1968. Find out why. When Charlie Finley moved the A\'s west to Oakland, Kansas City leaders had to put together a plan to bring baseball back. Find out how they did it. More »  Ewing Kauffman  Ewing Kauffman\'s life was an all-American tale and his success was a great Kansas City story. The Royals founder brought baseball back to his hometown, but that is just one part of his legacy. Learn more about his life and philosophy in Mr. K\'s own words. More »  Franchise Vision  Founders Ewing and Muriel Kauffman believed the franchise belonged not to them but to the community. That spirit inspired the selection of the team\'s name, logo and more. So where did the name Royals come from? The answer can be found right here. More »  The Royals Way  Here you can get a full perspective of the importance placed on player acquisition and development both at the Royals founding and through to the current day. Follow the path of a Royals prospect from the rookie leagues all the way to Kansas City. More »  The Royals Baseball Academy  The Royals Baseball Academy is one of the least known but influential baseball stories of the late 20th century. New ways of teaching and understanding the game succeeded in developing talent. Find out who it graduated and the lasting mark it made on the game. More »  The Royals have called two ballparks home -- both with an interesting place in Kansas City history. Municipal Stadium\'s half-century included great players, many teams and unique events. Royals Stadium set a new standard which Kauffman Stadium continues.  \'Design Your Own Ballpark\' presented by Populous  Here\'s your chance to be a ballpark architect. This one-of-a-kind interactive lets you make stadium and field design decisions. Move the fences in or out, add another scoreboard and more -- you decide. Plus email a picture of your park anywhere. More »  Royals Timeline  It always begins in 1969 and leads to today. The featured events may change but you can learn many of the greatest and most interesting details. What rookie led the original Royals? no-hitters? Cy Young winners? Through artifacts and video you\'re in the know. More »  Ballpark Fun  "Nothing beats fun at the old ballpark." That\'s been true here in Kansas City for a long time, from unique promotions and special events to the creative passion of fans themselves. Plus a full roster of Royals bobbleheads -- how many can you name? More »  Cooperstown Corner  See how many members of the National Baseball Hall of Fame have a tie to Kansas City (hint it\'s a lot). Plus discover the interesting evolution of baseball\'s basic equipment: the bat, the glove, and the baseball. These touch and feel | Question: In baseball, where do the Royals come from?', ['kansas city'], 'mrqa_triviaqa-validation-93'), tensor(-0.2888)), (("Context: The Cold War was a state of geopolitical tension after World War II between powers in the Eastern Bloc ( the Soviet Union and its satellite states ) and powers in the Western Bloc ( the United States , its NATO allies and others ) . Historians do not fully agree on the dates , but a common timeframe is the period between 1947 , the year the Truman Doctrine , a U.S. foreign policy pledging to aid nations threatened by Soviet expansionism , was announced , and either 1989 , when communism fell in Eastern Europe , or 1991 , when the Soviet Union collapsed . The term `` cold '' is used because there was no large - scale fighting directly between the two sides , but they each supported major regional wars known as proxy wars . | Question: who were the major countries involved in the cold war ?", ['the Soviet Union', 'the United States'], 'mrqa_naturalquestions-validation-5180'), tensor(-0.3415)), (('Context: The Super Bowl 50 Host Committee has vowed to be "the most giving Super Bowl ever", and will dedicate 25 percent of all money it raises for philanthropic causes in the Bay Area. The committee created the 50 fund as its philanthropic initiative and focuses on providing grants to aid with youth development, community investment and sustainable environments. | Question: What was the name of the fund setup to help with investing in the community?', ['50 fund', 'the 50 fund'], 'mrqa_squad-validation-392'), tensor(-0.4257)), (('Context: Moonlight (TV series)  Moonlight is an American paranormal romance television drama created by Ron Koslow and Trevor Munson, who was also executive producer for all episodes with Joel Silver, Gerard Bocaccio, Gabrielle Stanton and Harry Werksman.  The series follows private investigator Mick St. John (Alex O\'Loughlin), who was turned into a vampire by his bride Coraline (Shannyn Sossamon) on the couple\'s wedding night fifty-five years earlier.  In the present day, he struggles with his attraction to a mortal woman, Beth Turner (Sophia Myles), his friendship with Josef Kostan (Jason Dohring), and his dealings with other vampires in Los Angeles.   Alex O\'Loughlin  Alex O\'Loughlin (born 24 August 1976) is an Australian actor, who plays Lieutenant Commander Steve McGarrett on CBS\' remake of the TV series "Hawaii Five-0".  He had starring roles in the films "Oyster Farmer" (2004) and "The Back-up Plan" (2010), as well as on such television series as "Moonlight" (2008) and "Three Rivers" (2009). | Question: Moonlight starred the actor who played whom on the remake of Hawaii Five-0?', ['Steve McGarrett'], 'mrqa_hotpotqa-validation-1168'), tensor(-0.4747)), (("Context: May 14 : The Corps of Discovery departs from Camp Dubois at 4 p.m. , marking the beginning of the voyage to the Pacific coast .   May 16 : The Corps of Discovery arrives at St. Charles , Missouri .   May 21 : Departure from St. Charles at 3 : 30 p.m.   May 24 : Pass Boones Settlement . Home of famous woodsman L. Willenborg .   May 25 : The expedition passes the small village of La Charrette on the Missouri River . Charles Floyd writes in his journal that this is `` the last settlement of whites on this river '' .   June 1 : The expedition reaches the Osage River .   June 12 : Lewis and Clark meet three trappers in two pirogues . One of the men was Pierre Dorion , Jr. -- who knew George Rogers Clark . Lewis and Clark persuade Dorion to return to Sioux camp to act as interpreter .   June 26 : The expedition arrives at Kaw Point where the Kansas River drains into the Missouri River basin .   June 28 -- 29 : First trial in new territory . Pvt . John Collins is on guard duty and breaks into the supplies and gets drunk . Collins invites Pvt . Hugh Hall to drink also . Collins receives 100 lashes , Hall receives 50 lashes .   July 4 : Marking Independence Day , the expedition names Independence Creek located near Atchison , Kansas .   July 11 -- 12 : Second trial in new territory . Pvt . Alexander Hamilton Willard is on guard duty . Is charged with lying down and sleeping at his post whilst a sentinel . Punishable by death . He receives 100 lashes for four straight days .   July 21 : Reaches the Platte River , 640 miles from St. Louis . Entering Sioux Territory .   August 1 : Captain William Clark 's 34th birthday .   August 3 : The Corps of Discovery holds the first official council between representatives of the United States and the Oto and Missouri tribes at Council Bluffs , Iowa . They hand out peace medals , 15 - star flags and other gifts , parade men and show off technology .   August 4 : Moses Reed said he was returning to a previous camp to retrieve a knife but deserted to St. Louis .   August 18 : George Drouillard returns to camp with Reed and Otos ' Chief Little Thief . Reed is sentenced to run the gauntlet ( 500 lashes ) and is discharged from the permanent party .   August 18 : Captain Meriwether Lewis 's 30th birthday .   August 20 : Sergeant Charles Floyd dies . He dies from bilious chorlick ( ruptured appendix ) . He is the only member lost during the expedition .   August 23 : Pvt . Joseph Field kills first bison .   August 26 : Pvt . Patrick Gass is elected to sergeant . First election in new territory west of Mississippi River . George Shannon is selected to get the horses back from native Americans .   August 30 : A friendly council with the Yankton Sioux held . According to a legend , Lewis wraps a newborn baby in a United States flag and declares him `` an American '' .   September 4 : Reach the mouth of the Niobrara River .   September 7 : The expedition drives a prairie dog out of its den ( by pouring water into it ) to send back to Jefferson .   September 14 : Hunters kill and describe prairie goat ( antelope ) .   September 25 -- 29 : A band of Lakota Sioux demand one of the boats as a toll for moving further upriver . Meet with Teton Sioux . Close order drill , air gun demo , gifts of medals , military coat , hats , tobacco . Hard to communicate language problems . Invite chiefs on board keelboat , give each \u200b ⁄ glass whiskey , acted drunk wanted more . Two armed confrontations with Sioux . Some of the chiefs sleep on boat , move up river to another village , meet in lodge , hold scalp dance .   October 8 -- 11 : Pass Grand River home of the Arikara people , 2,000 + . Joseph Gravelins trader , lived with Arikara for 13 yrs . Pierre Antoine Tabeau lived in another village was from Quebec .   October 13 : Pvt . John Newman tried for | Question: on which river did the exploration of the louisiana purchase begin ?", ['the Missouri River'], 'mrqa_naturalquestions-validation-8948'), tensor(-0.4899)), (('Context: Important production centres today are the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ) , the Yakima ( Washington ) and Willamette ( Oregon ) valleys , and western Canyon County , Idaho ( including the communities of Parma , Wilder , Greenleaf , and Notus ) . The principal production centres in the UK are in Kent ( which produces Kent Goldings hops ) , Herefordshire , and Worcestershire . Essentially all of the harvested hops are used in beer making . | Question: where do they grow hops in the us ?', ['Washington', 'Oregon', 'Idaho'], 'mrqa_naturalquestions-validation-2248'), tensor(-0.5106)), (("Context: The phrase has also been used as slang for certain drugs . An Oxford Reference page defined `` Beam me up , Scotty '' as `` a mixture of phencyclidine and cocaine '' and to `` talk to Scotty , high off Scotty , see Scotty ... etc . '' . | Question: what 's in a beam me up scotty ?", ['phencyclidine', 'cocaine'], 'mrqa_naturalquestions-validation-5437'), tensor(-0.5280)), (('Context: In Hinduism the spiritual teacher is known as a guru, and, in many traditions of Hinduism - especially those common in the West - the emphasis on spiritual mentorship is extremely high, with gurus often exercising a great deal of control over the lives of their disciples. | Question: In what area is it common for spiritual mentorship to be extremely high?', ['the West'], 'mrqa_squad-validation-2069'), tensor(-0.6075)), (('Context: \'European Olympics\' to be held in 2015 - CNN.com\'European Olympics\' to be held in 2015 - CNN.com  \'European Olympics\' to be held in 2015  By  Updated 11:33 AM ET, Sat December 8, 2012  Chat with us in Facebook Messenger. Find out what\'s happening in the world as it unfolds.  European Olympic Committees president Patrick Hickey led  the presentation at Saturday\'s general assembly in Rome.  Story highlights  Azerbaijan\'s capital Baku to host the first European Games in 2015  European Olympic Committees\' 49 members vote to launch the event  The EOC says it has "plenty of technical details to decide"  The Games will be in same year as athletics and swimming worlds  Europe will stage its own "Olympics" in 2015, the region\'s 49 member committees voted on Saturday.  The Azerbaijan capital of Baku will host the first European Games, with around 15 sports to be on the program for the event -- which will be held "in spring or early summer" according to the European Olympic Committees website .  The decision may put it in conflict with other sporting bodies. The International Athletics Federation will stage its world championships in Beijing in August 2015, while the swimming world championships will be held in Russia in July that year.  The European Games will come a year ahead of the next Summer Olympics, to be held in Brazil.  "The NOCs of Europe voted the proposal by secret ballot with an overwhelming majority," the EOC website reported after day two of the body\'s 41st general assembly in Rome.  JUST WATCHED  MUST WATCH  Who makes Michael Phelps giddy? 03:00  "There are plenty of technical details to decide, but the Assembly has above all shown its will to go ahead and make this sports event, which is in no way intended to be a copy of the Olympic Games, a tool with which to enhance the attractiveness of sport.  "The NOCs have received assurance that the event will not cost them a penny, but bring them financial gains."  The idea of a European Games was first mooted three years ago but has been "in the \'Olympic air\' for a long time," the EOC said.  Baku bidded to host the 2020 Summer Olympics but failed to make the IOC\'s shortlist as Istanbul, Madrid and Tokyo were selected. The vote for the host city will take place next September.  "The Baku delegation illustrated their project, presenting a modern and dynamic city that is practically ready to host this new sports event," the EOC said.  Azerbaijani organisers of European Games in Baku to pay ...Azerbaijani organisers of European Games in Baku to pay athletes’ bill | Sport | The Guardian  Azerbaijani organisers of European Games in Baku to pay athletes’ bill  • Baku organisers to cover competitors’ accommodation and travel  • Human rights activists insist event aims to whitewash violations  Protesters have accused organisers of the European Games in Baku of using the event to mask human rights violations. Photograph: Anadolu Agency/Getty Images  Thursday 26 February 2015 13.12\xa0EST  Last modified on Monday 4 April 2016 09.39\xa0EDT  Close  This article is 1 year old  The Azerbaijani organisers of the first European Games in Baku, under fire from campaigners over claims they are using the event to gloss over human rights violations, have confirmed they are footing the bill for athletes from all 50 competing countries.  In June, more than 6,000 athletes will compete in 20 sports at the inaugural Games, organised by the European Olympic Committee and staged in the Azerbaijani capital.  After it emerged that the Baku organising committee was paying the bill for the entire British team of 160, on Thursday it issued a joint statement with the EOC confirming it was covering the accommodation and travel of all competing athletes.  President Ilham Aliyev, a keen evangelist for the project who has invested a reported £6.5bn in sports venues and infrastructure, has been accused by the human rights group Index on Censorship of using the Games to “whitewash” the country’s reputation in the wake of a crackdown on freedom of speech.  There are | Question: In which country were the 2015 European Games held?', ['azerbaijan'], 'mrqa_triviaqa-validation-5654'), tensor(-0.6235)), (('Context: The Right Stuff (film)  The Right Stuff is a 1983 American epic historical drama film.  It was adapted from Tom Wolfe\'s best-selling 1979 book of the same name about the Navy, Marine and Air Force test pilots who were involved in aeronautical research at Edwards Air Force Base, California, as well as the Mercury Seven, the seven military pilots who were selected to be the astronauts for Project Mercury, the first manned spaceflight by the United States.  "The Right Stuff" was written and directed by Philip Kaufman and stars Ed Harris, Scott Glenn, Sam Shepard, Fred Ward, Dennis Quaid and Barbara Hershey.  Levon Helm is the narrator in the introduction and elsewhere in the film, as well as having a co-starring role as Air Force test pilot Jack Ridley.   Jack Ridley (pilot)  Colonel Jackie Lynwood "Jack" Ridley (June 16, 1915 – March 12, 1957) was an aeronautical engineer, USAF test pilot and chief of the U.S. Air Force\'s Flight Test Engineering Laboratory.  He helped develop and test many Cold War era military aircraft but is best known for his work on the Bell X-1, the first aircraft to achieve supersonic flight.  He was highly respected among fellow test pilots, most notably Chuck Yeager, for his engineering skills. | Question: What was the middle name of the pilot who was portrayed by Levon Helm in the film The Right Stuff ?', ['Lynwood'], 'mrqa_hotpotqa-validation-2944'), tensor(-0.6993)), (("Context: An acetate / ˈæsɪteɪt / is a salt formed by the combination of acetic acid with an alkaline , earthy , or metallic base . `` Acetate '' also describes the conjugate base or ion ( specifically , the negatively charged ion called an anion ) typically found in aqueous solution and written with the chemical formula C H O . The neutral molecules formed by the combination of the acetate ion and a positive ion ( called a cation ) are also commonly called `` acetates '' ( hence , acetate of lead , acetate of aluminum , etc . ) . The simplest of these is hydrogen acetate ( called acetic acid ) with corresponding salts , esters , and the polyatomic anion CH CO , or CH COO . | Question: what is the name for the ch3coo - ion ?", ['polyatomic anion'], 'mrqa_naturalquestions-validation-1202'), tensor(-0.7453)), (('Context: Formula One drivers from Mexico  There have been six Formula One drivers from Mexico who have taken part in races since the championship began in 1950.  Pedro Rodríguez is the most successful Mexican driver being the only one to have won a grand prix.  Sergio Pérez, the only other Mexican to finish on the podium, currently races with Sahara Force India F1 Team .   Formula One drivers from Mexico  There have been six Formula One drivers from Mexico who have taken part in races since the championship began in 1950.  Pedro Rodríguez is the most successful Mexican driver being the only one to have won a grand prix.  Sergio Pérez, the only other Mexican to finish on the podium, currently races with Sahara Force India F1 Team .   Sergio Pérez  Sergio Pérez Mendoza (    ; born 26 January 1990) also known as "Checo" Pérez, is a Mexican racing driver, currently driving for Force India. | Question: Which other Mexican Formula One race car driver has held the podium besides the Force India driver born in 1990?', ['Pedro Rodríguez'], 'mrqa_hotpotqa-validation-14'), tensor(-0.8243)), (('Context: Shaqtin\' a Fool  Shaqtin\' a Fool is a weekly segment from the television show "Inside the NBA", the postgame show of "NBA on TNT" following the conclusion of National Basketball Association (NBA) games airing on cable TV channel TNT.  It first aired during the 2011–12 NBA season, when retired NBA All-Star Shaquille O\'Neal voiced it upon joining the show and was created by Turner Sports producer Mike Goldfarb.  It highlights humorous and uncommon basketball plays that have occurred during NBA games in the past week.  O\'Neal is the host and presenter, while the other analysts in studio react and provide commentary.  Most often, those have been fellow "Inside" regulars Ernie Johnson, Kenny Smith, and Charles Barkley, but other "Inside" hosts have also participated, including Chris Webber, Grant Hill, Steve Smith and Matt Winer.   Matt Winer  Matthew Ward Winer (born January 10, 1969) is an American television personality who is currently working for Turner Sports.  Winer also was known for working eight years at ESPN.   Matt Winer  Matthew Ward Winer (born January 10, 1969) is an American television personality who is currently working for Turner Sports.  Winer also was known for working eight years at ESPN. | Question: Shaqtin\' a Fool has included which television personality who worked for 8 yearst at ESPN?', ['Matthew Ward Winer'], 'mrqa_hotpotqa-validation-4367'), tensor(-1.1653)), (('Context: The axons of the tract cells cross over ( decussate ) to the other side of the spinal cord via the anterior white commissure , and to the anterolateral corner of the spinal cord ( hence the spinothalamic tract being part of the anterolateral system ) . Decussation usually occurs 1 - 2 spinal nerve segments above the point of entry . The axons travel up the length of the spinal cord into the brainstem , specifically the rostral ventromedial medulla . | Question: where does decussation occur in the spinothalamic pathway ?', ['usually occurs 1 - 2 spinal nerve segments above the point of entry'], 'mrqa_naturalquestions-validation-7511'), tensor(-1.4654)), (("Context: In the early twentieth century , biologists thought that proteins carried genetic information . This was based on the belief that proteins were more complex than DNA . Phoebus Levene 's influential `` tetranucleotide hypothesis '' , which incorrectly proposed that DNA was a repeating set of identical nucleotides , supported this conclusion . The results of the Avery -- MacLeod -- McCarty experiment , published in 1944 , suggested that DNA was the genetic material , but there was still some hesitation within the general scientific community to accept this , which set the stage for the Hershey -- Chase experiment . | Question: what was the key factor that allowed hershey and chase ?", ['The results of the Avery -- MacLeod -- McCarty experiment'], 'mrqa_naturalquestions-validation-5818'), tensor(-1.4947)), (('Context: The Soulages collection of Italian and French Renaissance objects was acquired between 1859 and 1865, and includes several cassone. The John Jones Collection of French 18th-century art and furnishings was left to the museum in 1882, then valued at £250,000. One of the most important pieces in this collection is a marquetry commode by the ébéniste Jean Henri Riesener dated c1780. Other signed pieces of furniture in the collection include a bureau by Jean-François Oeben, a pair of pedestals with inlaid brass work by André Charles Boulle, a commode by Bernard Vanrisamburgh and a work-table by Martin Carlin. Other 18th-century ébénistes represented in the Museum collection include Adam Weisweiler, David Roentgen, Gilles Joubert & Pierre Langlois. In 1901, Sir George Donaldson donated several pieces of art Nouveau furniture to the museum, which he had acquired the previous year at the Paris Exposition Universelle. This was criticized at the time, with the result that the museum ceased to collect contemporary items and did not do so again until the 1960s. In 1986 the Lady Abingdon collection of French Empire furniture was bequeathed by Mrs T. R. P. Hole. | Question: What items comprise the John Jones Collection?', ['art and furnishings', 'French 18th-century art and furnishings'], 'mrqa_squad-validation-5622'), tensor(-1.4994))]
09/23/2021 13:08:56 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-1516', 'mrqa_squad-validation-3021', 'mrqa_hotpotqa-validation-5699', 'mrqa_naturalquestions-validation-5647', 'mrqa_squad-validation-2629', 'mrqa_triviaqa-validation-2722', 'mrqa_triviaqa-validation-4729', 'mrqa_naturalquestions-validation-2900', 'mrqa_triviaqa-validation-5231', 'mrqa_naturalquestions-validation-3828', 'mrqa_squad-validation-8542', 'mrqa_triviaqa-validation-5362', 'mrqa_naturalquestions-validation-114', 'mrqa_hotpotqa-validation-4825', 'mrqa_squad-validation-6677', 'mrqa_triviaqa-validation-893', 'mrqa_naturalquestions-validation-5502', 'mrqa_triviaqa-validation-5972', 'mrqa_naturalquestions-validation-8119', 'mrqa_naturalquestions-validation-1364', 'mrqa_triviaqa-validation-1792', 'mrqa_squad-validation-4456', 'mrqa_squad-validation-10410', 'mrqa_naturalquestions-validation-2571', 'mrqa_triviaqa-validation-2376', 'mrqa_squad-validation-7746', 'mrqa_hotpotqa-validation-3241', 'mrqa_triviaqa-validation-3901', 'mrqa_squad-validation-1539', 'mrqa_triviaqa-validation-5026', 'mrqa_squad-validation-5360', 'mrqa_naturalquestions-validation-9688']
09/23/2021 13:08:56 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:08:56 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=60) .... Timecode: 5
09/23/2021 13:10:12 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:10:13 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:10:13 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:10:17 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:10:19 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:10:20 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:10:20 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:10:20 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:10:23 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:10:26 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:10:26 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:10:26 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:10:26 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:10:26 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:10:26 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:10:27 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:10:27 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:10:27 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:10:27 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:10:27 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:10:28 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:10:28 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:10:28 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:10:31 - INFO - __main__ - Finished.
09/23/2021 13:10:31 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:10:31 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:10:31 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:10:31 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:10:31 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:10:33 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:10:33 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:10:33 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:10:37 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:10:37 - INFO - __main__ - Found 32 errors.
09/23/2021 13:10:37 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:10:43 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:10:43 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:10:46 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:10:46 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:10:46 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:10:48 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:10:48 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:10:48 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:10:48 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:10:48 - INFO - __main__ - Finished.
09/23/2021 13:10:48 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:10:48 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:10:48 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:10:51 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:10:51 - INFO - __main__ - Found 26 errors.
09/23/2021 13:10:51 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:10:51 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:10:51 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:12:01 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:12:01 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:12:01 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:12:07 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:12:09 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:12:10 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:12:10 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:12:10 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:12:14 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:12:17 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:12:17 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:12:17 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:12:17 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:12:17 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:12:17 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:12:19 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:12:19 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:12:19 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:12:19 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:12:19 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:12:19 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:12:19 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:12:19 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:12:23 - INFO - __main__ - Finished.
09/23/2021 13:12:23 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:12:23 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:12:23 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:12:23 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:12:23 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:12:25 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:12:25 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:12:25 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:12:30 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:12:30 - INFO - __main__ - Found 32 errors.
09/23/2021 13:12:30 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:12:36 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:12:36 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:12:39 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:12:39 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:12:39 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:12:41 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:12:41 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:12:41 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:12:41 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:12:41 - INFO - __main__ - Finished.
09/23/2021 13:12:41 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:12:41 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:12:41 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:12:45 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:12:45 - INFO - __main__ - Found 26 errors.
09/23/2021 13:12:45 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:12:45 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:12:45 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:12:47 - INFO - __main__ - before_loss=1.6672357320785522
09/23/2021 13:12:47 - INFO - __main__ - after_loss=1.5215215682983398
09/23/2021 13:12:47 - INFO - __main__ - interference_scores=[tensor(-0.0543), tensor(-0.2538), tensor(-0.0026), tensor(-0.0171), tensor(-0.0222), tensor(0.2778), tensor(0.1651), tensor(0.0325), tensor(0.0408), tensor(0.3404), tensor(-0.3587), tensor(-0.1814), tensor(0.0087), tensor(0.6209), tensor(-1.1107), tensor(-0.0075), tensor(0.9726), tensor(1.5536), tensor(-0.0919), tensor(0.1775), tensor(-1.0603), tensor(-0.2744), tensor(0.0136), tensor(-0.0396), tensor(0.7536), tensor(-0.1767), tensor(-0.0643), tensor(-0.1964), tensor(-0.7106), tensor(0.1726), tensor(-0.6079), tensor(-0.1457)]
09/23/2021 13:12:47 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-4185', 'mrqa_triviaqa-validation-4054', 'mrqa_squad-validation-392', 'mrqa_triviaqa-validation-7369', 'mrqa_naturalquestions-validation-2730', 'mrqa_hotpotqa-validation-2970', 'mrqa_naturalquestions-validation-8948', 'mrqa_triviaqa-validation-1551', 'mrqa_hotpotqa-validation-1968', 'mrqa_triviaqa-validation-1616', 'mrqa_triviaqa-validation-3915', 'mrqa_naturalquestions-validation-7017', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-5937', 'mrqa_hotpotqa-validation-3241', 'mrqa_naturalquestions-validation-6157', 'mrqa_naturalquestions-validation-9688', 'mrqa_squad-validation-3478', 'mrqa_squad-validation-7746', 'mrqa_squad-validation-6677', 'mrqa_triviaqa-validation-671', 'mrqa_squad-validation-7821', 'mrqa_squad-validation-10322', 'mrqa_naturalquestions-validation-2248', 'mrqa_squad-validation-6303', 'mrqa_hotpotqa-validation-409', 'mrqa_naturalquestions-validation-5465', 'mrqa_triviaqa-validation-2376', 'mrqa_squad-validation-5622', 'mrqa_naturalquestions-validation-3028', 'mrqa_triviaqa-validation-5972', 'mrqa_triviaqa-validation-6351']
09/23/2021 13:12:47 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:12:47 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 1
09/23/2021 13:13:00 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:13:00 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 1
09/23/2021 13:13:04 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:13:04 - INFO - __main__ - Instant Fixing Rate: 0.9615384615384616
09/23/2021 13:13:04 - INFO - __main__ - Instant Retention Rate: 0.33333333277777777
09/23/2021 13:13:06 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_002.pt.
09/23/2021 13:13:06 - INFO - __main__ - Saving the current error examples (len=26) to the memory.
09/23/2021 13:13:06 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:13:06 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:13:06 - INFO - __main__ - Finished.
09/23/2021 13:13:06 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:13:06 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:13:06 - INFO - __main__ - Evaluating to get errors .... Timecode: 2
09/23/2021 13:13:10 - INFO - __main__ - Before Error Fixing: 0.0625
09/23/2021 13:13:10 - INFO - __main__ - Found 30 errors.
09/23/2021 13:13:10 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:13:10 - INFO - __main__ - Current memory size: 58.
09/23/2021 13:13:10 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=58;
09/23/2021 13:13:14 - INFO - __main__ - before_loss=1.4606165885925293
09/23/2021 13:13:14 - INFO - __main__ - after_loss=1.4901171922683716
09/23/2021 13:13:14 - INFO - __main__ - interference_scores=[tensor(0.3969), tensor(0.7486), tensor(-0.0247), tensor(-0.0241), tensor(-0.0899), tensor(0.0563), tensor(-0.0228), tensor(-0.1835), tensor(-0.0961), tensor(0.0590), tensor(0.2218), tensor(-0.0174), tensor(-0.3367), tensor(-0.0138), tensor(0.0361), tensor(-0.6917), tensor(0.0099), tensor(0.0389), tensor(-0.0359), tensor(-0.6166), tensor(-0.0002), tensor(-0.1427), tensor(0.5328), tensor(-0.0227), tensor(0.0554), tensor(-0.0943), tensor(-0.3411), tensor(0.1673), tensor(0.0199), tensor(-0.0089), tensor(1.0201), tensor(-0.8124), tensor(-0.0526), tensor(-1.5120), tensor(0.1053), tensor(0.5982), tensor(0.0104), tensor(-0.7154), tensor(1.5011), tensor(0.0435), tensor(0.0398), tensor(-0.0282), tensor(0.0827), tensor(0.0177), tensor(0.3998), tensor(-0.0404), tensor(-0.0135), tensor(0.0495), tensor(-0.3848), tensor(-0.0376), tensor(-0.0644), tensor(-0.0295), tensor(-0.2543), tensor(-0.3867), tensor(0.0177), tensor(-0.0803), tensor(0.0542), tensor(0.0295)]
09/23/2021 13:13:14 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-2376', 'mrqa_naturalquestions-validation-2248', 'mrqa_hotpotqa-validation-1968', 'mrqa_triviaqa-validation-1924', 'mrqa_triviaqa-validation-1551', 'mrqa_naturalquestions-validation-2730', 'mrqa_hotpotqa-validation-5325', 'mrqa_triviaqa-validation-1437', 'mrqa_triviaqa-validation-5972', 'mrqa_squad-validation-1516', 'mrqa_triviaqa-validation-6351', 'mrqa_squad-validation-5622', 'mrqa_hotpotqa-validation-3241', 'mrqa_hotpotqa-validation-1201', 'mrqa_naturalquestions-validation-6157', 'mrqa_triviaqa-validation-5026', 'mrqa_naturalquestions-validation-8948', 'mrqa_hotpotqa-validation-5802', 'mrqa_naturalquestions-validation-7511', 'mrqa_naturalquestions-validation-7017', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-7369', 'mrqa_triviaqa-validation-3915', 'mrqa_squad-validation-392', 'mrqa_squad-validation-7746', 'mrqa_hotpotqa-validation-409', 'mrqa_naturalquestions-validation-1364', 'mrqa_squad-validation-3478', 'mrqa_naturalquestions-validation-9688', 'mrqa_naturalquestions-validation-3028', 'mrqa_hotpotqa-validation-400', 'mrqa_hotpotqa-validation-1626']
09/23/2021 13:13:14 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:13:14 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=62) .... Timecode: 2
09/23/2021 13:13:27 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:13:27 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 2
09/23/2021 13:13:45 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:13:45 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:13:45 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:13:51 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:13:54 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:13:54 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:13:54 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:13:55 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:13:58 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:14:01 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:14:01 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:14:01 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:14:01 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:14:01 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:14:01 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:14:03 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:14:03 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:14:03 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:14:03 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:14:03 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:14:03 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:14:03 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:14:04 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:14:08 - INFO - __main__ - Finished.
09/23/2021 13:14:08 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:14:08 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:14:08 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:14:08 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:14:08 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:14:09 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:14:09 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:14:09 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:14:14 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:14:14 - INFO - __main__ - Found 32 errors.
09/23/2021 13:14:14 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:14:22 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:14:22 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:14:25 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:14:25 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:14:25 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:14:27 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:14:27 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:14:27 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:14:27 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:14:27 - INFO - __main__ - Finished.
09/23/2021 13:14:27 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:14:27 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:14:27 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:14:31 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:14:31 - INFO - __main__ - Found 26 errors.
09/23/2021 13:14:31 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:14:31 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:14:31 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:14:33 - INFO - __main__ - before_losses=[tensor(1.5547), tensor(1.8127), tensor(1.4551), tensor(1.5770), tensor(1.4772), tensor(1.4888), tensor(1.9199), tensor(1.5381), tensor(1.5362), tensor(1.5215), tensor(1.8749), tensor(1.8486), tensor(1.5163), tensor(1.5249), tensor(2.9214), tensor(1.5589), tensor(1.8541), tensor(1.5203), tensor(1.6300), tensor(1.4525), tensor(3.4037), tensor(1.8106), tensor(1.5514), tensor(1.5599), tensor(1.5650), tensor(1.8619), tensor(1.5706), tensor(1.6852), tensor(2.5855), tensor(1.5005), tensor(2.0851), tensor(1.6672)]
09/23/2021 13:14:33 - INFO - __main__ - after_losses=[tensor(1.5005), tensor(1.5589), tensor(1.4525), tensor(1.5599), tensor(1.4551), tensor(1.7666), tensor(2.0851), tensor(1.5706), tensor(1.5770), tensor(1.8619), tensor(1.5163), tensor(1.6672), tensor(1.5249), tensor(2.1458), tensor(1.8106), tensor(1.5514), tensor(2.8267), tensor(3.0739), tensor(1.5381), tensor(1.6300), tensor(2.3435), tensor(1.5362), tensor(1.5650), tensor(1.5203), tensor(2.3185), tensor(1.6852), tensor(1.5062), tensor(1.4888), tensor(1.8749), tensor(1.6731), tensor(1.4772), tensor(1.5215)]
09/23/2021 13:14:33 - INFO - __main__ - interference_scores=[tensor(-0.0543), tensor(-0.2538), tensor(-0.0026), tensor(-0.0171), tensor(-0.0222), tensor(0.2778), tensor(0.1651), tensor(0.0325), tensor(0.0408), tensor(0.3404), tensor(-0.3587), tensor(-0.1814), tensor(0.0087), tensor(0.6209), tensor(-1.1107), tensor(-0.0075), tensor(0.9726), tensor(1.5536), tensor(-0.0919), tensor(0.1775), tensor(-1.0603), tensor(-0.2744), tensor(0.0136), tensor(-0.0396), tensor(0.7536), tensor(-0.1767), tensor(-0.0643), tensor(-0.1964), tensor(-0.7106), tensor(0.1726), tensor(-0.6079), tensor(-0.1457)]
09/23/2021 13:14:33 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-4185', 'mrqa_triviaqa-validation-4054', 'mrqa_squad-validation-392', 'mrqa_triviaqa-validation-7369', 'mrqa_naturalquestions-validation-2730', 'mrqa_hotpotqa-validation-2970', 'mrqa_naturalquestions-validation-8948', 'mrqa_triviaqa-validation-1551', 'mrqa_hotpotqa-validation-1968', 'mrqa_triviaqa-validation-1616', 'mrqa_triviaqa-validation-3915', 'mrqa_naturalquestions-validation-7017', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-5937', 'mrqa_hotpotqa-validation-3241', 'mrqa_naturalquestions-validation-6157', 'mrqa_naturalquestions-validation-9688', 'mrqa_squad-validation-3478', 'mrqa_squad-validation-7746', 'mrqa_squad-validation-6677', 'mrqa_triviaqa-validation-671', 'mrqa_squad-validation-7821', 'mrqa_squad-validation-10322', 'mrqa_naturalquestions-validation-2248', 'mrqa_squad-validation-6303', 'mrqa_hotpotqa-validation-409', 'mrqa_naturalquestions-validation-5465', 'mrqa_triviaqa-validation-2376', 'mrqa_squad-validation-5622', 'mrqa_naturalquestions-validation-3028', 'mrqa_triviaqa-validation-5972', 'mrqa_triviaqa-validation-6351']
09/23/2021 13:14:33 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:14:33 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 1
09/23/2021 13:14:45 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:14:45 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 1
09/23/2021 13:14:49 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:14:49 - INFO - __main__ - Instant Fixing Rate: 0.9615384615384616
09/23/2021 13:14:49 - INFO - __main__ - Instant Retention Rate: 0.33333333277777777
09/23/2021 13:14:51 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_002.pt.
09/23/2021 13:14:51 - INFO - __main__ - Saving the current error examples (len=26) to the memory.
09/23/2021 13:14:51 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:14:51 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:14:51 - INFO - __main__ - Finished.
09/23/2021 13:14:51 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:14:51 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:14:51 - INFO - __main__ - Evaluating to get errors .... Timecode: 2
09/23/2021 13:15:02 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:15:03 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:15:03 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:15:09 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:15:11 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:15:12 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:15:12 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:15:12 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:15:16 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:15:19 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:15:19 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:15:19 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:15:19 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:15:19 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:15:19 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:15:21 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:15:21 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:15:21 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:15:21 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:15:21 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:15:21 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:15:21 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:15:22 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:15:26 - INFO - __main__ - Finished.
09/23/2021 13:15:26 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:15:26 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:15:26 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:15:26 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:15:26 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:15:28 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:15:28 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:15:28 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:15:32 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:15:32 - INFO - __main__ - Found 32 errors.
09/23/2021 13:15:32 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:15:38 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:15:38 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:15:41 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:15:41 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:15:41 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:15:43 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:15:43 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:15:43 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:15:43 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:15:43 - INFO - __main__ - Finished.
09/23/2021 13:15:43 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:15:43 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:15:43 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:15:47 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:15:47 - INFO - __main__ - Found 26 errors.
09/23/2021 13:15:47 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:15:47 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:15:47 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:15:48 - INFO - __main__ - before_losses=[tensor(1.5547), tensor(1.8127), tensor(1.4551), tensor(1.5770), tensor(1.4772), tensor(1.4888), tensor(1.9199), tensor(1.5381), tensor(1.5362), tensor(1.5215), tensor(1.8749), tensor(1.8486), tensor(1.5163), tensor(1.5249), tensor(2.9214), tensor(1.5589), tensor(1.8541), tensor(1.5203), tensor(1.6300), tensor(1.4525), tensor(3.4037), tensor(1.8106), tensor(1.5514), tensor(1.5599), tensor(1.5650), tensor(1.8619), tensor(1.5706), tensor(1.6852), tensor(2.5855), tensor(1.5005), tensor(2.0851), tensor(1.6672)]
09/23/2021 13:15:48 - INFO - __main__ - after_losses=[tensor(1.5005), tensor(1.5589), tensor(1.4525), tensor(1.5599), tensor(1.4551), tensor(1.7666), tensor(2.0851), tensor(1.5706), tensor(1.5770), tensor(1.8619), tensor(1.5163), tensor(1.6672), tensor(1.5249), tensor(2.1458), tensor(1.8106), tensor(1.5514), tensor(2.8267), tensor(3.0739), tensor(1.5381), tensor(1.6300), tensor(2.3435), tensor(1.5362), tensor(1.5650), tensor(1.5203), tensor(2.3185), tensor(1.6852), tensor(1.5062), tensor(1.4888), tensor(1.8749), tensor(1.6731), tensor(1.4772), tensor(1.5215)]
09/23/2021 13:15:48 - INFO - __main__ - interference_scores=[tensor(-0.0543), tensor(-0.2538), tensor(-0.0026), tensor(-0.0171), tensor(-0.0222), tensor(0.2778), tensor(0.1651), tensor(0.0325), tensor(0.0408), tensor(0.3404), tensor(-0.3587), tensor(-0.1814), tensor(0.0087), tensor(0.6209), tensor(-1.1107), tensor(-0.0075), tensor(0.9726), tensor(1.5536), tensor(-0.0919), tensor(0.1775), tensor(-1.0603), tensor(-0.2744), tensor(0.0136), tensor(-0.0396), tensor(0.7536), tensor(-0.1767), tensor(-0.0643), tensor(-0.1964), tensor(-0.7106), tensor(0.1726), tensor(-0.6079), tensor(-0.1457)]
09/23/2021 13:15:48 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-4185', 'mrqa_triviaqa-validation-4054', 'mrqa_squad-validation-392', 'mrqa_triviaqa-validation-7369', 'mrqa_naturalquestions-validation-2730', 'mrqa_hotpotqa-validation-2970', 'mrqa_naturalquestions-validation-8948', 'mrqa_triviaqa-validation-1551', 'mrqa_hotpotqa-validation-1968', 'mrqa_triviaqa-validation-1616', 'mrqa_triviaqa-validation-3915', 'mrqa_naturalquestions-validation-7017', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-5937', 'mrqa_hotpotqa-validation-3241', 'mrqa_naturalquestions-validation-6157', 'mrqa_naturalquestions-validation-9688', 'mrqa_squad-validation-3478', 'mrqa_squad-validation-7746', 'mrqa_squad-validation-6677', 'mrqa_triviaqa-validation-671', 'mrqa_squad-validation-7821', 'mrqa_squad-validation-10322', 'mrqa_naturalquestions-validation-2248', 'mrqa_squad-validation-6303', 'mrqa_hotpotqa-validation-409', 'mrqa_naturalquestions-validation-5465', 'mrqa_triviaqa-validation-2376', 'mrqa_squad-validation-5622', 'mrqa_naturalquestions-validation-3028', 'mrqa_triviaqa-validation-5972', 'mrqa_triviaqa-validation-6351']
09/23/2021 13:15:48 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:15:48 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 1
09/23/2021 13:20:56 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:20:57 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:20:57 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:21:03 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:21:05 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:21:06 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:21:06 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:21:06 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:21:10 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:21:13 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:21:13 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:21:13 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:21:13 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:21:13 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:21:13 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:21:14 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:21:14 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:21:14 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:21:14 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:21:14 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:21:15 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:21:15 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:21:15 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:21:19 - INFO - __main__ - Finished.
09/23/2021 13:21:19 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:21:19 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:21:19 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:21:19 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:21:19 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:21:21 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:21:21 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:21:21 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:21:25 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:21:25 - INFO - __main__ - Found 32 errors.
09/23/2021 13:21:25 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:21:32 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:21:32 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:21:35 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:21:35 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:21:35 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:21:37 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:21:37 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:21:37 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:21:37 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:21:37 - INFO - __main__ - Finished.
09/23/2021 13:21:37 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:21:37 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:21:37 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:21:41 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:21:41 - INFO - __main__ - Found 26 errors.
09/23/2021 13:21:41 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:21:41 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:21:41 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:22:51 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:22:52 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:22:52 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:22:57 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:22:59 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:23:00 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:23:00 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:23:01 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:23:04 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:23:08 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:23:08 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:23:08 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:23:08 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:23:08 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:23:08 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:23:09 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:23:09 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:23:09 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:23:09 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:23:09 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:23:09 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:23:09 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:23:10 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:23:14 - INFO - __main__ - Finished.
09/23/2021 13:23:14 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:23:14 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:23:14 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:23:14 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:23:14 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:23:16 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:23:16 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:23:16 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:23:20 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:23:20 - INFO - __main__ - Found 32 errors.
09/23/2021 13:23:20 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:23:26 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:23:26 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:23:30 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:23:30 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:23:30 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:23:32 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:23:32 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:23:32 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:23:32 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:23:32 - INFO - __main__ - Finished.
09/23/2021 13:23:32 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:23:32 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:23:32 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:23:35 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:23:35 - INFO - __main__ - Found 26 errors.
09/23/2021 13:23:35 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:23:35 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:23:35 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:25:59 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:25:59 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:25:59 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:26:05 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:26:07 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:26:08 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:26:08 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:26:08 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:26:12 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:26:15 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:26:15 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:26:15 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:26:15 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:26:15 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:26:15 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:26:17 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:26:17 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:26:17 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:26:17 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:26:17 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:26:17 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:26:17 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:26:17 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:26:21 - INFO - __main__ - Finished.
09/23/2021 13:26:21 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:26:21 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:26:21 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:26:21 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:26:21 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:26:23 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:26:23 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:26:23 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:26:28 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:26:28 - INFO - __main__ - Found 32 errors.
09/23/2021 13:26:28 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:26:35 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:26:35 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:26:39 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:26:39 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:26:39 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:26:41 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:26:41 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:26:41 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:26:41 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:26:41 - INFO - __main__ - Finished.
09/23/2021 13:26:41 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:26:41 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:26:41 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:26:45 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:26:45 - INFO - __main__ - Found 26 errors.
09/23/2021 13:26:45 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:26:45 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:26:45 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:26:46 - INFO - __main__ - before_losses=[tensor(1.5589), tensor(1.5005), tensor(1.8127), tensor(1.4888), tensor(1.5770), tensor(1.5163), tensor(1.4772), tensor(3.4037), tensor(2.5855), tensor(1.5249), tensor(2.9214), tensor(1.8106), tensor(2.2514), tensor(1.8749), tensor(1.5362), tensor(1.4525), tensor(1.5706), tensor(1.5381), tensor(1.6300), tensor(1.5215), tensor(1.8312), tensor(1.5514), tensor(1.5321), tensor(1.5203), tensor(1.5773), tensor(1.5257), tensor(1.5062), tensor(1.5650), tensor(1.4551), tensor(3.0739), tensor(1.5547), tensor(1.6852)]
09/23/2021 13:26:46 - INFO - __main__ - after_losses=[tensor(1.5589), tensor(1.5005), tensor(1.8127), tensor(1.4888), tensor(1.5770), tensor(1.5163), tensor(1.4772), tensor(2.3435), tensor(2.5855), tensor(1.5249), tensor(2.0737), tensor(1.8106), tensor(1.7202), tensor(1.8749), tensor(1.5362), tensor(1.4525), tensor(1.5706), tensor(1.5381), tensor(1.6300), tensor(1.5215), tensor(1.5599), tensor(1.5514), tensor(1.8619), tensor(1.5203), tensor(1.5773), tensor(1.5257), tensor(1.5062), tensor(1.5650), tensor(1.4551), tensor(3.0739), tensor(1.5547), tensor(1.6852)]
09/23/2021 13:26:46 - INFO - __main__ - interference_scores=[tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(-1.0603), tensor(0.), tensor(0.), tensor(-0.8477), tensor(0.), tensor(-0.5312), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(-0.2714), tensor(0.), tensor(0.3298), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]
09/23/2021 13:26:46 - INFO - __main__ - retrieved candidates ids = ['mrqa_naturalquestions-validation-7017', 'mrqa_squad-validation-7746', 'mrqa_hotpotqa-validation-409', 'mrqa_triviaqa-validation-5937', 'mrqa_naturalquestions-validation-6157', 'mrqa_naturalquestions-validation-9688', 'mrqa_hotpotqa-validation-2970', 'mrqa_hotpotqa-validation-1968', 'mrqa_triviaqa-validation-1616', 'mrqa_naturalquestions-validation-2730', 'mrqa_naturalquestions-validation-2248', 'mrqa_triviaqa-validation-7369', 'mrqa_triviaqa-validation-6351', 'mrqa_hotpotqa-validation-3241', 'mrqa_triviaqa-validation-4054', 'mrqa_squad-validation-4185', 'mrqa_triviaqa-validation-671', 'mrqa_naturalquestions-validation-8948', 'mrqa_naturalquestions-validation-5465', 'mrqa_squad-validation-3478', 'mrqa_squad-validation-392', 'mrqa_squad-validation-10322', 'mrqa_squad-validation-6677', 'mrqa_squad-validation-6303', 'mrqa_naturalquestions-validation-3028', 'mrqa_triviaqa-validation-1551', 'mrqa_squad-validation-5622', 'mrqa_squad-validation-7821', 'mrqa_triviaqa-validation-5972', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-2376', 'mrqa_triviaqa-validation-3915']
09/23/2021 13:26:46 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:26:46 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 1
09/23/2021 13:26:58 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:26:58 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 1
09/23/2021 13:27:02 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:27:02 - INFO - __main__ - Instant Fixing Rate: 0.9230769230769231
09/23/2021 13:27:02 - INFO - __main__ - Instant Retention Rate: 0.49999999916666665
09/23/2021 13:27:04 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_002.pt.
09/23/2021 13:27:04 - INFO - __main__ - Saving the current error examples (len=26) to the memory.
09/23/2021 13:27:04 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:27:04 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:27:04 - INFO - __main__ - Finished.
09/23/2021 13:27:04 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:27:04 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:27:04 - INFO - __main__ - Evaluating to get errors .... Timecode: 2
09/23/2021 13:28:23 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:28:24 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:28:24 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:28:29 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:28:32 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:28:33 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:28:33 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:28:33 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:28:37 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:28:40 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:28:40 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:28:40 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:28:40 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:28:40 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:28:40 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:28:42 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:28:42 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:28:42 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:28:42 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:28:42 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:28:42 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:28:42 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:28:42 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:28:46 - INFO - __main__ - Finished.
09/23/2021 13:28:46 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:28:46 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:28:46 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:28:46 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:28:46 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:28:48 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:28:48 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:28:48 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:28:52 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:28:52 - INFO - __main__ - Found 32 errors.
09/23/2021 13:28:52 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:28:59 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:28:59 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:29:02 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:29:02 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:29:02 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:29:04 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:29:04 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:29:04 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:29:04 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:29:04 - INFO - __main__ - Finished.
09/23/2021 13:29:04 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:29:04 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:29:04 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:29:07 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:29:07 - INFO - __main__ - Found 26 errors.
09/23/2021 13:29:07 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:29:07 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:29:07 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:29:09 - INFO - __main__ - before_losses=[tensor(1.5589), tensor(1.5005), tensor(1.8127), tensor(1.4888), tensor(1.5770), tensor(1.5163), tensor(1.4772), tensor(2.3435), tensor(2.8267), tensor(1.5249), tensor(2.4410), tensor(1.8106), tensor(2.1518), tensor(1.8749), tensor(1.5362), tensor(1.4525), tensor(1.5706), tensor(1.5381), tensor(1.6300), tensor(1.5215), tensor(1.5599), tensor(1.5514), tensor(1.8619), tensor(1.5203), tensor(1.5773), tensor(2.0851), tensor(1.5062), tensor(1.5650), tensor(1.4551), tensor(1.8541), tensor(1.6731), tensor(1.6852)]
09/23/2021 13:29:09 - INFO - __main__ - after_losses=[tensor(1.5589), tensor(1.5005), tensor(1.8127), tensor(1.4888), tensor(1.5770), tensor(1.5163), tensor(1.4772), tensor(3.4037), tensor(2.5855), tensor(1.5249), tensor(2.4410), tensor(1.6460), tensor(2.1518), tensor(1.8749), tensor(1.5362), tensor(1.4525), tensor(1.5706), tensor(1.5381), tensor(1.6300), tensor(1.5215), tensor(1.8312), tensor(1.5514), tensor(1.8619), tensor(1.5203), tensor(1.6672), tensor(1.5257), tensor(1.9199), tensor(1.5650), tensor(1.4551), tensor(3.0739), tensor(1.6731), tensor(1.6852)]
09/23/2021 13:29:09 - INFO - __main__ - interference_scores=[tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.0603), tensor(-0.2412), tensor(0.), tensor(0.), tensor(-0.1646), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.2714), tensor(0.), tensor(0.), tensor(0.), tensor(0.0899), tensor(-0.5594), tensor(0.4137), tensor(0.), tensor(0.), tensor(1.2198), tensor(0.), tensor(0.)]
09/23/2021 13:29:09 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-1551', 'mrqa_triviaqa-validation-3915', 'mrqa_squad-validation-6677', 'mrqa_triviaqa-validation-5972', 'mrqa_squad-validation-392', 'mrqa_squad-validation-7746', 'mrqa_hotpotqa-validation-409', 'mrqa_triviaqa-validation-5937', 'mrqa_naturalquestions-validation-6157', 'mrqa_naturalquestions-validation-9688', 'mrqa_hotpotqa-validation-2970', 'mrqa_hotpotqa-validation-1968', 'mrqa_naturalquestions-validation-2730', 'mrqa_triviaqa-validation-2376', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-7369', 'mrqa_triviaqa-validation-6351', 'mrqa_hotpotqa-validation-3241', 'mrqa_triviaqa-validation-4054', 'mrqa_squad-validation-4185', 'mrqa_triviaqa-validation-671', 'mrqa_naturalquestions-validation-8948', 'mrqa_naturalquestions-validation-5465', 'mrqa_naturalquestions-validation-7017', 'mrqa_squad-validation-3478', 'mrqa_squad-validation-6303', 'mrqa_naturalquestions-validation-3028', 'mrqa_squad-validation-5622', 'mrqa_squad-validation-7821', 'mrqa_naturalquestions-validation-2248', 'mrqa_triviaqa-validation-1616', 'mrqa_squad-validation-10322']
09/23/2021 13:29:09 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:29:09 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 1
09/23/2021 13:32:55 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:32:56 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:32:56 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:33:02 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:33:05 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:33:05 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:33:05 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:33:06 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:33:09 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:33:13 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:33:13 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:33:13 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:33:13 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:33:13 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:33:13 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:33:14 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:33:14 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:33:14 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:33:14 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:33:14 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:33:15 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:33:15 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:33:15 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:33:19 - INFO - __main__ - Finished.
09/23/2021 13:33:19 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:33:19 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:33:19 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:33:19 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:33:19 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:33:21 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:33:21 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:33:21 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:33:25 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:33:25 - INFO - __main__ - Found 32 errors.
09/23/2021 13:33:25 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:33:33 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:33:33 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:33:36 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:33:36 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:33:36 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:33:37 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:33:38 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:33:38 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:33:38 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:33:38 - INFO - __main__ - Finished.
09/23/2021 13:33:38 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:33:38 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:33:38 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:33:41 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:33:41 - INFO - __main__ - Found 26 errors.
09/23/2021 13:33:41 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:33:41 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:33:41 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:34:04 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:34:04 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:34:04 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:34:10 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:34:13 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:34:14 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:34:14 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:34:14 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:34:18 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:34:21 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:34:21 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:34:21 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:34:21 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:34:21 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:34:21 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:34:22 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:34:22 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:34:22 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:34:22 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:34:22 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:34:23 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:34:23 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:34:23 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:34:27 - INFO - __main__ - Finished.
09/23/2021 13:34:27 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:34:27 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:34:27 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:34:27 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:34:27 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:34:29 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:34:29 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:34:29 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:34:33 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:34:33 - INFO - __main__ - Found 32 errors.
09/23/2021 13:34:33 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:34:40 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:34:40 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:34:43 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:34:43 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:34:43 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:34:45 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:34:45 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:34:45 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:34:45 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:34:45 - INFO - __main__ - Finished.
09/23/2021 13:34:45 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:34:45 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:34:45 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:34:49 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:34:49 - INFO - __main__ - Found 26 errors.
09/23/2021 13:34:49 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:34:49 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:34:49 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:34:49 - INFO - __main__ - candidate_examples: [('Context: It has been claimed that the transmission of the first episode was delayed by ten minutes due to extended news coverage of the assassination of US President John F. Kennedy the previous day; whereas in fact it went out after a delay of eighty seconds. The BBC believed that many viewers had missed this introduction to a new series due to the coverage of the assassination, as well as a series of power blackouts across the country, and they broadcast it again on 30 November 1963, just before episode two. | Question: What other event made the BBC concerned that viewers had not seen the premier of Doctor Who?', ['a series of power blackouts across the country', 'power blackouts'], 'mrqa_squad-validation-7746'), ("Context: Petra Kvitová career statistics  This is a list of the main career statistics of Czech professional tennis player Petra Kvitová.  To date, Kvitová has won 20 singles titles including two Grand Slam singles titles at the Wimbledon Championships, one WTA Tour Championships singles title, two WTA Premier Mandatory singles titles and four WTA Premier 5 singles titles.  She was also the bronze medallist at the 2016 Rio Olympics, a semifinalist at the 2010 Wimbledon Championships, 2012 Australian Open and 2012 French Open and a quarterfinalist at the 2011 Australian Open, 2012 Wimbledon Championships, 2013 Wimbledon Championships, 2015 US Open and 2017 US Open.  Kvitová reached her career-high ranking of world no. 2 on 31 October 2011.   2017 US Open (tennis)  The 2017 US Open was the 137th edition of tennis' US Open and the fourth and final Grand Slam event of the year.  It was held on outdoor hard courts at the USTA Billie Jean King National Tennis Center in New York City.  Experimental rules featured in qualifying for the main draw as well as in the junior, wheelchair and exhibition events. | Question: What edition of tennis' US Open was the 2017 US Open when Petra Kvitova was a quarterfinalist?", ['137th'], 'mrqa_hotpotqa-validation-409'), ('Context: Sunday in the Park with George - Music Theatre InternationalSunday In The Park... | Music Theatre International  Music Theatre International  Request Licenses & Perusals, Pay Invoices  Your Web Profile  We are working quickly to merge our two logins.  Region  Sunday In The Park With George  Original Broadway Version (1984)  Inspired by George Seurat\'s famous painting, this poetic masterpiece explores the challenges in understanding life and art.  Inspired by the painting A Sunday Afternoon on the Island of La Grande Jatte by Georges Seurat, Sunday In The Park With George, Stephen Sondheim and James Lapine\'s stunning masterpiece merges past and present into beautiful, poignant truths about life, love and the creation of art. One of the most acclaimed musicals of our time, this moving study of the enigmatic painter Georges Seurat won a Pulitzer Prize and was nominated for an astounding 10 Tony Awards including Best Musical.  The days leading up to the completion of his most famous painting, A Sunday Afternoon on the Island of La Grande Jatte, Georges Seurat is struggling to make meaningful art and maintaining a relationship with his lover Dot. Amid the scorn of the artistic community, Seurat\'s artistic ability thrives while his love diminishes. A century later, Seurat\'s descendant - named George and also an artist - finds himself burnt out and in search of what artistic path to follow, but he finds the answer to his future in the past.  An ensemble of strong singing actors performs this challenging and heartbreaking work about our need to connect to the past, present and future. Sunday In The Park With George features two coveted starring roles made famous by the Broadway performances of Mandy Patinkin and Bernadette Peters. The show may be staged simply as in its original workshop production or with the grandeur of Seurat\'s masterpiece.  "Sunday in the Park with George" - About.com Education"Sunday in the Park with George"  "Sunday in the Park with George"  "Sunday in the Park with George"  From Canvas to Characters  By Rosalind Flynn  Updated February 27, 2016.  Anyone who reads or plans to attend a performance of Sunday in the Park With George by Stephen Sondheim and James Lapine should first spend some time looking at an image of the this painting by Georges Seurat :\xa0 Its original French title is Un dimanche après-midi à l\'Île de la Grande Jatte – English translation: “ Sunday Afternoon on the Island of la Grande Jatte .” (Click on the painting title to view the online image.)  The first act of Sunday in the Park With George occurs with Seurat the artist creating and perfecting his painting. The creative musical theatre artists who were inspired by this painting drew the other characters in Act One from figures in the painting.  Looking Closely at Art  It would not be surprising to learn that Sondheim and Lapine used their own version of Visual Thinking Strategies to examine this painting that is 10 feet wide and 6 and a half feet tall. Visual Thinking Strategies provide ways to look closely at works of art by asking observers to respond to these three questions:  continue reading below our video  10 Best Universities in the United States  1. What’s going on in this picture?  2. What do you see that\xa0makes you say that?  3. What more can we find?  In fact, it would be excellent to begin a study of this play by engaging students in doing precisely that – looking at an image of “Sunday Afternoon on the Island of la Grande Jatte” and asking them to respond to those questions.  What’s Going On? According to Sondheim and Lapine  The painting’s setting is the island of la Grande Jatte, which is located northwest of Paris in the Seine River. In 1884 (when Seurat began working on his painting), the island was a pastoral place where people could go to get away from city life. The musical’s creative artists pulled their characters from the canvas. Here’s a Who’s Who:  The most prominent figure is the woman in profile on the bottom right hand side. Sondheim and Lapine imagined her as Seurat’s mistress and they | Question: The musical \'Sunday In The Park With George\' was inspired by a painting by which artist?', ['georges seurat', 'seurat'], 'mrqa_triviaqa-validation-5937'), ('Context: <Ol>  A new ruler unites China , founds a new dynasty , and gains the Mandate of Heaven .   China , under the new dynasty , achieves prosperity .   The population increases .   Corruption becomes rampant in the imperial court , and the empire begins to enter decline and instability .   A natural disaster wipes out farm land . The disaster normally would not have been a problem ; however , together with the Corruption and overpopulation , it causes famine .   The famine causes the population to rebel and a civil war ensues .   The ruler loses the Mandate of Heaven .   The population decreases because of the violence .   China goes through a warring states period .   One state emerges victorious .   The state starts a new empire .   The empire gains the Mandate of Heaven .  </Ol> | Question: in the dynastic cycle what is the right to rule called ?', ['the Mandate of Heaven'], 'mrqa_naturalquestions-validation-6157'), ('Context: Although partly functional , weather vanes are generally decorative , often featuring the traditional cockerel design with letters indicating the points of the compass . Other common motifs include ships , arrows and horses . Not all weather vanes have pointers . When the wind is sufficiently strong , the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing . | Question: where does the arrow of a wind vane point ?', ['the direction from which the wind is blowing'], 'mrqa_naturalquestions-validation-9688'), ('Context: Cor, Blimey!  Cor, Blimey!  is a 2000 TV film that follows the relationship between "Carry On" film actors Sid James (played by Geoffrey Hutchings) and Barbara Windsor (played by Samantha Spiro).   Samantha Spiro  Samantha Spiro (born 20 June 1968) is a double Olivier Award-winning English actress.  She is best known for portraying Barbara Windsor in the stage play "Cleo, Camping, Emmanuelle and Dick" and the television film "Cor, Blimey! ", DI Vivien Friend in "", and Melessa Tarly in the HBO series "Game of Thrones". | Question: Who plays opposite the double Olivier Award-winning English actress in the 2000 TV film following the relationship between "Carry On" film actors Sid James and Barbara Windsor?', ['Geoffrey Hutchings'], 'mrqa_hotpotqa-validation-2970'), ('Context: Walking in the Air  "Walking in the Air" is a song written by Howard Blake for the 1982 animated film of Raymond Briggs\' 1978 children\'s book "The Snowman".  The song forms the centrepiece of "The Snowman", which has become a seasonal favorite on British and Finnish television.  The story relates the fleeting adventures of a young boy and a snowman who has come to life.  In the second part of the story, the boy and the snowman fly to the North Pole.  "Walking in the Air" is the theme for the journey.  They attend a party of snowmen, at which the boy seems to be the only human until they meet Father Christmas with his reindeer, and the boy is given a scarf with a snowman pattern.  In the film, the song was performed by St Paul\'s Cathedral choirboy Peter Auty, and reissued in 1985 (on Stiff Records) and 1987.   Walking in the Air  "Walking in the Air" is a song written by Howard Blake for the 1982 animated film of Raymond Briggs\' 1978 children\'s book "The Snowman".  The song forms the centrepiece of "The Snowman", which has become a seasonal favorite on British and Finnish television.  The story relates the fleeting adventures of a young boy and a snowman who has come to life.  In the second part of the story, the boy and the snowman fly to the North Pole.  "Walking in the Air" is the theme for the journey.  They attend a party of snowmen, at which the boy seems to be the only human until they meet Father Christmas with his reindeer, and the boy is given a scarf with a snowman pattern.  In the film, the song was performed by St Paul\'s Cathedral choirboy Peter Auty, and reissued in 1985 (on Stiff Records) and 1987.   Peter Auty  Peter Robert Auty (born 4 November 1969) is an English operatic tenor who has worked with most of the major opera companies in Britain and a number of companies in continental Europe. | Question: An English operatic tenor performed "Walking in the Air" for which 1982 animated film?', ['The Snowman'], 'mrqa_hotpotqa-validation-1968'), ('Context: Bodhi Natural Health Products | Bodhi Natural ProductsBodhi Natural Health Products | Bodhi Natural Products  Stockists  Welcome  “It is my wish that by using these products you can enjoy life to the fullest by having good health, hopefully relieve suffering, and of course protecting our beautiful planet by using only natural substances. All Bodhi products come from nature, and are organic”\xa0 Julie Herbison  Bodhi originated as Body Mind Balancing which started off as my business as a Natural Health Practitioner. Over time my products became more and more popular so the name Bodhi came about to keep it short and simple. Bodhi comes from the Bodhi Tree which is a type of fig tree. The Bodhi tree has played an important part in human history featuring strongly in religious history and mythology in various parts of the world.  The ficus religiosa, Bodhi tree, has large heart shaped leaves. It is one of the most sacred trees in India, Sri Lanka and Nepal, where it is venerated by both Hindus and Buddhists. The tree under which Siddhartha Gautama gained enlightenment over 2,600 years ago still grows today in North East India.  In Hindu religion, Vishnu is said to have been born under a Bodhi tree and is often depicted sitting on its heart-shaped leaves. The Bodhi tree is often planted in the grounds of temples and, of all the sacred trees of India, it is the most widely worshiped.  The Bodhi tree in Sri Lanka is located in Anuradhapura and is said to be the oldest tree in the world with a known planting date. This fig tree is said to have grown from a branch taken from the original Bodhi tree in India under which Buddha gained enlightenment.  For Homeopathic and Body Mind consultations please call  Julie Herbison  Bodhi Tree, Bodh Gaya - Sacred SitesBodhi Tree, Bodh Gaya  Bodhi Tree, Bodh Gaya  Buddhist Monks at Bodhi Tree (The site of Buddha\'s enlightenment) \xa0 \xa0\xa0  Bodh Gaya, located 100 km (62 mi) south of Patna in the Indian state of Bihar, is the most venerated sacred place in Buddhism. It is the place where Prince Siddhartha Guatama, while meditating beneath the Bodhi Tree, attained enlightenment and became the Buddha.  Traditional accounts say that, in the early years of the 4th century BC, Siddhartha Gautama saw the suffering of the world and wanted to be free from it. As a young man, following the ancient traditions of Hinduism, he sought out spiritual teachers. Inquiring of their knowledge, he diligently practiced various yogas and meditations. Seven years passed, the last three in extreme asceticism, yet still he had not achieved his goal of enlightenment.  Impression of Buddha feet, Bodh Gaya \xa0\xa0\xa0  Siddhartha then journeyed toward the ancient sacred forests of Uruvela (modern Gaya in Bihar, in north India) with the intention of finally and completely realizing the infinite. Guided by visionary dreams and following in the footsteps of the Buddhas of three previous ages, Krakucchanda, Kanakamuni and Kasyapa (who had each attained enlightenment at the site) Siddhartha sat beneath the Bodhi Tree. Touching the earth, thereby calling it to witness the countless lifetimes of virtue that had led him to this place of enlightenment, he resolved not to rise again until enlightenment was attained.  "Here on this seat my body may shrivel up, my skin, my bones, my flesh may dissolve, but my body will not move from this seat until I have attained Enlightenment, so difficult to obtain in the course of great periods of time".  As Siddhartha sat in deep meditation beneath the Bodhi Tree, Mara, the Dark Lord of Death, came to distract him from his endeavor. When the earth shook, confirming the truth of Gautama\'s words, Mara unleashed his army of demons. In the epic battle that ensued, Siddhartha\'s wisdom broke through Mara’s illusions. The power of his compassion transformed the demons\' weapons into flowers and Mara and all his forces fled. Three days and nights passed and Siddhartha’s intention was realized. He became the Buddha, meaning the ‘Enlightened One’.  The Mahabodhi Temple, Bodh Gaya, India \xa0 \xa0\xa0  The Buddha then spent the | Question: Who is said to have gained enlightenment sitting under the Bodhi Tree?', ['buddha', 'enlightened one'], 'mrqa_triviaqa-validation-3915'), ('Context: Br\'er RabbitBr\'er Rabbit, also spelled Bre\'r Rabbit or Brer Rabbit or Bruh Rabbit, is a central figure as Uncle Remus tells stories of the Southern United States. Br\'er Rabbit is a trickster who succeeds by his wits rather than by brawn, provoking authority figures and bending social mores as he sees fit. The Walt Disney Company later adapted this character for its 1946 animated motion picture Song of the South.  Tar-Baby story   In one tale, Br\'er Fox constructs a doll out of a lump of tar and dresses it with some clothes. When Br\'er Rabbit comes along he addresses the Tar-Baby amiably, but receives no response. Br\'er Rabbit becomes offended by what he perceives as the Tar-Baby\'s lack of manners, punches it, and in doing so becomes stuck. The more Br\'er Rabbit punches and kicks the tar "baby" out of rage, the more he gets stuck. When Br\'er Fox reveals himself, the helpless but cunning Br\'er Rabbit pleads, "please, Br\'er Fox, don\'t fling me in dat brier-patch," prompting Fox to do exactly that. As rabbits are at home in thickets, the resourceful Br\'er Rabbit uses the thorns and briers to escape. The story was originally published in Harper\'s Weekly by Robert Roosevelt; years later Joel Chandler Harris included his version of the tale in his Uncle Remus stories.  African origins   The Br\'er Rabbit stories can be traced back to trickster figures in Africa, particularly the hare that figures prominently in the storytelling traditions in West, Central, and Southern Africa. These tales continue to be part of the traditional folklore of numerous peoples throughout those regions. In the Akan traditions of West Africa, the trickster is usually the spider Anansi, though the plots in his tales are often identical with those of stories of Br\'er Rabbit.  However, Anansi does encounter a tricky rabbit called "Adanko" (Asante-Twi to mean "Hare") in some stories. The Jamaican character with the same name "Brer Rabbit", is an adaptation of the Ananse stories of the Akan people.   Some scholars have suggested that in his American incarnation, Br\'er Rabbit represented the enslaved Africans who used their wits to overcome adversity and to exact revenge on their adversaries, the White slave-owners.  Though not always successful, the efforts of Br\'er Rabbit made him a folk hero. However, the trickster is a multi-dimensional character. While he can be a hero, his amoral nature and his lack of any positive restraint can make him into a villain as well.   For both Africans and African Americans, the animal trickster represents an extreme form of behavior that people may be forced to adopt in extreme circumstances in order to survive. The trickster is not to be admired in every situation. He is an example of what to do, but also an example of what not to do. The trickster\'s behavior can be summed up in the common African proverb: "It\'s trouble that makes the monkey chew on hot peppers." In other words, sometimes people must use extreme measures in extreme circumstances.  Several elements in the Brer Rabbit Tar Baby story (e.g., rabbit needing to be taught a lesson, punches and head-butting the rabbit does, the stuck rabbit being swung around and around) are reminiscent of those found in a Zimbabwe-Botswana folktale.   Folklorists in the late 19th century first documented evidence that the American versions of the stories originated among enslaved West Africans based on connections between Br\'er Rabbit and Leuk, a rabbit trickster in Senegalese folklore.  The stories of Br\'er Rabbit were written down by Robert Roosevelt, an uncle of US President Theodore Roosevelt. Theodore Roosevelt wrote in his autobiography about his aunt from the State of Georgia, that "She knew all the \'Br\'er Rabbit\' stories, and I was brought up on them. One of my uncles, Robert Roosevelt, was much struck with them, and took them down from her dictation, publishing them in Harper\'s, where they fell flat. This was a good many years before a genius arose who, in \'Uncle Remus\', made the stories | Question: Who gets Brer Rabbit in a terrible tangle in the stories by Joel Chandler Harris?', ['Tar Baby', 'Tar-Baby', 'tar baby', 'tar "baby'], 'mrqa_triviaqa-validation-1616'), ('Context: An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses . The same principle is used for funicular railways with two connected railway cars on inclined tracks , and for the elevators on the Eiffel Tower which counterbalance each other . Ski lifts are another example , where the gondolas move on a closed ( continuous ) pulley system up and down the mountain . The ski lift is similar to the counter-weighted elevator , but with a constraining force provided by the cable in the vertical dimension thereby achieving work in both the horizontal and vertical dimensions . Boat lifts are another type of counter-weighted elevator system approximating an Atwood machine . | Question: what is a real world application of an atwood machine ?', ['An elevator with a counterbalance'], 'mrqa_naturalquestions-validation-2730'), ('Context: TurbineA turbine (from the Latin turbo, a vortex, related to the Greek , tyrbē, meaning "turbulence"),   is a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work. A turbine is a turbomachine with at least one moving part called a rotor assembly, which is a shaft or drum with blades attached. Moving fluid acts on the blades so that they move and impart rotational energy to the rotor. Early turbine examples are windmills and waterwheels.  Gas, steam, and water turbines have a casing around the blades that contains and controls the working fluid. Credit for invention of the steam turbine is given both to the British engineer Sir Charles Parsons (1854–1931), for invention of the reaction turbine and to Swedish engineer Gustaf de Laval (1845–1913), for invention of the impulse turbine. Modern steam turbines frequently employ both reaction and impulse in the same unit, typically varying the degree of reaction and impulse from the blade root to its periphery.  The word "turbine" was coined in 1822 by the French mining engineer Claude Burdin from the Latin turbo, or vortex, in a memo, "Des turbines hydrauliques ou machines rotatoires à grande vitesse", which he submitted to the Académie royale des sciences in Paris.  Benoit Fourneyron, a former student of Claude Burdin, built the first practical water turbine.   Operation theory   A working fluid contains potential energy (pressure head) and kinetic energy (velocity head). The fluid may be compressible or incompressible. Several physical principles are employed by turbines to collect this energy:  Impulse turbines change the direction of flow of a high velocity fluid or gas jet. The resulting impulse spins the turbine and leaves the fluid flow with diminished kinetic energy. There is no pressure change of the fluid or gas in the turbine blades (the moving blades), as in the case of a steam or gas turbine, all the pressure drop takes place in the stationary blades (the nozzles). Before reaching the turbine, the fluid\'s pressure head is changed to velocity head by accelerating the fluid with a nozzle.  Pelton wheels and de Laval turbines use this process exclusively. Impulse turbines do not require a pressure casement around the rotor since the fluid jet is created by the nozzle prior to reaching the blades on the rotor. Newton\'s second law describes the transfer of energy for impulse turbines.  Reaction turbines develop torque by reacting to the gas or fluid\'s pressure or mass.  The pressure of the gas or fluid changes as it passes through the turbine rotor blades. A pressure casement is needed to contain the working fluid as it acts on the turbine stage(s) or the turbine must be fully immersed in the fluid flow (such as with wind turbines). The casing contains and directs the working fluid and, for water turbines, maintains the suction imparted by the draft tube. Francis turbines and most steam turbines use this concept. For compressible working fluids, multiple turbine stages are usually used to harness the expanding gas efficiently. Newton\'s third law describes the transfer of energy for reaction turbines.  In the case of steam turbines, such as would be used for marine applications or for land-based electricity generation, a Parsons type reaction turbine would require approximately double the number of blade rows as a de Laval type impulse turbine, for the same degree of thermal energy conversion. Whilst this makes the Parsons turbine much longer and heavier, the overall efficiency of a reaction turbine is slightly higher than the equivalent impulse turbine for the same thermal energy conversion.  In practice, modern turbine designs use both reaction and impulse concepts to varying degrees whenever possible. Wind turbines use an airfoil to generate a reaction lift from the moving fluid and impart it to the rotor. Wind turbines also gain some energy from the impulse of the wind, by deflecting it at an angle. Turbines with multiple stages may utilize either reaction or impulse blading at high pressure. Steam turbines were traditionally more impulse but continue to move towards reaction designs similar to those used in gas turbines. At low pressure the operating fluid medium expands in volume for small reductions in | Question: What main category of machines \'produce continuous power from a wheel/rotor, usually with vanes, revolved by fast flowing fluid\'?', ['turbine engines', 'reaction turbine', 'turbines', 'turbine', 'turbine blades', 'impulse turbine'], 'mrqa_triviaqa-validation-2376'), ('Context: Important production centres today are the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ) , the Yakima ( Washington ) and Willamette ( Oregon ) valleys , and western Canyon County , Idaho ( including the communities of Parma , Wilder , Greenleaf , and Notus ) . The principal production centres in the UK are in Kent ( which produces Kent Goldings hops ) , Herefordshire , and Worcestershire . Essentially all of the harvested hops are used in beer making . | Question: where do they grow hops in the us ?', ['Washington', 'Oregon', 'Idaho'], 'mrqa_naturalquestions-validation-2248'), ("Context: Steve Carell as Felonious Gru , the former villain turned Anti-Villain League agent , Margo , Edith , and Agnes ' adoptive father , and Lucy 's husband .   Carell also voices Dru , Gru 's long - lost twin brother and the girls ' adoptive uncle .     Kristen Wiig as Lucy Wilde , an Anti-Villain League agent , Gru 's wife and the girls ' adoptive mother .   Trey Parker as Balthazar Bratt , a supervillain and former child star who grows up to become obsessed with the character he played in the ' 80s and is bent on world domination .   Miranda Cosgrove as Margo , Gru and Lucy 's oldest adoptive daughter .   Dana Gaier as Edith , Gru and Lucy 's middle adoptive daughter .   Nev Scharrel as Agnes , Gru and Lucy 's youngest adoptive daughter . She was previously voiced by Elsie Fisher in the first two films .   Pierre Coffin as Mel and the Minions .   Coffin also voices a Museum Director .     Steve Coogan as Silas Ramsbottom , the director of the Anti-Villain League .   Coogan also voices Fritz , the butler of Dru .     Julie Andrews as Marlena Gru , Gru and Dru 's mother .   Jenny Slate as Valerie Da Vinci , a member of the Anti-Villain League who becomes the brand new AVL director .   Andy Nyman as Clive , a robot who is Bratt 's sidekick .   Adrian Ciscato as Niko , a boy from Freedonia who falls in love with Margo .   John Cygan ( in his final film role ) as an Additional Voice . | Question: who is the cast of despicable me 3 ?", ['Nev Scharrel', 'Andy Nyman', 'Adrian Ciscato', 'Pierre Coffin', 'Steve Coogan', 'Trey Parker', 'Julie Andrews', 'Kristen Wiig', 'John Cygan', 'Jenny Slate', 'Dana Gaier', 'Steve Carell', 'Miranda Cosgrove'], 'mrqa_naturalquestions-validation-3490'), ('Context: Great Train Robbery Anniversary - Heart Beds, Bucks ...Great Train Robbery Anniversary - Heart Beds, Bucks & Herts News  Great Train Robbery Anniversary  Great Train Robbery Anniversary  See pictures of the bridge near Leighton Buzzard where The Great Train Robbery took place fifty years ago.  See all photos (15) in this gallery  By John Stratford (@Heart_JohnS), 8th August 2013, 10:50  Fifty years after The Great Train Robbery, an event\'s taken place to praise police who searched for the men who robbed a mail train in Buckinghamshire.  You may also like  On 8 August 1963, a gang of robbers, masterminded by Bruce Reynolds, stopped the Glasgow-Euston overnight mail train as it passed close to Cheddington, near Leighton Buzzard.  On board were huge numbers of used bank notes.  Twelve of the robbers were jailed for a total of more than 300 years but more than one broke out of prison, including notorious criminal Ronnie Biggs, who spent over 30 years on the run before he finally returned to Britain in 2001 to face arrest.  Reynolds returned in 1968, five years after the crime, and was captured in Torquay and jailed for 25 years.  Two Buckinghamshire Constabulary police officers who were involved in the investigation attended a police commemoration event alongside serving Thames Valley Police officers at Eynsham Hall in Witney, Oxfordshire, on Wednesday 7 August 2013.  Keith Milner was a detective at Aylesbury at the time of the robbery, while John Woolley was a PC and discovered Leatherslade Farm near Brill, Buckinghamshire, where the men hid after committing the crime.  Biggs insisted in July 2013 that he was proud to have been part of the gang.  The famous fugitive, who celebrates his 84th birthday on 8 August 2013, escaped from prison in 1965 and spent 36 years on the run before finally being arrested and jailed in 2001.  Released from prison on compassionate grounds in 2009 due to ill health he is still alive, being cared for in a north London nursing home, and he has few regrets about the crime that made him a household name.  Biggs, who cannot speak and communicates through a spelling board, said: "If you want to ask me if I have any regrets about being one of the train robbers, my answer is, \'No!\'.  "I will go further: I am proud to have been one of them. I am equally happy to be described as the \'tea-boy\' or \'The Brain\'.  "I was there that August night and that is what counts. I am one of the few witnesses - living or dead - to what was \'The Crime of the Century\'.\'\'  But although he is proud to have been involved in the headline-grabbing crime, he admitted he does have some regrets.  "It is regrettable, as I have said many times, that the train driver was injured,\'\' he said. "And he was not the only victim.  "The people who paid the heaviest price for the Great Train Robbery are the families. The families of everyone involved in the Great Train Robbery, and from both sides of the track.  "All have paid a price for our collective involvement in the robbery. A very heavy price, in the case of my family.  "For that, I do have my regrets.\'\'  Ronnie\'s son Michael, who was born while Ronnie was on the run, spoke exclusively to Heart and said he isn\'t proud of his dad being a criminal, but he is proud of the man his dad is.  "I believe if you commit a crime, you have to do the time," he said, "but the time has to be proportional with the crime that you\'ve committed.  "The sad bit was that he went into the train robbery because he needed £500 to put a deposit on his house. About a week before the robbery, he won the £500 in a bet on the horses but there was no turning back then."  Listen to Michael Biggs | Question: What criminal offence took place in Cheddington, Buckinghamshire in August (8th) 1963?', ['great train robbery'], 'mrqa_triviaqa-validation-7369'), ("Context: Prague: Six things you must do in the Czech capital city ...Prague: Six things you must do in the Czech capital city | Daily Mail Online  Prague: Six things you must do in the Czech capital city  comments  Prague is one of Europe's architectural gems, with more knockout picture-postcard views than some entire countries. Unsurprisingly, the Czech capital can also get busy as thousands of tourists jostle for space.  Here is a selection of things to do during a visit to the city - from taking in its best-loved sights to stopping for a drink at a beautifully restored cafe and watching a comic take on a famous Mozart opera.  1... New Jerusalem  Emperor Charles IV planned this intensely beautiful city as the ‘New Jerusalem’ in the 14th Century, and architects maintained the vision until its last golden age in the Thirties. The old centre, with outstanding buildings such as Valdstejn Palace, St James’ Church and St Vitus Cathedral, is still largely intact after 800 years and is now a World Heritage Site.  With nifty footwork you can outwit the crowds. Try walking the famous Charles Bridge before breakfast, and save the striking of the Astronomical Clock in the Old Town Square until the evening when it’s quieter. I also like the peacefulness of Mala Strana, the cafe-lined cobbled streets under Prague Castle. And take a tram to the Jewish Quarter and wander Petrin Hill for wonderful views over the city’s baroque roofs.  Atmospheric: Prague's beautiful Charles Bridge  2... Grand re-opening  How about this for a comeback? The Grand Cafe Orient ( www.grandcafeorient.cz ) opened in 1912 in the House of the Black Madonna, Prague’s architectural take on cubism, the art form Picasso championed. The cafe closed eight years later when cubism fell out of favour but reopened in 2005, faithfully recreated with period furniture and fittings. Try the apple strudel with vanilla ice cream.  Another venue, the Mysak pastry shop, reopened in 2008 inside the Mysak Gallery ( www.gallerymysak.cz ), with original doors, mosaics floors, a marble staircase and wooden alcoves. The signature dish is karamelovy pohar – ice cream topped with caramel, chocolate and walnuts.  RELATED ARTICLES  Share  3...Fighting for freedom  After the euphoria of spring 1968, and Czechoslovakia’s short-lived break for freedom from the Soviet Union, came deep gloom that summer when Russian tanks rolled in to consign this proud nation to 20 more years of oppression.  The Museum of Communism ( www.muzeumkomunismu.cz ) tells the story of those lost years. I came closest to Prague’s tragedy and eventual triumph on the steps of the National Museum in Wenceslas Square. I looked down on the bronze cross at the spot where Jan Palach burnt himself to death in protest in 1969. It's also where people celebrated their new-found freedom after 1989’s so-called Velvet Revolution.  Historic: The Duke of Bohemia is represented by an equestrian statue in Wenceslas Square  4...Recovered treasures  In 1989, William Lobkowicz returned to Prague from exile in Boston to restore the great estate his family had built up over 700 years. It had been confiscated first by the Nazis, then by the communists. Diligent legal teams gradually recovered land, thousands of books, paintings and pieces of furniture. William’s mission culminated in the opening of Lobkowicz Palace in 2007, with its exhibition of the best of the family’s reclaimed treasures. Star exhibits include two views of the River Thames by Canaletto and Haymaking by Pieter Bruegel the Elder.  Another of the city’s new attractions is totally different. The Kafka Museum ( www.kafkamuseum.cz ) celebrates the writer (he was born here) whose name stands for the way the ordinary man is defeated by ‘the system’.  5...Pulling the strings  With his mischievous sense of fun, Mozart would have approved of the National Marionette Theatre’s comic performances of his opera Don Giovanni ( www.mozart.cz ). I particularly loved the puppet cat hissing at an anguished diva in mid aria. Don Giovanni is Prague’s top claim to cultural fame – in 1787, Mozart staged its world premiere at the exquisite opera | Question: By what name do Czechs know their capital city, Prague?", ['prague'], 'mrqa_triviaqa-validation-6351'), ('Context: 67th Tony Awards  The 67th Annual Tony Awards were held June 9, 2013, to recognize achievement in Broadway productions during the 2012–13 season.  The ceremony returned to Radio City Music Hall in New York City, after two years at Beacon Theatre, and was broadcast live on CBS television.  Neil Patrick Harris hosted for the third consecutive year, his fourth time as host.  Awards in four of the eight acting categories, (Best Actress in a Play, Best Actor in a Musical, Best Actress in a Musical, Best Featured Actor in a Play) were given to African-American performers.  Furthermore, it is the second time in Tony history that both directing prizes went to women.  Garry Hynes and Julie Taymor had previously won in 1998. " Kinky Boots" had a season best 13 nominations and 6 awards.  Cyndi Lauper, who wrote the score for "Kinky Boots", is the first solo female winner for Best Original Score.   67th Tony Awards  The 67th Annual Tony Awards were held June 9, 2013, to recognize achievement in Broadway productions during the 2012–13 season.  The ceremony returned to Radio City Music Hall in New York City, after two years at Beacon Theatre, and was broadcast live on CBS television.  Neil Patrick Harris hosted for the third consecutive year, his fourth time as host.  Awards in four of the eight acting categories, (Best Actress in a Play, Best Actor in a Musical, Best Actress in a Musical, Best Featured Actor in a Play) were given to African-American performers.  Furthermore, it is the second time in Tony history that both directing prizes went to women.  Garry Hynes and Julie Taymor had previously won in 1998. " Kinky Boots" had a season best 13 nominations and 6 awards.  Cyndi Lauper, who wrote the score for "Kinky Boots", is the first solo female winner for Best Original Score.   Julie Taymor  Julie Taymor (born December 15, 1952) is an American director of theater, opera and film.  She is best known for directing the stage musical "The Lion King", for which she became the first woman to win the Tony Award for directing a musical, in addition to a Tony Award for Original Costume Design.   Julie Taymor  Julie Taymor (born December 15, 1952) is an American director of theater, opera and film.  She is best known for directing the stage musical "The Lion King", for which she became the first woman to win the Tony Award for directing a musical, in addition to a Tony Award for Original Costume Design. | Question: In what year did the director of The Lion King win the Tony Awards?', ['1998'], 'mrqa_hotpotqa-validation-3241'), ('Context: That Was the Week That WasThat Was the Week That Was, informally TWTWTW or TW3, was a satirical television comedy programme on BBC Television in 1962 and 1963. It was devised, produced and directed by Ned Sherrin and presented by David Frost. An American version by the same name aired on NBC from 1964 to 1965, also featuring Frost.  The programme is considered a significant element of the satire boom in the UK in the early 1960s. It broke ground in comedy through lampooning the establishment and political figures. Its broadcast coincided with coverage of the politically charged Profumo affair and John Profumo, the politician at the centre of the affair, became a target for derision. TW3 was first broadcast on Saturday, 24 November 1962.  Cast and writers  Cast members included cartoonist Timothy Birdsall, political commentator Bernard Levin, and actors Lance Percival, who sang topical calypsos, many improvised to suggestions from the audience, Kenneth Cope, Roy Kinnear, Willie Rushton, Al Mancini, Robert Lang,  David Kernan and Millicent Martin. The last two were also singers and the programme opened with a song\xa0– That Was The Week That Was\xa0– sung by Martin to Ron Grainer\'s theme tune and enumerating topics in the news. Frankie Howerd also guested with stand-up comedy.   Script-writers included John Albery, John Antrobus, John Betjeman, John Bird, Graham Chapman, John Cleese, Peter Cook, Roald Dahl, Richard Ingrams, Lyndon Irving, Gerald Kaufman, Frank Muir, David Nobbs, Denis Norden, Bill Oddie, Dennis Potter, Eric Sykes, Kenneth Tynan, and Keith Waterhouse.   Programme  The programme opened with a song ("That was the week that was, It\'s over, let it go ...") sung by Millicent Martin, referring to news of the week just gone. Lance Percival sang a topical calypso each week. Satirical targets, such as Prime Minister Harold Macmillan and Home Secretary Henry Brooke were lampooned in sketches, debates and monologues. Other targets were the monarchy, Britain\'s declining status as a global power, racism (particularly in the American South and South Africa under Apartheid), sexual and social hypocrisy, the class system, and the BBC itself. Well-remembered sketches include a \'consumers\' guide to religion\', which discussed relative merits of faiths in the manner of a Which? magazine report.  On Saturday, 20 October 1962 the award of Nobel prizes to John Kendrew and Max Perutz, and to Francis Crick, James D. Watson, and Maurice Wilkins was satirised in a short sketch with the prizes referred to as the Alfred Nobel Peace Pools; in this sketch Watson was called "Little J.D. Watson" and "Who\'d have thought he\'d ever get the Nobel Prize? Makes you think, doesn\'t it". The germ of the joke was that Watson was only 25 when he helped discover DNA; much younger than the others.  TW3 was broadcast on Saturday night and attracted an audience of 12 million. It often under- or overran as cast and crew worked through material as they saw fit. At the beginning of the second season in the autumn of 1963, in an attempt to assert control over the programme, the BBC scheduled repeats of The Third Man television series after the end of TW3. Frost suggested a means of sabotaging this tactic to Sherrin, and he agreed. For three weeks, Frost read out the plot of The Third Man, until the repeats were abandoned following the direct intervention of the BBCs Director General Hugh Greene.   Frost often ended a satirical attack with the remark "But seriously, he\'s doing a grand job".  At the end of each episode, Frost usually signed off with: "That was the week, that was." At the end of the final programme he announced: "That was That Was The Week That Was...that was."  Kennedy tribute  For the edition on Saturday, 23 November 1963, the day after the assassination of United States President John F. Kennedy, TW3 produced a shortened 20-minute programme with no satire, reflecting on the loss, including a contribution from Dame | Question: Who was the deviser, producer and director \'That Was The Week That Was\'?', ['ned sherrin'], 'mrqa_triviaqa-validation-4054'), ('Context: The rainforest contains several species that can pose a hazard. Among the largest predatory creatures are the black caiman, jaguar, cougar, and anaconda. In the river, electric eels can produce an electric shock that can stun or kill, while piranha are known to bite and injure humans. Various species of poison dart frogs secrete lipophilic alkaloid toxins through their flesh. There are also numerous parasites and disease vectors. Vampire bats dwell in the rainforest and can spread the rabies virus. Malaria, yellow fever and Dengue fever can also be contracted in the Amazon region. | Question: What fish living in the Amazon river is known to bit humans?', ['piranha'], 'mrqa_squad-validation-4185'), ('Context: Amphitrite in Greek Mythology | MythographyAmphitrite in Greek Mythology | Mythography  Amphitrite in Greek Mythology  Home | Greek Gods | Free Spirits | Amphitrite  In Greek mythology, Amphitrite was a sea goddess. Sources state that she was the daughter of Nereus and Doris, which makes Amphitrite one of the fifty sea goddesses (or nymphs) who are known collectively as the Nereids. The Greek poet Hesiod lists the names of these sea nymphs in his Theogony:  “To Nereus and Doris of the lovely hair,  the daughter of Okeanos, the stream surrounding the earth,  a host of godly daughters was born in the barren sea:  Proto, Eukrante, Amphitrite, and Sao…”  (Hesiod, Theogony, lines 240-43)  According to the legend, the Olympian Poseidon (who was himself a god of the sea) saw Amphitrite one day and fell in love with the graceful and beautiful goddess. However, the sea nymph initially resisted Poseidon’s advances. But Poseidon was not easily discouraged. He sent his sea creatures to find Amphitrite. A dolphin succeeded in locating the Nereid, and it was this creature - the dolphin - who persuaded her to consider Poseidon’s proposal. Eventually, Amphitrite accepted Poseidon, and the two gods were married.  Amphitrite and Poseidon were together the parents of several children, including the sea god Triton. Here’s Hesiod again:  “From the union of rumbling Poseidon and Amphitrite  came the great Triton, whose might is far-flung,  an awesome god dwelling in a golden house that lies  at the sea’s bottom, near his cherished mother and lordly father.”  (Hesiod, Theogony, lines 930 ff. )  Amphitrite Goddess of the Sea - Greek Gods and GoddessesAmphitrite Goddess of the Sea  [ ? ]Subscribe To This Site  Amphitrite Goddess of the Sea  In Greek mythology, Amphitrite goddess of the Sea was one of the fifty Nereids, that is a daughter of Nereus and Doris - that\'s what Hesiod writes in his Theogony (but Apollodorus says her parents were Oceanus and Thetys). Her Roman equivalent was Salacia, god Neptune\'s wife.  Her name means "The third one who encircles (everything)" (because the ancient Greeks thought the whole world was encircled by a river-god, Oceanus ).  She spent most of the time singing and dancing with her sisters. One day Poseidon, god of the sea , saw her near the island of Naxos and fell in love with her. He kidnapped her and made her his wife.  In another story, Amphitrite, being a shy girl, fled away and hid somewhere in the Atlantic Ocean. Poseidon wanted desperately to find her, so he sent all the marine creatures to look for her. Delphinus, a dolphin, managed to find her and convinced her to go back to Poseidon and marry him. That\'s what made Amphitrite goddess of the sea. (By the way, the dolphin who found her was turned into a constellation by the grateful Poseidon ).  Amphitrite goddess of the sea has no special attributions (but sometimes she can calm the waters), she is just the mistress of the sea (and sometimes she is considered the mother of the marine creatures, but they existed even before she became queen of the sea). Some even say that she is just a personification of the sea.  She also appeared briefly in the story of Theseus. King Minos asked the hero to prove that he really was Poseidon\'s son. A ring was thrown into the sea and Theseus had to bring it back. The sea creatures found the ring and gave it to the hero, while Amphitrite goddess of the sea received him in her underwater palace and gave him a golden crown, which he took to Ariadne as a wedding gift. (As you can see, Amphitrite was the ideal wife, as she was never jealous of her husband\'s love affairs.)  Amphitrite and Poseidon had a son, Triton (a fish-man or a he-mermaid) and a daughter, Rhode, and maybe another daughter, Benthesicyme.  In art | Question: In Greek mythology, Amphitrite, queen of the sea, was the wife of which god?', ['poseidon'], 'mrqa_triviaqa-validation-671'), ("Context: May 14 : The Corps of Discovery departs from Camp Dubois at 4 p.m. , marking the beginning of the voyage to the Pacific coast .   May 16 : The Corps of Discovery arrives at St. Charles , Missouri .   May 21 : Departure from St. Charles at 3 : 30 p.m.   May 24 : Pass Boones Settlement . Home of famous woodsman L. Willenborg .   May 25 : The expedition passes the small village of La Charrette on the Missouri River . Charles Floyd writes in his journal that this is `` the last settlement of whites on this river '' .   June 1 : The expedition reaches the Osage River .   June 12 : Lewis and Clark meet three trappers in two pirogues . One of the men was Pierre Dorion , Jr. -- who knew George Rogers Clark . Lewis and Clark persuade Dorion to return to Sioux camp to act as interpreter .   June 26 : The expedition arrives at Kaw Point where the Kansas River drains into the Missouri River basin .   June 28 -- 29 : First trial in new territory . Pvt . John Collins is on guard duty and breaks into the supplies and gets drunk . Collins invites Pvt . Hugh Hall to drink also . Collins receives 100 lashes , Hall receives 50 lashes .   July 4 : Marking Independence Day , the expedition names Independence Creek located near Atchison , Kansas .   July 11 -- 12 : Second trial in new territory . Pvt . Alexander Hamilton Willard is on guard duty . Is charged with lying down and sleeping at his post whilst a sentinel . Punishable by death . He receives 100 lashes for four straight days .   July 21 : Reaches the Platte River , 640 miles from St. Louis . Entering Sioux Territory .   August 1 : Captain William Clark 's 34th birthday .   August 3 : The Corps of Discovery holds the first official council between representatives of the United States and the Oto and Missouri tribes at Council Bluffs , Iowa . They hand out peace medals , 15 - star flags and other gifts , parade men and show off technology .   August 4 : Moses Reed said he was returning to a previous camp to retrieve a knife but deserted to St. Louis .   August 18 : George Drouillard returns to camp with Reed and Otos ' Chief Little Thief . Reed is sentenced to run the gauntlet ( 500 lashes ) and is discharged from the permanent party .   August 18 : Captain Meriwether Lewis 's 30th birthday .   August 20 : Sergeant Charles Floyd dies . He dies from bilious chorlick ( ruptured appendix ) . He is the only member lost during the expedition .   August 23 : Pvt . Joseph Field kills first bison .   August 26 : Pvt . Patrick Gass is elected to sergeant . First election in new territory west of Mississippi River . George Shannon is selected to get the horses back from native Americans .   August 30 : A friendly council with the Yankton Sioux held . According to a legend , Lewis wraps a newborn baby in a United States flag and declares him `` an American '' .   September 4 : Reach the mouth of the Niobrara River .   September 7 : The expedition drives a prairie dog out of its den ( by pouring water into it ) to send back to Jefferson .   September 14 : Hunters kill and describe prairie goat ( antelope ) .   September 25 -- 29 : A band of Lakota Sioux demand one of the boats as a toll for moving further upriver . Meet with Teton Sioux . Close order drill , air gun demo , gifts of medals , military coat , hats , tobacco . Hard to communicate language problems . Invite chiefs on board keelboat , give each \u200b ⁄ glass whiskey , acted drunk wanted more . Two armed confrontations with Sioux . Some of the chiefs sleep on boat , move up river to another village , meet in lodge , hold scalp dance .   October 8 -- 11 : Pass Grand River home of the Arikara people , 2,000 + . Joseph Gravelins trader , lived with Arikara for 13 yrs . Pierre Antoine Tabeau lived in another village was from Quebec .   October 13 : Pvt . John Newman tried for | Question: on which river did the exploration of the louisiana purchase begin ?", ['the Missouri River'], 'mrqa_naturalquestions-validation-8948'), ('Context: What is a Mulligan in Golf? - About.com SportsWhat is a Mulligan in Golf?  What Is a \'Mulligan\' in Golf?  What Is a \'Mulligan\' in Golf?  This golfer looks like he needed a mulligan.\xa0 John Cumming/Photodisc/Getty Images  By Brent Kelley  Updated February 05, 2016.  A mulligan, most simply put, is a "do-over" in golf. Hit a bad shot? Take a mulligan and replay that stroke . Drop a ball on the spot from which you just played, and re-play. The first (bad) shot is not counted.  Are Mulligans \'Legal\'?  No. There is never a time, when playing under the\xa0 Rules of Golf , that a mulligan is "legal." Mulligans are not allowed under the rules.  But Mulligans Are Very Popular  But just because mulligans aren\'t "legal" doesn\'t mean their use isn\'t common and popular among recreational golfers. Many amateurs and recreational golfers - players just out to have a fun round with buddies, as opposed to serious golfers looking for competition - are lax in observing the rules anyway.  So mulligans are most often employed during friendly rounds by golf buddies; or during charity or playday tournaments where mulligans are sometimes sold. If mulligans are for sale at a charity tournament, that means the golfer can buy, say, three mulligans for a set price each.  continue reading below our video  What Are Mulligans?  The sale of mulligans is sometimes used as an additional fund-raiser at charitable events.  Common Ways of Using Mulligans  Do all golfers use mulligans in the same way? No - whatever a group of golfers agrees upon is what counts (unless you are using mulligans in something like a charity tournament or association outing setting - then do what the organizers tell you).  How many mulligans you get to use during a round, what types of shots you can use them for, and so on, are things that differ from golfer to golfer. So if you\'re playing with a new group and mulligans are in effect, you need to clarify what is allowed.  Here are some common ways that mulligans are used:  Some golfers limit the use of mulligans to the first tee only, or to the first and 10th tees only.  Some golfers use one mulligan per nine holes, but anywhere on each nine.  It\'s most common for mulligans to be used only off the tee , i.e., you can only use a mulligan to replay a drive. However, some groups allow mulligans from the fairway , too.  Less common is allowing mulligans from the rough or out of hazards, but some golfers even do that.  It is rarer still - rarely seen, in fact - for mulligans to be used on the putting green .  And some groups allow mulligans from just about anywhere on the golf course, but set a limit - say, three mulligans per round, or nine, or 18.  Clearly, there are many different ways that golfers use mulligans. If you have a regular group of friends you play with, and your group allows mulligans, you probably long ago settled into your own "rules" about using them.  Note that mulligans are common in the United States, but rarely seen in the UK, for example. As with all localized golf customs, formats and betting games , when playing with golfers you don\'t know clear up the ground rules before play starts to avoid possible confusion later.  And again, please note: If you are playing in a tournament, handicap round or other setting in which the Rules of Golf are strictly followed, you cannot play mulligans.  Why Is It Called a \'Mulligan\'?  Good question! And the fact is, nobody knows for sure how a do-over in golf came to be called a mulligan. There are multiple theories, however, and we go over some of them in our FAQ on the topic:  Why are mulligans called that?  Other Terms/Uses  Mulligans | Question: In which sport may a \'Mulligan\' be awarded?', ['golf', 'golfer'], 'mrqa_triviaqa-validation-5972'), ("Context: The White Coat Ceremony ( WCC ) is a relatively new ritual in some medical ( MD , DO ) , dental , optometry , audiology , chiropractic , dietetic , occupational therapy , physical therapy , podiatric , pharmacy , physician assistant , pathologists ' assistant , nursing , naturopathic and veterinary schools that marks the student 's transition from the study of preclinical to clinical health sciences . At some schools , where students begin meeting patients early in their education , the white coat ceremony is held before the first year begins . It is an example of a matriculation . | Question: when do you get your white coat in pharmacy school ?", ["the student 's transition from the study of preclinical to clinical health sciences"], 'mrqa_naturalquestions-validation-5465'), ("Context: It is an allusion to Samuel Taylor Coleridge 's poem The Rime of the Ancient Mariner ( 1798 ) . In the poem , an albatross starts to follow a ship -- being followed by an albatross was generally considered a sign of good luck . However , the titular mariner shoots the albatross with a crossbow , which is regarded as an act that will curse the ship ( which indeed suffers terrible mishaps ) . Even when they are too thirsty to speak , the ship 's crew let the mariner know through their glances that they blame his action for the curse . The albatross is then literally hung around the mariner 's neck by the crew to symbolize his guilt in killing the bird . Thus , the albatross can be both an omen of good or bad luck , as well as a metaphor for a burden to be carried as penance . | Question: the rime of the ancient mariner albatross symbolism ?", ['a metaphor for a burden to be carried as penance', 'omen of good or bad luck'], 'mrqa_naturalquestions-validation-7017'), ('Context: Concentrated O2 will allow combustion to proceed rapidly and energetically. Steel pipes and storage vessels used to store and transmit both gaseous and liquid oxygen will act as a fuel; and therefore the design and manufacture of O2 systems requires special training to ensure that ignition sources are minimized. The fire that killed the Apollo 1 crew in a launch pad test spread so rapidly because the capsule was pressurized with pure O2 but at slightly more than atmospheric pressure, instead of the 1⁄3 normal pressure that would be used in a mission.[k] | Question: ______ In both liquid and gas form can fastly result in an exlposion. ?', ['oxygen'], 'mrqa_squad-validation-3478'), ('Context: The Super Bowl 50 Host Committee has vowed to be "the most giving Super Bowl ever", and will dedicate 25 percent of all money it raises for philanthropic causes in the Bay Area. The committee created the 50 fund as its philanthropic initiative and focuses on providing grants to aid with youth development, community investment and sustainable environments. | Question: What was the name of the fund setup to help with investing in the community?', ['50 fund', 'the 50 fund'], 'mrqa_squad-validation-392'), ('Context: Newton came to realize that the effects of gravity might be observed in different ways at larger distances. In particular, Newton determined that the acceleration of the Moon around the Earth could be ascribed to the same force of gravity if the acceleration due to gravity decreased as an inverse square law. Further, Newton realized that the acceleration due to gravity is proportional to the mass of the attracting body. Combining these ideas gives a formula that relates the mass () and the radius () of the Earth to the gravitational acceleration: | Question: How might gravity effects be observed differently according to Newton?', ['at larger distances', 'at larger distances.'], 'mrqa_squad-validation-10322'), ('Context: Evolution of the adaptive immune system occurred in an ancestor of the jawed vertebrates. Many of the classical molecules of the adaptive immune system (e.g., immunoglobulins and T cell receptors) exist only in jawed vertebrates. However, a distinct lymphocyte-derived molecule has been discovered in primitive jawless vertebrates, such as the lamprey and hagfish. These animals possess a large array of molecules called Variable lymphocyte receptors (VLRs) that, like the antigen receptors of jawed vertebrates, are produced from only a small number (one or two) of genes. These molecules are believed to bind pathogenic antigens in a similar way to antibodies, and with the same degree of specificity. | Question: Evolution of what part of the immune system occurred in the evolutionary ancestor of jawed vertebrates?', ['adaptive', 'adaptive immune system', 'the adaptive immune system'], 'mrqa_squad-validation-6677'), ('Context: Genghis Khan, the title is spelled in variety of ways in different languages such as Mongolian Chinggis Khaan, English Chinghiz, Chinghis, and Chingiz, Chinese: 成吉思汗; pinyin: Chéngjísī Hán, Turkic: Cengiz Han, Çingiz Xan, Çingiz Han, Chingizxon, Çıñğız Xan, Chengez Khan, Chinggis Khan, Chinggis Xaan, Chingis Khan, Jenghis Khan, Chinggis Qan, Djingis Kahn, Russian: Чингисхан (Čingiskhan) or Чингиз-хан (Čingiz-khan), etc. Temüjin is written in Chinese as simplified Chinese: 铁木真; traditional Chinese: 鐵木眞; pinyin: Tiěmùzhēn. | Question: What are alternate English spelling of Genghis?', ['Chinghiz, Chinghis, and Chingiz'], 'mrqa_squad-validation-6303'), ('Context: Pi Day is an annual celebration of the mathematical constant π ( pi ) . Pi Day is observed on March 14 ( 3 / 14 in the month / day format ) since 3 , 1 , and 4 are the first three significant digits of π . In 2009 , the United States House of Representatives supported the designation of Pi Day . | Question: how long have we been celebrating pi day ?', ['2009'], 'mrqa_naturalquestions-validation-3028'), ('Context: Rumpole of the Bailey - TV.comRumpole of the Bailey - Show News, Reviews, Recaps and Photos - TV.com  Rumpole of the Bailey  EDIT  Welcome to the Rumpole of the Bailey guide at TV.com.  Horace Rumpole (played by the late Leo McKern) is an untidy, ageing London barrister with one glass eye who defends in criminal cases. His clients rarely cut elegant figures. He is fond of red wine, poetry, and fair dealing, and is not looked on as a great success in life by his wife, Hilda (\'She Who Must Be Obeyed\'). Rumpole has had a few triumphs, and the Penge Bungalow murders are often on his mind... Rumpole shares Chambers at Number 3, Equity Court, with a mixed group of barrister colleagues, including Guthrie Featherstone (Peter Bowles) and Phyllida Trant (Patricia Hodge). He also takes pupils - notably Fiona Allways (played by Rosalyn Landor) and Liz Probert. The creator and writer of the series, Sir John Mortimer, received an Edgar Allan Poe Award for crime and mystery for Rumpole.  Horace Rumpole - Thrilling DetectiveHorace Rumpole  Horace Rumpole  Created by John Mortimer (1923-2009 )  "Crime doesn\'t pay, but it\'s a living."  Down those mean streets and meaner chambers a man must waddle...  Lord knows, he\'s not a private eye, but God, I wish he were. Like Philip Marlowe , Rumpole is ""a relatively poor man... a common man or he could not go among common people. He has a sense of character, or he would not know his job. He will take no man\'s money dishonestly and no man\'s insolence without a due and dispassionate revenge. He is a lonely man and his pride is that you will treat him as a proud man or be very sorry you ever saw him. He talks as the man of his age talks -- that is, with a rude wit, a lively sense of the grotesque, a disgust for sham, and a contempt for pettiness."  So, may I submit for your consideration, Your Honour....  That great defender of most muddled and sinful humanity...  With his jowls a-quiver...  His fondness for Wordsworth, Chateau Thames Embankment and hopeless cases...  His cheroot-puffing and claret-quaffing...  His food-bespeckled robe and raggedy wig...  And his beloved and tattered copy of The Oxford Book of English Verse clutched to his bosom...  For his oratorical outbursts...  His always entertaining jabs at the soft underbelly of hypocrisy, pomposity and upper class twits...  And for standing up for truth, justice, honour and the Golden Thread of Justice...  May I submit for inclusion, Your Honour, this most British of all lawyers...  This proud, this defiant Old Bailey Hack...  With his best gal, Hilda, She Who Must Be Obeyed, standing, NOT amused, by his side...  The one, the only...  HORACE RUMPOLE.  May there always be an England, and may there always be Horace Rumpole to see that justice be done. And thev pompous may squirm.  Your Honour, I rest my case.  ****  Rumpole is, of course John Mortimer\'s rotund, defiant British criminal lawyer who, as brilliantly brought to life by the late, great Australian actor Leo McKern, the star of Rumpole of the Bailey, the popular British courtroom comedy/drama that originally aired on Thames Television in 1978, and soon became popular on both sides of the Atlantic, appearring on the American Public Broadcasting Service as part of its Mystery! series.  Mortimer wrote each and every episode of the television series, and their subsequent novelizations. The show ran, off and on, for seventeen years, an incredible run, and inspired not just the short stories, but novels and two radio series. It was only in 1995, with the publication of the Rumpole and the Angel of Death collection, that Mortimer began writing original Rumpole stories (ie: not adapted from his own TV scripts). Sice then, two | Question: Who created Rumpole of the Bailey?', ['john mortimer', 'john mortimer qc'], 'mrqa_triviaqa-validation-1551'), ('Context: The Soulages collection of Italian and French Renaissance objects was acquired between 1859 and 1865, and includes several cassone. The John Jones Collection of French 18th-century art and furnishings was left to the museum in 1882, then valued at £250,000. One of the most important pieces in this collection is a marquetry commode by the ébéniste Jean Henri Riesener dated c1780. Other signed pieces of furniture in the collection include a bureau by Jean-François Oeben, a pair of pedestals with inlaid brass work by André Charles Boulle, a commode by Bernard Vanrisamburgh and a work-table by Martin Carlin. Other 18th-century ébénistes represented in the Museum collection include Adam Weisweiler, David Roentgen, Gilles Joubert & Pierre Langlois. In 1901, Sir George Donaldson donated several pieces of art Nouveau furniture to the museum, which he had acquired the previous year at the Paris Exposition Universelle. This was criticized at the time, with the result that the museum ceased to collect contemporary items and did not do so again until the 1960s. In 1986 the Lady Abingdon collection of French Empire furniture was bequeathed by Mrs T. R. P. Hole. | Question: What items comprise the John Jones Collection?', ['art and furnishings', 'French 18th-century art and furnishings'], 'mrqa_squad-validation-5622'), ("Context: Doctor Who has appeared on stage numerous times. In the early 1970s, Trevor Martin played the role in Doctor Who and the Daleks in the Seven Keys to Doomsday. In the late 1980s, Jon Pertwee and Colin Baker both played the Doctor at different times during the run of a play titled Doctor Who – The Ultimate Adventure. For two performances, while Pertwee was ill, David Banks (better known for playing Cybermen) played the Doctor. Other original plays have been staged as amateur productions, with other actors playing the Doctor, while Terry Nation wrote The Curse of the Daleks, a stage play mounted in the late 1960s, but without the Doctor. | Question: What was the name of the Doctor Who play from the 1980's?", ['Doctor Who – The Ultimate Adventure'], 'mrqa_squad-validation-7821')]
09/23/2021 13:37:11 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:37:12 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:37:12 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:37:18 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:37:22 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:37:23 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:37:23 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:37:29 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:37:32 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:37:32 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:37:32 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:37:33 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:37:36 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:37:40 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:37:40 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:37:40 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:37:40 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:37:40 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:37:40 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:37:41 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:37:41 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:37:41 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:37:41 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:37:41 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:37:41 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:37:41 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:37:42 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:37:46 - INFO - __main__ - Finished.
09/23/2021 13:37:46 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:37:46 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:37:46 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:37:46 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:37:46 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:37:47 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:37:47 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:37:47 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:37:52 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:37:52 - INFO - __main__ - Found 32 errors.
09/23/2021 13:37:52 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:37:59 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:37:59 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:38:03 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:38:03 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:38:03 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:38:05 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:38:05 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:38:05 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:38:05 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:38:05 - INFO - __main__ - Finished.
09/23/2021 13:38:05 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:38:05 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:38:05 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:38:09 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:38:09 - INFO - __main__ - Found 26 errors.
09/23/2021 13:38:09 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:38:09 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:38:09 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:38:11 - INFO - __main__ - before_losses=[tensor(1.5589), tensor(1.5005), tensor(2.1458), tensor(1.4888), tensor(1.5770), tensor(1.5163), tensor(1.4772), tensor(2.3435), tensor(2.1019), tensor(1.5249), tensor(2.9214), tensor(1.8106), tensor(2.3314), tensor(1.8749), tensor(1.5362), tensor(1.4525), tensor(1.5706), tensor(1.5381), tensor(1.6300), tensor(1.5215), tensor(1.5599), tensor(1.5514), tensor(1.5321), tensor(1.5203), tensor(1.5773), tensor(2.0851), tensor(2.6389), tensor(1.5650), tensor(1.4551), tensor(1.8541), tensor(1.5547), tensor(1.6852)]
09/23/2021 13:38:11 - INFO - __main__ - after_losses=[tensor(1.5589), tensor(1.5005), tensor(2.1458), tensor(1.4888), tensor(1.5770), tensor(1.5163), tensor(1.4772), tensor(2.3435), tensor(2.1019), tensor(1.5249), tensor(2.9214), tensor(1.8106), tensor(2.3314), tensor(1.8749), tensor(1.5362), tensor(1.4525), tensor(1.5706), tensor(1.5381), tensor(1.6300), tensor(1.5215), tensor(1.5599), tensor(1.5514), tensor(1.5321), tensor(1.5203), tensor(1.5773), tensor(2.0851), tensor(2.6389), tensor(1.5650), tensor(1.4551), tensor(1.8541), tensor(1.5547), tensor(1.6852)]
09/23/2021 13:38:11 - INFO - __main__ - interference_scores=[tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]
09/23/2021 13:38:11 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-7746', 'mrqa_hotpotqa-validation-409', 'mrqa_triviaqa-validation-5937', 'mrqa_naturalquestions-validation-6157', 'mrqa_naturalquestions-validation-9688', 'mrqa_hotpotqa-validation-2970', 'mrqa_hotpotqa-validation-1968', 'mrqa_triviaqa-validation-3915', 'mrqa_triviaqa-validation-1616', 'mrqa_naturalquestions-validation-2730', 'mrqa_triviaqa-validation-2376', 'mrqa_naturalquestions-validation-2248', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-7369', 'mrqa_triviaqa-validation-6351', 'mrqa_hotpotqa-validation-3241', 'mrqa_triviaqa-validation-4054', 'mrqa_squad-validation-4185', 'mrqa_triviaqa-validation-671', 'mrqa_naturalquestions-validation-8948', 'mrqa_triviaqa-validation-5972', 'mrqa_naturalquestions-validation-5465', 'mrqa_naturalquestions-validation-7017', 'mrqa_squad-validation-3478', 'mrqa_squad-validation-392', 'mrqa_squad-validation-10322', 'mrqa_squad-validation-6677', 'mrqa_squad-validation-6303', 'mrqa_naturalquestions-validation-3028', 'mrqa_triviaqa-validation-1551', 'mrqa_squad-validation-5622', 'mrqa_squad-validation-7821']
09/23/2021 13:38:11 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:38:11 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 1
09/23/2021 13:38:24 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:38:24 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 1
09/23/2021 13:38:28 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:38:28 - INFO - __main__ - Instant Fixing Rate: 0.9615384615384616
09/23/2021 13:38:28 - INFO - __main__ - Instant Retention Rate: 0.33333333277777777
09/23/2021 13:38:30 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_002.pt.
09/23/2021 13:38:30 - INFO - __main__ - Saving the current error examples (len=26) to the memory.
09/23/2021 13:38:30 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:38:30 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:38:30 - INFO - __main__ - Finished.
09/23/2021 13:38:30 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:38:30 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:38:30 - INFO - __main__ - Evaluating to get errors .... Timecode: 2
09/23/2021 13:38:33 - INFO - __main__ - Before Error Fixing: 0.09375
09/23/2021 13:38:33 - INFO - __main__ - Found 29 errors.
09/23/2021 13:38:33 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:38:33 - INFO - __main__ - Current memory size: 58.
09/23/2021 13:38:33 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=58;
09/23/2021 13:38:36 - INFO - __main__ - before_losses=[tensor(1.6630), tensor(1.5315), tensor(1.5155), tensor(1.5262), tensor(2.4735), tensor(1.5913), tensor(1.4785), tensor(1.5668), tensor(1.4761), tensor(1.5175), tensor(1.4957), tensor(1.5180), tensor(1.5577), tensor(1.4898), tensor(1.7683), tensor(1.5242), tensor(2.7127), tensor(2.0047), tensor(2.3395), tensor(1.5187), tensor(1.4878), tensor(1.5636), tensor(1.7650), tensor(1.5130), tensor(1.4961), tensor(1.7230), tensor(1.4478), tensor(1.4933), tensor(1.5210), tensor(1.5449), tensor(1.5432), tensor(1.5816), tensor(1.4956), tensor(1.5335), tensor(1.6840), tensor(1.4692), tensor(1.4676), tensor(1.5408), tensor(1.6639), tensor(1.5116), tensor(1.5672), tensor(1.5258), tensor(1.6046), tensor(3.3337), tensor(1.5105), tensor(1.4949), tensor(1.5644), tensor(1.9756), tensor(1.4588), tensor(1.5634), tensor(1.4966), tensor(1.5999), tensor(1.4614), tensor(1.4878), tensor(1.5515), tensor(1.9569), tensor(1.7700), tensor(2.2633)]
09/23/2021 13:38:36 - INFO - __main__ - after_losses=[tensor(1.6630), tensor(1.5315), tensor(1.5155), tensor(1.5262), tensor(2.4735), tensor(1.5913), tensor(1.4785), tensor(1.5668), tensor(1.4761), tensor(1.5175), tensor(1.4957), tensor(1.5180), tensor(1.5577), tensor(1.4898), tensor(1.7683), tensor(1.5242), tensor(2.7127), tensor(2.0047), tensor(2.3395), tensor(1.5187), tensor(1.4878), tensor(1.5636), tensor(1.7650), tensor(1.5130), tensor(1.4961), tensor(1.7230), tensor(1.4478), tensor(1.4933), tensor(1.5210), tensor(1.5449), tensor(1.5432), tensor(1.5816), tensor(1.4956), tensor(1.5335), tensor(1.6840), tensor(1.4692), tensor(1.4676), tensor(1.5408), tensor(1.6639), tensor(1.5116), tensor(1.5672), tensor(1.5258), tensor(1.6046), tensor(3.3337), tensor(1.5105), tensor(1.4949), tensor(1.5644), tensor(1.9756), tensor(1.4588), tensor(1.5634), tensor(1.4966), tensor(1.5999), tensor(1.4614), tensor(1.4878), tensor(1.5515), tensor(1.9569), tensor(1.7700), tensor(2.2633)]
09/23/2021 13:38:36 - INFO - __main__ - interference_scores=[tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]
09/23/2021 13:38:36 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-2210', 'mrqa_squad-validation-4185', 'mrqa_naturalquestions-validation-5465', 'mrqa_naturalquestions-validation-7017', 'mrqa_naturalquestions-validation-3490', 'mrqa_squad-validation-5622', 'mrqa_hotpotqa-validation-5325', 'mrqa_triviaqa-validation-4856', 'mrqa_naturalquestions-validation-8948', 'mrqa_squad-validation-7821', 'mrqa_naturalquestions-validation-1364', 'mrqa_naturalquestions-validation-5144', 'mrqa_triviaqa-validation-5026', 'mrqa_squad-validation-3478', 'mrqa_triviaqa-validation-1551', 'mrqa_naturalquestions-validation-7511', 'mrqa_squad-validation-10015', 'mrqa_triviaqa-validation-5406', 'mrqa_triviaqa-validation-3915', 'mrqa_hotpotqa-validation-5802', 'mrqa_naturalquestions-validation-2753', 'mrqa_triviaqa-validation-2096', 'mrqa_squad-validation-6677', 'mrqa_hotpotqa-validation-14', 'mrqa_naturalquestions-validation-9386', 'mrqa_triviaqa-validation-1924', 'mrqa_naturalquestions-validation-3028', 'mrqa_squad-validation-10410', 'mrqa_triviaqa-validation-671', 'mrqa_naturalquestions-validation-3828', 'mrqa_hotpotqa-validation-2970', 'mrqa_triviaqa-validation-7369']
09/23/2021 13:38:36 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:38:36 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=61) .... Timecode: 2
09/23/2021 13:38:49 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:38:49 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 2
09/23/2021 13:38:51 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 13:38:51 - INFO - __main__ - Instant Fixing Rate: 0.9310344827586207
09/23/2021 13:38:51 - INFO - __main__ - Instant Retention Rate: 0.6666666644444444
09/23/2021 13:39:10 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:39:11 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:39:11 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:39:17 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:39:19 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:39:20 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:39:20 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:39:20 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:39:24 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:39:27 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:39:27 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:39:27 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:39:27 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:39:27 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:39:27 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:39:29 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:39:29 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:39:29 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:39:29 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:39:29 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:39:29 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:39:29 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:39:29 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:39:34 - INFO - __main__ - Finished.
09/23/2021 13:39:34 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:39:34 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:39:34 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:39:34 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:39:34 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:39:36 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:39:36 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:39:36 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:39:40 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:39:40 - INFO - __main__ - Found 32 errors.
09/23/2021 13:39:40 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:39:47 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:39:47 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:39:51 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:39:51 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:39:51 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:39:53 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:39:53 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:39:53 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:39:53 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:39:53 - INFO - __main__ - Finished.
09/23/2021 13:39:53 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:39:53 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:39:53 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:39:56 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:39:56 - INFO - __main__ - Found 26 errors.
09/23/2021 13:39:56 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:39:56 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:39:56 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:40:03 - INFO - __main__ - before_losses=[tensor(1.5589), tensor(1.5005), tensor(2.1458), tensor(1.4888), tensor(1.5770), tensor(1.5163), tensor(1.4772), tensor(2.3435), tensor(2.1019), tensor(1.5249), tensor(2.9214), tensor(1.8106), tensor(2.3314), tensor(1.8749), tensor(1.5362), tensor(1.4525), tensor(1.5706), tensor(1.5381), tensor(1.6300), tensor(1.5215), tensor(1.5599), tensor(1.5514), tensor(1.5321), tensor(1.5203), tensor(1.5773), tensor(2.0851), tensor(2.6389), tensor(1.5650), tensor(1.4551), tensor(1.8541), tensor(1.5547), tensor(1.6852)]
09/23/2021 13:40:03 - INFO - __main__ - after_losses=[tensor(1.5589), tensor(1.5005), tensor(2.1458), tensor(1.4888), tensor(1.5770), tensor(1.5163), tensor(1.4772), tensor(2.3435), tensor(2.1019), tensor(1.5249), tensor(2.9214), tensor(1.8106), tensor(2.3314), tensor(1.8749), tensor(1.5362), tensor(1.4525), tensor(1.5706), tensor(1.5381), tensor(1.6300), tensor(1.5215), tensor(1.5599), tensor(1.5514), tensor(1.5321), tensor(1.5203), tensor(1.5773), tensor(2.0851), tensor(2.6389), tensor(1.5650), tensor(1.4551), tensor(1.8541), tensor(1.5547), tensor(1.6852)]
09/23/2021 13:40:03 - INFO - __main__ - interference_scores=[tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]
09/23/2021 13:40:03 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-7746', 'mrqa_hotpotqa-validation-409', 'mrqa_triviaqa-validation-5937', 'mrqa_naturalquestions-validation-6157', 'mrqa_naturalquestions-validation-9688', 'mrqa_hotpotqa-validation-2970', 'mrqa_hotpotqa-validation-1968', 'mrqa_triviaqa-validation-3915', 'mrqa_triviaqa-validation-1616', 'mrqa_naturalquestions-validation-2730', 'mrqa_triviaqa-validation-2376', 'mrqa_naturalquestions-validation-2248', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-7369', 'mrqa_triviaqa-validation-6351', 'mrqa_hotpotqa-validation-3241', 'mrqa_triviaqa-validation-4054', 'mrqa_squad-validation-4185', 'mrqa_triviaqa-validation-671', 'mrqa_naturalquestions-validation-8948', 'mrqa_triviaqa-validation-5972', 'mrqa_naturalquestions-validation-5465', 'mrqa_naturalquestions-validation-7017', 'mrqa_squad-validation-3478', 'mrqa_squad-validation-392', 'mrqa_squad-validation-10322', 'mrqa_squad-validation-6677', 'mrqa_squad-validation-6303', 'mrqa_naturalquestions-validation-3028', 'mrqa_triviaqa-validation-1551', 'mrqa_squad-validation-5622', 'mrqa_squad-validation-7821']
09/23/2021 13:40:03 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:40:03 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 1
09/23/2021 13:40:16 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:40:16 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 1
09/23/2021 13:40:20 - INFO - __main__ - After Error Fixing: 0.875
09/23/2021 13:40:20 - INFO - __main__ - Instant Fixing Rate: 0.9615384615384616
09/23/2021 13:40:20 - INFO - __main__ - Instant Retention Rate: 0.49999999916666665
09/23/2021 13:40:22 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_002.pt.
09/23/2021 13:40:22 - INFO - __main__ - Saving the current error examples (len=26) to the memory.
09/23/2021 13:40:22 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:40:22 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:40:22 - INFO - __main__ - Finished.
09/23/2021 13:40:22 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:40:22 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:40:22 - INFO - __main__ - Evaluating to get errors .... Timecode: 2
09/23/2021 13:40:25 - INFO - __main__ - Before Error Fixing: 0.09375
09/23/2021 13:40:25 - INFO - __main__ - Found 29 errors.
09/23/2021 13:40:25 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:40:25 - INFO - __main__ - Current memory size: 58.
09/23/2021 13:40:25 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=58;
09/23/2021 13:40:36 - INFO - __main__ - before_losses=[tensor(1.5115), tensor(1.6075), tensor(1.4551), tensor(1.5505), tensor(1.5243), tensor(1.5141), tensor(1.5388), tensor(1.5297), tensor(1.5226), tensor(1.5900), tensor(1.5537), tensor(1.6444), tensor(1.5423), tensor(2.2817), tensor(1.4800), tensor(3.2008), tensor(2.2142), tensor(1.5536), tensor(1.5102), tensor(1.6023), tensor(1.4689), tensor(1.5400), tensor(1.5006), tensor(1.5449), tensor(2.2301), tensor(1.5294), tensor(1.5017), tensor(1.5938), tensor(1.5806), tensor(1.5263), tensor(1.8709), tensor(1.5856), tensor(1.5519), tensor(1.4836), tensor(1.5529), tensor(1.8525), tensor(1.7309), tensor(1.6134), tensor(1.5595), tensor(1.5293), tensor(1.5220), tensor(1.5269), tensor(1.5508), tensor(2.7372), tensor(1.4691), tensor(2.8438), tensor(1.6314), tensor(1.4519), tensor(3.2629), tensor(1.5003), tensor(1.4933), tensor(1.5023), tensor(1.5580), tensor(1.5008), tensor(1.4857), tensor(1.4789), tensor(1.7736), tensor(2.0215)]
09/23/2021 13:40:36 - INFO - __main__ - after_losses=[tensor(1.5115), tensor(1.6075), tensor(1.4551), tensor(1.5505), tensor(1.5243), tensor(1.5141), tensor(1.5388), tensor(1.5297), tensor(1.5226), tensor(1.5900), tensor(1.5537), tensor(1.6444), tensor(1.5423), tensor(2.2817), tensor(1.4800), tensor(3.2008), tensor(2.2142), tensor(1.5536), tensor(1.5102), tensor(1.6023), tensor(1.4689), tensor(1.5400), tensor(1.5006), tensor(1.5449), tensor(2.2301), tensor(1.5294), tensor(1.5017), tensor(1.5938), tensor(1.5806), tensor(1.5263), tensor(1.8709), tensor(1.5856), tensor(1.5519), tensor(1.4836), tensor(1.5529), tensor(1.8525), tensor(1.7309), tensor(1.6134), tensor(1.5595), tensor(1.5293), tensor(1.5220), tensor(1.5269), tensor(1.5508), tensor(2.7372), tensor(1.4691), tensor(2.8438), tensor(1.6314), tensor(1.4519), tensor(3.2629), tensor(1.5003), tensor(1.4933), tensor(1.5023), tensor(1.5580), tensor(1.5008), tensor(1.4857), tensor(1.4789), tensor(1.7736), tensor(2.0215)]
09/23/2021 13:40:36 - INFO - __main__ - interference_scores=[tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]
09/23/2021 13:40:36 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-6677', 'mrqa_triviaqa-validation-1551', 'mrqa_naturalquestions-validation-3028', 'mrqa_squad-validation-4185', 'mrqa_squad-validation-6303', 'mrqa_naturalquestions-validation-2753', 'mrqa_squad-validation-5622', 'mrqa_hotpotqa-validation-14', 'mrqa_naturalquestions-validation-5465', 'mrqa_triviaqa-validation-6351', 'mrqa_naturalquestions-validation-3828', 'mrqa_triviaqa-validation-4856', 'mrqa_naturalquestions-validation-5144', 'mrqa_triviaqa-validation-5937', 'mrqa_hotpotqa-validation-5325', 'mrqa_triviaqa-validation-2376', 'mrqa_triviaqa-validation-3915', 'mrqa_triviaqa-validation-5026', 'mrqa_squad-validation-7746', 'mrqa_triviaqa-validation-7369', 'mrqa_hotpotqa-validation-1968', 'mrqa_triviaqa-validation-671', 'mrqa_naturalquestions-validation-1364', 'mrqa_triviaqa-validation-1437', 'mrqa_triviaqa-validation-5406', 'mrqa_hotpotqa-validation-400', 'mrqa_hotpotqa-validation-5802', 'mrqa_triviaqa-validation-5972', 'mrqa_squad-validation-1516', 'mrqa_naturalquestions-validation-2730', 'mrqa_squad-validation-10322', 'mrqa_triviaqa-validation-2210']
09/23/2021 13:40:36 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:40:36 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=61) .... Timecode: 2
09/23/2021 13:40:49 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:40:49 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 2
09/23/2021 13:40:53 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 13:40:53 - INFO - __main__ - Instant Fixing Rate: 0.9655172413793104
09/23/2021 13:40:53 - INFO - __main__ - Instant Retention Rate: 0.3333333322222222
09/23/2021 13:40:55 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_003.pt.
09/23/2021 13:40:55 - INFO - __main__ - Saving the current error examples (len=29) to the memory.
09/23/2021 13:40:55 - INFO - __main__ - Current memory size: 58.
09/23/2021 13:40:55 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:40:55 - INFO - __main__ - Finished.
09/23/2021 13:40:55 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:40:55 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:40:55 - INFO - __main__ - Evaluating to get errors .... Timecode: 3
09/23/2021 13:40:59 - INFO - __main__ - Before Error Fixing: 0.125
09/23/2021 13:40:59 - INFO - __main__ - Found 28 errors.
09/23/2021 13:40:59 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:40:59 - INFO - __main__ - Current memory size: 87.
09/23/2021 13:40:59 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=87;
09/23/2021 13:41:12 - INFO - __main__ - before_losses=[tensor(1.5606), tensor(1.7734), tensor(3.4914), tensor(1.4814), tensor(1.6152), tensor(1.5619), tensor(1.4860), tensor(1.5192), tensor(1.5029), tensor(1.4628), tensor(1.5255), tensor(1.4673), tensor(1.5056), tensor(1.4991), tensor(1.6207), tensor(3.7294), tensor(1.5583), tensor(1.5034), tensor(1.5483), tensor(1.6119), tensor(2.1550), tensor(1.4501), tensor(1.5162), tensor(1.5218), tensor(1.8578), tensor(1.4728), tensor(1.4876), tensor(1.9240), tensor(2.7395), tensor(1.4945), tensor(1.4887), tensor(1.5406), tensor(1.5303), tensor(1.4930), tensor(1.4999), tensor(1.5486), tensor(1.5277), tensor(1.5147), tensor(1.5217), tensor(2.0388), tensor(1.5056), tensor(1.4795), tensor(1.5084), tensor(1.5153), tensor(1.5306), tensor(1.5177), tensor(1.4711), tensor(1.4685), tensor(2.4132), tensor(1.5603), tensor(1.5194), tensor(1.5211), tensor(1.5009), tensor(1.5604), tensor(1.4875), tensor(1.4692), tensor(1.6213), tensor(1.5258), tensor(1.5692), tensor(2.1671), tensor(1.5034), tensor(2.7203), tensor(1.6804), tensor(1.4953), tensor(1.4643), tensor(1.5310), tensor(1.4896), tensor(2.1257), tensor(1.5008), tensor(1.5126), tensor(1.4601), tensor(1.5756), tensor(1.4840), tensor(1.5705), tensor(1.4475), tensor(1.8747), tensor(1.5956), tensor(1.5113), tensor(1.5152), tensor(1.4984), tensor(1.8198), tensor(1.5331), tensor(1.4928), tensor(1.4798), tensor(1.6758), tensor(1.5026), tensor(1.5569)]
09/23/2021 13:41:12 - INFO - __main__ - after_losses=[tensor(1.5606), tensor(1.7734), tensor(3.4914), tensor(1.4814), tensor(1.6152), tensor(1.5619), tensor(1.4860), tensor(1.5192), tensor(1.5029), tensor(1.4628), tensor(1.5255), tensor(1.4673), tensor(1.5056), tensor(1.4991), tensor(1.6207), tensor(3.7294), tensor(1.5583), tensor(1.5034), tensor(1.5483), tensor(1.6119), tensor(2.1550), tensor(1.4501), tensor(1.5162), tensor(1.5218), tensor(1.8578), tensor(1.4728), tensor(1.4876), tensor(1.9240), tensor(2.7395), tensor(1.4945), tensor(1.4887), tensor(1.5406), tensor(1.5303), tensor(1.4930), tensor(1.4999), tensor(1.5486), tensor(1.5277), tensor(1.5147), tensor(1.5217), tensor(2.0388), tensor(1.5056), tensor(1.4795), tensor(1.5084), tensor(1.5153), tensor(1.5306), tensor(1.5177), tensor(1.4711), tensor(1.4685), tensor(2.4132), tensor(1.5603), tensor(1.5194), tensor(1.5211), tensor(1.5009), tensor(1.5604), tensor(1.4875), tensor(1.4692), tensor(1.6213), tensor(1.5258), tensor(1.5692), tensor(2.1671), tensor(1.5034), tensor(2.7203), tensor(1.6804), tensor(1.4953), tensor(1.4643), tensor(1.5310), tensor(1.4896), tensor(2.1257), tensor(1.5008), tensor(1.5126), tensor(1.4601), tensor(1.5756), tensor(1.4840), tensor(1.5705), tensor(1.4475), tensor(1.8747), tensor(1.5956), tensor(1.5113), tensor(1.5152), tensor(1.4984), tensor(1.8198), tensor(1.5331), tensor(1.4928), tensor(1.4798), tensor(1.6758), tensor(1.5026), tensor(1.5569)]
09/23/2021 13:41:12 - INFO - __main__ - interference_scores=[tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]
09/23/2021 13:41:12 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-8513', 'mrqa_triviaqa-validation-2096', 'mrqa_triviaqa-validation-1616', 'mrqa_hotpotqa-validation-1888', 'mrqa_hotpotqa-validation-2679', 'mrqa_triviaqa-validation-4856', 'mrqa_hotpotqa-validation-400', 'mrqa_triviaqa-validation-4729', 'mrqa_squad-validation-6303', 'mrqa_squad-validation-4253', 'mrqa_squad-validation-5622', 'mrqa_naturalquestions-validation-9386', 'mrqa_hotpotqa-validation-5802', 'mrqa_squad-validation-4185', 'mrqa_squad-validation-1516', 'mrqa_triviaqa-validation-2376', 'mrqa_triviaqa-validation-1935', 'mrqa_triviaqa-validation-5026', 'mrqa_hotpotqa-validation-3971', 'mrqa_naturalquestions-validation-10680', 'mrqa_triviaqa-validation-3915', 'mrqa_naturalquestions-validation-3028', 'mrqa_triviaqa-validation-671', 'mrqa_naturalquestions-validation-2730', 'mrqa_triviaqa-validation-5937', 'mrqa_naturalquestions-validation-6157', 'mrqa_hotpotqa-validation-3632', 'mrqa_triviaqa-validation-1551', 'mrqa_squad-validation-2629', 'mrqa_naturalquestions-validation-6896', 'mrqa_naturalquestions-validation-10364', 'mrqa_triviaqa-validation-5972']
09/23/2021 13:41:12 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:41:12 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=60) .... Timecode: 3
09/23/2021 13:41:26 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:41:26 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 3
09/23/2021 13:41:30 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 13:41:30 - INFO - __main__ - Instant Fixing Rate: 0.9642857142857143
09/23/2021 13:41:30 - INFO - __main__ - Instant Retention Rate: 0.49999999875
09/23/2021 13:41:32 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_004.pt.
09/23/2021 13:41:32 - INFO - __main__ - Saving the current error examples (len=28) to the memory.
09/23/2021 13:41:32 - INFO - __main__ - Current memory size: 87.
09/23/2021 13:41:32 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:41:32 - INFO - __main__ - Finished.
09/23/2021 13:41:32 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:41:32 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:41:32 - INFO - __main__ - Evaluating to get errors .... Timecode: 4
09/23/2021 13:41:35 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:41:35 - INFO - __main__ - Found 26 errors.
09/23/2021 13:41:35 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:41:35 - INFO - __main__ - Current memory size: 115.
09/23/2021 13:41:35 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=115;
09/23/2021 13:41:49 - INFO - __main__ - before_losses=[tensor(1.5176), tensor(1.4996), tensor(2.5671), tensor(1.4758), tensor(1.5542), tensor(1.4891), tensor(1.4465), tensor(2.5589), tensor(1.4754), tensor(1.4588), tensor(1.6380), tensor(1.4442), tensor(1.5145), tensor(1.7375), tensor(1.4718), tensor(1.5211), tensor(2.0246), tensor(1.5347), tensor(2.1030), tensor(1.4583), tensor(2.3029), tensor(1.4861), tensor(1.5204), tensor(1.7475), tensor(1.5518), tensor(1.5167), tensor(1.5264), tensor(1.4663), tensor(1.5191), tensor(2.0868), tensor(1.5129), tensor(1.5181), tensor(1.5259), tensor(1.5578), tensor(1.4817), tensor(1.7366), tensor(1.4806), tensor(1.6748), tensor(1.5506), tensor(1.4498), tensor(1.5830), tensor(1.6286), tensor(2.9095), tensor(1.5411), tensor(1.4910), tensor(1.5286), tensor(1.5014), tensor(1.5293), tensor(1.4722), tensor(1.5393), tensor(1.4587), tensor(1.4966), tensor(1.4874), tensor(1.5427), tensor(1.4627), tensor(1.4916), tensor(1.5362), tensor(1.4957), tensor(1.5169), tensor(1.5959), tensor(1.4952), tensor(1.4657), tensor(1.5201), tensor(1.6087), tensor(2.0918), tensor(1.5139), tensor(1.4648), tensor(1.4967), tensor(1.5577), tensor(2.1645), tensor(1.5238), tensor(1.5664), tensor(1.5189), tensor(1.5497), tensor(1.5359), tensor(1.4943), tensor(1.5071), tensor(1.4974), tensor(1.4984), tensor(1.4844), tensor(1.4838), tensor(1.5064), tensor(1.4745), tensor(1.4654), tensor(1.5632), tensor(1.4705), tensor(1.5825), tensor(1.5408), tensor(2.0772), tensor(1.4996), tensor(1.5022), tensor(1.5026), tensor(1.5272), tensor(1.4820), tensor(2.1068), tensor(1.9973), tensor(1.4617), tensor(1.5194), tensor(1.5300), tensor(1.5224), tensor(1.4943), tensor(1.5705), tensor(1.4578), tensor(1.9461), tensor(1.4955), tensor(1.5489), tensor(1.5008), tensor(1.5003), tensor(1.5315), tensor(1.4842), tensor(1.5278), tensor(1.4884), tensor(1.5062), tensor(1.5819), tensor(1.6563)]
09/23/2021 13:41:49 - INFO - __main__ - after_losses=[tensor(1.5176), tensor(1.4996), tensor(2.5671), tensor(1.4758), tensor(1.5542), tensor(1.4891), tensor(1.4465), tensor(2.5589), tensor(1.4754), tensor(1.4588), tensor(1.6380), tensor(1.4442), tensor(1.5145), tensor(1.7375), tensor(1.4718), tensor(1.5211), tensor(2.0246), tensor(1.5347), tensor(2.1030), tensor(1.4583), tensor(2.3029), tensor(1.4861), tensor(1.5204), tensor(1.7475), tensor(1.5518), tensor(1.5167), tensor(1.5264), tensor(1.4663), tensor(1.5191), tensor(2.0868), tensor(1.5129), tensor(1.5181), tensor(1.5259), tensor(1.5578), tensor(1.4817), tensor(1.7366), tensor(1.4806), tensor(1.6748), tensor(1.5506), tensor(1.4498), tensor(1.5830), tensor(1.6286), tensor(2.9095), tensor(1.5411), tensor(1.4910), tensor(1.5286), tensor(1.5014), tensor(1.5293), tensor(1.4722), tensor(1.5393), tensor(1.4587), tensor(1.4966), tensor(1.4874), tensor(1.5427), tensor(1.4627), tensor(1.4916), tensor(1.5362), tensor(1.4957), tensor(1.5169), tensor(1.5959), tensor(1.4952), tensor(1.4657), tensor(1.5201), tensor(1.6087), tensor(2.0918), tensor(1.5139), tensor(1.4648), tensor(1.4967), tensor(1.5577), tensor(2.1645), tensor(1.5238), tensor(1.5664), tensor(1.5189), tensor(1.5497), tensor(1.5359), tensor(1.4943), tensor(1.5071), tensor(1.4974), tensor(1.4984), tensor(1.4844), tensor(1.4838), tensor(1.5064), tensor(1.4745), tensor(1.4654), tensor(1.5632), tensor(1.4705), tensor(1.5825), tensor(1.5408), tensor(2.0772), tensor(1.4996), tensor(1.5022), tensor(1.5026), tensor(1.5272), tensor(1.4820), tensor(2.1068), tensor(1.9973), tensor(1.4617), tensor(1.5194), tensor(1.5300), tensor(1.5224), tensor(1.4943), tensor(1.5705), tensor(1.4578), tensor(1.9461), tensor(1.4955), tensor(1.5489), tensor(1.5008), tensor(1.5003), tensor(1.5315), tensor(1.4842), tensor(1.5278), tensor(1.4884), tensor(1.5062), tensor(1.5819), tensor(1.6563)]
09/23/2021 13:41:49 - INFO - __main__ - interference_scores=[tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]
09/23/2021 13:41:49 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-2069', 'mrqa_naturalquestions-validation-5647', 'mrqa_triviaqa-validation-3915', 'mrqa_triviaqa-validation-2136', 'mrqa_squad-validation-8542', 'mrqa_naturalquestions-validation-868', 'mrqa_naturalquestions-validation-3028', 'mrqa_triviaqa-validation-6402', 'mrqa_squad-validation-4185', 'mrqa_hotpotqa-validation-4825', 'mrqa_triviaqa-validation-2530', 'mrqa_squad-validation-2191', 'mrqa_triviaqa-validation-5026', 'mrqa_triviaqa-validation-4729', 'mrqa_hotpotqa-validation-1376', 'mrqa_squad-validation-5622', 'mrqa_squad-validation-1539', 'mrqa_naturalquestions-validation-2730', 'mrqa_squad-validation-10322', 'mrqa_hotpotqa-validation-3241', 'mrqa_naturalquestions-validation-5180', 'mrqa_hotpotqa-validation-1201', 'mrqa_hotpotqa-validation-5699', 'mrqa_triviaqa-validation-1616', 'mrqa_squad-validation-7746', 'mrqa_naturalquestions-validation-3828', 'mrqa_triviaqa-validation-4768', 'mrqa_naturalquestions-validation-683', 'mrqa_naturalquestions-validation-5465', 'mrqa_triviaqa-validation-1551', 'mrqa_naturalquestions-validation-5818', 'mrqa_squad-validation-7821']
09/23/2021 13:41:49 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:41:49 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 4
09/23/2021 13:42:01 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:42:01 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 4
09/23/2021 13:44:19 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:44:19 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:44:19 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:44:25 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:44:28 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:44:29 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:44:29 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:44:29 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:44:33 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:44:36 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:44:36 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:44:36 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:44:36 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:44:36 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:44:36 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:44:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:44:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:44:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:44:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:44:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:44:38 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:44:38 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:44:38 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:44:42 - INFO - __main__ - Finished.
09/23/2021 13:44:42 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:44:42 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:44:42 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:44:42 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:44:42 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:44:44 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:44:44 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:44:44 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:44:48 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:44:48 - INFO - __main__ - Found 32 errors.
09/23/2021 13:44:48 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:44:55 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:44:55 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:44:58 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:44:58 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:44:58 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:45:00 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:45:00 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:45:00 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:45:00 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:45:00 - INFO - __main__ - Finished.
09/23/2021 13:45:00 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:45:00 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:45:00 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:45:03 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:45:03 - INFO - __main__ - Found 26 errors.
09/23/2021 13:45:03 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:45:03 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:45:03 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:45:08 - INFO - __main__ - before_losses=[tensor(1.5589), tensor(1.5005), tensor(2.1458), tensor(1.4888), tensor(1.5770), tensor(1.5163), tensor(1.4772), tensor(2.3435), tensor(2.1019), tensor(1.5249), tensor(2.9214), tensor(1.8106), tensor(2.3314), tensor(1.8749), tensor(1.5362), tensor(1.4525), tensor(1.5706), tensor(1.5381), tensor(1.6300), tensor(1.5215), tensor(1.5599), tensor(1.5514), tensor(1.5321), tensor(1.5203), tensor(1.5773), tensor(2.0851), tensor(2.6389), tensor(1.5650), tensor(1.4551), tensor(1.8541), tensor(1.5547), tensor(1.6852)]
09/23/2021 13:45:08 - INFO - __main__ - after_losses=[tensor(1.5589), tensor(1.5005), tensor(2.1458), tensor(1.4888), tensor(1.5770), tensor(1.5163), tensor(1.4772), tensor(2.3435), tensor(2.1019), tensor(1.5249), tensor(2.9214), tensor(1.8106), tensor(2.3314), tensor(1.8749), tensor(1.5362), tensor(1.4525), tensor(1.5706), tensor(1.5381), tensor(1.6300), tensor(1.5215), tensor(1.5599), tensor(1.5514), tensor(1.5321), tensor(1.5203), tensor(1.5773), tensor(2.0851), tensor(2.6389), tensor(1.5650), tensor(1.4551), tensor(1.8541), tensor(1.5547), tensor(1.6852)]
09/23/2021 13:45:08 - INFO - __main__ - interference_scores=[tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]
09/23/2021 13:45:08 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-7746', 'mrqa_hotpotqa-validation-409', 'mrqa_triviaqa-validation-5937', 'mrqa_naturalquestions-validation-6157', 'mrqa_naturalquestions-validation-9688', 'mrqa_hotpotqa-validation-2970', 'mrqa_hotpotqa-validation-1968', 'mrqa_triviaqa-validation-3915', 'mrqa_triviaqa-validation-1616', 'mrqa_naturalquestions-validation-2730', 'mrqa_triviaqa-validation-2376', 'mrqa_naturalquestions-validation-2248', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-7369', 'mrqa_triviaqa-validation-6351', 'mrqa_hotpotqa-validation-3241', 'mrqa_triviaqa-validation-4054', 'mrqa_squad-validation-4185', 'mrqa_triviaqa-validation-671', 'mrqa_naturalquestions-validation-8948', 'mrqa_triviaqa-validation-5972', 'mrqa_naturalquestions-validation-5465', 'mrqa_naturalquestions-validation-7017', 'mrqa_squad-validation-3478', 'mrqa_squad-validation-392', 'mrqa_squad-validation-10322', 'mrqa_squad-validation-6677', 'mrqa_squad-validation-6303', 'mrqa_naturalquestions-validation-3028', 'mrqa_triviaqa-validation-1551', 'mrqa_squad-validation-5622', 'mrqa_squad-validation-7821']
09/23/2021 13:45:08 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:45:08 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 1
09/23/2021 13:45:21 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:45:21 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 1
09/23/2021 13:45:25 - INFO - __main__ - After Error Fixing: 0.875
09/23/2021 13:45:25 - INFO - __main__ - Instant Fixing Rate: 0.9615384615384616
09/23/2021 13:45:25 - INFO - __main__ - Instant Retention Rate: 0.49999999916666665
09/23/2021 13:45:26 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_002.pt.
09/23/2021 13:45:26 - INFO - __main__ - Saving the current error examples (len=26) to the memory.
09/23/2021 13:45:26 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:45:26 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:45:26 - INFO - __main__ - Finished.
09/23/2021 13:45:26 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:45:26 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:45:26 - INFO - __main__ - Evaluating to get errors .... Timecode: 2
09/23/2021 13:45:30 - INFO - __main__ - Before Error Fixing: 0.09375
09/23/2021 13:45:30 - INFO - __main__ - Found 29 errors.
09/23/2021 13:45:30 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:45:30 - INFO - __main__ - Current memory size: 58.
09/23/2021 13:45:30 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=58;
09/23/2021 13:46:33 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:46:33 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:46:33 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:46:39 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:46:42 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:46:43 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:46:43 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:46:43 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:46:46 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:46:50 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:46:50 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:46:50 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:46:50 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:46:50 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:46:50 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:46:51 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:46:51 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:46:51 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:46:51 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:46:51 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:46:51 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:46:51 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:46:52 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:46:56 - INFO - __main__ - Finished.
09/23/2021 13:46:56 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:46:56 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:46:56 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:46:56 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:46:56 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:46:58 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:46:58 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:46:58 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:47:02 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:47:02 - INFO - __main__ - Found 32 errors.
09/23/2021 13:47:02 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:47:09 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:47:09 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:47:13 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:47:13 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:47:13 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:47:15 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:47:15 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:47:15 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:47:15 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:47:15 - INFO - __main__ - Finished.
09/23/2021 13:47:15 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:47:15 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:47:15 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:47:18 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:47:18 - INFO - __main__ - Found 26 errors.
09/23/2021 13:47:18 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:47:18 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:47:18 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:47:23 - INFO - __main__ - before_losses=[tensor(1.5589), tensor(1.5005), tensor(2.1458), tensor(1.4888), tensor(1.5770), tensor(1.5163), tensor(1.4772), tensor(2.3435), tensor(2.1019), tensor(1.5249), tensor(2.9214), tensor(1.8106), tensor(2.3314), tensor(1.8749), tensor(1.5362), tensor(1.4525), tensor(1.5706), tensor(1.5381), tensor(1.6300), tensor(1.5215), tensor(1.5599), tensor(1.5514), tensor(1.5321), tensor(1.5203), tensor(1.5773), tensor(2.0851), tensor(2.6389), tensor(1.5650), tensor(1.4551), tensor(1.8541), tensor(1.5547), tensor(1.6852)]
09/23/2021 13:47:23 - INFO - __main__ - after_losses=[tensor(1.5589), tensor(1.5005), tensor(2.1458), tensor(1.4888), tensor(1.5770), tensor(1.5163), tensor(1.4772), tensor(2.3435), tensor(2.1019), tensor(1.5249), tensor(2.9214), tensor(1.8106), tensor(2.3314), tensor(1.8749), tensor(1.5362), tensor(1.4525), tensor(1.5706), tensor(1.5381), tensor(1.6300), tensor(1.5215), tensor(1.5599), tensor(1.5514), tensor(1.5321), tensor(1.5203), tensor(1.5773), tensor(2.0851), tensor(2.6389), tensor(1.5650), tensor(1.4551), tensor(1.8541), tensor(1.5547), tensor(1.6852)]
09/23/2021 13:47:23 - INFO - __main__ - interference_scores=[tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]
09/23/2021 13:47:23 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-7746', 'mrqa_hotpotqa-validation-409', 'mrqa_triviaqa-validation-5937', 'mrqa_naturalquestions-validation-6157', 'mrqa_naturalquestions-validation-9688', 'mrqa_hotpotqa-validation-2970', 'mrqa_hotpotqa-validation-1968', 'mrqa_triviaqa-validation-3915', 'mrqa_triviaqa-validation-1616', 'mrqa_naturalquestions-validation-2730', 'mrqa_triviaqa-validation-2376', 'mrqa_naturalquestions-validation-2248', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-7369', 'mrqa_triviaqa-validation-6351', 'mrqa_hotpotqa-validation-3241', 'mrqa_triviaqa-validation-4054', 'mrqa_squad-validation-4185', 'mrqa_triviaqa-validation-671', 'mrqa_naturalquestions-validation-8948', 'mrqa_triviaqa-validation-5972', 'mrqa_naturalquestions-validation-5465', 'mrqa_naturalquestions-validation-7017', 'mrqa_squad-validation-3478', 'mrqa_squad-validation-392', 'mrqa_squad-validation-10322', 'mrqa_squad-validation-6677', 'mrqa_squad-validation-6303', 'mrqa_naturalquestions-validation-3028', 'mrqa_triviaqa-validation-1551', 'mrqa_squad-validation-5622', 'mrqa_squad-validation-7821']
09/23/2021 13:47:23 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:47:23 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 1
09/23/2021 13:47:36 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:47:36 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 1
09/23/2021 13:47:40 - INFO - __main__ - After Error Fixing: 0.875
09/23/2021 13:47:40 - INFO - __main__ - Instant Fixing Rate: 0.9615384615384616
09/23/2021 13:47:40 - INFO - __main__ - Instant Retention Rate: 0.49999999916666665
09/23/2021 13:47:42 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_002.pt.
09/23/2021 13:47:42 - INFO - __main__ - Saving the current error examples (len=26) to the memory.
09/23/2021 13:47:42 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:47:42 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:47:42 - INFO - __main__ - Finished.
09/23/2021 13:47:42 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:47:42 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:47:42 - INFO - __main__ - Evaluating to get errors .... Timecode: 2
09/23/2021 13:47:45 - INFO - __main__ - Before Error Fixing: 0.0625
09/23/2021 13:47:45 - INFO - __main__ - Found 30 errors.
09/23/2021 13:47:45 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:47:45 - INFO - __main__ - Current memory size: 58.
09/23/2021 13:47:45 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=58;
09/23/2021 13:47:55 - INFO - __main__ - before_losses=[tensor(1.5676), tensor(1.5161), tensor(1.6224), tensor(1.4685), tensor(1.5417), tensor(1.8096), tensor(1.5130), tensor(1.5310), tensor(1.7063), tensor(1.5531), tensor(1.5320), tensor(1.4925), tensor(1.6117), tensor(1.6126), tensor(1.4485), tensor(1.5529), tensor(3.5415), tensor(1.4859), tensor(1.5408), tensor(1.5020), tensor(1.4646), tensor(1.6335), tensor(1.5172), tensor(1.5246), tensor(1.5998), tensor(1.4963), tensor(1.6329), tensor(1.5474), tensor(2.9656), tensor(1.5369), tensor(1.6534), tensor(1.5452), tensor(1.4788), tensor(1.5072), tensor(1.4954), tensor(1.5972), tensor(2.6651), tensor(1.4748), tensor(1.5273), tensor(1.7534), tensor(1.7895), tensor(1.5178), tensor(1.5979), tensor(3.7400), tensor(1.4761), tensor(1.4833), tensor(1.5364), tensor(1.5007), tensor(2.1008), tensor(1.5229), tensor(1.9803), tensor(1.7020), tensor(1.5115), tensor(1.4533), tensor(1.5195), tensor(1.5156), tensor(1.5660), tensor(2.1371)]
09/23/2021 13:47:55 - INFO - __main__ - after_losses=[tensor(1.5676), tensor(1.5161), tensor(1.6224), tensor(1.4685), tensor(1.5417), tensor(1.8096), tensor(1.5130), tensor(1.5310), tensor(1.7063), tensor(1.5531), tensor(1.5320), tensor(1.4925), tensor(1.6117), tensor(1.6126), tensor(1.4485), tensor(1.5529), tensor(3.5415), tensor(1.4859), tensor(1.5408), tensor(1.5020), tensor(1.4646), tensor(1.6335), tensor(1.5172), tensor(1.5246), tensor(1.5998), tensor(1.4963), tensor(1.6329), tensor(1.5474), tensor(2.9656), tensor(1.5369), tensor(1.6534), tensor(1.5452), tensor(1.4788), tensor(1.5072), tensor(1.4954), tensor(1.5972), tensor(2.6651), tensor(1.4748), tensor(1.5273), tensor(1.7534), tensor(1.7895), tensor(1.5178), tensor(1.5979), tensor(3.7400), tensor(1.4761), tensor(1.4833), tensor(1.5364), tensor(1.5007), tensor(2.1008), tensor(1.5229), tensor(1.9803), tensor(1.7020), tensor(1.5115), tensor(1.4533), tensor(1.5195), tensor(1.5156), tensor(1.5660), tensor(2.1371)]
09/23/2021 13:47:55 - INFO - __main__ - interference_scores=[tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]
09/23/2021 13:47:55 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-5026', 'mrqa_squad-validation-7746', 'mrqa_triviaqa-validation-7369', 'mrqa_hotpotqa-validation-1968', 'mrqa_triviaqa-validation-671', 'mrqa_triviaqa-validation-1924', 'mrqa_hotpotqa-validation-400', 'mrqa_naturalquestions-validation-5144', 'mrqa_triviaqa-validation-5937', 'mrqa_squad-validation-1516', 'mrqa_squad-validation-5622', 'mrqa_naturalquestions-validation-2753', 'mrqa_naturalquestions-validation-10680', 'mrqa_triviaqa-validation-2210', 'mrqa_naturalquestions-validation-3028', 'mrqa_naturalquestions-validation-3828', 'mrqa_triviaqa-validation-1616', 'mrqa_hotpotqa-validation-1201', 'mrqa_squad-validation-4185', 'mrqa_naturalquestions-validation-9688', 'mrqa_hotpotqa-validation-1626', 'mrqa_triviaqa-validation-4856', 'mrqa_squad-validation-6303', 'mrqa_squad-validation-3478', 'mrqa_triviaqa-validation-6351', 'mrqa_hotpotqa-validation-409', 'mrqa_triviaqa-validation-1437', 'mrqa_hotpotqa-validation-3971', 'mrqa_squad-validation-2629', 'mrqa_squad-validation-7821', 'mrqa_triviaqa-validation-1551', 'mrqa_naturalquestions-validation-7511']
09/23/2021 13:47:55 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:47:55 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=62) .... Timecode: 2
09/23/2021 13:48:09 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:48:09 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 2
09/23/2021 13:48:12 - INFO - __main__ - After Error Fixing: 0.9375
09/23/2021 13:48:12 - INFO - __main__ - Instant Fixing Rate: 1.0
09/23/2021 13:48:12 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:48:14 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_003.pt.
09/23/2021 13:48:14 - INFO - __main__ - Saving the current error examples (len=30) to the memory.
09/23/2021 13:48:14 - INFO - __main__ - Current memory size: 58.
09/23/2021 13:48:14 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:48:14 - INFO - __main__ - Finished.
09/23/2021 13:48:14 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:48:14 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:48:14 - INFO - __main__ - Evaluating to get errors .... Timecode: 3
09/23/2021 13:48:17 - INFO - __main__ - Before Error Fixing: 0.09375
09/23/2021 13:48:17 - INFO - __main__ - Found 29 errors.
09/23/2021 13:48:17 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:48:17 - INFO - __main__ - Current memory size: 88.
09/23/2021 13:48:17 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=88;
09/23/2021 13:48:29 - INFO - __main__ - before_losses=[tensor(1.4873), tensor(1.4991), tensor(1.5456), tensor(3.9837), tensor(1.5136), tensor(2.0981), tensor(1.5137), tensor(1.5057), tensor(1.5079), tensor(1.5333), tensor(1.4888), tensor(1.4951), tensor(1.6124), tensor(1.5180), tensor(1.4950), tensor(1.4634), tensor(1.4801), tensor(1.4920), tensor(1.4765), tensor(1.6236), tensor(1.5175), tensor(1.5442), tensor(1.9848), tensor(1.5148), tensor(1.5621), tensor(1.5108), tensor(1.5275), tensor(1.4571), tensor(1.6029), tensor(1.4651), tensor(1.5789), tensor(1.5082), tensor(1.4464), tensor(1.8516), tensor(1.6446), tensor(1.4848), tensor(1.5051), tensor(1.4869), tensor(1.5161), tensor(1.7134), tensor(1.4895), tensor(1.5002), tensor(1.4657), tensor(2.4235), tensor(1.5533), tensor(1.5386), tensor(1.4769), tensor(1.4498), tensor(1.5457), tensor(1.5033), tensor(1.5038), tensor(1.5319), tensor(1.7361), tensor(1.4668), tensor(3.6954), tensor(1.5232), tensor(1.4996), tensor(1.4517), tensor(1.4726), tensor(1.4660), tensor(1.4795), tensor(1.5085), tensor(1.4706), tensor(1.5150), tensor(1.9802), tensor(2.1368), tensor(1.4709), tensor(1.6398), tensor(1.4763), tensor(1.4679), tensor(1.5672), tensor(1.4957), tensor(1.4913), tensor(2.7023), tensor(1.5013), tensor(1.9896), tensor(1.5329), tensor(1.4821), tensor(1.5030), tensor(1.4494), tensor(1.5255), tensor(1.5031), tensor(1.5274), tensor(1.5322), tensor(1.4872), tensor(1.4790), tensor(1.5562), tensor(1.4907)]
09/23/2021 13:48:29 - INFO - __main__ - after_losses=[tensor(1.4873), tensor(1.4991), tensor(1.5456), tensor(3.9837), tensor(1.5136), tensor(2.0981), tensor(1.5137), tensor(1.5057), tensor(1.5079), tensor(1.5333), tensor(1.4888), tensor(1.4951), tensor(1.6124), tensor(1.5180), tensor(1.4950), tensor(1.4634), tensor(1.4801), tensor(1.4920), tensor(1.4765), tensor(1.6236), tensor(1.5175), tensor(1.5442), tensor(1.9848), tensor(1.5148), tensor(1.5621), tensor(1.5108), tensor(1.5275), tensor(1.4571), tensor(1.6029), tensor(1.4651), tensor(1.5789), tensor(1.5082), tensor(1.4464), tensor(1.8516), tensor(1.6446), tensor(1.4848), tensor(1.5051), tensor(1.4869), tensor(1.5161), tensor(1.7134), tensor(1.4895), tensor(1.5002), tensor(1.4657), tensor(2.4235), tensor(1.5533), tensor(1.5386), tensor(1.4769), tensor(1.4498), tensor(1.5457), tensor(1.5033), tensor(1.5038), tensor(1.5319), tensor(1.7361), tensor(1.4668), tensor(3.6954), tensor(1.5232), tensor(1.4996), tensor(1.4517), tensor(1.4726), tensor(1.4660), tensor(1.4795), tensor(1.5085), tensor(1.4706), tensor(1.5150), tensor(1.9802), tensor(2.1368), tensor(1.4709), tensor(1.6398), tensor(1.4763), tensor(1.4679), tensor(1.5672), tensor(1.4957), tensor(1.4913), tensor(2.7023), tensor(1.5013), tensor(1.9896), tensor(1.5329), tensor(1.4821), tensor(1.5030), tensor(1.4494), tensor(1.5255), tensor(1.5031), tensor(1.5274), tensor(1.5322), tensor(1.4872), tensor(1.4790), tensor(1.5562), tensor(1.4907)]
09/23/2021 13:48:29 - INFO - __main__ - interference_scores=[tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]
09/23/2021 13:48:29 - INFO - __main__ - retrieved candidates ids = ['mrqa_naturalquestions-validation-9688', 'mrqa_naturalquestions-validation-5180', 'mrqa_naturalquestions-validation-8448', 'mrqa_triviaqa-validation-1616', 'mrqa_naturalquestions-validation-5465', 'mrqa_naturalquestions-validation-2501', 'mrqa_squad-validation-4456', 'mrqa_naturalquestions-validation-539', 'mrqa_squad-validation-7821', 'mrqa_triviaqa-validation-2136', 'mrqa_hotpotqa-validation-3632', 'mrqa_naturalquestions-validation-683', 'mrqa_squad-validation-7149', 'mrqa_triviaqa-validation-893', 'mrqa_squad-validation-5622', 'mrqa_squad-validation-4283', 'mrqa_hotpotqa-validation-1888', 'mrqa_squad-validation-4185', 'mrqa_hotpotqa-validation-1376', 'mrqa_triviaqa-validation-7018', 'mrqa_hotpotqa-validation-2970', 'mrqa_squad-validation-7746', 'mrqa_triviaqa-validation-2096', 'mrqa_triviaqa-validation-2722', 'mrqa_triviaqa-validation-4856', 'mrqa_triviaqa-validation-6351', 'mrqa_naturalquestions-validation-7511', 'mrqa_squad-validation-194', 'mrqa_hotpotqa-validation-2679', 'mrqa_hotpotqa-validation-5802', 'mrqa_squad-validation-10322', 'mrqa_naturalquestions-validation-2730']
09/23/2021 13:48:29 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:48:29 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=61) .... Timecode: 3
09/23/2021 13:48:43 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:48:43 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 3
09/23/2021 13:48:45 - INFO - __main__ - After Error Fixing: 1.0
09/23/2021 13:48:45 - INFO - __main__ - Instant Fixing Rate: 1.0
09/23/2021 13:48:45 - INFO - __main__ - Instant Retention Rate: 0.9999999966666667
09/23/2021 13:48:47 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_004.pt.
09/23/2021 13:48:47 - INFO - __main__ - Saving the current error examples (len=29) to the memory.
09/23/2021 13:48:47 - INFO - __main__ - Current memory size: 88.
09/23/2021 13:48:47 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:48:47 - INFO - __main__ - Finished.
09/23/2021 13:48:47 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:48:47 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:48:47 - INFO - __main__ - Evaluating to get errors .... Timecode: 4
09/23/2021 13:48:50 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:48:50 - INFO - __main__ - Found 26 errors.
09/23/2021 13:48:50 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:48:50 - INFO - __main__ - Current memory size: 117.
09/23/2021 13:48:50 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=117;
09/23/2021 13:49:04 - INFO - __main__ - before_losses=[tensor(1.4591), tensor(1.5222), tensor(1.5101), tensor(1.5683), tensor(1.4597), tensor(1.4837), tensor(1.6556), tensor(1.5665), tensor(1.4846), tensor(1.5553), tensor(1.5658), tensor(1.4795), tensor(1.5118), tensor(1.5284), tensor(1.5064), tensor(1.5607), tensor(1.5605), tensor(1.4791), tensor(1.5729), tensor(1.4831), tensor(1.4981), tensor(1.5148), tensor(1.4934), tensor(1.5187), tensor(1.6556), tensor(2.0366), tensor(1.4958), tensor(1.4627), tensor(1.5085), tensor(1.4496), tensor(1.5463), tensor(1.5564), tensor(1.4983), tensor(1.4795), tensor(2.4089), tensor(1.4633), tensor(1.4894), tensor(1.5860), tensor(1.4673), tensor(1.5151), tensor(1.8363), tensor(1.4856), tensor(3.6518), tensor(1.5020), tensor(1.5436), tensor(1.5205), tensor(1.5005), tensor(1.5826), tensor(1.4940), tensor(1.6751), tensor(1.5029), tensor(1.5495), tensor(1.7237), tensor(1.4989), tensor(1.5679), tensor(1.4845), tensor(1.5133), tensor(1.6951), tensor(1.4562), tensor(1.5173), tensor(1.5236), tensor(1.4968), tensor(1.5109), tensor(1.4766), tensor(1.8378), tensor(1.4818), tensor(1.4947), tensor(1.4462), tensor(1.5824), tensor(1.6392), tensor(1.5425), tensor(1.4991), tensor(1.5054), tensor(1.5427), tensor(1.4978), tensor(1.5037), tensor(1.4647), tensor(1.5458), tensor(1.5247), tensor(2.9763), tensor(1.5162), tensor(1.4659), tensor(1.9436), tensor(1.9804), tensor(1.6457), tensor(1.4610), tensor(1.5362), tensor(1.4751), tensor(1.5382), tensor(1.4658), tensor(1.4830), tensor(1.6332), tensor(1.7956), tensor(1.4862), tensor(1.5009), tensor(1.4873), tensor(1.8347), tensor(1.8492), tensor(1.5110), tensor(1.4675), tensor(1.4851), tensor(2.1449), tensor(1.8768), tensor(1.4530), tensor(1.5362), tensor(1.5555), tensor(1.5131), tensor(1.5211), tensor(1.5967), tensor(1.4889), tensor(1.4885), tensor(1.5101), tensor(1.5177), tensor(1.4953), tensor(1.6275), tensor(1.4903), tensor(2.6808)]
09/23/2021 13:49:04 - INFO - __main__ - after_losses=[tensor(1.4591), tensor(1.5222), tensor(1.5101), tensor(1.5683), tensor(1.4597), tensor(1.4837), tensor(1.6556), tensor(1.5665), tensor(1.4846), tensor(1.5553), tensor(1.5658), tensor(1.4795), tensor(1.5118), tensor(1.5284), tensor(1.5064), tensor(1.5607), tensor(1.5605), tensor(1.4791), tensor(1.5729), tensor(1.4831), tensor(1.4981), tensor(1.5148), tensor(1.4934), tensor(1.5187), tensor(1.6556), tensor(2.0366), tensor(1.4958), tensor(1.4627), tensor(1.5085), tensor(1.4496), tensor(1.5463), tensor(1.5564), tensor(1.4983), tensor(1.4795), tensor(2.4089), tensor(1.4633), tensor(1.4894), tensor(1.5860), tensor(1.4673), tensor(1.5151), tensor(1.8363), tensor(1.4856), tensor(3.6518), tensor(1.5020), tensor(1.5436), tensor(1.5205), tensor(1.5005), tensor(1.5826), tensor(1.4940), tensor(1.6751), tensor(1.5029), tensor(1.5495), tensor(1.7237), tensor(1.4989), tensor(1.5679), tensor(1.4845), tensor(1.5133), tensor(1.6951), tensor(1.4562), tensor(1.5173), tensor(1.5236), tensor(1.4968), tensor(1.5109), tensor(1.4766), tensor(1.8378), tensor(1.4818), tensor(1.4947), tensor(1.4462), tensor(1.5824), tensor(1.6392), tensor(1.5425), tensor(1.4991), tensor(1.5054), tensor(1.5427), tensor(1.4978), tensor(1.5037), tensor(1.4647), tensor(1.5458), tensor(1.5247), tensor(2.9763), tensor(1.5162), tensor(1.4659), tensor(1.9436), tensor(1.9804), tensor(1.6457), tensor(1.4610), tensor(1.5362), tensor(1.4751), tensor(1.5382), tensor(1.4658), tensor(1.4830), tensor(1.6332), tensor(1.7956), tensor(1.4862), tensor(1.5009), tensor(1.4873), tensor(1.8347), tensor(1.8492), tensor(1.5110), tensor(1.4675), tensor(1.4851), tensor(2.1449), tensor(1.8768), tensor(1.4530), tensor(1.5362), tensor(1.5555), tensor(1.5131), tensor(1.5211), tensor(1.5967), tensor(1.4889), tensor(1.4885), tensor(1.5101), tensor(1.5177), tensor(1.4953), tensor(1.6275), tensor(1.4903), tensor(2.6808)]
09/23/2021 13:49:04 - INFO - __main__ - interference_scores=[tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]
09/23/2021 13:49:04 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-3478', 'mrqa_naturalquestions-validation-1202', 'mrqa_naturalquestions-validation-3189', 'mrqa_triviaqa-validation-2096', 'mrqa_hotpotqa-validation-1626', 'mrqa_hotpotqa-validation-4367', 'mrqa_squad-validation-7149', 'mrqa_triviaqa-validation-4856', 'mrqa_naturalquestions-validation-2753', 'mrqa_triviaqa-validation-893', 'mrqa_triviaqa-validation-287', 'mrqa_naturalquestions-validation-9688', 'mrqa_naturalquestions-validation-7017', 'mrqa_naturalquestions-validation-8448', 'mrqa_triviaqa-validation-2210', 'mrqa_hotpotqa-validation-3971', 'mrqa_triviaqa-validation-365', 'mrqa_squad-validation-2629', 'mrqa_triviaqa-validation-5362', 'mrqa_naturalquestions-validation-9650', 'mrqa_naturalquestions-validation-1364', 'mrqa_naturalquestions-validation-3828', 'mrqa_hotpotqa-validation-3632', 'mrqa_triviaqa-validation-5026', 'mrqa_triviaqa-validation-1935', 'mrqa_triviaqa-validation-1616', 'mrqa_hotpotqa-validation-2679', 'mrqa_hotpotqa-validation-3241', 'mrqa_naturalquestions-validation-2730', 'mrqa_squad-validation-194', 'mrqa_triviaqa-validation-7018', 'mrqa_triviaqa-validation-5937']
09/23/2021 13:49:04 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:49:04 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 4
09/23/2021 13:49:16 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:49:16 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 4
09/23/2021 13:49:19 - INFO - __main__ - After Error Fixing: 0.9375
09/23/2021 13:49:19 - INFO - __main__ - Instant Fixing Rate: 1.0
09/23/2021 13:49:19 - INFO - __main__ - Instant Retention Rate: 0.6666666655555555
09/23/2021 13:49:21 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_005.pt.
09/23/2021 13:49:21 - INFO - __main__ - Saving the current error examples (len=26) to the memory.
09/23/2021 13:49:21 - INFO - __main__ - Current memory size: 117.
09/23/2021 13:49:21 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:49:21 - INFO - __main__ - Finished.
09/23/2021 13:49:21 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:49:21 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:49:21 - INFO - __main__ - Evaluating to get errors .... Timecode: 5
09/23/2021 13:49:23 - INFO - __main__ - Before Error Fixing: 0.09375
09/23/2021 13:49:23 - INFO - __main__ - Found 29 errors.
09/23/2021 13:49:23 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:49:23 - INFO - __main__ - Current memory size: 143.
09/23/2021 13:49:23 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=143;
09/23/2021 13:49:38 - INFO - __main__ - before_losses=[tensor(1.5264), tensor(1.4782), tensor(1.8909), tensor(1.4692), tensor(1.5124), tensor(1.4734), tensor(1.4936), tensor(1.7383), tensor(1.5227), tensor(1.4452), tensor(1.5141), tensor(1.5081), tensor(1.5207), tensor(1.5656), tensor(1.5210), tensor(2.2443), tensor(1.8166), tensor(1.4936), tensor(1.5099), tensor(1.4658), tensor(1.7899), tensor(1.4621), tensor(1.6402), tensor(1.4486), tensor(1.4723), tensor(1.4649), tensor(1.4886), tensor(1.8269), tensor(2.4377), tensor(1.5023), tensor(1.4960), tensor(1.8087), tensor(1.5061), tensor(1.4960), tensor(1.5857), tensor(1.5335), tensor(1.5351), tensor(1.5096), tensor(1.5284), tensor(1.5128), tensor(1.6151), tensor(1.4481), tensor(1.4579), tensor(1.8944), tensor(1.4360), tensor(1.4978), tensor(1.4999), tensor(1.5457), tensor(1.4989), tensor(2.1265), tensor(1.5641), tensor(1.7275), tensor(1.4921), tensor(1.5025), tensor(1.5191), tensor(1.5216), tensor(1.5000), tensor(1.4932), tensor(1.9931), tensor(1.5442), tensor(1.5053), tensor(2.7289), tensor(1.4689), tensor(1.5584), tensor(1.5071), tensor(1.4683), tensor(1.5046), tensor(1.4400), tensor(1.4863), tensor(1.4542), tensor(1.5451), tensor(1.4952), tensor(1.5305), tensor(1.4574), tensor(1.6372), tensor(1.5237), tensor(1.4967), tensor(1.4795), tensor(1.4591), tensor(1.4983), tensor(2.5630), tensor(2.8747), tensor(1.4853), tensor(1.5235), tensor(1.4851), tensor(1.7526), tensor(1.5097), tensor(1.4643), tensor(1.4688), tensor(1.6577), tensor(1.5120), tensor(1.8023), tensor(1.4983), tensor(1.5689), tensor(1.5151), tensor(3.4697), tensor(1.5019), tensor(1.5835), tensor(1.7636), tensor(1.5193), tensor(1.5104), tensor(1.4826), tensor(1.4583), tensor(1.4876), tensor(1.4955), tensor(1.5522), tensor(1.5032), tensor(1.4516), tensor(1.4505), tensor(1.5248), tensor(1.5238), tensor(1.4881), tensor(1.5763), tensor(1.4825), tensor(1.4553), tensor(1.4957), tensor(1.4893), tensor(1.4952), tensor(1.4769), tensor(2.1358), tensor(1.4516), tensor(1.4959), tensor(1.8867), tensor(1.5510), tensor(1.4867), tensor(1.5647), tensor(1.5126), tensor(1.5267), tensor(1.8914), tensor(1.4986), tensor(1.6292), tensor(1.4727), tensor(1.5168), tensor(1.4861), tensor(1.5839), tensor(1.5205), tensor(1.5401), tensor(1.4820), tensor(1.5050), tensor(1.4748), tensor(1.4940), tensor(1.4678), tensor(1.6414)]
09/23/2021 13:49:38 - INFO - __main__ - after_losses=[tensor(1.5264), tensor(1.4782), tensor(1.8909), tensor(1.4692), tensor(1.5124), tensor(1.4734), tensor(1.4936), tensor(1.7383), tensor(1.5227), tensor(1.4452), tensor(1.5141), tensor(1.5081), tensor(1.5207), tensor(1.5656), tensor(1.5210), tensor(2.2443), tensor(1.8166), tensor(1.4936), tensor(1.5099), tensor(1.4658), tensor(1.7899), tensor(1.4621), tensor(1.6402), tensor(1.4486), tensor(1.4723), tensor(1.4649), tensor(1.4886), tensor(1.8269), tensor(2.4377), tensor(1.5023), tensor(1.4960), tensor(1.8087), tensor(1.5061), tensor(1.4960), tensor(1.5857), tensor(1.5335), tensor(1.5351), tensor(1.5096), tensor(1.5284), tensor(1.5128), tensor(1.6151), tensor(1.4481), tensor(1.4579), tensor(1.8944), tensor(1.4360), tensor(1.4978), tensor(1.4999), tensor(1.5457), tensor(1.4989), tensor(2.1265), tensor(1.5641), tensor(1.7275), tensor(1.4921), tensor(1.5025), tensor(1.5191), tensor(1.5216), tensor(1.5000), tensor(1.4932), tensor(1.9931), tensor(1.5442), tensor(1.5053), tensor(2.7289), tensor(1.4689), tensor(1.5584), tensor(1.5071), tensor(1.4683), tensor(1.5046), tensor(1.4400), tensor(1.4863), tensor(1.4542), tensor(1.5451), tensor(1.4952), tensor(1.5305), tensor(1.4574), tensor(1.6372), tensor(1.5237), tensor(1.4967), tensor(1.4795), tensor(1.4591), tensor(1.4983), tensor(2.5630), tensor(2.8747), tensor(1.4853), tensor(1.5235), tensor(1.4851), tensor(1.7526), tensor(1.5097), tensor(1.4643), tensor(1.4688), tensor(1.6577), tensor(1.5120), tensor(1.8023), tensor(1.4983), tensor(1.5689), tensor(1.5151), tensor(3.4697), tensor(1.5019), tensor(1.5835), tensor(1.7636), tensor(1.5193), tensor(1.5104), tensor(1.4826), tensor(1.4583), tensor(1.4876), tensor(1.4955), tensor(1.5522), tensor(1.5032), tensor(1.4516), tensor(1.4505), tensor(1.5248), tensor(1.5238), tensor(1.4881), tensor(1.5763), tensor(1.4825), tensor(1.4553), tensor(1.4957), tensor(1.4893), tensor(1.4952), tensor(1.4769), tensor(2.1358), tensor(1.4516), tensor(1.4959), tensor(1.8867), tensor(1.5510), tensor(1.4867), tensor(1.5647), tensor(1.5126), tensor(1.5267), tensor(1.8914), tensor(1.4986), tensor(1.6292), tensor(1.4727), tensor(1.5168), tensor(1.4861), tensor(1.5839), tensor(1.5205), tensor(1.5401), tensor(1.4820), tensor(1.5050), tensor(1.4748), tensor(1.4940), tensor(1.4678), tensor(1.6414)]
09/23/2021 13:49:38 - INFO - __main__ - interference_scores=[tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]
09/23/2021 13:49:38 - INFO - __main__ - retrieved candidates ids = ['mrqa_naturalquestions-validation-8448', 'mrqa_squad-validation-4456', 'mrqa_squad-validation-6677', 'mrqa_squad-validation-1688', 'mrqa_naturalquestions-validation-2385', 'mrqa_squad-validation-5517', 'mrqa_triviaqa-validation-2136', 'mrqa_squad-validation-1539', 'mrqa_triviaqa-validation-4856', 'mrqa_naturalquestions-validation-3028', 'mrqa_naturalquestions-validation-868', 'mrqa_triviaqa-validation-7018', 'mrqa_naturalquestions-validation-3189', 'mrqa_triviaqa-validation-3339', 'mrqa_naturalquestions-validation-5144', 'mrqa_naturalquestions-validation-5437', 'mrqa_triviaqa-validation-1437', 'mrqa_squad-validation-5622', 'mrqa_squad-validation-3467', 'mrqa_hotpotqa-validation-2263', 'mrqa_triviaqa-validation-3915', 'mrqa_hotpotqa-validation-4825', 'mrqa_triviaqa-validation-6402', 'mrqa_squad-validation-4253', 'mrqa_squad-validation-3058', 'mrqa_hotpotqa-validation-1376', 'mrqa_hotpotqa-validation-1289', 'mrqa_squad-validation-7149', 'mrqa_squad-validation-2629', 'mrqa_squad-validation-7821', 'mrqa_naturalquestions-validation-8948', 'mrqa_triviaqa-validation-5654']
09/23/2021 13:49:38 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:49:38 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=61) .... Timecode: 5
09/23/2021 13:49:51 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:49:51 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 5
09/23/2021 13:49:54 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 13:49:54 - INFO - __main__ - Instant Fixing Rate: 0.9655172413793104
09/23/2021 13:49:54 - INFO - __main__ - Instant Retention Rate: 0.3333333322222222
09/23/2021 13:49:56 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_006.pt.
09/23/2021 13:49:56 - INFO - __main__ - Saving the current error examples (len=29) to the memory.
09/23/2021 13:49:56 - INFO - __main__ - Current memory size: 143.
09/23/2021 13:49:56 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:49:56 - INFO - __main__ - Finished.
09/23/2021 13:49:56 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:49:56 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:49:56 - INFO - __main__ - Evaluating to get errors .... Timecode: 6
09/23/2021 13:49:59 - INFO - __main__ - Before Error Fixing: 0.09375
09/23/2021 13:49:59 - INFO - __main__ - Found 29 errors.
09/23/2021 13:49:59 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:49:59 - INFO - __main__ - Current memory size: 172.
09/23/2021 13:49:59 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=172;
09/23/2021 13:50:15 - INFO - __main__ - before_losses=[tensor(1.5148), tensor(1.4924), tensor(1.4385), tensor(1.4755), tensor(1.5032), tensor(1.5477), tensor(1.5187), tensor(1.5054), tensor(2.0009), tensor(1.4961), tensor(1.4635), tensor(1.4946), tensor(1.8552), tensor(1.4504), tensor(2.8680), tensor(1.4660), tensor(1.5260), tensor(1.5591), tensor(1.5176), tensor(1.4894), tensor(1.6358), tensor(1.5203), tensor(1.5068), tensor(1.5508), tensor(1.4729), tensor(1.4912), tensor(1.4911), tensor(1.5110), tensor(1.5339), tensor(1.4730), tensor(1.5471), tensor(1.4758), tensor(1.4652), tensor(1.5175), tensor(1.8048), tensor(1.4673), tensor(1.4766), tensor(1.5222), tensor(1.5293), tensor(1.4958), tensor(1.6616), tensor(1.4669), tensor(1.4806), tensor(1.4948), tensor(1.5130), tensor(1.4727), tensor(1.5088), tensor(1.4719), tensor(1.8342), tensor(1.7017), tensor(1.5385), tensor(1.4774), tensor(1.5195), tensor(2.1999), tensor(1.5077), tensor(1.4820), tensor(1.4972), tensor(1.4844), tensor(2.0269), tensor(1.5023), tensor(1.7722), tensor(1.5259), tensor(1.5723), tensor(2.7477), tensor(1.7407), tensor(1.4913), tensor(1.4747), tensor(1.4683), tensor(1.6199), tensor(1.4962), tensor(1.4879), tensor(1.5063), tensor(1.4649), tensor(1.4623), tensor(1.5156), tensor(1.5147), tensor(1.4439), tensor(1.4667), tensor(1.4869), tensor(1.4444), tensor(1.5184), tensor(1.5497), tensor(1.4968), tensor(1.5950), tensor(1.4843), tensor(1.4872), tensor(1.8209), tensor(1.6381), tensor(1.4990), tensor(1.4907), tensor(1.5277), tensor(1.6778), tensor(2.1334), tensor(1.4887), tensor(1.5029), tensor(1.5711), tensor(1.4647), tensor(1.4916), tensor(2.1502), tensor(1.7953), tensor(1.5553), tensor(1.4546), tensor(2.9484), tensor(1.4493), tensor(1.8012), tensor(1.4519), tensor(1.4810), tensor(1.4909), tensor(1.5450), tensor(1.5214), tensor(1.4820), tensor(1.4735), tensor(1.4510), tensor(1.4599), tensor(1.4867), tensor(1.6161), tensor(2.5622), tensor(1.4865), tensor(1.5089), tensor(1.7404), tensor(1.4608), tensor(3.1096), tensor(1.4858), tensor(1.7794), tensor(1.4867), tensor(1.7232), tensor(1.5080), tensor(1.5471), tensor(1.4605), tensor(1.5264), tensor(2.3748), tensor(1.5130), tensor(1.5501), tensor(1.4653), tensor(1.4950), tensor(1.6263), tensor(1.4953), tensor(1.4415), tensor(1.4554), tensor(1.5161), tensor(1.4654), tensor(2.0225), tensor(1.4966), tensor(1.4631), tensor(1.9805), tensor(1.4920), tensor(1.6832), tensor(1.6799), tensor(1.5139), tensor(1.4410), tensor(1.4969), tensor(1.5094), tensor(1.4932), tensor(2.1310), tensor(1.5486), tensor(1.5129), tensor(1.4988), tensor(1.4986), tensor(1.4665), tensor(1.5054), tensor(3.6037), tensor(1.4858), tensor(1.4925), tensor(1.5054), tensor(1.4906), tensor(1.4448), tensor(1.6627), tensor(1.4973), tensor(1.4353), tensor(1.5659), tensor(1.4570), tensor(1.4429)]
09/23/2021 13:50:15 - INFO - __main__ - after_losses=[tensor(1.5148), tensor(1.4924), tensor(1.4385), tensor(1.4755), tensor(1.5032), tensor(1.5477), tensor(1.5187), tensor(1.5054), tensor(2.0009), tensor(1.4961), tensor(1.4635), tensor(1.4946), tensor(1.8552), tensor(1.4504), tensor(2.8680), tensor(1.4660), tensor(1.5260), tensor(1.5591), tensor(1.5176), tensor(1.4894), tensor(1.6358), tensor(1.5203), tensor(1.5068), tensor(1.5508), tensor(1.4729), tensor(1.4912), tensor(1.4911), tensor(1.5110), tensor(1.5339), tensor(1.4730), tensor(1.5471), tensor(1.4758), tensor(1.4652), tensor(1.5175), tensor(1.8048), tensor(1.4673), tensor(1.4766), tensor(1.5222), tensor(1.5293), tensor(1.4958), tensor(1.6616), tensor(1.4669), tensor(1.4806), tensor(1.4948), tensor(1.5130), tensor(1.4727), tensor(1.5088), tensor(1.4719), tensor(1.8342), tensor(1.7017), tensor(1.5385), tensor(1.4774), tensor(1.5195), tensor(2.1999), tensor(1.5077), tensor(1.4820), tensor(1.4972), tensor(1.4844), tensor(2.0269), tensor(1.5023), tensor(1.7722), tensor(1.5259), tensor(1.5723), tensor(2.7477), tensor(1.7407), tensor(1.4913), tensor(1.4747), tensor(1.4683), tensor(1.6199), tensor(1.4962), tensor(1.4879), tensor(1.5063), tensor(1.4649), tensor(1.4623), tensor(1.5156), tensor(1.5147), tensor(1.4439), tensor(1.4667), tensor(1.4869), tensor(1.4444), tensor(1.5184), tensor(1.5497), tensor(1.4968), tensor(1.5950), tensor(1.4843), tensor(1.4872), tensor(1.8209), tensor(1.6381), tensor(1.4990), tensor(1.4907), tensor(1.5277), tensor(1.6778), tensor(2.1334), tensor(1.4887), tensor(1.5029), tensor(1.5711), tensor(1.4647), tensor(1.4916), tensor(2.1502), tensor(1.7953), tensor(1.5553), tensor(1.4546), tensor(2.9484), tensor(1.4493), tensor(1.8012), tensor(1.4519), tensor(1.4810), tensor(1.4909), tensor(1.5450), tensor(1.5214), tensor(1.4820), tensor(1.4735), tensor(1.4510), tensor(1.4599), tensor(1.4867), tensor(1.6161), tensor(2.5622), tensor(1.4865), tensor(1.5089), tensor(1.7404), tensor(1.4608), tensor(3.1096), tensor(1.4858), tensor(1.7794), tensor(1.4867), tensor(1.7232), tensor(1.5080), tensor(1.5471), tensor(1.4605), tensor(1.5264), tensor(2.3748), tensor(1.5130), tensor(1.5501), tensor(1.4653), tensor(1.4950), tensor(1.6263), tensor(1.4953), tensor(1.4415), tensor(1.4554), tensor(1.5161), tensor(1.4654), tensor(2.0225), tensor(1.4966), tensor(1.4631), tensor(1.9805), tensor(1.4920), tensor(1.6832), tensor(1.6799), tensor(1.5139), tensor(1.4410), tensor(1.4969), tensor(1.5094), tensor(1.4932), tensor(2.1310), tensor(1.5486), tensor(1.5129), tensor(1.4988), tensor(1.4986), tensor(1.4665), tensor(1.5054), tensor(3.6037), tensor(1.4858), tensor(1.4925), tensor(1.5054), tensor(1.4906), tensor(1.4448), tensor(1.6627), tensor(1.4973), tensor(1.4353), tensor(1.5659), tensor(1.4570), tensor(1.4429)]
09/23/2021 13:50:15 - INFO - __main__ - interference_scores=[tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)]
09/23/2021 13:50:15 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-5972', 'mrqa_naturalquestions-validation-9688', 'mrqa_squad-validation-2191', 'mrqa_hotpotqa-validation-3978', 'mrqa_squad-validation-7821', 'mrqa_triviaqa-validation-5937', 'mrqa_naturalquestions-validation-5818', 'mrqa_squad-validation-9358', 'mrqa_squad-validation-6677', 'mrqa_hotpotqa-validation-5802', 'mrqa_naturalquestions-validation-8948', 'mrqa_hotpotqa-validation-2263', 'mrqa_squad-validation-2629', 'mrqa_naturalquestions-validation-868', 'mrqa_triviaqa-validation-1616', 'mrqa_squad-validation-4283', 'mrqa_squad-validation-5622', 'mrqa_triviaqa-validation-4054', 'mrqa_squad-validation-8513', 'mrqa_naturalquestions-validation-2020', 'mrqa_triviaqa-validation-2210', 'mrqa_squad-validation-1516', 'mrqa_hotpotqa-validation-1201', 'mrqa_triviaqa-validation-3339', 'mrqa_squad-validation-6399', 'mrqa_hotpotqa-validation-14', 'mrqa_triviaqa-validation-893', 'mrqa_naturalquestions-validation-8448', 'mrqa_naturalquestions-validation-2571', 'mrqa_squad-validation-10410', 'mrqa_triviaqa-validation-7369', 'mrqa_naturalquestions-validation-2385']
09/23/2021 13:50:15 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:50:15 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=61) .... Timecode: 6
09/23/2021 13:50:23 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:50:24 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:50:24 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:50:34 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:50:35 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:50:35 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:50:40 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:50:43 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:50:43 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:50:43 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:50:44 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:50:47 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:50:50 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:50:50 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:50:50 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:50:50 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:50:50 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:50:50 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:50:52 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:50:52 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:50:52 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:50:52 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:50:52 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:50:52 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:50:52 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:50:52 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:50:56 - INFO - __main__ - Finished.
09/23/2021 13:50:56 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:50:56 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:50:56 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:50:56 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:50:56 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:50:58 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:50:58 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:50:58 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:51:03 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:51:03 - INFO - __main__ - Found 32 errors.
09/23/2021 13:51:03 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:51:09 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:51:09 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:51:12 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:51:12 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:51:12 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:51:14 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:51:14 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:51:14 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:51:14 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:51:14 - INFO - __main__ - Finished.
09/23/2021 13:51:14 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:51:14 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:51:14 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:51:18 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:51:18 - INFO - __main__ - Found 26 errors.
09/23/2021 13:51:18 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:51:18 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:51:18 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:51:19 - INFO - __main__ - model update
09/23/2021 13:51:19 - INFO - __main__ - model update
09/23/2021 13:51:20 - INFO - __main__ - model update
09/23/2021 13:51:20 - INFO - __main__ - model update
09/23/2021 13:51:20 - INFO - __main__ - model update
09/23/2021 13:51:21 - INFO - __main__ - model update
09/23/2021 13:51:21 - INFO - __main__ - model update
09/23/2021 13:51:21 - INFO - __main__ - model update
09/23/2021 13:51:22 - INFO - __main__ - model update
09/23/2021 13:51:22 - INFO - __main__ - model update
09/23/2021 13:51:22 - INFO - __main__ - model update
09/23/2021 13:51:22 - INFO - __main__ - model update
09/23/2021 13:51:23 - INFO - __main__ - before_losses=[tensor(1.5589), tensor(1.5005), tensor(2.1458), tensor(1.4888), tensor(1.5770), tensor(1.5163), tensor(1.4772), tensor(2.3435), tensor(2.1019), tensor(1.5249), tensor(2.9214), tensor(1.8106), tensor(2.3314), tensor(1.8749), tensor(1.5362), tensor(1.4525), tensor(1.5706), tensor(1.5381), tensor(1.6300), tensor(1.5215), tensor(1.5599), tensor(1.5514), tensor(1.5321), tensor(1.5203), tensor(1.5773), tensor(2.0851), tensor(2.6389), tensor(1.5650), tensor(1.4551), tensor(1.8541), tensor(1.5547), tensor(1.6852)]
09/23/2021 13:51:23 - INFO - __main__ - after_losses=[tensor(1.6965), tensor(1.4899), tensor(1.5662), tensor(1.4617), tensor(1.4979), tensor(1.4937), tensor(1.4614), tensor(2.0322), tensor(3.5988), tensor(1.5028), tensor(3.0618), tensor(1.4866), tensor(2.3907), tensor(1.6218), tensor(1.5151), tensor(1.4439), tensor(1.6342), tensor(1.5213), tensor(1.5024), tensor(1.4776), tensor(1.5422), tensor(1.5075), tensor(1.6629), tensor(1.4626), tensor(1.9873), tensor(1.5125), tensor(1.7068), tensor(1.4957), tensor(1.4437), tensor(1.5608), tensor(1.6155), tensor(1.5283)]
09/23/2021 13:51:23 - INFO - __main__ - interference_scores=[tensor(0.1376), tensor(-0.0106), tensor(-0.5796), tensor(-0.0271), tensor(-0.0791), tensor(-0.0225), tensor(-0.0158), tensor(-0.3113), tensor(1.4969), tensor(-0.0222), tensor(0.1404), tensor(-0.3240), tensor(0.0593), tensor(-0.2531), tensor(-0.0211), tensor(-0.0085), tensor(0.0636), tensor(-0.0168), tensor(-0.1275), tensor(-0.0439), tensor(-0.0177), tensor(-0.0439), tensor(0.1307), tensor(-0.0577), tensor(0.4100), tensor(-0.5726), tensor(-0.9321), tensor(-0.0692), tensor(-0.0113), tensor(-0.2933), tensor(0.0608), tensor(-0.1569)]
09/23/2021 13:51:23 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-1616', 'mrqa_squad-validation-392', 'mrqa_triviaqa-validation-2376', 'mrqa_squad-validation-7746', 'mrqa_naturalquestions-validation-7017', 'mrqa_triviaqa-validation-4054', 'mrqa_squad-validation-5622', 'mrqa_naturalquestions-validation-3490', 'mrqa_hotpotqa-validation-3241', 'mrqa_hotpotqa-validation-409', 'mrqa_naturalquestions-validation-3028', 'mrqa_hotpotqa-validation-1968', 'mrqa_squad-validation-4185', 'mrqa_triviaqa-validation-5972', 'mrqa_triviaqa-validation-6351', 'mrqa_naturalquestions-validation-2730', 'mrqa_hotpotqa-validation-2970', 'mrqa_naturalquestions-validation-6157', 'mrqa_naturalquestions-validation-5465', 'mrqa_naturalquestions-validation-8948', 'mrqa_squad-validation-3478', 'mrqa_squad-validation-6303', 'mrqa_naturalquestions-validation-9688', 'mrqa_triviaqa-validation-671', 'mrqa_squad-validation-7821', 'mrqa_triviaqa-validation-7369', 'mrqa_triviaqa-validation-1551', 'mrqa_triviaqa-validation-3915', 'mrqa_naturalquestions-validation-2248', 'mrqa_squad-validation-10322', 'mrqa_triviaqa-validation-5937', 'mrqa_squad-validation-6677']
09/23/2021 13:51:23 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:51:23 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 1
09/23/2021 13:51:36 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:51:36 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 1
09/23/2021 13:51:41 - INFO - __main__ - After Error Fixing: 0.875
09/23/2021 13:51:41 - INFO - __main__ - Instant Fixing Rate: 1.0
09/23/2021 13:51:41 - INFO - __main__ - Instant Retention Rate: 0.33333333277777777
09/23/2021 13:51:42 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_002.pt.
09/23/2021 13:51:43 - INFO - __main__ - Saving the current error examples (len=26) to the memory.
09/23/2021 13:51:43 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:51:43 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:51:43 - INFO - __main__ - Finished.
09/23/2021 13:51:43 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:51:43 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:51:43 - INFO - __main__ - Evaluating to get errors .... Timecode: 2
09/23/2021 13:51:46 - INFO - __main__ - Before Error Fixing: 0.09375
09/23/2021 13:51:46 - INFO - __main__ - Found 29 errors.
09/23/2021 13:51:46 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:51:46 - INFO - __main__ - Current memory size: 58.
09/23/2021 13:51:46 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=58;
09/23/2021 13:51:48 - INFO - __main__ - model update
09/23/2021 13:51:49 - INFO - __main__ - model update
09/23/2021 13:51:49 - INFO - __main__ - model update
09/23/2021 13:51:50 - INFO - __main__ - model update
09/23/2021 13:51:50 - INFO - __main__ - model update
09/23/2021 13:51:50 - INFO - __main__ - model update
09/23/2021 13:51:51 - INFO - __main__ - model update
09/23/2021 13:51:51 - INFO - __main__ - model update
09/23/2021 13:51:51 - INFO - __main__ - model update
09/23/2021 13:51:51 - INFO - __main__ - model update
09/23/2021 13:51:52 - INFO - __main__ - model update
09/23/2021 13:51:52 - INFO - __main__ - model update
09/23/2021 13:51:52 - INFO - __main__ - model update
09/23/2021 13:51:53 - INFO - __main__ - model update
09/23/2021 13:51:53 - INFO - __main__ - model update
09/23/2021 13:51:53 - INFO - __main__ - model update
09/23/2021 13:51:54 - INFO - __main__ - model update
09/23/2021 13:51:54 - INFO - __main__ - model update
09/23/2021 13:51:54 - INFO - __main__ - model update
09/23/2021 13:51:55 - INFO - __main__ - model update
09/23/2021 13:51:55 - INFO - __main__ - model update
09/23/2021 13:51:55 - INFO - __main__ - model update
09/23/2021 13:51:56 - INFO - __main__ - model update
09/23/2021 13:51:56 - INFO - __main__ - model update
09/23/2021 13:51:58 - INFO - __main__ - before_losses=[tensor(1.5514), tensor(1.7556), tensor(1.5753), tensor(1.4537), tensor(1.5266), tensor(1.7264), tensor(1.5400), tensor(1.5107), tensor(2.1815), tensor(1.5674), tensor(1.5100), tensor(1.4918), tensor(1.8628), tensor(1.6070), tensor(1.4439), tensor(1.5376), tensor(1.5584), tensor(1.4801), tensor(1.5284), tensor(1.4832), tensor(1.4646), tensor(1.6595), tensor(1.5079), tensor(1.5036), tensor(1.5681), tensor(1.4995), tensor(1.5319), tensor(1.5354), tensor(1.5677), tensor(1.5348), tensor(1.7500), tensor(1.5239), tensor(1.4827), tensor(1.5002), tensor(1.5116), tensor(1.6521), tensor(1.9511), tensor(1.4733), tensor(1.5130), tensor(2.7494), tensor(1.6398), tensor(1.5460), tensor(1.5770), tensor(4.4306), tensor(1.4952), tensor(1.4808), tensor(1.5217), tensor(1.5029), tensor(1.5577), tensor(1.5158), tensor(1.8453), tensor(1.9214), tensor(1.9702), tensor(1.4430), tensor(1.5221), tensor(1.5177), tensor(1.5864), tensor(1.8825)]
09/23/2021 13:51:58 - INFO - __main__ - after_losses=[tensor(1.5120), tensor(1.5841), tensor(1.5838), tensor(1.4601), tensor(1.5103), tensor(1.5521), tensor(1.5078), tensor(1.5117), tensor(1.5397), tensor(1.5414), tensor(2.2056), tensor(1.4826), tensor(1.4945), tensor(1.5304), tensor(1.4449), tensor(1.5153), tensor(3.5191), tensor(1.5057), tensor(1.4712), tensor(1.4852), tensor(1.4565), tensor(1.5825), tensor(1.4633), tensor(1.4649), tensor(1.5307), tensor(1.4847), tensor(1.5388), tensor(1.5464), tensor(1.5025), tensor(1.5197), tensor(2.4025), tensor(1.5063), tensor(1.4639), tensor(1.5058), tensor(1.4711), tensor(1.5493), tensor(2.8904), tensor(1.4585), tensor(1.4945), tensor(2.6220), tensor(1.7754), tensor(1.5477), tensor(1.5847), tensor(2.0838), tensor(1.4781), tensor(1.4653), tensor(2.3208), tensor(1.4839), tensor(1.8463), tensor(1.5045), tensor(1.4728), tensor(1.5593), tensor(1.5360), tensor(1.4642), tensor(1.5004), tensor(1.5023), tensor(1.5250), tensor(1.5708)]
09/23/2021 13:51:58 - INFO - __main__ - interference_scores=[tensor(-0.0394), tensor(-0.1714), tensor(0.0086), tensor(0.0064), tensor(-0.0164), tensor(-0.1744), tensor(-0.0323), tensor(0.0010), tensor(-0.6417), tensor(-0.0260), tensor(0.6955), tensor(-0.0092), tensor(-0.3683), tensor(-0.0765), tensor(0.0010), tensor(-0.0223), tensor(1.9608), tensor(0.0256), tensor(-0.0573), tensor(0.0020), tensor(-0.0081), tensor(-0.0771), tensor(-0.0446), tensor(-0.0387), tensor(-0.0375), tensor(-0.0148), tensor(0.0069), tensor(0.0110), tensor(-0.0652), tensor(-0.0151), tensor(0.6525), tensor(-0.0176), tensor(-0.0188), tensor(0.0056), tensor(-0.0405), tensor(-0.1028), tensor(0.9393), tensor(-0.0149), tensor(-0.0186), tensor(-0.1274), tensor(0.1356), tensor(0.0017), tensor(0.0076), tensor(-2.3468), tensor(-0.0171), tensor(-0.0155), tensor(0.7990), tensor(-0.0190), tensor(0.2886), tensor(-0.0112), tensor(-0.3725), tensor(-0.3621), tensor(-0.4342), tensor(0.0212), tensor(-0.0217), tensor(-0.0154), tensor(-0.0613), tensor(-0.3117)]
09/23/2021 13:51:58 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-1616', 'mrqa_squad-validation-10015', 'mrqa_squad-validation-392', 'mrqa_squad-validation-5622', 'mrqa_triviaqa-validation-1551', 'mrqa_naturalquestions-validation-2248', 'mrqa_triviaqa-validation-3915', 'mrqa_hotpotqa-validation-1201', 'mrqa_hotpotqa-validation-3241', 'mrqa_hotpotqa-validation-3971', 'mrqa_triviaqa-validation-7369', 'mrqa_triviaqa-validation-4054', 'mrqa_triviaqa-validation-1437', 'mrqa_hotpotqa-validation-1968', 'mrqa_naturalquestions-validation-1364', 'mrqa_naturalquestions-validation-9688', 'mrqa_triviaqa-validation-5972', 'mrqa_naturalquestions-validation-3028', 'mrqa_naturalquestions-validation-5144', 'mrqa_hotpotqa-validation-1626', 'mrqa_naturalquestions-validation-2753', 'mrqa_hotpotqa-validation-2970', 'mrqa_hotpotqa-validation-409', 'mrqa_naturalquestions-validation-6157', 'mrqa_squad-validation-7821', 'mrqa_naturalquestions-validation-5465', 'mrqa_naturalquestions-validation-9386', 'mrqa_triviaqa-validation-671', 'mrqa_naturalquestions-validation-8948', 'mrqa_naturalquestions-validation-7511', 'mrqa_naturalquestions-validation-2730', 'mrqa_hotpotqa-validation-5325']
09/23/2021 13:51:58 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:51:58 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=61) .... Timecode: 2
09/23/2021 13:52:11 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:52:11 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 2
09/23/2021 13:52:14 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 13:52:14 - INFO - __main__ - Instant Fixing Rate: 0.9655172413793104
09/23/2021 13:52:14 - INFO - __main__ - Instant Retention Rate: 0.3333333322222222
09/23/2021 13:52:16 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_003.pt.
09/23/2021 13:52:16 - INFO - __main__ - Saving the current error examples (len=29) to the memory.
09/23/2021 13:52:16 - INFO - __main__ - Current memory size: 58.
09/23/2021 13:52:16 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:52:16 - INFO - __main__ - Finished.
09/23/2021 13:52:16 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:52:16 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:52:16 - INFO - __main__ - Evaluating to get errors .... Timecode: 3
09/23/2021 13:52:19 - INFO - __main__ - Before Error Fixing: 0.125
09/23/2021 13:52:19 - INFO - __main__ - Found 28 errors.
09/23/2021 13:52:19 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:52:19 - INFO - __main__ - Current memory size: 87.
09/23/2021 13:52:19 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=87;
09/23/2021 13:52:22 - INFO - __main__ - model update
09/23/2021 13:52:22 - INFO - __main__ - model update
09/23/2021 13:52:22 - INFO - __main__ - model update
09/23/2021 13:52:23 - INFO - __main__ - model update
09/23/2021 13:52:23 - INFO - __main__ - model update
09/23/2021 13:52:23 - INFO - __main__ - model update
09/23/2021 13:52:24 - INFO - __main__ - model update
09/23/2021 13:52:24 - INFO - __main__ - model update
09/23/2021 13:52:24 - INFO - __main__ - model update
09/23/2021 13:52:25 - INFO - __main__ - model update
09/23/2021 13:52:25 - INFO - __main__ - model update
09/23/2021 13:52:25 - INFO - __main__ - model update
09/23/2021 13:52:26 - INFO - __main__ - model update
09/23/2021 13:52:26 - INFO - __main__ - model update
09/23/2021 13:52:26 - INFO - __main__ - model update
09/23/2021 13:52:27 - INFO - __main__ - model update
09/23/2021 13:52:27 - INFO - __main__ - model update
09/23/2021 13:52:27 - INFO - __main__ - model update
09/23/2021 13:52:27 - INFO - __main__ - model update
09/23/2021 13:52:28 - INFO - __main__ - model update
09/23/2021 13:52:28 - INFO - __main__ - model update
09/23/2021 13:52:28 - INFO - __main__ - model update
09/23/2021 13:52:29 - INFO - __main__ - model update
09/23/2021 13:52:29 - INFO - __main__ - model update
09/23/2021 13:52:31 - INFO - __main__ - before_losses=[tensor(1.5830), tensor(1.6166), tensor(1.9750), tensor(1.4979), tensor(1.4594), tensor(1.4845), tensor(2.9533), tensor(1.8086), tensor(1.4660), tensor(1.4838), tensor(1.4897), tensor(1.6094), tensor(1.5601), tensor(1.4997), tensor(1.5216), tensor(4.8277), tensor(1.5290), tensor(1.4779), tensor(2.0212), tensor(1.4999), tensor(1.5025), tensor(1.9821), tensor(2.4896), tensor(2.4936), tensor(1.8107), tensor(1.8996), tensor(1.5273), tensor(2.5789), tensor(1.5210), tensor(2.1911), tensor(1.4753), tensor(1.4890), tensor(2.6366), tensor(1.5413), tensor(1.4951), tensor(1.5254), tensor(1.5634), tensor(1.5039), tensor(1.5644), tensor(1.4749), tensor(1.4856), tensor(1.5488), tensor(1.5092), tensor(1.4969), tensor(1.6874), tensor(1.5145), tensor(1.4990), tensor(1.4519), tensor(1.4902), tensor(1.5161), tensor(1.4769), tensor(1.4962), tensor(1.5143), tensor(1.5242), tensor(1.5345), tensor(1.4403), tensor(1.5323), tensor(1.9246), tensor(1.5094), tensor(1.4735), tensor(1.4561), tensor(1.8113), tensor(1.4560), tensor(1.5800), tensor(1.4807), tensor(1.4728), tensor(1.5600), tensor(1.4578), tensor(1.5078), tensor(2.4185), tensor(1.4696), tensor(2.0168), tensor(1.4505), tensor(1.4921), tensor(1.4495), tensor(1.8058), tensor(1.5036), tensor(1.4812), tensor(2.2707), tensor(1.4757), tensor(1.4729), tensor(1.4564), tensor(1.4963), tensor(1.9973), tensor(1.5249), tensor(1.5121), tensor(1.5257)]
09/23/2021 13:52:31 - INFO - __main__ - after_losses=[tensor(1.6754), tensor(1.7853), tensor(1.4826), tensor(1.4999), tensor(1.4586), tensor(1.5046), tensor(2.8080), tensor(2.5109), tensor(1.4707), tensor(1.4683), tensor(1.4719), tensor(1.7173), tensor(1.8567), tensor(1.4710), tensor(1.4966), tensor(4.4120), tensor(1.5115), tensor(1.4726), tensor(3.6343), tensor(1.4932), tensor(1.4857), tensor(1.6120), tensor(2.0897), tensor(1.4703), tensor(1.9868), tensor(2.1244), tensor(1.5076), tensor(1.5089), tensor(1.4741), tensor(2.0681), tensor(1.4553), tensor(1.4653), tensor(1.5511), tensor(1.4833), tensor(1.5040), tensor(1.5143), tensor(1.5948), tensor(1.4794), tensor(1.5150), tensor(1.4855), tensor(1.4585), tensor(1.5329), tensor(1.4733), tensor(1.4804), tensor(1.6987), tensor(1.4811), tensor(1.4577), tensor(1.4384), tensor(1.4880), tensor(1.4781), tensor(1.4715), tensor(1.5066), tensor(1.4922), tensor(1.4751), tensor(1.5498), tensor(1.4354), tensor(1.5202), tensor(1.8759), tensor(3.0427), tensor(1.4705), tensor(1.4488), tensor(1.9950), tensor(1.5346), tensor(1.5516), tensor(1.4704), tensor(2.2271), tensor(1.5840), tensor(1.4469), tensor(1.5105), tensor(2.4514), tensor(1.4520), tensor(2.0861), tensor(1.4511), tensor(1.4866), tensor(1.4434), tensor(1.7529), tensor(1.4871), tensor(1.4568), tensor(2.2531), tensor(1.4539), tensor(1.4629), tensor(1.4717), tensor(1.4662), tensor(2.3697), tensor(1.5009), tensor(1.6876), tensor(1.4944)]
09/23/2021 13:52:31 - INFO - __main__ - interference_scores=[tensor(0.0923), tensor(0.1687), tensor(-0.4924), tensor(0.0021), tensor(-0.0007), tensor(0.0201), tensor(-0.1453), tensor(0.7023), tensor(0.0047), tensor(-0.0155), tensor(-0.0179), tensor(0.1079), tensor(0.2965), tensor(-0.0287), tensor(-0.0250), tensor(-0.4157), tensor(-0.0175), tensor(-0.0053), tensor(1.6131), tensor(-0.0066), tensor(-0.0168), tensor(-0.3702), tensor(-0.3999), tensor(-1.0233), tensor(0.1761), tensor(0.2247), tensor(-0.0197), tensor(-1.0699), tensor(-0.0469), tensor(-0.1230), tensor(-0.0199), tensor(-0.0237), tensor(-1.0856), tensor(-0.0580), tensor(0.0088), tensor(-0.0111), tensor(0.0314), tensor(-0.0244), tensor(-0.0494), tensor(0.0106), tensor(-0.0271), tensor(-0.0159), tensor(-0.0359), tensor(-0.0164), tensor(0.0114), tensor(-0.0335), tensor(-0.0413), tensor(-0.0135), tensor(-0.0022), tensor(-0.0380), tensor(-0.0054), tensor(0.0104), tensor(-0.0221), tensor(-0.0490), tensor(0.0153), tensor(-0.0049), tensor(-0.0120), tensor(-0.0487), tensor(1.5332), tensor(-0.0030), tensor(-0.0073), tensor(0.1838), tensor(0.0785), tensor(-0.0284), tensor(-0.0102), tensor(0.7543), tensor(0.0240), tensor(-0.0109), tensor(0.0026), tensor(0.0329), tensor(-0.0175), tensor(0.0694), tensor(0.0006), tensor(-0.0054), tensor(-0.0061), tensor(-0.0529), tensor(-0.0165), tensor(-0.0245), tensor(-0.0176), tensor(-0.0218), tensor(-0.0100), tensor(0.0153), tensor(-0.0301), tensor(0.3724), tensor(-0.0239), tensor(0.1755), tensor(-0.0313)]
09/23/2021 13:52:31 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-1616', 'mrqa_triviaqa-validation-4729', 'mrqa_naturalquestions-validation-5180', 'mrqa_triviaqa-validation-2210', 'mrqa_naturalquestions-validation-10680', 'mrqa_triviaqa-validation-7018', 'mrqa_triviaqa-validation-1924', 'mrqa_squad-validation-6677', 'mrqa_squad-validation-392', 'mrqa_triviaqa-validation-5972', 'mrqa_triviaqa-validation-3915', 'mrqa_triviaqa-validation-2096', 'mrqa_triviaqa-validation-4856', 'mrqa_naturalquestions-validation-114', 'mrqa_triviaqa-validation-5937', 'mrqa_triviaqa-validation-5406', 'mrqa_triviaqa-validation-7369', 'mrqa_triviaqa-validation-1437', 'mrqa_squad-validation-2629', 'mrqa_squad-validation-4253', 'mrqa_triviaqa-validation-1551', 'mrqa_squad-validation-7746', 'mrqa_naturalquestions-validation-2753', 'mrqa_naturalquestions-validation-5144', 'mrqa_naturalquestions-validation-1364', 'mrqa_naturalquestions-validation-9386', 'mrqa_squad-validation-8513', 'mrqa_triviaqa-validation-2722', 'mrqa_hotpotqa-validation-1968', 'mrqa_hotpotqa-validation-5325', 'mrqa_hotpotqa-validation-5699', 'mrqa_naturalquestions-validation-10364']
09/23/2021 13:52:31 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:52:31 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=60) .... Timecode: 3
09/23/2021 13:52:44 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:52:44 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 3
09/23/2021 13:52:48 - INFO - __main__ - After Error Fixing: 0.9375
09/23/2021 13:52:48 - INFO - __main__ - Instant Fixing Rate: 0.9642857142857143
09/23/2021 13:52:48 - INFO - __main__ - Instant Retention Rate: 0.7499999981250001
09/23/2021 13:52:50 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_004.pt.
09/23/2021 13:52:50 - INFO - __main__ - Saving the current error examples (len=28) to the memory.
09/23/2021 13:52:50 - INFO - __main__ - Current memory size: 87.
09/23/2021 13:52:50 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:52:50 - INFO - __main__ - Finished.
09/23/2021 13:52:50 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:52:50 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:52:50 - INFO - __main__ - Evaluating to get errors .... Timecode: 4
09/23/2021 13:52:52 - INFO - __main__ - Before Error Fixing: 0.28125
09/23/2021 13:52:52 - INFO - __main__ - Found 23 errors.
09/23/2021 13:52:52 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:52:52 - INFO - __main__ - Current memory size: 115.
09/23/2021 13:52:52 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=115;
09/23/2021 13:52:56 - INFO - __main__ - model update
09/23/2021 13:52:56 - INFO - __main__ - model update
09/23/2021 13:52:57 - INFO - __main__ - model update
09/23/2021 13:52:57 - INFO - __main__ - model update
09/23/2021 13:52:57 - INFO - __main__ - model update
09/23/2021 13:52:58 - INFO - __main__ - model update
09/23/2021 13:52:58 - INFO - __main__ - model update
09/23/2021 13:52:58 - INFO - __main__ - model update
09/23/2021 13:52:59 - INFO - __main__ - model update
09/23/2021 13:52:59 - INFO - __main__ - model update
09/23/2021 13:52:59 - INFO - __main__ - model update
09/23/2021 13:53:00 - INFO - __main__ - model update
09/23/2021 13:53:00 - INFO - __main__ - model update
09/23/2021 13:53:00 - INFO - __main__ - model update
09/23/2021 13:53:01 - INFO - __main__ - model update
09/23/2021 13:53:01 - INFO - __main__ - model update
09/23/2021 13:53:01 - INFO - __main__ - model update
09/23/2021 13:53:01 - INFO - __main__ - model update
09/23/2021 13:53:02 - INFO - __main__ - model update
09/23/2021 13:53:02 - INFO - __main__ - model update
09/23/2021 13:53:02 - INFO - __main__ - model update
09/23/2021 13:53:03 - INFO - __main__ - model update
09/23/2021 13:53:03 - INFO - __main__ - model update
09/23/2021 13:53:03 - INFO - __main__ - model update
09/23/2021 13:53:06 - INFO - __main__ - before_losses=[tensor(1.5318), tensor(1.5369), tensor(1.9505), tensor(1.5886), tensor(1.4538), tensor(2.9488), tensor(2.5193), tensor(1.4835), tensor(1.5831), tensor(1.5117), tensor(1.5575), tensor(1.5476), tensor(1.5666), tensor(1.5432), tensor(1.4687), tensor(1.4556), tensor(1.5706), tensor(1.4485), tensor(1.5630), tensor(1.4542), tensor(1.5411), tensor(1.5089), tensor(1.5445), tensor(1.5102), tensor(1.4809), tensor(1.4942), tensor(1.5432), tensor(1.5055), tensor(2.1602), tensor(1.5617), tensor(1.5272), tensor(1.4883), tensor(1.4576), tensor(1.4683), tensor(1.5236), tensor(1.5151), tensor(1.4834), tensor(1.7253), tensor(2.1768), tensor(2.5921), tensor(1.5241), tensor(2.2116), tensor(1.4704), tensor(1.5092), tensor(1.5108), tensor(1.4743), tensor(1.5141), tensor(1.5413), tensor(1.5076), tensor(1.5404), tensor(1.7545), tensor(1.5181), tensor(1.5177), tensor(1.4952), tensor(2.0788), tensor(1.6560), tensor(1.4770), tensor(1.6064), tensor(1.5987), tensor(1.5447), tensor(1.5150), tensor(1.4857), tensor(1.5424), tensor(1.4577), tensor(1.5346), tensor(1.4997), tensor(2.5659), tensor(1.5501), tensor(1.5446), tensor(1.5617), tensor(1.5274), tensor(1.6478), tensor(1.5016), tensor(2.2555), tensor(1.5378), tensor(1.4850), tensor(1.5276), tensor(1.5039), tensor(1.4837), tensor(1.4788), tensor(1.5108), tensor(1.4633), tensor(1.4642), tensor(2.7198), tensor(1.4914), tensor(1.5911), tensor(2.0821), tensor(1.4904), tensor(1.5642), tensor(1.5181), tensor(1.4998), tensor(1.5034), tensor(1.4981), tensor(1.5260), tensor(1.5143), tensor(1.5338), tensor(2.0512), tensor(1.4598), tensor(1.4822), tensor(1.4956), tensor(4.6283), tensor(1.7607), tensor(1.4844), tensor(1.5023), tensor(1.5302), tensor(1.4940), tensor(1.5426), tensor(2.9523), tensor(1.9470), tensor(2.0833), tensor(1.5760), tensor(1.5708), tensor(1.4894), tensor(1.5110), tensor(1.4437)]
09/23/2021 13:53:06 - INFO - __main__ - after_losses=[tensor(1.5198), tensor(1.5387), tensor(1.9999), tensor(2.1819), tensor(1.4489), tensor(2.8951), tensor(1.5147), tensor(1.4713), tensor(1.5240), tensor(1.5097), tensor(1.5022), tensor(1.5548), tensor(1.5814), tensor(1.5612), tensor(1.4566), tensor(1.5056), tensor(1.4491), tensor(1.4443), tensor(1.5477), tensor(1.4650), tensor(1.5270), tensor(1.4710), tensor(1.5376), tensor(1.5206), tensor(1.4727), tensor(1.4817), tensor(1.5313), tensor(1.5222), tensor(2.2198), tensor(1.5322), tensor(1.5288), tensor(1.4807), tensor(1.4507), tensor(1.4653), tensor(1.5116), tensor(1.5160), tensor(1.4744), tensor(1.6254), tensor(1.6448), tensor(2.5937), tensor(1.5093), tensor(2.2090), tensor(1.4541), tensor(1.5017), tensor(1.5064), tensor(1.4736), tensor(1.4963), tensor(1.5329), tensor(1.5020), tensor(1.5129), tensor(2.1117), tensor(1.5144), tensor(1.5077), tensor(1.4847), tensor(1.4942), tensor(1.7059), tensor(1.4687), tensor(1.5554), tensor(1.5814), tensor(1.5567), tensor(1.4976), tensor(1.4551), tensor(1.5226), tensor(1.4653), tensor(1.6354), tensor(1.4869), tensor(2.6208), tensor(1.5253), tensor(1.4890), tensor(1.5354), tensor(1.4906), tensor(1.7370), tensor(1.4876), tensor(2.2890), tensor(1.5345), tensor(1.4785), tensor(1.5079), tensor(1.4903), tensor(1.4741), tensor(1.4691), tensor(1.4986), tensor(1.4704), tensor(1.4587), tensor(2.5895), tensor(1.4783), tensor(1.5306), tensor(1.5114), tensor(1.4821), tensor(1.5496), tensor(1.5260), tensor(1.4721), tensor(1.5038), tensor(1.4874), tensor(1.5116), tensor(1.5062), tensor(1.5134), tensor(2.5407), tensor(1.4579), tensor(1.4890), tensor(2.1736), tensor(4.6619), tensor(1.6530), tensor(1.7466), tensor(1.5246), tensor(1.5155), tensor(1.4752), tensor(1.5294), tensor(2.7131), tensor(1.8451), tensor(2.1601), tensor(1.5485), tensor(1.5535), tensor(1.4835), tensor(1.4891), tensor(1.4386)]
09/23/2021 13:53:06 - INFO - __main__ - interference_scores=[tensor(-0.0120), tensor(0.0017), tensor(0.0494), tensor(0.5933), tensor(-0.0049), tensor(-0.0537), tensor(-1.0046), tensor(-0.0123), tensor(-0.0590), tensor(-0.0021), tensor(-0.0553), tensor(0.0071), tensor(0.0148), tensor(0.0180), tensor(-0.0121), tensor(0.0500), tensor(-0.1214), tensor(-0.0042), tensor(-0.0153), tensor(0.0109), tensor(-0.0141), tensor(-0.0379), tensor(-0.0070), tensor(0.0105), tensor(-0.0082), tensor(-0.0124), tensor(-0.0119), tensor(0.0168), tensor(0.0596), tensor(-0.0296), tensor(0.0016), tensor(-0.0076), tensor(-0.0069), tensor(-0.0030), tensor(-0.0120), tensor(0.0009), tensor(-0.0090), tensor(-0.0999), tensor(-0.5319), tensor(0.0015), tensor(-0.0149), tensor(-0.0026), tensor(-0.0163), tensor(-0.0075), tensor(-0.0044), tensor(-0.0006), tensor(-0.0178), tensor(-0.0084), tensor(-0.0055), tensor(-0.0274), tensor(0.3573), tensor(-0.0038), tensor(-0.0100), tensor(-0.0105), tensor(-0.5847), tensor(0.0499), tensor(-0.0082), tensor(-0.0510), tensor(-0.0173), tensor(0.0120), tensor(-0.0174), tensor(-0.0306), tensor(-0.0199), tensor(0.0076), tensor(0.1008), tensor(-0.0128), tensor(0.0549), tensor(-0.0248), tensor(-0.0556), tensor(-0.0263), tensor(-0.0368), tensor(0.0892), tensor(-0.0140), tensor(0.0335), tensor(-0.0033), tensor(-0.0065), tensor(-0.0197), tensor(-0.0136), tensor(-0.0096), tensor(-0.0097), tensor(-0.0122), tensor(0.0071), tensor(-0.0054), tensor(-0.1304), tensor(-0.0131), tensor(-0.0605), tensor(-0.5707), tensor(-0.0084), tensor(-0.0146), tensor(0.0078), tensor(-0.0277), tensor(0.0004), tensor(-0.0107), tensor(-0.0144), tensor(-0.0081), tensor(-0.0204), tensor(0.4895), tensor(-0.0019), tensor(0.0068), tensor(0.6780), tensor(0.0336), tensor(-0.1077), tensor(0.2622), tensor(0.0223), tensor(-0.0146), tensor(-0.0188), tensor(-0.0132), tensor(-0.2392), tensor(-0.1019), tensor(0.0769), tensor(-0.0276), tensor(-0.0173), tensor(-0.0060), tensor(-0.0219), tensor(-0.0052)]
09/23/2021 13:53:06 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-5972', 'mrqa_triviaqa-validation-5937', 'mrqa_triviaqa-validation-6402', 'mrqa_triviaqa-validation-1935', 'mrqa_squad-validation-6677', 'mrqa_triviaqa-validation-3901', 'mrqa_triviaqa-validation-2530', 'mrqa_squad-validation-1516', 'mrqa_triviaqa-validation-3915', 'mrqa_squad-validation-7149', 'mrqa_hotpotqa-validation-5325', 'mrqa_naturalquestions-validation-2501', 'mrqa_triviaqa-validation-893', 'mrqa_triviaqa-validation-2376', 'mrqa_squad-validation-1539', 'mrqa_triviaqa-validation-2327', 'mrqa_triviaqa-validation-5362', 'mrqa_triviaqa-validation-2136', 'mrqa_triviaqa-validation-6351', 'mrqa_naturalquestions-validation-8948', 'mrqa_hotpotqa-validation-1968', 'mrqa_triviaqa-validation-1437', 'mrqa_squad-validation-8513', 'mrqa_naturalquestions-validation-114', 'mrqa_hotpotqa-validation-2679', 'mrqa_squad-validation-4253', 'mrqa_hotpotqa-validation-2263', 'mrqa_triviaqa-validation-365', 'mrqa_naturalquestions-validation-3189', 'mrqa_squad-validation-5622', 'mrqa_naturalquestions-validation-5818', 'mrqa_naturalquestions-validation-8119']
09/23/2021 13:53:06 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:53:06 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=55) .... Timecode: 4
09/23/2021 13:53:19 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:53:19 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 4
09/23/2021 13:53:22 - INFO - __main__ - After Error Fixing: 0.875
09/23/2021 13:53:22 - INFO - __main__ - Instant Fixing Rate: 1.0
09/23/2021 13:53:22 - INFO - __main__ - Instant Retention Rate: 0.5555555549382716
09/23/2021 13:53:24 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_005.pt.
09/23/2021 13:53:24 - INFO - __main__ - Saving the current error examples (len=23) to the memory.
09/23/2021 13:53:24 - INFO - __main__ - Current memory size: 115.
09/23/2021 13:53:24 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:53:24 - INFO - __main__ - Finished.
09/23/2021 13:53:24 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:53:24 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:53:24 - INFO - __main__ - Evaluating to get errors .... Timecode: 5
09/23/2021 13:53:27 - INFO - __main__ - Before Error Fixing: 0.125
09/23/2021 13:53:27 - INFO - __main__ - Found 28 errors.
09/23/2021 13:53:27 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:53:27 - INFO - __main__ - Current memory size: 138.
09/23/2021 13:53:27 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=138;
09/23/2021 13:53:32 - INFO - __main__ - model update
09/23/2021 13:53:33 - INFO - __main__ - model update
09/23/2021 13:53:33 - INFO - __main__ - model update
09/23/2021 13:53:33 - INFO - __main__ - model update
09/23/2021 13:53:34 - INFO - __main__ - model update
09/23/2021 13:53:34 - INFO - __main__ - model update
09/23/2021 13:53:34 - INFO - __main__ - model update
09/23/2021 13:53:35 - INFO - __main__ - model update
09/23/2021 13:53:35 - INFO - __main__ - model update
09/23/2021 13:53:35 - INFO - __main__ - model update
09/23/2021 13:53:36 - INFO - __main__ - model update
09/23/2021 13:53:36 - INFO - __main__ - model update
09/23/2021 13:53:36 - INFO - __main__ - model update
09/23/2021 13:53:37 - INFO - __main__ - model update
09/23/2021 13:53:37 - INFO - __main__ - model update
09/23/2021 13:53:37 - INFO - __main__ - model update
09/23/2021 13:53:38 - INFO - __main__ - model update
09/23/2021 13:53:38 - INFO - __main__ - model update
09/23/2021 13:53:38 - INFO - __main__ - model update
09/23/2021 13:53:39 - INFO - __main__ - model update
09/23/2021 13:53:39 - INFO - __main__ - model update
09/23/2021 13:54:32 - INFO - __main__ - Namespace(adam_epsilon=1e-08, adapter_dim=32, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', cl_method_name='mir', current_thread_id=None, data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, ewc_gamma=1, ewc_lambda=0.5, example_encoder_name='roberta-base', freeze_embeds=False, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_beams=4, num_threads_eval=0, num_train_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', path_to_thread_result=None, predict_batch_size=16, prefix='0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA', replay_candidate_size=256, replay_frequency=1, replay_size=32, replay_stream_json_path='', result_file='exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', save_all_ckpts=1, seed=42, stream_mode='dynamic', task_emb_dim=768, task_name='mrqa_naturalquestions', train_batch_size=8, use_replay_mix=True, use_sampled_upstream=False, weight_decay=0.01)
09/23/2021 13:54:33 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:54:33 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:54:38 - INFO - __main__ - Not creating the replay-stream for evaluation.
09/23/2021 13:54:40 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
09/23/2021 13:54:44 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:54:44 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:54:44 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:54:48 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
09/23/2021 13:54:53 - INFO - __main__ - Moving to the GPUs.
09/23/2021 13:54:53 - INFO - __main__ - Debugger Setup ......
09/23/2021 13:54:53 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=False, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01) ......
09/23/2021 13:54:53 - INFO - __main__ - Debugger Setup ...... Done!
09/23/2021 13:54:53 - INFO - __main__ - Starting to load the key encoder (facebook/bart-base) for the memory module.
09/23/2021 13:54:53 - INFO - transformers.tokenization_utils - Model name 'facebook/bart-base' not found in model shortcut name list (bart-large, bart-large-mnli, bart-large-cnn, bart-large-xsum). Assuming 'facebook/bart-base' is a path, a model identifier, or url to a directory containing tokenizer files.
09/23/2021 13:54:55 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/7a7fc1746df9ea150ffd06a23351e5854c2105db3a0be1e0307dfd74fbf4a65c.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
09/23/2021 13:54:55 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/3d0d1735635ef097afbf08cf5af21618c94286b8e8b53cfb9bd6cdcfc91d4e14.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
09/23/2021 13:54:55 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/added_tokens.json from cache at None
09/23/2021 13:54:55 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/special_tokens_map.json from cache at None
09/23/2021 13:54:55 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/tokenizer_config.json from cache at None
09/23/2021 13:54:55 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
09/23/2021 13:54:55 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

09/23/2021 13:54:55 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
09/23/2021 13:55:02 - INFO - __main__ - Finished.
09/23/2021 13:55:02 - INFO - __main__ - Warning: exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl doesn't exist.
09/23/2021 13:55:02 - INFO - __main__ - Initializing an empty memory key cache.
09/23/2021 13:55:02 - INFO - __main__ - Start Online Debugging with Dynamic Error Mode
09/23/2021 13:55:02 - INFO - __main__ - Number of Batches of Data: 100
09/23/2021 13:55:02 - INFO - __main__ - Data Batch Size: 32;
09/23/2021 13:55:05 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_000.pt.
09/23/2021 13:55:05 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:55:05 - INFO - __main__ - Evaluating to get errors .... Timecode: 0
09/23/2021 13:55:09 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 13:55:09 - INFO - __main__ - Found 32 errors.
09/23/2021 13:55:09 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=32) .... Timecode: 0
09/23/2021 13:55:16 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:55:16 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 0
09/23/2021 13:55:19 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 13:55:19 - INFO - __main__ - Instant Fixing Rate: 0.84375
09/23/2021 13:55:19 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 13:55:22 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_001.pt.
09/23/2021 13:55:22 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 13:55:22 - INFO - __main__ - Current memory size: 0.
09/23/2021 13:55:22 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:55:22 - INFO - __main__ - Finished.
09/23/2021 13:55:22 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:55:22 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:55:22 - INFO - __main__ - Evaluating to get errors .... Timecode: 1
09/23/2021 13:55:25 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:55:25 - INFO - __main__ - Found 26 errors.
09/23/2021 13:55:25 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:55:25 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:55:25 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=32;
09/23/2021 13:55:31 - INFO - __main__ - before_losses=[tensor(1.5589), tensor(1.5005), tensor(2.1458), tensor(1.4888), tensor(1.5770), tensor(1.5163), tensor(1.4772), tensor(2.3435), tensor(2.1019), tensor(1.5249), tensor(2.9214), tensor(1.8106), tensor(2.3314), tensor(1.8749), tensor(1.5362), tensor(1.4525), tensor(1.5706), tensor(1.5381), tensor(1.6300), tensor(1.5215), tensor(1.5599), tensor(1.5514), tensor(1.5321), tensor(1.5203), tensor(1.5773), tensor(2.0851), tensor(2.6389), tensor(1.5650), tensor(1.4551), tensor(1.8541), tensor(1.5547), tensor(1.6852)]
09/23/2021 13:55:31 - INFO - __main__ - after_losses=[tensor(1.6965), tensor(1.4899), tensor(1.5662), tensor(1.4617), tensor(1.4979), tensor(1.4937), tensor(1.4614), tensor(2.0322), tensor(3.5988), tensor(1.5028), tensor(3.0618), tensor(1.4866), tensor(2.3907), tensor(1.6218), tensor(1.5151), tensor(1.4439), tensor(1.6342), tensor(1.5213), tensor(1.5024), tensor(1.4776), tensor(1.5422), tensor(1.5075), tensor(1.6629), tensor(1.4626), tensor(1.9873), tensor(1.5125), tensor(1.7068), tensor(1.4957), tensor(1.4437), tensor(1.5608), tensor(1.6155), tensor(1.5283)]
09/23/2021 13:55:31 - INFO - __main__ - interference_scores=[tensor(0.1376), tensor(-0.0106), tensor(-0.5796), tensor(-0.0271), tensor(-0.0791), tensor(-0.0225), tensor(-0.0158), tensor(-0.3113), tensor(1.4969), tensor(-0.0222), tensor(0.1404), tensor(-0.3240), tensor(0.0593), tensor(-0.2531), tensor(-0.0211), tensor(-0.0085), tensor(0.0636), tensor(-0.0168), tensor(-0.1275), tensor(-0.0439), tensor(-0.0177), tensor(-0.0439), tensor(0.1307), tensor(-0.0577), tensor(0.4100), tensor(-0.5726), tensor(-0.9321), tensor(-0.0692), tensor(-0.0113), tensor(-0.2933), tensor(0.0608), tensor(-0.1569)]
09/23/2021 13:55:31 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-1616', 'mrqa_squad-validation-392', 'mrqa_triviaqa-validation-2376', 'mrqa_squad-validation-7746', 'mrqa_naturalquestions-validation-7017', 'mrqa_triviaqa-validation-4054', 'mrqa_squad-validation-5622', 'mrqa_naturalquestions-validation-3490', 'mrqa_hotpotqa-validation-3241', 'mrqa_hotpotqa-validation-409', 'mrqa_naturalquestions-validation-3028', 'mrqa_hotpotqa-validation-1968', 'mrqa_squad-validation-4185', 'mrqa_triviaqa-validation-5972', 'mrqa_triviaqa-validation-6351', 'mrqa_naturalquestions-validation-2730', 'mrqa_hotpotqa-validation-2970', 'mrqa_naturalquestions-validation-6157', 'mrqa_naturalquestions-validation-5465', 'mrqa_naturalquestions-validation-8948', 'mrqa_squad-validation-3478', 'mrqa_squad-validation-6303', 'mrqa_naturalquestions-validation-9688', 'mrqa_triviaqa-validation-671', 'mrqa_squad-validation-7821', 'mrqa_triviaqa-validation-7369', 'mrqa_triviaqa-validation-1551', 'mrqa_triviaqa-validation-3915', 'mrqa_naturalquestions-validation-2248', 'mrqa_squad-validation-10322', 'mrqa_triviaqa-validation-5937', 'mrqa_squad-validation-6677']
09/23/2021 13:55:31 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:55:31 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 1
09/23/2021 13:55:44 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:55:44 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 1
09/23/2021 13:55:47 - INFO - __main__ - After Error Fixing: 0.875
09/23/2021 13:55:47 - INFO - __main__ - Instant Fixing Rate: 1.0
09/23/2021 13:55:47 - INFO - __main__ - Instant Retention Rate: 0.33333333277777777
09/23/2021 13:55:49 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_002.pt.
09/23/2021 13:55:49 - INFO - __main__ - Saving the current error examples (len=26) to the memory.
09/23/2021 13:55:49 - INFO - __main__ - Current memory size: 32.
09/23/2021 13:55:49 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:55:49 - INFO - __main__ - Finished.
09/23/2021 13:55:49 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:55:49 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:55:49 - INFO - __main__ - Evaluating to get errors .... Timecode: 2
09/23/2021 13:55:52 - INFO - __main__ - Before Error Fixing: 0.09375
09/23/2021 13:55:52 - INFO - __main__ - Found 29 errors.
09/23/2021 13:55:52 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:55:52 - INFO - __main__ - Current memory size: 58.
09/23/2021 13:55:52 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=58;
09/23/2021 13:56:03 - INFO - __main__ - before_losses=[tensor(1.5514), tensor(1.7556), tensor(1.5753), tensor(1.4537), tensor(1.5266), tensor(1.7264), tensor(1.5400), tensor(1.5107), tensor(2.1815), tensor(1.5674), tensor(1.5100), tensor(1.4918), tensor(1.8628), tensor(1.6070), tensor(1.4439), tensor(1.5376), tensor(1.5584), tensor(1.4801), tensor(1.5284), tensor(1.4832), tensor(1.4646), tensor(1.6595), tensor(1.5079), tensor(1.5036), tensor(1.5681), tensor(1.4995), tensor(1.5319), tensor(1.5354), tensor(1.5677), tensor(1.5348), tensor(1.7500), tensor(1.5239), tensor(1.4827), tensor(1.5002), tensor(1.5116), tensor(1.6521), tensor(1.9511), tensor(1.4733), tensor(1.5130), tensor(2.7494), tensor(1.6398), tensor(1.5460), tensor(1.5770), tensor(4.4306), tensor(1.4952), tensor(1.4808), tensor(1.5217), tensor(1.5029), tensor(1.5577), tensor(1.5158), tensor(1.8453), tensor(1.9214), tensor(1.9702), tensor(1.4430), tensor(1.5221), tensor(1.5177), tensor(1.5864), tensor(1.8825)]
09/23/2021 13:56:03 - INFO - __main__ - after_losses=[tensor(1.5120), tensor(1.5841), tensor(1.5838), tensor(1.4601), tensor(1.5103), tensor(1.5521), tensor(1.5078), tensor(1.5117), tensor(1.5397), tensor(1.5414), tensor(2.2056), tensor(1.4826), tensor(1.4945), tensor(1.5304), tensor(1.4449), tensor(1.5153), tensor(3.5191), tensor(1.5057), tensor(1.4712), tensor(1.4852), tensor(1.4565), tensor(1.5825), tensor(1.4633), tensor(1.4649), tensor(1.5307), tensor(1.4847), tensor(1.5388), tensor(1.5464), tensor(1.5025), tensor(1.5197), tensor(2.4025), tensor(1.5063), tensor(1.4639), tensor(1.5058), tensor(1.4711), tensor(1.5493), tensor(2.8904), tensor(1.4585), tensor(1.4945), tensor(2.6220), tensor(1.7754), tensor(1.5477), tensor(1.5847), tensor(2.0838), tensor(1.4781), tensor(1.4653), tensor(2.3208), tensor(1.4839), tensor(1.8463), tensor(1.5045), tensor(1.4728), tensor(1.5593), tensor(1.5360), tensor(1.4642), tensor(1.5004), tensor(1.5023), tensor(1.5250), tensor(1.5708)]
09/23/2021 13:56:03 - INFO - __main__ - interference_scores=[tensor(-0.0394), tensor(-0.1714), tensor(0.0086), tensor(0.0064), tensor(-0.0164), tensor(-0.1744), tensor(-0.0323), tensor(0.0010), tensor(-0.6417), tensor(-0.0260), tensor(0.6955), tensor(-0.0092), tensor(-0.3683), tensor(-0.0765), tensor(0.0010), tensor(-0.0223), tensor(1.9608), tensor(0.0256), tensor(-0.0573), tensor(0.0020), tensor(-0.0081), tensor(-0.0771), tensor(-0.0446), tensor(-0.0387), tensor(-0.0375), tensor(-0.0148), tensor(0.0069), tensor(0.0110), tensor(-0.0652), tensor(-0.0151), tensor(0.6525), tensor(-0.0176), tensor(-0.0188), tensor(0.0056), tensor(-0.0405), tensor(-0.1028), tensor(0.9393), tensor(-0.0149), tensor(-0.0186), tensor(-0.1274), tensor(0.1356), tensor(0.0017), tensor(0.0076), tensor(-2.3468), tensor(-0.0171), tensor(-0.0155), tensor(0.7990), tensor(-0.0190), tensor(0.2886), tensor(-0.0112), tensor(-0.3725), tensor(-0.3621), tensor(-0.4342), tensor(0.0212), tensor(-0.0217), tensor(-0.0154), tensor(-0.0613), tensor(-0.3117)]
09/23/2021 13:56:03 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-1616', 'mrqa_squad-validation-10015', 'mrqa_squad-validation-392', 'mrqa_squad-validation-5622', 'mrqa_triviaqa-validation-1551', 'mrqa_naturalquestions-validation-2248', 'mrqa_triviaqa-validation-3915', 'mrqa_hotpotqa-validation-1201', 'mrqa_hotpotqa-validation-3241', 'mrqa_hotpotqa-validation-3971', 'mrqa_triviaqa-validation-7369', 'mrqa_triviaqa-validation-4054', 'mrqa_triviaqa-validation-1437', 'mrqa_hotpotqa-validation-1968', 'mrqa_naturalquestions-validation-1364', 'mrqa_naturalquestions-validation-9688', 'mrqa_triviaqa-validation-5972', 'mrqa_naturalquestions-validation-3028', 'mrqa_naturalquestions-validation-5144', 'mrqa_hotpotqa-validation-1626', 'mrqa_naturalquestions-validation-2753', 'mrqa_hotpotqa-validation-2970', 'mrqa_hotpotqa-validation-409', 'mrqa_naturalquestions-validation-6157', 'mrqa_squad-validation-7821', 'mrqa_naturalquestions-validation-5465', 'mrqa_naturalquestions-validation-9386', 'mrqa_triviaqa-validation-671', 'mrqa_naturalquestions-validation-8948', 'mrqa_naturalquestions-validation-7511', 'mrqa_naturalquestions-validation-2730', 'mrqa_hotpotqa-validation-5325']
09/23/2021 13:56:03 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:56:03 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=61) .... Timecode: 2
09/23/2021 13:56:15 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:56:15 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 2
09/23/2021 13:56:18 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 13:56:18 - INFO - __main__ - Instant Fixing Rate: 0.9655172413793104
09/23/2021 13:56:18 - INFO - __main__ - Instant Retention Rate: 0.3333333322222222
09/23/2021 13:56:20 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_003.pt.
09/23/2021 13:56:20 - INFO - __main__ - Saving the current error examples (len=29) to the memory.
09/23/2021 13:56:20 - INFO - __main__ - Current memory size: 58.
09/23/2021 13:56:20 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:56:20 - INFO - __main__ - Finished.
09/23/2021 13:56:20 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:56:20 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:56:20 - INFO - __main__ - Evaluating to get errors .... Timecode: 3
09/23/2021 13:56:23 - INFO - __main__ - Before Error Fixing: 0.125
09/23/2021 13:56:23 - INFO - __main__ - Found 28 errors.
09/23/2021 13:56:23 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:56:23 - INFO - __main__ - Current memory size: 87.
09/23/2021 13:56:23 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=87;
09/23/2021 13:56:35 - INFO - __main__ - before_losses=[tensor(1.5830), tensor(1.6166), tensor(1.9750), tensor(1.4979), tensor(1.4594), tensor(1.4845), tensor(2.9533), tensor(1.8086), tensor(1.4660), tensor(1.4838), tensor(1.4897), tensor(1.6094), tensor(1.5601), tensor(1.4997), tensor(1.5216), tensor(4.8277), tensor(1.5290), tensor(1.4779), tensor(2.0212), tensor(1.4999), tensor(1.5025), tensor(1.9821), tensor(2.4896), tensor(2.4936), tensor(1.8107), tensor(1.8996), tensor(1.5273), tensor(2.5789), tensor(1.5210), tensor(2.1911), tensor(1.4753), tensor(1.4890), tensor(2.6366), tensor(1.5413), tensor(1.4951), tensor(1.5254), tensor(1.5634), tensor(1.5039), tensor(1.5644), tensor(1.4749), tensor(1.4856), tensor(1.5488), tensor(1.5092), tensor(1.4969), tensor(1.6874), tensor(1.5145), tensor(1.4990), tensor(1.4519), tensor(1.4902), tensor(1.5161), tensor(1.4769), tensor(1.4962), tensor(1.5143), tensor(1.5242), tensor(1.5345), tensor(1.4403), tensor(1.5323), tensor(1.9246), tensor(1.5094), tensor(1.4735), tensor(1.4561), tensor(1.8113), tensor(1.4560), tensor(1.5800), tensor(1.4807), tensor(1.4728), tensor(1.5600), tensor(1.4578), tensor(1.5078), tensor(2.4185), tensor(1.4696), tensor(2.0168), tensor(1.4505), tensor(1.4921), tensor(1.4495), tensor(1.8058), tensor(1.5036), tensor(1.4812), tensor(2.2707), tensor(1.4757), tensor(1.4729), tensor(1.4564), tensor(1.4963), tensor(1.9973), tensor(1.5249), tensor(1.5121), tensor(1.5257)]
09/23/2021 13:56:35 - INFO - __main__ - after_losses=[tensor(1.6754), tensor(1.7853), tensor(1.4826), tensor(1.4999), tensor(1.4586), tensor(1.5046), tensor(2.8080), tensor(2.5109), tensor(1.4707), tensor(1.4683), tensor(1.4719), tensor(1.7173), tensor(1.8567), tensor(1.4710), tensor(1.4966), tensor(4.4120), tensor(1.5115), tensor(1.4726), tensor(3.6343), tensor(1.4932), tensor(1.4857), tensor(1.6120), tensor(2.0897), tensor(1.4703), tensor(1.9868), tensor(2.1244), tensor(1.5076), tensor(1.5089), tensor(1.4741), tensor(2.0681), tensor(1.4553), tensor(1.4653), tensor(1.5511), tensor(1.4833), tensor(1.5040), tensor(1.5143), tensor(1.5948), tensor(1.4794), tensor(1.5150), tensor(1.4855), tensor(1.4585), tensor(1.5329), tensor(1.4733), tensor(1.4804), tensor(1.6987), tensor(1.4811), tensor(1.4577), tensor(1.4384), tensor(1.4880), tensor(1.4781), tensor(1.4715), tensor(1.5066), tensor(1.4922), tensor(1.4751), tensor(1.5498), tensor(1.4354), tensor(1.5202), tensor(1.8759), tensor(3.0427), tensor(1.4705), tensor(1.4488), tensor(1.9950), tensor(1.5346), tensor(1.5516), tensor(1.4704), tensor(2.2271), tensor(1.5840), tensor(1.4469), tensor(1.5105), tensor(2.4514), tensor(1.4520), tensor(2.0861), tensor(1.4511), tensor(1.4866), tensor(1.4434), tensor(1.7529), tensor(1.4871), tensor(1.4568), tensor(2.2531), tensor(1.4539), tensor(1.4629), tensor(1.4717), tensor(1.4662), tensor(2.3697), tensor(1.5009), tensor(1.6876), tensor(1.4944)]
09/23/2021 13:56:35 - INFO - __main__ - interference_scores=[tensor(0.0923), tensor(0.1687), tensor(-0.4924), tensor(0.0021), tensor(-0.0007), tensor(0.0201), tensor(-0.1453), tensor(0.7023), tensor(0.0047), tensor(-0.0155), tensor(-0.0179), tensor(0.1079), tensor(0.2965), tensor(-0.0287), tensor(-0.0250), tensor(-0.4157), tensor(-0.0175), tensor(-0.0053), tensor(1.6131), tensor(-0.0066), tensor(-0.0168), tensor(-0.3702), tensor(-0.3999), tensor(-1.0233), tensor(0.1761), tensor(0.2247), tensor(-0.0197), tensor(-1.0699), tensor(-0.0469), tensor(-0.1230), tensor(-0.0199), tensor(-0.0237), tensor(-1.0856), tensor(-0.0580), tensor(0.0088), tensor(-0.0111), tensor(0.0314), tensor(-0.0244), tensor(-0.0494), tensor(0.0106), tensor(-0.0271), tensor(-0.0159), tensor(-0.0359), tensor(-0.0164), tensor(0.0114), tensor(-0.0335), tensor(-0.0413), tensor(-0.0135), tensor(-0.0022), tensor(-0.0380), tensor(-0.0054), tensor(0.0104), tensor(-0.0221), tensor(-0.0490), tensor(0.0153), tensor(-0.0049), tensor(-0.0120), tensor(-0.0487), tensor(1.5332), tensor(-0.0030), tensor(-0.0073), tensor(0.1838), tensor(0.0785), tensor(-0.0284), tensor(-0.0102), tensor(0.7543), tensor(0.0240), tensor(-0.0109), tensor(0.0026), tensor(0.0329), tensor(-0.0175), tensor(0.0694), tensor(0.0006), tensor(-0.0054), tensor(-0.0061), tensor(-0.0529), tensor(-0.0165), tensor(-0.0245), tensor(-0.0176), tensor(-0.0218), tensor(-0.0100), tensor(0.0153), tensor(-0.0301), tensor(0.3724), tensor(-0.0239), tensor(0.1755), tensor(-0.0313)]
09/23/2021 13:56:35 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-1616', 'mrqa_triviaqa-validation-4729', 'mrqa_naturalquestions-validation-5180', 'mrqa_triviaqa-validation-2210', 'mrqa_naturalquestions-validation-10680', 'mrqa_triviaqa-validation-7018', 'mrqa_triviaqa-validation-1924', 'mrqa_squad-validation-6677', 'mrqa_squad-validation-392', 'mrqa_triviaqa-validation-5972', 'mrqa_triviaqa-validation-3915', 'mrqa_triviaqa-validation-2096', 'mrqa_triviaqa-validation-4856', 'mrqa_naturalquestions-validation-114', 'mrqa_triviaqa-validation-5937', 'mrqa_triviaqa-validation-5406', 'mrqa_triviaqa-validation-7369', 'mrqa_triviaqa-validation-1437', 'mrqa_squad-validation-2629', 'mrqa_squad-validation-4253', 'mrqa_triviaqa-validation-1551', 'mrqa_squad-validation-7746', 'mrqa_naturalquestions-validation-2753', 'mrqa_naturalquestions-validation-5144', 'mrqa_naturalquestions-validation-1364', 'mrqa_naturalquestions-validation-9386', 'mrqa_squad-validation-8513', 'mrqa_triviaqa-validation-2722', 'mrqa_hotpotqa-validation-1968', 'mrqa_hotpotqa-validation-5325', 'mrqa_hotpotqa-validation-5699', 'mrqa_naturalquestions-validation-10364']
09/23/2021 13:56:35 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:56:35 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=60) .... Timecode: 3
09/23/2021 13:56:48 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:56:48 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 3
09/23/2021 13:56:51 - INFO - __main__ - After Error Fixing: 0.9375
09/23/2021 13:56:51 - INFO - __main__ - Instant Fixing Rate: 0.9642857142857143
09/23/2021 13:56:51 - INFO - __main__ - Instant Retention Rate: 0.7499999981250001
09/23/2021 13:56:53 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_004.pt.
09/23/2021 13:56:53 - INFO - __main__ - Saving the current error examples (len=28) to the memory.
09/23/2021 13:56:53 - INFO - __main__ - Current memory size: 87.
09/23/2021 13:56:53 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:56:53 - INFO - __main__ - Finished.
09/23/2021 13:56:53 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:56:53 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:56:53 - INFO - __main__ - Evaluating to get errors .... Timecode: 4
09/23/2021 13:56:56 - INFO - __main__ - Before Error Fixing: 0.28125
09/23/2021 13:56:56 - INFO - __main__ - Found 23 errors.
09/23/2021 13:56:56 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:56:56 - INFO - __main__ - Current memory size: 115.
09/23/2021 13:56:56 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=115;
09/23/2021 13:57:10 - INFO - __main__ - before_losses=[tensor(1.5318), tensor(1.5369), tensor(1.9505), tensor(1.5886), tensor(1.4538), tensor(2.9488), tensor(2.5193), tensor(1.4835), tensor(1.5831), tensor(1.5117), tensor(1.5575), tensor(1.5476), tensor(1.5666), tensor(1.5432), tensor(1.4687), tensor(1.4556), tensor(1.5706), tensor(1.4485), tensor(1.5630), tensor(1.4542), tensor(1.5411), tensor(1.5089), tensor(1.5445), tensor(1.5102), tensor(1.4809), tensor(1.4942), tensor(1.5432), tensor(1.5055), tensor(2.1602), tensor(1.5617), tensor(1.5272), tensor(1.4883), tensor(1.4576), tensor(1.4683), tensor(1.5236), tensor(1.5151), tensor(1.4834), tensor(1.7253), tensor(2.1768), tensor(2.5921), tensor(1.5241), tensor(2.2116), tensor(1.4704), tensor(1.5092), tensor(1.5108), tensor(1.4743), tensor(1.5141), tensor(1.5413), tensor(1.5076), tensor(1.5404), tensor(1.7545), tensor(1.5181), tensor(1.5177), tensor(1.4952), tensor(2.0788), tensor(1.6560), tensor(1.4770), tensor(1.6064), tensor(1.5987), tensor(1.5447), tensor(1.5150), tensor(1.4857), tensor(1.5424), tensor(1.4577), tensor(1.5346), tensor(1.4997), tensor(2.5659), tensor(1.5501), tensor(1.5446), tensor(1.5617), tensor(1.5274), tensor(1.6478), tensor(1.5016), tensor(2.2555), tensor(1.5378), tensor(1.4850), tensor(1.5276), tensor(1.5039), tensor(1.4837), tensor(1.4788), tensor(1.5108), tensor(1.4633), tensor(1.4642), tensor(2.7198), tensor(1.4914), tensor(1.5911), tensor(2.0821), tensor(1.4904), tensor(1.5642), tensor(1.5181), tensor(1.4998), tensor(1.5034), tensor(1.4981), tensor(1.5260), tensor(1.5143), tensor(1.5338), tensor(2.0512), tensor(1.4598), tensor(1.4822), tensor(1.4956), tensor(4.6283), tensor(1.7607), tensor(1.4844), tensor(1.5023), tensor(1.5302), tensor(1.4940), tensor(1.5426), tensor(2.9523), tensor(1.9470), tensor(2.0833), tensor(1.5760), tensor(1.5708), tensor(1.4894), tensor(1.5110), tensor(1.4437)]
09/23/2021 13:57:10 - INFO - __main__ - after_losses=[tensor(1.5198), tensor(1.5387), tensor(1.9999), tensor(2.1819), tensor(1.4489), tensor(2.8951), tensor(1.5147), tensor(1.4713), tensor(1.5240), tensor(1.5097), tensor(1.5022), tensor(1.5548), tensor(1.5814), tensor(1.5612), tensor(1.4566), tensor(1.5056), tensor(1.4491), tensor(1.4443), tensor(1.5477), tensor(1.4650), tensor(1.5270), tensor(1.4710), tensor(1.5376), tensor(1.5206), tensor(1.4727), tensor(1.4817), tensor(1.5313), tensor(1.5222), tensor(2.2198), tensor(1.5322), tensor(1.5288), tensor(1.4807), tensor(1.4507), tensor(1.4653), tensor(1.5116), tensor(1.5160), tensor(1.4744), tensor(1.6254), tensor(1.6448), tensor(2.5937), tensor(1.5093), tensor(2.2090), tensor(1.4541), tensor(1.5017), tensor(1.5064), tensor(1.4736), tensor(1.4963), tensor(1.5329), tensor(1.5020), tensor(1.5129), tensor(2.1117), tensor(1.5144), tensor(1.5077), tensor(1.4847), tensor(1.4942), tensor(1.7059), tensor(1.4687), tensor(1.5554), tensor(1.5814), tensor(1.5567), tensor(1.4976), tensor(1.4551), tensor(1.5226), tensor(1.4653), tensor(1.6354), tensor(1.4869), tensor(2.6208), tensor(1.5253), tensor(1.4890), tensor(1.5354), tensor(1.4906), tensor(1.7370), tensor(1.4876), tensor(2.2890), tensor(1.5345), tensor(1.4785), tensor(1.5079), tensor(1.4903), tensor(1.4741), tensor(1.4691), tensor(1.4986), tensor(1.4704), tensor(1.4587), tensor(2.5895), tensor(1.4783), tensor(1.5306), tensor(1.5114), tensor(1.4821), tensor(1.5496), tensor(1.5260), tensor(1.4721), tensor(1.5038), tensor(1.4874), tensor(1.5116), tensor(1.5062), tensor(1.5134), tensor(2.5407), tensor(1.4579), tensor(1.4890), tensor(2.1736), tensor(4.6619), tensor(1.6530), tensor(1.7466), tensor(1.5246), tensor(1.5155), tensor(1.4752), tensor(1.5294), tensor(2.7131), tensor(1.8451), tensor(2.1601), tensor(1.5485), tensor(1.5535), tensor(1.4835), tensor(1.4891), tensor(1.4386)]
09/23/2021 13:57:10 - INFO - __main__ - interference_scores=[tensor(-0.0120), tensor(0.0017), tensor(0.0494), tensor(0.5933), tensor(-0.0049), tensor(-0.0537), tensor(-1.0046), tensor(-0.0123), tensor(-0.0590), tensor(-0.0021), tensor(-0.0553), tensor(0.0071), tensor(0.0148), tensor(0.0180), tensor(-0.0121), tensor(0.0500), tensor(-0.1214), tensor(-0.0042), tensor(-0.0153), tensor(0.0109), tensor(-0.0141), tensor(-0.0379), tensor(-0.0070), tensor(0.0105), tensor(-0.0082), tensor(-0.0124), tensor(-0.0119), tensor(0.0168), tensor(0.0596), tensor(-0.0296), tensor(0.0016), tensor(-0.0076), tensor(-0.0069), tensor(-0.0030), tensor(-0.0120), tensor(0.0009), tensor(-0.0090), tensor(-0.0999), tensor(-0.5319), tensor(0.0015), tensor(-0.0149), tensor(-0.0026), tensor(-0.0163), tensor(-0.0075), tensor(-0.0044), tensor(-0.0006), tensor(-0.0178), tensor(-0.0084), tensor(-0.0055), tensor(-0.0274), tensor(0.3573), tensor(-0.0038), tensor(-0.0100), tensor(-0.0105), tensor(-0.5847), tensor(0.0499), tensor(-0.0082), tensor(-0.0510), tensor(-0.0173), tensor(0.0120), tensor(-0.0174), tensor(-0.0306), tensor(-0.0199), tensor(0.0076), tensor(0.1008), tensor(-0.0128), tensor(0.0549), tensor(-0.0248), tensor(-0.0556), tensor(-0.0263), tensor(-0.0368), tensor(0.0892), tensor(-0.0140), tensor(0.0335), tensor(-0.0033), tensor(-0.0065), tensor(-0.0197), tensor(-0.0136), tensor(-0.0096), tensor(-0.0097), tensor(-0.0122), tensor(0.0071), tensor(-0.0054), tensor(-0.1304), tensor(-0.0131), tensor(-0.0605), tensor(-0.5707), tensor(-0.0084), tensor(-0.0146), tensor(0.0078), tensor(-0.0277), tensor(0.0004), tensor(-0.0107), tensor(-0.0144), tensor(-0.0081), tensor(-0.0204), tensor(0.4895), tensor(-0.0019), tensor(0.0068), tensor(0.6780), tensor(0.0336), tensor(-0.1077), tensor(0.2622), tensor(0.0223), tensor(-0.0146), tensor(-0.0188), tensor(-0.0132), tensor(-0.2392), tensor(-0.1019), tensor(0.0769), tensor(-0.0276), tensor(-0.0173), tensor(-0.0060), tensor(-0.0219), tensor(-0.0052)]
09/23/2021 13:57:10 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-5972', 'mrqa_triviaqa-validation-5937', 'mrqa_triviaqa-validation-6402', 'mrqa_triviaqa-validation-1935', 'mrqa_squad-validation-6677', 'mrqa_triviaqa-validation-3901', 'mrqa_triviaqa-validation-2530', 'mrqa_squad-validation-1516', 'mrqa_triviaqa-validation-3915', 'mrqa_squad-validation-7149', 'mrqa_hotpotqa-validation-5325', 'mrqa_naturalquestions-validation-2501', 'mrqa_triviaqa-validation-893', 'mrqa_triviaqa-validation-2376', 'mrqa_squad-validation-1539', 'mrqa_triviaqa-validation-2327', 'mrqa_triviaqa-validation-5362', 'mrqa_triviaqa-validation-2136', 'mrqa_triviaqa-validation-6351', 'mrqa_naturalquestions-validation-8948', 'mrqa_hotpotqa-validation-1968', 'mrqa_triviaqa-validation-1437', 'mrqa_squad-validation-8513', 'mrqa_naturalquestions-validation-114', 'mrqa_hotpotqa-validation-2679', 'mrqa_squad-validation-4253', 'mrqa_hotpotqa-validation-2263', 'mrqa_triviaqa-validation-365', 'mrqa_naturalquestions-validation-3189', 'mrqa_squad-validation-5622', 'mrqa_naturalquestions-validation-5818', 'mrqa_naturalquestions-validation-8119']
09/23/2021 13:57:10 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:57:10 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=55) .... Timecode: 4
09/23/2021 13:57:22 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:57:22 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 4
09/23/2021 13:57:24 - INFO - __main__ - After Error Fixing: 0.875
09/23/2021 13:57:24 - INFO - __main__ - Instant Fixing Rate: 1.0
09/23/2021 13:57:24 - INFO - __main__ - Instant Retention Rate: 0.5555555549382716
09/23/2021 13:57:26 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_005.pt.
09/23/2021 13:57:26 - INFO - __main__ - Saving the current error examples (len=23) to the memory.
09/23/2021 13:57:26 - INFO - __main__ - Current memory size: 115.
09/23/2021 13:57:26 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:57:26 - INFO - __main__ - Finished.
09/23/2021 13:57:26 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:57:26 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:57:26 - INFO - __main__ - Evaluating to get errors .... Timecode: 5
09/23/2021 13:57:29 - INFO - __main__ - Before Error Fixing: 0.125
09/23/2021 13:57:29 - INFO - __main__ - Found 28 errors.
09/23/2021 13:57:29 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:57:29 - INFO - __main__ - Current memory size: 138.
09/23/2021 13:57:29 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=138;
09/23/2021 13:57:43 - INFO - __main__ - before_losses=[tensor(1.4595), tensor(1.4671), tensor(1.7961), tensor(1.5511), tensor(1.9737), tensor(1.4635), tensor(1.5237), tensor(1.4780), tensor(1.4794), tensor(2.3412), tensor(1.4757), tensor(1.4511), tensor(1.4548), tensor(1.4927), tensor(1.6830), tensor(2.7368), tensor(1.4361), tensor(1.4958), tensor(1.9161), tensor(1.4989), tensor(1.5142), tensor(1.4435), tensor(1.4890), tensor(1.5249), tensor(1.4616), tensor(1.5223), tensor(1.4719), tensor(1.4787), tensor(1.5247), tensor(1.9514), tensor(1.5493), tensor(1.6897), tensor(1.4445), tensor(1.5061), tensor(1.4720), tensor(1.5126), tensor(1.5330), tensor(1.4566), tensor(3.3877), tensor(1.4946), tensor(1.4830), tensor(1.4712), tensor(1.6086), tensor(1.7029), tensor(1.4822), tensor(1.4752), tensor(1.5109), tensor(1.6395), tensor(1.5598), tensor(1.5197), tensor(1.4546), tensor(1.5243), tensor(1.5040), tensor(1.5086), tensor(1.4745), tensor(1.4779), tensor(1.4579), tensor(1.5013), tensor(1.4476), tensor(1.4885), tensor(1.4720), tensor(1.4677), tensor(1.5022), tensor(1.4967), tensor(1.5287), tensor(1.4971), tensor(1.6880), tensor(1.6387), tensor(1.4875), tensor(1.4812), tensor(1.4990), tensor(2.9092), tensor(1.4899), tensor(1.4953), tensor(1.5097), tensor(1.5378), tensor(1.4927), tensor(1.5255), tensor(1.5058), tensor(1.7862), tensor(1.5250), tensor(1.4707), tensor(1.4583), tensor(1.5203), tensor(1.5058), tensor(1.5091), tensor(1.7916), tensor(1.4931), tensor(1.7328), tensor(1.4524), tensor(1.4918), tensor(1.4933), tensor(1.4801), tensor(2.0921), tensor(1.4790), tensor(1.4973), tensor(1.5581), tensor(1.6371), tensor(1.4724), tensor(1.4863), tensor(1.5096), tensor(1.5033), tensor(1.4675), tensor(1.6157), tensor(1.4869), tensor(1.4712), tensor(1.6341), tensor(1.4533), tensor(1.4636), tensor(2.7980), tensor(2.6404), tensor(1.5013), tensor(1.4795), tensor(1.5172), tensor(1.4789), tensor(1.7228), tensor(2.0767), tensor(1.7291), tensor(1.4428), tensor(1.5267), tensor(1.5699), tensor(1.4890), tensor(1.4915), tensor(1.5069), tensor(1.4846), tensor(1.4984), tensor(1.4896), tensor(1.4539), tensor(1.5223), tensor(2.7183), tensor(1.4646), tensor(1.4973), tensor(1.4600), tensor(1.5719), tensor(1.4745), tensor(1.6651), tensor(1.4418), tensor(1.4598)]
09/23/2021 13:57:43 - INFO - __main__ - after_losses=[tensor(1.4643), tensor(1.4609), tensor(1.8152), tensor(1.4911), tensor(2.0720), tensor(1.4586), tensor(1.5290), tensor(1.4860), tensor(1.4555), tensor(2.5489), tensor(1.4738), tensor(1.4743), tensor(1.4422), tensor(1.4990), tensor(1.9534), tensor(2.7402), tensor(1.4311), tensor(1.4993), tensor(1.5041), tensor(1.4875), tensor(1.6421), tensor(1.4468), tensor(1.4853), tensor(1.4715), tensor(1.4738), tensor(1.5184), tensor(1.4668), tensor(1.4863), tensor(1.5298), tensor(1.6915), tensor(1.5353), tensor(1.8181), tensor(1.4389), tensor(1.5212), tensor(1.4644), tensor(1.4896), tensor(1.5370), tensor(1.4545), tensor(3.3942), tensor(1.6651), tensor(1.4829), tensor(1.4714), tensor(1.6890), tensor(1.5650), tensor(1.4819), tensor(1.4630), tensor(1.5160), tensor(1.5978), tensor(1.4802), tensor(2.7153), tensor(1.4540), tensor(1.5674), tensor(1.4867), tensor(1.5109), tensor(1.4650), tensor(1.4934), tensor(1.4597), tensor(1.5374), tensor(1.4464), tensor(1.4921), tensor(1.4453), tensor(1.4648), tensor(1.4945), tensor(1.5177), tensor(1.5073), tensor(1.4885), tensor(1.5330), tensor(2.4945), tensor(1.4907), tensor(1.4843), tensor(1.4930), tensor(2.8865), tensor(1.4978), tensor(1.4916), tensor(1.5043), tensor(1.4947), tensor(1.4826), tensor(1.5309), tensor(1.5208), tensor(1.5430), tensor(1.5147), tensor(1.4769), tensor(1.4527), tensor(1.5256), tensor(1.5048), tensor(1.5145), tensor(2.0100), tensor(1.5020), tensor(1.7670), tensor(1.4540), tensor(1.4725), tensor(1.4876), tensor(1.4856), tensor(2.0792), tensor(1.4716), tensor(1.5079), tensor(1.5521), tensor(1.5695), tensor(1.4851), tensor(1.5185), tensor(1.4952), tensor(1.4701), tensor(1.5253), tensor(1.6750), tensor(1.4676), tensor(1.4576), tensor(1.7084), tensor(1.4707), tensor(1.4470), tensor(2.7867), tensor(2.6646), tensor(1.5088), tensor(1.4756), tensor(1.5190), tensor(1.4842), tensor(1.7253), tensor(1.5402), tensor(1.6103), tensor(1.4375), tensor(1.5222), tensor(1.6098), tensor(1.4859), tensor(1.4969), tensor(1.5101), tensor(1.4853), tensor(1.7142), tensor(1.4649), tensor(1.4592), tensor(1.4665), tensor(2.6868), tensor(1.4676), tensor(1.4875), tensor(1.4436), tensor(2.2934), tensor(1.4675), tensor(1.8946), tensor(1.4403), tensor(1.4520)]
09/23/2021 13:57:43 - INFO - __main__ - interference_scores=[tensor(0.0048), tensor(-0.0062), tensor(0.0191), tensor(-0.0600), tensor(0.0983), tensor(-0.0049), tensor(0.0053), tensor(0.0080), tensor(-0.0239), tensor(0.2078), tensor(-0.0020), tensor(0.0232), tensor(-0.0127), tensor(0.0064), tensor(0.2704), tensor(0.0034), tensor(-0.0050), tensor(0.0035), tensor(-0.4120), tensor(-0.0114), tensor(0.1279), tensor(0.0033), tensor(-0.0037), tensor(-0.0534), tensor(0.0122), tensor(-0.0039), tensor(-0.0051), tensor(0.0075), tensor(0.0051), tensor(-0.2600), tensor(-0.0140), tensor(0.1284), tensor(-0.0055), tensor(0.0151), tensor(-0.0076), tensor(-0.0231), tensor(0.0041), tensor(-0.0021), tensor(0.0065), tensor(0.1705), tensor(-3.2187e-05), tensor(0.0002), tensor(0.0804), tensor(-0.1378), tensor(-0.0003), tensor(-0.0122), tensor(0.0051), tensor(-0.0417), tensor(-0.0796), tensor(1.1956), tensor(-0.0006), tensor(0.0431), tensor(-0.0173), tensor(0.0023), tensor(-0.0095), tensor(0.0156), tensor(0.0018), tensor(0.0361), tensor(-0.0011), tensor(0.0036), tensor(-0.0266), tensor(-0.0029), tensor(-0.0077), tensor(0.0210), tensor(-0.0214), tensor(-0.0086), tensor(-0.1549), tensor(0.8558), tensor(0.0032), tensor(0.0031), tensor(-0.0060), tensor(-0.0226), tensor(0.0079), tensor(-0.0037), tensor(-0.0054), tensor(-0.0431), tensor(-0.0101), tensor(0.0054), tensor(0.0150), tensor(-0.2432), tensor(-0.0102), tensor(0.0061), tensor(-0.0057), tensor(0.0053), tensor(-0.0010), tensor(0.0054), tensor(0.2185), tensor(0.0089), tensor(0.0342), tensor(0.0015), tensor(-0.0192), tensor(-0.0057), tensor(0.0055), tensor(-0.0129), tensor(-0.0073), tensor(0.0106), tensor(-0.0059), tensor(-0.0676), tensor(0.0126), tensor(0.0323), tensor(-0.0144), tensor(-0.0333), tensor(0.0578), tensor(0.0593), tensor(-0.0192), tensor(-0.0137), tensor(0.0743), tensor(0.0174), tensor(-0.0166), tensor(-0.0113), tensor(0.0242), tensor(0.0075), tensor(-0.0040), tensor(0.0018), tensor(0.0053), tensor(0.0025), tensor(-0.5365), tensor(-0.1188), tensor(-0.0054), tensor(-0.0045), tensor(0.0399), tensor(-0.0032), tensor(0.0055), tensor(0.0033), tensor(0.0008), tensor(0.2157), tensor(-0.0247), tensor(0.0052), tensor(-0.0558), tensor(-0.0314), tensor(0.0031), tensor(-0.0099), tensor(-0.0164), tensor(0.7216), tensor(-0.0071), tensor(0.2295), tensor(-0.0015), tensor(-0.0078)]
09/23/2021 13:57:43 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-3971', 'mrqa_naturalquestions-validation-5437', 'mrqa_squad-validation-7149', 'mrqa_triviaqa-validation-6402', 'mrqa_triviaqa-validation-5972', 'mrqa_triviaqa-validation-7382', 'mrqa_naturalquestions-validation-10680', 'mrqa_naturalquestions-validation-2501', 'mrqa_triviaqa-validation-2530', 'mrqa_squad-validation-392', 'mrqa_triviaqa-validation-7369', 'mrqa_squad-validation-2629', 'mrqa_triviaqa-validation-1924', 'mrqa_triviaqa-validation-4729', 'mrqa_triviaqa-validation-1551', 'mrqa_squad-validation-6677', 'mrqa_triviaqa-validation-7018', 'mrqa_triviaqa-validation-671', 'mrqa_triviaqa-validation-4856', 'mrqa_triviaqa-validation-287', 'mrqa_triviaqa-validation-6683', 'mrqa_squad-validation-5517', 'mrqa_squad-validation-3021', 'mrqa_naturalquestions-validation-3189', 'mrqa_naturalquestions-validation-7017', 'mrqa_naturalquestions-validation-868', 'mrqa_hotpotqa-validation-2679', 'mrqa_triviaqa-validation-365', 'mrqa_squad-validation-8513', 'mrqa_hotpotqa-validation-4734', 'mrqa_squad-validation-1688', 'mrqa_triviaqa-validation-3808']
09/23/2021 13:57:43 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:57:43 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=60) .... Timecode: 5
09/23/2021 13:57:56 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:57:56 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 5
09/23/2021 13:57:59 - INFO - __main__ - After Error Fixing: 0.9375
09/23/2021 13:57:59 - INFO - __main__ - Instant Fixing Rate: 0.9642857142857143
09/23/2021 13:57:59 - INFO - __main__ - Instant Retention Rate: 0.7499999981250001
09/23/2021 13:58:01 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_006.pt.
09/23/2021 13:58:01 - INFO - __main__ - Saving the current error examples (len=28) to the memory.
09/23/2021 13:58:01 - INFO - __main__ - Current memory size: 138.
09/23/2021 13:58:01 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:58:01 - INFO - __main__ - Finished.
09/23/2021 13:58:01 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:58:01 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:58:01 - INFO - __main__ - Evaluating to get errors .... Timecode: 6
09/23/2021 13:58:03 - INFO - __main__ - Before Error Fixing: 0.09375
09/23/2021 13:58:03 - INFO - __main__ - Found 29 errors.
09/23/2021 13:58:03 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:58:03 - INFO - __main__ - Current memory size: 166.
09/23/2021 13:58:03 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=166;
09/23/2021 13:58:20 - INFO - __main__ - before_losses=[tensor(1.5436), tensor(1.4894), tensor(1.5073), tensor(1.8606), tensor(1.4964), tensor(1.5146), tensor(1.5120), tensor(1.4923), tensor(1.5335), tensor(1.9848), tensor(1.5514), tensor(1.4982), tensor(1.4935), tensor(1.4440), tensor(1.4569), tensor(2.4684), tensor(1.7226), tensor(1.4841), tensor(1.5250), tensor(1.4817), tensor(1.5081), tensor(1.5341), tensor(1.4796), tensor(2.0083), tensor(1.4940), tensor(1.7311), tensor(1.4433), tensor(1.5471), tensor(1.5068), tensor(1.4668), tensor(1.4773), tensor(1.5399), tensor(1.5148), tensor(2.2116), tensor(2.0377), tensor(1.4727), tensor(1.5213), tensor(1.5465), tensor(2.9670), tensor(2.6944), tensor(1.6676), tensor(1.5098), tensor(1.5173), tensor(1.4760), tensor(1.4678), tensor(1.5128), tensor(1.5048), tensor(1.4780), tensor(1.5017), tensor(1.4823), tensor(1.5842), tensor(1.5289), tensor(1.6929), tensor(2.8549), tensor(1.4922), tensor(1.4976), tensor(1.5215), tensor(1.5086), tensor(1.4896), tensor(1.5091), tensor(1.4667), tensor(1.4750), tensor(1.4443), tensor(1.5231), tensor(1.5495), tensor(1.4714), tensor(1.5335), tensor(1.5147), tensor(1.4829), tensor(1.5455), tensor(1.4779), tensor(2.0721), tensor(1.4547), tensor(1.4331), tensor(1.5461), tensor(1.5650), tensor(1.5806), tensor(1.4782), tensor(1.5320), tensor(1.5227), tensor(3.1396), tensor(2.5022), tensor(1.4933), tensor(1.5050), tensor(1.5478), tensor(1.5139), tensor(1.4977), tensor(1.4803), tensor(1.4989), tensor(1.5467), tensor(1.4756), tensor(1.4700), tensor(1.5128), tensor(1.5112), tensor(1.4893), tensor(1.4562), tensor(1.4484), tensor(1.5032), tensor(1.4533), tensor(1.5628), tensor(1.5393), tensor(1.7844), tensor(1.5142), tensor(3.2428), tensor(2.1202), tensor(1.5000), tensor(1.5064), tensor(1.4887), tensor(1.4915), tensor(1.4908), tensor(1.5268), tensor(1.4572), tensor(1.4889), tensor(1.5198), tensor(1.5476), tensor(1.4706), tensor(1.5276), tensor(1.5415), tensor(1.4962), tensor(1.5075), tensor(3.2912), tensor(1.8419), tensor(1.4760), tensor(1.5130), tensor(1.8324), tensor(1.5760), tensor(1.5058), tensor(1.5281), tensor(1.4594), tensor(1.4983), tensor(1.9197), tensor(1.5329), tensor(2.2949), tensor(1.4859), tensor(1.5278), tensor(1.5024), tensor(1.5452), tensor(1.5088), tensor(1.7955), tensor(4.4024), tensor(1.4496), tensor(1.5285), tensor(1.5156), tensor(1.6246), tensor(1.4936), tensor(1.4932), tensor(1.4701), tensor(1.4876), tensor(1.4685), tensor(1.5021), tensor(1.4748), tensor(1.5573), tensor(2.4125), tensor(1.4384), tensor(1.5033), tensor(1.4739), tensor(1.5103), tensor(2.2188), tensor(1.7615), tensor(2.0640), tensor(1.5377), tensor(1.4746), tensor(1.5635), tensor(1.4778), tensor(1.4815), tensor(1.5182)]
09/23/2021 13:58:20 - INFO - __main__ - after_losses=[tensor(1.5006), tensor(1.5349), tensor(1.4995), tensor(1.6995), tensor(1.4897), tensor(1.5253), tensor(1.5044), tensor(1.4988), tensor(1.5102), tensor(1.4860), tensor(1.5296), tensor(1.5017), tensor(1.4865), tensor(1.4518), tensor(1.4544), tensor(1.8104), tensor(1.6260), tensor(1.4629), tensor(1.5181), tensor(1.5569), tensor(1.4990), tensor(1.5074), tensor(1.4826), tensor(1.9830), tensor(1.4745), tensor(1.7395), tensor(1.4433), tensor(1.5294), tensor(1.4888), tensor(1.4527), tensor(1.5231), tensor(1.4969), tensor(1.5203), tensor(1.5232), tensor(1.5145), tensor(1.4770), tensor(1.4930), tensor(1.5585), tensor(2.7979), tensor(2.6583), tensor(2.2630), tensor(1.4963), tensor(1.5624), tensor(1.4726), tensor(1.4612), tensor(1.5523), tensor(1.5276), tensor(1.4741), tensor(1.4898), tensor(1.4757), tensor(1.6208), tensor(1.5217), tensor(1.5450), tensor(2.9115), tensor(1.4898), tensor(1.4872), tensor(1.5628), tensor(1.4797), tensor(1.4885), tensor(1.4901), tensor(1.4790), tensor(1.4746), tensor(1.4457), tensor(2.5668), tensor(1.4991), tensor(1.4652), tensor(1.5360), tensor(1.5146), tensor(1.4691), tensor(1.5255), tensor(1.4746), tensor(1.9435), tensor(1.4506), tensor(1.4332), tensor(1.5383), tensor(1.6519), tensor(1.5492), tensor(1.4768), tensor(1.5430), tensor(1.5325), tensor(3.0388), tensor(2.4624), tensor(1.4818), tensor(1.4926), tensor(1.4886), tensor(1.4717), tensor(1.4833), tensor(1.4846), tensor(1.4889), tensor(1.5310), tensor(1.4703), tensor(1.4591), tensor(1.5433), tensor(1.4860), tensor(1.4733), tensor(1.4566), tensor(1.4556), tensor(1.5039), tensor(1.4548), tensor(1.5890), tensor(1.5664), tensor(1.8752), tensor(1.5097), tensor(1.5119), tensor(1.8909), tensor(1.4894), tensor(1.4953), tensor(1.4723), tensor(1.4834), tensor(1.4729), tensor(1.5173), tensor(1.4610), tensor(1.4982), tensor(1.5074), tensor(1.5027), tensor(1.4700), tensor(1.5528), tensor(1.5283), tensor(1.4863), tensor(1.4796), tensor(3.5302), tensor(1.7696), tensor(1.4764), tensor(1.5064), tensor(1.5376), tensor(2.5511), tensor(1.4769), tensor(1.4905), tensor(1.4697), tensor(1.4737), tensor(1.5143), tensor(1.5174), tensor(1.4964), tensor(1.4787), tensor(1.5002), tensor(1.4941), tensor(1.8011), tensor(1.4934), tensor(1.5309), tensor(1.5256), tensor(1.4483), tensor(1.5231), tensor(1.5185), tensor(2.1882), tensor(1.4844), tensor(1.4732), tensor(1.4733), tensor(1.4783), tensor(1.4653), tensor(1.4876), tensor(1.4607), tensor(1.5058), tensor(2.2460), tensor(1.4350), tensor(1.4794), tensor(1.4708), tensor(1.4805), tensor(1.4989), tensor(1.6914), tensor(2.0059), tensor(1.5209), tensor(1.4785), tensor(1.7158), tensor(1.4717), tensor(1.4861), tensor(1.5338)]
09/23/2021 13:58:20 - INFO - __main__ - interference_scores=[tensor(-0.0430), tensor(0.0455), tensor(-0.0077), tensor(-0.1611), tensor(-0.0068), tensor(0.0107), tensor(-0.0076), tensor(0.0065), tensor(-0.0233), tensor(-0.4988), tensor(-0.0218), tensor(0.0035), tensor(-0.0070), tensor(0.0077), tensor(-0.0026), tensor(-0.6580), tensor(-0.0966), tensor(-0.0212), tensor(-0.0069), tensor(0.0752), tensor(-0.0091), tensor(-0.0267), tensor(0.0030), tensor(-0.0252), tensor(-0.0195), tensor(0.0084), tensor(3.0041e-05), tensor(-0.0177), tensor(-0.0180), tensor(-0.0141), tensor(0.0458), tensor(-0.0430), tensor(0.0055), tensor(-0.6884), tensor(-0.5233), tensor(0.0042), tensor(-0.0284), tensor(0.0120), tensor(-0.1691), tensor(-0.0361), tensor(0.5955), tensor(-0.0136), tensor(0.0450), tensor(-0.0034), tensor(-0.0065), tensor(0.0394), tensor(0.0228), tensor(-0.0038), tensor(-0.0118), tensor(-0.0066), tensor(0.0366), tensor(-0.0072), tensor(-0.1479), tensor(0.0566), tensor(-0.0024), tensor(-0.0104), tensor(0.0413), tensor(-0.0289), tensor(-0.0011), tensor(-0.0190), tensor(0.0123), tensor(-0.0004), tensor(0.0014), tensor(1.0437), tensor(-0.0504), tensor(-0.0062), tensor(0.0025), tensor(-0.0001), tensor(-0.0138), tensor(-0.0200), tensor(-0.0034), tensor(-0.1285), tensor(-0.0041), tensor(8.9169e-05), tensor(-0.0078), tensor(0.0869), tensor(-0.0314), tensor(-0.0014), tensor(0.0110), tensor(0.0098), tensor(-0.1008), tensor(-0.0398), tensor(-0.0115), tensor(-0.0124), tensor(-0.0592), tensor(-0.0422), tensor(-0.0145), tensor(0.0043), tensor(-0.0100), tensor(-0.0157), tensor(-0.0053), tensor(-0.0109), tensor(0.0305), tensor(-0.0252), tensor(-0.0161), tensor(0.0004), tensor(0.0072), tensor(0.0007), tensor(0.0015), tensor(0.0262), tensor(0.0271), tensor(0.0908), tensor(-0.0045), tensor(-1.7308), tensor(-0.2293), tensor(-0.0106), tensor(-0.0111), tensor(-0.0164), tensor(-0.0081), tensor(-0.0178), tensor(-0.0095), tensor(0.0038), tensor(0.0093), tensor(-0.0124), tensor(-0.0449), tensor(-0.0006), tensor(0.0251), tensor(-0.0132), tensor(-0.0099), tensor(-0.0279), tensor(0.2391), tensor(-0.0722), tensor(0.0004), tensor(-0.0066), tensor(-0.2948), tensor(0.9751), tensor(-0.0289), tensor(-0.0377), tensor(0.0103), tensor(-0.0246), tensor(-0.4054), tensor(-0.0155), tensor(-0.7985), tensor(-0.0073), tensor(-0.0276), tensor(-0.0083), tensor(0.2559), tensor(-0.0154), tensor(-0.2646), tensor(-2.8767), tensor(-0.0013), tensor(-0.0053), tensor(0.0028), tensor(0.5636), tensor(-0.0092), tensor(-0.0201), tensor(0.0032), tensor(-0.0093), tensor(-0.0031), tensor(-0.0145), tensor(-0.0141), tensor(-0.0516), tensor(-0.1665), tensor(-0.0034), tensor(-0.0239), tensor(-0.0031), tensor(-0.0298), tensor(-0.7199), tensor(-0.0701), tensor(-0.0581), tensor(-0.0168), tensor(0.0038), tensor(0.1522), tensor(-0.0061), tensor(0.0046), tensor(0.0156)]
09/23/2021 13:58:20 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-2629', 'mrqa_naturalquestions-validation-5437', 'mrqa_triviaqa-validation-4729', 'mrqa_triviaqa-validation-1935', 'mrqa_triviaqa-validation-1550', 'mrqa_triviaqa-validation-1616', 'mrqa_triviaqa-validation-2210', 'mrqa_triviaqa-validation-3647', 'mrqa_triviaqa-validation-5937', 'mrqa_squad-validation-2069', 'mrqa_triviaqa-validation-2368', 'mrqa_naturalquestions-validation-5180', 'mrqa_triviaqa-validation-3901', 'mrqa_triviaqa-validation-365', 'mrqa_hotpotqa-validation-4367', 'mrqa_triviaqa-validation-893', 'mrqa_triviaqa-validation-93', 'mrqa_hotpotqa-validation-1888', 'mrqa_triviaqa-validation-1494', 'mrqa_triviaqa-validation-5026', 'mrqa_triviaqa-validation-1924', 'mrqa_triviaqa-validation-671', 'mrqa_triviaqa-validation-4856', 'mrqa_naturalquestions-validation-8948', 'mrqa_triviaqa-validation-6351', 'mrqa_triviaqa-validation-4054', 'mrqa_squad-validation-8542', 'mrqa_naturalquestions-validation-6157', 'mrqa_triviaqa-validation-2096', 'mrqa_naturalquestions-validation-10680', 'mrqa_squad-validation-7746', 'mrqa_hotpotqa-validation-1968']
09/23/2021 13:58:20 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:58:20 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=61) .... Timecode: 6
09/23/2021 13:58:33 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:58:33 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 6
09/23/2021 13:58:36 - INFO - __main__ - After Error Fixing: 0.9375
09/23/2021 13:58:36 - INFO - __main__ - Instant Fixing Rate: 0.9655172413793104
09/23/2021 13:58:36 - INFO - __main__ - Instant Retention Rate: 0.6666666644444444
09/23/2021 13:58:38 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_007.pt.
09/23/2021 13:58:38 - INFO - __main__ - Saving the current error examples (len=29) to the memory.
09/23/2021 13:58:38 - INFO - __main__ - Current memory size: 166.
09/23/2021 13:58:38 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:58:38 - INFO - __main__ - Finished.
09/23/2021 13:58:38 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:58:38 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:58:38 - INFO - __main__ - Evaluating to get errors .... Timecode: 7
09/23/2021 13:58:41 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 13:58:41 - INFO - __main__ - Found 26 errors.
09/23/2021 13:58:41 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:58:41 - INFO - __main__ - Current memory size: 195.
09/23/2021 13:58:41 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=195;
09/23/2021 13:59:00 - INFO - __main__ - before_losses=[tensor(1.5122), tensor(1.8235), tensor(1.4447), tensor(1.9865), tensor(1.4871), tensor(1.5805), tensor(1.4824), tensor(1.4793), tensor(1.4787), tensor(1.4811), tensor(1.4780), tensor(1.8846), tensor(2.1655), tensor(1.4867), tensor(1.5576), tensor(1.5351), tensor(1.6606), tensor(2.0380), tensor(1.5496), tensor(1.5328), tensor(1.4803), tensor(1.5327), tensor(1.4386), tensor(1.4534), tensor(1.4922), tensor(1.4964), tensor(1.4778), tensor(1.5275), tensor(1.4590), tensor(1.5347), tensor(1.5151), tensor(1.4777), tensor(1.5016), tensor(1.5410), tensor(1.5332), tensor(1.4985), tensor(1.5093), tensor(1.4925), tensor(1.4654), tensor(1.4973), tensor(1.5405), tensor(1.4695), tensor(1.4619), tensor(1.5190), tensor(1.4862), tensor(3.9094), tensor(1.4747), tensor(1.7133), tensor(1.5174), tensor(1.4889), tensor(1.7293), tensor(2.5214), tensor(1.4921), tensor(1.5019), tensor(1.4949), tensor(1.9467), tensor(2.5516), tensor(1.5078), tensor(1.6720), tensor(1.5142), tensor(1.4895), tensor(1.5003), tensor(1.4913), tensor(1.4999), tensor(1.4982), tensor(1.4446), tensor(2.1230), tensor(1.4675), tensor(1.5094), tensor(1.4978), tensor(1.4721), tensor(1.5233), tensor(1.4897), tensor(1.4503), tensor(1.4704), tensor(1.4931), tensor(1.5263), tensor(1.5948), tensor(1.4800), tensor(1.4578), tensor(1.5032), tensor(1.5859), tensor(1.7270), tensor(1.5942), tensor(1.4933), tensor(1.5032), tensor(1.4819), tensor(1.5012), tensor(1.5153), tensor(1.5681), tensor(1.4740), tensor(1.4945), tensor(1.6556), tensor(1.4645), tensor(1.5372), tensor(1.4999), tensor(1.8888), tensor(2.0605), tensor(1.4328), tensor(1.5437), tensor(1.4946), tensor(2.1977), tensor(2.0123), tensor(2.1104), tensor(2.1710), tensor(1.4877), tensor(1.5328), tensor(1.5141), tensor(1.7791), tensor(1.6479), tensor(1.4782), tensor(1.4759), tensor(1.4498), tensor(1.4905), tensor(1.4645), tensor(1.5062), tensor(1.4728), tensor(1.5171), tensor(1.4923), tensor(2.6251), tensor(1.4885), tensor(1.4742), tensor(1.4802), tensor(1.4822), tensor(1.7764), tensor(4.2905), tensor(1.5184), tensor(1.5150), tensor(1.5918), tensor(1.5978), tensor(1.4627), tensor(1.4888), tensor(1.4976), tensor(1.4893), tensor(1.4811), tensor(1.4831), tensor(1.4457), tensor(1.5452), tensor(1.5293), tensor(1.4572), tensor(2.0662), tensor(1.4925), tensor(1.5496), tensor(1.4902), tensor(1.4828), tensor(2.3105), tensor(1.4764), tensor(1.5507), tensor(1.4945), tensor(1.5961), tensor(1.4970), tensor(1.4694), tensor(1.4533), tensor(1.4579), tensor(1.4621), tensor(1.5033), tensor(1.4921), tensor(1.4715), tensor(1.4532), tensor(1.4489), tensor(1.4893), tensor(1.5150), tensor(1.8086), tensor(1.4799), tensor(1.5512), tensor(1.5864), tensor(1.4844), tensor(1.5213), tensor(1.4929), tensor(1.5344), tensor(1.5204), tensor(2.0666), tensor(1.7041), tensor(1.4869), tensor(1.4703), tensor(1.5773), tensor(1.4545), tensor(1.7137), tensor(1.5316), tensor(1.4727), tensor(2.6493), tensor(1.5420), tensor(1.8744), tensor(1.4803), tensor(2.6631), tensor(1.4959), tensor(1.5074), tensor(1.5972), tensor(2.7021), tensor(1.4507), tensor(1.7943), tensor(1.4987), tensor(2.4072), tensor(1.5131), tensor(1.6107)]
09/23/2021 13:59:00 - INFO - __main__ - after_losses=[tensor(1.4945), tensor(1.8577), tensor(1.4387), tensor(1.6893), tensor(1.4806), tensor(1.5194), tensor(1.4676), tensor(1.5039), tensor(1.4833), tensor(1.4736), tensor(1.4878), tensor(1.5191), tensor(2.3937), tensor(1.4863), tensor(1.5815), tensor(1.5358), tensor(1.4623), tensor(2.1212), tensor(1.5107), tensor(1.5856), tensor(1.4699), tensor(1.5284), tensor(1.4406), tensor(1.4527), tensor(1.4773), tensor(1.5225), tensor(1.4832), tensor(1.5266), tensor(1.4500), tensor(1.5246), tensor(1.4932), tensor(1.4786), tensor(1.4974), tensor(1.5733), tensor(1.4953), tensor(1.4962), tensor(1.4975), tensor(1.4908), tensor(1.4682), tensor(1.4820), tensor(1.5205), tensor(1.4718), tensor(1.4632), tensor(1.4940), tensor(1.4797), tensor(3.6318), tensor(1.4691), tensor(1.6875), tensor(1.5263), tensor(1.4820), tensor(1.7637), tensor(2.4739), tensor(1.4769), tensor(1.5117), tensor(1.4848), tensor(1.8317), tensor(2.2630), tensor(1.4746), tensor(1.9766), tensor(1.5087), tensor(1.4682), tensor(1.5073), tensor(1.4764), tensor(1.4891), tensor(1.4941), tensor(1.4429), tensor(2.0081), tensor(1.4686), tensor(1.5258), tensor(1.5075), tensor(1.4907), tensor(1.5133), tensor(1.5213), tensor(1.4428), tensor(1.4725), tensor(1.4685), tensor(1.5096), tensor(1.6168), tensor(1.4929), tensor(1.4560), tensor(1.5007), tensor(1.5611), tensor(1.7589), tensor(1.6246), tensor(1.4868), tensor(1.4732), tensor(1.4894), tensor(1.4911), tensor(1.5121), tensor(1.5300), tensor(1.4722), tensor(1.4860), tensor(1.7407), tensor(1.4531), tensor(1.5811), tensor(1.4963), tensor(1.4772), tensor(2.6208), tensor(1.4347), tensor(1.5204), tensor(1.4927), tensor(2.1377), tensor(1.7132), tensor(2.2918), tensor(1.9952), tensor(1.4881), tensor(1.5187), tensor(1.5077), tensor(1.5184), tensor(1.6340), tensor(1.4799), tensor(1.4801), tensor(1.4408), tensor(1.4784), tensor(1.4635), tensor(1.5032), tensor(1.4731), tensor(1.5170), tensor(1.4851), tensor(3.1119), tensor(1.4872), tensor(1.4706), tensor(1.4782), tensor(1.4965), tensor(1.5328), tensor(4.0713), tensor(1.5131), tensor(1.5110), tensor(1.5770), tensor(1.6416), tensor(1.4547), tensor(1.4934), tensor(1.5013), tensor(1.4965), tensor(1.4809), tensor(1.4807), tensor(1.4496), tensor(1.5308), tensor(1.5201), tensor(1.4587), tensor(2.0368), tensor(1.4943), tensor(1.5325), tensor(1.4863), tensor(1.4737), tensor(2.2517), tensor(1.4763), tensor(1.5269), tensor(1.4974), tensor(1.5678), tensor(1.4982), tensor(1.4553), tensor(1.4486), tensor(1.4537), tensor(1.4565), tensor(1.5139), tensor(1.5359), tensor(1.4710), tensor(1.4450), tensor(1.4503), tensor(1.4843), tensor(1.6074), tensor(2.5170), tensor(1.4751), tensor(3.1071), tensor(1.5929), tensor(1.5218), tensor(1.5190), tensor(1.4908), tensor(1.5573), tensor(1.5217), tensor(1.9936), tensor(1.9095), tensor(1.4752), tensor(1.4739), tensor(1.5460), tensor(1.4467), tensor(1.7094), tensor(1.4924), tensor(1.4727), tensor(2.7638), tensor(1.5368), tensor(1.7865), tensor(1.4740), tensor(2.6540), tensor(1.4871), tensor(1.4783), tensor(1.5560), tensor(2.6110), tensor(1.4468), tensor(1.7837), tensor(1.5001), tensor(2.1206), tensor(1.4949), tensor(1.7462)]
09/23/2021 13:59:00 - INFO - __main__ - interference_scores=[tensor(-0.0177), tensor(0.0342), tensor(-0.0060), tensor(-0.2972), tensor(-0.0065), tensor(-0.0611), tensor(-0.0148), tensor(0.0246), tensor(0.0046), tensor(-0.0075), tensor(0.0098), tensor(-0.3655), tensor(0.2281), tensor(-0.0004), tensor(0.0239), tensor(0.0007), tensor(-0.1983), tensor(0.0832), tensor(-0.0388), tensor(0.0528), tensor(-0.0104), tensor(-0.0044), tensor(0.0020), tensor(-0.0007), tensor(-0.0148), tensor(0.0261), tensor(0.0054), tensor(-0.0009), tensor(-0.0090), tensor(-0.0101), tensor(-0.0219), tensor(0.0009), tensor(-0.0042), tensor(0.0323), tensor(-0.0379), tensor(-0.0023), tensor(-0.0118), tensor(-0.0018), tensor(0.0028), tensor(-0.0153), tensor(-0.0200), tensor(0.0022), tensor(0.0012), tensor(-0.0250), tensor(-0.0065), tensor(-0.2776), tensor(-0.0057), tensor(-0.0258), tensor(0.0089), tensor(-0.0069), tensor(0.0344), tensor(-0.0476), tensor(-0.0153), tensor(0.0098), tensor(-0.0100), tensor(-0.1150), tensor(-0.2886), tensor(-0.0333), tensor(0.3046), tensor(-0.0055), tensor(-0.0214), tensor(0.0070), tensor(-0.0149), tensor(-0.0109), tensor(-0.0040), tensor(-0.0017), tensor(-0.1149), tensor(0.0011), tensor(0.0164), tensor(0.0097), tensor(0.0187), tensor(-0.0100), tensor(0.0317), tensor(-0.0074), tensor(0.0021), tensor(-0.0246), tensor(-0.0167), tensor(0.0220), tensor(0.0129), tensor(-0.0018), tensor(-0.0026), tensor(-0.0247), tensor(0.0319), tensor(0.0304), tensor(-0.0065), tensor(-0.0300), tensor(0.0075), tensor(-0.0101), tensor(-0.0032), tensor(-0.0381), tensor(-0.0017), tensor(-0.0085), tensor(0.0850), tensor(-0.0114), tensor(0.0439), tensor(-0.0036), tensor(-0.4116), tensor(0.5602), tensor(0.0019), tensor(-0.0232), tensor(-0.0020), tensor(-0.0599), tensor(-0.2991), tensor(0.1814), tensor(-0.1758), tensor(0.0004), tensor(-0.0141), tensor(-0.0064), tensor(-0.2608), tensor(-0.0139), tensor(0.0017), tensor(0.0043), tensor(-0.0090), tensor(-0.0121), tensor(-0.0010), tensor(-0.0030), tensor(0.0003), tensor(-0.0002), tensor(-0.0072), tensor(0.4868), tensor(-0.0013), tensor(-0.0036), tensor(-0.0020), tensor(0.0142), tensor(-0.2436), tensor(-0.2192), tensor(-0.0053), tensor(-0.0041), tensor(-0.0148), tensor(0.0438), tensor(-0.0080), tensor(0.0046), tensor(0.0036), tensor(0.0072), tensor(-0.0003), tensor(-0.0024), tensor(0.0039), tensor(-0.0144), tensor(-0.0092), tensor(0.0015), tensor(-0.0294), tensor(0.0018), tensor(-0.0171), tensor(-0.0039), tensor(-0.0091), tensor(-0.0588), tensor(-7.2002e-05), tensor(-0.0238), tensor(0.0029), tensor(-0.0282), tensor(0.0012), tensor(-0.0140), tensor(-0.0047), tensor(-0.0042), tensor(-0.0057), tensor(0.0107), tensor(0.0437), tensor(-0.0004), tensor(-0.0082), tensor(0.0013), tensor(-0.0050), tensor(0.0925), tensor(0.7084), tensor(-0.0049), tensor(1.5559), tensor(0.0065), tensor(0.0375), tensor(-0.0024), tensor(-0.0021), tensor(0.0229), tensor(0.0012), tensor(-0.0730), tensor(0.2054), tensor(-0.0118), tensor(0.0036), tensor(-0.0314), tensor(-0.0078), tensor(-0.0043), tensor(-0.0392), tensor(-7.0333e-06), tensor(0.1145), tensor(-0.0052), tensor(-0.0880), tensor(-0.0063), tensor(-0.0091), tensor(-0.0087), tensor(-0.0291), tensor(-0.0411), tensor(-0.0910), tensor(-0.0039), tensor(-0.0106), tensor(0.0013), tensor(-0.2866), tensor(-0.0182), tensor(0.1355)]
09/23/2021 13:59:00 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-6947', 'mrqa_squad-validation-3463', 'mrqa_triviaqa-validation-1616', 'mrqa_triviaqa-validation-1550', 'mrqa_squad-validation-8821', 'mrqa_squad-validation-6677', 'mrqa_triviaqa-validation-5406', 'mrqa_triviaqa-validation-1575', 'mrqa_triviaqa-validation-4729', 'mrqa_squad-validation-10015', 'mrqa_squad-validation-5622', 'mrqa_squad-validation-392', 'mrqa_triviaqa-validation-3074', 'mrqa_triviaqa-validation-3647', 'mrqa_triviaqa-validation-2210', 'mrqa_squad-validation-2757', 'mrqa_triviaqa-validation-1540', 'mrqa_squad-validation-2629', 'mrqa_squad-validation-1539', 'mrqa_triviaqa-validation-6402', 'mrqa_triviaqa-validation-2465', 'mrqa_squad-validation-10322', 'mrqa_naturalquestions-validation-1618', 'mrqa_triviaqa-validation-3339', 'mrqa_triviaqa-validation-5231', 'mrqa_triviaqa-validation-3808', 'mrqa_squad-validation-8832', 'mrqa_triviaqa-validation-1924', 'mrqa_triviaqa-validation-1935', 'mrqa_hotpotqa-validation-5802', 'mrqa_triviaqa-validation-3901', 'mrqa_naturalquestions-validation-10680']
09/23/2021 13:59:00 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:59:00 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 7
09/23/2021 13:59:13 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:59:13 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 7
09/23/2021 13:59:16 - INFO - __main__ - After Error Fixing: 0.875
09/23/2021 13:59:16 - INFO - __main__ - Instant Fixing Rate: 0.9615384615384616
09/23/2021 13:59:16 - INFO - __main__ - Instant Retention Rate: 0.49999999916666665
09/23/2021 13:59:18 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_008.pt.
09/23/2021 13:59:18 - INFO - __main__ - Saving the current error examples (len=26) to the memory.
09/23/2021 13:59:18 - INFO - __main__ - Current memory size: 195.
09/23/2021 13:59:18 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 13:59:18 - INFO - __main__ - Finished.
09/23/2021 13:59:18 - INFO - __main__ - --------------------------------------------------
09/23/2021 13:59:18 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 13:59:18 - INFO - __main__ - Evaluating to get errors .... Timecode: 8
09/23/2021 13:59:21 - INFO - __main__ - Before Error Fixing: 0.0625
09/23/2021 13:59:21 - INFO - __main__ - Found 30 errors.
09/23/2021 13:59:21 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 13:59:21 - INFO - __main__ - Current memory size: 221.
09/23/2021 13:59:21 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=221;
09/23/2021 13:59:42 - INFO - __main__ - before_losses=[tensor(1.4985), tensor(1.7093), tensor(1.4802), tensor(1.5194), tensor(1.5028), tensor(1.9425), tensor(1.5054), tensor(1.4459), tensor(1.4838), tensor(1.5237), tensor(1.5193), tensor(1.8087), tensor(1.5495), tensor(1.4828), tensor(1.5087), tensor(1.6483), tensor(1.5338), tensor(1.5628), tensor(1.4442), tensor(1.5099), tensor(2.7367), tensor(1.5358), tensor(1.4617), tensor(1.4695), tensor(1.4832), tensor(2.7102), tensor(1.4732), tensor(3.0621), tensor(1.4933), tensor(1.5086), tensor(1.5207), tensor(1.5195), tensor(1.5410), tensor(1.4801), tensor(2.3450), tensor(1.5379), tensor(2.6695), tensor(1.5152), tensor(1.9845), tensor(1.4537), tensor(1.5449), tensor(1.4659), tensor(1.5136), tensor(1.4930), tensor(2.5793), tensor(1.7980), tensor(1.5667), tensor(1.5369), tensor(2.0029), tensor(1.5083), tensor(1.4693), tensor(1.5669), tensor(1.5155), tensor(1.5477), tensor(1.4893), tensor(1.4649), tensor(2.2349), tensor(1.4979), tensor(1.4441), tensor(1.9050), tensor(1.4799), tensor(1.4783), tensor(1.4879), tensor(1.4946), tensor(1.5305), tensor(1.5989), tensor(2.4926), tensor(1.5427), tensor(1.5027), tensor(1.5008), tensor(1.5124), tensor(1.4836), tensor(1.4756), tensor(1.7072), tensor(1.5102), tensor(1.5428), tensor(1.4860), tensor(1.4843), tensor(1.5426), tensor(1.4955), tensor(1.5135), tensor(1.4794), tensor(2.1827), tensor(1.5031), tensor(1.4829), tensor(1.6050), tensor(2.2145), tensor(1.4899), tensor(3.2981), tensor(1.4784), tensor(1.5041), tensor(1.5013), tensor(1.4940), tensor(1.5374), tensor(1.5039), tensor(1.4837), tensor(3.9611), tensor(1.5119), tensor(1.5196), tensor(1.4551), tensor(2.3130), tensor(1.4725), tensor(1.6743), tensor(1.5244), tensor(1.4799), tensor(1.5089), tensor(1.4692), tensor(2.1072), tensor(1.4972), tensor(1.5372), tensor(1.5095), tensor(1.4906), tensor(1.6214), tensor(1.5516), tensor(1.4918), tensor(1.4983), tensor(1.5361), tensor(1.5683), tensor(2.0572), tensor(1.5309), tensor(2.0416), tensor(1.5168), tensor(1.4979), tensor(1.4721), tensor(1.7292), tensor(1.7837), tensor(1.4703), tensor(1.4536), tensor(1.5026), tensor(1.6383), tensor(2.7402), tensor(1.4855), tensor(1.5071), tensor(1.7225), tensor(1.6022), tensor(1.5468), tensor(3.2998), tensor(1.5084), tensor(4.1450), tensor(1.5088), tensor(1.5478), tensor(1.4992), tensor(1.5384), tensor(1.4572), tensor(1.4636), tensor(3.1621), tensor(1.5878), tensor(1.4844), tensor(1.7754), tensor(1.5204), tensor(2.1173), tensor(1.8870), tensor(1.6690), tensor(1.9348), tensor(1.9960), tensor(1.4762), tensor(1.4679), tensor(1.4839), tensor(1.4701), tensor(1.4960), tensor(1.5841), tensor(1.5122), tensor(1.5214), tensor(1.6325), tensor(1.5224), tensor(1.5779), tensor(1.4974), tensor(2.3643), tensor(2.0910), tensor(1.4750), tensor(1.4664), tensor(1.5438), tensor(1.5951), tensor(1.5119), tensor(1.5065), tensor(2.2897), tensor(1.4682), tensor(1.4925), tensor(1.5117), tensor(1.5304), tensor(1.4998), tensor(2.0239), tensor(1.4981), tensor(1.4674), tensor(1.4921), tensor(1.7770), tensor(1.5706), tensor(1.5240), tensor(1.5417), tensor(1.4738), tensor(1.5493), tensor(1.6595), tensor(1.9215), tensor(1.4626), tensor(1.4986), tensor(1.4913), tensor(1.5568), tensor(2.0717), tensor(1.5002), tensor(1.5097), tensor(1.6152), tensor(1.4962), tensor(1.5236), tensor(1.4607), tensor(1.5260), tensor(1.4512), tensor(1.5905), tensor(1.4779), tensor(1.5461), tensor(1.6072), tensor(1.5702), tensor(1.4760), tensor(1.4944), tensor(1.9788), tensor(1.4922), tensor(1.5206), tensor(1.5033), tensor(1.6316), tensor(1.5487), tensor(1.5173), tensor(1.4877)]
09/23/2021 13:59:42 - INFO - __main__ - after_losses=[tensor(1.4722), tensor(1.4782), tensor(1.4728), tensor(1.5000), tensor(1.4953), tensor(2.0407), tensor(1.4882), tensor(1.4404), tensor(1.4771), tensor(1.5070), tensor(1.5195), tensor(1.7777), tensor(1.5203), tensor(1.4676), tensor(1.4985), tensor(1.6505), tensor(1.5158), tensor(1.5217), tensor(1.4410), tensor(1.5086), tensor(1.7861), tensor(2.0698), tensor(1.4520), tensor(1.4593), tensor(1.4692), tensor(2.7208), tensor(1.4590), tensor(2.8995), tensor(1.4822), tensor(1.5277), tensor(1.5284), tensor(1.4634), tensor(1.5166), tensor(1.4689), tensor(2.4388), tensor(1.4921), tensor(2.5756), tensor(1.4986), tensor(1.9751), tensor(1.4443), tensor(1.5057), tensor(1.4465), tensor(1.5090), tensor(1.4892), tensor(2.7889), tensor(1.8686), tensor(1.5367), tensor(1.5628), tensor(1.9254), tensor(1.4910), tensor(1.4630), tensor(1.5489), tensor(1.5021), tensor(1.5164), tensor(1.4776), tensor(1.4536), tensor(2.1865), tensor(1.4888), tensor(1.4410), tensor(1.9368), tensor(1.4639), tensor(1.4551), tensor(1.4774), tensor(1.4762), tensor(1.4849), tensor(1.5136), tensor(2.5572), tensor(1.5015), tensor(1.4948), tensor(1.4901), tensor(1.4907), tensor(1.4732), tensor(1.4676), tensor(1.5084), tensor(1.4830), tensor(1.5094), tensor(1.4660), tensor(1.4716), tensor(1.5148), tensor(1.4893), tensor(1.5068), tensor(1.4564), tensor(2.2132), tensor(1.4900), tensor(1.4907), tensor(1.5495), tensor(2.3346), tensor(1.5267), tensor(2.4633), tensor(1.4786), tensor(1.4989), tensor(1.4724), tensor(1.4788), tensor(1.5474), tensor(1.4818), tensor(1.4807), tensor(3.9786), tensor(1.5090), tensor(1.5130), tensor(1.4558), tensor(2.1917), tensor(1.4578), tensor(1.6417), tensor(1.5173), tensor(1.4721), tensor(1.5033), tensor(1.4604), tensor(2.0089), tensor(1.4899), tensor(1.4717), tensor(1.4824), tensor(1.4551), tensor(2.0913), tensor(1.5301), tensor(1.4878), tensor(1.4836), tensor(1.4994), tensor(1.5846), tensor(1.8858), tensor(1.4955), tensor(2.2553), tensor(1.4893), tensor(1.4822), tensor(1.4528), tensor(2.4772), tensor(1.7855), tensor(1.4542), tensor(1.4442), tensor(1.4824), tensor(1.5269), tensor(1.6573), tensor(1.4719), tensor(1.5052), tensor(1.5852), tensor(1.5815), tensor(1.4884), tensor(1.8190), tensor(1.5034), tensor(3.3962), tensor(1.4998), tensor(1.5612), tensor(1.4740), tensor(1.5156), tensor(1.4492), tensor(1.4514), tensor(1.7736), tensor(1.5329), tensor(1.4608), tensor(1.6600), tensor(1.5171), tensor(1.6508), tensor(1.8787), tensor(1.5503), tensor(1.6401), tensor(2.0015), tensor(1.4700), tensor(1.4633), tensor(1.5497), tensor(1.4667), tensor(1.4786), tensor(1.5288), tensor(1.4917), tensor(1.4894), tensor(1.7569), tensor(1.5104), tensor(1.5702), tensor(1.4936), tensor(2.3686), tensor(1.8698), tensor(1.4652), tensor(1.4563), tensor(1.5315), tensor(1.5959), tensor(1.4738), tensor(1.5295), tensor(1.9530), tensor(1.4491), tensor(1.4771), tensor(1.5065), tensor(1.5139), tensor(1.4931), tensor(1.5111), tensor(1.4876), tensor(1.4532), tensor(1.5690), tensor(1.7218), tensor(1.5399), tensor(1.4900), tensor(1.5262), tensor(1.4693), tensor(1.5533), tensor(1.5834), tensor(1.7482), tensor(1.4527), tensor(1.4896), tensor(1.4809), tensor(1.5847), tensor(2.1166), tensor(1.4902), tensor(1.4873), tensor(1.6082), tensor(1.4794), tensor(1.5030), tensor(1.4518), tensor(1.4994), tensor(1.4493), tensor(1.4993), tensor(1.4682), tensor(1.4960), tensor(1.5316), tensor(1.5527), tensor(1.4654), tensor(1.4811), tensor(1.8434), tensor(1.4761), tensor(1.5234), tensor(1.4868), tensor(1.5379), tensor(1.4823), tensor(1.4935), tensor(1.4741)]
09/23/2021 13:59:42 - INFO - __main__ - interference_scores=[tensor(-0.0263), tensor(-0.2310), tensor(-0.0075), tensor(-0.0193), tensor(-0.0075), tensor(0.0982), tensor(-0.0172), tensor(-0.0055), tensor(-0.0067), tensor(-0.0167), tensor(0.0003), tensor(-0.0310), tensor(-0.0292), tensor(-0.0151), tensor(-0.0103), tensor(0.0022), tensor(-0.0180), tensor(-0.0411), tensor(-0.0032), tensor(-0.0013), tensor(-0.9506), tensor(0.5341), tensor(-0.0097), tensor(-0.0103), tensor(-0.0141), tensor(0.0106), tensor(-0.0142), tensor(-0.1626), tensor(-0.0112), tensor(0.0191), tensor(0.0076), tensor(-0.0561), tensor(-0.0244), tensor(-0.0112), tensor(0.0938), tensor(-0.0459), tensor(-0.0939), tensor(-0.0166), tensor(-0.0094), tensor(-0.0094), tensor(-0.0392), tensor(-0.0194), tensor(-0.0046), tensor(-0.0038), tensor(0.2096), tensor(0.0706), tensor(-0.0299), tensor(0.0259), tensor(-0.0775), tensor(-0.0173), tensor(-0.0063), tensor(-0.0180), tensor(-0.0134), tensor(-0.0312), tensor(-0.0117), tensor(-0.0113), tensor(-0.0484), tensor(-0.0091), tensor(-0.0031), tensor(0.0319), tensor(-0.0160), tensor(-0.0232), tensor(-0.0105), tensor(-0.0184), tensor(-0.0455), tensor(-0.0852), tensor(0.0646), tensor(-0.0412), tensor(-0.0079), tensor(-0.0107), tensor(-0.0217), tensor(-0.0105), tensor(-0.0080), tensor(-0.1989), tensor(-0.0272), tensor(-0.0335), tensor(-0.0201), tensor(-0.0127), tensor(-0.0279), tensor(-0.0062), tensor(-0.0066), tensor(-0.0230), tensor(0.0305), tensor(-0.0131), tensor(0.0079), tensor(-0.0555), tensor(0.1201), tensor(0.0369), tensor(-0.8348), tensor(0.0002), tensor(-0.0052), tensor(-0.0289), tensor(-0.0152), tensor(0.0100), tensor(-0.0220), tensor(-0.0030), tensor(0.0175), tensor(-0.0029), tensor(-0.0066), tensor(0.0007), tensor(-0.1213), tensor(-0.0147), tensor(-0.0325), tensor(-0.0071), tensor(-0.0077), tensor(-0.0056), tensor(-0.0088), tensor(-0.0982), tensor(-0.0073), tensor(-0.0656), tensor(-0.0271), tensor(-0.0355), tensor(0.4698), tensor(-0.0215), tensor(-0.0040), tensor(-0.0148), tensor(-0.0368), tensor(0.0163), tensor(-0.1713), tensor(-0.0354), tensor(0.2137), tensor(-0.0275), tensor(-0.0157), tensor(-0.0193), tensor(0.7480), tensor(0.0017), tensor(-0.0161), tensor(-0.0094), tensor(-0.0202), tensor(-0.1114), tensor(-1.0829), tensor(-0.0136), tensor(-0.0019), tensor(-0.1373), tensor(-0.0207), tensor(-0.0584), tensor(-1.4808), tensor(-0.0050), tensor(-0.7488), tensor(-0.0090), tensor(0.0134), tensor(-0.0252), tensor(-0.0228), tensor(-0.0080), tensor(-0.0122), tensor(-1.3885), tensor(-0.0549), tensor(-0.0236), tensor(-0.1155), tensor(-0.0032), tensor(-0.4664), tensor(-0.0083), tensor(-0.1187), tensor(-0.2947), tensor(0.0055), tensor(-0.0062), tensor(-0.0046), tensor(0.0658), tensor(-0.0034), tensor(-0.0174), tensor(-0.0554), tensor(-0.0205), tensor(-0.0320), tensor(0.1244), tensor(-0.0119), tensor(-0.0077), tensor(-0.0037), tensor(0.0043), tensor(-0.2211), tensor(-0.0099), tensor(-0.0101), tensor(-0.0123), tensor(0.0008), tensor(-0.0380), tensor(0.0230), tensor(-0.3367), tensor(-0.0192), tensor(-0.0154), tensor(-0.0052), tensor(-0.0166), tensor(-0.0067), tensor(-0.5127), tensor(-0.0105), tensor(-0.0142), tensor(0.0769), tensor(-0.0552), tensor(-0.0307), tensor(-0.0340), tensor(-0.0156), tensor(-0.0044), tensor(0.0039), tensor(-0.0760), tensor(-0.1733), tensor(-0.0099), tensor(-0.0090), tensor(-0.0104), tensor(0.0280), tensor(0.0449), tensor(-0.0100), tensor(-0.0225), tensor(-0.0070), tensor(-0.0168), tensor(-0.0206), tensor(-0.0089), tensor(-0.0265), tensor(-0.0019), tensor(-0.0912), tensor(-0.0097), tensor(-0.0501), tensor(-0.0756), tensor(-0.0176), tensor(-0.0106), tensor(-0.0133), tensor(-0.1354), tensor(-0.0161), tensor(0.0028), tensor(-0.0165), tensor(-0.0937), tensor(-0.0664), tensor(-0.0238), tensor(-0.0136)]
09/23/2021 13:59:42 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-8821', 'mrqa_squad-validation-392', 'mrqa_triviaqa-validation-2096', 'mrqa_squad-validation-7799', 'mrqa_triviaqa-validation-2368', 'mrqa_triviaqa-validation-254', 'mrqa_triviaqa-validation-2376', 'mrqa_triviaqa-validation-5795', 'mrqa_squad-validation-5517', 'mrqa_triviaqa-validation-1540', 'mrqa_triviaqa-validation-5937', 'mrqa_triviaqa-validation-3808', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-6887', 'mrqa_naturalquestions-validation-10680', 'mrqa_naturalquestions-validation-2501', 'mrqa_squad-validation-7149', 'mrqa_triviaqa-validation-5406', 'mrqa_naturalquestions-validation-7511', 'mrqa_triviaqa-validation-1924', 'mrqa_triviaqa-validation-1575', 'mrqa_naturalquestions-validation-6736', 'mrqa_triviaqa-validation-105', 'mrqa_triviaqa-validation-5362', 'mrqa_triviaqa-validation-1616', 'mrqa_triviaqa-validation-3647', 'mrqa_hotpotqa-validation-14', 'mrqa_triviaqa-validation-4560', 'mrqa_triviaqa-validation-5261', 'mrqa_triviaqa-validation-4966', 'mrqa_triviaqa-validation-287', 'mrqa_triviaqa-validation-1494']
09/23/2021 13:59:42 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 13:59:42 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=62) .... Timecode: 8
09/23/2021 13:59:55 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 13:59:55 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 8
09/23/2021 13:59:58 - INFO - __main__ - After Error Fixing: 0.9375
09/23/2021 13:59:58 - INFO - __main__ - Instant Fixing Rate: 0.9666666666666667
09/23/2021 13:59:58 - INFO - __main__ - Instant Retention Rate: 0.4999999975
09/23/2021 14:00:00 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_009.pt.
09/23/2021 14:00:00 - INFO - __main__ - Saving the current error examples (len=30) to the memory.
09/23/2021 14:00:00 - INFO - __main__ - Current memory size: 221.
09/23/2021 14:00:00 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:00:00 - INFO - __main__ - Finished.
09/23/2021 14:00:00 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:00:00 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:00:00 - INFO - __main__ - Evaluating to get errors .... Timecode: 9
09/23/2021 14:00:03 - INFO - __main__ - Before Error Fixing: 0.25
09/23/2021 14:00:03 - INFO - __main__ - Found 24 errors.
09/23/2021 14:00:03 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:00:03 - INFO - __main__ - Current memory size: 251.
09/23/2021 14:00:03 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=251;
09/23/2021 14:00:26 - INFO - __main__ - before_losses=[tensor(1.5355), tensor(1.5413), tensor(2.5758), tensor(1.4935), tensor(1.4612), tensor(1.5106), tensor(1.5107), tensor(3.1257), tensor(1.4550), tensor(1.4609), tensor(1.5265), tensor(1.6162), tensor(1.7410), tensor(1.4951), tensor(1.7809), tensor(1.4814), tensor(1.4978), tensor(2.6496), tensor(1.5108), tensor(1.4846), tensor(1.5073), tensor(1.5752), tensor(1.5006), tensor(1.5157), tensor(1.4571), tensor(1.4609), tensor(1.4956), tensor(1.4777), tensor(1.5145), tensor(1.6148), tensor(1.4697), tensor(1.4724), tensor(2.0848), tensor(1.8816), tensor(1.4923), tensor(1.5119), tensor(1.4832), tensor(1.4892), tensor(1.4933), tensor(1.4839), tensor(1.4953), tensor(1.5427), tensor(1.5443), tensor(1.5091), tensor(1.4743), tensor(1.4539), tensor(1.5459), tensor(1.5432), tensor(1.4949), tensor(1.7726), tensor(1.8476), tensor(1.5085), tensor(1.4878), tensor(1.8799), tensor(1.5057), tensor(1.4928), tensor(1.4737), tensor(1.5125), tensor(1.4592), tensor(1.6030), tensor(1.4677), tensor(2.1540), tensor(1.4525), tensor(1.5094), tensor(1.4990), tensor(1.5033), tensor(1.5036), tensor(1.5340), tensor(1.4876), tensor(2.0331), tensor(1.5480), tensor(1.4645), tensor(2.6305), tensor(1.4955), tensor(2.8028), tensor(1.4402), tensor(1.4916), tensor(1.4829), tensor(1.6374), tensor(1.4984), tensor(1.4899), tensor(1.7991), tensor(1.4840), tensor(1.5124), tensor(1.5005), tensor(2.2472), tensor(1.4667), tensor(1.5212), tensor(1.4407), tensor(1.5120), tensor(1.5113), tensor(1.5128), tensor(1.4833), tensor(1.5136), tensor(1.5193), tensor(1.5335), tensor(1.4780), tensor(1.4843), tensor(1.5659), tensor(1.4898), tensor(1.4412), tensor(1.5017), tensor(2.5068), tensor(1.5106), tensor(1.4670), tensor(1.4619), tensor(1.4660), tensor(1.5122), tensor(1.4877), tensor(1.5122), tensor(1.4576), tensor(2.1850), tensor(1.5369), tensor(1.4796), tensor(1.5089), tensor(1.4733), tensor(3.4796), tensor(1.4706), tensor(1.4952), tensor(1.5237), tensor(1.4654), tensor(1.4827), tensor(1.4555), tensor(1.4487), tensor(2.0217), tensor(1.5427), tensor(1.4848), tensor(1.4737), tensor(1.5074), tensor(1.5096), tensor(1.5198), tensor(1.4949), tensor(1.5275), tensor(1.5753), tensor(2.0014), tensor(1.4995), tensor(1.5325), tensor(1.4617), tensor(2.8542), tensor(1.4805), tensor(1.4851), tensor(1.4725), tensor(1.4899), tensor(1.4820), tensor(1.6686), tensor(1.5135), tensor(1.6863), tensor(1.4961), tensor(1.4902), tensor(1.6122), tensor(1.5283), tensor(1.5242), tensor(1.4862), tensor(1.4904), tensor(1.6282), tensor(1.5155), tensor(1.7497), tensor(1.5136), tensor(1.4734), tensor(1.5682), tensor(1.4934), tensor(1.5560), tensor(1.4990), tensor(1.4618), tensor(1.6828), tensor(1.5206), tensor(1.4893), tensor(1.8676), tensor(1.4706), tensor(1.4820), tensor(1.8895), tensor(1.5135), tensor(2.7151), tensor(1.5242), tensor(1.9618), tensor(1.5195), tensor(1.4816), tensor(1.5198), tensor(1.4980), tensor(1.5541), tensor(1.4802), tensor(1.8063), tensor(1.5225), tensor(1.4626), tensor(2.1026), tensor(1.4883), tensor(1.5272), tensor(1.5240), tensor(1.4717), tensor(2.1348), tensor(1.4909), tensor(1.4630), tensor(1.5061), tensor(1.5003), tensor(1.4820), tensor(1.5004), tensor(1.5141), tensor(1.4635), tensor(1.5054), tensor(1.5238), tensor(1.5984), tensor(1.4943), tensor(2.7448), tensor(1.4890), tensor(1.5170), tensor(1.4564), tensor(1.4654), tensor(1.4731), tensor(1.4796), tensor(1.5418), tensor(1.4714), tensor(1.5273), tensor(3.8683), tensor(1.6493), tensor(1.5301), tensor(1.5116), tensor(1.6763), tensor(1.5027), tensor(1.5246), tensor(2.4893), tensor(1.5247), tensor(1.5294), tensor(1.4486), tensor(1.4610), tensor(1.4952), tensor(1.5016), tensor(1.7402), tensor(1.5398), tensor(1.5120), tensor(1.7524), tensor(1.4939), tensor(1.5621), tensor(1.5064), tensor(1.5321), tensor(1.7558), tensor(1.5112), tensor(1.5168), tensor(1.4885), tensor(2.6067), tensor(1.4912), tensor(2.5840), tensor(1.5148), tensor(1.4636), tensor(1.4710), tensor(2.4392), tensor(1.5001), tensor(1.5336), tensor(1.4912), tensor(1.5542), tensor(1.4922), tensor(1.5041)]
09/23/2021 14:00:26 - INFO - __main__ - after_losses=[tensor(1.5204), tensor(1.5177), tensor(2.1520), tensor(1.4760), tensor(1.4442), tensor(1.4870), tensor(1.4968), tensor(3.1935), tensor(1.4403), tensor(1.4438), tensor(1.5054), tensor(1.5567), tensor(2.8146), tensor(1.4901), tensor(2.5352), tensor(1.4633), tensor(1.4705), tensor(2.2665), tensor(1.5093), tensor(1.4701), tensor(1.4781), tensor(1.5479), tensor(1.4867), tensor(1.4936), tensor(1.4460), tensor(1.7463), tensor(1.4850), tensor(1.4726), tensor(1.4936), tensor(1.5801), tensor(1.4558), tensor(1.4622), tensor(1.7359), tensor(1.9494), tensor(1.4734), tensor(1.5194), tensor(1.5105), tensor(1.4679), tensor(1.4763), tensor(1.4670), tensor(1.4734), tensor(1.6617), tensor(1.6054), tensor(1.5059), tensor(1.4554), tensor(1.4365), tensor(1.5066), tensor(1.5146), tensor(1.4810), tensor(1.6930), tensor(1.7978), tensor(1.4854), tensor(1.4665), tensor(2.0730), tensor(1.4790), tensor(1.4617), tensor(1.4705), tensor(1.4806), tensor(1.4390), tensor(1.5815), tensor(1.4544), tensor(2.1268), tensor(1.4424), tensor(1.4746), tensor(1.4833), tensor(1.4922), tensor(1.5126), tensor(1.6324), tensor(1.4843), tensor(2.0843), tensor(1.5246), tensor(1.4510), tensor(2.6220), tensor(1.4720), tensor(2.8317), tensor(1.4359), tensor(1.4679), tensor(1.4836), tensor(1.5651), tensor(1.4616), tensor(1.4701), tensor(1.8441), tensor(1.4766), tensor(1.4834), tensor(1.4752), tensor(3.3528), tensor(1.4714), tensor(1.5461), tensor(1.4309), tensor(1.5027), tensor(1.4739), tensor(1.4857), tensor(1.4701), tensor(1.5187), tensor(1.5378), tensor(1.6345), tensor(1.4825), tensor(1.4696), tensor(1.5482), tensor(1.4634), tensor(1.4291), tensor(1.4885), tensor(2.7383), tensor(1.4984), tensor(1.4418), tensor(1.4747), tensor(1.4423), tensor(1.5501), tensor(1.4700), tensor(1.4509), tensor(1.4454), tensor(2.2007), tensor(1.5171), tensor(1.4738), tensor(1.4884), tensor(1.4535), tensor(2.7022), tensor(1.4550), tensor(1.4753), tensor(1.5228), tensor(1.4512), tensor(1.4745), tensor(1.4331), tensor(1.4322), tensor(2.2034), tensor(1.6251), tensor(1.5222), tensor(1.4635), tensor(1.5030), tensor(1.4835), tensor(1.4999), tensor(1.4766), tensor(1.4747), tensor(1.5969), tensor(1.8140), tensor(1.4874), tensor(1.4904), tensor(1.4490), tensor(1.8744), tensor(1.4795), tensor(1.4681), tensor(1.4613), tensor(1.4681), tensor(1.4565), tensor(1.6457), tensor(1.4979), tensor(1.7082), tensor(1.5022), tensor(1.4988), tensor(1.5660), tensor(1.5400), tensor(1.5122), tensor(1.4613), tensor(1.4804), tensor(1.5356), tensor(1.5587), tensor(1.4685), tensor(1.5018), tensor(1.4548), tensor(1.5346), tensor(1.4814), tensor(1.5330), tensor(1.4774), tensor(1.4505), tensor(1.6898), tensor(1.4988), tensor(1.4856), tensor(1.9536), tensor(1.4668), tensor(1.4632), tensor(1.8676), tensor(1.5082), tensor(2.6537), tensor(1.5233), tensor(1.4835), tensor(1.4968), tensor(1.4703), tensor(1.5248), tensor(1.4810), tensor(1.5092), tensor(1.4655), tensor(1.7789), tensor(1.5055), tensor(1.4560), tensor(1.9854), tensor(1.4801), tensor(1.5229), tensor(1.4992), tensor(1.5053), tensor(2.0320), tensor(1.4699), tensor(1.4536), tensor(1.5010), tensor(1.4814), tensor(1.4726), tensor(1.4845), tensor(1.5560), tensor(1.4402), tensor(1.4723), tensor(1.5202), tensor(1.5818), tensor(1.4863), tensor(2.7851), tensor(1.4739), tensor(1.4954), tensor(1.4421), tensor(1.4719), tensor(1.4534), tensor(1.4661), tensor(1.5370), tensor(1.4525), tensor(1.5850), tensor(3.6446), tensor(1.6147), tensor(1.6734), tensor(1.4926), tensor(1.4780), tensor(1.4791), tensor(1.5146), tensor(2.3753), tensor(1.6618), tensor(1.5183), tensor(1.4369), tensor(1.4467), tensor(1.4718), tensor(1.4775), tensor(2.6157), tensor(1.5259), tensor(1.4948), tensor(1.8707), tensor(1.4791), tensor(1.5782), tensor(1.4828), tensor(1.4994), tensor(1.8217), tensor(1.4846), tensor(1.5006), tensor(1.4627), tensor(2.5363), tensor(1.4730), tensor(2.6477), tensor(1.4868), tensor(1.4509), tensor(1.4596), tensor(2.0763), tensor(1.4735), tensor(1.4986), tensor(2.0529), tensor(1.5425), tensor(1.4683), tensor(1.5043)]
09/23/2021 14:00:26 - INFO - __main__ - interference_scores=[tensor(-0.0152), tensor(-0.0237), tensor(-0.4238), tensor(-0.0175), tensor(-0.0170), tensor(-0.0236), tensor(-0.0139), tensor(0.0678), tensor(-0.0148), tensor(-0.0172), tensor(-0.0211), tensor(-0.0595), tensor(1.0736), tensor(-0.0049), tensor(0.7543), tensor(-0.0180), tensor(-0.0273), tensor(-0.3831), tensor(-0.0015), tensor(-0.0145), tensor(-0.0292), tensor(-0.0273), tensor(-0.0139), tensor(-0.0220), tensor(-0.0111), tensor(0.2854), tensor(-0.0106), tensor(-0.0051), tensor(-0.0209), tensor(-0.0347), tensor(-0.0139), tensor(-0.0101), tensor(-0.3489), tensor(0.0679), tensor(-0.0189), tensor(0.0075), tensor(0.0273), tensor(-0.0213), tensor(-0.0170), tensor(-0.0169), tensor(-0.0219), tensor(0.1190), tensor(0.0611), tensor(-0.0032), tensor(-0.0189), tensor(-0.0174), tensor(-0.0393), tensor(-0.0286), tensor(-0.0139), tensor(-0.0796), tensor(-0.0498), tensor(-0.0232), tensor(-0.0212), tensor(0.1931), tensor(-0.0267), tensor(-0.0311), tensor(-0.0032), tensor(-0.0319), tensor(-0.0202), tensor(-0.0215), tensor(-0.0133), tensor(-0.0272), tensor(-0.0101), tensor(-0.0348), tensor(-0.0158), tensor(-0.0111), tensor(0.0089), tensor(0.0983), tensor(-0.0033), tensor(0.0512), tensor(-0.0234), tensor(-0.0135), tensor(-0.0085), tensor(-0.0236), tensor(0.0289), tensor(-0.0043), tensor(-0.0237), tensor(0.0008), tensor(-0.0723), tensor(-0.0368), tensor(-0.0198), tensor(0.0450), tensor(-0.0074), tensor(-0.0290), tensor(-0.0253), tensor(1.1056), tensor(0.0047), tensor(0.0250), tensor(-0.0099), tensor(-0.0092), tensor(-0.0374), tensor(-0.0271), tensor(-0.0132), tensor(0.0051), tensor(0.0184), tensor(0.1010), tensor(0.0045), tensor(-0.0147), tensor(-0.0177), tensor(-0.0264), tensor(-0.0122), tensor(-0.0132), tensor(0.2314), tensor(-0.0122), tensor(-0.0252), tensor(0.0128), tensor(-0.0237), tensor(0.0379), tensor(-0.0177), tensor(-0.0613), tensor(-0.0121), tensor(0.0157), tensor(-0.0198), tensor(-0.0058), tensor(-0.0205), tensor(-0.0198), tensor(-0.7774), tensor(-0.0156), tensor(-0.0199), tensor(-0.0010), tensor(-0.0142), tensor(-0.0082), tensor(-0.0224), tensor(-0.0165), tensor(0.1817), tensor(0.0824), tensor(0.0374), tensor(-0.0102), tensor(-0.0044), tensor(-0.0261), tensor(-0.0199), tensor(-0.0183), tensor(-0.0528), tensor(0.0216), tensor(-0.1874), tensor(-0.0120), tensor(-0.0422), tensor(-0.0128), tensor(-0.9798), tensor(-0.0010), tensor(-0.0170), tensor(-0.0113), tensor(-0.0218), tensor(-0.0255), tensor(-0.0229), tensor(-0.0156), tensor(0.0219), tensor(0.0062), tensor(0.0086), tensor(-0.0462), tensor(0.0116), tensor(-0.0120), tensor(-0.0249), tensor(-0.0100), tensor(-0.0926), tensor(0.0432), tensor(-0.2811), tensor(-0.0118), tensor(-0.0186), tensor(-0.0335), tensor(-0.0120), tensor(-0.0230), tensor(-0.0215), tensor(-0.0112), tensor(0.0070), tensor(-0.0218), tensor(-0.0037), tensor(0.0860), tensor(-0.0038), tensor(-0.0188), tensor(-0.0219), tensor(-0.0053), tensor(-0.0615), tensor(-0.0009), tensor(-0.4782), tensor(-0.0227), tensor(-0.0114), tensor(0.0050), tensor(-0.0171), tensor(-0.0449), tensor(-0.0147), tensor(-0.0274), tensor(-0.0170), tensor(-0.0067), tensor(-0.1172), tensor(-0.0083), tensor(-0.0042), tensor(-0.0247), tensor(0.0336), tensor(-0.1028), tensor(-0.0210), tensor(-0.0093), tensor(-0.0050), tensor(-0.0189), tensor(-0.0095), tensor(-0.0159), tensor(0.0418), tensor(-0.0232), tensor(-0.0332), tensor(-0.0036), tensor(-0.0165), tensor(-0.0080), tensor(0.0403), tensor(-0.0151), tensor(-0.0216), tensor(-0.0144), tensor(0.0065), tensor(-0.0197), tensor(-0.0135), tensor(-0.0048), tensor(-0.0189), tensor(0.0577), tensor(-0.2236), tensor(-0.0346), tensor(0.1433), tensor(-0.0190), tensor(-0.1983), tensor(-0.0236), tensor(-0.0100), tensor(-0.1141), tensor(0.1371), tensor(-0.0111), tensor(-0.0118), tensor(-0.0143), tensor(-0.0234), tensor(-0.0241), tensor(0.8755), tensor(-0.0139), tensor(-0.0172), tensor(0.1183), tensor(-0.0149), tensor(0.0161), tensor(-0.0237), tensor(-0.0327), tensor(0.0659), tensor(-0.0266), tensor(-0.0162), tensor(-0.0258), tensor(-0.0704), tensor(-0.0183), tensor(0.0637), tensor(-0.0280), tensor(-0.0126), tensor(-0.0114), tensor(-0.3629), tensor(-0.0266), tensor(-0.0350), tensor(0.5617), tensor(-0.0117), tensor(-0.0239), tensor(0.0003)]
09/23/2021 14:00:26 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-7767', 'mrqa_triviaqa-validation-316', 'mrqa_triviaqa-validation-1616', 'mrqa_triviaqa-validation-2530', 'mrqa_triviaqa-validation-5261', 'mrqa_squad-validation-5517', 'mrqa_triviaqa-validation-2376', 'mrqa_squad-validation-5622', 'mrqa_triviaqa-validation-5654', 'mrqa_triviaqa-validation-365', 'mrqa_triviaqa-validation-1085', 'mrqa_triviaqa-validation-227', 'mrqa_triviaqa-validation-781', 'mrqa_triviaqa-validation-3339', 'mrqa_triviaqa-validation-2210', 'mrqa_squad-validation-10015', 'mrqa_triviaqa-validation-3714', 'mrqa_triviaqa-validation-3915', 'mrqa_squad-validation-3463', 'mrqa_squad-validation-6677', 'mrqa_triviaqa-validation-4729', 'mrqa_triviaqa-validation-5026', 'mrqa_triviaqa-validation-1494', 'mrqa_squad-validation-3971', 'mrqa_triviaqa-validation-1550', 'mrqa_triviaqa-validation-4560', 'mrqa_triviaqa-validation-4196', 'mrqa_naturalquestions-validation-5437', 'mrqa_triviaqa-validation-7382', 'mrqa_triviaqa-validation-7021', 'mrqa_naturalquestions-validation-10680', 'mrqa_squad-validation-6947']
09/23/2021 14:00:26 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:00:26 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=56) .... Timecode: 9
09/23/2021 14:00:39 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:00:39 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 9
09/23/2021 14:00:43 - INFO - __main__ - After Error Fixing: 0.875
09/23/2021 14:00:43 - INFO - __main__ - Instant Fixing Rate: 0.9583333333333334
09/23/2021 14:00:43 - INFO - __main__ - Instant Retention Rate: 0.6249999992187499
09/23/2021 14:00:45 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_010.pt.
09/23/2021 14:00:45 - INFO - __main__ - Saving the current error examples (len=24) to the memory.
09/23/2021 14:00:45 - INFO - __main__ - Current memory size: 251.
09/23/2021 14:00:45 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:00:45 - INFO - __main__ - Finished.
09/23/2021 14:00:45 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:00:45 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:00:45 - INFO - __main__ - Evaluating to get errors .... Timecode: 10
09/23/2021 14:00:48 - INFO - __main__ - Before Error Fixing: 0.15625
09/23/2021 14:00:48 - INFO - __main__ - Found 27 errors.
09/23/2021 14:00:48 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:00:48 - INFO - __main__ - Current memory size: 275.
09/23/2021 14:00:48 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:01:11 - INFO - __main__ - before_losses=[tensor(1.5110), tensor(1.4536), tensor(1.5177), tensor(1.6772), tensor(1.4902), tensor(2.8814), tensor(1.4998), tensor(1.5107), tensor(1.5160), tensor(1.4855), tensor(1.4667), tensor(1.4883), tensor(1.5361), tensor(3.7090), tensor(1.4993), tensor(1.5052), tensor(1.5025), tensor(1.5132), tensor(1.4839), tensor(2.2262), tensor(1.4637), tensor(1.4841), tensor(1.5205), tensor(1.5012), tensor(1.8488), tensor(1.5225), tensor(1.4657), tensor(1.7338), tensor(1.4840), tensor(1.5496), tensor(1.7796), tensor(1.5112), tensor(1.4989), tensor(1.4749), tensor(1.5059), tensor(1.5250), tensor(1.6263), tensor(1.5286), tensor(1.5144), tensor(1.5292), tensor(1.9271), tensor(1.5072), tensor(1.4859), tensor(1.4979), tensor(2.7019), tensor(1.4959), tensor(1.5144), tensor(1.5943), tensor(1.7327), tensor(1.4962), tensor(1.7047), tensor(1.4843), tensor(1.5221), tensor(1.5018), tensor(1.5731), tensor(2.6332), tensor(2.3409), tensor(1.4592), tensor(1.4773), tensor(1.5290), tensor(1.5569), tensor(1.5812), tensor(1.7483), tensor(1.5007), tensor(1.5463), tensor(1.5073), tensor(1.4514), tensor(1.5038), tensor(1.5209), tensor(1.5395), tensor(1.4828), tensor(1.5781), tensor(1.5497), tensor(1.4803), tensor(1.4664), tensor(1.4994), tensor(2.2731), tensor(2.4996), tensor(1.5530), tensor(1.5313), tensor(1.4598), tensor(1.5228), tensor(1.4744), tensor(1.5307), tensor(1.9015), tensor(1.8827), tensor(1.5014), tensor(1.5027), tensor(1.5372), tensor(1.5372), tensor(1.5067), tensor(1.4987), tensor(1.5550), tensor(1.5422), tensor(1.4379), tensor(1.4767), tensor(1.5362), tensor(2.3324), tensor(1.5044), tensor(1.5225), tensor(3.1442), tensor(1.5447), tensor(1.4585), tensor(1.4946), tensor(2.1803), tensor(1.5156), tensor(1.5199), tensor(1.4879), tensor(1.4409), tensor(2.1962), tensor(1.4810), tensor(1.5039), tensor(1.5160), tensor(1.4532), tensor(1.5288), tensor(1.4871), tensor(1.5065), tensor(1.4660), tensor(1.5468), tensor(1.4495), tensor(1.4941), tensor(1.5169), tensor(1.4833), tensor(1.4762), tensor(1.7756), tensor(1.4977), tensor(1.5096), tensor(1.4657), tensor(1.4491), tensor(1.5179), tensor(1.8573), tensor(1.4608), tensor(1.5064), tensor(1.6160), tensor(2.0975), tensor(2.4903), tensor(1.4633), tensor(1.5104), tensor(1.5497), tensor(2.1337), tensor(1.6058), tensor(1.4692), tensor(2.7482), tensor(1.4567), tensor(1.4933), tensor(1.4552), tensor(1.5175), tensor(1.4469), tensor(1.5091), tensor(1.5544), tensor(1.4822), tensor(1.5385), tensor(1.5675), tensor(1.5246), tensor(1.5187), tensor(1.6860), tensor(1.4701), tensor(1.4670), tensor(1.5185), tensor(1.4790), tensor(1.7302), tensor(1.4565), tensor(1.5149), tensor(1.4943), tensor(1.5019), tensor(2.4157), tensor(1.5146), tensor(1.5063), tensor(1.4912), tensor(1.4639), tensor(3.2722), tensor(1.4703), tensor(1.4937), tensor(1.4417), tensor(1.4796), tensor(1.4412), tensor(1.4769), tensor(2.3683), tensor(1.5751), tensor(1.5097), tensor(1.4682), tensor(1.4983), tensor(1.4777), tensor(1.4898), tensor(2.3645), tensor(1.5698), tensor(1.5400), tensor(1.5621), tensor(2.5214), tensor(1.5200), tensor(2.3095), tensor(1.5961), tensor(1.5044), tensor(1.5173), tensor(1.5070), tensor(3.1244), tensor(1.7411), tensor(1.4948), tensor(1.5101), tensor(1.5053), tensor(2.0485), tensor(1.4961), tensor(1.5499), tensor(2.0357), tensor(1.5200), tensor(1.4629), tensor(1.5277), tensor(1.4971), tensor(2.0501), tensor(1.5759), tensor(1.4786), tensor(1.4929), tensor(1.9044), tensor(1.4376), tensor(1.5294), tensor(1.4786), tensor(1.4769), tensor(1.4566), tensor(1.4838), tensor(1.4995), tensor(1.4965), tensor(2.6804), tensor(1.5811), tensor(2.0424), tensor(1.4848), tensor(1.4974), tensor(1.5083), tensor(1.4862), tensor(1.5080), tensor(1.5037), tensor(1.5086), tensor(1.5094), tensor(1.9558), tensor(2.8022), tensor(1.4856), tensor(1.5072), tensor(1.5947), tensor(1.5383), tensor(1.5018), tensor(1.5124), tensor(3.6871), tensor(2.1868), tensor(1.5660), tensor(1.4613), tensor(1.4751), tensor(1.4761), tensor(1.5099), tensor(2.5956), tensor(1.4820), tensor(1.5353), tensor(2.7522), tensor(1.5099), tensor(3.4655), tensor(1.4785), tensor(1.4683), tensor(1.5043)]
09/23/2021 14:01:11 - INFO - __main__ - after_losses=[tensor(1.5042), tensor(1.4535), tensor(1.5089), tensor(1.6644), tensor(1.4809), tensor(2.8551), tensor(1.4942), tensor(1.5062), tensor(1.5041), tensor(1.4832), tensor(1.4571), tensor(1.4753), tensor(1.5391), tensor(3.6882), tensor(1.4971), tensor(1.5043), tensor(1.4867), tensor(1.5220), tensor(1.4802), tensor(2.2531), tensor(1.4628), tensor(1.4609), tensor(1.4905), tensor(1.4998), tensor(1.5329), tensor(1.5853), tensor(1.4631), tensor(1.7759), tensor(1.4843), tensor(1.5438), tensor(1.6264), tensor(1.5084), tensor(1.4893), tensor(1.4608), tensor(1.5166), tensor(1.5222), tensor(1.5625), tensor(1.5225), tensor(1.5287), tensor(1.5363), tensor(1.5027), tensor(1.4966), tensor(1.4830), tensor(1.4594), tensor(3.1487), tensor(1.4883), tensor(1.4918), tensor(1.5463), tensor(1.8974), tensor(1.4802), tensor(1.7240), tensor(1.4802), tensor(1.5214), tensor(1.5020), tensor(1.5652), tensor(2.4817), tensor(2.2740), tensor(1.4606), tensor(1.4659), tensor(1.5014), tensor(1.6000), tensor(1.5904), tensor(1.6314), tensor(1.4666), tensor(1.5283), tensor(1.4995), tensor(1.4515), tensor(1.4976), tensor(1.4643), tensor(1.5222), tensor(1.4796), tensor(1.5125), tensor(1.5479), tensor(1.4815), tensor(1.4520), tensor(1.4997), tensor(2.2820), tensor(2.3633), tensor(1.5217), tensor(1.5069), tensor(1.4527), tensor(1.4975), tensor(1.4749), tensor(1.5104), tensor(1.8720), tensor(1.8765), tensor(1.4773), tensor(1.5053), tensor(1.5271), tensor(1.5348), tensor(1.5027), tensor(1.4932), tensor(1.5388), tensor(1.5342), tensor(1.4397), tensor(1.4754), tensor(1.5680), tensor(1.5264), tensor(1.4951), tensor(1.5052), tensor(2.7100), tensor(1.5352), tensor(1.4574), tensor(1.4825), tensor(2.1167), tensor(1.5070), tensor(1.5199), tensor(1.4843), tensor(1.4424), tensor(1.6665), tensor(1.4808), tensor(1.5325), tensor(1.4985), tensor(1.4525), tensor(1.5760), tensor(1.4819), tensor(1.4903), tensor(1.4584), tensor(1.5479), tensor(1.4494), tensor(1.4932), tensor(1.5070), tensor(1.4792), tensor(1.4699), tensor(1.6474), tensor(1.4898), tensor(1.5077), tensor(1.4624), tensor(1.4432), tensor(1.5018), tensor(1.8759), tensor(1.4572), tensor(1.5030), tensor(1.6712), tensor(1.9914), tensor(1.7983), tensor(1.4531), tensor(1.4951), tensor(1.5648), tensor(2.1729), tensor(1.6695), tensor(1.4738), tensor(2.7595), tensor(1.4565), tensor(1.5108), tensor(1.4536), tensor(1.5217), tensor(1.4484), tensor(1.5091), tensor(1.5307), tensor(1.4852), tensor(1.5267), tensor(1.5093), tensor(1.5124), tensor(1.5105), tensor(1.5220), tensor(1.4657), tensor(1.4579), tensor(1.4874), tensor(1.4711), tensor(1.5727), tensor(1.4501), tensor(1.5121), tensor(1.4652), tensor(1.4874), tensor(2.3820), tensor(1.5079), tensor(1.4998), tensor(1.4833), tensor(1.4571), tensor(3.1420), tensor(1.4661), tensor(1.4628), tensor(1.4420), tensor(1.4723), tensor(1.4387), tensor(1.4784), tensor(1.7263), tensor(1.5176), tensor(1.4880), tensor(1.4466), tensor(1.4832), tensor(1.6584), tensor(1.4825), tensor(1.6582), tensor(1.5517), tensor(1.5170), tensor(1.5465), tensor(2.6337), tensor(1.5238), tensor(2.6914), tensor(1.6499), tensor(1.4961), tensor(1.5129), tensor(1.4840), tensor(3.0345), tensor(1.8262), tensor(1.4854), tensor(1.5931), tensor(1.4873), tensor(1.9808), tensor(1.4951), tensor(1.5041), tensor(2.2897), tensor(1.5168), tensor(1.4564), tensor(1.5256), tensor(1.4857), tensor(1.6124), tensor(1.5584), tensor(1.4680), tensor(1.4904), tensor(1.8467), tensor(1.4383), tensor(1.5221), tensor(1.4855), tensor(1.4601), tensor(1.4528), tensor(1.4672), tensor(1.5014), tensor(1.4751), tensor(1.5668), tensor(1.4813), tensor(1.7111), tensor(1.5534), tensor(1.5033), tensor(1.5112), tensor(1.4818), tensor(1.4896), tensor(1.4943), tensor(1.5058), tensor(1.5019), tensor(1.5240), tensor(2.8935), tensor(1.4798), tensor(1.4947), tensor(1.5383), tensor(1.5383), tensor(1.5096), tensor(1.5086), tensor(3.4763), tensor(1.5302), tensor(1.5065), tensor(1.4533), tensor(1.4697), tensor(1.4780), tensor(1.4860), tensor(2.3296), tensor(1.4766), tensor(1.5215), tensor(1.6303), tensor(1.5012), tensor(1.4851), tensor(1.4873), tensor(1.4597), tensor(1.4741)]
09/23/2021 14:01:11 - INFO - __main__ - interference_scores=[tensor(-0.0068), tensor(-0.0002), tensor(-0.0088), tensor(-0.0128), tensor(-0.0093), tensor(-0.0263), tensor(-0.0056), tensor(-0.0045), tensor(-0.0119), tensor(-0.0023), tensor(-0.0095), tensor(-0.0130), tensor(0.0030), tensor(-0.0208), tensor(-0.0022), tensor(-0.0008), tensor(-0.0158), tensor(0.0088), tensor(-0.0037), tensor(0.0269), tensor(-0.0009), tensor(-0.0232), tensor(-0.0299), tensor(-0.0014), tensor(-0.3159), tensor(0.0628), tensor(-0.0026), tensor(0.0421), tensor(0.0003), tensor(-0.0057), tensor(-0.1532), tensor(-0.0029), tensor(-0.0096), tensor(-0.0141), tensor(0.0106), tensor(-0.0027), tensor(-0.0638), tensor(-0.0061), tensor(0.0143), tensor(0.0071), tensor(-0.4244), tensor(-0.0105), tensor(-0.0028), tensor(-0.0386), tensor(0.4468), tensor(-0.0076), tensor(-0.0227), tensor(-0.0480), tensor(0.1647), tensor(-0.0161), tensor(0.0193), tensor(-0.0041), tensor(-0.0007), tensor(0.0002), tensor(-0.0079), tensor(-0.1515), tensor(-0.0669), tensor(0.0014), tensor(-0.0113), tensor(-0.0276), tensor(0.0431), tensor(0.0092), tensor(-0.1170), tensor(-0.0340), tensor(-0.0180), tensor(-0.0078), tensor(0.0002), tensor(-0.0063), tensor(-0.0566), tensor(-0.0173), tensor(-0.0031), tensor(-0.0656), tensor(-0.0017), tensor(0.0011), tensor(-0.0144), tensor(0.0003), tensor(0.0089), tensor(-0.1363), tensor(-0.0312), tensor(-0.0244), tensor(-0.0071), tensor(-0.0253), tensor(0.0005), tensor(-0.0203), tensor(-0.0294), tensor(-0.0063), tensor(-0.0241), tensor(0.0026), tensor(-0.0101), tensor(-0.0024), tensor(-0.0039), tensor(-0.0055), tensor(-0.0161), tensor(-0.0081), tensor(0.0017), tensor(-0.0012), tensor(0.0318), tensor(-0.8060), tensor(-0.0094), tensor(-0.0173), tensor(-0.4342), tensor(-0.0095), tensor(-0.0012), tensor(-0.0120), tensor(-0.0636), tensor(-0.0086), tensor(4.0531e-05), tensor(-0.0036), tensor(0.0015), tensor(-0.5297), tensor(-0.0003), tensor(0.0285), tensor(-0.0174), tensor(-0.0007), tensor(0.0472), tensor(-0.0052), tensor(-0.0162), tensor(-0.0076), tensor(0.0010), tensor(-0.0001), tensor(-0.0009), tensor(-0.0100), tensor(-0.0041), tensor(-0.0063), tensor(-0.1282), tensor(-0.0079), tensor(-0.0019), tensor(-0.0033), tensor(-0.0059), tensor(-0.0161), tensor(0.0187), tensor(-0.0037), tensor(-0.0034), tensor(0.0552), tensor(-0.1061), tensor(-0.6920), tensor(-0.0102), tensor(-0.0152), tensor(0.0151), tensor(0.0392), tensor(0.0637), tensor(0.0045), tensor(0.0114), tensor(-0.0002), tensor(0.0174), tensor(-0.0015), tensor(0.0042), tensor(0.0015), tensor(-6.4492e-05), tensor(-0.0236), tensor(0.0030), tensor(-0.0118), tensor(-0.0582), tensor(-0.0121), tensor(-0.0083), tensor(-0.1640), tensor(-0.0044), tensor(-0.0091), tensor(-0.0312), tensor(-0.0079), tensor(-0.1575), tensor(-0.0064), tensor(-0.0028), tensor(-0.0291), tensor(-0.0145), tensor(-0.0337), tensor(-0.0067), tensor(-0.0065), tensor(-0.0079), tensor(-0.0068), tensor(-0.1303), tensor(-0.0042), tensor(-0.0309), tensor(0.0003), tensor(-0.0073), tensor(-0.0025), tensor(0.0016), tensor(-0.6420), tensor(-0.0575), tensor(-0.0217), tensor(-0.0216), tensor(-0.0150), tensor(0.1807), tensor(-0.0073), tensor(-0.7063), tensor(-0.0181), tensor(-0.0230), tensor(-0.0156), tensor(0.1123), tensor(0.0038), tensor(0.3818), tensor(0.0538), tensor(-0.0083), tensor(-0.0045), tensor(-0.0230), tensor(-0.0900), tensor(0.0851), tensor(-0.0094), tensor(0.0830), tensor(-0.0181), tensor(-0.0678), tensor(-0.0011), tensor(-0.0457), tensor(0.2541), tensor(-0.0033), tensor(-0.0065), tensor(-0.0020), tensor(-0.0114), tensor(-0.4378), tensor(-0.0175), tensor(-0.0106), tensor(-0.0025), tensor(-0.0577), tensor(0.0008), tensor(-0.0072), tensor(0.0069), tensor(-0.0168), tensor(-0.0038), tensor(-0.0166), tensor(0.0019), tensor(-0.0215), tensor(-1.1136), tensor(-0.0998), tensor(-0.3313), tensor(0.0686), tensor(0.0059), tensor(0.0030), tensor(-0.0044), tensor(-0.0184), tensor(-0.0094), tensor(-0.0027), tensor(-0.0075), tensor(-0.4318), tensor(0.0912), tensor(-0.0058), tensor(-0.0125), tensor(-0.0564), tensor(-4.6134e-05), tensor(0.0079), tensor(-0.0038), tensor(-0.2108), tensor(-0.6566), tensor(-0.0596), tensor(-0.0079), tensor(-0.0054), tensor(0.0019), tensor(-0.0239), tensor(-0.2659), tensor(-0.0054), tensor(-0.0137), tensor(-1.1219), tensor(-0.0087), tensor(-1.9803), tensor(0.0088), tensor(-0.0086), tensor(-0.0302)]
09/23/2021 14:01:11 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-7767', 'mrqa_triviaqa-validation-2376', 'mrqa_triviaqa-validation-3647', 'mrqa_squad-validation-3463', 'mrqa_squad-validation-3181', 'mrqa_triviaqa-validation-3915', 'mrqa_triviaqa-validation-2368', 'mrqa_triviaqa-validation-5704', 'mrqa_squad-validation-2757', 'mrqa_naturalquestions-validation-9650', 'mrqa_naturalquestions-validation-2501', 'mrqa_triviaqa-validation-3714', 'mrqa_triviaqa-validation-5937', 'mrqa_squad-validation-9281', 'mrqa_triviaqa-validation-4856', 'mrqa_triviaqa-validation-6402', 'mrqa_naturalquestions-validation-7017', 'mrqa_squad-validation-8821', 'mrqa_triviaqa-validation-1494', 'mrqa_triviaqa-validation-3901', 'mrqa_squad-validation-7554', 'mrqa_hotpotqa-validation-983', 'mrqa_squad-validation-2584', 'mrqa_hotpotqa-validation-3146', 'mrqa_squad-validation-5622', 'mrqa_triviaqa-validation-1924', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-7369', 'mrqa_naturalquestions-validation-5180', 'mrqa_squad-validation-2987', 'mrqa_naturalquestions-validation-10364', 'mrqa_squad-validation-3442']
09/23/2021 14:01:11 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:01:11 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=59) .... Timecode: 10
09/23/2021 14:01:24 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:01:24 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 10
09/23/2021 14:01:28 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 14:01:28 - INFO - __main__ - Instant Fixing Rate: 0.8148148148148148
09/23/2021 14:01:28 - INFO - __main__ - Instant Retention Rate: 0.9999999980000001
09/23/2021 14:01:30 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_011.pt.
09/23/2021 14:01:30 - INFO - __main__ - Saving the current error examples (len=27) to the memory.
09/23/2021 14:01:30 - INFO - __main__ - Current memory size: 275.
09/23/2021 14:01:30 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:01:30 - INFO - __main__ - Finished.
09/23/2021 14:01:30 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:01:30 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:01:30 - INFO - __main__ - Evaluating to get errors .... Timecode: 11
09/23/2021 14:01:33 - INFO - __main__ - Before Error Fixing: 0.15625
09/23/2021 14:01:33 - INFO - __main__ - Found 27 errors.
09/23/2021 14:01:33 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:01:33 - INFO - __main__ - Current memory size: 302.
09/23/2021 14:01:33 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:01:56 - INFO - __main__ - before_losses=[tensor(1.4903), tensor(1.5084), tensor(1.5458), tensor(1.5114), tensor(1.4734), tensor(3.4959), tensor(1.5241), tensor(1.4822), tensor(1.7390), tensor(1.5020), tensor(1.5111), tensor(1.5196), tensor(1.4750), tensor(1.4928), tensor(1.5557), tensor(1.4674), tensor(1.5135), tensor(1.4669), tensor(1.4986), tensor(1.5665), tensor(4.6472), tensor(1.5158), tensor(1.5221), tensor(1.4750), tensor(1.4513), tensor(1.5758), tensor(1.5422), tensor(1.4916), tensor(1.5320), tensor(1.4763), tensor(1.4639), tensor(1.4612), tensor(1.4910), tensor(1.5158), tensor(1.8728), tensor(1.5300), tensor(1.6534), tensor(1.5696), tensor(1.5813), tensor(1.5403), tensor(1.5912), tensor(1.6628), tensor(1.4743), tensor(1.5223), tensor(2.3033), tensor(1.6506), tensor(1.6227), tensor(1.5260), tensor(1.5284), tensor(1.5230), tensor(1.4880), tensor(1.5334), tensor(1.7218), tensor(1.5494), tensor(1.4818), tensor(1.5188), tensor(2.3477), tensor(2.4523), tensor(1.4977), tensor(2.7817), tensor(1.4526), tensor(1.5253), tensor(1.4863), tensor(1.4702), tensor(1.4627), tensor(1.5614), tensor(1.4439), tensor(1.5402), tensor(2.3005), tensor(1.7751), tensor(2.1556), tensor(1.4682), tensor(1.6251), tensor(1.4788), tensor(1.4476), tensor(1.4799), tensor(1.4677), tensor(1.5047), tensor(1.4969), tensor(3.1336), tensor(1.4958), tensor(1.5267), tensor(1.5257), tensor(1.4864), tensor(1.5294), tensor(1.5270), tensor(1.4763), tensor(1.4931), tensor(1.5499), tensor(1.6390), tensor(2.3476), tensor(1.5033), tensor(1.5147), tensor(1.5264), tensor(1.5027), tensor(1.8083), tensor(1.5696), tensor(1.5191), tensor(1.4394), tensor(2.1963), tensor(1.5108), tensor(1.4636), tensor(2.6612), tensor(2.3931), tensor(1.5292), tensor(1.4836), tensor(2.9116), tensor(2.1576), tensor(1.4498), tensor(2.7343), tensor(1.5045), tensor(1.6012), tensor(1.4867), tensor(1.4423), tensor(1.8595), tensor(1.7647), tensor(1.4916), tensor(2.6274), tensor(1.5089), tensor(1.4949), tensor(1.5074), tensor(4.0637), tensor(2.0363), tensor(1.5311), tensor(1.4886), tensor(1.9009), tensor(1.4966), tensor(1.4994), tensor(1.7776), tensor(1.4566), tensor(1.5636), tensor(1.5221), tensor(1.5762), tensor(1.4884), tensor(2.0646), tensor(1.4375), tensor(1.4672), tensor(1.5119), tensor(1.5081), tensor(1.5050), tensor(1.9809), tensor(1.4922), tensor(2.7290), tensor(1.4709), tensor(1.5206), tensor(1.5228), tensor(1.5184), tensor(1.5178), tensor(1.5197), tensor(1.4898), tensor(1.5171), tensor(2.6426), tensor(1.5430), tensor(1.8339), tensor(1.5073), tensor(1.5085), tensor(1.4809), tensor(1.4968), tensor(1.5049), tensor(1.4656), tensor(1.5210), tensor(1.5127), tensor(1.5675), tensor(1.4909), tensor(1.4716), tensor(1.5134), tensor(1.6850), tensor(1.4987), tensor(1.5120), tensor(2.1199), tensor(1.5374), tensor(1.5638), tensor(1.5717), tensor(1.7114), tensor(1.6461), tensor(3.4166), tensor(1.5312), tensor(1.4993), tensor(1.4896), tensor(1.4784), tensor(1.5340), tensor(2.9258), tensor(2.0982), tensor(1.5337), tensor(1.5417), tensor(1.4645), tensor(2.0757), tensor(1.4824), tensor(1.4526), tensor(1.6133), tensor(1.4836), tensor(1.5444), tensor(1.4764), tensor(1.4779), tensor(1.5161), tensor(1.5070), tensor(1.4379), tensor(1.4649), tensor(1.5067), tensor(1.4899), tensor(1.8066), tensor(1.5024), tensor(2.1496), tensor(1.8272), tensor(1.4694), tensor(1.5312), tensor(1.4895), tensor(1.5821), tensor(1.4841), tensor(1.4764), tensor(1.5591), tensor(2.3168), tensor(1.5134), tensor(2.3429), tensor(2.0563), tensor(1.4650), tensor(1.5105), tensor(1.4744), tensor(1.9221), tensor(1.7604), tensor(1.4835), tensor(1.5224), tensor(1.4845), tensor(5.3902), tensor(1.4750), tensor(1.5122), tensor(2.5094), tensor(1.7580), tensor(1.4604), tensor(1.5246), tensor(1.4741), tensor(1.4653), tensor(1.4880), tensor(1.5010), tensor(1.4817), tensor(2.3111), tensor(2.2753), tensor(1.4836), tensor(1.4538), tensor(1.4581), tensor(1.5227), tensor(1.4735), tensor(1.5641), tensor(1.5728), tensor(1.5330), tensor(1.5542), tensor(1.5070), tensor(1.4981), tensor(1.5381), tensor(2.2210), tensor(1.5048), tensor(2.1806), tensor(1.4494), tensor(1.4998), tensor(1.5401), tensor(2.2620)]
09/23/2021 14:01:56 - INFO - __main__ - after_losses=[tensor(1.4823), tensor(1.4780), tensor(1.5315), tensor(1.4981), tensor(1.4694), tensor(3.8187), tensor(1.5188), tensor(1.4839), tensor(1.6783), tensor(1.4982), tensor(1.5127), tensor(1.4981), tensor(1.4738), tensor(1.4882), tensor(1.5084), tensor(1.4662), tensor(1.5302), tensor(1.4712), tensor(1.4859), tensor(1.5426), tensor(3.7386), tensor(1.5044), tensor(1.5069), tensor(1.4711), tensor(1.4499), tensor(1.5532), tensor(1.5241), tensor(1.4919), tensor(1.5226), tensor(1.4983), tensor(1.4602), tensor(1.4588), tensor(1.4848), tensor(1.5077), tensor(1.9086), tensor(1.4993), tensor(1.6208), tensor(2.0626), tensor(1.5518), tensor(1.5249), tensor(1.5752), tensor(1.9461), tensor(1.4779), tensor(1.5289), tensor(2.2194), tensor(1.7671), tensor(1.6144), tensor(1.5445), tensor(1.5267), tensor(1.5018), tensor(1.4792), tensor(1.5600), tensor(1.9338), tensor(1.5488), tensor(1.4719), tensor(1.7391), tensor(2.4002), tensor(2.1338), tensor(1.4968), tensor(2.8678), tensor(1.4546), tensor(1.5197), tensor(1.4774), tensor(1.4681), tensor(1.4594), tensor(1.5264), tensor(1.4471), tensor(1.5336), tensor(1.5696), tensor(1.5502), tensor(2.1772), tensor(1.4642), tensor(2.7740), tensor(1.4741), tensor(1.4478), tensor(1.4960), tensor(1.4668), tensor(1.4891), tensor(1.5184), tensor(3.4891), tensor(1.4979), tensor(1.4947), tensor(1.5213), tensor(1.4712), tensor(1.5472), tensor(1.4945), tensor(1.4877), tensor(1.4619), tensor(1.5242), tensor(2.2159), tensor(2.3953), tensor(1.4872), tensor(1.5183), tensor(1.5385), tensor(1.5031), tensor(1.5926), tensor(1.5924), tensor(1.5033), tensor(1.4372), tensor(2.1006), tensor(1.5086), tensor(1.4645), tensor(2.6619), tensor(1.5343), tensor(1.4982), tensor(1.4746), tensor(3.4826), tensor(2.0268), tensor(1.4499), tensor(2.6251), tensor(1.4945), tensor(1.5513), tensor(1.4801), tensor(1.4400), tensor(1.6144), tensor(1.5772), tensor(1.4883), tensor(2.3059), tensor(1.5046), tensor(1.4890), tensor(1.5435), tensor(3.9344), tensor(2.4979), tensor(1.5203), tensor(1.4661), tensor(1.5023), tensor(1.4910), tensor(1.5046), tensor(1.5253), tensor(1.4620), tensor(1.5524), tensor(1.5221), tensor(1.5118), tensor(1.4894), tensor(1.5218), tensor(1.4416), tensor(1.4494), tensor(1.5054), tensor(1.5018), tensor(1.4986), tensor(1.7052), tensor(1.4874), tensor(2.2033), tensor(1.4723), tensor(1.5131), tensor(1.5444), tensor(1.5165), tensor(1.5141), tensor(1.5189), tensor(1.4888), tensor(1.5757), tensor(2.6405), tensor(1.5241), tensor(1.5536), tensor(1.5025), tensor(1.5020), tensor(1.4682), tensor(1.4923), tensor(1.4935), tensor(1.4629), tensor(1.5124), tensor(1.5122), tensor(1.5674), tensor(1.4766), tensor(1.4824), tensor(1.5054), tensor(1.5593), tensor(1.4835), tensor(1.5031), tensor(1.5480), tensor(1.5102), tensor(1.5395), tensor(1.5451), tensor(1.5554), tensor(1.5983), tensor(2.5209), tensor(2.1920), tensor(1.5129), tensor(1.9164), tensor(1.4713), tensor(1.5767), tensor(2.4697), tensor(1.8626), tensor(1.5507), tensor(1.5395), tensor(1.4633), tensor(2.0134), tensor(1.4792), tensor(1.4599), tensor(1.6478), tensor(1.4815), tensor(1.5351), tensor(1.4682), tensor(1.4734), tensor(1.5270), tensor(1.5657), tensor(1.4371), tensor(1.4672), tensor(1.4964), tensor(1.5489), tensor(1.5364), tensor(1.5042), tensor(1.7823), tensor(1.8013), tensor(1.4680), tensor(1.5017), tensor(1.4722), tensor(1.6105), tensor(1.4831), tensor(1.4699), tensor(1.5323), tensor(2.1072), tensor(1.5056), tensor(2.4590), tensor(2.2514), tensor(1.4588), tensor(1.4934), tensor(1.4732), tensor(1.9970), tensor(1.6643), tensor(1.4700), tensor(1.5143), tensor(1.4804), tensor(3.5873), tensor(1.4765), tensor(1.5069), tensor(1.8173), tensor(1.7104), tensor(1.4590), tensor(1.5116), tensor(1.4594), tensor(1.4596), tensor(1.4872), tensor(1.4971), tensor(1.4824), tensor(2.1901), tensor(2.3201), tensor(1.4750), tensor(1.4541), tensor(1.4531), tensor(1.5071), tensor(1.4741), tensor(1.5337), tensor(1.5574), tensor(1.5968), tensor(1.5422), tensor(1.5020), tensor(1.4871), tensor(1.5349), tensor(2.2302), tensor(1.5049), tensor(2.4295), tensor(1.4451), tensor(1.5000), tensor(1.5533), tensor(2.2521)]
09/23/2021 14:01:56 - INFO - __main__ - interference_scores=[tensor(-0.0080), tensor(-0.0304), tensor(-0.0143), tensor(-0.0133), tensor(-0.0041), tensor(0.3228), tensor(-0.0053), tensor(0.0018), tensor(-0.0607), tensor(-0.0038), tensor(0.0016), tensor(-0.0215), tensor(-0.0012), tensor(-0.0046), tensor(-0.0473), tensor(-0.0012), tensor(0.0167), tensor(0.0044), tensor(-0.0127), tensor(-0.0239), tensor(-0.9086), tensor(-0.0114), tensor(-0.0153), tensor(-0.0039), tensor(-0.0015), tensor(-0.0225), tensor(-0.0181), tensor(0.0003), tensor(-0.0094), tensor(0.0220), tensor(-0.0037), tensor(-0.0024), tensor(-0.0062), tensor(-0.0080), tensor(0.0358), tensor(-0.0307), tensor(-0.0326), tensor(0.4930), tensor(-0.0295), tensor(-0.0154), tensor(-0.0160), tensor(0.2832), tensor(0.0035), tensor(0.0066), tensor(-0.0839), tensor(0.1165), tensor(-0.0083), tensor(0.0185), tensor(-0.0017), tensor(-0.0212), tensor(-0.0089), tensor(0.0267), tensor(0.2120), tensor(-0.0006), tensor(-0.0099), tensor(0.2203), tensor(0.0525), tensor(-0.3184), tensor(-0.0009), tensor(0.0861), tensor(0.0019), tensor(-0.0057), tensor(-0.0089), tensor(-0.0021), tensor(-0.0033), tensor(-0.0350), tensor(0.0032), tensor(-0.0066), tensor(-0.7309), tensor(-0.2249), tensor(0.0217), tensor(-0.0040), tensor(1.1490), tensor(-0.0047), tensor(0.0001), tensor(0.0161), tensor(-0.0008), tensor(-0.0156), tensor(0.0215), tensor(0.3555), tensor(0.0020), tensor(-0.0320), tensor(-0.0044), tensor(-0.0152), tensor(0.0179), tensor(-0.0325), tensor(0.0114), tensor(-0.0312), tensor(-0.0257), tensor(0.5769), tensor(0.0478), tensor(-0.0162), tensor(0.0036), tensor(0.0121), tensor(0.0005), tensor(-0.2156), tensor(0.0228), tensor(-0.0158), tensor(-0.0022), tensor(-0.0957), tensor(-0.0022), tensor(0.0008), tensor(0.0007), tensor(-0.8587), tensor(-0.0310), tensor(-0.0090), tensor(0.5710), tensor(-0.1309), tensor(6.8188e-05), tensor(-0.1093), tensor(-0.0100), tensor(-0.0499), tensor(-0.0066), tensor(-0.0023), tensor(-0.2451), tensor(-0.1876), tensor(-0.0033), tensor(-0.3214), tensor(-0.0043), tensor(-0.0059), tensor(0.0362), tensor(-0.1293), tensor(0.4616), tensor(-0.0108), tensor(-0.0225), tensor(-0.3986), tensor(-0.0056), tensor(0.0051), tensor(-0.2522), tensor(0.0054), tensor(-0.0112), tensor(7.1526e-06), tensor(-0.0644), tensor(0.0010), tensor(-0.5427), tensor(0.0041), tensor(-0.0178), tensor(-0.0065), tensor(-0.0063), tensor(-0.0063), tensor(-0.2757), tensor(-0.0047), tensor(-0.5257), tensor(0.0014), tensor(-0.0075), tensor(0.0216), tensor(-0.0019), tensor(-0.0037), tensor(-0.0008), tensor(-0.0010), tensor(0.0586), tensor(-0.0022), tensor(-0.0189), tensor(-0.2804), tensor(-0.0048), tensor(-0.0064), tensor(-0.0127), tensor(-0.0045), tensor(-0.0114), tensor(-0.0028), tensor(-0.0086), tensor(-0.0005), tensor(-9.2745e-05), tensor(-0.0142), tensor(0.0108), tensor(-0.0081), tensor(-0.1258), tensor(-0.0152), tensor(-0.0089), tensor(-0.5719), tensor(-0.0272), tensor(-0.0243), tensor(-0.0266), tensor(-0.1559), tensor(-0.0478), tensor(-0.8956), tensor(0.6608), tensor(0.0136), tensor(0.4268), tensor(-0.0072), tensor(0.0426), tensor(-0.4561), tensor(-0.2356), tensor(0.0170), tensor(-0.0023), tensor(-0.0012), tensor(-0.0623), tensor(-0.0033), tensor(0.0073), tensor(0.0345), tensor(-0.0022), tensor(-0.0093), tensor(-0.0081), tensor(-0.0045), tensor(0.0109), tensor(0.0588), tensor(-0.0009), tensor(0.0023), tensor(-0.0103), tensor(0.0590), tensor(-0.2702), tensor(0.0018), tensor(-0.3673), tensor(-0.0259), tensor(-0.0013), tensor(-0.0294), tensor(-0.0173), tensor(0.0284), tensor(-0.0009), tensor(-0.0065), tensor(-0.0268), tensor(-0.2096), tensor(-0.0078), tensor(0.1161), tensor(0.1951), tensor(-0.0061), tensor(-0.0171), tensor(-0.0012), tensor(0.0749), tensor(-0.0962), tensor(-0.0135), tensor(-0.0081), tensor(-0.0041), tensor(-1.8029), tensor(0.0015), tensor(-0.0053), tensor(-0.6921), tensor(-0.0476), tensor(-0.0014), tensor(-0.0130), tensor(-0.0146), tensor(-0.0057), tensor(-0.0007), tensor(-0.0040), tensor(0.0006), tensor(-0.1210), tensor(0.0448), tensor(-0.0086), tensor(0.0003), tensor(-0.0050), tensor(-0.0156), tensor(0.0005), tensor(-0.0304), tensor(-0.0155), tensor(0.0638), tensor(-0.0120), tensor(-0.0050), tensor(-0.0110), tensor(-0.0032), tensor(0.0091), tensor(9.0361e-05), tensor(0.2489), tensor(-0.0042), tensor(0.0001), tensor(0.0133), tensor(-0.0098)]
09/23/2021 14:01:56 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-6402', 'mrqa_naturalquestions-validation-2501', 'mrqa_squad-validation-5622', 'mrqa_triviaqa-validation-1521', 'mrqa_triviaqa-validation-5937', 'mrqa_triviaqa-validation-2376', 'mrqa_triviaqa-validation-7382', 'mrqa_squad-validation-8259', 'mrqa_triviaqa-validation-2703', 'mrqa_triviaqa-validation-7157', 'mrqa_triviaqa-validation-3915', 'mrqa_triviaqa-validation-4268', 'mrqa_triviaqa-validation-4966', 'mrqa_squad-validation-7554', 'mrqa_triviaqa-validation-2210', 'mrqa_triviaqa-validation-5972', 'mrqa_naturalquestions-validation-6736', 'mrqa_squad-validation-10015', 'mrqa_naturalquestions-validation-7017', 'mrqa_triviaqa-validation-5026', 'mrqa_triviaqa-validation-254', 'mrqa_hotpotqa-validation-2679', 'mrqa_triviaqa-validation-1616', 'mrqa_squad-validation-6677', 'mrqa_squad-validation-5517', 'mrqa_triviaqa-validation-1494', 'mrqa_triviaqa-validation-1085', 'mrqa_hotpotqa-validation-1436', 'mrqa_triviaqa-validation-781', 'mrqa_squad-validation-3558', 'mrqa_hotpotqa-validation-5682', 'mrqa_naturalquestions-validation-3474']
09/23/2021 14:01:56 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:01:56 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=59) .... Timecode: 11
09/23/2021 14:02:10 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:02:10 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 11
09/23/2021 14:02:13 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 14:02:13 - INFO - __main__ - Instant Fixing Rate: 1.0
09/23/2021 14:02:13 - INFO - __main__ - Instant Retention Rate: 0.3999999992
09/23/2021 14:02:15 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_012.pt.
09/23/2021 14:02:15 - INFO - __main__ - Saving the current error examples (len=27) to the memory.
09/23/2021 14:02:15 - INFO - __main__ - Current memory size: 302.
09/23/2021 14:02:15 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:02:15 - INFO - __main__ - Finished.
09/23/2021 14:02:15 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:02:15 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:02:15 - INFO - __main__ - Evaluating to get errors .... Timecode: 12
09/23/2021 14:02:18 - INFO - __main__ - Before Error Fixing: 0.125
09/23/2021 14:02:18 - INFO - __main__ - Found 28 errors.
09/23/2021 14:02:18 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:02:18 - INFO - __main__ - Current memory size: 329.
09/23/2021 14:02:18 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:02:41 - INFO - __main__ - before_losses=[tensor(1.5256), tensor(1.5712), tensor(1.8877), tensor(2.3880), tensor(3.2038), tensor(1.9146), tensor(1.5337), tensor(1.4797), tensor(1.5164), tensor(1.5343), tensor(1.7045), tensor(1.5207), tensor(1.5008), tensor(1.5416), tensor(1.5479), tensor(1.4792), tensor(2.2420), tensor(1.4854), tensor(1.5320), tensor(1.5436), tensor(1.4658), tensor(2.5129), tensor(1.5180), tensor(1.4900), tensor(1.5049), tensor(1.5320), tensor(1.4858), tensor(1.4551), tensor(1.5292), tensor(1.6082), tensor(1.5201), tensor(1.5183), tensor(1.4905), tensor(1.4798), tensor(1.5193), tensor(2.5065), tensor(1.5321), tensor(2.8932), tensor(1.5199), tensor(1.5073), tensor(1.5162), tensor(1.4557), tensor(1.5419), tensor(1.5472), tensor(1.4987), tensor(1.7202), tensor(1.5048), tensor(1.5197), tensor(1.6859), tensor(1.5169), tensor(1.5115), tensor(1.4669), tensor(1.4552), tensor(1.4885), tensor(1.5813), tensor(1.4462), tensor(1.4724), tensor(1.4867), tensor(1.5177), tensor(1.5530), tensor(1.4840), tensor(2.6439), tensor(1.5018), tensor(1.4854), tensor(1.4703), tensor(2.0885), tensor(1.4972), tensor(5.9186), tensor(1.4579), tensor(1.4796), tensor(1.4587), tensor(1.4677), tensor(1.4725), tensor(1.5092), tensor(1.4377), tensor(1.4780), tensor(1.5120), tensor(1.4853), tensor(2.2128), tensor(1.5005), tensor(1.5120), tensor(1.5773), tensor(2.4408), tensor(1.4807), tensor(1.6765), tensor(1.4433), tensor(2.3436), tensor(1.4899), tensor(1.5063), tensor(2.0288), tensor(2.2904), tensor(1.5881), tensor(1.4904), tensor(1.5079), tensor(1.4908), tensor(1.5180), tensor(1.5513), tensor(1.4654), tensor(2.0579), tensor(1.5618), tensor(4.4203), tensor(2.1378), tensor(3.4637), tensor(1.8773), tensor(2.2609), tensor(1.4794), tensor(1.5264), tensor(1.4820), tensor(1.4400), tensor(1.5146), tensor(1.5007), tensor(1.5272), tensor(1.9862), tensor(1.6197), tensor(1.4853), tensor(1.5234), tensor(1.5205), tensor(2.0751), tensor(1.5119), tensor(1.4619), tensor(1.4798), tensor(1.5184), tensor(1.5472), tensor(1.5525), tensor(1.4533), tensor(1.5106), tensor(1.5811), tensor(1.5624), tensor(1.4668), tensor(1.4869), tensor(1.8797), tensor(1.5040), tensor(1.4711), tensor(1.6534), tensor(1.4930), tensor(1.4911), tensor(1.5178), tensor(1.4986), tensor(3.8648), tensor(2.4408), tensor(1.5380), tensor(1.4974), tensor(1.5389), tensor(1.5360), tensor(1.5050), tensor(1.4737), tensor(2.3068), tensor(1.4845), tensor(1.4770), tensor(1.5580), tensor(1.4925), tensor(1.4589), tensor(1.4728), tensor(1.5203), tensor(1.7388), tensor(1.5122), tensor(1.6337), tensor(1.6188), tensor(1.4875), tensor(1.4629), tensor(1.4633), tensor(1.5870), tensor(1.4677), tensor(1.4704), tensor(1.6104), tensor(1.6091), tensor(1.5652), tensor(1.5059), tensor(1.5049), tensor(1.5277), tensor(1.7175), tensor(1.7721), tensor(1.4923), tensor(2.1961), tensor(1.5537), tensor(1.6479), tensor(1.4927), tensor(1.5164), tensor(1.4900), tensor(1.5952), tensor(1.5452), tensor(1.5222), tensor(1.4821), tensor(1.4894), tensor(1.6432), tensor(1.4987), tensor(1.5613), tensor(2.1048), tensor(1.5624), tensor(1.4631), tensor(1.5166), tensor(1.5253), tensor(1.6859), tensor(1.4828), tensor(1.6585), tensor(1.4524), tensor(1.5162), tensor(1.5063), tensor(1.5317), tensor(2.5370), tensor(1.4540), tensor(1.4802), tensor(3.1597), tensor(1.5165), tensor(2.2555), tensor(2.0489), tensor(1.5262), tensor(1.5310), tensor(1.5295), tensor(2.4831), tensor(1.5368), tensor(1.5935), tensor(1.5033), tensor(1.5525), tensor(1.5145), tensor(1.4739), tensor(1.5231), tensor(1.5865), tensor(1.4905), tensor(1.9115), tensor(2.2623), tensor(1.5025), tensor(1.5341), tensor(1.5338), tensor(3.7851), tensor(2.0545), tensor(1.5729), tensor(1.5404), tensor(1.5067), tensor(1.7044), tensor(2.0873), tensor(1.4888), tensor(1.5024), tensor(1.9282), tensor(1.4920), tensor(1.4617), tensor(1.9101), tensor(2.8163), tensor(1.5682), tensor(1.4998), tensor(2.3047), tensor(1.8081), tensor(1.5273), tensor(1.4928), tensor(1.5467), tensor(1.5019), tensor(1.5254), tensor(1.6102), tensor(1.5007), tensor(1.5000), tensor(1.4810), tensor(1.4739), tensor(1.5164), tensor(2.5574), tensor(1.5160), tensor(1.5265)]
09/23/2021 14:02:41 - INFO - __main__ - after_losses=[tensor(1.5251), tensor(1.5764), tensor(1.5945), tensor(2.4746), tensor(3.1806), tensor(1.8808), tensor(1.5289), tensor(1.4756), tensor(1.5462), tensor(1.5112), tensor(1.7688), tensor(1.5283), tensor(1.4828), tensor(1.5246), tensor(1.5443), tensor(1.4828), tensor(2.2046), tensor(1.5336), tensor(1.5220), tensor(1.5422), tensor(1.4540), tensor(2.5226), tensor(1.5587), tensor(1.4760), tensor(1.5087), tensor(1.5026), tensor(1.4905), tensor(1.4487), tensor(1.5075), tensor(1.6788), tensor(1.5169), tensor(1.5125), tensor(1.4970), tensor(1.4819), tensor(1.5084), tensor(1.5303), tensor(1.5507), tensor(1.7989), tensor(1.5115), tensor(1.4988), tensor(1.5002), tensor(1.4483), tensor(1.5497), tensor(1.5352), tensor(1.5011), tensor(1.5801), tensor(1.5062), tensor(1.9408), tensor(1.7142), tensor(1.4788), tensor(1.4999), tensor(1.4618), tensor(1.4528), tensor(1.4855), tensor(1.5394), tensor(1.4426), tensor(1.4673), tensor(1.4919), tensor(1.5062), tensor(1.5463), tensor(1.4956), tensor(2.2304), tensor(1.4964), tensor(1.4912), tensor(1.4654), tensor(2.0696), tensor(1.4775), tensor(6.3174), tensor(1.4527), tensor(1.4925), tensor(1.4647), tensor(1.4591), tensor(1.4752), tensor(1.6384), tensor(1.4378), tensor(1.4717), tensor(1.5395), tensor(1.4760), tensor(2.1680), tensor(1.4882), tensor(1.5050), tensor(1.5522), tensor(2.3115), tensor(1.4649), tensor(1.6797), tensor(1.4410), tensor(2.2242), tensor(1.4927), tensor(1.4903), tensor(2.1364), tensor(2.1438), tensor(1.5375), tensor(1.4881), tensor(1.5816), tensor(1.4950), tensor(1.5270), tensor(1.5143), tensor(1.4539), tensor(2.0538), tensor(1.5358), tensor(4.6640), tensor(1.7429), tensor(4.7847), tensor(1.8549), tensor(2.1174), tensor(1.4903), tensor(1.5140), tensor(1.4809), tensor(1.4378), tensor(1.4868), tensor(1.5094), tensor(1.4921), tensor(1.8430), tensor(1.6026), tensor(1.4956), tensor(1.5045), tensor(1.5724), tensor(1.9499), tensor(1.4994), tensor(1.4681), tensor(1.4758), tensor(1.5112), tensor(1.5520), tensor(1.5364), tensor(1.4547), tensor(1.5110), tensor(1.6763), tensor(1.5767), tensor(1.4547), tensor(1.4990), tensor(1.8137), tensor(1.5206), tensor(1.4623), tensor(1.6496), tensor(1.4899), tensor(1.4877), tensor(1.5053), tensor(1.4535), tensor(3.4846), tensor(2.2974), tensor(1.5267), tensor(1.5596), tensor(1.5130), tensor(1.6965), tensor(1.5095), tensor(1.4790), tensor(2.2533), tensor(1.4868), tensor(1.4847), tensor(1.5394), tensor(1.4885), tensor(1.4619), tensor(1.4549), tensor(1.5221), tensor(1.7409), tensor(1.5624), tensor(1.5346), tensor(1.7099), tensor(1.4808), tensor(1.4467), tensor(1.4562), tensor(1.7851), tensor(1.4593), tensor(1.4652), tensor(1.5995), tensor(1.5872), tensor(1.5358), tensor(1.5166), tensor(1.5101), tensor(1.5049), tensor(1.5327), tensor(1.8650), tensor(1.4709), tensor(2.6500), tensor(1.5484), tensor(1.5536), tensor(1.4954), tensor(1.5207), tensor(1.4889), tensor(1.7183), tensor(1.7910), tensor(1.5170), tensor(1.4780), tensor(1.5055), tensor(2.1108), tensor(1.4936), tensor(1.5447), tensor(2.0327), tensor(1.5532), tensor(1.4654), tensor(1.5282), tensor(1.5210), tensor(1.7906), tensor(1.4746), tensor(1.6223), tensor(1.4498), tensor(1.5247), tensor(1.5357), tensor(1.5184), tensor(2.2968), tensor(1.4488), tensor(1.4813), tensor(3.1132), tensor(1.5193), tensor(2.1351), tensor(1.8792), tensor(1.5183), tensor(1.5043), tensor(1.5184), tensor(2.7305), tensor(1.5437), tensor(1.5375), tensor(1.4988), tensor(1.5506), tensor(1.5100), tensor(1.4641), tensor(1.5223), tensor(1.5455), tensor(1.4913), tensor(3.0824), tensor(1.9619), tensor(1.5021), tensor(1.5247), tensor(1.5225), tensor(3.4767), tensor(1.5130), tensor(1.6102), tensor(1.5433), tensor(1.4910), tensor(1.6841), tensor(1.9735), tensor(1.4672), tensor(1.5127), tensor(2.0129), tensor(1.4862), tensor(1.4621), tensor(2.4166), tensor(2.4492), tensor(1.5106), tensor(1.5002), tensor(2.3001), tensor(1.7069), tensor(1.5248), tensor(1.4760), tensor(1.5998), tensor(1.4968), tensor(1.5490), tensor(1.5297), tensor(1.5364), tensor(1.4964), tensor(1.4709), tensor(1.4752), tensor(1.5674), tensor(2.5556), tensor(1.5068), tensor(1.5248)]
09/23/2021 14:02:41 - INFO - __main__ - interference_scores=[tensor(-0.0006), tensor(0.0051), tensor(-0.2932), tensor(0.0866), tensor(-0.0232), tensor(-0.0337), tensor(-0.0048), tensor(-0.0041), tensor(0.0298), tensor(-0.0231), tensor(0.0643), tensor(0.0076), tensor(-0.0180), tensor(-0.0169), tensor(-0.0037), tensor(0.0036), tensor(-0.0373), tensor(0.0482), tensor(-0.0101), tensor(-0.0014), tensor(-0.0118), tensor(0.0098), tensor(0.0407), tensor(-0.0139), tensor(0.0038), tensor(-0.0294), tensor(0.0048), tensor(-0.0064), tensor(-0.0217), tensor(0.0707), tensor(-0.0032), tensor(-0.0058), tensor(0.0065), tensor(0.0022), tensor(-0.0109), tensor(-0.9762), tensor(0.0187), tensor(-1.0943), tensor(-0.0084), tensor(-0.0085), tensor(-0.0160), tensor(-0.0075), tensor(0.0079), tensor(-0.0120), tensor(0.0024), tensor(-0.1401), tensor(0.0013), tensor(0.4211), tensor(0.0282), tensor(-0.0380), tensor(-0.0117), tensor(-0.0051), tensor(-0.0023), tensor(-0.0030), tensor(-0.0419), tensor(-0.0036), tensor(-0.0051), tensor(0.0052), tensor(-0.0115), tensor(-0.0067), tensor(0.0115), tensor(-0.4135), tensor(-0.0054), tensor(0.0058), tensor(-0.0049), tensor(-0.0189), tensor(-0.0197), tensor(0.3988), tensor(-0.0052), tensor(0.0129), tensor(0.0061), tensor(-0.0086), tensor(0.0026), tensor(0.1292), tensor(9.5129e-05), tensor(-0.0063), tensor(0.0275), tensor(-0.0093), tensor(-0.0448), tensor(-0.0124), tensor(-0.0070), tensor(-0.0251), tensor(-0.1292), tensor(-0.0158), tensor(0.0031), tensor(-0.0023), tensor(-0.1194), tensor(0.0028), tensor(-0.0160), tensor(0.1076), tensor(-0.1466), tensor(-0.0506), tensor(-0.0023), tensor(0.0736), tensor(0.0043), tensor(0.0090), tensor(-0.0370), tensor(-0.0115), tensor(-0.0041), tensor(-0.0260), tensor(0.2438), tensor(-0.3949), tensor(1.3210), tensor(-0.0224), tensor(-0.1436), tensor(0.0110), tensor(-0.0123), tensor(-0.0011), tensor(-0.0022), tensor(-0.0279), tensor(0.0087), tensor(-0.0351), tensor(-0.1432), tensor(-0.0170), tensor(0.0103), tensor(-0.0188), tensor(0.0520), tensor(-0.1252), tensor(-0.0125), tensor(0.0062), tensor(-0.0040), tensor(-0.0073), tensor(0.0048), tensor(-0.0161), tensor(0.0014), tensor(0.0004), tensor(0.0952), tensor(0.0142), tensor(-0.0121), tensor(0.0122), tensor(-0.0660), tensor(0.0166), tensor(-0.0088), tensor(-0.0038), tensor(-0.0032), tensor(-0.0034), tensor(-0.0125), tensor(-0.0451), tensor(-0.3802), tensor(-0.1434), tensor(-0.0113), tensor(0.0623), tensor(-0.0260), tensor(0.1604), tensor(0.0045), tensor(0.0053), tensor(-0.0536), tensor(0.0023), tensor(0.0077), tensor(-0.0186), tensor(-0.0039), tensor(0.0030), tensor(-0.0179), tensor(0.0018), tensor(0.0021), tensor(0.0502), tensor(-0.0990), tensor(0.0911), tensor(-0.0067), tensor(-0.0162), tensor(-0.0070), tensor(0.1981), tensor(-0.0084), tensor(-0.0051), tensor(-0.0110), tensor(-0.0220), tensor(-0.0294), tensor(0.0107), tensor(0.0052), tensor(-0.0228), tensor(-0.1848), tensor(0.0929), tensor(-0.0214), tensor(0.4539), tensor(-0.0053), tensor(-0.0943), tensor(0.0026), tensor(0.0043), tensor(-0.0011), tensor(0.1231), tensor(0.2459), tensor(-0.0051), tensor(-0.0041), tensor(0.0160), tensor(0.4676), tensor(-0.0051), tensor(-0.0165), tensor(-0.0721), tensor(-0.0091), tensor(0.0023), tensor(0.0116), tensor(-0.0042), tensor(0.1047), tensor(-0.0081), tensor(-0.0362), tensor(-0.0026), tensor(0.0085), tensor(0.0294), tensor(-0.0133), tensor(-0.2403), tensor(-0.0051), tensor(0.0011), tensor(-0.0464), tensor(0.0028), tensor(-0.1204), tensor(-0.1697), tensor(-0.0079), tensor(-0.0267), tensor(-0.0111), tensor(0.2473), tensor(0.0069), tensor(-0.0560), tensor(-0.0045), tensor(-0.0019), tensor(-0.0045), tensor(-0.0098), tensor(-0.0008), tensor(-0.0410), tensor(0.0008), tensor(1.1709), tensor(-0.3004), tensor(-0.0003), tensor(-0.0094), tensor(-0.0114), tensor(-0.3084), tensor(-0.5415), tensor(0.0374), tensor(0.0029), tensor(-0.0157), tensor(-0.0203), tensor(-0.1138), tensor(-0.0217), tensor(0.0102), tensor(0.0847), tensor(-0.0058), tensor(0.0004), tensor(0.5065), tensor(-0.3671), tensor(-0.0576), tensor(0.0004), tensor(-0.0046), tensor(-0.1012), tensor(-0.0025), tensor(-0.0168), tensor(0.0531), tensor(-0.0051), tensor(0.0236), tensor(-0.0805), tensor(0.0357), tensor(-0.0035), tensor(-0.0100), tensor(0.0013), tensor(0.0509), tensor(-0.0018), tensor(-0.0092), tensor(-0.0017)]
09/23/2021 14:02:41 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-2703', 'mrqa_naturalquestions-validation-3208', 'mrqa_squad-validation-8259', 'mrqa_triviaqa-validation-105', 'mrqa_triviaqa-validation-1521', 'mrqa_squad-validation-3558', 'mrqa_triviaqa-validation-6639', 'mrqa_triviaqa-validation-2368', 'mrqa_triviaqa-validation-6402', 'mrqa_triviaqa-validation-3515', 'mrqa_naturalquestions-validation-4837', 'mrqa_triviaqa-validation-1494', 'mrqa_triviaqa-validation-1575', 'mrqa_triviaqa-validation-5795', 'mrqa_triviaqa-validation-2530', 'mrqa_squad-validation-392', 'mrqa_triviaqa-validation-4560', 'mrqa_squad-validation-2987', 'mrqa_naturalquestions-validation-7017', 'mrqa_squad-validation-7554', 'mrqa_triviaqa-validation-1616', 'mrqa_squad-validation-5622', 'mrqa_triviaqa-validation-2096', 'mrqa_squad-validation-7296', 'mrqa_triviaqa-validation-6699', 'mrqa_naturalquestions-validation-10205', 'mrqa_hotpotqa-validation-3419', 'mrqa_triviaqa-validation-7332', 'mrqa_squad-validation-9281', 'mrqa_triviaqa-validation-5026', 'mrqa_hotpotqa-validation-5117', 'mrqa_squad-validation-10369']
09/23/2021 14:02:41 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:02:41 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=60) .... Timecode: 12
09/23/2021 14:02:54 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:02:54 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 12
09/23/2021 14:02:58 - INFO - __main__ - After Error Fixing: 0.96875
09/23/2021 14:02:58 - INFO - __main__ - Instant Fixing Rate: 1.0
09/23/2021 14:02:58 - INFO - __main__ - Instant Retention Rate: 0.7499999981250001
09/23/2021 14:03:00 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_013.pt.
09/23/2021 14:03:00 - INFO - __main__ - Saving the current error examples (len=28) to the memory.
09/23/2021 14:03:00 - INFO - __main__ - Current memory size: 329.
09/23/2021 14:03:00 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:03:00 - INFO - __main__ - Finished.
09/23/2021 14:03:00 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:03:00 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:03:00 - INFO - __main__ - Evaluating to get errors .... Timecode: 13
09/23/2021 14:03:03 - INFO - __main__ - Before Error Fixing: 0.0625
09/23/2021 14:03:03 - INFO - __main__ - Found 30 errors.
09/23/2021 14:03:03 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:03:03 - INFO - __main__ - Current memory size: 357.
09/23/2021 14:03:03 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:03:26 - INFO - __main__ - before_losses=[tensor(1.5222), tensor(1.5295), tensor(1.5541), tensor(1.4829), tensor(2.1296), tensor(1.5119), tensor(1.5110), tensor(1.5184), tensor(1.4880), tensor(1.4554), tensor(1.4850), tensor(1.5321), tensor(1.5020), tensor(1.5109), tensor(1.5466), tensor(1.5139), tensor(1.4781), tensor(5.4800), tensor(1.5009), tensor(1.6821), tensor(1.4791), tensor(3.3955), tensor(1.6176), tensor(2.0264), tensor(1.4938), tensor(1.5783), tensor(1.7636), tensor(1.8741), tensor(1.6784), tensor(1.5387), tensor(1.5903), tensor(2.4478), tensor(1.5052), tensor(2.2701), tensor(2.2979), tensor(1.7955), tensor(1.4882), tensor(1.4548), tensor(1.4467), tensor(2.2611), tensor(1.5017), tensor(1.5117), tensor(1.4957), tensor(1.5209), tensor(1.5156), tensor(1.5318), tensor(1.4983), tensor(1.4581), tensor(1.5534), tensor(1.4680), tensor(1.4645), tensor(1.4976), tensor(1.5872), tensor(1.6293), tensor(1.5928), tensor(1.4853), tensor(1.4831), tensor(2.4324), tensor(2.5842), tensor(1.5314), tensor(1.7422), tensor(1.5308), tensor(1.5124), tensor(1.4630), tensor(1.5153), tensor(1.5662), tensor(1.5175), tensor(1.5008), tensor(1.5187), tensor(1.4890), tensor(1.4754), tensor(1.4985), tensor(1.4758), tensor(1.6461), tensor(1.4783), tensor(1.5577), tensor(1.5583), tensor(1.5824), tensor(1.5427), tensor(1.4998), tensor(1.5622), tensor(2.6611), tensor(1.6202), tensor(2.8079), tensor(1.4709), tensor(1.6491), tensor(1.5193), tensor(1.4734), tensor(1.5057), tensor(1.5279), tensor(1.5193), tensor(1.5288), tensor(1.5180), tensor(1.7497), tensor(1.5262), tensor(2.5994), tensor(1.4698), tensor(1.4922), tensor(1.4866), tensor(1.5692), tensor(1.4890), tensor(1.4569), tensor(1.6490), tensor(2.0707), tensor(1.8930), tensor(1.7679), tensor(1.5629), tensor(1.4809), tensor(2.6614), tensor(1.4968), tensor(1.5092), tensor(1.5231), tensor(1.4996), tensor(1.4742), tensor(1.5061), tensor(1.5572), tensor(2.2851), tensor(1.4613), tensor(1.5030), tensor(1.4914), tensor(1.6120), tensor(1.5001), tensor(1.5162), tensor(1.4748), tensor(5.8913), tensor(1.5274), tensor(2.0562), tensor(2.1713), tensor(1.4904), tensor(1.4797), tensor(1.5266), tensor(1.5293), tensor(1.4855), tensor(1.7398), tensor(1.5915), tensor(1.5201), tensor(2.2960), tensor(1.5159), tensor(1.5170), tensor(1.5431), tensor(1.5476), tensor(2.0788), tensor(1.5057), tensor(1.5085), tensor(1.4934), tensor(1.5086), tensor(1.5426), tensor(1.5291), tensor(1.4974), tensor(1.4742), tensor(1.5224), tensor(1.8713), tensor(1.8583), tensor(1.4735), tensor(1.4888), tensor(1.5013), tensor(1.4493), tensor(1.4875), tensor(1.5103), tensor(1.6278), tensor(1.4888), tensor(1.5945), tensor(1.4733), tensor(1.5196), tensor(3.2102), tensor(1.4755), tensor(1.4948), tensor(2.1174), tensor(1.4650), tensor(1.4642), tensor(1.5050), tensor(2.1065), tensor(1.4777), tensor(2.6774), tensor(1.5813), tensor(2.3779), tensor(1.5071), tensor(2.0853), tensor(1.4993), tensor(1.5281), tensor(1.7997), tensor(1.5062), tensor(1.7721), tensor(2.3771), tensor(1.5214), tensor(1.5228), tensor(2.4545), tensor(1.4935), tensor(1.4759), tensor(1.4814), tensor(1.5198), tensor(3.1954), tensor(1.4740), tensor(1.4874), tensor(1.5153), tensor(1.6206), tensor(1.5085), tensor(1.5328), tensor(1.8913), tensor(1.5141), tensor(1.5116), tensor(1.5138), tensor(1.5268), tensor(1.5170), tensor(1.5310), tensor(1.4851), tensor(1.4783), tensor(1.5148), tensor(3.1500), tensor(1.5633), tensor(1.5197), tensor(1.4958), tensor(1.5512), tensor(1.7497), tensor(1.5616), tensor(1.5229), tensor(1.5047), tensor(4.1290), tensor(2.1846), tensor(1.6256), tensor(1.4503), tensor(1.8031), tensor(1.6757), tensor(1.5222), tensor(1.4835), tensor(1.5309), tensor(1.4780), tensor(1.5332), tensor(1.4441), tensor(1.5566), tensor(1.5062), tensor(1.5103), tensor(1.4866), tensor(1.4860), tensor(2.0753), tensor(1.4864), tensor(2.7400), tensor(1.5180), tensor(1.4912), tensor(1.5249), tensor(1.4989), tensor(1.5032), tensor(1.4944), tensor(1.4979), tensor(1.4407), tensor(2.5350), tensor(1.4810), tensor(1.5660), tensor(1.5613), tensor(1.5065), tensor(1.4412), tensor(1.4575), tensor(1.4907), tensor(1.4633), tensor(2.1298), tensor(1.4802)]
09/23/2021 14:03:26 - INFO - __main__ - after_losses=[tensor(1.5116), tensor(1.5104), tensor(1.5498), tensor(1.4733), tensor(2.3682), tensor(3.1618), tensor(1.5074), tensor(1.5109), tensor(1.4700), tensor(1.4456), tensor(1.4753), tensor(1.4989), tensor(1.4749), tensor(1.4833), tensor(1.5100), tensor(1.5033), tensor(1.4869), tensor(2.5378), tensor(1.4862), tensor(1.5761), tensor(1.4846), tensor(3.3969), tensor(1.5569), tensor(2.0581), tensor(1.4757), tensor(1.5916), tensor(1.6335), tensor(2.2010), tensor(1.6265), tensor(1.5310), tensor(1.5354), tensor(2.4056), tensor(1.4752), tensor(2.1111), tensor(2.0807), tensor(1.8137), tensor(1.4745), tensor(1.4778), tensor(1.4464), tensor(2.3094), tensor(1.5018), tensor(1.4733), tensor(1.4716), tensor(1.4913), tensor(1.5003), tensor(1.5023), tensor(1.4757), tensor(1.4475), tensor(1.4863), tensor(1.4558), tensor(1.4589), tensor(1.4855), tensor(2.1224), tensor(1.5307), tensor(1.5555), tensor(1.4633), tensor(1.5511), tensor(2.2977), tensor(2.5123), tensor(1.5215), tensor(1.7639), tensor(1.5267), tensor(1.4702), tensor(1.4770), tensor(1.4975), tensor(1.5086), tensor(1.4903), tensor(1.4899), tensor(1.5070), tensor(1.4768), tensor(1.4712), tensor(1.4906), tensor(1.4547), tensor(1.5440), tensor(1.4619), tensor(1.6105), tensor(1.5193), tensor(1.5283), tensor(1.5497), tensor(1.4856), tensor(1.5237), tensor(2.8978), tensor(1.5561), tensor(2.6018), tensor(1.4568), tensor(1.6293), tensor(1.4905), tensor(1.4622), tensor(1.4940), tensor(1.4874), tensor(1.4799), tensor(1.5366), tensor(1.5070), tensor(1.7663), tensor(1.5228), tensor(2.5408), tensor(1.4595), tensor(1.4708), tensor(1.4679), tensor(1.5370), tensor(1.4709), tensor(1.4516), tensor(1.6557), tensor(2.0073), tensor(1.8688), tensor(1.8878), tensor(1.5152), tensor(1.4563), tensor(2.4207), tensor(1.4814), tensor(1.5114), tensor(1.5328), tensor(1.4960), tensor(1.4698), tensor(1.4948), tensor(1.8523), tensor(1.4872), tensor(1.4456), tensor(1.4803), tensor(1.4638), tensor(1.5430), tensor(1.5139), tensor(1.5050), tensor(1.4691), tensor(2.8327), tensor(1.4855), tensor(2.0257), tensor(2.2806), tensor(1.4829), tensor(1.4765), tensor(1.5045), tensor(1.5111), tensor(1.4640), tensor(1.7062), tensor(1.5320), tensor(1.4999), tensor(2.3056), tensor(1.5019), tensor(1.4966), tensor(1.4989), tensor(1.5247), tensor(2.2214), tensor(1.4782), tensor(1.4980), tensor(1.4967), tensor(1.4878), tensor(1.5115), tensor(1.5097), tensor(1.4761), tensor(1.4560), tensor(1.4978), tensor(1.6611), tensor(1.6355), tensor(1.4609), tensor(1.4802), tensor(1.4916), tensor(1.4400), tensor(1.5083), tensor(1.4762), tensor(1.5742), tensor(1.4685), tensor(1.5324), tensor(1.4532), tensor(1.4941), tensor(1.8304), tensor(1.5516), tensor(1.4790), tensor(2.2434), tensor(1.4634), tensor(1.4630), tensor(1.4934), tensor(2.1541), tensor(1.4601), tensor(2.1307), tensor(1.5039), tensor(1.8742), tensor(1.5019), tensor(1.9043), tensor(1.4780), tensor(1.5052), tensor(1.7644), tensor(1.4991), tensor(1.7852), tensor(2.2303), tensor(1.5035), tensor(1.5269), tensor(2.4451), tensor(1.5063), tensor(1.4623), tensor(1.4737), tensor(1.4930), tensor(3.3265), tensor(1.4657), tensor(1.4722), tensor(1.4900), tensor(1.5829), tensor(1.4756), tensor(1.5061), tensor(1.9684), tensor(1.4998), tensor(1.4875), tensor(1.4854), tensor(1.4986), tensor(1.4884), tensor(1.4772), tensor(1.4808), tensor(1.4658), tensor(1.4930), tensor(3.0380), tensor(1.5353), tensor(1.4930), tensor(1.4848), tensor(1.5567), tensor(1.9541), tensor(1.5577), tensor(1.5047), tensor(1.5035), tensor(3.8167), tensor(2.1580), tensor(1.5548), tensor(1.4433), tensor(1.8381), tensor(1.9324), tensor(1.4992), tensor(1.4733), tensor(1.5142), tensor(1.5117), tensor(1.4747), tensor(1.4369), tensor(1.5394), tensor(1.4655), tensor(1.5124), tensor(1.4663), tensor(1.4746), tensor(2.0950), tensor(1.4678), tensor(1.5406), tensor(1.5056), tensor(1.4873), tensor(1.5260), tensor(1.4868), tensor(1.4758), tensor(1.4730), tensor(1.4759), tensor(1.4376), tensor(2.2619), tensor(1.4838), tensor(1.5751), tensor(1.5355), tensor(1.4795), tensor(1.4326), tensor(1.4523), tensor(1.4863), tensor(1.4442), tensor(2.0365), tensor(1.4605)]
09/23/2021 14:03:26 - INFO - __main__ - interference_scores=[tensor(-0.0105), tensor(-0.0192), tensor(-0.0042), tensor(-0.0096), tensor(0.2386), tensor(1.6500), tensor(-0.0036), tensor(-0.0076), tensor(-0.0180), tensor(-0.0098), tensor(-0.0097), tensor(-0.0332), tensor(-0.0271), tensor(-0.0275), tensor(-0.0366), tensor(-0.0105), tensor(0.0088), tensor(-2.9422), tensor(-0.0147), tensor(-0.1061), tensor(0.0056), tensor(0.0014), tensor(-0.0608), tensor(0.0316), tensor(-0.0181), tensor(0.0133), tensor(-0.1301), tensor(0.3269), tensor(-0.0519), tensor(-0.0077), tensor(-0.0549), tensor(-0.0423), tensor(-0.0300), tensor(-0.1590), tensor(-0.2172), tensor(0.0181), tensor(-0.0137), tensor(0.0230), tensor(-0.0003), tensor(0.0483), tensor(0.0001), tensor(-0.0384), tensor(-0.0241), tensor(-0.0297), tensor(-0.0153), tensor(-0.0295), tensor(-0.0226), tensor(-0.0106), tensor(-0.0671), tensor(-0.0122), tensor(-0.0056), tensor(-0.0121), tensor(0.5353), tensor(-0.0986), tensor(-0.0374), tensor(-0.0220), tensor(0.0680), tensor(-0.1346), tensor(-0.0719), tensor(-0.0099), tensor(0.0217), tensor(-0.0041), tensor(-0.0422), tensor(0.0140), tensor(-0.0179), tensor(-0.0576), tensor(-0.0271), tensor(-0.0108), tensor(-0.0117), tensor(-0.0123), tensor(-0.0041), tensor(-0.0078), tensor(-0.0211), tensor(-0.1021), tensor(-0.0164), tensor(0.0528), tensor(-0.0389), tensor(-0.0541), tensor(0.0070), tensor(-0.0142), tensor(-0.0386), tensor(0.2366), tensor(-0.0641), tensor(-0.2061), tensor(-0.0140), tensor(-0.0198), tensor(-0.0288), tensor(-0.0112), tensor(-0.0117), tensor(-0.0404), tensor(-0.0393), tensor(0.0078), tensor(-0.0109), tensor(0.0167), tensor(-0.0034), tensor(-0.0586), tensor(-0.0104), tensor(-0.0214), tensor(-0.0187), tensor(-0.0323), tensor(-0.0181), tensor(-0.0053), tensor(0.0066), tensor(-0.0633), tensor(-0.0242), tensor(0.1199), tensor(-0.0476), tensor(-0.0246), tensor(-0.2407), tensor(-0.0154), tensor(0.0021), tensor(0.0097), tensor(-0.0036), tensor(-0.0044), tensor(-0.0113), tensor(0.2951), tensor(-0.7978), tensor(-0.0157), tensor(-0.0227), tensor(-0.0275), tensor(-0.0690), tensor(0.0138), tensor(-0.0112), tensor(-0.0057), tensor(-3.0586), tensor(-0.0419), tensor(-0.0305), tensor(0.1092), tensor(-0.0076), tensor(-0.0032), tensor(-0.0222), tensor(-0.0182), tensor(-0.0215), tensor(-0.0335), tensor(-0.0595), tensor(-0.0202), tensor(0.0096), tensor(-0.0141), tensor(-0.0204), tensor(-0.0442), tensor(-0.0229), tensor(0.1426), tensor(-0.0275), tensor(-0.0104), tensor(0.0033), tensor(-0.0208), tensor(-0.0311), tensor(-0.0193), tensor(-0.0212), tensor(-0.0182), tensor(-0.0246), tensor(-0.2102), tensor(-0.2228), tensor(-0.0125), tensor(-0.0085), tensor(-0.0097), tensor(-0.0093), tensor(0.0208), tensor(-0.0341), tensor(-0.0536), tensor(-0.0203), tensor(-0.0621), tensor(-0.0202), tensor(-0.0255), tensor(-1.3798), tensor(0.0761), tensor(-0.0158), tensor(0.1261), tensor(-0.0015), tensor(-0.0012), tensor(-0.0116), tensor(0.0476), tensor(-0.0176), tensor(-0.5467), tensor(-0.0774), tensor(-0.5037), tensor(-0.0053), tensor(-0.1810), tensor(-0.0213), tensor(-0.0229), tensor(-0.0353), tensor(-0.0071), tensor(0.0130), tensor(-0.1468), tensor(-0.0179), tensor(0.0041), tensor(-0.0093), tensor(0.0128), tensor(-0.0136), tensor(-0.0078), tensor(-0.0267), tensor(0.1310), tensor(-0.0083), tensor(-0.0153), tensor(-0.0253), tensor(-0.0377), tensor(-0.0329), tensor(-0.0267), tensor(0.0771), tensor(-0.0143), tensor(-0.0241), tensor(-0.0285), tensor(-0.0282), tensor(-0.0286), tensor(-0.0538), tensor(-0.0043), tensor(-0.0125), tensor(-0.0218), tensor(-0.1120), tensor(-0.0280), tensor(-0.0267), tensor(-0.0110), tensor(0.0055), tensor(0.2044), tensor(-0.0040), tensor(-0.0182), tensor(-0.0012), tensor(-0.3123), tensor(-0.0265), tensor(-0.0709), tensor(-0.0070), tensor(0.0350), tensor(0.2567), tensor(-0.0230), tensor(-0.0102), tensor(-0.0168), tensor(0.0337), tensor(-0.0584), tensor(-0.0072), tensor(-0.0171), tensor(-0.0407), tensor(0.0021), tensor(-0.0203), tensor(-0.0113), tensor(0.0197), tensor(-0.0186), tensor(-1.1993), tensor(-0.0124), tensor(-0.0040), tensor(0.0011), tensor(-0.0122), tensor(-0.0273), tensor(-0.0214), tensor(-0.0220), tensor(-0.0030), tensor(-0.2731), tensor(0.0028), tensor(0.0090), tensor(-0.0257), tensor(-0.0271), tensor(-0.0085), tensor(-0.0052), tensor(-0.0045), tensor(-0.0190), tensor(-0.0932), tensor(-0.0197)]
09/23/2021 14:03:26 - INFO - __main__ - retrieved candidates ids = ['mrqa_naturalquestions-validation-3208', 'mrqa_squad-validation-9793', 'mrqa_triviaqa-validation-1521', 'mrqa_triviaqa-validation-6692', 'mrqa_triviaqa-validation-1451', 'mrqa_naturalquestions-validation-98', 'mrqa_triviaqa-validation-6639', 'mrqa_triviaqa-validation-6389', 'mrqa_squad-validation-6947', 'mrqa_triviaqa-validation-2703', 'mrqa_triviaqa-validation-1293', 'mrqa_triviaqa-validation-2465', 'mrqa_triviaqa-validation-792', 'mrqa_triviaqa-validation-5972', 'mrqa_hotpotqa-validation-1864', 'mrqa_triviaqa-validation-1540', 'mrqa_naturalquestions-validation-6736', 'mrqa_squad-validation-1003', 'mrqa_squad-validation-6811', 'mrqa_squad-validation-3181', 'mrqa_hotpotqa-validation-4904', 'mrqa_naturalquestions-validation-1085', 'mrqa_naturalquestions-validation-4803', 'mrqa_squad-validation-4181', 'mrqa_naturalquestions-validation-3309', 'mrqa_squad-validation-7554', 'mrqa_hotpotqa-validation-4162', 'mrqa_triviaqa-validation-5078', 'mrqa_naturalquestions-validation-9650', 'mrqa_naturalquestions-validation-6207', 'mrqa_triviaqa-validation-6556', 'mrqa_squad-validation-4506']
09/23/2021 14:03:26 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:03:26 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=62) .... Timecode: 13
09/23/2021 14:03:40 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:03:40 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 13
09/23/2021 14:03:43 - INFO - __main__ - After Error Fixing: 0.9375
09/23/2021 14:03:43 - INFO - __main__ - Instant Fixing Rate: 0.9333333333333333
09/23/2021 14:03:43 - INFO - __main__ - Instant Retention Rate: 0.999999995
09/23/2021 14:03:45 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_014.pt.
09/23/2021 14:03:45 - INFO - __main__ - Saving the current error examples (len=30) to the memory.
09/23/2021 14:03:45 - INFO - __main__ - Current memory size: 357.
09/23/2021 14:03:45 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:03:45 - INFO - __main__ - Finished.
09/23/2021 14:03:45 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:03:45 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:03:45 - INFO - __main__ - Evaluating to get errors .... Timecode: 14
09/23/2021 14:03:48 - INFO - __main__ - Before Error Fixing: 0.0
09/23/2021 14:03:48 - INFO - __main__ - Found 32 errors.
09/23/2021 14:03:48 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:03:48 - INFO - __main__ - Current memory size: 387.
09/23/2021 14:03:48 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:04:11 - INFO - __main__ - before_losses=[tensor(1.5085), tensor(1.5038), tensor(1.8125), tensor(1.6592), tensor(1.4591), tensor(5.1076), tensor(1.4658), tensor(1.5217), tensor(1.6976), tensor(2.4645), tensor(1.5026), tensor(1.5110), tensor(1.4856), tensor(1.7077), tensor(1.5099), tensor(1.5630), tensor(1.4952), tensor(1.5047), tensor(1.5548), tensor(2.0111), tensor(1.4952), tensor(1.5166), tensor(1.4603), tensor(1.5275), tensor(1.6651), tensor(1.6566), tensor(1.5554), tensor(1.4967), tensor(1.4878), tensor(2.3623), tensor(1.4737), tensor(1.5036), tensor(1.5108), tensor(1.4614), tensor(1.4883), tensor(1.6089), tensor(1.5086), tensor(1.4893), tensor(1.4792), tensor(1.4415), tensor(1.4708), tensor(2.0544), tensor(1.5095), tensor(1.5136), tensor(1.4901), tensor(2.7049), tensor(1.4997), tensor(1.4642), tensor(1.5381), tensor(1.4755), tensor(1.6506), tensor(1.4989), tensor(1.4711), tensor(1.5322), tensor(2.5968), tensor(1.5320), tensor(1.5324), tensor(1.4931), tensor(1.4641), tensor(1.4551), tensor(3.3640), tensor(1.5045), tensor(1.5351), tensor(1.4969), tensor(2.3321), tensor(1.7239), tensor(1.5739), tensor(1.8898), tensor(1.5310), tensor(1.5319), tensor(1.5106), tensor(1.4884), tensor(1.5045), tensor(1.4840), tensor(1.5170), tensor(1.5606), tensor(1.5287), tensor(1.5019), tensor(1.6509), tensor(1.5418), tensor(1.4793), tensor(1.5476), tensor(1.4939), tensor(1.4690), tensor(1.7423), tensor(1.4855), tensor(2.6257), tensor(1.5587), tensor(2.5496), tensor(1.4604), tensor(1.5828), tensor(1.5103), tensor(1.4741), tensor(1.5010), tensor(1.4686), tensor(1.8556), tensor(1.5224), tensor(2.4330), tensor(1.5011), tensor(1.4679), tensor(2.2897), tensor(1.5382), tensor(1.8503), tensor(1.5514), tensor(1.5263), tensor(1.5585), tensor(1.6035), tensor(1.4596), tensor(1.5152), tensor(1.5056), tensor(1.9628), tensor(1.4780), tensor(1.4877), tensor(1.4687), tensor(1.4857), tensor(2.3160), tensor(3.1499), tensor(1.5436), tensor(1.8050), tensor(1.4410), tensor(1.5296), tensor(1.5724), tensor(1.4410), tensor(1.4610), tensor(1.4797), tensor(1.5266), tensor(1.5029), tensor(1.4860), tensor(1.5023), tensor(1.6520), tensor(3.2151), tensor(2.1976), tensor(1.4816), tensor(1.4880), tensor(1.5709), tensor(1.7407), tensor(1.5391), tensor(1.4695), tensor(1.5237), tensor(1.7974), tensor(1.5038), tensor(1.4996), tensor(1.4699), tensor(1.5160), tensor(1.4535), tensor(1.4695), tensor(1.5251), tensor(1.4952), tensor(1.6866), tensor(1.5073), tensor(1.5076), tensor(1.5033), tensor(1.4935), tensor(1.5112), tensor(1.4773), tensor(1.4941), tensor(2.4766), tensor(1.4444), tensor(1.4580), tensor(1.4720), tensor(1.5094), tensor(1.4773), tensor(1.5211), tensor(1.4877), tensor(4.3022), tensor(1.5134), tensor(2.1933), tensor(1.5224), tensor(1.4986), tensor(1.5384), tensor(3.1294), tensor(1.5207), tensor(1.4766), tensor(1.4841), tensor(1.5434), tensor(1.5030), tensor(1.5044), tensor(1.4773), tensor(1.4859), tensor(1.4986), tensor(1.4396), tensor(1.8203), tensor(1.5928), tensor(1.6870), tensor(1.4804), tensor(1.9816), tensor(1.4921), tensor(1.5519), tensor(1.5549), tensor(2.1366), tensor(1.5053), tensor(1.5056), tensor(1.5293), tensor(1.4990), tensor(1.4555), tensor(1.4800), tensor(1.4828), tensor(1.4781), tensor(1.5125), tensor(2.0129), tensor(1.5128), tensor(2.2340), tensor(1.5182), tensor(1.4949), tensor(1.5013), tensor(1.5160), tensor(1.4832), tensor(1.5096), tensor(1.4889), tensor(1.4819), tensor(1.4853), tensor(1.4555), tensor(1.4964), tensor(1.5863), tensor(1.5276), tensor(1.5431), tensor(1.5510), tensor(1.5199), tensor(1.4567), tensor(1.4958), tensor(1.5263), tensor(2.9975), tensor(2.1650), tensor(1.4969), tensor(1.5557), tensor(1.4913), tensor(1.6178), tensor(1.4712), tensor(1.4683), tensor(1.5161), tensor(1.4807), tensor(1.4796), tensor(1.4895), tensor(1.4905), tensor(1.6066), tensor(1.5605), tensor(1.4985), tensor(1.4970), tensor(1.5731), tensor(1.5120), tensor(1.6080), tensor(1.5177), tensor(1.5460), tensor(1.4960), tensor(1.5023), tensor(1.5133), tensor(1.4969), tensor(1.4939), tensor(2.2143), tensor(2.1723), tensor(1.4740), tensor(1.4947), tensor(2.5751), tensor(1.4534), tensor(1.5088), tensor(1.4642)]
09/23/2021 14:04:11 - INFO - __main__ - after_losses=[tensor(1.5220), tensor(1.4851), tensor(1.9283), tensor(1.5472), tensor(1.4595), tensor(4.8671), tensor(1.4710), tensor(1.5226), tensor(1.5043), tensor(2.4986), tensor(1.4956), tensor(1.5533), tensor(1.4954), tensor(3.5100), tensor(1.4903), tensor(1.5848), tensor(1.4867), tensor(1.4954), tensor(1.5549), tensor(1.9560), tensor(1.5191), tensor(1.5324), tensor(1.4676), tensor(1.5382), tensor(1.5824), tensor(1.6739), tensor(1.5725), tensor(1.5825), tensor(1.4830), tensor(2.7764), tensor(1.4615), tensor(1.5074), tensor(1.5179), tensor(1.4905), tensor(1.5668), tensor(1.7429), tensor(1.5042), tensor(1.4899), tensor(1.4825), tensor(1.4638), tensor(1.4783), tensor(2.0001), tensor(1.5300), tensor(1.5099), tensor(1.5038), tensor(2.8677), tensor(1.5105), tensor(1.5021), tensor(1.5401), tensor(1.4736), tensor(1.5893), tensor(1.4919), tensor(1.4861), tensor(1.6542), tensor(2.6462), tensor(1.5542), tensor(1.5166), tensor(1.4843), tensor(1.4607), tensor(1.4517), tensor(3.0202), tensor(1.5532), tensor(1.5470), tensor(1.4858), tensor(2.1914), tensor(1.8184), tensor(1.5649), tensor(1.5087), tensor(1.5483), tensor(1.5346), tensor(1.5275), tensor(1.4898), tensor(1.4870), tensor(1.4805), tensor(1.5161), tensor(1.6804), tensor(1.5301), tensor(1.5654), tensor(1.5195), tensor(1.5783), tensor(1.4823), tensor(1.5786), tensor(1.4922), tensor(1.4764), tensor(1.7575), tensor(1.5019), tensor(2.7486), tensor(1.5550), tensor(2.2554), tensor(1.4657), tensor(1.5359), tensor(1.5076), tensor(1.4976), tensor(1.5008), tensor(1.5184), tensor(1.9631), tensor(1.5198), tensor(2.4112), tensor(1.5111), tensor(1.4651), tensor(2.2736), tensor(1.5374), tensor(1.8562), tensor(1.5598), tensor(1.5311), tensor(1.5522), tensor(1.5660), tensor(1.4522), tensor(1.5403), tensor(1.5012), tensor(2.1919), tensor(1.4909), tensor(1.4778), tensor(1.4711), tensor(1.4815), tensor(2.3410), tensor(2.9367), tensor(1.5678), tensor(1.6899), tensor(1.4430), tensor(1.5393), tensor(1.5663), tensor(1.4406), tensor(1.4597), tensor(1.4864), tensor(1.5835), tensor(1.5072), tensor(1.5058), tensor(1.5043), tensor(1.6303), tensor(1.8347), tensor(2.0978), tensor(1.5638), tensor(1.4864), tensor(1.5803), tensor(1.8417), tensor(1.6024), tensor(1.4618), tensor(1.5530), tensor(1.5300), tensor(1.5097), tensor(1.5002), tensor(1.4691), tensor(1.5129), tensor(1.4551), tensor(1.4620), tensor(1.5352), tensor(1.4950), tensor(1.5914), tensor(1.5391), tensor(1.5054), tensor(1.5044), tensor(1.4977), tensor(1.5187), tensor(1.4813), tensor(1.5036), tensor(2.2942), tensor(1.4451), tensor(1.4500), tensor(1.4729), tensor(1.5196), tensor(1.4564), tensor(1.5717), tensor(1.4724), tensor(4.0197), tensor(1.5239), tensor(2.3540), tensor(1.5289), tensor(1.4889), tensor(1.5208), tensor(3.6345), tensor(1.5280), tensor(1.4660), tensor(1.5036), tensor(1.5510), tensor(1.5169), tensor(1.5376), tensor(1.4763), tensor(1.7206), tensor(1.5239), tensor(1.4476), tensor(1.7315), tensor(1.5802), tensor(1.7490), tensor(1.4978), tensor(1.8739), tensor(1.4985), tensor(1.5445), tensor(1.5666), tensor(2.1991), tensor(1.5070), tensor(1.5056), tensor(1.5355), tensor(1.5058), tensor(1.4610), tensor(1.4804), tensor(1.4859), tensor(1.4902), tensor(1.4985), tensor(1.6154), tensor(1.5202), tensor(2.2032), tensor(1.7716), tensor(1.4964), tensor(1.4829), tensor(1.5264), tensor(1.4767), tensor(1.5152), tensor(1.4912), tensor(1.4708), tensor(1.4755), tensor(1.4487), tensor(1.4921), tensor(1.7242), tensor(1.5048), tensor(1.5314), tensor(1.5276), tensor(1.5136), tensor(1.4743), tensor(1.4893), tensor(1.5193), tensor(2.7676), tensor(2.1833), tensor(1.5060), tensor(1.5626), tensor(1.4872), tensor(1.5979), tensor(1.4659), tensor(1.4713), tensor(1.5286), tensor(1.4796), tensor(1.4811), tensor(1.4924), tensor(1.4849), tensor(1.5560), tensor(1.5717), tensor(1.4891), tensor(1.4912), tensor(1.5783), tensor(1.5061), tensor(1.6327), tensor(1.5167), tensor(1.5476), tensor(1.4988), tensor(1.4953), tensor(1.5411), tensor(1.4921), tensor(1.4918), tensor(2.3734), tensor(1.5742), tensor(1.4782), tensor(1.5438), tensor(2.4865), tensor(1.4514), tensor(1.4941), tensor(1.4670)]
09/23/2021 14:04:11 - INFO - __main__ - interference_scores=[tensor(0.0135), tensor(-0.0187), tensor(0.1158), tensor(-0.1120), tensor(0.0004), tensor(-0.2405), tensor(0.0051), tensor(0.0009), tensor(-0.1933), tensor(0.0341), tensor(-0.0069), tensor(0.0422), tensor(0.0099), tensor(1.8023), tensor(-0.0196), tensor(0.0219), tensor(-0.0085), tensor(-0.0093), tensor(0.0002), tensor(-0.0550), tensor(0.0239), tensor(0.0158), tensor(0.0073), tensor(0.0107), tensor(-0.0828), tensor(0.0173), tensor(0.0171), tensor(0.0858), tensor(-0.0048), tensor(0.4141), tensor(-0.0123), tensor(0.0038), tensor(0.0071), tensor(0.0292), tensor(0.0785), tensor(0.1340), tensor(-0.0044), tensor(0.0007), tensor(0.0033), tensor(0.0223), tensor(0.0075), tensor(-0.0543), tensor(0.0205), tensor(-0.0037), tensor(0.0138), tensor(0.1628), tensor(0.0107), tensor(0.0379), tensor(0.0020), tensor(-0.0018), tensor(-0.0613), tensor(-0.0071), tensor(0.0149), tensor(0.1220), tensor(0.0494), tensor(0.0221), tensor(-0.0158), tensor(-0.0089), tensor(-0.0034), tensor(-0.0034), tensor(-0.3439), tensor(0.0488), tensor(0.0120), tensor(-0.0111), tensor(-0.1407), tensor(0.0945), tensor(-0.0090), tensor(-0.3811), tensor(0.0174), tensor(0.0027), tensor(0.0169), tensor(0.0013), tensor(-0.0175), tensor(-0.0034), tensor(-0.0009), tensor(0.1198), tensor(0.0015), tensor(0.0635), tensor(-0.1314), tensor(0.0366), tensor(0.0030), tensor(0.0310), tensor(-0.0017), tensor(0.0075), tensor(0.0152), tensor(0.0164), tensor(0.1229), tensor(-0.0037), tensor(-0.2942), tensor(0.0053), tensor(-0.0469), tensor(-0.0027), tensor(0.0235), tensor(-0.0003), tensor(0.0499), tensor(0.1076), tensor(-0.0026), tensor(-0.0218), tensor(0.0100), tensor(-0.0027), tensor(-0.0161), tensor(-0.0007), tensor(0.0059), tensor(0.0084), tensor(0.0048), tensor(-0.0062), tensor(-0.0375), tensor(-0.0074), tensor(0.0251), tensor(-0.0044), tensor(0.2292), tensor(0.0129), tensor(-0.0099), tensor(0.0024), tensor(-0.0042), tensor(0.0250), tensor(-0.2132), tensor(0.0242), tensor(-0.1151), tensor(0.0020), tensor(0.0097), tensor(-0.0062), tensor(-0.0003), tensor(-0.0014), tensor(0.0067), tensor(0.0569), tensor(0.0043), tensor(0.0198), tensor(0.0019), tensor(-0.0217), tensor(-1.3803), tensor(-0.0999), tensor(0.0822), tensor(-0.0015), tensor(0.0094), tensor(0.1009), tensor(0.0633), tensor(-0.0077), tensor(0.0293), tensor(-0.2675), tensor(0.0059), tensor(0.0006), tensor(-0.0008), tensor(-0.0031), tensor(0.0016), tensor(-0.0074), tensor(0.0102), tensor(-0.0003), tensor(-0.0952), tensor(0.0318), tensor(-0.0022), tensor(0.0011), tensor(0.0042), tensor(0.0075), tensor(0.0040), tensor(0.0095), tensor(-0.1824), tensor(0.0007), tensor(-0.0080), tensor(0.0009), tensor(0.0102), tensor(-0.0209), tensor(0.0506), tensor(-0.0153), tensor(-0.2824), tensor(0.0106), tensor(0.1607), tensor(0.0065), tensor(-0.0098), tensor(-0.0176), tensor(0.5051), tensor(0.0073), tensor(-0.0106), tensor(0.0194), tensor(0.0076), tensor(0.0139), tensor(0.0332), tensor(-0.0010), tensor(0.2347), tensor(0.0253), tensor(0.0080), tensor(-0.0888), tensor(-0.0126), tensor(0.0620), tensor(0.0173), tensor(-0.1077), tensor(0.0064), tensor(-0.0075), tensor(0.0118), tensor(0.0624), tensor(0.0016), tensor(1.3709e-05), tensor(0.0063), tensor(0.0067), tensor(0.0055), tensor(0.0004), tensor(0.0031), tensor(0.0121), tensor(-0.0140), tensor(-0.3974), tensor(0.0075), tensor(-0.0308), tensor(0.2534), tensor(0.0014), tensor(-0.0184), tensor(0.0103), tensor(-0.0065), tensor(0.0055), tensor(0.0023), tensor(-0.0111), tensor(-0.0099), tensor(-0.0068), tensor(-0.0043), tensor(0.1380), tensor(-0.0228), tensor(-0.0117), tensor(-0.0234), tensor(-0.0063), tensor(0.0177), tensor(-0.0066), tensor(-0.0070), tensor(-0.2298), tensor(0.0183), tensor(0.0092), tensor(0.0069), tensor(-0.0041), tensor(-0.0199), tensor(-0.0053), tensor(0.0030), tensor(0.0125), tensor(-0.0010), tensor(0.0015), tensor(0.0029), tensor(-0.0056), tensor(-0.0506), tensor(0.0111), tensor(-0.0093), tensor(-0.0058), tensor(0.0051), tensor(-0.0059), tensor(0.0247), tensor(-0.0010), tensor(0.0016), tensor(0.0028), tensor(-0.0070), tensor(0.0279), tensor(-0.0048), tensor(-0.0021), tensor(0.1592), tensor(-0.5981), tensor(0.0042), tensor(0.0491), tensor(-0.0885), tensor(-0.0020), tensor(-0.0147), tensor(0.0028)]
09/23/2021 14:04:11 - INFO - __main__ - retrieved candidates ids = ['mrqa_naturalquestions-validation-6736', 'mrqa_triviaqa-validation-7767', 'mrqa_triviaqa-validation-316', 'mrqa_squad-validation-7351', 'mrqa_triviaqa-validation-2896', 'mrqa_squad-validation-7554', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-1293', 'mrqa_squad-validation-6677', 'mrqa_triviaqa-validation-7638', 'mrqa_triviaqa-validation-1551', 'mrqa_triviaqa-validation-444', 'mrqa_triviaqa-validation-2136', 'mrqa_triviaqa-validation-7371', 'mrqa_triviaqa-validation-7703', 'mrqa_triviaqa-validation-1521', 'mrqa_triviaqa-validation-4583', 'mrqa_squad-validation-4181', 'mrqa_hotpotqa-validation-187', 'mrqa_squad-validation-3490', 'mrqa_triviaqa-validation-5362', 'mrqa_naturalquestions-validation-5647', 'mrqa_triviaqa-validation-4402', 'mrqa_squad-validation-6924', 'mrqa_squad-validation-2629', 'mrqa_hotpotqa-validation-1888', 'mrqa_triviaqa-validation-893', 'mrqa_hotpotqa-validation-4904', 'mrqa_triviaqa-validation-1860', 'mrqa_hotpotqa-validation-400', 'mrqa_hotpotqa-validation-1355', 'mrqa_naturalquestions-validation-6644']
09/23/2021 14:04:11 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:04:11 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=64) .... Timecode: 14
09/23/2021 14:04:25 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:04:25 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 14
09/23/2021 14:04:28 - INFO - __main__ - After Error Fixing: 0.96875
09/23/2021 14:04:28 - INFO - __main__ - Instant Fixing Rate: 0.96875
09/23/2021 14:04:28 - INFO - __main__ - Instant Retention Rate: 0.0
09/23/2021 14:04:30 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_015.pt.
09/23/2021 14:04:30 - INFO - __main__ - Saving the current error examples (len=32) to the memory.
09/23/2021 14:04:30 - INFO - __main__ - Current memory size: 387.
09/23/2021 14:04:30 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:04:30 - INFO - __main__ - Finished.
09/23/2021 14:04:30 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:04:30 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:04:30 - INFO - __main__ - Evaluating to get errors .... Timecode: 15
09/23/2021 14:04:32 - INFO - __main__ - Before Error Fixing: 0.125
09/23/2021 14:04:32 - INFO - __main__ - Found 28 errors.
09/23/2021 14:04:32 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:04:32 - INFO - __main__ - Current memory size: 419.
09/23/2021 14:04:32 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:04:55 - INFO - __main__ - before_losses=[tensor(2.1191), tensor(1.5429), tensor(1.4857), tensor(1.6638), tensor(1.7504), tensor(4.5723), tensor(1.5830), tensor(1.4844), tensor(1.6351), tensor(1.5325), tensor(1.4937), tensor(1.6439), tensor(1.5542), tensor(1.6022), tensor(1.5572), tensor(1.5497), tensor(1.5380), tensor(1.4646), tensor(1.5101), tensor(1.5422), tensor(1.5312), tensor(1.4896), tensor(1.4822), tensor(1.5684), tensor(1.4581), tensor(1.4830), tensor(1.5229), tensor(1.5375), tensor(1.4595), tensor(1.5493), tensor(1.5652), tensor(1.5892), tensor(1.5331), tensor(1.5226), tensor(1.5621), tensor(1.5218), tensor(1.5048), tensor(1.5261), tensor(2.4151), tensor(1.4743), tensor(1.8116), tensor(1.4968), tensor(1.5140), tensor(1.5600), tensor(1.5183), tensor(1.9661), tensor(1.5112), tensor(1.5555), tensor(1.4438), tensor(1.5829), tensor(1.4983), tensor(1.5322), tensor(1.4947), tensor(2.1388), tensor(1.5374), tensor(1.9094), tensor(1.4637), tensor(1.5077), tensor(1.4594), tensor(1.5263), tensor(1.5686), tensor(1.4937), tensor(1.4821), tensor(1.5668), tensor(1.5440), tensor(1.5040), tensor(1.5193), tensor(1.5665), tensor(1.5041), tensor(1.5143), tensor(1.4938), tensor(1.5056), tensor(1.4906), tensor(1.5284), tensor(1.4538), tensor(1.5312), tensor(1.4967), tensor(1.5504), tensor(1.4994), tensor(1.9624), tensor(1.5073), tensor(1.5081), tensor(1.5307), tensor(1.4902), tensor(2.6783), tensor(1.5018), tensor(1.8354), tensor(1.5285), tensor(1.5081), tensor(1.5163), tensor(1.5125), tensor(1.5937), tensor(1.5663), tensor(1.5900), tensor(1.4933), tensor(1.4715), tensor(1.5054), tensor(1.6322), tensor(1.5311), tensor(1.5063), tensor(2.1251), tensor(1.4818), tensor(1.5080), tensor(1.5251), tensor(1.5364), tensor(1.5373), tensor(2.0461), tensor(1.5026), tensor(1.4896), tensor(1.4964), tensor(1.5157), tensor(1.4969), tensor(1.5909), tensor(3.2890), tensor(1.4789), tensor(1.4685), tensor(3.0854), tensor(1.4869), tensor(1.4954), tensor(1.5197), tensor(1.5516), tensor(1.4719), tensor(1.5554), tensor(1.4991), tensor(1.7262), tensor(1.4888), tensor(1.5747), tensor(1.4515), tensor(1.5401), tensor(1.4955), tensor(5.2170), tensor(1.5384), tensor(1.4821), tensor(1.4858), tensor(1.5248), tensor(1.5116), tensor(1.5456), tensor(1.4905), tensor(1.9583), tensor(1.4792), tensor(1.5691), tensor(1.5275), tensor(1.4461), tensor(1.5150), tensor(1.5179), tensor(1.5099), tensor(1.5421), tensor(1.6119), tensor(1.5609), tensor(1.5890), tensor(1.8013), tensor(1.5094), tensor(1.4955), tensor(1.9824), tensor(2.1825), tensor(1.5398), tensor(1.5371), tensor(1.4663), tensor(1.9415), tensor(1.7201), tensor(2.2534), tensor(1.5162), tensor(1.4895), tensor(1.6048), tensor(1.6223), tensor(1.5719), tensor(1.5250), tensor(2.0139), tensor(1.5755), tensor(1.4937), tensor(1.4909), tensor(1.4861), tensor(1.4828), tensor(1.5185), tensor(1.4998), tensor(1.5319), tensor(1.5695), tensor(1.8000), tensor(1.5382), tensor(1.4414), tensor(1.4789), tensor(2.1130), tensor(1.5143), tensor(1.5359), tensor(1.5206), tensor(1.5071), tensor(1.5774), tensor(1.5125), tensor(2.2946), tensor(1.5083), tensor(1.4864), tensor(2.1247), tensor(1.5782), tensor(1.4841), tensor(1.5319), tensor(3.1665), tensor(1.4453), tensor(1.5896), tensor(1.5216), tensor(1.4968), tensor(1.8605), tensor(1.4521), tensor(1.5193), tensor(1.5696), tensor(2.0128), tensor(1.5659), tensor(2.2001), tensor(1.5205), tensor(1.5397), tensor(2.0411), tensor(1.6919), tensor(1.5188), tensor(1.4895), tensor(1.5600), tensor(1.5744), tensor(1.4623), tensor(1.5149), tensor(1.5184), tensor(2.2332), tensor(1.4532), tensor(1.5663), tensor(1.5393), tensor(1.5428), tensor(1.5229), tensor(1.4779), tensor(1.4912), tensor(1.5990), tensor(1.4759), tensor(2.7002), tensor(2.7347), tensor(2.3401), tensor(1.6885), tensor(1.6518), tensor(2.3035), tensor(2.0251), tensor(3.0585), tensor(1.5628), tensor(2.2043), tensor(1.5187), tensor(1.5290), tensor(1.9945), tensor(1.5000), tensor(1.5727), tensor(2.3359), tensor(1.4847), tensor(1.5205), tensor(1.5053), tensor(1.4936), tensor(1.4909), tensor(1.4992), tensor(1.4707), tensor(1.5624), tensor(1.5520), tensor(1.5537), tensor(1.6479), tensor(1.4634)]
09/23/2021 14:04:55 - INFO - __main__ - after_losses=[tensor(1.9253), tensor(1.5145), tensor(1.4907), tensor(1.6054), tensor(1.7618), tensor(4.3024), tensor(1.5549), tensor(1.4683), tensor(1.5324), tensor(1.5411), tensor(1.4985), tensor(1.5154), tensor(1.5307), tensor(1.7190), tensor(1.5075), tensor(1.5286), tensor(1.4929), tensor(1.4447), tensor(1.4987), tensor(1.5380), tensor(1.4821), tensor(1.4699), tensor(1.4658), tensor(1.5316), tensor(1.4463), tensor(1.4550), tensor(1.4744), tensor(1.4891), tensor(1.5390), tensor(1.5262), tensor(1.5393), tensor(1.6869), tensor(1.5054), tensor(1.4751), tensor(1.5488), tensor(1.4925), tensor(1.4969), tensor(1.4630), tensor(2.2128), tensor(1.4469), tensor(1.7299), tensor(1.4724), tensor(1.4970), tensor(1.5456), tensor(1.4957), tensor(1.9813), tensor(1.4874), tensor(1.5177), tensor(1.4343), tensor(1.6859), tensor(1.4792), tensor(1.4990), tensor(1.4680), tensor(2.0489), tensor(1.4914), tensor(2.5969), tensor(1.4457), tensor(1.4723), tensor(1.4489), tensor(1.5115), tensor(1.5711), tensor(1.4665), tensor(1.4771), tensor(1.5330), tensor(1.5013), tensor(1.4767), tensor(1.4977), tensor(1.4971), tensor(1.5002), tensor(1.4886), tensor(1.5405), tensor(1.4708), tensor(1.4798), tensor(1.5149), tensor(1.4427), tensor(1.4744), tensor(1.4928), tensor(1.5379), tensor(1.4696), tensor(1.8174), tensor(1.4879), tensor(1.5276), tensor(1.5152), tensor(1.6440), tensor(2.6175), tensor(1.4887), tensor(1.4841), tensor(1.5043), tensor(1.4922), tensor(1.4867), tensor(1.5150), tensor(1.5710), tensor(1.5259), tensor(1.5541), tensor(1.4557), tensor(1.4495), tensor(1.4840), tensor(1.9430), tensor(1.5120), tensor(1.4932), tensor(2.2199), tensor(1.4455), tensor(1.4922), tensor(1.5308), tensor(1.5088), tensor(1.5130), tensor(1.5913), tensor(1.4747), tensor(1.4695), tensor(1.4836), tensor(1.4892), tensor(1.4854), tensor(1.6482), tensor(2.9182), tensor(1.4605), tensor(1.4535), tensor(3.0471), tensor(1.4715), tensor(1.4854), tensor(1.5282), tensor(1.7177), tensor(1.4621), tensor(1.5621), tensor(1.4736), tensor(1.7491), tensor(1.5442), tensor(1.5367), tensor(1.4459), tensor(1.4817), tensor(1.4810), tensor(5.0215), tensor(1.5844), tensor(1.4501), tensor(1.4601), tensor(1.4900), tensor(1.4964), tensor(1.5111), tensor(1.4759), tensor(1.9850), tensor(1.4623), tensor(1.7195), tensor(1.5259), tensor(1.4368), tensor(1.4843), tensor(1.4975), tensor(1.4784), tensor(1.4703), tensor(1.5870), tensor(1.6012), tensor(1.5495), tensor(1.7720), tensor(1.4984), tensor(1.4848), tensor(1.5202), tensor(1.5122), tensor(1.5213), tensor(1.5422), tensor(1.4679), tensor(1.9666), tensor(1.5869), tensor(2.2413), tensor(1.4695), tensor(1.4699), tensor(1.5524), tensor(1.5975), tensor(1.6140), tensor(1.4964), tensor(1.9684), tensor(1.5426), tensor(1.5326), tensor(1.4645), tensor(1.4632), tensor(1.4676), tensor(1.4600), tensor(1.4842), tensor(1.5205), tensor(1.6903), tensor(1.8061), tensor(1.5145), tensor(1.4341), tensor(1.4687), tensor(1.7461), tensor(1.4920), tensor(1.5087), tensor(1.5105), tensor(1.4517), tensor(1.5567), tensor(1.4799), tensor(2.3905), tensor(1.6982), tensor(1.4690), tensor(1.5004), tensor(1.5001), tensor(1.4612), tensor(1.5258), tensor(3.2143), tensor(1.4367), tensor(1.5982), tensor(1.4989), tensor(1.4883), tensor(1.8438), tensor(1.4447), tensor(1.5101), tensor(1.6007), tensor(1.8311), tensor(1.5154), tensor(2.1413), tensor(1.4931), tensor(1.5176), tensor(1.9755), tensor(1.6149), tensor(1.4983), tensor(1.4648), tensor(1.5249), tensor(1.5052), tensor(1.4481), tensor(1.4949), tensor(1.4936), tensor(2.2213), tensor(1.4473), tensor(1.5705), tensor(1.4937), tensor(1.5178), tensor(1.5065), tensor(1.4543), tensor(1.4807), tensor(1.5555), tensor(1.4546), tensor(1.9595), tensor(2.7822), tensor(2.3392), tensor(1.6579), tensor(1.6259), tensor(2.4484), tensor(1.6906), tensor(2.7037), tensor(1.5646), tensor(2.4864), tensor(1.4961), tensor(1.5164), tensor(2.2192), tensor(1.4514), tensor(1.5465), tensor(2.1595), tensor(1.4673), tensor(1.5008), tensor(1.5717), tensor(1.4827), tensor(1.4723), tensor(1.4736), tensor(1.4763), tensor(1.5483), tensor(1.5105), tensor(1.5172), tensor(1.8289), tensor(1.4627)]
09/23/2021 14:04:55 - INFO - __main__ - interference_scores=[tensor(-0.1938), tensor(-0.0284), tensor(0.0050), tensor(-0.0584), tensor(0.0114), tensor(-0.2699), tensor(-0.0281), tensor(-0.0161), tensor(-0.1027), tensor(0.0086), tensor(0.0048), tensor(-0.1284), tensor(-0.0235), tensor(0.1169), tensor(-0.0497), tensor(-0.0211), tensor(-0.0451), tensor(-0.0199), tensor(-0.0114), tensor(-0.0041), tensor(-0.0491), tensor(-0.0198), tensor(-0.0164), tensor(-0.0368), tensor(-0.0118), tensor(-0.0281), tensor(-0.0485), tensor(-0.0484), tensor(0.0796), tensor(-0.0231), tensor(-0.0258), tensor(0.0977), tensor(-0.0277), tensor(-0.0474), tensor(-0.0134), tensor(-0.0294), tensor(-0.0079), tensor(-0.0631), tensor(-0.2023), tensor(-0.0275), tensor(-0.0818), tensor(-0.0245), tensor(-0.0171), tensor(-0.0144), tensor(-0.0226), tensor(0.0153), tensor(-0.0238), tensor(-0.0378), tensor(-0.0095), tensor(0.1030), tensor(-0.0191), tensor(-0.0332), tensor(-0.0267), tensor(-0.0899), tensor(-0.0460), tensor(0.6876), tensor(-0.0180), tensor(-0.0354), tensor(-0.0105), tensor(-0.0148), tensor(0.0024), tensor(-0.0272), tensor(-0.0050), tensor(-0.0337), tensor(-0.0427), tensor(-0.0273), tensor(-0.0216), tensor(-0.0694), tensor(-0.0039), tensor(-0.0257), tensor(0.0467), tensor(-0.0348), tensor(-0.0108), tensor(-0.0134), tensor(-0.0111), tensor(-0.0568), tensor(-0.0039), tensor(-0.0125), tensor(-0.0297), tensor(-0.1449), tensor(-0.0194), tensor(0.0194), tensor(-0.0155), tensor(0.1538), tensor(-0.0607), tensor(-0.0131), tensor(-0.3513), tensor(-0.0241), tensor(-0.0159), tensor(-0.0297), tensor(0.0024), tensor(-0.0226), tensor(-0.0404), tensor(-0.0359), tensor(-0.0376), tensor(-0.0220), tensor(-0.0214), tensor(0.3108), tensor(-0.0191), tensor(-0.0131), tensor(0.0948), tensor(-0.0362), tensor(-0.0158), tensor(0.0057), tensor(-0.0275), tensor(-0.0244), tensor(-0.4547), tensor(-0.0280), tensor(-0.0202), tensor(-0.0128), tensor(-0.0265), tensor(-0.0116), tensor(0.0573), tensor(-0.3708), tensor(-0.0185), tensor(-0.0150), tensor(-0.0383), tensor(-0.0155), tensor(-0.0100), tensor(0.0085), tensor(0.1662), tensor(-0.0099), tensor(0.0067), tensor(-0.0256), tensor(0.0230), tensor(0.0553), tensor(-0.0380), tensor(-0.0056), tensor(-0.0584), tensor(-0.0145), tensor(-0.1955), tensor(0.0460), tensor(-0.0321), tensor(-0.0258), tensor(-0.0347), tensor(-0.0151), tensor(-0.0344), tensor(-0.0145), tensor(0.0267), tensor(-0.0168), tensor(0.1505), tensor(-0.0016), tensor(-0.0093), tensor(-0.0306), tensor(-0.0204), tensor(-0.0316), tensor(-0.0717), tensor(-0.0249), tensor(0.0403), tensor(-0.0395), tensor(-0.0293), tensor(-0.0110), tensor(-0.0106), tensor(-0.4622), tensor(-0.6703), tensor(-0.0185), tensor(0.0051), tensor(0.0015), tensor(0.0251), tensor(-0.1332), tensor(-0.0120), tensor(-0.0467), tensor(-0.0196), tensor(-0.0523), tensor(-0.0248), tensor(0.0420), tensor(-0.0286), tensor(-0.0455), tensor(-0.0329), tensor(0.0389), tensor(-0.0264), tensor(-0.0229), tensor(-0.0152), tensor(-0.0585), tensor(-0.0155), tensor(-0.0115), tensor(0.1207), tensor(0.0060), tensor(-0.0238), tensor(-0.0073), tensor(-0.0103), tensor(-0.3669), tensor(-0.0222), tensor(-0.0272), tensor(-0.0101), tensor(-0.0554), tensor(-0.0208), tensor(-0.0325), tensor(0.0958), tensor(0.1898), tensor(-0.0174), tensor(-0.6243), tensor(-0.0781), tensor(-0.0229), tensor(-0.0062), tensor(0.0478), tensor(-0.0085), tensor(0.0087), tensor(-0.0227), tensor(-0.0084), tensor(-0.0168), tensor(-0.0074), tensor(-0.0093), tensor(0.0311), tensor(-0.1817), tensor(-0.0506), tensor(-0.0589), tensor(-0.0274), tensor(-0.0221), tensor(-0.0656), tensor(-0.0771), tensor(-0.0205), tensor(-0.0247), tensor(-0.0351), tensor(-0.0692), tensor(-0.0142), tensor(-0.0200), tensor(-0.0249), tensor(-0.0119), tensor(-0.0059), tensor(0.0042), tensor(-0.0455), tensor(-0.0250), tensor(-0.0164), tensor(-0.0236), tensor(-0.0104), tensor(-0.0435), tensor(-0.0214), tensor(-0.7408), tensor(0.0475), tensor(-0.0009), tensor(-0.0306), tensor(-0.0259), tensor(0.1448), tensor(-0.3346), tensor(-0.3548), tensor(0.0019), tensor(0.2822), tensor(-0.0226), tensor(-0.0127), tensor(0.2247), tensor(-0.0486), tensor(-0.0262), tensor(-0.1764), tensor(-0.0173), tensor(-0.0197), tensor(0.0664), tensor(-0.0109), tensor(-0.0186), tensor(-0.0256), tensor(0.0055), tensor(-0.0142), tensor(-0.0416), tensor(-0.0365), tensor(0.1810), tensor(-0.0007)]
09/23/2021 14:04:55 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-9751', 'mrqa_triviaqa-validation-6913', 'mrqa_triviaqa-validation-1551', 'mrqa_triviaqa-validation-6639', 'mrqa_triviaqa-validation-7332', 'mrqa_triviaqa-validation-1935', 'mrqa_triviaqa-validation-4486', 'mrqa_triviaqa-validation-4268', 'mrqa_triviaqa-validation-1293', 'mrqa_triviaqa-validation-5852', 'mrqa_triviaqa-validation-6259', 'mrqa_triviaqa-validation-5406', 'mrqa_triviaqa-validation-792', 'mrqa_naturalquestions-validation-9897', 'mrqa_triviaqa-validation-7506', 'mrqa_triviaqa-validation-7300', 'mrqa_squad-validation-9281', 'mrqa_triviaqa-validation-254', 'mrqa_triviaqa-validation-7415', 'mrqa_triviaqa-validation-7382', 'mrqa_triviaqa-validation-2703', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-7157', 'mrqa_hotpotqa-validation-5682', 'mrqa_naturalquestions-validation-10271', 'mrqa_triviaqa-validation-6556', 'mrqa_triviaqa-validation-6683', 'mrqa_triviaqa-validation-4054', 'mrqa_triviaqa-validation-3915', 'mrqa_triviaqa-validation-2530', 'mrqa_squad-validation-392', 'mrqa_squad-validation-8513']
09/23/2021 14:04:55 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:04:55 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=60) .... Timecode: 15
09/23/2021 14:05:09 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:05:09 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 15
09/23/2021 14:05:12 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 14:05:12 - INFO - __main__ - Instant Fixing Rate: 0.8928571428571429
09/23/2021 14:05:12 - INFO - __main__ - Instant Retention Rate: 0.49999999875
09/23/2021 14:05:14 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_016.pt.
09/23/2021 14:05:14 - INFO - __main__ - Saving the current error examples (len=28) to the memory.
09/23/2021 14:05:14 - INFO - __main__ - Current memory size: 419.
09/23/2021 14:05:14 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:05:14 - INFO - __main__ - Finished.
09/23/2021 14:05:14 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:05:14 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:05:14 - INFO - __main__ - Evaluating to get errors .... Timecode: 16
09/23/2021 14:05:17 - INFO - __main__ - Before Error Fixing: 0.0625
09/23/2021 14:05:17 - INFO - __main__ - Found 30 errors.
09/23/2021 14:05:17 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:05:17 - INFO - __main__ - Current memory size: 447.
09/23/2021 14:05:17 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:05:39 - INFO - __main__ - before_losses=[tensor(1.5078), tensor(1.5146), tensor(1.4842), tensor(1.4581), tensor(1.4968), tensor(1.5131), tensor(1.5049), tensor(1.5161), tensor(2.1914), tensor(1.4725), tensor(1.5291), tensor(1.4851), tensor(1.5452), tensor(1.4779), tensor(1.6615), tensor(1.4724), tensor(1.4449), tensor(4.9674), tensor(1.5314), tensor(1.4718), tensor(1.5386), tensor(1.5798), tensor(1.5423), tensor(1.4700), tensor(1.4801), tensor(5.4621), tensor(1.5065), tensor(1.4554), tensor(1.9391), tensor(1.4458), tensor(1.5153), tensor(1.5147), tensor(1.5329), tensor(1.4897), tensor(2.8109), tensor(1.5388), tensor(1.4761), tensor(1.5278), tensor(1.4914), tensor(1.4720), tensor(1.4949), tensor(1.5494), tensor(1.5536), tensor(2.0889), tensor(1.5835), tensor(1.5285), tensor(1.4911), tensor(1.4736), tensor(1.5045), tensor(1.4734), tensor(1.5089), tensor(1.4457), tensor(1.5214), tensor(1.5294), tensor(1.5077), tensor(1.4905), tensor(1.4770), tensor(2.3838), tensor(1.5384), tensor(1.5896), tensor(1.5693), tensor(1.5223), tensor(1.5519), tensor(1.4566), tensor(1.4995), tensor(1.5486), tensor(1.4755), tensor(1.5285), tensor(1.5032), tensor(1.5363), tensor(1.8515), tensor(1.4503), tensor(1.5375), tensor(1.5243), tensor(1.5061), tensor(1.4741), tensor(1.5040), tensor(1.6602), tensor(1.5070), tensor(1.4977), tensor(1.5103), tensor(1.5185), tensor(1.5694), tensor(1.5112), tensor(1.4893), tensor(1.5179), tensor(1.5070), tensor(1.8018), tensor(1.4651), tensor(1.4869), tensor(1.5143), tensor(1.7163), tensor(2.4210), tensor(1.4573), tensor(1.5265), tensor(2.5604), tensor(1.7522), tensor(1.4663), tensor(1.4991), tensor(1.5187), tensor(1.5417), tensor(1.5758), tensor(1.4964), tensor(1.4968), tensor(1.4625), tensor(1.5195), tensor(1.4866), tensor(1.5058), tensor(2.0281), tensor(1.4553), tensor(1.5268), tensor(1.4912), tensor(1.4730), tensor(1.5325), tensor(1.4904), tensor(1.5047), tensor(1.4889), tensor(1.5134), tensor(1.4536), tensor(1.4994), tensor(1.5445), tensor(1.5048), tensor(1.5201), tensor(1.6214), tensor(1.5881), tensor(1.7407), tensor(1.4840), tensor(1.4843), tensor(1.4662), tensor(1.4965), tensor(2.0650), tensor(1.4952), tensor(1.5171), tensor(1.4813), tensor(1.4953), tensor(1.5011), tensor(1.5424), tensor(1.5302), tensor(1.4867), tensor(1.4736), tensor(2.2580), tensor(1.5592), tensor(1.5351), tensor(1.5516), tensor(1.5212), tensor(1.5379), tensor(1.4953), tensor(1.5364), tensor(1.4988), tensor(1.6067), tensor(1.5721), tensor(1.4948), tensor(2.4589), tensor(2.0161), tensor(1.4955), tensor(1.6777), tensor(1.4785), tensor(1.5217), tensor(1.4616), tensor(2.3451), tensor(1.6152), tensor(1.5110), tensor(1.4678), tensor(1.7975), tensor(1.5261), tensor(1.5296), tensor(1.4850), tensor(1.8951), tensor(1.5054), tensor(1.5070), tensor(1.5528), tensor(1.5266), tensor(1.5111), tensor(1.4924), tensor(1.4891), tensor(1.6276), tensor(1.5430), tensor(3.4814), tensor(1.5024), tensor(1.5105), tensor(1.5123), tensor(2.3401), tensor(1.5520), tensor(1.5230), tensor(1.5132), tensor(1.4843), tensor(1.4633), tensor(1.5843), tensor(1.8806), tensor(1.4951), tensor(1.4586), tensor(1.5525), tensor(2.4113), tensor(1.4764), tensor(1.4727), tensor(1.5224), tensor(1.4875), tensor(1.4868), tensor(1.4998), tensor(1.4883), tensor(2.1619), tensor(1.5630), tensor(1.6065), tensor(1.9392), tensor(1.5939), tensor(1.5794), tensor(1.5617), tensor(1.5297), tensor(1.4998), tensor(1.4963), tensor(1.5519), tensor(1.4953), tensor(1.5080), tensor(1.9791), tensor(1.5050), tensor(1.4939), tensor(1.7809), tensor(1.5264), tensor(1.5243), tensor(1.6127), tensor(1.5504), tensor(1.4695), tensor(2.5629), tensor(1.5376), tensor(1.5701), tensor(1.4652), tensor(1.8487), tensor(1.4903), tensor(1.6176), tensor(1.4881), tensor(1.5001), tensor(1.6489), tensor(1.5014), tensor(2.4615), tensor(1.4981), tensor(2.0681), tensor(1.4537), tensor(2.2989), tensor(1.5178), tensor(1.5324), tensor(1.4954), tensor(1.5062), tensor(1.5474), tensor(1.4997), tensor(1.7989), tensor(1.5618), tensor(1.4644), tensor(1.4770), tensor(1.5172), tensor(1.5589), tensor(1.4558), tensor(1.5955), tensor(1.6072), tensor(1.5133), tensor(2.2672), tensor(1.4914)]
09/23/2021 14:05:40 - INFO - __main__ - after_losses=[tensor(1.4987), tensor(1.5025), tensor(1.4681), tensor(1.4499), tensor(1.4871), tensor(1.5116), tensor(1.5049), tensor(1.4943), tensor(2.2315), tensor(1.4692), tensor(1.5027), tensor(1.4919), tensor(1.5197), tensor(1.4724), tensor(1.7664), tensor(1.4545), tensor(1.4388), tensor(4.4542), tensor(1.5046), tensor(1.4604), tensor(1.5251), tensor(1.7794), tensor(1.5350), tensor(1.4605), tensor(1.4743), tensor(4.5461), tensor(1.4754), tensor(1.4524), tensor(2.5610), tensor(1.4402), tensor(1.5141), tensor(1.4912), tensor(1.5997), tensor(1.4984), tensor(2.8761), tensor(1.5439), tensor(1.5170), tensor(1.5171), tensor(1.4799), tensor(1.4839), tensor(1.4863), tensor(1.5337), tensor(1.6441), tensor(2.0339), tensor(1.5604), tensor(1.5275), tensor(1.4969), tensor(1.4714), tensor(1.5005), tensor(1.4692), tensor(1.5046), tensor(1.4425), tensor(1.5236), tensor(1.5285), tensor(1.5045), tensor(1.4897), tensor(1.4809), tensor(2.4148), tensor(1.5332), tensor(1.5690), tensor(1.5259), tensor(1.5222), tensor(1.5572), tensor(1.4467), tensor(1.4873), tensor(1.5535), tensor(1.4717), tensor(1.5201), tensor(1.5041), tensor(1.5408), tensor(1.8178), tensor(1.4448), tensor(1.5642), tensor(1.5295), tensor(1.4982), tensor(1.4927), tensor(1.4845), tensor(1.7535), tensor(1.4880), tensor(1.4914), tensor(1.5043), tensor(1.5140), tensor(1.5631), tensor(1.5024), tensor(1.4747), tensor(1.5066), tensor(1.5117), tensor(2.4100), tensor(1.4751), tensor(1.4939), tensor(1.5050), tensor(1.6327), tensor(1.5522), tensor(1.4509), tensor(1.5264), tensor(2.5265), tensor(1.7519), tensor(1.4574), tensor(1.4829), tensor(1.5242), tensor(1.5385), tensor(1.5689), tensor(1.4920), tensor(1.4988), tensor(1.4494), tensor(1.5488), tensor(1.4862), tensor(1.5005), tensor(1.9223), tensor(1.4450), tensor(1.5173), tensor(1.4952), tensor(1.4780), tensor(1.5253), tensor(1.4729), tensor(1.4988), tensor(1.4866), tensor(1.4729), tensor(1.4449), tensor(1.5100), tensor(1.5329), tensor(1.5037), tensor(1.5207), tensor(1.7278), tensor(1.6122), tensor(1.7294), tensor(1.4815), tensor(1.4809), tensor(1.4602), tensor(1.5035), tensor(1.9496), tensor(1.4858), tensor(1.5172), tensor(1.4646), tensor(1.4931), tensor(1.4849), tensor(1.5383), tensor(1.5303), tensor(1.4764), tensor(1.4739), tensor(2.3330), tensor(1.5123), tensor(1.5484), tensor(1.5667), tensor(1.5225), tensor(1.5273), tensor(1.4887), tensor(1.5402), tensor(1.4600), tensor(1.6284), tensor(1.6067), tensor(1.4782), tensor(2.3849), tensor(2.2154), tensor(1.5046), tensor(1.9491), tensor(1.4788), tensor(1.5221), tensor(1.4679), tensor(2.5282), tensor(1.6001), tensor(1.4811), tensor(1.4967), tensor(2.1101), tensor(1.5344), tensor(1.5115), tensor(1.5174), tensor(1.9256), tensor(1.5151), tensor(1.5142), tensor(1.5382), tensor(1.5060), tensor(1.5036), tensor(1.4930), tensor(1.4698), tensor(1.5619), tensor(1.5237), tensor(3.4056), tensor(1.4993), tensor(2.2824), tensor(1.5066), tensor(2.3533), tensor(1.5458), tensor(1.5205), tensor(1.6753), tensor(1.4784), tensor(1.4547), tensor(1.5868), tensor(1.7651), tensor(1.4893), tensor(1.4528), tensor(1.5513), tensor(2.3220), tensor(1.4678), tensor(1.4727), tensor(1.5269), tensor(1.4882), tensor(1.4750), tensor(1.4828), tensor(1.4894), tensor(2.4670), tensor(1.5420), tensor(1.8248), tensor(2.1501), tensor(1.6684), tensor(1.5475), tensor(1.5591), tensor(1.5039), tensor(1.4905), tensor(1.4945), tensor(1.5497), tensor(1.4662), tensor(1.5096), tensor(2.0433), tensor(1.5200), tensor(1.4955), tensor(1.7143), tensor(1.5275), tensor(1.5117), tensor(1.6235), tensor(1.5558), tensor(1.4527), tensor(2.5761), tensor(1.5313), tensor(1.5855), tensor(1.4609), tensor(1.8571), tensor(1.4763), tensor(1.5412), tensor(1.4855), tensor(1.4816), tensor(1.5897), tensor(1.5052), tensor(1.8204), tensor(1.4916), tensor(2.1730), tensor(1.4554), tensor(2.2501), tensor(1.5044), tensor(1.5871), tensor(1.4976), tensor(1.5080), tensor(1.7022), tensor(1.4990), tensor(1.8488), tensor(1.6238), tensor(1.4637), tensor(1.4811), tensor(1.5212), tensor(1.5890), tensor(1.4544), tensor(1.5920), tensor(1.5723), tensor(1.5072), tensor(2.4341), tensor(1.4732)]
09/23/2021 14:05:40 - INFO - __main__ - interference_scores=[tensor(-0.0091), tensor(-0.0121), tensor(-0.0161), tensor(-0.0083), tensor(-0.0098), tensor(-0.0015), tensor(1.2755e-05), tensor(-0.0218), tensor(0.0400), tensor(-0.0033), tensor(-0.0264), tensor(0.0068), tensor(-0.0256), tensor(-0.0055), tensor(0.1049), tensor(-0.0179), tensor(-0.0060), tensor(-0.5131), tensor(-0.0268), tensor(-0.0114), tensor(-0.0135), tensor(0.1996), tensor(-0.0073), tensor(-0.0095), tensor(-0.0057), tensor(-0.9160), tensor(-0.0311), tensor(-0.0030), tensor(0.6219), tensor(-0.0055), tensor(-0.0012), tensor(-0.0235), tensor(0.0667), tensor(0.0087), tensor(0.0652), tensor(0.0051), tensor(0.0409), tensor(-0.0107), tensor(-0.0115), tensor(0.0120), tensor(-0.0086), tensor(-0.0157), tensor(0.0905), tensor(-0.0550), tensor(-0.0230), tensor(-0.0010), tensor(0.0058), tensor(-0.0023), tensor(-0.0040), tensor(-0.0042), tensor(-0.0043), tensor(-0.0032), tensor(0.0022), tensor(-0.0009), tensor(-0.0032), tensor(-0.0008), tensor(0.0038), tensor(0.0310), tensor(-0.0052), tensor(-0.0206), tensor(-0.0434), tensor(-0.0001), tensor(0.0052), tensor(-0.0099), tensor(-0.0123), tensor(0.0049), tensor(-0.0039), tensor(-0.0085), tensor(0.0009), tensor(0.0046), tensor(-0.0338), tensor(-0.0055), tensor(0.0268), tensor(0.0052), tensor(-0.0079), tensor(0.0185), tensor(-0.0196), tensor(0.0932), tensor(-0.0190), tensor(-0.0063), tensor(-0.0060), tensor(-0.0045), tensor(-0.0063), tensor(-0.0088), tensor(-0.0146), tensor(-0.0113), tensor(0.0047), tensor(0.6082), tensor(0.0100), tensor(0.0070), tensor(-0.0093), tensor(-0.0836), tensor(-0.8688), tensor(-0.0064), tensor(-2.9087e-05), tensor(-0.0339), tensor(-0.0003), tensor(-0.0089), tensor(-0.0162), tensor(0.0055), tensor(-0.0032), tensor(-0.0069), tensor(-0.0044), tensor(0.0020), tensor(-0.0131), tensor(0.0293), tensor(-0.0003), tensor(-0.0053), tensor(-0.1058), tensor(-0.0104), tensor(-0.0095), tensor(0.0041), tensor(0.0050), tensor(-0.0072), tensor(-0.0175), tensor(-0.0059), tensor(-0.0022), tensor(-0.0405), tensor(-0.0087), tensor(0.0106), tensor(-0.0116), tensor(-0.0011), tensor(0.0006), tensor(0.1064), tensor(0.0242), tensor(-0.0113), tensor(-0.0026), tensor(-0.0035), tensor(-0.0060), tensor(0.0071), tensor(-0.1154), tensor(-0.0094), tensor(6.7830e-05), tensor(-0.0167), tensor(-0.0022), tensor(-0.0161), tensor(-0.0042), tensor(0.0001), tensor(-0.0103), tensor(0.0003), tensor(0.0751), tensor(-0.0469), tensor(0.0133), tensor(0.0152), tensor(0.0012), tensor(-0.0106), tensor(-0.0067), tensor(0.0038), tensor(-0.0388), tensor(0.0217), tensor(0.0346), tensor(-0.0166), tensor(-0.0740), tensor(0.1994), tensor(0.0091), tensor(0.2715), tensor(0.0004), tensor(0.0003), tensor(0.0063), tensor(0.1831), tensor(-0.0152), tensor(-0.0299), tensor(0.0289), tensor(0.3126), tensor(0.0083), tensor(-0.0181), tensor(0.0324), tensor(0.0305), tensor(0.0097), tensor(0.0072), tensor(-0.0146), tensor(-0.0207), tensor(-0.0075), tensor(0.0006), tensor(-0.0193), tensor(-0.0657), tensor(-0.0193), tensor(-0.0758), tensor(-0.0031), tensor(0.7719), tensor(-0.0057), tensor(0.0132), tensor(-0.0062), tensor(-0.0025), tensor(0.1622), tensor(-0.0059), tensor(-0.0086), tensor(0.0025), tensor(-0.1154), tensor(-0.0058), tensor(-0.0058), tensor(-0.0012), tensor(-0.0892), tensor(-0.0086), tensor(-3.3379e-05), tensor(0.0045), tensor(0.0007), tensor(-0.0117), tensor(-0.0169), tensor(0.0011), tensor(0.3051), tensor(-0.0210), tensor(0.2183), tensor(0.2109), tensor(0.0746), tensor(-0.0319), tensor(-0.0025), tensor(-0.0257), tensor(-0.0093), tensor(-0.0018), tensor(-0.0022), tensor(-0.0291), tensor(0.0016), tensor(0.0642), tensor(0.0151), tensor(0.0016), tensor(-0.0666), tensor(0.0012), tensor(-0.0126), tensor(0.0108), tensor(0.0054), tensor(-0.0168), tensor(0.0132), tensor(-0.0063), tensor(0.0154), tensor(-0.0043), tensor(0.0084), tensor(-0.0140), tensor(-0.0764), tensor(-0.0026), tensor(-0.0184), tensor(-0.0592), tensor(0.0038), tensor(-0.6410), tensor(-0.0064), tensor(0.1049), tensor(0.0017), tensor(-0.0488), tensor(-0.0134), tensor(0.0547), tensor(0.0022), tensor(0.0018), tensor(0.1549), tensor(-0.0007), tensor(0.0499), tensor(0.0620), tensor(-0.0007), tensor(0.0041), tensor(0.0040), tensor(0.0300), tensor(-0.0013), tensor(-0.0034), tensor(-0.0349), tensor(-0.0061), tensor(0.1668), tensor(-0.0182)]
09/23/2021 14:05:40 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-7506', 'mrqa_triviaqa-validation-1551', 'mrqa_triviaqa-validation-2530', 'mrqa_squad-validation-6947', 'mrqa_squad-validation-6924', 'mrqa_triviaqa-validation-5937', 'mrqa_triviaqa-validation-1860', 'mrqa_triviaqa-validation-6639', 'mrqa_hotpotqa-validation-5810', 'mrqa_triviaqa-validation-4583', 'mrqa_triviaqa-validation-681', 'mrqa_triviaqa-validation-7767', 'mrqa_naturalquestions-validation-868', 'mrqa_squad-validation-1374', 'mrqa_triviaqa-validation-7179', 'mrqa_squad-validation-6677', 'mrqa_triviaqa-validation-7153', 'mrqa_triviaqa-validation-1451', 'mrqa_triviaqa-validation-781', 'mrqa_naturalquestions-validation-6736', 'mrqa_triviaqa-validation-5654', 'mrqa_hotpotqa-validation-1591', 'mrqa_triviaqa-validation-1736', 'mrqa_triviaqa-validation-6119', 'mrqa_naturalquestions-validation-5522', 'mrqa_hotpotqa-validation-3161', 'mrqa_hotpotqa-validation-4621', 'mrqa_triviaqa-validation-3808', 'mrqa_squad-validation-2509', 'mrqa_squad-validation-5313', 'mrqa_squad-validation-2372', 'mrqa_squad-validation-5618']
09/23/2021 14:05:40 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:05:40 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=62) .... Timecode: 16
09/23/2021 14:05:54 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:05:54 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 16
09/23/2021 14:05:58 - INFO - __main__ - After Error Fixing: 0.875
09/23/2021 14:05:58 - INFO - __main__ - Instant Fixing Rate: 0.9
09/23/2021 14:05:58 - INFO - __main__ - Instant Retention Rate: 0.4999999975
09/23/2021 14:05:59 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_017.pt.
09/23/2021 14:05:59 - INFO - __main__ - Saving the current error examples (len=30) to the memory.
09/23/2021 14:05:59 - INFO - __main__ - Current memory size: 447.
09/23/2021 14:05:59 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:05:59 - INFO - __main__ - Finished.
09/23/2021 14:05:59 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:05:59 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:05:59 - INFO - __main__ - Evaluating to get errors .... Timecode: 17
09/23/2021 14:06:03 - INFO - __main__ - Before Error Fixing: 0.15625
09/23/2021 14:06:03 - INFO - __main__ - Found 27 errors.
09/23/2021 14:06:03 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:06:03 - INFO - __main__ - Current memory size: 477.
09/23/2021 14:06:03 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:06:26 - INFO - __main__ - before_losses=[tensor(1.5658), tensor(1.5644), tensor(1.4910), tensor(1.5057), tensor(2.1669), tensor(1.5354), tensor(1.5264), tensor(1.6128), tensor(1.5047), tensor(1.5427), tensor(1.5182), tensor(1.5540), tensor(1.5233), tensor(1.9120), tensor(1.8625), tensor(2.2291), tensor(1.5458), tensor(1.5602), tensor(2.1268), tensor(1.6794), tensor(1.4850), tensor(2.0470), tensor(1.5401), tensor(1.5138), tensor(2.0739), tensor(1.5162), tensor(1.5214), tensor(1.5036), tensor(1.4741), tensor(1.5455), tensor(1.5937), tensor(1.6036), tensor(2.3907), tensor(1.5330), tensor(1.5272), tensor(1.9713), tensor(1.5798), tensor(1.5718), tensor(1.5639), tensor(1.6767), tensor(1.5051), tensor(1.5166), tensor(1.5389), tensor(4.4471), tensor(1.5641), tensor(1.5109), tensor(1.6120), tensor(1.4659), tensor(1.5140), tensor(1.5488), tensor(1.5062), tensor(2.0069), tensor(2.5780), tensor(1.5066), tensor(1.5534), tensor(1.4948), tensor(1.6070), tensor(1.4834), tensor(1.5593), tensor(3.5062), tensor(1.4779), tensor(1.5415), tensor(1.5380), tensor(1.5199), tensor(2.1422), tensor(1.5935), tensor(1.5221), tensor(1.4653), tensor(2.6569), tensor(1.9475), tensor(1.5543), tensor(1.4897), tensor(1.5272), tensor(1.5371), tensor(1.5123), tensor(1.4851), tensor(1.6744), tensor(1.5067), tensor(1.5251), tensor(1.5104), tensor(1.5626), tensor(1.8140), tensor(1.5274), tensor(1.4911), tensor(1.5023), tensor(1.5197), tensor(1.4967), tensor(1.5798), tensor(1.5949), tensor(1.5388), tensor(1.9707), tensor(1.5123), tensor(1.5457), tensor(1.5864), tensor(1.5255), tensor(1.5941), tensor(1.4820), tensor(1.5610), tensor(1.9149), tensor(1.9505), tensor(1.5710), tensor(1.5838), tensor(1.6111), tensor(1.5032), tensor(1.5470), tensor(1.5249), tensor(1.5427), tensor(1.5185), tensor(1.5450), tensor(1.5672), tensor(1.5310), tensor(1.5394), tensor(1.4509), tensor(1.5429), tensor(1.6916), tensor(1.5007), tensor(1.5917), tensor(1.5293), tensor(1.5764), tensor(1.5418), tensor(1.5045), tensor(1.5365), tensor(1.5455), tensor(1.4905), tensor(1.6311), tensor(1.5637), tensor(1.5510), tensor(1.5190), tensor(1.4721), tensor(1.5028), tensor(1.7331), tensor(1.4856), tensor(1.4962), tensor(1.5707), tensor(1.6049), tensor(1.5717), tensor(1.5360), tensor(1.8757), tensor(1.4573), tensor(1.5263), tensor(1.5476), tensor(1.4847), tensor(1.5369), tensor(1.4867), tensor(1.5268), tensor(5.5524), tensor(1.5669), tensor(2.0267), tensor(1.5299), tensor(1.5478), tensor(1.4772), tensor(1.5166), tensor(1.4458), tensor(1.5120), tensor(1.5194), tensor(1.5668), tensor(1.5659), tensor(1.5324), tensor(2.5062), tensor(1.5235), tensor(1.4930), tensor(1.5761), tensor(1.4903), tensor(1.4991), tensor(1.7432), tensor(1.5978), tensor(1.5459), tensor(1.5247), tensor(2.6183), tensor(1.4603), tensor(1.5301), tensor(1.5720), tensor(1.5280), tensor(1.5303), tensor(3.0364), tensor(1.4499), tensor(1.5948), tensor(1.4747), tensor(1.5372), tensor(1.9089), tensor(1.4944), tensor(1.5096), tensor(1.4679), tensor(1.4675), tensor(1.5991), tensor(1.4822), tensor(1.5708), tensor(2.6752), tensor(1.6976), tensor(1.5762), tensor(1.5239), tensor(1.4791), tensor(1.4936), tensor(1.8812), tensor(1.8973), tensor(1.4977), tensor(1.5548), tensor(1.4983), tensor(1.6727), tensor(1.7145), tensor(1.5168), tensor(1.5192), tensor(1.5001), tensor(1.4869), tensor(1.5105), tensor(1.5354), tensor(1.5723), tensor(1.5556), tensor(1.5214), tensor(1.4883), tensor(1.5359), tensor(1.5264), tensor(1.5570), tensor(1.8629), tensor(1.5706), tensor(1.5686), tensor(1.5472), tensor(1.6161), tensor(1.5743), tensor(2.1598), tensor(1.5112), tensor(2.5149), tensor(1.5511), tensor(1.5272), tensor(1.5064), tensor(1.5053), tensor(1.5332), tensor(1.4676), tensor(1.5654), tensor(1.5874), tensor(2.9228), tensor(1.5015), tensor(1.5459), tensor(1.5147), tensor(1.5789), tensor(1.4920), tensor(1.6998), tensor(1.4988), tensor(1.4621), tensor(1.4740), tensor(1.5414), tensor(1.5056), tensor(1.5982), tensor(1.7433), tensor(1.5377), tensor(1.5203), tensor(1.4744), tensor(1.5920), tensor(1.6673), tensor(1.5517), tensor(1.4661), tensor(2.1935), tensor(1.5520), tensor(1.5093), tensor(1.7991), tensor(4.5682)]
09/23/2021 14:06:26 - INFO - __main__ - after_losses=[tensor(1.5953), tensor(1.5452), tensor(1.4805), tensor(1.4848), tensor(2.3174), tensor(1.5211), tensor(1.4941), tensor(1.5633), tensor(1.4861), tensor(1.5202), tensor(1.5047), tensor(1.5539), tensor(1.4884), tensor(2.1030), tensor(1.9738), tensor(2.0038), tensor(1.5290), tensor(1.5250), tensor(2.3419), tensor(1.6537), tensor(1.4816), tensor(2.9818), tensor(1.5334), tensor(1.4946), tensor(1.9563), tensor(1.5180), tensor(1.4828), tensor(1.4963), tensor(1.4564), tensor(1.5203), tensor(1.5817), tensor(1.5910), tensor(2.0477), tensor(1.4953), tensor(1.5293), tensor(1.9575), tensor(1.5379), tensor(2.2325), tensor(1.5494), tensor(1.5979), tensor(1.4917), tensor(1.4917), tensor(1.5195), tensor(4.0884), tensor(1.5859), tensor(1.4951), tensor(1.5310), tensor(1.4478), tensor(1.4904), tensor(1.5103), tensor(1.4870), tensor(1.8471), tensor(2.2042), tensor(1.4919), tensor(1.5392), tensor(1.4957), tensor(1.5901), tensor(1.4687), tensor(1.5445), tensor(3.7405), tensor(1.4815), tensor(1.5643), tensor(1.5198), tensor(1.4935), tensor(2.1311), tensor(1.5524), tensor(1.5131), tensor(1.4595), tensor(1.6978), tensor(1.7729), tensor(1.5396), tensor(1.4715), tensor(1.5013), tensor(1.5203), tensor(1.5018), tensor(1.4709), tensor(1.6767), tensor(1.4986), tensor(1.5086), tensor(1.4918), tensor(1.5610), tensor(1.5480), tensor(1.5255), tensor(1.4738), tensor(1.4963), tensor(1.6955), tensor(1.4813), tensor(1.5662), tensor(1.5725), tensor(1.5364), tensor(1.7796), tensor(1.4867), tensor(1.5346), tensor(1.5773), tensor(1.4847), tensor(1.5494), tensor(1.4654), tensor(1.5180), tensor(1.6921), tensor(1.5617), tensor(1.5525), tensor(1.6205), tensor(1.5300), tensor(1.4885), tensor(1.5235), tensor(1.5171), tensor(1.5260), tensor(1.5019), tensor(1.5270), tensor(1.5545), tensor(1.5169), tensor(1.5199), tensor(1.4477), tensor(1.5167), tensor(1.5800), tensor(1.4905), tensor(1.5796), tensor(1.5090), tensor(1.5252), tensor(1.5356), tensor(1.4821), tensor(1.5162), tensor(1.5425), tensor(1.4775), tensor(1.5444), tensor(1.5322), tensor(1.5251), tensor(1.5036), tensor(1.4655), tensor(1.4756), tensor(1.6704), tensor(1.4692), tensor(1.4842), tensor(1.5604), tensor(1.5935), tensor(1.5552), tensor(1.5150), tensor(1.9662), tensor(1.4521), tensor(1.5153), tensor(1.5183), tensor(1.4792), tensor(1.5114), tensor(1.4763), tensor(1.5049), tensor(5.7029), tensor(1.5553), tensor(1.5459), tensor(1.5128), tensor(1.5461), tensor(1.4652), tensor(1.4867), tensor(1.4433), tensor(1.5371), tensor(1.5084), tensor(1.5349), tensor(1.5447), tensor(1.5224), tensor(2.5570), tensor(1.5060), tensor(1.4758), tensor(1.5670), tensor(1.4685), tensor(1.4777), tensor(1.5033), tensor(1.5685), tensor(1.5183), tensor(1.5153), tensor(2.3150), tensor(1.4522), tensor(1.5244), tensor(1.5359), tensor(1.5006), tensor(1.5228), tensor(3.0154), tensor(1.4540), tensor(1.6106), tensor(1.4576), tensor(1.5175), tensor(1.7742), tensor(1.4786), tensor(1.4917), tensor(1.4627), tensor(1.4576), tensor(1.5519), tensor(1.4645), tensor(1.5427), tensor(2.8356), tensor(1.6434), tensor(1.5543), tensor(1.5144), tensor(1.4804), tensor(1.4806), tensor(2.1531), tensor(1.8396), tensor(1.4918), tensor(1.8326), tensor(1.4919), tensor(1.6785), tensor(1.7103), tensor(1.4956), tensor(1.5054), tensor(1.4897), tensor(1.4770), tensor(1.4937), tensor(1.5231), tensor(1.5296), tensor(1.5454), tensor(1.4875), tensor(1.4808), tensor(1.5139), tensor(1.5042), tensor(1.5463), tensor(1.6044), tensor(1.5484), tensor(1.5398), tensor(1.5136), tensor(1.5962), tensor(1.8332), tensor(2.5125), tensor(1.4953), tensor(2.4468), tensor(1.5183), tensor(1.5049), tensor(1.4906), tensor(1.6658), tensor(1.5075), tensor(1.4651), tensor(1.5317), tensor(1.5456), tensor(2.9792), tensor(1.4849), tensor(1.5455), tensor(1.5022), tensor(1.5382), tensor(1.5076), tensor(1.5462), tensor(1.4789), tensor(1.4446), tensor(1.4681), tensor(1.5222), tensor(1.5003), tensor(1.5710), tensor(1.7743), tensor(1.5139), tensor(1.5000), tensor(1.4593), tensor(1.5673), tensor(1.5932), tensor(1.5286), tensor(1.4752), tensor(2.0918), tensor(1.5399), tensor(1.4850), tensor(1.7881), tensor(4.5643)]
09/23/2021 14:06:26 - INFO - __main__ - interference_scores=[tensor(0.0296), tensor(-0.0192), tensor(-0.0104), tensor(-0.0209), tensor(0.1506), tensor(-0.0143), tensor(-0.0323), tensor(-0.0495), tensor(-0.0185), tensor(-0.0224), tensor(-0.0136), tensor(-0.0001), tensor(-0.0349), tensor(0.1910), tensor(0.1113), tensor(-0.2254), tensor(-0.0169), tensor(-0.0353), tensor(0.2150), tensor(-0.0257), tensor(-0.0034), tensor(0.9348), tensor(-0.0067), tensor(-0.0192), tensor(-0.1177), tensor(0.0018), tensor(-0.0386), tensor(-0.0073), tensor(-0.0177), tensor(-0.0252), tensor(-0.0120), tensor(-0.0126), tensor(-0.3430), tensor(-0.0377), tensor(0.0021), tensor(-0.0138), tensor(-0.0419), tensor(0.6608), tensor(-0.0145), tensor(-0.0787), tensor(-0.0134), tensor(-0.0249), tensor(-0.0195), tensor(-0.3588), tensor(0.0219), tensor(-0.0157), tensor(-0.0810), tensor(-0.0180), tensor(-0.0237), tensor(-0.0386), tensor(-0.0192), tensor(-0.1597), tensor(-0.3738), tensor(-0.0147), tensor(-0.0142), tensor(0.0009), tensor(-0.0169), tensor(-0.0147), tensor(-0.0148), tensor(0.2343), tensor(0.0036), tensor(0.0228), tensor(-0.0183), tensor(-0.0265), tensor(-0.0110), tensor(-0.0410), tensor(-0.0090), tensor(-0.0058), tensor(-0.9592), tensor(-0.1746), tensor(-0.0147), tensor(-0.0182), tensor(-0.0259), tensor(-0.0168), tensor(-0.0105), tensor(-0.0141), tensor(0.0023), tensor(-0.0081), tensor(-0.0165), tensor(-0.0186), tensor(-0.0016), tensor(-0.2661), tensor(-0.0019), tensor(-0.0173), tensor(-0.0060), tensor(0.1758), tensor(-0.0153), tensor(-0.0136), tensor(-0.0224), tensor(-0.0024), tensor(-0.1911), tensor(-0.0256), tensor(-0.0111), tensor(-0.0091), tensor(-0.0408), tensor(-0.0447), tensor(-0.0166), tensor(-0.0429), tensor(-0.2227), tensor(-0.3888), tensor(-0.0185), tensor(0.0367), tensor(-0.0811), tensor(-0.0147), tensor(-0.0235), tensor(-0.0078), tensor(-0.0167), tensor(-0.0166), tensor(-0.0180), tensor(-0.0127), tensor(-0.0141), tensor(-0.0195), tensor(-0.0032), tensor(-0.0262), tensor(-0.1116), tensor(-0.0102), tensor(-0.0122), tensor(-0.0204), tensor(-0.0512), tensor(-0.0061), tensor(-0.0223), tensor(-0.0203), tensor(-0.0030), tensor(-0.0130), tensor(-0.0866), tensor(-0.0315), tensor(-0.0258), tensor(-0.0154), tensor(-0.0066), tensor(-0.0271), tensor(-0.0628), tensor(-0.0164), tensor(-0.0120), tensor(-0.0103), tensor(-0.0113), tensor(-0.0166), tensor(-0.0210), tensor(0.0905), tensor(-0.0051), tensor(-0.0110), tensor(-0.0293), tensor(-0.0055), tensor(-0.0255), tensor(-0.0103), tensor(-0.0218), tensor(0.1505), tensor(-0.0116), tensor(-0.4807), tensor(-0.0170), tensor(-0.0016), tensor(-0.0121), tensor(-0.0299), tensor(-0.0026), tensor(0.0251), tensor(-0.0110), tensor(-0.0318), tensor(-0.0212), tensor(-0.0100), tensor(0.0508), tensor(-0.0175), tensor(-0.0172), tensor(-0.0091), tensor(-0.0219), tensor(-0.0215), tensor(-0.2399), tensor(-0.0293), tensor(-0.0276), tensor(-0.0095), tensor(-0.3033), tensor(-0.0081), tensor(-0.0057), tensor(-0.0360), tensor(-0.0274), tensor(-0.0075), tensor(-0.0210), tensor(0.0041), tensor(0.0159), tensor(-0.0171), tensor(-0.0197), tensor(-0.1346), tensor(-0.0158), tensor(-0.0179), tensor(-0.0052), tensor(-0.0100), tensor(-0.0472), tensor(-0.0177), tensor(-0.0281), tensor(0.1604), tensor(-0.0542), tensor(-0.0219), tensor(-0.0095), tensor(0.0013), tensor(-0.0131), tensor(0.2719), tensor(-0.0577), tensor(-0.0059), tensor(0.2778), tensor(-0.0064), tensor(0.0058), tensor(-0.0041), tensor(-0.0212), tensor(-0.0138), tensor(-0.0104), tensor(-0.0099), tensor(-0.0168), tensor(-0.0124), tensor(-0.0427), tensor(-0.0102), tensor(-0.0339), tensor(-0.0075), tensor(-0.0221), tensor(-0.0222), tensor(-0.0108), tensor(-0.2585), tensor(-0.0223), tensor(-0.0288), tensor(-0.0336), tensor(-0.0199), tensor(0.2588), tensor(0.3527), tensor(-0.0160), tensor(-0.0680), tensor(-0.0328), tensor(-0.0222), tensor(-0.0159), tensor(0.1605), tensor(-0.0257), tensor(-0.0025), tensor(-0.0337), tensor(-0.0418), tensor(0.0565), tensor(-0.0166), tensor(-0.0004), tensor(-0.0125), tensor(-0.0407), tensor(0.0157), tensor(-0.1536), tensor(-0.0198), tensor(-0.0176), tensor(-0.0058), tensor(-0.0192), tensor(-0.0053), tensor(-0.0272), tensor(0.0310), tensor(-0.0237), tensor(-0.0203), tensor(-0.0150), tensor(-0.0247), tensor(-0.0741), tensor(-0.0231), tensor(0.0090), tensor(-0.1017), tensor(-0.0122), tensor(-0.0243), tensor(-0.0110), tensor(-0.0039)]
09/23/2021 14:06:26 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-1736', 'mrqa_triviaqa-validation-2551', 'mrqa_triviaqa-validation-7512', 'mrqa_triviaqa-validation-5972', 'mrqa_triviaqa-validation-1293', 'mrqa_triviaqa-validation-6944', 'mrqa_triviaqa-validation-1671', 'mrqa_triviaqa-validation-2368', 'mrqa_squad-validation-3181', 'mrqa_squad-validation-1374', 'mrqa_hotpotqa-validation-1099', 'mrqa_hotpotqa-validation-2612', 'mrqa_triviaqa-validation-6913', 'mrqa_triviaqa-validation-6902', 'mrqa_naturalquestions-validation-1085', 'mrqa_squad-validation-10369', 'mrqa_triviaqa-validation-1616', 'mrqa_naturalquestions-validation-98', 'mrqa_hotpotqa-validation-983', 'mrqa_hotpotqa-validation-1436', 'mrqa_hotpotqa-validation-241', 'mrqa_triviaqa-validation-1770', 'mrqa_squad-validation-5622', 'mrqa_triviaqa-validation-2530', 'mrqa_triviaqa-validation-3945', 'mrqa_squad-validation-392', 'mrqa_naturalquestions-validation-3483', 'mrqa_triviaqa-validation-4768', 'mrqa_naturalquestions-validation-4803', 'mrqa_hotpotqa-validation-1968', 'mrqa_triviaqa-validation-1437', 'mrqa_squad-validation-1705']
09/23/2021 14:06:26 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:06:26 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=59) .... Timecode: 17
09/23/2021 14:06:39 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:06:39 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 17
09/23/2021 14:06:43 - INFO - __main__ - After Error Fixing: 0.875
09/23/2021 14:06:43 - INFO - __main__ - Instant Fixing Rate: 0.9259259259259259
09/23/2021 14:06:43 - INFO - __main__ - Instant Retention Rate: 0.5999999988
09/23/2021 14:06:44 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_018.pt.
09/23/2021 14:06:44 - INFO - __main__ - Saving the current error examples (len=27) to the memory.
09/23/2021 14:06:44 - INFO - __main__ - Current memory size: 477.
09/23/2021 14:06:44 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:06:44 - INFO - __main__ - Finished.
09/23/2021 14:06:44 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:06:44 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:06:44 - INFO - __main__ - Evaluating to get errors .... Timecode: 18
09/23/2021 14:06:48 - INFO - __main__ - Before Error Fixing: 0.15625
09/23/2021 14:06:48 - INFO - __main__ - Found 27 errors.
09/23/2021 14:06:48 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:06:48 - INFO - __main__ - Current memory size: 504.
09/23/2021 14:06:48 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:07:11 - INFO - __main__ - before_losses=[tensor(1.5152), tensor(1.4936), tensor(1.5008), tensor(1.4708), tensor(1.5464), tensor(1.5085), tensor(1.5422), tensor(1.5165), tensor(1.5880), tensor(1.4939), tensor(1.6765), tensor(1.4770), tensor(2.3371), tensor(1.6482), tensor(1.6408), tensor(1.4913), tensor(2.5945), tensor(1.6389), tensor(2.5563), tensor(2.4811), tensor(1.5103), tensor(2.0211), tensor(1.5243), tensor(1.5451), tensor(1.5072), tensor(1.5385), tensor(2.0900), tensor(1.5758), tensor(1.4974), tensor(1.4970), tensor(1.5271), tensor(1.7907), tensor(1.5300), tensor(1.5322), tensor(1.5033), tensor(1.4813), tensor(1.5956), tensor(1.4897), tensor(1.6919), tensor(1.5251), tensor(1.4977), tensor(1.5024), tensor(1.5108), tensor(1.5329), tensor(1.5549), tensor(1.4880), tensor(1.5503), tensor(1.5125), tensor(1.5036), tensor(1.5045), tensor(1.4946), tensor(1.5370), tensor(1.5577), tensor(1.5314), tensor(1.4903), tensor(2.5326), tensor(2.5046), tensor(2.1211), tensor(1.5348), tensor(1.5338), tensor(1.7223), tensor(1.5116), tensor(1.4835), tensor(1.4775), tensor(1.4631), tensor(1.4804), tensor(2.8693), tensor(1.5629), tensor(1.5051), tensor(1.5592), tensor(1.4713), tensor(1.4803), tensor(2.2616), tensor(1.5044), tensor(1.5854), tensor(1.5210), tensor(2.4150), tensor(1.5720), tensor(1.5237), tensor(1.5698), tensor(2.4104), tensor(2.1611), tensor(1.5534), tensor(1.5290), tensor(1.4993), tensor(1.4947), tensor(1.4591), tensor(1.4912), tensor(1.5525), tensor(1.5153), tensor(1.5132), tensor(1.5125), tensor(1.4983), tensor(1.7868), tensor(1.5414), tensor(1.4828), tensor(1.5355), tensor(1.4949), tensor(1.6948), tensor(1.4605), tensor(1.5445), tensor(1.5079), tensor(1.5037), tensor(2.4805), tensor(1.5282), tensor(1.4817), tensor(1.5011), tensor(1.5083), tensor(1.5094), tensor(1.5288), tensor(1.4943), tensor(1.5023), tensor(1.4674), tensor(1.4982), tensor(1.5974), tensor(1.5641), tensor(1.5802), tensor(1.4642), tensor(1.9950), tensor(2.3660), tensor(1.4846), tensor(1.4889), tensor(1.6163), tensor(2.5735), tensor(1.5470), tensor(1.5609), tensor(2.1986), tensor(1.5455), tensor(1.5547), tensor(1.4722), tensor(1.5998), tensor(1.5091), tensor(1.8447), tensor(1.5226), tensor(1.5302), tensor(1.6358), tensor(1.4878), tensor(1.5602), tensor(1.5471), tensor(1.5599), tensor(1.5258), tensor(1.5344), tensor(1.4989), tensor(1.5545), tensor(1.5139), tensor(1.5200), tensor(1.5100), tensor(1.5167), tensor(1.5242), tensor(1.4988), tensor(3.3878), tensor(1.5262), tensor(1.7179), tensor(3.0071), tensor(1.4812), tensor(1.5142), tensor(1.5707), tensor(1.9184), tensor(1.5236), tensor(1.4890), tensor(1.4796), tensor(1.4959), tensor(1.5781), tensor(1.5673), tensor(1.4786), tensor(2.2176), tensor(1.5503), tensor(1.5106), tensor(1.5648), tensor(1.5181), tensor(1.9007), tensor(1.5855), tensor(1.5088), tensor(1.5897), tensor(1.5007), tensor(1.5330), tensor(1.7557), tensor(1.5677), tensor(1.8223), tensor(1.5289), tensor(1.8653), tensor(1.4870), tensor(1.5412), tensor(1.5256), tensor(1.4785), tensor(3.4671), tensor(1.5618), tensor(1.8689), tensor(1.5401), tensor(2.4620), tensor(1.5432), tensor(1.5322), tensor(1.5819), tensor(1.5089), tensor(1.4764), tensor(1.4985), tensor(1.5220), tensor(1.4791), tensor(1.4975), tensor(1.5270), tensor(2.5551), tensor(1.5546), tensor(1.4945), tensor(1.5193), tensor(1.7507), tensor(1.5403), tensor(1.5002), tensor(1.5686), tensor(1.5293), tensor(1.5168), tensor(1.5759), tensor(1.5179), tensor(1.8013), tensor(1.5606), tensor(1.5665), tensor(1.5047), tensor(1.5146), tensor(1.5649), tensor(1.5043), tensor(1.4997), tensor(2.0686), tensor(1.4911), tensor(2.2664), tensor(2.6935), tensor(1.5311), tensor(1.5055), tensor(1.5636), tensor(1.9022), tensor(1.4899), tensor(2.1385), tensor(1.5230), tensor(1.5728), tensor(1.4569), tensor(1.5246), tensor(1.5013), tensor(1.6566), tensor(1.6119), tensor(1.4922), tensor(1.4935), tensor(1.6058), tensor(1.4935), tensor(1.5410), tensor(1.4762), tensor(1.4960), tensor(1.6021), tensor(1.5103), tensor(1.5391), tensor(1.5309), tensor(1.4869), tensor(1.4996), tensor(1.5329), tensor(1.4641), tensor(1.5266), tensor(1.4924), tensor(1.5509), tensor(1.5260)]
09/23/2021 14:07:11 - INFO - __main__ - after_losses=[tensor(1.5586), tensor(1.4648), tensor(1.4843), tensor(1.4590), tensor(1.5654), tensor(1.4853), tensor(1.5305), tensor(1.5058), tensor(1.5187), tensor(1.4676), tensor(1.5784), tensor(1.4743), tensor(2.3662), tensor(1.6971), tensor(1.5516), tensor(1.4705), tensor(2.4566), tensor(1.6219), tensor(2.6928), tensor(2.3536), tensor(1.4833), tensor(2.1988), tensor(1.5042), tensor(1.5293), tensor(1.4838), tensor(1.5244), tensor(1.9804), tensor(1.5254), tensor(1.4853), tensor(1.4829), tensor(1.4919), tensor(1.7077), tensor(1.5168), tensor(1.5197), tensor(1.4887), tensor(1.4754), tensor(1.5729), tensor(1.4668), tensor(1.6205), tensor(1.5162), tensor(1.4686), tensor(1.5068), tensor(1.4922), tensor(1.5184), tensor(1.5178), tensor(1.4595), tensor(1.4936), tensor(1.4906), tensor(1.4915), tensor(1.4885), tensor(1.4813), tensor(1.4884), tensor(1.5174), tensor(1.5164), tensor(1.4824), tensor(2.5435), tensor(2.4909), tensor(2.0843), tensor(1.5157), tensor(1.4843), tensor(1.7059), tensor(1.5077), tensor(1.4646), tensor(1.4715), tensor(1.4579), tensor(1.4968), tensor(3.1479), tensor(1.5664), tensor(1.4863), tensor(1.5238), tensor(1.4585), tensor(1.4652), tensor(2.3820), tensor(1.4818), tensor(1.5916), tensor(1.4850), tensor(2.1587), tensor(1.5405), tensor(1.4902), tensor(1.5451), tensor(2.4215), tensor(2.1283), tensor(1.5381), tensor(1.4596), tensor(1.4943), tensor(1.4901), tensor(1.4515), tensor(1.4787), tensor(1.5381), tensor(1.5207), tensor(1.4998), tensor(1.4890), tensor(1.4795), tensor(1.6746), tensor(1.5463), tensor(1.4603), tensor(1.5168), tensor(1.4809), tensor(1.6929), tensor(1.4472), tensor(1.5230), tensor(1.4847), tensor(1.4987), tensor(2.2667), tensor(1.5163), tensor(1.4715), tensor(1.4857), tensor(1.4947), tensor(1.4855), tensor(1.5138), tensor(1.4791), tensor(1.4842), tensor(1.4482), tensor(1.5100), tensor(1.5426), tensor(1.5400), tensor(1.5341), tensor(1.4544), tensor(1.9811), tensor(2.6426), tensor(1.4768), tensor(1.4672), tensor(1.5919), tensor(2.6022), tensor(1.5017), tensor(1.5337), tensor(2.1461), tensor(1.5302), tensor(1.5284), tensor(1.4566), tensor(1.7417), tensor(1.4874), tensor(1.8756), tensor(1.5036), tensor(1.5050), tensor(1.5791), tensor(1.4704), tensor(1.5420), tensor(1.5178), tensor(1.5255), tensor(1.4907), tensor(1.5150), tensor(1.4754), tensor(1.5035), tensor(1.4824), tensor(1.4949), tensor(1.4946), tensor(1.4687), tensor(1.4957), tensor(1.4671), tensor(2.4278), tensor(1.5070), tensor(1.7193), tensor(3.0307), tensor(1.4562), tensor(1.4969), tensor(1.6102), tensor(2.0884), tensor(1.5118), tensor(1.4785), tensor(1.4636), tensor(1.4760), tensor(1.5398), tensor(1.6018), tensor(1.4604), tensor(2.1659), tensor(1.5350), tensor(2.2829), tensor(1.5437), tensor(1.4909), tensor(1.8410), tensor(1.5498), tensor(1.4892), tensor(1.5936), tensor(1.4901), tensor(1.5137), tensor(1.6511), tensor(1.5367), tensor(1.8103), tensor(1.5132), tensor(1.8355), tensor(1.4682), tensor(1.6048), tensor(1.5091), tensor(1.4659), tensor(3.5163), tensor(1.5237), tensor(1.8940), tensor(1.5078), tensor(1.5328), tensor(1.5178), tensor(1.5136), tensor(1.5059), tensor(1.4775), tensor(1.4609), tensor(1.4833), tensor(1.4975), tensor(1.4645), tensor(1.4724), tensor(1.5584), tensor(2.5333), tensor(1.5045), tensor(1.4795), tensor(1.5045), tensor(1.9626), tensor(1.5042), tensor(1.4885), tensor(1.5456), tensor(1.4749), tensor(1.4800), tensor(1.5536), tensor(1.4918), tensor(1.5303), tensor(1.5494), tensor(1.5741), tensor(1.4858), tensor(1.5013), tensor(1.5518), tensor(1.4889), tensor(1.4828), tensor(2.1620), tensor(1.4888), tensor(2.1752), tensor(2.5990), tensor(1.4965), tensor(1.4823), tensor(1.5513), tensor(2.1390), tensor(1.4818), tensor(2.3450), tensor(1.5111), tensor(1.5651), tensor(1.4437), tensor(1.4987), tensor(1.5098), tensor(1.6047), tensor(1.6253), tensor(1.4720), tensor(1.4725), tensor(1.5932), tensor(1.4839), tensor(1.5134), tensor(1.4674), tensor(1.4753), tensor(1.5577), tensor(1.5130), tensor(1.4826), tensor(1.5251), tensor(1.4673), tensor(1.5995), tensor(1.5356), tensor(1.4443), tensor(1.5003), tensor(1.4785), tensor(1.5119), tensor(1.5006)]
09/23/2021 14:07:11 - INFO - __main__ - interference_scores=[tensor(0.0434), tensor(-0.0289), tensor(-0.0164), tensor(-0.0118), tensor(0.0190), tensor(-0.0232), tensor(-0.0117), tensor(-0.0107), tensor(-0.0694), tensor(-0.0264), tensor(-0.0981), tensor(-0.0028), tensor(0.0291), tensor(0.0489), tensor(-0.0891), tensor(-0.0209), tensor(-0.1379), tensor(-0.0171), tensor(0.1365), tensor(-0.1275), tensor(-0.0270), tensor(0.1776), tensor(-0.0201), tensor(-0.0157), tensor(-0.0234), tensor(-0.0141), tensor(-0.1096), tensor(-0.0504), tensor(-0.0121), tensor(-0.0141), tensor(-0.0352), tensor(-0.0831), tensor(-0.0132), tensor(-0.0125), tensor(-0.0146), tensor(-0.0059), tensor(-0.0227), tensor(-0.0229), tensor(-0.0714), tensor(-0.0089), tensor(-0.0291), tensor(0.0043), tensor(-0.0187), tensor(-0.0145), tensor(-0.0371), tensor(-0.0285), tensor(-0.0567), tensor(-0.0218), tensor(-0.0121), tensor(-0.0160), tensor(-0.0133), tensor(-0.0486), tensor(-0.0404), tensor(-0.0151), tensor(-0.0078), tensor(0.0108), tensor(-0.0136), tensor(-0.0368), tensor(-0.0191), tensor(-0.0495), tensor(-0.0164), tensor(-0.0038), tensor(-0.0189), tensor(-0.0060), tensor(-0.0052), tensor(0.0164), tensor(0.2786), tensor(0.0035), tensor(-0.0188), tensor(-0.0353), tensor(-0.0128), tensor(-0.0151), tensor(0.1204), tensor(-0.0225), tensor(0.0063), tensor(-0.0360), tensor(-0.2564), tensor(-0.0315), tensor(-0.0335), tensor(-0.0247), tensor(0.0111), tensor(-0.0329), tensor(-0.0153), tensor(-0.0694), tensor(-0.0050), tensor(-0.0046), tensor(-0.0077), tensor(-0.0125), tensor(-0.0144), tensor(0.0055), tensor(-0.0134), tensor(-0.0234), tensor(-0.0188), tensor(-0.1122), tensor(0.0049), tensor(-0.0226), tensor(-0.0187), tensor(-0.0140), tensor(-0.0019), tensor(-0.0133), tensor(-0.0215), tensor(-0.0232), tensor(-0.0050), tensor(-0.2138), tensor(-0.0119), tensor(-0.0102), tensor(-0.0154), tensor(-0.0135), tensor(-0.0239), tensor(-0.0150), tensor(-0.0151), tensor(-0.0181), tensor(-0.0192), tensor(0.0118), tensor(-0.0548), tensor(-0.0241), tensor(-0.0461), tensor(-0.0098), tensor(-0.0139), tensor(0.2766), tensor(-0.0079), tensor(-0.0217), tensor(-0.0244), tensor(0.0287), tensor(-0.0453), tensor(-0.0272), tensor(-0.0526), tensor(-0.0153), tensor(-0.0263), tensor(-0.0155), tensor(0.1419), tensor(-0.0217), tensor(0.0309), tensor(-0.0190), tensor(-0.0252), tensor(-0.0567), tensor(-0.0174), tensor(-0.0182), tensor(-0.0294), tensor(-0.0344), tensor(-0.0351), tensor(-0.0193), tensor(-0.0234), tensor(-0.0510), tensor(-0.0316), tensor(-0.0251), tensor(-0.0154), tensor(-0.0480), tensor(-0.0284), tensor(-0.0317), tensor(-0.9601), tensor(-0.0191), tensor(0.0014), tensor(0.0236), tensor(-0.0250), tensor(-0.0173), tensor(0.0396), tensor(0.1700), tensor(-0.0119), tensor(-0.0105), tensor(-0.0160), tensor(-0.0199), tensor(-0.0383), tensor(0.0344), tensor(-0.0181), tensor(-0.0517), tensor(-0.0153), tensor(0.7723), tensor(-0.0211), tensor(-0.0272), tensor(-0.0597), tensor(-0.0357), tensor(-0.0196), tensor(0.0040), tensor(-0.0106), tensor(-0.0193), tensor(-0.1046), tensor(-0.0310), tensor(-0.0120), tensor(-0.0157), tensor(-0.0298), tensor(-0.0189), tensor(0.0636), tensor(-0.0165), tensor(-0.0126), tensor(0.0492), tensor(-0.0381), tensor(0.0251), tensor(-0.0323), tensor(-0.9292), tensor(-0.0255), tensor(-0.0186), tensor(-0.0759), tensor(-0.0314), tensor(-0.0156), tensor(-0.0152), tensor(-0.0245), tensor(-0.0145), tensor(-0.0251), tensor(0.0314), tensor(-0.0218), tensor(-0.0501), tensor(-0.0150), tensor(-0.0148), tensor(0.2119), tensor(-0.0361), tensor(-0.0117), tensor(-0.0230), tensor(-0.0544), tensor(-0.0368), tensor(-0.0224), tensor(-0.0261), tensor(-0.2710), tensor(-0.0112), tensor(0.0076), tensor(-0.0188), tensor(-0.0133), tensor(-0.0131), tensor(-0.0154), tensor(-0.0169), tensor(0.0933), tensor(-0.0023), tensor(-0.0912), tensor(-0.0945), tensor(-0.0346), tensor(-0.0232), tensor(-0.0123), tensor(0.2368), tensor(-0.0082), tensor(0.2065), tensor(-0.0119), tensor(-0.0078), tensor(-0.0132), tensor(-0.0259), tensor(0.0085), tensor(-0.0519), tensor(0.0135), tensor(-0.0202), tensor(-0.0210), tensor(-0.0126), tensor(-0.0096), tensor(-0.0276), tensor(-0.0087), tensor(-0.0206), tensor(-0.0444), tensor(0.0026), tensor(-0.0565), tensor(-0.0058), tensor(-0.0197), tensor(0.0999), tensor(0.0028), tensor(-0.0199), tensor(-0.0263), tensor(-0.0139), tensor(-0.0390), tensor(-0.0255)]
09/23/2021 14:07:11 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-3808', 'mrqa_triviaqa-validation-2703', 'mrqa_triviaqa-validation-4729', 'mrqa_triviaqa-validation-7512', 'mrqa_triviaqa-validation-83', 'mrqa_triviaqa-validation-298', 'mrqa_triviaqa-validation-4681', 'mrqa_squad-validation-9751', 'mrqa_naturalquestions-validation-1085', 'mrqa_squad-validation-2812', 'mrqa_naturalquestions-validation-6736', 'mrqa_triviaqa-validation-123', 'mrqa_triviaqa-validation-1521', 'mrqa_triviaqa-validation-5078', 'mrqa_squad-validation-6915', 'mrqa_triviaqa-validation-1860', 'mrqa_triviaqa-validation-4301', 'mrqa_squad-validation-3558', 'mrqa_triviaqa-validation-893', 'mrqa_triviaqa-validation-6692', 'mrqa_squad-validation-6811', 'mrqa_squad-validation-5517', 'mrqa_squad-validation-10015', 'mrqa_naturalquestions-validation-2501', 'mrqa_naturalquestions-validation-2666', 'mrqa_naturalquestions-validation-10205', 'mrqa_squad-validation-2069', 'mrqa_naturalquestions-validation-1135', 'mrqa_squad-validation-4181', 'mrqa_squad-validation-3971', 'mrqa_squad-validation-8821', 'mrqa_squad-validation-4456']
09/23/2021 14:07:11 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:07:11 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=59) .... Timecode: 18
09/23/2021 14:07:24 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:07:24 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 18
09/23/2021 14:07:27 - INFO - __main__ - After Error Fixing: 0.9375
09/23/2021 14:07:27 - INFO - __main__ - Instant Fixing Rate: 0.9259259259259259
09/23/2021 14:07:27 - INFO - __main__ - Instant Retention Rate: 0.9999999980000001
09/23/2021 14:07:29 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_019.pt.
09/23/2021 14:07:29 - INFO - __main__ - Saving the current error examples (len=27) to the memory.
09/23/2021 14:07:29 - INFO - __main__ - Current memory size: 504.
09/23/2021 14:07:29 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:07:29 - INFO - __main__ - Finished.
09/23/2021 14:07:29 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:07:29 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:07:29 - INFO - __main__ - Evaluating to get errors .... Timecode: 19
09/23/2021 14:07:31 - INFO - __main__ - Before Error Fixing: 0.21875
09/23/2021 14:07:31 - INFO - __main__ - Found 25 errors.
09/23/2021 14:07:31 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:07:31 - INFO - __main__ - Current memory size: 531.
09/23/2021 14:07:31 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:07:54 - INFO - __main__ - before_losses=[tensor(1.4922), tensor(1.5136), tensor(1.4982), tensor(1.5153), tensor(1.4831), tensor(1.9383), tensor(1.5414), tensor(1.4847), tensor(1.4980), tensor(1.5624), tensor(1.5602), tensor(1.4592), tensor(3.1966), tensor(2.2847), tensor(1.4705), tensor(1.5124), tensor(1.4770), tensor(1.5341), tensor(1.6618), tensor(1.5818), tensor(1.7225), tensor(2.7486), tensor(1.5592), tensor(1.4618), tensor(1.4726), tensor(2.1855), tensor(1.4740), tensor(1.6489), tensor(1.5427), tensor(1.5134), tensor(1.4857), tensor(1.5043), tensor(1.5137), tensor(1.5131), tensor(1.4475), tensor(1.6020), tensor(1.5054), tensor(1.5129), tensor(1.4708), tensor(1.4623), tensor(1.4866), tensor(1.4953), tensor(1.4876), tensor(2.0126), tensor(1.5920), tensor(1.8566), tensor(2.2908), tensor(1.4575), tensor(1.5342), tensor(1.4686), tensor(1.5374), tensor(1.4901), tensor(1.7311), tensor(1.4909), tensor(1.5410), tensor(2.4435), tensor(1.5405), tensor(1.5255), tensor(1.5362), tensor(1.4900), tensor(1.9550), tensor(1.5329), tensor(1.5177), tensor(1.5972), tensor(2.2201), tensor(1.4685), tensor(1.4818), tensor(1.5344), tensor(1.5066), tensor(1.5362), tensor(1.5187), tensor(1.4796), tensor(1.6917), tensor(1.5523), tensor(1.4779), tensor(1.5527), tensor(1.5397), tensor(1.4975), tensor(1.5061), tensor(1.4732), tensor(1.4757), tensor(1.5491), tensor(1.5306), tensor(1.4440), tensor(1.5649), tensor(1.5002), tensor(1.4768), tensor(1.6134), tensor(1.5514), tensor(1.5089), tensor(1.8949), tensor(1.8264), tensor(1.4482), tensor(1.4723), tensor(1.4705), tensor(1.5072), tensor(1.4649), tensor(1.4994), tensor(1.4807), tensor(2.0410), tensor(1.4692), tensor(1.4768), tensor(1.4637), tensor(1.5114), tensor(1.5104), tensor(1.4872), tensor(1.4729), tensor(1.4988), tensor(1.5763), tensor(1.6550), tensor(1.4854), tensor(1.5030), tensor(1.5549), tensor(1.9604), tensor(1.5420), tensor(1.4934), tensor(2.1833), tensor(2.6794), tensor(1.4818), tensor(1.9159), tensor(4.1120), tensor(1.5693), tensor(1.5128), tensor(1.5506), tensor(1.4772), tensor(1.4653), tensor(1.5716), tensor(1.4997), tensor(1.6244), tensor(2.0480), tensor(1.9965), tensor(1.5077), tensor(1.4972), tensor(1.5943), tensor(1.4663), tensor(1.5251), tensor(2.4179), tensor(1.4895), tensor(1.5665), tensor(1.5232), tensor(1.9028), tensor(1.4644), tensor(1.5418), tensor(1.5210), tensor(1.4529), tensor(1.4939), tensor(1.4652), tensor(1.5161), tensor(1.4844), tensor(1.6853), tensor(2.1497), tensor(1.4624), tensor(1.5001), tensor(2.4707), tensor(1.4767), tensor(1.5298), tensor(1.4508), tensor(2.0008), tensor(1.5074), tensor(1.5591), tensor(1.5358), tensor(1.5142), tensor(1.5459), tensor(1.5433), tensor(1.4681), tensor(1.4799), tensor(1.5757), tensor(1.5240), tensor(1.4824), tensor(1.5606), tensor(1.5313), tensor(1.5176), tensor(1.6361), tensor(1.5500), tensor(1.4953), tensor(1.5305), tensor(3.5286), tensor(1.4569), tensor(1.4834), tensor(1.9306), tensor(1.7022), tensor(1.4807), tensor(1.4721), tensor(2.9402), tensor(2.4425), tensor(1.5869), tensor(1.4508), tensor(1.4863), tensor(1.5587), tensor(3.0375), tensor(1.5032), tensor(1.5848), tensor(2.5135), tensor(1.5113), tensor(1.5456), tensor(1.5555), tensor(1.5311), tensor(1.5094), tensor(1.5140), tensor(1.8063), tensor(1.4891), tensor(1.4667), tensor(1.4993), tensor(1.4905), tensor(1.4974), tensor(1.4977), tensor(1.4911), tensor(1.8714), tensor(1.5268), tensor(1.5186), tensor(1.5384), tensor(2.0560), tensor(1.5355), tensor(1.4852), tensor(1.4797), tensor(1.9975), tensor(1.5904), tensor(1.4851), tensor(1.5847), tensor(1.6663), tensor(1.4768), tensor(1.5053), tensor(1.4980), tensor(1.5063), tensor(1.4962), tensor(1.7275), tensor(1.4485), tensor(1.5463), tensor(2.1258), tensor(1.5647), tensor(1.4864), tensor(1.5305), tensor(2.1838), tensor(2.3243), tensor(1.4560), tensor(1.7987), tensor(1.6034), tensor(1.4929), tensor(1.5749), tensor(1.9811), tensor(1.9812), tensor(5.1746), tensor(1.4836), tensor(1.5229), tensor(1.5340), tensor(1.7171), tensor(2.3492), tensor(1.4466), tensor(1.5661), tensor(1.4947), tensor(1.5234), tensor(1.5082), tensor(1.5195), tensor(1.4514), tensor(1.7374), tensor(1.5218)]
09/23/2021 14:07:54 - INFO - __main__ - after_losses=[tensor(1.4852), tensor(1.5525), tensor(2.1186), tensor(1.4755), tensor(1.4738), tensor(1.8035), tensor(1.4759), tensor(1.4597), tensor(1.4904), tensor(1.6031), tensor(1.5771), tensor(1.4537), tensor(3.1099), tensor(2.3567), tensor(1.4523), tensor(1.4969), tensor(1.4724), tensor(1.5087), tensor(1.7469), tensor(1.5469), tensor(1.6201), tensor(2.2504), tensor(1.5490), tensor(1.4505), tensor(1.4743), tensor(2.1202), tensor(1.4751), tensor(1.4867), tensor(1.5076), tensor(1.5051), tensor(1.4695), tensor(1.4847), tensor(1.5044), tensor(1.4997), tensor(1.4450), tensor(1.5710), tensor(1.5006), tensor(1.6668), tensor(1.4472), tensor(1.4473), tensor(1.4708), tensor(1.4681), tensor(1.4690), tensor(2.2014), tensor(1.5701), tensor(1.6101), tensor(2.0759), tensor(1.4505), tensor(1.5290), tensor(1.4588), tensor(1.7093), tensor(1.5059), tensor(2.2554), tensor(1.4742), tensor(1.5309), tensor(2.4400), tensor(1.5238), tensor(1.4948), tensor(1.5104), tensor(1.4666), tensor(1.7958), tensor(1.5168), tensor(1.5187), tensor(1.6999), tensor(2.2358), tensor(1.4494), tensor(1.4692), tensor(1.5292), tensor(1.4897), tensor(1.4936), tensor(1.5090), tensor(1.4593), tensor(1.7091), tensor(1.5289), tensor(1.4701), tensor(1.5384), tensor(1.5237), tensor(1.4827), tensor(1.4843), tensor(1.4618), tensor(1.4826), tensor(1.5318), tensor(1.8477), tensor(1.4472), tensor(1.5341), tensor(1.4831), tensor(1.4801), tensor(1.7388), tensor(1.5359), tensor(1.4977), tensor(1.7630), tensor(1.7729), tensor(1.4447), tensor(1.4585), tensor(1.4650), tensor(1.5078), tensor(1.4548), tensor(1.5137), tensor(1.4812), tensor(2.2128), tensor(1.4507), tensor(1.4611), tensor(1.4646), tensor(1.4956), tensor(1.5013), tensor(1.4724), tensor(1.4515), tensor(1.4857), tensor(1.5081), tensor(1.6524), tensor(1.4661), tensor(1.5192), tensor(1.5398), tensor(1.5072), tensor(1.5231), tensor(2.6162), tensor(1.5032), tensor(2.5712), tensor(1.4725), tensor(2.2012), tensor(3.7136), tensor(1.5473), tensor(1.4805), tensor(1.5551), tensor(1.4603), tensor(1.4556), tensor(1.5421), tensor(1.4791), tensor(1.5899), tensor(2.1641), tensor(2.0320), tensor(1.5093), tensor(1.4829), tensor(1.6154), tensor(1.4619), tensor(1.5098), tensor(2.8350), tensor(1.4750), tensor(1.4892), tensor(1.5493), tensor(2.0235), tensor(1.4587), tensor(1.5395), tensor(1.5781), tensor(1.4566), tensor(1.5144), tensor(1.4581), tensor(1.5144), tensor(1.5008), tensor(1.6212), tensor(2.1459), tensor(1.4560), tensor(1.4857), tensor(2.3555), tensor(1.4949), tensor(1.5260), tensor(1.4585), tensor(1.6025), tensor(1.4761), tensor(1.5776), tensor(1.5019), tensor(1.5063), tensor(1.5347), tensor(1.6305), tensor(1.4585), tensor(1.4567), tensor(1.4966), tensor(1.5102), tensor(1.4801), tensor(1.5215), tensor(1.5113), tensor(1.5248), tensor(1.5877), tensor(1.5336), tensor(1.5051), tensor(1.5438), tensor(3.5408), tensor(1.4499), tensor(1.4757), tensor(1.9572), tensor(1.6636), tensor(1.4740), tensor(1.4627), tensor(1.5098), tensor(2.3156), tensor(1.6191), tensor(1.4419), tensor(1.4714), tensor(1.5448), tensor(2.7447), tensor(1.4888), tensor(1.5393), tensor(2.4001), tensor(1.5200), tensor(1.5412), tensor(1.4652), tensor(1.5295), tensor(1.4942), tensor(1.4889), tensor(1.8978), tensor(1.4894), tensor(1.4602), tensor(1.5019), tensor(1.4564), tensor(1.4912), tensor(1.6185), tensor(1.4787), tensor(1.8512), tensor(1.5122), tensor(1.5260), tensor(1.4894), tensor(1.8667), tensor(1.5834), tensor(1.4801), tensor(1.4719), tensor(1.9779), tensor(1.5849), tensor(1.4826), tensor(1.6213), tensor(1.6596), tensor(1.4681), tensor(1.4871), tensor(1.4999), tensor(1.5105), tensor(1.4878), tensor(1.7358), tensor(1.4445), tensor(1.5669), tensor(1.6504), tensor(1.6739), tensor(1.4784), tensor(1.5173), tensor(2.2228), tensor(2.0368), tensor(1.4466), tensor(1.7543), tensor(1.5947), tensor(1.4754), tensor(1.5499), tensor(1.6495), tensor(1.9513), tensor(5.1396), tensor(1.4756), tensor(1.5132), tensor(1.5778), tensor(1.7970), tensor(1.9673), tensor(1.4450), tensor(1.7781), tensor(1.5058), tensor(1.5062), tensor(1.5132), tensor(1.5043), tensor(1.4491), tensor(1.8023), tensor(1.5044)]
09/23/2021 14:07:54 - INFO - __main__ - interference_scores=[tensor(-0.0069), tensor(0.0389), tensor(0.6204), tensor(-0.0398), tensor(-0.0093), tensor(-0.1347), tensor(-0.0655), tensor(-0.0250), tensor(-0.0076), tensor(0.0408), tensor(0.0169), tensor(-0.0055), tensor(-0.0867), tensor(0.0720), tensor(-0.0182), tensor(-0.0155), tensor(-0.0046), tensor(-0.0254), tensor(0.0851), tensor(-0.0348), tensor(-0.1024), tensor(-0.4982), tensor(-0.0102), tensor(-0.0114), tensor(0.0016), tensor(-0.0653), tensor(0.0011), tensor(-0.1623), tensor(-0.0351), tensor(-0.0084), tensor(-0.0162), tensor(-0.0196), tensor(-0.0093), tensor(-0.0134), tensor(-0.0025), tensor(-0.0310), tensor(-0.0049), tensor(0.1539), tensor(-0.0236), tensor(-0.0150), tensor(-0.0158), tensor(-0.0272), tensor(-0.0186), tensor(0.1888), tensor(-0.0219), tensor(-0.2465), tensor(-0.2149), tensor(-0.0069), tensor(-0.0052), tensor(-0.0098), tensor(0.1720), tensor(0.0158), tensor(0.5243), tensor(-0.0168), tensor(-0.0100), tensor(-0.0035), tensor(-0.0167), tensor(-0.0307), tensor(-0.0258), tensor(-0.0234), tensor(-0.1592), tensor(-0.0161), tensor(0.0010), tensor(0.1027), tensor(0.0157), tensor(-0.0190), tensor(-0.0125), tensor(-0.0053), tensor(-0.0170), tensor(-0.0426), tensor(-0.0097), tensor(-0.0204), tensor(0.0174), tensor(-0.0235), tensor(-0.0078), tensor(-0.0143), tensor(-0.0160), tensor(-0.0148), tensor(-0.0218), tensor(-0.0114), tensor(0.0069), tensor(-0.0173), tensor(0.3170), tensor(0.0031), tensor(-0.0308), tensor(-0.0171), tensor(0.0033), tensor(0.1253), tensor(-0.0155), tensor(-0.0112), tensor(-0.1319), tensor(-0.0536), tensor(-0.0034), tensor(-0.0139), tensor(-0.0055), tensor(0.0006), tensor(-0.0101), tensor(0.0143), tensor(0.0004), tensor(0.1719), tensor(-0.0186), tensor(-0.0157), tensor(0.0008), tensor(-0.0158), tensor(-0.0091), tensor(-0.0148), tensor(-0.0215), tensor(-0.0132), tensor(-0.0682), tensor(-0.0026), tensor(-0.0194), tensor(0.0162), tensor(-0.0151), tensor(-0.4532), tensor(-0.0189), tensor(1.1228), tensor(-0.6801), tensor(-0.1082), tensor(-0.0092), tensor(0.2853), tensor(-0.3984), tensor(-0.0220), tensor(-0.0324), tensor(0.0045), tensor(-0.0170), tensor(-0.0097), tensor(-0.0295), tensor(-0.0206), tensor(-0.0345), tensor(0.1161), tensor(0.0355), tensor(0.0016), tensor(-0.0143), tensor(0.0211), tensor(-0.0044), tensor(-0.0153), tensor(0.4172), tensor(-0.0145), tensor(-0.0772), tensor(0.0262), tensor(0.1208), tensor(-0.0057), tensor(-0.0022), tensor(0.0571), tensor(0.0037), tensor(0.0205), tensor(-0.0071), tensor(-0.0017), tensor(0.0164), tensor(-0.0641), tensor(-0.0038), tensor(-0.0064), tensor(-0.0144), tensor(-0.1152), tensor(0.0182), tensor(-0.0038), tensor(0.0077), tensor(-0.3983), tensor(-0.0313), tensor(0.0185), tensor(-0.0339), tensor(-0.0079), tensor(-0.0112), tensor(0.0872), tensor(-0.0096), tensor(-0.0232), tensor(-0.0791), tensor(-0.0139), tensor(-0.0024), tensor(-0.0392), tensor(-0.0200), tensor(0.0072), tensor(-0.0484), tensor(-0.0164), tensor(0.0098), tensor(0.0133), tensor(0.0122), tensor(-0.0070), tensor(-0.0077), tensor(0.0266), tensor(-0.0386), tensor(-0.0068), tensor(-0.0094), tensor(-1.4304), tensor(-0.1269), tensor(0.0323), tensor(-0.0089), tensor(-0.0149), tensor(-0.0139), tensor(-0.2928), tensor(-0.0144), tensor(-0.0455), tensor(-0.1135), tensor(0.0087), tensor(-0.0043), tensor(-0.0903), tensor(-0.0016), tensor(-0.0153), tensor(-0.0251), tensor(0.0915), tensor(0.0004), tensor(-0.0065), tensor(0.0026), tensor(-0.0341), tensor(-0.0062), tensor(0.1208), tensor(-0.0125), tensor(-0.0202), tensor(-0.0147), tensor(0.0073), tensor(-0.0490), tensor(-0.1892), tensor(0.0479), tensor(-0.0051), tensor(-0.0078), tensor(-0.0196), tensor(-0.0055), tensor(-0.0026), tensor(0.0366), tensor(-0.0067), tensor(-0.0086), tensor(-0.0182), tensor(0.0019), tensor(0.0042), tensor(-0.0084), tensor(0.0082), tensor(-0.0040), tensor(0.0206), tensor(-0.4754), tensor(0.1092), tensor(-0.0080), tensor(-0.0132), tensor(0.0390), tensor(-0.2875), tensor(-0.0094), tensor(-0.0444), tensor(-0.0087), tensor(-0.0175), tensor(-0.0250), tensor(-0.3316), tensor(-0.0299), tensor(-0.0350), tensor(-0.0081), tensor(-0.0096), tensor(0.0438), tensor(0.0799), tensor(-0.3819), tensor(-0.0016), tensor(0.2121), tensor(0.0111), tensor(-0.0173), tensor(0.0050), tensor(-0.0152), tensor(-0.0023), tensor(0.0649), tensor(-0.0173)]
09/23/2021 14:07:54 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-1914', 'mrqa_squad-validation-4181', 'mrqa_triviaqa-validation-4729', 'mrqa_naturalquestions-validation-2666', 'mrqa_squad-validation-8821', 'mrqa_squad-validation-5517', 'mrqa_naturalquestions-validation-368', 'mrqa_triviaqa-validation-7300', 'mrqa_squad-validation-9793', 'mrqa_triviaqa-validation-1616', 'mrqa_naturalquestions-validation-4513', 'mrqa_hotpotqa-validation-2382', 'mrqa_squad-validation-10015', 'mrqa_triviaqa-validation-5877', 'mrqa_squad-validation-7447', 'mrqa_triviaqa-validation-671', 'mrqa_triviaqa-validation-6389', 'mrqa_triviaqa-validation-5997', 'mrqa_hotpotqa-validation-501', 'mrqa_triviaqa-validation-866', 'mrqa_triviaqa-validation-7512', 'mrqa_triviaqa-validation-6639', 'mrqa_squad-validation-7042', 'mrqa_naturalquestions-validation-3309', 'mrqa_naturalquestions-validation-1294', 'mrqa_naturalquestions-validation-4309', 'mrqa_squad-validation-7296', 'mrqa_squad-validation-8259', 'mrqa_naturalquestions-validation-1301', 'mrqa_hotpotqa-validation-335', 'mrqa_triviaqa-validation-6385', 'mrqa_triviaqa-validation-83']
09/23/2021 14:07:54 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:07:54 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=57) .... Timecode: 19
09/23/2021 14:08:08 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:08:08 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 19
09/23/2021 14:08:10 - INFO - __main__ - After Error Fixing: 0.875
09/23/2021 14:08:10 - INFO - __main__ - Instant Fixing Rate: 0.96
09/23/2021 14:08:10 - INFO - __main__ - Instant Retention Rate: 0.571428570612245
09/23/2021 14:08:12 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_020.pt.
09/23/2021 14:08:12 - INFO - __main__ - Saving the current error examples (len=25) to the memory.
09/23/2021 14:08:12 - INFO - __main__ - Current memory size: 531.
09/23/2021 14:08:12 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:08:12 - INFO - __main__ - Finished.
09/23/2021 14:08:12 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:08:12 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:08:12 - INFO - __main__ - Evaluating to get errors .... Timecode: 20
09/23/2021 14:08:15 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 14:08:15 - INFO - __main__ - Found 26 errors.
09/23/2021 14:08:15 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:08:15 - INFO - __main__ - Current memory size: 556.
09/23/2021 14:08:15 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:08:38 - INFO - __main__ - before_losses=[tensor(1.4785), tensor(1.4464), tensor(1.4774), tensor(1.4760), tensor(1.9401), tensor(1.6646), tensor(1.4637), tensor(1.6712), tensor(1.6458), tensor(1.4784), tensor(1.4798), tensor(1.4757), tensor(1.5037), tensor(1.6470), tensor(1.4731), tensor(1.5329), tensor(1.4757), tensor(1.5382), tensor(1.9752), tensor(5.0859), tensor(1.5020), tensor(1.5003), tensor(1.6071), tensor(1.5004), tensor(1.4581), tensor(1.4511), tensor(1.6691), tensor(2.2686), tensor(1.4957), tensor(1.4965), tensor(1.5227), tensor(1.4659), tensor(1.4693), tensor(2.1427), tensor(1.4885), tensor(1.5046), tensor(1.4481), tensor(1.4876), tensor(1.5299), tensor(1.4875), tensor(1.5134), tensor(1.4859), tensor(1.4756), tensor(1.4971), tensor(1.4826), tensor(2.6852), tensor(1.5190), tensor(2.1910), tensor(2.0785), tensor(1.5478), tensor(1.9314), tensor(1.5676), tensor(1.4998), tensor(1.5320), tensor(1.5612), tensor(1.5029), tensor(1.5013), tensor(1.5074), tensor(1.4688), tensor(1.8324), tensor(1.6239), tensor(1.5493), tensor(1.4579), tensor(1.4755), tensor(1.5086), tensor(1.4708), tensor(1.4726), tensor(2.6297), tensor(1.4529), tensor(1.7636), tensor(1.4512), tensor(1.4665), tensor(1.8481), tensor(3.1260), tensor(1.4505), tensor(1.4623), tensor(2.4056), tensor(1.5405), tensor(1.5220), tensor(1.4950), tensor(2.1667), tensor(1.6302), tensor(1.5424), tensor(1.5161), tensor(1.8042), tensor(1.5141), tensor(1.7860), tensor(1.5330), tensor(1.6225), tensor(1.5057), tensor(1.4582), tensor(1.8832), tensor(1.4370), tensor(1.8704), tensor(1.6990), tensor(1.4884), tensor(1.4963), tensor(1.5863), tensor(1.5799), tensor(1.4684), tensor(1.4472), tensor(1.5049), tensor(1.4920), tensor(1.5115), tensor(1.4628), tensor(4.4259), tensor(1.5108), tensor(1.4625), tensor(1.5252), tensor(1.4778), tensor(2.1031), tensor(1.4931), tensor(1.5036), tensor(1.5007), tensor(1.4570), tensor(1.8153), tensor(1.4865), tensor(1.5094), tensor(2.5614), tensor(1.5220), tensor(1.8951), tensor(1.4938), tensor(1.6926), tensor(1.4907), tensor(1.5285), tensor(1.6433), tensor(1.9305), tensor(1.4758), tensor(1.6684), tensor(1.4796), tensor(1.5290), tensor(1.5288), tensor(1.5174), tensor(2.6866), tensor(1.4952), tensor(2.2230), tensor(1.4875), tensor(1.5255), tensor(1.5159), tensor(1.4863), tensor(1.5205), tensor(1.5353), tensor(1.7054), tensor(1.5451), tensor(1.5156), tensor(1.8823), tensor(1.4738), tensor(1.6114), tensor(1.4442), tensor(1.5499), tensor(1.6024), tensor(1.5161), tensor(1.6658), tensor(1.4862), tensor(1.8756), tensor(1.4809), tensor(1.5074), tensor(2.1511), tensor(1.7597), tensor(1.5791), tensor(1.5498), tensor(1.4811), tensor(1.4769), tensor(1.5199), tensor(1.5237), tensor(1.4653), tensor(1.4494), tensor(1.4825), tensor(1.4873), tensor(1.4647), tensor(2.3756), tensor(1.5147), tensor(1.5145), tensor(1.5104), tensor(1.5011), tensor(1.4774), tensor(1.4522), tensor(1.5625), tensor(1.5293), tensor(1.4826), tensor(1.4749), tensor(1.5230), tensor(1.5722), tensor(1.5206), tensor(1.6371), tensor(1.5190), tensor(1.5471), tensor(1.4708), tensor(1.4909), tensor(1.4753), tensor(1.5400), tensor(1.5772), tensor(2.6972), tensor(1.9866), tensor(1.6075), tensor(1.4756), tensor(1.4738), tensor(1.4751), tensor(1.4731), tensor(1.8181), tensor(1.5466), tensor(1.4765), tensor(1.4626), tensor(1.5437), tensor(1.4494), tensor(1.4503), tensor(2.2133), tensor(1.4937), tensor(1.4672), tensor(1.4758), tensor(1.7070), tensor(1.4911), tensor(1.5655), tensor(2.4144), tensor(1.4909), tensor(1.4891), tensor(1.4485), tensor(1.4854), tensor(1.4947), tensor(1.4888), tensor(1.8380), tensor(1.4683), tensor(2.1970), tensor(1.5974), tensor(1.4641), tensor(1.4960), tensor(1.4612), tensor(1.4729), tensor(1.6940), tensor(1.5819), tensor(1.5234), tensor(1.6993), tensor(1.5032), tensor(1.5047), tensor(1.4822), tensor(3.6065), tensor(1.6749), tensor(1.5291), tensor(1.4742), tensor(1.5157), tensor(1.5680), tensor(1.5249), tensor(1.4719), tensor(1.5333), tensor(1.5075), tensor(1.4614), tensor(1.4586), tensor(1.4688), tensor(1.4785), tensor(1.4792), tensor(1.4784), tensor(1.5151), tensor(1.4824), tensor(1.8331), tensor(1.5133), tensor(1.4818)]
09/23/2021 14:08:38 - INFO - __main__ - after_losses=[tensor(1.4790), tensor(1.4473), tensor(1.4921), tensor(1.4627), tensor(1.4705), tensor(1.6732), tensor(1.4860), tensor(1.7539), tensor(1.6787), tensor(1.5383), tensor(1.4767), tensor(1.4748), tensor(1.5057), tensor(1.7099), tensor(1.4849), tensor(1.4937), tensor(1.4653), tensor(1.5268), tensor(2.0951), tensor(4.8300), tensor(1.4951), tensor(1.5292), tensor(1.7522), tensor(1.5002), tensor(1.4682), tensor(1.4547), tensor(1.6699), tensor(2.3599), tensor(1.4932), tensor(1.4898), tensor(1.4821), tensor(1.4579), tensor(1.4743), tensor(1.9350), tensor(1.4825), tensor(1.5102), tensor(1.4457), tensor(1.4878), tensor(1.5773), tensor(1.4809), tensor(1.4943), tensor(1.4911), tensor(1.4782), tensor(1.4971), tensor(1.4757), tensor(2.6265), tensor(1.5051), tensor(2.2860), tensor(2.2149), tensor(1.6117), tensor(2.3518), tensor(1.5618), tensor(1.4926), tensor(1.5399), tensor(1.5389), tensor(1.4935), tensor(1.5031), tensor(1.5355), tensor(1.4666), tensor(1.8274), tensor(1.5911), tensor(1.5519), tensor(1.4545), tensor(1.4784), tensor(1.4919), tensor(1.4680), tensor(1.4899), tensor(2.0962), tensor(1.4493), tensor(1.8105), tensor(1.4498), tensor(1.4629), tensor(2.2238), tensor(3.0513), tensor(1.4465), tensor(1.4652), tensor(2.4530), tensor(1.5368), tensor(1.5158), tensor(1.4925), tensor(2.1614), tensor(1.5747), tensor(1.5133), tensor(1.5231), tensor(1.7629), tensor(1.4971), tensor(2.2704), tensor(1.5028), tensor(1.5910), tensor(1.5189), tensor(1.4489), tensor(2.0425), tensor(1.4353), tensor(1.8417), tensor(1.5967), tensor(1.4892), tensor(1.4908), tensor(1.5644), tensor(1.5574), tensor(1.4612), tensor(1.4434), tensor(1.5518), tensor(1.4926), tensor(1.5039), tensor(1.4549), tensor(4.1711), tensor(1.5105), tensor(1.4811), tensor(1.5121), tensor(1.4732), tensor(2.4460), tensor(1.5014), tensor(1.4961), tensor(1.4982), tensor(1.4553), tensor(1.7598), tensor(1.4766), tensor(1.5000), tensor(2.1854), tensor(1.5232), tensor(1.8574), tensor(1.4824), tensor(1.5581), tensor(1.4861), tensor(1.5468), tensor(1.6133), tensor(1.9592), tensor(1.4647), tensor(1.5189), tensor(1.4851), tensor(1.5182), tensor(1.8882), tensor(1.5104), tensor(1.6592), tensor(1.4846), tensor(2.2927), tensor(1.4732), tensor(1.5108), tensor(1.5209), tensor(1.4851), tensor(1.5308), tensor(1.5238), tensor(1.6553), tensor(1.5439), tensor(1.5150), tensor(1.8330), tensor(1.4757), tensor(1.6602), tensor(1.4460), tensor(1.5441), tensor(1.7150), tensor(1.5278), tensor(1.4940), tensor(1.5060), tensor(2.3150), tensor(1.5112), tensor(1.5061), tensor(2.0490), tensor(1.6470), tensor(1.6294), tensor(1.5482), tensor(1.4834), tensor(1.4756), tensor(1.5093), tensor(1.5199), tensor(1.4741), tensor(1.4497), tensor(1.4758), tensor(1.4924), tensor(1.4547), tensor(2.4984), tensor(1.5095), tensor(1.5202), tensor(1.5235), tensor(1.5809), tensor(1.4753), tensor(1.4462), tensor(1.5739), tensor(1.5245), tensor(1.4748), tensor(1.4726), tensor(1.5142), tensor(1.6088), tensor(1.5198), tensor(1.5979), tensor(1.5150), tensor(1.5200), tensor(1.4886), tensor(1.5087), tensor(1.4816), tensor(1.5154), tensor(1.5453), tensor(2.7461), tensor(1.9806), tensor(1.5609), tensor(1.4793), tensor(1.4548), tensor(1.4907), tensor(1.4813), tensor(1.7580), tensor(1.7062), tensor(1.4682), tensor(1.4627), tensor(1.5331), tensor(1.4469), tensor(1.4491), tensor(2.4271), tensor(1.4917), tensor(1.4601), tensor(1.4756), tensor(1.7021), tensor(1.4830), tensor(1.5416), tensor(2.3090), tensor(1.5092), tensor(1.5059), tensor(1.4447), tensor(1.4822), tensor(1.5013), tensor(1.4772), tensor(1.8652), tensor(1.4725), tensor(2.2323), tensor(1.6488), tensor(1.4681), tensor(1.4885), tensor(1.4586), tensor(1.4766), tensor(1.7492), tensor(1.5389), tensor(1.5786), tensor(1.7443), tensor(1.4971), tensor(1.4998), tensor(1.4813), tensor(3.5745), tensor(1.6807), tensor(1.4902), tensor(1.4643), tensor(1.5137), tensor(1.5886), tensor(1.5037), tensor(1.4688), tensor(1.5204), tensor(1.5080), tensor(1.4601), tensor(1.4561), tensor(1.4691), tensor(1.4838), tensor(1.4684), tensor(1.4743), tensor(1.5089), tensor(1.4812), tensor(1.6870), tensor(1.4953), tensor(1.5046)]
09/23/2021 14:08:38 - INFO - __main__ - interference_scores=[tensor(0.0005), tensor(0.0009), tensor(0.0147), tensor(-0.0133), tensor(-0.4696), tensor(0.0086), tensor(0.0223), tensor(0.0827), tensor(0.0329), tensor(0.0599), tensor(-0.0030), tensor(-0.0009), tensor(0.0020), tensor(0.0629), tensor(0.0118), tensor(-0.0393), tensor(-0.0104), tensor(-0.0113), tensor(0.1199), tensor(-0.2559), tensor(-0.0069), tensor(0.0289), tensor(0.1451), tensor(-0.0002), tensor(0.0101), tensor(0.0036), tensor(0.0008), tensor(0.0913), tensor(-0.0025), tensor(-0.0067), tensor(-0.0406), tensor(-0.0080), tensor(0.0050), tensor(-0.2077), tensor(-0.0060), tensor(0.0057), tensor(-0.0024), tensor(0.0001), tensor(0.0474), tensor(-0.0066), tensor(-0.0191), tensor(0.0051), tensor(0.0026), tensor(2.3842e-07), tensor(-0.0069), tensor(-0.0587), tensor(-0.0139), tensor(0.0949), tensor(0.1363), tensor(0.0639), tensor(0.4204), tensor(-0.0057), tensor(-0.0072), tensor(0.0078), tensor(-0.0223), tensor(-0.0094), tensor(0.0018), tensor(0.0281), tensor(-0.0022), tensor(-0.0050), tensor(-0.0328), tensor(0.0026), tensor(-0.0034), tensor(0.0029), tensor(-0.0167), tensor(-0.0028), tensor(0.0173), tensor(-0.5335), tensor(-0.0036), tensor(0.0469), tensor(-0.0014), tensor(-0.0036), tensor(0.3757), tensor(-0.0746), tensor(-0.0039), tensor(0.0029), tensor(0.0474), tensor(-0.0037), tensor(-0.0063), tensor(-0.0025), tensor(-0.0053), tensor(-0.0555), tensor(-0.0292), tensor(0.0070), tensor(-0.0413), tensor(-0.0170), tensor(0.4844), tensor(-0.0302), tensor(-0.0314), tensor(0.0131), tensor(-0.0093), tensor(0.1593), tensor(-0.0017), tensor(-0.0287), tensor(-0.1023), tensor(0.0008), tensor(-0.0055), tensor(-0.0219), tensor(-0.0225), tensor(-0.0072), tensor(-0.0038), tensor(0.0469), tensor(0.0006), tensor(-0.0076), tensor(-0.0079), tensor(-0.2548), tensor(-0.0004), tensor(0.0187), tensor(-0.0131), tensor(-0.0046), tensor(0.3429), tensor(0.0083), tensor(-0.0074), tensor(-0.0025), tensor(-0.0017), tensor(-0.0555), tensor(-0.0099), tensor(-0.0094), tensor(-0.3760), tensor(0.0013), tensor(-0.0377), tensor(-0.0114), tensor(-0.1344), tensor(-0.0046), tensor(0.0184), tensor(-0.0299), tensor(0.0287), tensor(-0.0111), tensor(-0.1495), tensor(0.0056), tensor(-0.0108), tensor(0.3595), tensor(-0.0071), tensor(-1.0274), tensor(-0.0106), tensor(0.0697), tensor(-0.0143), tensor(-0.0147), tensor(0.0050), tensor(-0.0013), tensor(0.0103), tensor(-0.0115), tensor(-0.0501), tensor(-0.0011), tensor(-0.0006), tensor(-0.0493), tensor(0.0019), tensor(0.0487), tensor(0.0018), tensor(-0.0058), tensor(0.1127), tensor(0.0117), tensor(-0.1719), tensor(0.0198), tensor(0.4394), tensor(0.0303), tensor(-0.0012), tensor(-0.1021), tensor(-0.1127), tensor(0.0503), tensor(-0.0016), tensor(0.0023), tensor(-0.0013), tensor(-0.0107), tensor(-0.0038), tensor(0.0088), tensor(0.0003), tensor(-0.0067), tensor(0.0051), tensor(-0.0101), tensor(0.1228), tensor(-0.0053), tensor(0.0057), tensor(0.0132), tensor(0.0798), tensor(-0.0021), tensor(-0.0061), tensor(0.0114), tensor(-0.0048), tensor(-0.0078), tensor(-0.0023), tensor(-0.0088), tensor(0.0366), tensor(-0.0007), tensor(-0.0392), tensor(-0.0040), tensor(-0.0272), tensor(0.0178), tensor(0.0178), tensor(0.0063), tensor(-0.0246), tensor(-0.0319), tensor(0.0489), tensor(-0.0060), tensor(-0.0466), tensor(0.0037), tensor(-0.0191), tensor(0.0156), tensor(0.0082), tensor(-0.0601), tensor(0.1596), tensor(-0.0083), tensor(8.4043e-05), tensor(-0.0106), tensor(-0.0025), tensor(-0.0011), tensor(0.2138), tensor(-0.0020), tensor(-0.0072), tensor(-0.0002), tensor(-0.0048), tensor(-0.0081), tensor(-0.0239), tensor(-0.1055), tensor(0.0184), tensor(0.0168), tensor(-0.0037), tensor(-0.0032), tensor(0.0066), tensor(-0.0116), tensor(0.0273), tensor(0.0042), tensor(0.0353), tensor(0.0514), tensor(0.0041), tensor(-0.0076), tensor(-0.0026), tensor(0.0037), tensor(0.0552), tensor(-0.0430), tensor(0.0552), tensor(0.0450), tensor(-0.0061), tensor(-0.0049), tensor(-0.0009), tensor(-0.0320), tensor(0.0058), tensor(-0.0390), tensor(-0.0099), tensor(-0.0020), tensor(0.0206), tensor(-0.0212), tensor(-0.0031), tensor(-0.0129), tensor(0.0005), tensor(-0.0013), tensor(-0.0025), tensor(0.0003), tensor(0.0053), tensor(-0.0108), tensor(-0.0040), tensor(-0.0063), tensor(-0.0012), tensor(-0.1461), tensor(-0.0180), tensor(0.0228)]
09/23/2021 14:08:38 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-7512', 'mrqa_triviaqa-validation-866', 'mrqa_squad-validation-10369', 'mrqa_hotpotqa-validation-3757', 'mrqa_triviaqa-validation-7100', 'mrqa_squad-validation-6947', 'mrqa_triviaqa-validation-5261', 'mrqa_triviaqa-validation-3901', 'mrqa_triviaqa-validation-3071', 'mrqa_triviaqa-validation-6639', 'mrqa_squad-validation-2757', 'mrqa_triviaqa-validation-1671', 'mrqa_triviaqa-validation-314', 'mrqa_triviaqa-validation-123', 'mrqa_naturalquestions-validation-5437', 'mrqa_triviaqa-validation-7703', 'mrqa_triviaqa-validation-2715', 'mrqa_hotpotqa-validation-3921', 'mrqa_squad-validation-7799', 'mrqa_hotpotqa-validation-1142', 'mrqa_triviaqa-validation-3915', 'mrqa_naturalquestions-validation-3951', 'mrqa_triviaqa-validation-2530', 'mrqa_triviaqa-validation-893', 'mrqa_triviaqa-validation-93', 'mrqa_squad-validation-1374', 'mrqa_naturalquestions-validation-3208', 'mrqa_naturalquestions-validation-9031', 'mrqa_triviaqa-validation-316', 'mrqa_triviaqa-validation-1588', 'mrqa_triviaqa-validation-6385', 'mrqa_triviaqa-validation-1521']
09/23/2021 14:08:38 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:08:38 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 20
09/23/2021 14:08:51 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:08:51 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 20
09/23/2021 14:08:55 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 14:08:55 - INFO - __main__ - Instant Fixing Rate: 0.9230769230769231
09/23/2021 14:08:55 - INFO - __main__ - Instant Retention Rate: 0.8333333319444445
09/23/2021 14:08:56 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_021.pt.
09/23/2021 14:08:57 - INFO - __main__ - Saving the current error examples (len=26) to the memory.
09/23/2021 14:08:57 - INFO - __main__ - Current memory size: 556.
09/23/2021 14:08:57 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:08:57 - INFO - __main__ - Finished.
09/23/2021 14:08:57 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:08:57 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:08:57 - INFO - __main__ - Evaluating to get errors .... Timecode: 21
09/23/2021 14:09:00 - INFO - __main__ - Before Error Fixing: 0.125
09/23/2021 14:09:00 - INFO - __main__ - Found 28 errors.
09/23/2021 14:09:00 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:09:00 - INFO - __main__ - Current memory size: 582.
09/23/2021 14:09:00 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:09:23 - INFO - __main__ - before_losses=[tensor(1.5118), tensor(1.6536), tensor(1.5347), tensor(2.2770), tensor(2.0676), tensor(1.5231), tensor(1.5154), tensor(1.4792), tensor(1.4669), tensor(1.8359), tensor(1.5069), tensor(1.5440), tensor(1.7827), tensor(3.4709), tensor(1.5101), tensor(1.5395), tensor(1.5060), tensor(1.5349), tensor(1.7027), tensor(1.4795), tensor(1.5576), tensor(1.4864), tensor(1.5719), tensor(1.4948), tensor(1.5871), tensor(1.4858), tensor(1.4990), tensor(1.4890), tensor(1.4771), tensor(1.5886), tensor(1.5172), tensor(1.4838), tensor(1.4871), tensor(1.5181), tensor(1.5296), tensor(1.4705), tensor(1.5162), tensor(1.5706), tensor(1.4723), tensor(1.4838), tensor(2.3520), tensor(1.9198), tensor(2.5513), tensor(1.5823), tensor(1.4563), tensor(1.4630), tensor(1.4586), tensor(1.5165), tensor(1.7801), tensor(1.5399), tensor(1.5392), tensor(2.5621), tensor(1.4755), tensor(1.4769), tensor(1.5774), tensor(1.5062), tensor(1.5902), tensor(1.5008), tensor(1.4835), tensor(1.5942), tensor(1.9749), tensor(1.5864), tensor(1.6300), tensor(1.5079), tensor(1.5297), tensor(1.5423), tensor(1.5307), tensor(1.4977), tensor(1.5259), tensor(1.5927), tensor(1.4860), tensor(1.4759), tensor(1.5474), tensor(1.4855), tensor(1.5153), tensor(1.5195), tensor(1.5062), tensor(3.5398), tensor(1.4798), tensor(1.4851), tensor(1.4648), tensor(1.4646), tensor(1.4779), tensor(2.3226), tensor(1.9031), tensor(1.5259), tensor(3.0818), tensor(1.5155), tensor(1.4651), tensor(1.4727), tensor(1.7824), tensor(1.5089), tensor(1.5529), tensor(1.4868), tensor(1.4480), tensor(1.4931), tensor(1.5824), tensor(1.6106), tensor(1.5435), tensor(1.6251), tensor(1.4688), tensor(1.5073), tensor(2.5799), tensor(1.4855), tensor(1.5096), tensor(1.4922), tensor(1.4802), tensor(1.5453), tensor(1.4615), tensor(3.1166), tensor(1.6761), tensor(1.5742), tensor(1.5257), tensor(1.5333), tensor(2.7287), tensor(1.5029), tensor(1.5095), tensor(1.4946), tensor(1.4855), tensor(2.2539), tensor(2.8557), tensor(1.4763), tensor(1.4968), tensor(1.6886), tensor(1.5865), tensor(1.4962), tensor(1.5122), tensor(1.4730), tensor(1.5244), tensor(1.5630), tensor(1.4640), tensor(1.4935), tensor(1.5367), tensor(1.4970), tensor(1.5648), tensor(1.9218), tensor(1.4753), tensor(2.1749), tensor(1.4887), tensor(1.5816), tensor(1.4890), tensor(1.5242), tensor(1.4854), tensor(1.6154), tensor(1.7351), tensor(1.6613), tensor(1.5141), tensor(1.5231), tensor(2.5004), tensor(1.5153), tensor(1.5022), tensor(1.4859), tensor(1.4443), tensor(1.4832), tensor(1.5461), tensor(1.8967), tensor(1.4883), tensor(1.4796), tensor(1.4835), tensor(1.5442), tensor(1.5418), tensor(1.4806), tensor(4.9207), tensor(1.6899), tensor(1.4647), tensor(1.5283), tensor(1.4891), tensor(1.5057), tensor(1.5742), tensor(1.4756), tensor(1.4894), tensor(1.5006), tensor(1.9565), tensor(1.4873), tensor(1.5404), tensor(1.5218), tensor(1.5195), tensor(1.4454), tensor(1.4692), tensor(1.4939), tensor(1.4649), tensor(1.8249), tensor(1.7594), tensor(1.5101), tensor(1.4758), tensor(1.9392), tensor(1.5343), tensor(1.5182), tensor(1.5634), tensor(1.4722), tensor(1.4820), tensor(1.5029), tensor(1.5818), tensor(1.4959), tensor(1.4649), tensor(1.4687), tensor(1.5121), tensor(1.5160), tensor(1.4985), tensor(1.5370), tensor(1.4570), tensor(1.8207), tensor(1.4776), tensor(1.4774), tensor(1.4798), tensor(1.4905), tensor(1.4925), tensor(1.4812), tensor(1.6483), tensor(3.7429), tensor(1.4785), tensor(1.7434), tensor(1.4642), tensor(1.5297), tensor(1.4950), tensor(1.6076), tensor(1.4468), tensor(1.4727), tensor(1.4834), tensor(1.5222), tensor(1.4833), tensor(4.3573), tensor(1.4684), tensor(1.4741), tensor(1.5064), tensor(1.8079), tensor(1.5088), tensor(1.5456), tensor(1.4704), tensor(1.5934), tensor(1.4841), tensor(1.4840), tensor(1.4762), tensor(1.5061), tensor(1.4883), tensor(1.4564), tensor(1.5629), tensor(1.5459), tensor(1.4851), tensor(1.5021), tensor(1.5828), tensor(1.4827), tensor(1.7789), tensor(1.7867), tensor(1.5064), tensor(1.5077), tensor(1.6073), tensor(1.5023), tensor(1.4636), tensor(1.4677), tensor(1.5522), tensor(1.4503), tensor(1.7211), tensor(1.5310), tensor(1.4843), tensor(1.4985)]
09/23/2021 14:09:23 - INFO - __main__ - after_losses=[tensor(1.5206), tensor(1.6290), tensor(1.5354), tensor(2.4130), tensor(2.0640), tensor(1.5254), tensor(1.4919), tensor(1.4747), tensor(1.4586), tensor(1.7625), tensor(1.4731), tensor(1.5243), tensor(1.8271), tensor(1.6892), tensor(1.4977), tensor(2.2853), tensor(1.6470), tensor(1.5009), tensor(1.6838), tensor(1.4766), tensor(1.5786), tensor(1.4851), tensor(1.6056), tensor(1.4767), tensor(1.6057), tensor(1.4882), tensor(1.5058), tensor(1.4745), tensor(1.4700), tensor(1.5700), tensor(1.5204), tensor(1.5022), tensor(1.5050), tensor(1.5091), tensor(1.5080), tensor(1.4727), tensor(1.4993), tensor(1.5260), tensor(1.4654), tensor(1.4654), tensor(1.8526), tensor(1.9011), tensor(2.5450), tensor(1.5927), tensor(1.4560), tensor(1.4901), tensor(1.4796), tensor(1.4965), tensor(1.5841), tensor(1.5161), tensor(1.5213), tensor(2.6257), tensor(1.4622), tensor(1.4810), tensor(1.6521), tensor(1.5047), tensor(1.5671), tensor(1.4944), tensor(1.4728), tensor(1.5292), tensor(1.9700), tensor(1.7011), tensor(1.4855), tensor(1.5129), tensor(1.5124), tensor(1.5346), tensor(1.5262), tensor(1.4766), tensor(1.5204), tensor(1.5932), tensor(1.4836), tensor(1.4630), tensor(1.5392), tensor(1.4704), tensor(1.5042), tensor(1.5146), tensor(1.5010), tensor(2.8583), tensor(1.4764), tensor(1.4756), tensor(1.4566), tensor(1.4605), tensor(1.4676), tensor(2.7316), tensor(1.7303), tensor(1.5171), tensor(2.9249), tensor(1.5038), tensor(1.4597), tensor(1.4704), tensor(1.5024), tensor(1.4954), tensor(1.4990), tensor(1.4693), tensor(1.4432), tensor(1.4846), tensor(1.5826), tensor(1.9824), tensor(1.5256), tensor(1.7654), tensor(1.4635), tensor(1.5169), tensor(2.4293), tensor(1.4751), tensor(1.4965), tensor(1.4731), tensor(1.4749), tensor(1.5106), tensor(1.4585), tensor(3.0463), tensor(1.7005), tensor(1.5864), tensor(1.5010), tensor(1.5150), tensor(2.8593), tensor(1.4889), tensor(1.4987), tensor(1.5597), tensor(1.4673), tensor(2.2624), tensor(2.5078), tensor(1.5204), tensor(1.4989), tensor(2.7615), tensor(1.7811), tensor(1.4855), tensor(1.5165), tensor(1.4679), tensor(1.5469), tensor(1.6217), tensor(1.4572), tensor(1.4721), tensor(1.5248), tensor(1.4898), tensor(1.5548), tensor(1.8248), tensor(1.4770), tensor(2.0369), tensor(1.4809), tensor(1.5825), tensor(1.4798), tensor(1.5134), tensor(1.5336), tensor(1.6709), tensor(1.7144), tensor(1.8219), tensor(1.5195), tensor(1.6331), tensor(2.5597), tensor(1.5054), tensor(1.5065), tensor(1.4791), tensor(1.4388), tensor(1.4740), tensor(1.5413), tensor(1.7566), tensor(1.4882), tensor(1.4963), tensor(1.4600), tensor(1.5138), tensor(1.5725), tensor(1.4748), tensor(4.0840), tensor(1.7098), tensor(1.4629), tensor(1.5003), tensor(1.4892), tensor(1.4915), tensor(1.5779), tensor(1.4686), tensor(1.4871), tensor(1.4815), tensor(1.9298), tensor(1.4835), tensor(1.5225), tensor(1.5556), tensor(1.5171), tensor(1.4445), tensor(1.4889), tensor(1.4842), tensor(1.4591), tensor(2.0066), tensor(1.5660), tensor(1.5648), tensor(1.4662), tensor(1.9621), tensor(1.5316), tensor(1.5124), tensor(1.5364), tensor(1.4824), tensor(1.4591), tensor(1.4909), tensor(1.5581), tensor(1.4927), tensor(1.4600), tensor(1.4609), tensor(1.5049), tensor(1.4946), tensor(1.4924), tensor(1.6820), tensor(1.4547), tensor(1.8268), tensor(1.4734), tensor(1.4681), tensor(1.4767), tensor(1.4784), tensor(1.4856), tensor(1.4683), tensor(1.8215), tensor(3.5798), tensor(1.4709), tensor(1.7655), tensor(1.4563), tensor(1.5517), tensor(1.4898), tensor(1.5864), tensor(1.4430), tensor(1.4576), tensor(1.6580), tensor(1.5932), tensor(1.4812), tensor(4.1195), tensor(1.4686), tensor(1.4937), tensor(1.4928), tensor(1.7131), tensor(1.5284), tensor(1.5715), tensor(1.4584), tensor(1.5831), tensor(1.4808), tensor(1.4763), tensor(1.4735), tensor(1.4972), tensor(1.4791), tensor(1.4537), tensor(1.5960), tensor(1.5328), tensor(1.4828), tensor(1.4956), tensor(1.6606), tensor(1.5053), tensor(2.0345), tensor(1.7746), tensor(1.5025), tensor(1.4904), tensor(1.5011), tensor(1.4972), tensor(1.4545), tensor(1.4963), tensor(1.5324), tensor(1.4415), tensor(1.6594), tensor(1.4902), tensor(1.5050), tensor(1.4875)]
09/23/2021 14:09:23 - INFO - __main__ - interference_scores=[tensor(0.0088), tensor(-0.0246), tensor(0.0007), tensor(0.1360), tensor(-0.0035), tensor(0.0023), tensor(-0.0235), tensor(-0.0045), tensor(-0.0083), tensor(-0.0734), tensor(-0.0338), tensor(-0.0197), tensor(0.0444), tensor(-1.7817), tensor(-0.0124), tensor(0.7458), tensor(0.1410), tensor(-0.0341), tensor(-0.0188), tensor(-0.0029), tensor(0.0210), tensor(-0.0013), tensor(0.0337), tensor(-0.0181), tensor(0.0186), tensor(0.0024), tensor(0.0068), tensor(-0.0145), tensor(-0.0071), tensor(-0.0186), tensor(0.0031), tensor(0.0183), tensor(0.0179), tensor(-0.0090), tensor(-0.0216), tensor(0.0021), tensor(-0.0169), tensor(-0.0447), tensor(-0.0069), tensor(-0.0184), tensor(-0.4994), tensor(-0.0188), tensor(-0.0063), tensor(0.0104), tensor(-0.0003), tensor(0.0271), tensor(0.0210), tensor(-0.0200), tensor(-0.1960), tensor(-0.0238), tensor(-0.0179), tensor(0.0636), tensor(-0.0133), tensor(0.0041), tensor(0.0747), tensor(-0.0015), tensor(-0.0230), tensor(-0.0065), tensor(-0.0107), tensor(-0.0649), tensor(-0.0049), tensor(0.1147), tensor(-0.1445), tensor(0.0050), tensor(-0.0174), tensor(-0.0077), tensor(-0.0045), tensor(-0.0211), tensor(-0.0055), tensor(0.0006), tensor(-0.0024), tensor(-0.0129), tensor(-0.0082), tensor(-0.0151), tensor(-0.0111), tensor(-0.0048), tensor(-0.0052), tensor(-0.6815), tensor(-0.0034), tensor(-0.0095), tensor(-0.0082), tensor(-0.0042), tensor(-0.0103), tensor(0.4090), tensor(-0.1728), tensor(-0.0088), tensor(-0.1569), tensor(-0.0117), tensor(-0.0054), tensor(-0.0024), tensor(-0.2800), tensor(-0.0135), tensor(-0.0539), tensor(-0.0176), tensor(-0.0047), tensor(-0.0085), tensor(0.0002), tensor(0.3718), tensor(-0.0179), tensor(0.1403), tensor(-0.0053), tensor(0.0096), tensor(-0.1506), tensor(-0.0104), tensor(-0.0131), tensor(-0.0192), tensor(-0.0053), tensor(-0.0347), tensor(-0.0030), tensor(-0.0703), tensor(0.0244), tensor(0.0122), tensor(-0.0247), tensor(-0.0183), tensor(0.1306), tensor(-0.0140), tensor(-0.0108), tensor(0.0651), tensor(-0.0181), tensor(0.0085), tensor(-0.3479), tensor(0.0441), tensor(0.0021), tensor(1.0729), tensor(0.1946), tensor(-0.0107), tensor(0.0044), tensor(-0.0051), tensor(0.0225), tensor(0.0587), tensor(-0.0068), tensor(-0.0214), tensor(-0.0119), tensor(-0.0072), tensor(-0.0100), tensor(-0.0970), tensor(0.0016), tensor(-0.1380), tensor(-0.0078), tensor(0.0010), tensor(-0.0092), tensor(-0.0108), tensor(0.0482), tensor(0.0555), tensor(-0.0208), tensor(0.1606), tensor(0.0054), tensor(0.1100), tensor(0.0593), tensor(-0.0099), tensor(0.0043), tensor(-0.0068), tensor(-0.0055), tensor(-0.0092), tensor(-0.0048), tensor(-0.1400), tensor(-8.4162e-05), tensor(0.0168), tensor(-0.0235), tensor(-0.0304), tensor(0.0307), tensor(-0.0057), tensor(-0.8367), tensor(0.0199), tensor(-0.0019), tensor(-0.0280), tensor(0.0001), tensor(-0.0143), tensor(0.0037), tensor(-0.0071), tensor(-0.0023), tensor(-0.0190), tensor(-0.0266), tensor(-0.0038), tensor(-0.0180), tensor(0.0339), tensor(-0.0024), tensor(-0.0009), tensor(0.0197), tensor(-0.0097), tensor(-0.0058), tensor(0.1817), tensor(-0.1934), tensor(0.0548), tensor(-0.0096), tensor(0.0229), tensor(-0.0027), tensor(-0.0058), tensor(-0.0270), tensor(0.0103), tensor(-0.0229), tensor(-0.0121), tensor(-0.0237), tensor(-0.0032), tensor(-0.0049), tensor(-0.0078), tensor(-0.0072), tensor(-0.0215), tensor(-0.0061), tensor(0.1450), tensor(-0.0024), tensor(0.0061), tensor(-0.0042), tensor(-0.0093), tensor(-0.0031), tensor(-0.0121), tensor(-0.0069), tensor(-0.0129), tensor(0.1732), tensor(-0.1631), tensor(-0.0076), tensor(0.0222), tensor(-0.0078), tensor(0.0220), tensor(-0.0052), tensor(-0.0212), tensor(-0.0038), tensor(-0.0151), tensor(0.1746), tensor(0.0709), tensor(-0.0021), tensor(-0.2378), tensor(0.0001), tensor(0.0196), tensor(-0.0136), tensor(-0.0948), tensor(0.0196), tensor(0.0259), tensor(-0.0119), tensor(-0.0102), tensor(-0.0033), tensor(-0.0077), tensor(-0.0027), tensor(-0.0089), tensor(-0.0092), tensor(-0.0027), tensor(0.0331), tensor(-0.0131), tensor(-0.0023), tensor(-0.0066), tensor(0.0778), tensor(0.0226), tensor(0.2556), tensor(-0.0122), tensor(-0.0038), tensor(-0.0173), tensor(-0.1062), tensor(-0.0051), tensor(-0.0092), tensor(0.0286), tensor(-0.0199), tensor(-0.0088), tensor(-0.0616), tensor(-0.0408), tensor(0.0207), tensor(-0.0110)]
09/23/2021 14:09:23 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-7703', 'mrqa_triviaqa-validation-1671', 'mrqa_squad-validation-2659', 'mrqa_triviaqa-validation-46', 'mrqa_triviaqa-validation-4402', 'mrqa_triviaqa-validation-1860', 'mrqa_squad-validation-5407', 'mrqa_triviaqa-validation-1770', 'mrqa_hotpotqa-validation-2937', 'mrqa_triviaqa-validation-2096', 'mrqa_triviaqa-validation-2530', 'mrqa_naturalquestions-validation-7906', 'mrqa_triviaqa-validation-7415', 'mrqa_squad-validation-3463', 'mrqa_naturalquestions-validation-1279', 'mrqa_triviaqa-validation-2959', 'mrqa_triviaqa-validation-1924', 'mrqa_hotpotqa-validation-3253', 'mrqa_hotpotqa-validation-1897', 'mrqa_squad-validation-830', 'mrqa_naturalquestions-validation-6736', 'mrqa_naturalquestions-validation-2666', 'mrqa_squad-validation-2584', 'mrqa_squad-validation-10322', 'mrqa_triviaqa-validation-3647', 'mrqa_naturalquestions-validation-5437', 'mrqa_naturalquestions-validation-8948', 'mrqa_squad-validation-608', 'mrqa_squad-validation-5210', 'mrqa_naturalquestions-validation-6207', 'mrqa_squad-validation-2812', 'mrqa_hotpotqa-validation-62']
09/23/2021 14:09:23 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:09:23 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=60) .... Timecode: 21
09/23/2021 14:09:36 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:09:36 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 21
09/23/2021 14:09:40 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 14:09:40 - INFO - __main__ - Instant Fixing Rate: 0.9285714285714286
09/23/2021 14:09:40 - INFO - __main__ - Instant Retention Rate: 0.7499999981250001
09/23/2021 14:09:42 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_022.pt.
09/23/2021 14:09:42 - INFO - __main__ - Saving the current error examples (len=28) to the memory.
09/23/2021 14:09:42 - INFO - __main__ - Current memory size: 582.
09/23/2021 14:09:42 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:09:42 - INFO - __main__ - Finished.
09/23/2021 14:09:42 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:09:42 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:09:42 - INFO - __main__ - Evaluating to get errors .... Timecode: 22
09/23/2021 14:09:45 - INFO - __main__ - Before Error Fixing: 0.0625
09/23/2021 14:09:45 - INFO - __main__ - Found 30 errors.
09/23/2021 14:09:45 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:09:45 - INFO - __main__ - Current memory size: 586.
09/23/2021 14:09:45 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:10:09 - INFO - __main__ - before_losses=[tensor(1.6054), tensor(1.4765), tensor(1.4598), tensor(1.4556), tensor(1.4960), tensor(1.9532), tensor(1.4800), tensor(1.5373), tensor(1.4506), tensor(1.4712), tensor(1.4837), tensor(1.4765), tensor(1.4719), tensor(1.5101), tensor(5.1755), tensor(1.4854), tensor(1.9472), tensor(1.5020), tensor(2.4395), tensor(1.6781), tensor(1.5214), tensor(1.5443), tensor(1.4698), tensor(1.5085), tensor(1.5008), tensor(1.4982), tensor(2.8157), tensor(1.4688), tensor(2.4759), tensor(1.6278), tensor(1.7343), tensor(1.4731), tensor(1.5323), tensor(1.5417), tensor(1.6243), tensor(1.5502), tensor(1.4559), tensor(1.4498), tensor(1.4960), tensor(1.4919), tensor(2.1021), tensor(1.5802), tensor(1.4988), tensor(1.4810), tensor(1.4973), tensor(1.5130), tensor(1.4779), tensor(1.4869), tensor(1.5667), tensor(1.4508), tensor(1.7692), tensor(1.8846), tensor(1.5459), tensor(1.5574), tensor(2.8790), tensor(1.5302), tensor(1.4826), tensor(1.5000), tensor(1.5009), tensor(1.5236), tensor(1.5610), tensor(1.5517), tensor(1.4943), tensor(1.5024), tensor(1.4477), tensor(3.1684), tensor(1.4963), tensor(1.5490), tensor(1.5001), tensor(1.5145), tensor(1.4684), tensor(1.5355), tensor(1.5499), tensor(1.4682), tensor(1.4857), tensor(1.4492), tensor(1.6175), tensor(1.5367), tensor(1.4670), tensor(1.4818), tensor(1.5015), tensor(1.4923), tensor(1.4491), tensor(1.4773), tensor(2.5252), tensor(1.6165), tensor(1.4436), tensor(1.4936), tensor(1.4983), tensor(1.4406), tensor(1.5765), tensor(1.6465), tensor(1.4838), tensor(1.5258), tensor(2.0069), tensor(1.4638), tensor(1.7702), tensor(1.5732), tensor(1.4967), tensor(1.4564), tensor(1.5306), tensor(1.4816), tensor(1.4711), tensor(1.5855), tensor(2.6401), tensor(1.5545), tensor(1.9607), tensor(1.7172), tensor(1.8813), tensor(1.4777), tensor(1.9426), tensor(2.0822), tensor(1.5234), tensor(1.5172), tensor(1.4722), tensor(1.5198), tensor(1.4797), tensor(1.5357), tensor(1.8228), tensor(1.4734), tensor(1.6632), tensor(1.5010), tensor(1.5393), tensor(1.4738), tensor(1.4734), tensor(1.6059), tensor(1.5275), tensor(1.4871), tensor(1.4813), tensor(1.5684), tensor(1.6231), tensor(1.4738), tensor(1.4779), tensor(1.4493), tensor(1.4961), tensor(1.4777), tensor(1.5488), tensor(2.5025), tensor(1.4702), tensor(1.4614), tensor(1.9719), tensor(1.4977), tensor(1.4787), tensor(1.5013), tensor(1.5302), tensor(1.4605), tensor(1.4647), tensor(1.7089), tensor(1.4708), tensor(1.5007), tensor(1.5989), tensor(1.5374), tensor(1.8731), tensor(1.4889), tensor(1.9834), tensor(1.6269), tensor(1.5225), tensor(1.7497), tensor(1.4872), tensor(1.4799), tensor(1.5270), tensor(1.5400), tensor(1.4990), tensor(1.8215), tensor(1.4629), tensor(1.4988), tensor(1.5621), tensor(1.4764), tensor(1.5489), tensor(1.4880), tensor(1.7232), tensor(1.5155), tensor(1.4792), tensor(1.4633), tensor(1.5055), tensor(1.5047), tensor(2.1478), tensor(1.5754), tensor(1.4767), tensor(1.5358), tensor(1.4671), tensor(1.4890), tensor(1.5418), tensor(1.9543), tensor(1.4976), tensor(1.8109), tensor(1.5314), tensor(1.5543), tensor(1.4936), tensor(1.4806), tensor(1.4796), tensor(1.5279), tensor(1.5094), tensor(1.5323), tensor(1.4832), tensor(1.5361), tensor(1.4842), tensor(1.5043), tensor(1.7120), tensor(1.4992), tensor(1.4818), tensor(1.4912), tensor(1.5074), tensor(1.4611), tensor(1.4925), tensor(2.2185), tensor(1.4857), tensor(1.9992), tensor(1.4430), tensor(1.4994), tensor(1.5545), tensor(1.4687), tensor(1.5126), tensor(1.4858), tensor(1.5036), tensor(1.5316), tensor(1.6096), tensor(1.5596), tensor(1.5860), tensor(1.4606), tensor(1.5333), tensor(1.4791), tensor(1.4801), tensor(1.5096), tensor(1.5370), tensor(1.4678), tensor(2.9063), tensor(1.5022), tensor(1.4814), tensor(1.5087), tensor(1.5024), tensor(1.4867), tensor(1.5576), tensor(1.5141), tensor(1.8556), tensor(1.4504), tensor(1.4853), tensor(1.5000), tensor(1.4978), tensor(1.4709), tensor(1.8331), tensor(1.8987), tensor(3.1948), tensor(1.5206), tensor(1.4862), tensor(1.4779), tensor(2.1337), tensor(1.4993), tensor(2.3204), tensor(1.4830), tensor(1.6106), tensor(1.5980), tensor(1.4613), tensor(1.5302), tensor(1.4613), tensor(1.4772)]
09/23/2021 14:10:09 - INFO - __main__ - after_losses=[tensor(1.5668), tensor(1.4623), tensor(1.4483), tensor(1.4439), tensor(1.4574), tensor(1.7814), tensor(1.4829), tensor(1.5255), tensor(1.4376), tensor(1.4705), tensor(1.4781), tensor(1.4596), tensor(1.4487), tensor(1.5315), tensor(5.0886), tensor(1.4881), tensor(1.9166), tensor(1.4902), tensor(2.2838), tensor(1.4987), tensor(1.4948), tensor(1.5128), tensor(1.4574), tensor(1.4900), tensor(1.5093), tensor(1.4926), tensor(2.6746), tensor(1.4701), tensor(2.5073), tensor(1.6216), tensor(1.7206), tensor(1.4699), tensor(1.5302), tensor(1.5250), tensor(1.5418), tensor(1.5450), tensor(1.4479), tensor(1.4367), tensor(1.4899), tensor(1.4814), tensor(1.9618), tensor(1.5743), tensor(1.4793), tensor(1.4754), tensor(1.4862), tensor(1.5003), tensor(1.4648), tensor(1.4850), tensor(1.5131), tensor(1.4353), tensor(1.8031), tensor(2.1439), tensor(1.4958), tensor(1.5308), tensor(3.0276), tensor(1.5269), tensor(1.4741), tensor(1.4836), tensor(1.4885), tensor(1.4928), tensor(1.5246), tensor(1.5366), tensor(1.4527), tensor(1.5155), tensor(1.4388), tensor(3.1827), tensor(1.4672), tensor(1.4973), tensor(1.4747), tensor(1.5055), tensor(1.4585), tensor(1.5181), tensor(1.5276), tensor(1.4506), tensor(1.4804), tensor(1.4454), tensor(1.7136), tensor(1.5883), tensor(1.4635), tensor(1.4779), tensor(1.4859), tensor(1.4772), tensor(1.4419), tensor(1.4590), tensor(2.3384), tensor(1.7898), tensor(1.4365), tensor(1.4807), tensor(1.4802), tensor(1.4333), tensor(1.6208), tensor(1.6435), tensor(1.4910), tensor(1.5148), tensor(1.9349), tensor(1.4598), tensor(1.7631), tensor(1.5718), tensor(1.4634), tensor(1.4471), tensor(1.5149), tensor(1.4705), tensor(1.4677), tensor(1.5837), tensor(2.6382), tensor(1.5333), tensor(1.7063), tensor(1.6187), tensor(2.0268), tensor(1.4718), tensor(1.5171), tensor(2.0053), tensor(1.4952), tensor(1.5074), tensor(1.4585), tensor(1.4955), tensor(1.4591), tensor(1.5211), tensor(1.8443), tensor(1.4673), tensor(1.5533), tensor(1.4841), tensor(1.4865), tensor(1.4680), tensor(1.4583), tensor(1.6256), tensor(1.5300), tensor(1.4682), tensor(1.4766), tensor(1.5513), tensor(1.5669), tensor(1.4623), tensor(1.4633), tensor(1.4362), tensor(1.4792), tensor(1.4605), tensor(1.5245), tensor(2.3690), tensor(1.4657), tensor(1.4532), tensor(1.9472), tensor(1.4842), tensor(1.5083), tensor(1.4956), tensor(1.5001), tensor(1.4502), tensor(1.4606), tensor(1.7514), tensor(1.4591), tensor(1.4917), tensor(1.6299), tensor(1.5299), tensor(1.5528), tensor(1.4793), tensor(1.9682), tensor(1.5557), tensor(1.5004), tensor(1.5444), tensor(1.4748), tensor(1.4736), tensor(1.5564), tensor(1.5158), tensor(1.4867), tensor(1.8691), tensor(1.4500), tensor(1.4792), tensor(1.6111), tensor(1.4489), tensor(1.5545), tensor(1.4661), tensor(1.9553), tensor(1.4809), tensor(1.4692), tensor(1.4606), tensor(1.4691), tensor(1.4759), tensor(2.4535), tensor(1.6436), tensor(1.4687), tensor(1.5279), tensor(1.4658), tensor(1.4727), tensor(1.5567), tensor(2.0013), tensor(1.4623), tensor(1.9062), tensor(1.4855), tensor(1.5351), tensor(1.4788), tensor(1.4679), tensor(1.4562), tensor(1.5168), tensor(1.5045), tensor(1.5216), tensor(1.4618), tensor(1.5219), tensor(1.4850), tensor(1.4765), tensor(1.7112), tensor(1.4742), tensor(1.4646), tensor(1.4821), tensor(1.4749), tensor(1.4552), tensor(1.4745), tensor(2.1272), tensor(1.4943), tensor(1.8357), tensor(1.4384), tensor(1.4864), tensor(1.5348), tensor(1.4502), tensor(1.4757), tensor(1.4709), tensor(1.5606), tensor(1.4923), tensor(1.5966), tensor(1.5117), tensor(1.6693), tensor(1.4624), tensor(1.5249), tensor(1.4686), tensor(1.4659), tensor(1.4736), tensor(1.5191), tensor(1.4529), tensor(2.7019), tensor(1.5099), tensor(1.4709), tensor(1.4844), tensor(1.4657), tensor(1.4802), tensor(1.5228), tensor(1.4971), tensor(1.4993), tensor(1.4409), tensor(1.4752), tensor(1.4911), tensor(1.4810), tensor(1.4581), tensor(1.4906), tensor(2.1662), tensor(2.2978), tensor(1.4795), tensor(1.4723), tensor(1.4955), tensor(1.9728), tensor(1.5022), tensor(2.4642), tensor(1.4519), tensor(1.8719), tensor(1.5507), tensor(1.4432), tensor(1.5524), tensor(1.4444), tensor(1.4539)]
09/23/2021 14:10:09 - INFO - __main__ - interference_scores=[tensor(-0.0386), tensor(-0.0142), tensor(-0.0114), tensor(-0.0117), tensor(-0.0386), tensor(-0.1718), tensor(0.0029), tensor(-0.0118), tensor(-0.0130), tensor(-0.0007), tensor(-0.0057), tensor(-0.0169), tensor(-0.0232), tensor(0.0214), tensor(-0.0869), tensor(0.0027), tensor(-0.0306), tensor(-0.0118), tensor(-0.1557), tensor(-0.1794), tensor(-0.0266), tensor(-0.0315), tensor(-0.0125), tensor(-0.0185), tensor(0.0085), tensor(-0.0056), tensor(-0.1411), tensor(0.0013), tensor(0.0313), tensor(-0.0061), tensor(-0.0138), tensor(-0.0032), tensor(-0.0022), tensor(-0.0167), tensor(-0.0825), tensor(-0.0052), tensor(-0.0080), tensor(-0.0132), tensor(-0.0061), tensor(-0.0105), tensor(-0.1402), tensor(-0.0059), tensor(-0.0195), tensor(-0.0056), tensor(-0.0111), tensor(-0.0127), tensor(-0.0131), tensor(-0.0020), tensor(-0.0536), tensor(-0.0155), tensor(0.0339), tensor(0.2593), tensor(-0.0501), tensor(-0.0266), tensor(0.1486), tensor(-0.0032), tensor(-0.0086), tensor(-0.0164), tensor(-0.0125), tensor(-0.0308), tensor(-0.0363), tensor(-0.0152), tensor(-0.0416), tensor(0.0131), tensor(-0.0090), tensor(0.0142), tensor(-0.0291), tensor(-0.0517), tensor(-0.0254), tensor(-0.0090), tensor(-0.0098), tensor(-0.0174), tensor(-0.0223), tensor(-0.0176), tensor(-0.0053), tensor(-0.0039), tensor(0.0961), tensor(0.0516), tensor(-0.0035), tensor(-0.0039), tensor(-0.0156), tensor(-0.0151), tensor(-0.0072), tensor(-0.0183), tensor(-0.1868), tensor(0.1733), tensor(-0.0071), tensor(-0.0129), tensor(-0.0181), tensor(-0.0073), tensor(0.0444), tensor(-0.0030), tensor(0.0072), tensor(-0.0110), tensor(-0.0720), tensor(-0.0040), tensor(-0.0071), tensor(-0.0013), tensor(-0.0333), tensor(-0.0093), tensor(-0.0157), tensor(-0.0110), tensor(-0.0034), tensor(-0.0018), tensor(-0.0019), tensor(-0.0212), tensor(-0.2543), tensor(-0.0986), tensor(0.1456), tensor(-0.0060), tensor(-0.4255), tensor(-0.0769), tensor(-0.0282), tensor(-0.0098), tensor(-0.0137), tensor(-0.0243), tensor(-0.0206), tensor(-0.0146), tensor(0.0215), tensor(-0.0060), tensor(-0.1100), tensor(-0.0169), tensor(-0.0528), tensor(-0.0058), tensor(-0.0152), tensor(0.0197), tensor(0.0025), tensor(-0.0189), tensor(-0.0047), tensor(-0.0170), tensor(-0.0562), tensor(-0.0114), tensor(-0.0146), tensor(-0.0131), tensor(-0.0168), tensor(-0.0171), tensor(-0.0243), tensor(-0.1334), tensor(-0.0045), tensor(-0.0082), tensor(-0.0247), tensor(-0.0135), tensor(0.0296), tensor(-0.0057), tensor(-0.0302), tensor(-0.0103), tensor(-0.0041), tensor(0.0425), tensor(-0.0117), tensor(-0.0090), tensor(0.0310), tensor(-0.0075), tensor(-0.3203), tensor(-0.0096), tensor(-0.0152), tensor(-0.0712), tensor(-0.0221), tensor(-0.2054), tensor(-0.0124), tensor(-0.0063), tensor(0.0294), tensor(-0.0243), tensor(-0.0123), tensor(0.0476), tensor(-0.0129), tensor(-0.0196), tensor(0.0490), tensor(-0.0275), tensor(0.0056), tensor(-0.0220), tensor(0.2321), tensor(-0.0346), tensor(-0.0100), tensor(-0.0027), tensor(-0.0364), tensor(-0.0288), tensor(0.3057), tensor(0.0682), tensor(-0.0080), tensor(-0.0079), tensor(-0.0013), tensor(-0.0163), tensor(0.0149), tensor(0.0470), tensor(-0.0353), tensor(0.0953), tensor(-0.0459), tensor(-0.0192), tensor(-0.0149), tensor(-0.0128), tensor(-0.0233), tensor(-0.0111), tensor(-0.0048), tensor(-0.0106), tensor(-0.0213), tensor(-0.0142), tensor(0.0009), tensor(-0.0278), tensor(-0.0008), tensor(-0.0250), tensor(-0.0172), tensor(-0.0091), tensor(-0.0325), tensor(-0.0058), tensor(-0.0180), tensor(-0.0913), tensor(0.0086), tensor(-0.1635), tensor(-0.0046), tensor(-0.0131), tensor(-0.0197), tensor(-0.0186), tensor(-0.0369), tensor(-0.0149), tensor(0.0571), tensor(-0.0393), tensor(-0.0130), tensor(-0.0479), tensor(0.0833), tensor(0.0017), tensor(-0.0084), tensor(-0.0105), tensor(-0.0142), tensor(-0.0360), tensor(-0.0180), tensor(-0.0150), tensor(-0.2044), tensor(0.0077), tensor(-0.0105), tensor(-0.0243), tensor(-0.0366), tensor(-0.0065), tensor(-0.0348), tensor(-0.0170), tensor(-0.3563), tensor(-0.0095), tensor(-0.0101), tensor(-0.0089), tensor(-0.0168), tensor(-0.0128), tensor(-0.3424), tensor(0.2674), tensor(-0.8971), tensor(-0.0412), tensor(-0.0139), tensor(0.0176), tensor(-0.1610), tensor(0.0029), tensor(0.1437), tensor(-0.0312), tensor(0.2613), tensor(-0.0473), tensor(-0.0181), tensor(0.0221), tensor(-0.0169), tensor(-0.0233)]
09/23/2021 14:10:09 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-2812', 'mrqa_squad-validation-3151', 'mrqa_triviaqa-validation-5248', 'mrqa_triviaqa-validation-4055', 'mrqa_triviaqa-validation-444', 'mrqa_hotpotqa-validation-2612', 'mrqa_naturalquestions-validation-5017', 'mrqa_triviaqa-validation-4068', 'mrqa_triviaqa-validation-7767', 'mrqa_triviaqa-validation-287', 'mrqa_squad-validation-4181', 'mrqa_squad-validation-2757', 'mrqa_triviaqa-validation-6389', 'mrqa_naturalquestions-validation-8006', 'mrqa_triviaqa-validation-1860', 'mrqa_hotpotqa-validation-4977', 'mrqa_triviaqa-validation-5704', 'mrqa_naturalquestions-validation-1085', 'mrqa_triviaqa-validation-314', 'mrqa_squad-validation-3181', 'mrqa_triviaqa-validation-1521', 'mrqa_triviaqa-validation-5261', 'mrqa_triviaqa-validation-7512', 'mrqa_naturalquestions-validation-10205', 'mrqa_squad-validation-8464', 'mrqa_triviaqa-validation-4486', 'mrqa_squad-validation-3558', 'mrqa_triviaqa-validation-1575', 'mrqa_triviaqa-validation-4560', 'mrqa_hotpotqa-validation-1210', 'mrqa_triviaqa-validation-3714', 'mrqa_triviaqa-validation-5996']
09/23/2021 14:10:09 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:10:09 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=62) .... Timecode: 22
09/23/2021 14:10:23 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:10:23 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 22
09/23/2021 14:10:27 - INFO - __main__ - After Error Fixing: 0.96875
09/23/2021 14:10:27 - INFO - __main__ - Instant Fixing Rate: 0.9666666666666667
09/23/2021 14:10:27 - INFO - __main__ - Instant Retention Rate: 0.999999995
09/23/2021 14:10:28 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_023.pt.
09/23/2021 14:10:28 - INFO - __main__ - Saving the current error examples (len=30) to the memory.
09/23/2021 14:10:28 - INFO - __main__ - Current memory size: 586.
09/23/2021 14:10:28 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:10:28 - INFO - __main__ - Finished.
09/23/2021 14:10:28 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:10:28 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:10:28 - INFO - __main__ - Evaluating to get errors .... Timecode: 23
09/23/2021 14:10:32 - INFO - __main__ - Before Error Fixing: 0.3125
09/23/2021 14:10:32 - INFO - __main__ - Found 22 errors.
09/23/2021 14:10:32 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:10:32 - INFO - __main__ - Current memory size: 616.
09/23/2021 14:10:32 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:10:55 - INFO - __main__ - before_losses=[tensor(1.4850), tensor(1.5046), tensor(1.5359), tensor(1.5547), tensor(1.5109), tensor(1.5940), tensor(1.5436), tensor(1.5152), tensor(1.4851), tensor(1.5012), tensor(1.6376), tensor(1.5408), tensor(1.4579), tensor(1.4581), tensor(1.5273), tensor(1.6142), tensor(1.5286), tensor(1.5179), tensor(1.5148), tensor(1.5036), tensor(1.5000), tensor(1.4926), tensor(1.5309), tensor(1.5097), tensor(1.4552), tensor(1.5320), tensor(1.5596), tensor(1.4895), tensor(1.5465), tensor(1.5133), tensor(1.5605), tensor(1.5177), tensor(1.5147), tensor(1.4766), tensor(1.5355), tensor(2.3211), tensor(1.4651), tensor(1.5366), tensor(1.7034), tensor(1.8603), tensor(1.5136), tensor(1.8070), tensor(2.0504), tensor(1.5154), tensor(1.9159), tensor(1.5744), tensor(1.5400), tensor(1.5855), tensor(1.5074), tensor(1.4759), tensor(1.5016), tensor(1.4594), tensor(1.5685), tensor(1.4995), tensor(1.4967), tensor(2.1902), tensor(1.4855), tensor(1.5191), tensor(1.5797), tensor(1.4719), tensor(1.4916), tensor(1.5561), tensor(1.5259), tensor(1.7489), tensor(1.5136), tensor(1.4978), tensor(1.4584), tensor(1.6095), tensor(1.5155), tensor(1.5301), tensor(1.5070), tensor(1.4559), tensor(1.5221), tensor(1.5971), tensor(1.5078), tensor(1.4716), tensor(1.4774), tensor(1.6888), tensor(1.4612), tensor(5.3285), tensor(1.5362), tensor(1.4900), tensor(1.5383), tensor(1.7179), tensor(1.5400), tensor(1.4852), tensor(1.5316), tensor(1.5241), tensor(1.4607), tensor(2.5450), tensor(1.5169), tensor(1.4987), tensor(1.5110), tensor(1.4596), tensor(1.5690), tensor(1.7938), tensor(1.4863), tensor(1.4915), tensor(1.5148), tensor(1.4462), tensor(1.4802), tensor(2.2167), tensor(1.5268), tensor(1.5149), tensor(1.5231), tensor(1.4871), tensor(1.4660), tensor(1.4770), tensor(1.4697), tensor(1.5544), tensor(1.5974), tensor(1.5399), tensor(1.4883), tensor(1.5114), tensor(1.5346), tensor(1.5017), tensor(1.5420), tensor(1.5082), tensor(1.5099), tensor(3.8055), tensor(1.5327), tensor(1.4823), tensor(1.4447), tensor(1.5212), tensor(1.4832), tensor(1.4812), tensor(1.5225), tensor(1.4900), tensor(1.5104), tensor(1.5520), tensor(1.5102), tensor(1.4740), tensor(1.5670), tensor(1.7517), tensor(1.5399), tensor(1.5207), tensor(2.2407), tensor(1.4883), tensor(1.4957), tensor(1.5156), tensor(1.5206), tensor(1.4836), tensor(1.5128), tensor(1.5266), tensor(1.5062), tensor(1.4634), tensor(1.5109), tensor(2.1735), tensor(1.6581), tensor(1.4818), tensor(1.5161), tensor(1.4747), tensor(1.7756), tensor(1.9739), tensor(1.5270), tensor(4.5795), tensor(1.5056), tensor(2.7084), tensor(1.4998), tensor(1.9491), tensor(1.5320), tensor(1.5132), tensor(1.4593), tensor(1.5501), tensor(1.5375), tensor(3.1158), tensor(1.5572), tensor(1.5613), tensor(1.4808), tensor(1.4768), tensor(1.4692), tensor(1.5758), tensor(1.4666), tensor(1.5012), tensor(1.5027), tensor(1.5368), tensor(1.4987), tensor(1.4850), tensor(2.0313), tensor(1.4920), tensor(1.5414), tensor(1.5743), tensor(1.5361), tensor(1.4626), tensor(1.5616), tensor(1.5026), tensor(2.5345), tensor(1.4903), tensor(1.5479), tensor(2.1015), tensor(1.5055), tensor(1.5892), tensor(1.4876), tensor(1.4947), tensor(1.6815), tensor(1.4795), tensor(1.4571), tensor(1.4924), tensor(1.4733), tensor(1.5087), tensor(1.5150), tensor(1.4885), tensor(1.5003), tensor(1.9997), tensor(1.5331), tensor(1.8873), tensor(2.3027), tensor(1.6074), tensor(1.4582), tensor(1.4997), tensor(1.4978), tensor(1.4773), tensor(1.5105), tensor(1.4841), tensor(1.5395), tensor(1.4742), tensor(1.4904), tensor(1.5753), tensor(1.5178), tensor(1.4847), tensor(1.5278), tensor(1.9443), tensor(1.6657), tensor(1.5618), tensor(1.4577), tensor(1.5543), tensor(1.5550), tensor(1.4971), tensor(1.5116), tensor(1.6473), tensor(1.5064), tensor(1.5122), tensor(1.4869), tensor(1.5353), tensor(3.3061), tensor(1.5832), tensor(1.9812), tensor(1.5205), tensor(1.4829), tensor(1.5240), tensor(2.9428), tensor(1.5605), tensor(1.5895), tensor(1.6343), tensor(1.5048), tensor(1.5358), tensor(1.5138), tensor(1.5432), tensor(1.4819), tensor(1.5936), tensor(1.4909), tensor(1.4723), tensor(1.5353), tensor(1.5216), tensor(1.5099), tensor(1.4972)]
09/23/2021 14:10:55 - INFO - __main__ - after_losses=[tensor(1.4729), tensor(1.5358), tensor(1.5335), tensor(1.5485), tensor(1.4945), tensor(1.5824), tensor(1.5331), tensor(1.4996), tensor(1.4605), tensor(1.4653), tensor(1.5563), tensor(1.5820), tensor(1.5302), tensor(1.4533), tensor(1.6480), tensor(1.8118), tensor(1.5048), tensor(1.5047), tensor(1.4953), tensor(1.5268), tensor(1.4756), tensor(1.5534), tensor(1.5248), tensor(1.4989), tensor(1.4530), tensor(1.5035), tensor(1.7441), tensor(1.4825), tensor(1.4837), tensor(1.4933), tensor(1.5269), tensor(1.5110), tensor(1.5241), tensor(1.4748), tensor(1.5808), tensor(2.3542), tensor(1.4647), tensor(1.5027), tensor(1.6789), tensor(1.8586), tensor(1.5046), tensor(1.7850), tensor(2.1354), tensor(1.5129), tensor(1.9098), tensor(1.6022), tensor(1.5311), tensor(1.6024), tensor(1.5023), tensor(1.5513), tensor(1.4786), tensor(1.4554), tensor(1.5585), tensor(1.5024), tensor(1.4939), tensor(2.2185), tensor(1.4898), tensor(1.5213), tensor(1.5810), tensor(1.4855), tensor(1.4642), tensor(1.5398), tensor(1.4972), tensor(1.7144), tensor(1.4970), tensor(1.4977), tensor(1.4569), tensor(1.5109), tensor(1.5039), tensor(1.5014), tensor(1.4953), tensor(1.4559), tensor(1.5399), tensor(1.5324), tensor(1.4928), tensor(1.4839), tensor(1.4749), tensor(2.1430), tensor(1.4648), tensor(5.4258), tensor(1.5197), tensor(1.4816), tensor(1.5502), tensor(1.7417), tensor(1.5428), tensor(1.5171), tensor(1.5170), tensor(1.5359), tensor(1.4597), tensor(2.4274), tensor(1.5002), tensor(1.4819), tensor(1.5003), tensor(1.4466), tensor(1.5019), tensor(1.7267), tensor(1.4797), tensor(1.5217), tensor(1.4894), tensor(1.4404), tensor(1.4818), tensor(2.1452), tensor(1.5188), tensor(1.5492), tensor(1.5818), tensor(1.4777), tensor(1.4542), tensor(1.4811), tensor(1.4689), tensor(1.5401), tensor(1.5314), tensor(1.5340), tensor(1.4962), tensor(1.4813), tensor(1.5165), tensor(1.4975), tensor(1.5297), tensor(1.4871), tensor(1.5008), tensor(3.4609), tensor(1.5428), tensor(1.4724), tensor(1.4548), tensor(1.5716), tensor(1.4654), tensor(1.4706), tensor(1.6181), tensor(1.4909), tensor(1.4866), tensor(1.5178), tensor(1.4936), tensor(1.5445), tensor(1.6474), tensor(2.0189), tensor(1.5158), tensor(1.5688), tensor(2.2918), tensor(1.4927), tensor(1.4671), tensor(1.4893), tensor(1.5477), tensor(1.4683), tensor(1.5133), tensor(1.5099), tensor(1.5106), tensor(1.4549), tensor(1.4723), tensor(2.1017), tensor(1.6417), tensor(1.6061), tensor(2.5105), tensor(1.4608), tensor(1.7199), tensor(1.9425), tensor(1.5227), tensor(3.6320), tensor(1.5007), tensor(2.0185), tensor(1.5469), tensor(1.8636), tensor(1.5316), tensor(1.5111), tensor(1.4468), tensor(1.5451), tensor(1.5315), tensor(2.6388), tensor(1.5430), tensor(1.5735), tensor(1.4817), tensor(1.4743), tensor(1.4663), tensor(1.5631), tensor(1.4784), tensor(1.5385), tensor(1.5244), tensor(1.5220), tensor(1.4980), tensor(1.7010), tensor(2.0404), tensor(1.4800), tensor(1.5229), tensor(1.7110), tensor(1.5271), tensor(1.4567), tensor(1.5597), tensor(1.4955), tensor(2.1352), tensor(1.4689), tensor(1.5302), tensor(2.0610), tensor(1.4985), tensor(1.5558), tensor(1.4795), tensor(1.4968), tensor(1.6322), tensor(1.4757), tensor(1.4680), tensor(1.4752), tensor(1.4616), tensor(1.5073), tensor(1.4844), tensor(1.4796), tensor(1.4757), tensor(1.5841), tensor(1.5282), tensor(1.8257), tensor(2.3742), tensor(1.5680), tensor(1.4588), tensor(1.4996), tensor(1.4839), tensor(1.4837), tensor(1.5039), tensor(1.4735), tensor(1.5250), tensor(1.4671), tensor(1.4904), tensor(1.5466), tensor(1.5171), tensor(1.4777), tensor(1.5280), tensor(2.0118), tensor(1.6028), tensor(1.5473), tensor(1.4563), tensor(1.5329), tensor(1.5542), tensor(1.4804), tensor(1.6145), tensor(1.6473), tensor(1.5055), tensor(1.4772), tensor(1.4721), tensor(1.5282), tensor(1.5525), tensor(1.5892), tensor(2.0125), tensor(1.5064), tensor(1.4773), tensor(1.5508), tensor(2.8320), tensor(1.5434), tensor(1.5772), tensor(1.9967), tensor(1.5013), tensor(1.5591), tensor(1.5093), tensor(1.5120), tensor(1.4665), tensor(1.8809), tensor(1.5072), tensor(1.4576), tensor(1.5215), tensor(1.5093), tensor(1.4875), tensor(1.4829)]
09/23/2021 14:10:55 - INFO - __main__ - interference_scores=[tensor(-0.0120), tensor(0.0312), tensor(-0.0025), tensor(-0.0062), tensor(-0.0164), tensor(-0.0116), tensor(-0.0104), tensor(-0.0157), tensor(-0.0247), tensor(-0.0358), tensor(-0.0814), tensor(0.0413), tensor(0.0723), tensor(-0.0048), tensor(0.1208), tensor(0.1976), tensor(-0.0238), tensor(-0.0131), tensor(-0.0195), tensor(0.0232), tensor(-0.0244), tensor(0.0608), tensor(-0.0061), tensor(-0.0107), tensor(-0.0022), tensor(-0.0286), tensor(0.1845), tensor(-0.0070), tensor(-0.0628), tensor(-0.0199), tensor(-0.0335), tensor(-0.0067), tensor(0.0094), tensor(-0.0018), tensor(0.0453), tensor(0.0331), tensor(-0.0004), tensor(-0.0338), tensor(-0.0245), tensor(-0.0017), tensor(-0.0090), tensor(-0.0220), tensor(0.0851), tensor(-0.0025), tensor(-0.0061), tensor(0.0278), tensor(-0.0089), tensor(0.0169), tensor(-0.0050), tensor(0.0754), tensor(-0.0230), tensor(-0.0040), tensor(-0.0100), tensor(0.0029), tensor(-0.0028), tensor(0.0283), tensor(0.0042), tensor(0.0022), tensor(0.0012), tensor(0.0136), tensor(-0.0274), tensor(-0.0163), tensor(-0.0288), tensor(-0.0346), tensor(-0.0165), tensor(-0.0001), tensor(-0.0015), tensor(-0.0986), tensor(-0.0117), tensor(-0.0288), tensor(-0.0117), tensor(8.7619e-05), tensor(0.0177), tensor(-0.0647), tensor(-0.0151), tensor(0.0123), tensor(-0.0025), tensor(0.4542), tensor(0.0036), tensor(0.0973), tensor(-0.0164), tensor(-0.0084), tensor(0.0118), tensor(0.0238), tensor(0.0028), tensor(0.0319), tensor(-0.0146), tensor(0.0118), tensor(-0.0010), tensor(-0.1176), tensor(-0.0167), tensor(-0.0168), tensor(-0.0107), tensor(-0.0130), tensor(-0.0671), tensor(-0.0671), tensor(-0.0067), tensor(0.0302), tensor(-0.0254), tensor(-0.0058), tensor(0.0016), tensor(-0.0715), tensor(-0.0080), tensor(0.0343), tensor(0.0587), tensor(-0.0095), tensor(-0.0118), tensor(0.0041), tensor(-0.0009), tensor(-0.0144), tensor(-0.0660), tensor(-0.0059), tensor(0.0079), tensor(-0.0301), tensor(-0.0181), tensor(-0.0042), tensor(-0.0122), tensor(-0.0210), tensor(-0.0090), tensor(-0.3447), tensor(0.0100), tensor(-0.0099), tensor(0.0101), tensor(0.0504), tensor(-0.0179), tensor(-0.0105), tensor(0.0956), tensor(0.0009), tensor(-0.0239), tensor(-0.0343), tensor(-0.0166), tensor(0.0705), tensor(0.0804), tensor(0.2672), tensor(-0.0241), tensor(0.0481), tensor(0.0511), tensor(0.0045), tensor(-0.0286), tensor(-0.0263), tensor(0.0271), tensor(-0.0153), tensor(0.0005), tensor(-0.0167), tensor(0.0044), tensor(-0.0084), tensor(-0.0387), tensor(-0.0718), tensor(-0.0164), tensor(0.1243), tensor(0.9945), tensor(-0.0139), tensor(-0.0557), tensor(-0.0314), tensor(-0.0044), tensor(-0.9475), tensor(-0.0049), tensor(-0.6899), tensor(0.0471), tensor(-0.0855), tensor(-0.0005), tensor(-0.0020), tensor(-0.0125), tensor(-0.0050), tensor(-0.0060), tensor(-0.4770), tensor(-0.0142), tensor(0.0122), tensor(0.0010), tensor(-0.0025), tensor(-0.0029), tensor(-0.0127), tensor(0.0118), tensor(0.0373), tensor(0.0216), tensor(-0.0148), tensor(-0.0007), tensor(0.2160), tensor(0.0091), tensor(-0.0120), tensor(-0.0185), tensor(0.1367), tensor(-0.0090), tensor(-0.0059), tensor(-0.0020), tensor(-0.0071), tensor(-0.3993), tensor(-0.0214), tensor(-0.0176), tensor(-0.0406), tensor(-0.0070), tensor(-0.0334), tensor(-0.0081), tensor(0.0022), tensor(-0.0492), tensor(-0.0039), tensor(0.0109), tensor(-0.0172), tensor(-0.0117), tensor(-0.0014), tensor(-0.0306), tensor(-0.0089), tensor(-0.0246), tensor(-0.4155), tensor(-0.0048), tensor(-0.0616), tensor(0.0715), tensor(-0.0395), tensor(0.0006), tensor(-0.0001), tensor(-0.0139), tensor(0.0063), tensor(-0.0066), tensor(-0.0106), tensor(-0.0145), tensor(-0.0070), tensor(-6.7115e-05), tensor(-0.0287), tensor(-0.0007), tensor(-0.0070), tensor(0.0001), tensor(0.0674), tensor(-0.0629), tensor(-0.0145), tensor(-0.0014), tensor(-0.0215), tensor(-0.0009), tensor(-0.0167), tensor(0.1030), tensor(-5.3644e-05), tensor(-0.0009), tensor(-0.0350), tensor(-0.0148), tensor(-0.0071), tensor(-1.7536), tensor(0.0059), tensor(0.0313), tensor(-0.0141), tensor(-0.0056), tensor(0.0268), tensor(-0.1108), tensor(-0.0171), tensor(-0.0124), tensor(0.3625), tensor(-0.0035), tensor(0.0233), tensor(-0.0046), tensor(-0.0312), tensor(-0.0154), tensor(0.2873), tensor(0.0163), tensor(-0.0147), tensor(-0.0139), tensor(-0.0123), tensor(-0.0224), tensor(-0.0143)]
09/23/2021 14:10:55 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-444', 'mrqa_squad-validation-8966', 'mrqa_squad-validation-6947', 'mrqa_squad-validation-2862', 'mrqa_naturalquestions-validation-1085', 'mrqa_squad-validation-3018', 'mrqa_squad-validation-6677', 'mrqa_squad-validation-9405', 'mrqa_triviaqa-validation-1935', 'mrqa_squad-validation-7272', 'mrqa_squad-validation-2709', 'mrqa_squad-validation-392', 'mrqa_triviaqa-validation-2442', 'mrqa_hotpotqa-validation-1250', 'mrqa_triviaqa-validation-5852', 'mrqa_squad-validation-4506', 'mrqa_triviaqa-validation-2136', 'mrqa_hotpotqa-validation-1855', 'mrqa_squad-validation-10322', 'mrqa_squad-validation-2629', 'mrqa_naturalquestions-validation-10490', 'mrqa_squad-validation-10042', 'mrqa_hotpotqa-validation-2064', 'mrqa_triviaqa-validation-1616', 'mrqa_triviaqa-validation-6721', 'mrqa_triviaqa-validation-93', 'mrqa_naturalquestions-validation-9194', 'mrqa_squad-validation-2769', 'mrqa_triviaqa-validation-7153', 'mrqa_squad-validation-1688', 'mrqa_triviaqa-validation-4729', 'mrqa_triviaqa-validation-1550']
09/23/2021 14:10:55 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:10:55 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=54) .... Timecode: 23
09/23/2021 14:11:07 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:11:07 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 23
09/23/2021 14:11:10 - INFO - __main__ - After Error Fixing: 0.71875
09/23/2021 14:11:10 - INFO - __main__ - Instant Fixing Rate: 0.7272727272727273
09/23/2021 14:11:10 - INFO - __main__ - Instant Retention Rate: 0.6999999992999999
09/23/2021 14:11:12 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_024.pt.
09/23/2021 14:11:12 - INFO - __main__ - Saving the current error examples (len=22) to the memory.
09/23/2021 14:11:12 - INFO - __main__ - Current memory size: 616.
09/23/2021 14:11:12 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:11:12 - INFO - __main__ - Finished.
09/23/2021 14:11:12 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:11:12 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:11:12 - INFO - __main__ - Evaluating to get errors .... Timecode: 24
09/23/2021 14:11:15 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 14:11:15 - INFO - __main__ - Found 26 errors.
09/23/2021 14:11:15 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:11:15 - INFO - __main__ - Current memory size: 638.
09/23/2021 14:11:15 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:11:37 - INFO - __main__ - before_losses=[tensor(1.4806), tensor(1.4780), tensor(5.3710), tensor(1.5478), tensor(1.4542), tensor(1.9977), tensor(1.5722), tensor(1.5017), tensor(1.4679), tensor(1.5170), tensor(1.7849), tensor(1.4969), tensor(1.5192), tensor(1.7141), tensor(1.4876), tensor(1.5642), tensor(1.6199), tensor(1.9677), tensor(1.8821), tensor(1.5067), tensor(1.4491), tensor(1.5148), tensor(1.6676), tensor(1.5016), tensor(1.5650), tensor(1.5347), tensor(2.1514), tensor(1.4962), tensor(1.4945), tensor(1.5328), tensor(1.4845), tensor(1.5051), tensor(1.5329), tensor(1.5207), tensor(1.5138), tensor(1.5059), tensor(1.5430), tensor(1.5534), tensor(1.5200), tensor(1.6311), tensor(1.4669), tensor(4.9709), tensor(1.8390), tensor(1.5210), tensor(1.5297), tensor(1.5019), tensor(1.4954), tensor(2.3238), tensor(1.7035), tensor(1.5237), tensor(1.4932), tensor(1.4550), tensor(1.5032), tensor(3.4852), tensor(1.5343), tensor(1.5231), tensor(1.5179), tensor(2.1056), tensor(1.4803), tensor(1.4888), tensor(1.4766), tensor(1.5022), tensor(1.4934), tensor(1.5038), tensor(1.4767), tensor(1.4676), tensor(1.4808), tensor(1.5221), tensor(1.4613), tensor(1.5258), tensor(1.4672), tensor(1.4600), tensor(1.4826), tensor(1.4538), tensor(1.5209), tensor(1.4751), tensor(1.4607), tensor(1.4772), tensor(1.4919), tensor(1.5377), tensor(1.4740), tensor(1.5294), tensor(1.5263), tensor(1.5632), tensor(2.4722), tensor(1.4963), tensor(1.5270), tensor(1.4542), tensor(2.0522), tensor(1.4990), tensor(1.4668), tensor(1.5298), tensor(1.5192), tensor(1.4751), tensor(2.5054), tensor(2.0577), tensor(1.4999), tensor(1.6185), tensor(1.5231), tensor(1.4910), tensor(1.4639), tensor(1.6908), tensor(1.6018), tensor(1.4521), tensor(1.7429), tensor(1.5158), tensor(1.4521), tensor(1.5023), tensor(1.5095), tensor(1.5455), tensor(1.4830), tensor(1.5159), tensor(1.4933), tensor(1.5333), tensor(1.5531), tensor(1.4750), tensor(1.5252), tensor(1.5129), tensor(1.4617), tensor(1.5537), tensor(1.5570), tensor(1.4864), tensor(1.4979), tensor(1.7116), tensor(1.4609), tensor(1.4526), tensor(1.5039), tensor(1.4822), tensor(1.4703), tensor(1.4663), tensor(1.8798), tensor(1.5189), tensor(1.6159), tensor(1.5123), tensor(1.6487), tensor(1.4914), tensor(1.4733), tensor(1.6368), tensor(1.4738), tensor(1.4890), tensor(4.0900), tensor(1.5136), tensor(1.4883), tensor(1.4846), tensor(1.5024), tensor(1.5012), tensor(1.5646), tensor(1.4758), tensor(1.4724), tensor(1.6603), tensor(2.7342), tensor(1.5400), tensor(1.4655), tensor(1.6042), tensor(1.7352), tensor(1.4919), tensor(2.3977), tensor(1.5387), tensor(1.5190), tensor(1.8172), tensor(1.5037), tensor(1.4866), tensor(1.4743), tensor(1.4816), tensor(1.5598), tensor(1.5113), tensor(1.4965), tensor(1.4937), tensor(1.4911), tensor(1.5113), tensor(1.5205), tensor(1.5608), tensor(1.4895), tensor(1.6458), tensor(1.7937), tensor(1.4817), tensor(1.4471), tensor(1.5336), tensor(1.7901), tensor(1.6868), tensor(1.5075), tensor(1.5407), tensor(1.6302), tensor(1.5772), tensor(2.1667), tensor(1.4836), tensor(1.4932), tensor(1.5099), tensor(1.9117), tensor(1.5023), tensor(1.5372), tensor(1.7810), tensor(1.4988), tensor(1.6289), tensor(1.5549), tensor(1.5501), tensor(1.5511), tensor(1.4931), tensor(1.4835), tensor(1.5002), tensor(1.7828), tensor(1.5359), tensor(1.4983), tensor(2.5318), tensor(2.9368), tensor(1.5250), tensor(2.0274), tensor(1.4725), tensor(1.5977), tensor(1.4608), tensor(1.8167), tensor(1.4980), tensor(1.5091), tensor(1.4775), tensor(1.5389), tensor(1.5107), tensor(1.4687), tensor(1.4782), tensor(1.5318), tensor(1.4728), tensor(1.5627), tensor(1.7609), tensor(2.1141), tensor(1.8130), tensor(1.5060), tensor(1.4875), tensor(1.5145), tensor(1.4921), tensor(1.5491), tensor(1.5152), tensor(1.4923), tensor(2.9619), tensor(1.6045), tensor(1.4910), tensor(1.4911), tensor(1.4798), tensor(1.5220), tensor(1.4748), tensor(1.4953), tensor(1.5891), tensor(1.5120), tensor(1.5025), tensor(1.5341), tensor(1.4987), tensor(1.5182), tensor(1.5251), tensor(1.4967), tensor(1.4950), tensor(1.4880), tensor(1.6718), tensor(1.5328), tensor(1.5365), tensor(2.3887), tensor(1.4900), tensor(1.5090), tensor(1.7878)]
09/23/2021 14:11:37 - INFO - __main__ - after_losses=[tensor(1.4715), tensor(1.4764), tensor(4.7013), tensor(1.5300), tensor(1.4574), tensor(2.0043), tensor(1.5490), tensor(1.4766), tensor(1.4679), tensor(1.5155), tensor(1.7268), tensor(1.4763), tensor(1.5129), tensor(1.6552), tensor(1.4758), tensor(1.5540), tensor(1.6649), tensor(1.8718), tensor(1.8828), tensor(1.4867), tensor(1.4433), tensor(1.5202), tensor(1.7143), tensor(1.4938), tensor(1.5510), tensor(1.5134), tensor(2.0847), tensor(1.5168), tensor(1.4825), tensor(1.5249), tensor(1.4877), tensor(1.4942), tensor(1.5142), tensor(1.5207), tensor(1.5020), tensor(1.5074), tensor(1.5431), tensor(1.5351), tensor(1.5103), tensor(1.7622), tensor(1.4541), tensor(4.3784), tensor(1.9075), tensor(1.5069), tensor(1.6129), tensor(1.4791), tensor(1.4775), tensor(2.4453), tensor(1.6352), tensor(1.4965), tensor(1.4930), tensor(1.4431), tensor(1.4914), tensor(3.5743), tensor(1.5278), tensor(1.4950), tensor(1.5032), tensor(2.2708), tensor(1.4733), tensor(1.4725), tensor(1.4622), tensor(1.4738), tensor(1.4782), tensor(1.4883), tensor(1.4609), tensor(1.4524), tensor(1.4862), tensor(1.5083), tensor(1.4560), tensor(1.5260), tensor(1.4555), tensor(1.4633), tensor(1.4830), tensor(1.4514), tensor(1.5207), tensor(1.4680), tensor(1.4598), tensor(1.4747), tensor(1.4840), tensor(1.5274), tensor(1.4721), tensor(1.5106), tensor(1.5146), tensor(1.5641), tensor(2.3484), tensor(1.4803), tensor(1.5138), tensor(1.4855), tensor(1.8851), tensor(1.4911), tensor(1.4733), tensor(1.5146), tensor(1.5089), tensor(1.4639), tensor(2.3528), tensor(2.0101), tensor(1.4989), tensor(1.5695), tensor(1.5308), tensor(1.4676), tensor(1.4569), tensor(1.7561), tensor(1.5873), tensor(1.4444), tensor(1.7076), tensor(1.5711), tensor(1.4494), tensor(1.4913), tensor(1.4934), tensor(1.5436), tensor(1.5068), tensor(1.5087), tensor(1.4910), tensor(1.4993), tensor(1.6182), tensor(1.4791), tensor(1.5082), tensor(1.4942), tensor(1.4756), tensor(1.5648), tensor(1.5190), tensor(1.4859), tensor(1.4924), tensor(1.7160), tensor(1.4622), tensor(1.4510), tensor(1.4951), tensor(1.4845), tensor(1.4656), tensor(1.4595), tensor(1.9565), tensor(1.5027), tensor(1.6248), tensor(1.5097), tensor(1.9745), tensor(1.4957), tensor(1.4631), tensor(1.5993), tensor(1.4567), tensor(1.5412), tensor(3.8607), tensor(1.4956), tensor(1.4892), tensor(1.5933), tensor(1.4901), tensor(1.4915), tensor(1.5527), tensor(1.4764), tensor(1.4622), tensor(1.4897), tensor(2.5165), tensor(1.5868), tensor(1.4577), tensor(1.5124), tensor(1.6086), tensor(1.4706), tensor(2.3637), tensor(1.5131), tensor(1.5002), tensor(1.6224), tensor(1.4982), tensor(1.4814), tensor(1.4663), tensor(1.4922), tensor(1.5653), tensor(1.4829), tensor(1.4863), tensor(1.4803), tensor(1.5068), tensor(1.4984), tensor(1.5150), tensor(1.5731), tensor(1.4714), tensor(1.5502), tensor(1.7195), tensor(1.5597), tensor(1.4419), tensor(1.5246), tensor(1.7721), tensor(1.5247), tensor(1.5059), tensor(1.5326), tensor(1.6352), tensor(1.6701), tensor(2.1933), tensor(1.4703), tensor(1.4805), tensor(1.5104), tensor(1.8852), tensor(1.5074), tensor(1.5252), tensor(1.7049), tensor(1.4809), tensor(1.5374), tensor(1.5355), tensor(1.5479), tensor(1.5178), tensor(1.4940), tensor(1.4736), tensor(1.4828), tensor(1.7609), tensor(1.5363), tensor(1.4872), tensor(2.6281), tensor(2.8205), tensor(1.5120), tensor(1.5929), tensor(1.4564), tensor(1.5796), tensor(1.4626), tensor(1.9936), tensor(1.4850), tensor(1.5095), tensor(1.4891), tensor(1.5526), tensor(1.5020), tensor(1.4624), tensor(1.4799), tensor(1.5241), tensor(1.4942), tensor(1.5627), tensor(1.6005), tensor(2.0597), tensor(1.6803), tensor(1.5471), tensor(1.4684), tensor(1.5007), tensor(1.4912), tensor(1.5520), tensor(1.4941), tensor(1.6005), tensor(1.8284), tensor(1.5989), tensor(1.4829), tensor(1.4818), tensor(1.4822), tensor(1.5034), tensor(1.4707), tensor(1.5240), tensor(1.6610), tensor(1.4981), tensor(1.4817), tensor(1.5104), tensor(1.4877), tensor(1.5124), tensor(1.5183), tensor(1.4777), tensor(1.4863), tensor(1.4688), tensor(1.7879), tensor(1.5302), tensor(1.4672), tensor(1.9827), tensor(1.4839), tensor(1.4940), tensor(1.4801)]
09/23/2021 14:11:37 - INFO - __main__ - interference_scores=[tensor(-0.0091), tensor(-0.0016), tensor(-0.6698), tensor(-0.0178), tensor(0.0032), tensor(0.0065), tensor(-0.0232), tensor(-0.0251), tensor(-3.8862e-05), tensor(-0.0015), tensor(-0.0581), tensor(-0.0205), tensor(-0.0063), tensor(-0.0589), tensor(-0.0118), tensor(-0.0102), tensor(0.0450), tensor(-0.0958), tensor(0.0007), tensor(-0.0200), tensor(-0.0058), tensor(0.0053), tensor(0.0467), tensor(-0.0078), tensor(-0.0140), tensor(-0.0213), tensor(-0.0667), tensor(0.0207), tensor(-0.0119), tensor(-0.0079), tensor(0.0032), tensor(-0.0109), tensor(-0.0187), tensor(1.4424e-05), tensor(-0.0118), tensor(0.0015), tensor(4.8041e-05), tensor(-0.0182), tensor(-0.0097), tensor(0.1311), tensor(-0.0127), tensor(-0.5925), tensor(0.0685), tensor(-0.0141), tensor(0.0832), tensor(-0.0227), tensor(-0.0179), tensor(0.1215), tensor(-0.0683), tensor(-0.0272), tensor(-0.0002), tensor(-0.0118), tensor(-0.0119), tensor(0.0891), tensor(-0.0065), tensor(-0.0282), tensor(-0.0146), tensor(0.1652), tensor(-0.0070), tensor(-0.0163), tensor(-0.0144), tensor(-0.0284), tensor(-0.0152), tensor(-0.0155), tensor(-0.0158), tensor(-0.0152), tensor(0.0054), tensor(-0.0137), tensor(-0.0053), tensor(0.0002), tensor(-0.0117), tensor(0.0033), tensor(0.0004), tensor(-0.0025), tensor(-0.0002), tensor(-0.0071), tensor(-0.0009), tensor(-0.0025), tensor(-0.0079), tensor(-0.0103), tensor(-0.0019), tensor(-0.0188), tensor(-0.0117), tensor(0.0009), tensor(-0.1239), tensor(-0.0160), tensor(-0.0132), tensor(0.0312), tensor(-0.1671), tensor(-0.0079), tensor(0.0064), tensor(-0.0151), tensor(-0.0103), tensor(-0.0112), tensor(-0.1527), tensor(-0.0477), tensor(-0.0010), tensor(-0.0490), tensor(0.0077), tensor(-0.0234), tensor(-0.0070), tensor(0.0653), tensor(-0.0145), tensor(-0.0077), tensor(-0.0352), tensor(0.0553), tensor(-0.0027), tensor(-0.0110), tensor(-0.0160), tensor(-0.0018), tensor(0.0238), tensor(-0.0072), tensor(-0.0023), tensor(-0.0340), tensor(0.0650), tensor(0.0041), tensor(-0.0170), tensor(-0.0186), tensor(0.0139), tensor(0.0111), tensor(-0.0380), tensor(-0.0005), tensor(-0.0055), tensor(0.0044), tensor(0.0012), tensor(-0.0017), tensor(-0.0088), tensor(0.0023), tensor(-0.0047), tensor(-0.0067), tensor(0.0767), tensor(-0.0162), tensor(0.0089), tensor(-0.0026), tensor(0.3257), tensor(0.0043), tensor(-0.0103), tensor(-0.0375), tensor(-0.0171), tensor(0.0521), tensor(-0.2293), tensor(-0.0180), tensor(0.0009), tensor(0.1086), tensor(-0.0123), tensor(-0.0097), tensor(-0.0119), tensor(0.0006), tensor(-0.0102), tensor(-0.1707), tensor(-0.2177), tensor(0.0468), tensor(-0.0078), tensor(-0.0918), tensor(-0.1266), tensor(-0.0213), tensor(-0.0340), tensor(-0.0255), tensor(-0.0188), tensor(-0.1948), tensor(-0.0056), tensor(-0.0052), tensor(-0.0080), tensor(0.0107), tensor(0.0055), tensor(-0.0284), tensor(-0.0102), tensor(-0.0135), tensor(0.0157), tensor(-0.0130), tensor(-0.0055), tensor(0.0123), tensor(-0.0180), tensor(-0.0956), tensor(-0.0742), tensor(0.0780), tensor(-0.0052), tensor(-0.0090), tensor(-0.0180), tensor(-0.1621), tensor(-0.0016), tensor(-0.0081), tensor(0.0051), tensor(0.0929), tensor(0.0266), tensor(-0.0133), tensor(-0.0127), tensor(0.0005), tensor(-0.0265), tensor(0.0051), tensor(-0.0120), tensor(-0.0761), tensor(-0.0179), tensor(-0.0915), tensor(-0.0194), tensor(-0.0022), tensor(-0.0334), tensor(0.0009), tensor(-0.0099), tensor(-0.0174), tensor(-0.0219), tensor(0.0004), tensor(-0.0111), tensor(0.0963), tensor(-0.1163), tensor(-0.0131), tensor(-0.4345), tensor(-0.0161), tensor(-0.0181), tensor(0.0018), tensor(0.1769), tensor(-0.0130), tensor(0.0004), tensor(0.0116), tensor(0.0137), tensor(-0.0086), tensor(-0.0063), tensor(0.0017), tensor(-0.0077), tensor(0.0214), tensor(-6.2943e-05), tensor(-0.1604), tensor(-0.0545), tensor(-0.1327), tensor(0.0411), tensor(-0.0192), tensor(-0.0138), tensor(-0.0009), tensor(0.0030), tensor(-0.0210), tensor(0.1082), tensor(-1.1335), tensor(-0.0056), tensor(-0.0081), tensor(-0.0092), tensor(0.0024), tensor(-0.0186), tensor(-0.0041), tensor(0.0288), tensor(0.0718), tensor(-0.0139), tensor(-0.0207), tensor(-0.0237), tensor(-0.0111), tensor(-0.0059), tensor(-0.0068), tensor(-0.0190), tensor(-0.0087), tensor(-0.0192), tensor(0.1161), tensor(-0.0026), tensor(-0.0693), tensor(-0.4060), tensor(-0.0061), tensor(-0.0150), tensor(-0.3077)]
09/23/2021 14:11:37 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-392', 'mrqa_triviaqa-validation-3420', 'mrqa_triviaqa-validation-4731', 'mrqa_squad-validation-8464', 'mrqa_triviaqa-validation-2376', 'mrqa_triviaqa-validation-1555', 'mrqa_triviaqa-validation-123', 'mrqa_hotpotqa-validation-573', 'mrqa_naturalquestions-validation-5017', 'mrqa_hotpotqa-validation-1509', 'mrqa_triviaqa-validation-2961', 'mrqa_triviaqa-validation-681', 'mrqa_naturalquestions-validation-9218', 'mrqa_triviaqa-validation-6887', 'mrqa_triviaqa-validation-1494', 'mrqa_squad-validation-2509', 'mrqa_squad-validation-3181', 'mrqa_hotpotqa-validation-5810', 'mrqa_hotpotqa-validation-501', 'mrqa_naturalquestions-validation-5522', 'mrqa_hotpotqa-validation-2263', 'mrqa_triviaqa-validation-1616', 'mrqa_naturalquestions-validation-2190', 'mrqa_triviaqa-validation-3552', 'mrqa_triviaqa-validation-4279', 'mrqa_hotpotqa-validation-5651', 'mrqa_triviaqa-validation-1293', 'mrqa_triviaqa-validation-7021', 'mrqa_hotpotqa-validation-1250', 'mrqa_hotpotqa-validation-3146', 'mrqa_hotpotqa-validation-1888', 'mrqa_naturalquestions-validation-7906']
09/23/2021 14:11:37 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:11:37 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 24
09/23/2021 14:11:50 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:11:50 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 24
09/23/2021 14:11:54 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 14:11:54 - INFO - __main__ - Instant Fixing Rate: 0.8846153846153846
09/23/2021 14:11:54 - INFO - __main__ - Instant Retention Rate: 0.9999999983333333
09/23/2021 14:11:56 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_025.pt.
09/23/2021 14:11:56 - INFO - __main__ - Saving the current error examples (len=26) to the memory.
09/23/2021 14:11:56 - INFO - __main__ - Current memory size: 638.
09/23/2021 14:11:56 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:11:56 - INFO - __main__ - Finished.
09/23/2021 14:11:56 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:11:56 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:11:56 - INFO - __main__ - Evaluating to get errors .... Timecode: 25
09/23/2021 14:11:59 - INFO - __main__ - Before Error Fixing: 0.21875
09/23/2021 14:11:59 - INFO - __main__ - Found 25 errors.
09/23/2021 14:11:59 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:11:59 - INFO - __main__ - Current memory size: 664.
09/23/2021 14:11:59 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:12:22 - INFO - __main__ - before_losses=[tensor(1.4710), tensor(1.4574), tensor(2.3744), tensor(1.4924), tensor(1.7129), tensor(1.5354), tensor(1.5155), tensor(1.5364), tensor(1.5144), tensor(1.5280), tensor(1.4885), tensor(1.6994), tensor(1.5280), tensor(2.3570), tensor(1.4838), tensor(1.5285), tensor(3.3299), tensor(1.5378), tensor(1.5933), tensor(1.6544), tensor(2.8346), tensor(1.5771), tensor(1.6299), tensor(1.8696), tensor(1.4732), tensor(1.5478), tensor(1.4591), tensor(1.5108), tensor(1.5331), tensor(1.9089), tensor(1.5090), tensor(1.6543), tensor(1.4533), tensor(1.4931), tensor(1.4880), tensor(1.5364), tensor(1.4557), tensor(1.5019), tensor(2.0148), tensor(1.7432), tensor(1.5426), tensor(1.5753), tensor(3.0959), tensor(1.4697), tensor(1.9203), tensor(1.5813), tensor(1.4929), tensor(1.8040), tensor(1.4976), tensor(1.5220), tensor(1.5648), tensor(1.5077), tensor(1.5163), tensor(1.4598), tensor(1.5674), tensor(1.4813), tensor(1.4798), tensor(1.6371), tensor(1.6907), tensor(1.4510), tensor(1.5488), tensor(1.5229), tensor(1.5043), tensor(1.4799), tensor(1.9713), tensor(1.5089), tensor(1.5583), tensor(1.5151), tensor(2.0106), tensor(1.5158), tensor(1.5642), tensor(1.5997), tensor(1.5336), tensor(1.4692), tensor(1.4580), tensor(1.5104), tensor(2.4057), tensor(1.5177), tensor(1.7743), tensor(1.5164), tensor(1.6748), tensor(1.4929), tensor(1.5188), tensor(1.5763), tensor(1.4902), tensor(1.4869), tensor(1.5449), tensor(1.5564), tensor(1.5649), tensor(1.4701), tensor(1.7766), tensor(1.5369), tensor(1.4634), tensor(1.4715), tensor(1.5140), tensor(1.4759), tensor(1.4796), tensor(1.4624), tensor(2.0309), tensor(1.8107), tensor(1.4803), tensor(1.4516), tensor(1.5123), tensor(1.4823), tensor(1.4951), tensor(2.6157), tensor(1.5224), tensor(1.5273), tensor(1.4841), tensor(1.9293), tensor(1.9640), tensor(1.4771), tensor(1.5341), tensor(1.5564), tensor(1.5013), tensor(1.4972), tensor(1.5122), tensor(1.5030), tensor(1.9068), tensor(1.4904), tensor(1.4701), tensor(1.6153), tensor(1.6114), tensor(1.7626), tensor(1.5388), tensor(1.5567), tensor(1.6101), tensor(1.5049), tensor(1.6500), tensor(1.5009), tensor(2.9676), tensor(2.5467), tensor(1.4575), tensor(1.4977), tensor(1.4795), tensor(1.5165), tensor(1.5301), tensor(1.4860), tensor(2.6264), tensor(1.5046), tensor(4.0421), tensor(1.5227), tensor(1.4618), tensor(1.7699), tensor(1.4899), tensor(2.7820), tensor(1.4946), tensor(1.4674), tensor(1.6363), tensor(1.5011), tensor(1.4804), tensor(1.4996), tensor(1.5064), tensor(2.9266), tensor(1.5364), tensor(1.5192), tensor(1.7522), tensor(1.5315), tensor(1.5277), tensor(1.8752), tensor(1.4773), tensor(1.4958), tensor(1.4759), tensor(1.4962), tensor(1.8620), tensor(1.4812), tensor(1.4846), tensor(1.5720), tensor(4.6770), tensor(1.9087), tensor(1.4792), tensor(1.5211), tensor(1.4972), tensor(1.5312), tensor(1.4606), tensor(1.5460), tensor(1.5975), tensor(1.5258), tensor(1.5068), tensor(1.5340), tensor(1.6532), tensor(2.2238), tensor(1.4528), tensor(1.4680), tensor(1.5033), tensor(1.5529), tensor(1.5281), tensor(1.4731), tensor(1.5469), tensor(2.2901), tensor(3.2130), tensor(1.6132), tensor(1.6037), tensor(1.4993), tensor(1.4945), tensor(1.7904), tensor(1.5169), tensor(1.5359), tensor(1.4573), tensor(1.4812), tensor(1.6404), tensor(1.5484), tensor(1.4915), tensor(1.6044), tensor(1.5489), tensor(1.5981), tensor(2.8357), tensor(1.4837), tensor(1.5427), tensor(1.5196), tensor(1.4760), tensor(1.4862), tensor(1.4858), tensor(1.6047), tensor(2.2767), tensor(1.6244), tensor(4.1556), tensor(4.9592), tensor(1.4773), tensor(1.5833), tensor(1.4691), tensor(1.5201), tensor(1.5004), tensor(1.4446), tensor(2.1161), tensor(1.5702), tensor(1.9618), tensor(1.8555), tensor(1.4624), tensor(1.5312), tensor(2.1956), tensor(1.7480), tensor(1.5308), tensor(1.5029), tensor(1.5174), tensor(1.4882), tensor(1.4892), tensor(1.5094), tensor(1.5279), tensor(1.4651), tensor(1.9463), tensor(1.4950), tensor(1.4926), tensor(1.5215), tensor(1.8315), tensor(1.4808), tensor(1.4986), tensor(1.5859), tensor(1.5151), tensor(1.7658), tensor(1.4762), tensor(1.5047), tensor(1.4512), tensor(1.5582), tensor(1.5215), tensor(2.4867)]
09/23/2021 14:12:22 - INFO - __main__ - after_losses=[tensor(1.5211), tensor(1.4511), tensor(2.4883), tensor(1.4748), tensor(1.6139), tensor(1.5006), tensor(1.4880), tensor(1.5082), tensor(1.4816), tensor(1.5113), tensor(1.4690), tensor(1.7004), tensor(1.4976), tensor(4.1600), tensor(1.4684), tensor(1.5052), tensor(3.6170), tensor(1.5069), tensor(1.5707), tensor(1.5968), tensor(2.5245), tensor(1.5380), tensor(1.5296), tensor(1.8503), tensor(1.4522), tensor(1.6311), tensor(1.4526), tensor(1.5261), tensor(1.4979), tensor(1.7650), tensor(1.4853), tensor(1.7124), tensor(1.4504), tensor(1.4721), tensor(1.4806), tensor(1.6295), tensor(1.4527), tensor(1.4751), tensor(1.8232), tensor(1.7197), tensor(1.4960), tensor(1.5343), tensor(2.7299), tensor(3.4154), tensor(1.5842), tensor(1.5696), tensor(1.4969), tensor(1.6640), tensor(1.4852), tensor(1.4973), tensor(1.5424), tensor(1.4657), tensor(1.4820), tensor(1.4513), tensor(1.6092), tensor(1.4656), tensor(1.4448), tensor(1.7309), tensor(1.6523), tensor(1.4380), tensor(1.5088), tensor(1.4950), tensor(1.4884), tensor(1.4691), tensor(1.9066), tensor(1.4869), tensor(1.5129), tensor(1.4773), tensor(2.0243), tensor(1.5206), tensor(1.5389), tensor(1.5731), tensor(1.5009), tensor(1.4597), tensor(1.4569), tensor(1.4736), tensor(2.3703), tensor(1.5092), tensor(1.5696), tensor(1.4924), tensor(1.5914), tensor(1.4701), tensor(1.5019), tensor(1.4910), tensor(1.4691), tensor(1.4810), tensor(1.5090), tensor(1.5362), tensor(1.5279), tensor(1.4597), tensor(1.7704), tensor(1.5112), tensor(1.4418), tensor(1.4443), tensor(1.5278), tensor(1.4855), tensor(1.4724), tensor(1.4492), tensor(1.9892), tensor(1.8911), tensor(1.4803), tensor(1.4442), tensor(1.4936), tensor(1.4711), tensor(1.4764), tensor(2.3910), tensor(1.4834), tensor(1.4949), tensor(1.4654), tensor(1.8022), tensor(1.8295), tensor(1.4620), tensor(1.5064), tensor(1.5300), tensor(1.4932), tensor(1.4604), tensor(1.4793), tensor(1.4733), tensor(1.8921), tensor(1.4788), tensor(1.4742), tensor(1.6018), tensor(1.5393), tensor(1.5328), tensor(1.6187), tensor(1.5273), tensor(1.5871), tensor(1.4805), tensor(1.6013), tensor(1.4828), tensor(2.7518), tensor(1.5142), tensor(1.4632), tensor(1.4722), tensor(1.4632), tensor(1.4776), tensor(1.4966), tensor(1.4652), tensor(2.6277), tensor(1.4703), tensor(3.6938), tensor(1.4912), tensor(1.4485), tensor(1.8738), tensor(1.4590), tensor(1.9111), tensor(1.4688), tensor(1.4445), tensor(1.7347), tensor(1.4794), tensor(1.4477), tensor(1.4791), tensor(1.4773), tensor(2.9107), tensor(1.5010), tensor(1.5189), tensor(1.6280), tensor(1.5456), tensor(1.4968), tensor(1.8101), tensor(1.4637), tensor(1.4725), tensor(1.4531), tensor(1.4782), tensor(1.9021), tensor(1.4657), tensor(1.4785), tensor(1.5595), tensor(4.5065), tensor(1.9407), tensor(1.4461), tensor(1.4931), tensor(1.4794), tensor(1.4998), tensor(1.4400), tensor(1.5071), tensor(1.5822), tensor(1.5087), tensor(1.4693), tensor(1.4954), tensor(1.5928), tensor(2.2368), tensor(1.4508), tensor(1.4560), tensor(1.4831), tensor(1.5355), tensor(1.5227), tensor(1.4514), tensor(1.5086), tensor(2.3681), tensor(3.4976), tensor(1.5621), tensor(1.5679), tensor(1.4746), tensor(1.4712), tensor(1.7862), tensor(1.4801), tensor(1.4910), tensor(1.4546), tensor(1.4940), tensor(1.6959), tensor(1.5283), tensor(1.5034), tensor(1.6827), tensor(1.6143), tensor(1.8742), tensor(3.0030), tensor(1.5813), tensor(1.5036), tensor(1.4904), tensor(1.4553), tensor(1.4563), tensor(1.4659), tensor(1.5201), tensor(2.0902), tensor(1.5243), tensor(4.0252), tensor(4.5885), tensor(1.4509), tensor(1.7312), tensor(1.4616), tensor(1.4763), tensor(1.4557), tensor(1.4286), tensor(2.0216), tensor(1.5097), tensor(1.9504), tensor(2.0389), tensor(1.4433), tensor(1.5118), tensor(2.0899), tensor(1.7774), tensor(1.4798), tensor(1.5102), tensor(1.4963), tensor(1.4633), tensor(1.4631), tensor(1.4915), tensor(1.5115), tensor(1.4470), tensor(1.7868), tensor(1.4714), tensor(1.4806), tensor(1.5029), tensor(1.8331), tensor(1.4623), tensor(1.4759), tensor(1.4839), tensor(1.4905), tensor(1.7807), tensor(1.4681), tensor(1.4840), tensor(1.4433), tensor(1.5284), tensor(1.4983), tensor(2.3769)]
09/23/2021 14:12:22 - INFO - __main__ - interference_scores=[tensor(0.0500), tensor(-0.0064), tensor(0.1139), tensor(-0.0176), tensor(-0.0990), tensor(-0.0348), tensor(-0.0275), tensor(-0.0283), tensor(-0.0328), tensor(-0.0167), tensor(-0.0195), tensor(0.0010), tensor(-0.0305), tensor(1.8030), tensor(-0.0154), tensor(-0.0232), tensor(0.2871), tensor(-0.0309), tensor(-0.0227), tensor(-0.0576), tensor(-0.3101), tensor(-0.0391), tensor(-0.1003), tensor(-0.0193), tensor(-0.0210), tensor(0.0833), tensor(-0.0065), tensor(0.0153), tensor(-0.0352), tensor(-0.1439), tensor(-0.0238), tensor(0.0580), tensor(-0.0029), tensor(-0.0210), tensor(-0.0074), tensor(0.0932), tensor(-0.0030), tensor(-0.0268), tensor(-0.1917), tensor(-0.0235), tensor(-0.0466), tensor(-0.0410), tensor(-0.3660), tensor(1.9457), tensor(-0.3361), tensor(-0.0117), tensor(0.0040), tensor(-0.1400), tensor(-0.0124), tensor(-0.0247), tensor(-0.0224), tensor(-0.0420), tensor(-0.0343), tensor(-0.0085), tensor(0.0418), tensor(-0.0157), tensor(-0.0350), tensor(0.0938), tensor(-0.0384), tensor(-0.0130), tensor(-0.0401), tensor(-0.0279), tensor(-0.0159), tensor(-0.0108), tensor(-0.0647), tensor(-0.0220), tensor(-0.0454), tensor(-0.0378), tensor(0.0136), tensor(0.0048), tensor(-0.0253), tensor(-0.0266), tensor(-0.0326), tensor(-0.0095), tensor(-0.0011), tensor(-0.0368), tensor(-0.0354), tensor(-0.0085), tensor(-0.2047), tensor(-0.0240), tensor(-0.0835), tensor(-0.0228), tensor(-0.0169), tensor(-0.0852), tensor(-0.0211), tensor(-0.0059), tensor(-0.0360), tensor(-0.0202), tensor(-0.0370), tensor(-0.0104), tensor(-0.0062), tensor(-0.0256), tensor(-0.0216), tensor(-0.0272), tensor(0.0137), tensor(0.0096), tensor(-0.0071), tensor(-0.0133), tensor(-0.0417), tensor(0.0805), tensor(2.1458e-05), tensor(-0.0074), tensor(-0.0187), tensor(-0.0112), tensor(-0.0186), tensor(-0.2247), tensor(-0.0390), tensor(-0.0324), tensor(-0.0186), tensor(-0.1271), tensor(-0.1345), tensor(-0.0152), tensor(-0.0277), tensor(-0.0265), tensor(-0.0081), tensor(-0.0368), tensor(-0.0330), tensor(-0.0297), tensor(-0.0147), tensor(-0.0116), tensor(0.0042), tensor(-0.0135), tensor(-0.0721), tensor(-0.2297), tensor(0.0798), tensor(-0.0294), tensor(-0.0230), tensor(-0.0244), tensor(-0.0487), tensor(-0.0181), tensor(-0.2158), tensor(-1.0325), tensor(0.0057), tensor(-0.0256), tensor(-0.0163), tensor(-0.0390), tensor(-0.0334), tensor(-0.0208), tensor(0.0013), tensor(-0.0342), tensor(-0.3483), tensor(-0.0315), tensor(-0.0133), tensor(0.1039), tensor(-0.0309), tensor(-0.8708), tensor(-0.0258), tensor(-0.0229), tensor(0.0984), tensor(-0.0217), tensor(-0.0327), tensor(-0.0206), tensor(-0.0291), tensor(-0.0159), tensor(-0.0354), tensor(-0.0004), tensor(-0.1242), tensor(0.0142), tensor(-0.0309), tensor(-0.0650), tensor(-0.0136), tensor(-0.0234), tensor(-0.0228), tensor(-0.0180), tensor(0.0401), tensor(-0.0155), tensor(-0.0061), tensor(-0.0126), tensor(-0.1705), tensor(0.0320), tensor(-0.0332), tensor(-0.0280), tensor(-0.0178), tensor(-0.0313), tensor(-0.0206), tensor(-0.0389), tensor(-0.0153), tensor(-0.0171), tensor(-0.0374), tensor(-0.0386), tensor(-0.0604), tensor(0.0130), tensor(-0.0020), tensor(-0.0120), tensor(-0.0201), tensor(-0.0174), tensor(-0.0054), tensor(-0.0216), tensor(-0.0383), tensor(0.0780), tensor(0.2846), tensor(-0.0511), tensor(-0.0359), tensor(-0.0247), tensor(-0.0233), tensor(-0.0042), tensor(-0.0368), tensor(-0.0449), tensor(-0.0027), tensor(0.0128), tensor(0.0554), tensor(-0.0200), tensor(0.0120), tensor(0.0783), tensor(0.0655), tensor(0.2761), tensor(0.1673), tensor(0.0976), tensor(-0.0391), tensor(-0.0292), tensor(-0.0208), tensor(-0.0298), tensor(-0.0199), tensor(-0.0846), tensor(-0.1865), tensor(-0.1001), tensor(-0.1304), tensor(-0.3707), tensor(-0.0264), tensor(0.1480), tensor(-0.0075), tensor(-0.0438), tensor(-0.0447), tensor(-0.0160), tensor(-0.0945), tensor(-0.0605), tensor(-0.0114), tensor(0.1834), tensor(-0.0191), tensor(-0.0194), tensor(-0.1058), tensor(0.0294), tensor(-0.0510), tensor(0.0073), tensor(-0.0211), tensor(-0.0249), tensor(-0.0261), tensor(-0.0179), tensor(-0.0164), tensor(-0.0181), tensor(-0.1595), tensor(-0.0236), tensor(-0.0120), tensor(-0.0186), tensor(0.0016), tensor(-0.0185), tensor(-0.0227), tensor(-0.1020), tensor(-0.0247), tensor(0.0149), tensor(-0.0082), tensor(-0.0207), tensor(-0.0079), tensor(-0.0298), tensor(-0.0232), tensor(-0.1098)]
09/23/2021 14:12:22 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-8464', 'mrqa_naturalquestions-validation-9011', 'mrqa_triviaqa-validation-751', 'mrqa_triviaqa-validation-6872', 'mrqa_squad-validation-1750', 'mrqa_triviaqa-validation-6935', 'mrqa_triviaqa-validation-2961', 'mrqa_triviaqa-validation-5997', 'mrqa_triviaqa-validation-935', 'mrqa_triviaqa-validation-6125', 'mrqa_triviaqa-validation-639', 'mrqa_triviaqa-validation-444', 'mrqa_triviaqa-validation-3339', 'mrqa_triviaqa-validation-2210', 'mrqa_triviaqa-validation-1494', 'mrqa_squad-validation-2509', 'mrqa_triviaqa-validation-5036', 'mrqa_triviaqa-validation-7300', 'mrqa_triviaqa-validation-2376', 'mrqa_naturalquestions-validation-2884', 'mrqa_triviaqa-validation-1551', 'mrqa_triviaqa-validation-4301', 'mrqa_triviaqa-validation-1437', 'mrqa_naturalquestions-validation-2212', 'mrqa_squad-validation-608', 'mrqa_squad-validation-1003', 'mrqa_triviaqa-validation-5937', 'mrqa_hotpotqa-validation-2287', 'mrqa_squad-validation-1374', 'mrqa_naturalquestions-validation-10046', 'mrqa_triviaqa-validation-893', 'mrqa_squad-validation-3558']
09/23/2021 14:12:22 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:12:22 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=57) .... Timecode: 25
09/23/2021 14:12:35 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:12:35 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 25
09/23/2021 14:12:39 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 14:12:39 - INFO - __main__ - Instant Fixing Rate: 0.84
09/23/2021 14:12:39 - INFO - __main__ - Instant Retention Rate: 0.8571428559183674
09/23/2021 14:12:41 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_026.pt.
09/23/2021 14:12:41 - INFO - __main__ - Saving the current error examples (len=25) to the memory.
09/23/2021 14:12:41 - INFO - __main__ - Current memory size: 664.
09/23/2021 14:12:41 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:12:41 - INFO - __main__ - Finished.
09/23/2021 14:12:41 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:12:41 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:12:41 - INFO - __main__ - Evaluating to get errors .... Timecode: 26
09/23/2021 14:12:45 - INFO - __main__ - Before Error Fixing: 0.25
09/23/2021 14:12:45 - INFO - __main__ - Found 24 errors.
09/23/2021 14:12:45 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:12:45 - INFO - __main__ - Current memory size: 689.
09/23/2021 14:12:45 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:13:08 - INFO - __main__ - before_losses=[tensor(1.5306), tensor(1.4793), tensor(1.5632), tensor(1.5183), tensor(2.3244), tensor(1.4824), tensor(1.5199), tensor(1.7700), tensor(1.5377), tensor(1.4669), tensor(1.5161), tensor(1.5281), tensor(1.5330), tensor(1.4803), tensor(1.5336), tensor(1.5530), tensor(1.5934), tensor(1.5066), tensor(2.1955), tensor(1.5002), tensor(1.4799), tensor(2.5726), tensor(1.5391), tensor(1.4674), tensor(1.4754), tensor(1.4684), tensor(1.5273), tensor(1.5241), tensor(1.4665), tensor(1.4471), tensor(1.6874), tensor(1.8189), tensor(1.5047), tensor(1.5903), tensor(1.5819), tensor(1.5487), tensor(1.4892), tensor(2.1346), tensor(1.4975), tensor(1.5821), tensor(1.4851), tensor(1.5097), tensor(1.4807), tensor(1.5074), tensor(1.5408), tensor(1.5008), tensor(1.5646), tensor(1.5400), tensor(1.5088), tensor(1.4873), tensor(1.5389), tensor(2.3115), tensor(1.5302), tensor(1.8458), tensor(1.4985), tensor(1.4851), tensor(1.4886), tensor(1.5572), tensor(1.6791), tensor(1.5165), tensor(1.7444), tensor(1.4992), tensor(1.4892), tensor(1.5989), tensor(1.4946), tensor(2.2272), tensor(1.4965), tensor(1.4885), tensor(1.4782), tensor(1.4776), tensor(1.5369), tensor(1.5424), tensor(1.5318), tensor(2.4642), tensor(1.5555), tensor(1.5171), tensor(1.6985), tensor(2.7934), tensor(1.4861), tensor(1.6860), tensor(1.5364), tensor(1.6510), tensor(1.8123), tensor(1.5697), tensor(1.4994), tensor(1.4738), tensor(1.8192), tensor(4.9175), tensor(1.5476), tensor(1.4825), tensor(1.5251), tensor(1.4928), tensor(1.5521), tensor(1.6239), tensor(1.4652), tensor(1.5346), tensor(1.4994), tensor(1.5183), tensor(1.5962), tensor(1.6166), tensor(1.5060), tensor(1.6703), tensor(1.4433), tensor(1.5350), tensor(1.4929), tensor(1.8980), tensor(1.6545), tensor(1.5977), tensor(1.5225), tensor(1.5102), tensor(1.5449), tensor(1.5118), tensor(1.6496), tensor(1.5445), tensor(1.5259), tensor(1.5724), tensor(1.4788), tensor(1.5307), tensor(1.7315), tensor(1.4940), tensor(1.5295), tensor(2.5440), tensor(1.4784), tensor(1.4943), tensor(2.7936), tensor(1.5510), tensor(1.5467), tensor(1.5329), tensor(2.0249), tensor(1.4446), tensor(1.5090), tensor(4.3609), tensor(1.5966), tensor(1.5051), tensor(1.4879), tensor(1.8828), tensor(1.4442), tensor(1.5181), tensor(1.5242), tensor(1.5605), tensor(1.4959), tensor(1.5141), tensor(1.5061), tensor(1.5446), tensor(1.4965), tensor(1.5448), tensor(1.5427), tensor(1.4868), tensor(1.4846), tensor(1.4911), tensor(1.4999), tensor(1.5230), tensor(1.4838), tensor(1.4901), tensor(1.4930), tensor(1.5481), tensor(1.5439), tensor(1.6452), tensor(1.5283), tensor(1.4893), tensor(1.7305), tensor(2.2155), tensor(1.4750), tensor(1.5100), tensor(1.4935), tensor(1.4664), tensor(1.5372), tensor(1.5034), tensor(1.5144), tensor(1.5196), tensor(1.6024), tensor(1.4957), tensor(1.4846), tensor(1.7158), tensor(1.9770), tensor(1.5280), tensor(1.4845), tensor(1.4698), tensor(1.5438), tensor(1.5388), tensor(1.5491), tensor(1.5626), tensor(1.4944), tensor(1.5020), tensor(1.4695), tensor(2.0631), tensor(1.7795), tensor(1.4748), tensor(1.4719), tensor(1.5666), tensor(2.0235), tensor(1.4827), tensor(1.4761), tensor(1.5228), tensor(1.5366), tensor(1.5542), tensor(1.5736), tensor(1.5471), tensor(1.5748), tensor(1.5100), tensor(1.5189), tensor(1.4781), tensor(1.5083), tensor(1.4878), tensor(1.6317), tensor(1.4484), tensor(1.5484), tensor(1.5593), tensor(1.4967), tensor(2.0677), tensor(1.5443), tensor(1.4999), tensor(1.5935), tensor(1.7324), tensor(1.6711), tensor(1.5380), tensor(1.5384), tensor(1.5240), tensor(1.5122), tensor(1.5023), tensor(1.5136), tensor(1.4802), tensor(1.5903), tensor(1.9450), tensor(1.5406), tensor(1.4659), tensor(1.5308), tensor(1.5919), tensor(1.8347), tensor(1.6749), tensor(1.5744), tensor(1.5560), tensor(1.5217), tensor(2.9422), tensor(1.5205), tensor(1.4612), tensor(2.8681), tensor(1.4855), tensor(1.6245), tensor(1.5069), tensor(1.4950), tensor(1.5280), tensor(1.5498), tensor(1.9694), tensor(1.5677), tensor(2.7369), tensor(1.5095), tensor(1.5386), tensor(1.5595), tensor(3.8086), tensor(1.4971), tensor(1.5615), tensor(1.5617), tensor(2.0235), tensor(1.4786), tensor(1.4683)]
09/23/2021 14:13:08 - INFO - __main__ - after_losses=[tensor(1.5268), tensor(1.4674), tensor(1.5745), tensor(1.5213), tensor(2.3623), tensor(1.4884), tensor(1.5283), tensor(1.4876), tensor(1.5302), tensor(1.4594), tensor(1.5018), tensor(1.5047), tensor(1.5536), tensor(1.4798), tensor(1.5169), tensor(1.5917), tensor(1.6076), tensor(1.4714), tensor(2.2239), tensor(1.4905), tensor(1.4806), tensor(2.5799), tensor(1.5259), tensor(1.4576), tensor(1.4656), tensor(1.4626), tensor(1.5194), tensor(1.4848), tensor(1.4699), tensor(1.4496), tensor(1.7594), tensor(1.6179), tensor(1.4928), tensor(1.6386), tensor(1.5976), tensor(1.7795), tensor(1.4724), tensor(2.1766), tensor(1.4904), tensor(1.5592), tensor(1.4922), tensor(1.5058), tensor(1.4845), tensor(1.5061), tensor(1.5279), tensor(1.4916), tensor(1.5495), tensor(1.5278), tensor(1.5082), tensor(1.4695), tensor(1.5420), tensor(1.6082), tensor(1.5241), tensor(2.0428), tensor(1.4960), tensor(1.4976), tensor(1.4817), tensor(1.5547), tensor(1.5840), tensor(1.5023), tensor(1.6310), tensor(1.4989), tensor(1.4870), tensor(1.5924), tensor(1.4903), tensor(1.9405), tensor(1.4909), tensor(1.4864), tensor(1.4732), tensor(1.5087), tensor(1.5328), tensor(1.5453), tensor(1.5548), tensor(2.3426), tensor(1.5575), tensor(1.5013), tensor(1.6956), tensor(2.5860), tensor(1.4569), tensor(1.7013), tensor(1.5233), tensor(1.6358), tensor(1.7347), tensor(1.6927), tensor(1.4862), tensor(1.4730), tensor(1.7795), tensor(4.0318), tensor(1.5286), tensor(1.4810), tensor(1.5036), tensor(1.4884), tensor(1.5931), tensor(1.6778), tensor(1.4602), tensor(1.5056), tensor(1.4852), tensor(1.4868), tensor(1.6222), tensor(1.6284), tensor(1.4974), tensor(1.6513), tensor(1.4383), tensor(1.5160), tensor(1.4819), tensor(1.6193), tensor(1.6865), tensor(1.5874), tensor(1.5050), tensor(1.4883), tensor(1.5365), tensor(1.5037), tensor(1.6254), tensor(1.5287), tensor(1.5211), tensor(1.5580), tensor(1.4796), tensor(1.5274), tensor(1.5174), tensor(1.4903), tensor(1.5150), tensor(2.4826), tensor(1.4710), tensor(1.4805), tensor(2.8858), tensor(1.5940), tensor(1.5287), tensor(1.5130), tensor(1.8032), tensor(1.4422), tensor(1.4902), tensor(4.1219), tensor(1.5553), tensor(1.4934), tensor(1.4869), tensor(1.5143), tensor(1.4395), tensor(1.5051), tensor(1.5297), tensor(1.4926), tensor(1.4919), tensor(1.5001), tensor(1.5047), tensor(1.5717), tensor(1.4992), tensor(1.6569), tensor(1.5240), tensor(1.4938), tensor(1.5263), tensor(1.4901), tensor(1.4937), tensor(1.4925), tensor(1.4758), tensor(1.4851), tensor(1.4964), tensor(1.5260), tensor(1.5956), tensor(1.6681), tensor(1.5698), tensor(1.4926), tensor(1.5621), tensor(2.3498), tensor(1.5027), tensor(1.5000), tensor(1.5053), tensor(1.4679), tensor(1.5375), tensor(1.4949), tensor(1.5196), tensor(1.4997), tensor(1.7132), tensor(1.4911), tensor(1.5056), tensor(1.8642), tensor(1.8146), tensor(1.5203), tensor(1.4746), tensor(1.4706), tensor(1.5504), tensor(1.5231), tensor(1.5073), tensor(1.5446), tensor(1.4815), tensor(1.4937), tensor(1.4679), tensor(2.1138), tensor(1.7125), tensor(1.4828), tensor(1.4715), tensor(1.6023), tensor(1.9623), tensor(1.4733), tensor(1.4688), tensor(1.5210), tensor(1.5176), tensor(1.6186), tensor(1.5495), tensor(1.5388), tensor(1.6005), tensor(1.4976), tensor(1.4966), tensor(1.4720), tensor(1.5082), tensor(1.5513), tensor(1.6464), tensor(1.4536), tensor(1.5155), tensor(1.5780), tensor(1.4952), tensor(2.0127), tensor(1.5272), tensor(1.4877), tensor(1.5908), tensor(1.8231), tensor(1.6925), tensor(1.5261), tensor(1.5058), tensor(1.5140), tensor(1.5057), tensor(1.5143), tensor(1.5036), tensor(1.4742), tensor(1.8067), tensor(1.9024), tensor(2.0211), tensor(1.4576), tensor(1.5358), tensor(1.5871), tensor(1.6729), tensor(1.5843), tensor(1.5599), tensor(1.5294), tensor(1.5064), tensor(2.6748), tensor(1.5059), tensor(1.4646), tensor(2.6330), tensor(1.4979), tensor(1.5725), tensor(1.4962), tensor(1.4885), tensor(1.4915), tensor(1.5392), tensor(1.9032), tensor(1.5273), tensor(2.7328), tensor(1.4873), tensor(1.5432), tensor(1.9644), tensor(3.8875), tensor(1.4669), tensor(1.5489), tensor(1.5183), tensor(1.8648), tensor(1.4778), tensor(1.4624)]
09/23/2021 14:13:09 - INFO - __main__ - interference_scores=[tensor(-0.0039), tensor(-0.0119), tensor(0.0112), tensor(0.0029), tensor(0.0379), tensor(0.0061), tensor(0.0084), tensor(-0.2824), tensor(-0.0076), tensor(-0.0075), tensor(-0.0143), tensor(-0.0234), tensor(0.0206), tensor(-0.0005), tensor(-0.0167), tensor(0.0387), tensor(0.0143), tensor(-0.0352), tensor(0.0284), tensor(-0.0097), tensor(0.0007), tensor(0.0073), tensor(-0.0132), tensor(-0.0098), tensor(-0.0097), tensor(-0.0059), tensor(-0.0079), tensor(-0.0393), tensor(0.0035), tensor(0.0025), tensor(0.0720), tensor(-0.2010), tensor(-0.0119), tensor(0.0484), tensor(0.0157), tensor(0.2308), tensor(-0.0167), tensor(0.0420), tensor(-0.0070), tensor(-0.0230), tensor(0.0072), tensor(-0.0039), tensor(0.0038), tensor(-0.0013), tensor(-0.0129), tensor(-0.0092), tensor(-0.0152), tensor(-0.0121), tensor(-0.0005), tensor(-0.0178), tensor(0.0031), tensor(-0.7033), tensor(-0.0060), tensor(0.1971), tensor(-0.0025), tensor(0.0126), tensor(-0.0069), tensor(-0.0025), tensor(-0.0951), tensor(-0.0142), tensor(-0.1134), tensor(-0.0003), tensor(-0.0022), tensor(-0.0065), tensor(-0.0043), tensor(-0.2867), tensor(-0.0057), tensor(-0.0021), tensor(-0.0050), tensor(0.0311), tensor(-0.0041), tensor(0.0029), tensor(0.0230), tensor(-0.1216), tensor(0.0020), tensor(-0.0158), tensor(-0.0029), tensor(-0.2075), tensor(-0.0293), tensor(0.0154), tensor(-0.0131), tensor(-0.0152), tensor(-0.0776), tensor(0.1230), tensor(-0.0132), tensor(-0.0008), tensor(-0.0397), tensor(-0.8856), tensor(-0.0190), tensor(-0.0015), tensor(-0.0214), tensor(-0.0044), tensor(0.0410), tensor(0.0539), tensor(-0.0050), tensor(-0.0291), tensor(-0.0141), tensor(-0.0315), tensor(0.0261), tensor(0.0118), tensor(-0.0086), tensor(-0.0191), tensor(-0.0050), tensor(-0.0190), tensor(-0.0110), tensor(-0.2787), tensor(0.0320), tensor(-0.0103), tensor(-0.0175), tensor(-0.0219), tensor(-0.0085), tensor(-0.0081), tensor(-0.0243), tensor(-0.0158), tensor(-0.0049), tensor(-0.0144), tensor(0.0008), tensor(-0.0033), tensor(-0.2142), tensor(-0.0037), tensor(-0.0144), tensor(-0.0614), tensor(-0.0074), tensor(-0.0138), tensor(0.0923), tensor(0.0430), tensor(-0.0180), tensor(-0.0199), tensor(-0.2217), tensor(-0.0024), tensor(-0.0188), tensor(-0.2390), tensor(-0.0413), tensor(-0.0117), tensor(-0.0009), tensor(-0.3685), tensor(-0.0046), tensor(-0.0130), tensor(0.0054), tensor(-0.0679), tensor(-0.0040), tensor(-0.0140), tensor(-0.0014), tensor(0.0271), tensor(0.0028), tensor(0.1121), tensor(-0.0187), tensor(0.0071), tensor(0.0417), tensor(-0.0009), tensor(-0.0063), tensor(-0.0305), tensor(-0.0081), tensor(-0.0050), tensor(0.0035), tensor(-0.0222), tensor(0.0517), tensor(0.0229), tensor(0.0415), tensor(0.0033), tensor(-0.1684), tensor(0.1343), tensor(0.0277), tensor(-0.0100), tensor(0.0118), tensor(0.0015), tensor(0.0003), tensor(-0.0085), tensor(0.0053), tensor(-0.0199), tensor(0.1108), tensor(-0.0046), tensor(0.0209), tensor(0.1484), tensor(-0.1625), tensor(-0.0077), tensor(-0.0099), tensor(0.0007), tensor(0.0066), tensor(-0.0158), tensor(-0.0418), tensor(-0.0180), tensor(-0.0129), tensor(-0.0083), tensor(-0.0015), tensor(0.0508), tensor(-0.0670), tensor(0.0080), tensor(-0.0004), tensor(0.0358), tensor(-0.0612), tensor(-0.0094), tensor(-0.0073), tensor(-0.0018), tensor(-0.0190), tensor(0.0645), tensor(-0.0241), tensor(-0.0083), tensor(0.0258), tensor(-0.0124), tensor(-0.0223), tensor(-0.0061), tensor(-0.0001), tensor(0.0635), tensor(0.0147), tensor(0.0052), tensor(-0.0330), tensor(0.0186), tensor(-0.0015), tensor(-0.0550), tensor(-0.0171), tensor(-0.0122), tensor(-0.0027), tensor(0.0907), tensor(0.0214), tensor(-0.0119), tensor(-0.0326), tensor(-0.0100), tensor(-0.0065), tensor(0.0120), tensor(-0.0101), tensor(-0.0060), tensor(0.2163), tensor(-0.0426), tensor(0.4804), tensor(-0.0083), tensor(0.0051), tensor(-0.0047), tensor(-0.1618), tensor(-0.0906), tensor(-0.0146), tensor(-0.0266), tensor(-0.0153), tensor(-0.2674), tensor(-0.0146), tensor(0.0034), tensor(-0.2352), tensor(0.0125), tensor(-0.0521), tensor(-0.0107), tensor(-0.0065), tensor(-0.0365), tensor(-0.0106), tensor(-0.0662), tensor(-0.0404), tensor(-0.0041), tensor(-0.0222), tensor(0.0046), tensor(0.4048), tensor(0.0789), tensor(-0.0302), tensor(-0.0126), tensor(-0.0433), tensor(-0.1587), tensor(-0.0008), tensor(-0.0059)]
09/23/2021 14:13:09 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-2454', 'mrqa_triviaqa-validation-6935', 'mrqa_naturalquestions-validation-2064', 'mrqa_triviaqa-validation-5852', 'mrqa_triviaqa-validation-7512', 'mrqa_triviaqa-validation-4583', 'mrqa_triviaqa-validation-2368', 'mrqa_hotpotqa-validation-1441', 'mrqa_triviaqa-validation-2797', 'mrqa_triviaqa-validation-298', 'mrqa_naturalquestions-validation-2666', 'mrqa_squad-validation-9751', 'mrqa_triviaqa-validation-1995', 'mrqa_squad-validation-9214', 'mrqa_naturalquestions-validation-4653', 'mrqa_squad-validation-8456', 'mrqa_triviaqa-validation-667', 'mrqa_triviaqa-validation-5429', 'mrqa_triviaqa-validation-6438', 'mrqa_squad-validation-2769', 'mrqa_triviaqa-validation-1306', 'mrqa_squad-validation-2884', 'mrqa_triviaqa-validation-6721', 'mrqa_triviaqa-validation-1924', 'mrqa_hotpotqa-validation-2382', 'mrqa_hotpotqa-validation-335', 'mrqa_squad-validation-3964', 'mrqa_triviaqa-validation-3945', 'mrqa_naturalquestions-validation-5180', 'mrqa_hotpotqa-validation-5848', 'mrqa_triviaqa-validation-7578', 'mrqa_triviaqa-validation-7157']
09/23/2021 14:13:09 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:13:09 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=56) .... Timecode: 26
09/23/2021 14:13:21 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:13:21 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 26
09/23/2021 14:13:25 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 14:13:25 - INFO - __main__ - Instant Fixing Rate: 1.0
09/23/2021 14:13:25 - INFO - __main__ - Instant Retention Rate: 0.6249999992187499
09/23/2021 14:13:27 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_027.pt.
09/23/2021 14:13:27 - INFO - __main__ - Saving the current error examples (len=24) to the memory.
09/23/2021 14:13:27 - INFO - __main__ - Current memory size: 689.
09/23/2021 14:13:27 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:13:27 - INFO - __main__ - Finished.
09/23/2021 14:13:27 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:13:27 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:13:27 - INFO - __main__ - Evaluating to get errors .... Timecode: 27
09/23/2021 14:13:30 - INFO - __main__ - Before Error Fixing: 0.09375
09/23/2021 14:13:30 - INFO - __main__ - Found 29 errors.
09/23/2021 14:13:30 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:13:30 - INFO - __main__ - Current memory size: 713.
09/23/2021 14:13:30 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:13:53 - INFO - __main__ - before_losses=[tensor(1.8860), tensor(1.5205), tensor(1.7713), tensor(1.8898), tensor(1.4749), tensor(1.5095), tensor(1.4967), tensor(1.4849), tensor(1.4934), tensor(1.5089), tensor(1.7014), tensor(1.5387), tensor(1.4960), tensor(1.4866), tensor(1.9685), tensor(1.7492), tensor(1.5234), tensor(1.4905), tensor(1.5589), tensor(1.5159), tensor(1.6399), tensor(1.4841), tensor(1.8559), tensor(2.0233), tensor(1.5454), tensor(1.7380), tensor(1.5163), tensor(1.8497), tensor(1.5445), tensor(1.5759), tensor(1.5211), tensor(1.4589), tensor(1.8035), tensor(1.4737), tensor(2.2183), tensor(1.4517), tensor(1.5796), tensor(1.9387), tensor(1.4655), tensor(1.5811), tensor(2.0414), tensor(1.5477), tensor(1.5496), tensor(1.5318), tensor(1.8181), tensor(1.5162), tensor(1.5472), tensor(2.1499), tensor(1.5342), tensor(1.4402), tensor(1.4707), tensor(1.6703), tensor(1.4788), tensor(2.4915), tensor(1.4414), tensor(1.8709), tensor(1.5249), tensor(1.4835), tensor(2.0877), tensor(1.7631), tensor(1.7124), tensor(1.5422), tensor(1.5433), tensor(1.5205), tensor(1.4770), tensor(1.4500), tensor(1.5571), tensor(1.5203), tensor(1.4864), tensor(1.4843), tensor(1.4912), tensor(1.4879), tensor(1.5138), tensor(1.5291), tensor(1.4859), tensor(2.5584), tensor(1.6578), tensor(2.3123), tensor(1.4922), tensor(1.5418), tensor(1.4950), tensor(1.5298), tensor(1.4775), tensor(1.6178), tensor(2.7229), tensor(1.7481), tensor(1.6257), tensor(3.5086), tensor(1.4900), tensor(1.4929), tensor(1.5141), tensor(1.4879), tensor(1.5359), tensor(1.4744), tensor(2.3259), tensor(1.5396), tensor(1.5049), tensor(1.8379), tensor(1.5359), tensor(1.5544), tensor(1.4712), tensor(1.5153), tensor(1.4981), tensor(1.6692), tensor(4.1765), tensor(1.5193), tensor(1.4643), tensor(1.5510), tensor(1.4924), tensor(1.4656), tensor(1.5357), tensor(1.4930), tensor(1.9517), tensor(1.4941), tensor(1.4803), tensor(2.0277), tensor(1.4978), tensor(1.4984), tensor(1.4621), tensor(1.5599), tensor(1.5716), tensor(1.5218), tensor(1.5172), tensor(1.4793), tensor(1.4633), tensor(1.5338), tensor(1.5361), tensor(1.5905), tensor(1.5102), tensor(1.6826), tensor(1.4834), tensor(1.5065), tensor(1.4629), tensor(1.4683), tensor(1.4882), tensor(1.5500), tensor(1.4644), tensor(1.6363), tensor(3.6022), tensor(1.4896), tensor(1.7637), tensor(1.5296), tensor(1.6801), tensor(1.5395), tensor(1.7642), tensor(2.1177), tensor(1.6152), tensor(1.6299), tensor(2.3874), tensor(1.5783), tensor(2.3042), tensor(1.5781), tensor(2.1574), tensor(1.5190), tensor(1.7766), tensor(1.4925), tensor(4.3117), tensor(1.5562), tensor(1.4862), tensor(1.4815), tensor(1.4336), tensor(1.4886), tensor(1.4830), tensor(1.4716), tensor(1.6947), tensor(1.7097), tensor(1.5378), tensor(1.4939), tensor(1.4898), tensor(1.5091), tensor(1.5356), tensor(1.4794), tensor(1.4771), tensor(1.4554), tensor(1.5248), tensor(1.5109), tensor(1.6014), tensor(1.5259), tensor(1.6387), tensor(1.4577), tensor(1.5329), tensor(1.9632), tensor(1.4719), tensor(1.5069), tensor(1.4836), tensor(1.4925), tensor(1.4701), tensor(1.5034), tensor(1.6380), tensor(1.5093), tensor(1.4782), tensor(1.4812), tensor(1.4576), tensor(1.4599), tensor(1.5249), tensor(1.5110), tensor(1.6115), tensor(1.4777), tensor(1.5268), tensor(1.4844), tensor(1.6618), tensor(1.5170), tensor(1.4650), tensor(1.5107), tensor(1.5067), tensor(1.5941), tensor(1.8779), tensor(1.4995), tensor(1.5356), tensor(1.5266), tensor(1.8818), tensor(1.5147), tensor(1.5651), tensor(1.5660), tensor(1.5518), tensor(1.4519), tensor(1.4986), tensor(1.4456), tensor(1.5737), tensor(1.5438), tensor(1.4854), tensor(1.4630), tensor(2.1166), tensor(1.5114), tensor(2.3016), tensor(2.5918), tensor(1.5354), tensor(1.5061), tensor(2.0085), tensor(1.6118), tensor(1.4695), tensor(2.1071), tensor(1.6828), tensor(1.6060), tensor(1.4994), tensor(1.5254), tensor(1.5315), tensor(1.5762), tensor(2.1284), tensor(1.6748), tensor(1.7202), tensor(1.4999), tensor(1.5580), tensor(1.4523), tensor(1.4586), tensor(1.5576), tensor(1.5237), tensor(1.5124), tensor(1.6109), tensor(1.5963), tensor(1.5874), tensor(1.5367), tensor(1.5738), tensor(1.6821), tensor(1.4882), tensor(1.6268)]
09/23/2021 14:13:53 - INFO - __main__ - after_losses=[tensor(1.8717), tensor(1.5067), tensor(1.7922), tensor(1.7181), tensor(1.4683), tensor(1.5072), tensor(1.4919), tensor(1.4619), tensor(1.4963), tensor(1.4850), tensor(1.7073), tensor(1.5034), tensor(1.4857), tensor(1.4865), tensor(1.8298), tensor(2.6424), tensor(1.5239), tensor(1.4854), tensor(1.5774), tensor(1.4868), tensor(1.6008), tensor(1.4828), tensor(1.6169), tensor(1.8833), tensor(1.4850), tensor(1.5068), tensor(1.5024), tensor(1.8333), tensor(1.5343), tensor(1.5359), tensor(1.5075), tensor(1.4565), tensor(1.4853), tensor(1.4639), tensor(1.4634), tensor(1.4434), tensor(1.6285), tensor(1.9176), tensor(1.4643), tensor(1.5639), tensor(1.9297), tensor(1.5705), tensor(1.5248), tensor(1.5133), tensor(1.8301), tensor(1.5012), tensor(1.7430), tensor(1.9440), tensor(1.5224), tensor(1.4412), tensor(1.4600), tensor(1.6236), tensor(1.4855), tensor(2.4073), tensor(1.4418), tensor(2.1669), tensor(1.5063), tensor(1.4813), tensor(2.1789), tensor(1.7854), tensor(1.6032), tensor(1.4987), tensor(1.5527), tensor(1.5077), tensor(1.4661), tensor(1.4734), tensor(1.5439), tensor(1.4985), tensor(1.4690), tensor(1.4684), tensor(1.5214), tensor(1.4602), tensor(1.5686), tensor(1.5093), tensor(1.4628), tensor(2.5354), tensor(1.5665), tensor(2.2962), tensor(1.4839), tensor(1.5123), tensor(1.4926), tensor(1.5311), tensor(1.4678), tensor(1.6343), tensor(2.6041), tensor(1.6608), tensor(2.0643), tensor(2.9847), tensor(1.4761), tensor(1.4715), tensor(1.5134), tensor(1.4783), tensor(1.5287), tensor(1.4689), tensor(2.0117), tensor(1.5284), tensor(1.4922), tensor(1.4968), tensor(1.6302), tensor(1.5364), tensor(1.4809), tensor(1.4948), tensor(1.4882), tensor(1.7928), tensor(4.0803), tensor(1.5088), tensor(1.4564), tensor(1.6260), tensor(1.4855), tensor(1.5695), tensor(1.5293), tensor(1.4830), tensor(2.0057), tensor(1.4797), tensor(1.4723), tensor(2.2890), tensor(1.4861), tensor(1.4954), tensor(1.4560), tensor(1.5741), tensor(1.5398), tensor(1.5281), tensor(1.5088), tensor(1.4851), tensor(1.4673), tensor(1.5141), tensor(1.4915), tensor(1.6585), tensor(1.5023), tensor(1.8216), tensor(1.4885), tensor(1.4985), tensor(1.4564), tensor(1.4641), tensor(1.4664), tensor(1.5406), tensor(1.4621), tensor(1.7123), tensor(3.2642), tensor(1.4786), tensor(1.4902), tensor(1.5098), tensor(1.6996), tensor(1.5316), tensor(1.8537), tensor(2.3924), tensor(1.5634), tensor(1.5899), tensor(1.8798), tensor(1.5344), tensor(2.2454), tensor(1.5515), tensor(2.2490), tensor(1.4981), tensor(1.8358), tensor(1.4861), tensor(4.3964), tensor(1.5602), tensor(1.4770), tensor(1.4711), tensor(1.4326), tensor(1.4783), tensor(1.4697), tensor(1.4716), tensor(1.6265), tensor(1.6015), tensor(1.5148), tensor(1.4777), tensor(1.4649), tensor(1.4995), tensor(1.5215), tensor(1.4638), tensor(1.4642), tensor(1.4627), tensor(1.5110), tensor(1.5411), tensor(1.5713), tensor(1.5122), tensor(1.4976), tensor(1.4628), tensor(1.5193), tensor(2.1001), tensor(1.4728), tensor(1.4999), tensor(1.4842), tensor(1.4779), tensor(1.4533), tensor(1.4719), tensor(1.5710), tensor(1.5047), tensor(1.4891), tensor(1.4769), tensor(1.4465), tensor(1.4530), tensor(1.5112), tensor(1.4960), tensor(1.5748), tensor(1.4781), tensor(1.5108), tensor(1.4792), tensor(1.5131), tensor(1.5690), tensor(1.4498), tensor(1.5008), tensor(1.4671), tensor(1.5072), tensor(1.8520), tensor(1.4779), tensor(1.5058), tensor(1.5285), tensor(2.0869), tensor(1.5405), tensor(1.5785), tensor(1.5075), tensor(1.5278), tensor(1.4559), tensor(1.4834), tensor(1.4479), tensor(1.5597), tensor(1.5001), tensor(1.4744), tensor(1.4645), tensor(1.8936), tensor(1.4828), tensor(2.1171), tensor(2.6377), tensor(1.5085), tensor(1.5208), tensor(1.7693), tensor(1.6073), tensor(1.4692), tensor(2.2139), tensor(1.6524), tensor(2.9721), tensor(1.5022), tensor(1.5098), tensor(1.5065), tensor(1.5762), tensor(1.9714), tensor(1.5911), tensor(2.1144), tensor(1.4852), tensor(1.5453), tensor(1.4440), tensor(1.4482), tensor(1.5381), tensor(1.5023), tensor(1.4908), tensor(1.5556), tensor(1.5848), tensor(1.5557), tensor(1.5265), tensor(1.5674), tensor(1.6721), tensor(1.4716), tensor(1.5305)]
09/23/2021 14:13:53 - INFO - __main__ - interference_scores=[tensor(-0.0143), tensor(-0.0139), tensor(0.0210), tensor(-0.1716), tensor(-0.0066), tensor(-0.0024), tensor(-0.0047), tensor(-0.0230), tensor(0.0029), tensor(-0.0240), tensor(0.0058), tensor(-0.0353), tensor(-0.0103), tensor(-0.0001), tensor(-0.1386), tensor(0.8932), tensor(0.0005), tensor(-0.0051), tensor(0.0185), tensor(-0.0291), tensor(-0.0391), tensor(-0.0012), tensor(-0.2390), tensor(-0.1401), tensor(-0.0603), tensor(-0.2312), tensor(-0.0138), tensor(-0.0164), tensor(-0.0103), tensor(-0.0400), tensor(-0.0137), tensor(-0.0023), tensor(-0.3182), tensor(-0.0098), tensor(-0.7548), tensor(-0.0082), tensor(0.0489), tensor(-0.0211), tensor(-0.0012), tensor(-0.0173), tensor(-0.1118), tensor(0.0228), tensor(-0.0248), tensor(-0.0184), tensor(0.0120), tensor(-0.0150), tensor(0.1958), tensor(-0.2059), tensor(-0.0118), tensor(0.0011), tensor(-0.0107), tensor(-0.0467), tensor(0.0067), tensor(-0.0841), tensor(0.0004), tensor(0.2960), tensor(-0.0186), tensor(-0.0022), tensor(0.0912), tensor(0.0223), tensor(-0.1092), tensor(-0.0434), tensor(0.0095), tensor(-0.0127), tensor(-0.0109), tensor(0.0234), tensor(-0.0132), tensor(-0.0217), tensor(-0.0174), tensor(-0.0158), tensor(0.0303), tensor(-0.0276), tensor(0.0549), tensor(-0.0199), tensor(-0.0231), tensor(-0.0230), tensor(-0.0913), tensor(-0.0161), tensor(-0.0083), tensor(-0.0295), tensor(-0.0024), tensor(0.0013), tensor(-0.0097), tensor(0.0166), tensor(-0.1188), tensor(-0.0873), tensor(0.4386), tensor(-0.5239), tensor(-0.0138), tensor(-0.0214), tensor(-0.0007), tensor(-0.0095), tensor(-0.0072), tensor(-0.0054), tensor(-0.3141), tensor(-0.0112), tensor(-0.0127), tensor(-0.3411), tensor(0.0943), tensor(-0.0179), tensor(0.0098), tensor(-0.0205), tensor(-0.0099), tensor(0.1236), tensor(-0.0962), tensor(-0.0105), tensor(-0.0080), tensor(0.0749), tensor(-0.0068), tensor(0.1038), tensor(-0.0065), tensor(-0.0100), tensor(0.0540), tensor(-0.0144), tensor(-0.0080), tensor(0.2613), tensor(-0.0118), tensor(-0.0030), tensor(-0.0061), tensor(0.0142), tensor(-0.0318), tensor(0.0063), tensor(-0.0084), tensor(0.0058), tensor(0.0040), tensor(-0.0197), tensor(-0.0446), tensor(0.0680), tensor(-0.0080), tensor(0.1390), tensor(0.0050), tensor(-0.0081), tensor(-0.0065), tensor(-0.0042), tensor(-0.0218), tensor(-0.0094), tensor(-0.0023), tensor(0.0759), tensor(-0.3379), tensor(-0.0110), tensor(-0.2735), tensor(-0.0198), tensor(0.0195), tensor(-0.0079), tensor(0.0895), tensor(0.2747), tensor(-0.0518), tensor(-0.0400), tensor(-0.5076), tensor(-0.0439), tensor(-0.0588), tensor(-0.0266), tensor(0.0916), tensor(-0.0209), tensor(0.0593), tensor(-0.0065), tensor(0.0847), tensor(0.0040), tensor(-0.0091), tensor(-0.0104), tensor(-0.0010), tensor(-0.0103), tensor(-0.0133), tensor(1.3113e-06), tensor(-0.0682), tensor(-0.1081), tensor(-0.0230), tensor(-0.0162), tensor(-0.0249), tensor(-0.0095), tensor(-0.0141), tensor(-0.0157), tensor(-0.0129), tensor(0.0073), tensor(-0.0138), tensor(0.0302), tensor(-0.0301), tensor(-0.0137), tensor(-0.1411), tensor(0.0051), tensor(-0.0135), tensor(0.1369), tensor(0.0010), tensor(-0.0071), tensor(0.0007), tensor(-0.0146), tensor(-0.0169), tensor(-0.0315), tensor(-0.0669), tensor(-0.0046), tensor(0.0109), tensor(-0.0043), tensor(-0.0110), tensor(-0.0069), tensor(-0.0137), tensor(-0.0150), tensor(-0.0367), tensor(0.0004), tensor(-0.0160), tensor(-0.0052), tensor(-0.1487), tensor(0.0520), tensor(-0.0151), tensor(-0.0099), tensor(-0.0396), tensor(-0.0869), tensor(-0.0259), tensor(-0.0215), tensor(-0.0298), tensor(0.0019), tensor(0.2051), tensor(0.0259), tensor(0.0133), tensor(-0.0585), tensor(-0.0241), tensor(0.0040), tensor(-0.0152), tensor(0.0023), tensor(-0.0141), tensor(-0.0437), tensor(-0.0110), tensor(0.0015), tensor(-0.2230), tensor(-0.0287), tensor(-0.1845), tensor(0.0458), tensor(-0.0269), tensor(0.0148), tensor(-0.2392), tensor(-0.0044), tensor(-0.0004), tensor(0.1068), tensor(-0.0305), tensor(1.3661), tensor(0.0028), tensor(-0.0156), tensor(-0.0250), tensor(2.1338e-05), tensor(-0.1570), tensor(-0.0837), tensor(0.3942), tensor(-0.0147), tensor(-0.0128), tensor(-0.0083), tensor(-0.0103), tensor(-0.0195), tensor(-0.0215), tensor(-0.0217), tensor(-0.0554), tensor(-0.0115), tensor(-0.0317), tensor(-0.0102), tensor(-0.0064), tensor(-0.0100), tensor(-0.0166), tensor(-0.0963)]
09/23/2021 14:13:53 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-1995', 'mrqa_squad-validation-3964', 'mrqa_triviaqa-validation-6944', 'mrqa_triviaqa-validation-5852', 'mrqa_squad-validation-8464', 'mrqa_triviaqa-validation-6872', 'mrqa_triviaqa-validation-7336', 'mrqa_triviaqa-validation-2551', 'mrqa_triviaqa-validation-7415', 'mrqa_triviaqa-validation-4068', 'mrqa_triviaqa-validation-1764', 'mrqa_hotpotqa-validation-35', 'mrqa_squad-validation-6677', 'mrqa_naturalquestions-validation-3309', 'mrqa_hotpotqa-validation-5699', 'mrqa_triviaqa-validation-5877', 'mrqa_triviaqa-validation-4055', 'mrqa_triviaqa-validation-1259', 'mrqa_triviaqa-validation-2442', 'mrqa_squad-validation-2812', 'mrqa_hotpotqa-validation-2679', 'mrqa_triviaqa-validation-116', 'mrqa_squad-validation-392', 'mrqa_triviaqa-validation-2996', 'mrqa_squad-validation-5622', 'mrqa_hotpotqa-validation-4015', 'mrqa_triviaqa-validation-3716', 'mrqa_squad-validation-4506', 'mrqa_naturalquestions-validation-10347', 'mrqa_triviaqa-validation-6683', 'mrqa_hotpotqa-validation-2262', 'mrqa_squad-validation-358']
09/23/2021 14:13:53 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:13:53 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=61) .... Timecode: 27
09/23/2021 14:14:07 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:14:07 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 27
09/23/2021 14:14:10 - INFO - __main__ - After Error Fixing: 0.9375
09/23/2021 14:14:10 - INFO - __main__ - Instant Fixing Rate: 0.9310344827586207
09/23/2021 14:14:10 - INFO - __main__ - Instant Retention Rate: 0.9999999966666667
09/23/2021 14:14:12 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_028.pt.
09/23/2021 14:14:12 - INFO - __main__ - Saving the current error examples (len=29) to the memory.
09/23/2021 14:14:12 - INFO - __main__ - Current memory size: 713.
09/23/2021 14:14:12 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:14:12 - INFO - __main__ - Finished.
09/23/2021 14:14:12 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:14:12 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:14:12 - INFO - __main__ - Evaluating to get errors .... Timecode: 28
09/23/2021 14:14:16 - INFO - __main__ - Before Error Fixing: 0.125
09/23/2021 14:14:16 - INFO - __main__ - Found 28 errors.
09/23/2021 14:14:16 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:14:16 - INFO - __main__ - Current memory size: 742.
09/23/2021 14:14:16 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:14:39 - INFO - __main__ - before_losses=[tensor(1.4972), tensor(1.4814), tensor(1.4755), tensor(1.5291), tensor(1.5142), tensor(1.5768), tensor(1.5325), tensor(1.5687), tensor(1.5119), tensor(1.5936), tensor(1.5198), tensor(1.5566), tensor(2.2377), tensor(1.5431), tensor(1.5001), tensor(1.5392), tensor(1.5194), tensor(1.4851), tensor(1.5333), tensor(1.4485), tensor(1.4836), tensor(1.5191), tensor(1.5523), tensor(1.4963), tensor(1.5607), tensor(1.4532), tensor(1.9718), tensor(1.4576), tensor(1.5271), tensor(1.5827), tensor(1.4997), tensor(1.4899), tensor(5.1556), tensor(1.5184), tensor(1.5102), tensor(1.5888), tensor(1.8554), tensor(1.4661), tensor(1.5013), tensor(1.4745), tensor(1.8224), tensor(1.5462), tensor(1.4600), tensor(1.5376), tensor(1.6998), tensor(1.4783), tensor(1.5340), tensor(1.5733), tensor(1.5190), tensor(1.4974), tensor(2.0614), tensor(1.5170), tensor(1.4712), tensor(1.4909), tensor(1.5450), tensor(1.9613), tensor(2.8695), tensor(1.6290), tensor(1.5072), tensor(1.7402), tensor(1.5908), tensor(1.4863), tensor(1.8390), tensor(1.5961), tensor(2.4358), tensor(1.5222), tensor(1.5266), tensor(1.4949), tensor(1.5261), tensor(1.4655), tensor(1.5708), tensor(1.6620), tensor(1.8173), tensor(1.5806), tensor(1.5049), tensor(1.4688), tensor(1.4982), tensor(1.5417), tensor(1.6089), tensor(2.1694), tensor(2.8389), tensor(1.4859), tensor(1.5205), tensor(1.6015), tensor(2.2098), tensor(1.5016), tensor(1.7608), tensor(1.5233), tensor(1.5101), tensor(1.5010), tensor(1.4822), tensor(1.5370), tensor(1.4875), tensor(2.7233), tensor(2.2393), tensor(2.7165), tensor(1.4809), tensor(1.5318), tensor(1.5502), tensor(1.4899), tensor(1.4859), tensor(1.5293), tensor(1.5991), tensor(1.9614), tensor(1.5413), tensor(1.5831), tensor(1.5374), tensor(1.4788), tensor(1.7301), tensor(1.6252), tensor(1.4769), tensor(1.5255), tensor(1.4422), tensor(1.5046), tensor(1.5241), tensor(1.5249), tensor(1.4960), tensor(1.7913), tensor(1.4773), tensor(1.5381), tensor(1.4803), tensor(1.5457), tensor(1.4946), tensor(1.4969), tensor(1.5005), tensor(1.5340), tensor(1.4966), tensor(1.6889), tensor(1.4686), tensor(1.8769), tensor(1.5252), tensor(1.5138), tensor(1.7139), tensor(3.0423), tensor(2.4482), tensor(2.5233), tensor(1.5224), tensor(1.5604), tensor(1.5043), tensor(1.5086), tensor(1.4923), tensor(1.4959), tensor(1.5149), tensor(1.4829), tensor(1.5323), tensor(1.5428), tensor(2.2515), tensor(1.4669), tensor(1.5341), tensor(1.5315), tensor(1.5013), tensor(1.4820), tensor(2.2805), tensor(1.8914), tensor(1.5417), tensor(1.5282), tensor(1.5496), tensor(1.5050), tensor(1.8842), tensor(1.7501), tensor(1.7265), tensor(1.9555), tensor(1.4872), tensor(1.5981), tensor(1.4706), tensor(1.5377), tensor(1.4777), tensor(1.5327), tensor(1.5489), tensor(1.5113), tensor(1.5190), tensor(1.4793), tensor(1.5585), tensor(2.2725), tensor(1.5580), tensor(1.5106), tensor(1.4949), tensor(1.5525), tensor(2.1562), tensor(1.5062), tensor(1.6034), tensor(1.4566), tensor(1.5398), tensor(1.5133), tensor(1.4930), tensor(1.5464), tensor(1.5373), tensor(1.5095), tensor(1.4828), tensor(1.5423), tensor(1.5097), tensor(1.5044), tensor(1.5010), tensor(1.4983), tensor(1.5335), tensor(1.5167), tensor(2.2934), tensor(1.9554), tensor(1.5112), tensor(1.5643), tensor(1.5653), tensor(1.4814), tensor(1.5763), tensor(1.5770), tensor(1.4920), tensor(2.2234), tensor(1.4990), tensor(1.5016), tensor(1.5061), tensor(1.4653), tensor(1.5236), tensor(1.5155), tensor(1.6000), tensor(1.4788), tensor(1.4471), tensor(1.5237), tensor(1.4634), tensor(1.6068), tensor(1.4874), tensor(1.6514), tensor(1.5400), tensor(1.4803), tensor(2.3980), tensor(1.4669), tensor(1.5532), tensor(1.4971), tensor(2.8753), tensor(1.4916), tensor(1.5365), tensor(2.4014), tensor(2.0455), tensor(1.7334), tensor(2.4318), tensor(1.5065), tensor(2.6545), tensor(1.4888), tensor(1.4978), tensor(1.5209), tensor(1.5180), tensor(1.5907), tensor(1.5152), tensor(1.5417), tensor(1.6894), tensor(1.5393), tensor(1.4836), tensor(1.7287), tensor(1.5116), tensor(1.6109), tensor(1.7096), tensor(2.1686), tensor(1.5228), tensor(1.5270), tensor(1.5466), tensor(1.4996), tensor(1.5533), tensor(1.4603)]
09/23/2021 14:14:39 - INFO - __main__ - after_losses=[tensor(1.4947), tensor(1.4742), tensor(1.4599), tensor(1.5211), tensor(1.4910), tensor(1.5720), tensor(1.5129), tensor(1.5155), tensor(1.4941), tensor(1.5792), tensor(1.5026), tensor(1.5227), tensor(2.1522), tensor(1.5257), tensor(1.4989), tensor(1.4923), tensor(1.4951), tensor(1.5113), tensor(1.5109), tensor(1.4507), tensor(1.4657), tensor(1.5014), tensor(1.5251), tensor(1.4849), tensor(1.5508), tensor(1.4453), tensor(1.6537), tensor(1.4601), tensor(1.5091), tensor(1.5519), tensor(1.4854), tensor(1.4861), tensor(5.0702), tensor(1.5150), tensor(1.5160), tensor(1.5593), tensor(1.7500), tensor(1.4706), tensor(1.4924), tensor(1.4657), tensor(1.8888), tensor(1.5513), tensor(1.4525), tensor(1.4881), tensor(1.5977), tensor(1.4630), tensor(1.5093), tensor(1.5232), tensor(1.5037), tensor(1.4890), tensor(1.9874), tensor(1.4974), tensor(1.4470), tensor(1.4846), tensor(1.5237), tensor(1.8249), tensor(2.7446), tensor(1.6613), tensor(1.4867), tensor(1.7300), tensor(1.5681), tensor(1.4711), tensor(1.6781), tensor(1.5815), tensor(1.4945), tensor(1.5076), tensor(1.5334), tensor(1.4798), tensor(1.5334), tensor(1.4752), tensor(1.6003), tensor(1.6000), tensor(2.1263), tensor(1.8368), tensor(1.4779), tensor(1.4572), tensor(1.5894), tensor(1.5235), tensor(1.5967), tensor(2.0211), tensor(3.0427), tensor(1.4815), tensor(1.5112), tensor(1.5239), tensor(2.2431), tensor(1.4914), tensor(1.6578), tensor(1.5169), tensor(1.4951), tensor(1.4882), tensor(1.4740), tensor(1.5363), tensor(1.4674), tensor(2.9144), tensor(2.1456), tensor(2.4089), tensor(1.4596), tensor(1.5280), tensor(1.5688), tensor(1.4718), tensor(1.4811), tensor(1.5079), tensor(1.6971), tensor(1.9437), tensor(1.5131), tensor(1.6682), tensor(1.5112), tensor(1.4574), tensor(1.6828), tensor(1.5718), tensor(1.4525), tensor(1.5071), tensor(1.4404), tensor(1.4868), tensor(1.5114), tensor(1.5133), tensor(1.4759), tensor(1.6689), tensor(1.4691), tensor(1.5473), tensor(1.4685), tensor(1.5244), tensor(1.4757), tensor(1.4836), tensor(1.6590), tensor(1.5541), tensor(1.4819), tensor(1.6402), tensor(1.4656), tensor(2.0772), tensor(1.5092), tensor(1.4798), tensor(1.9908), tensor(2.9567), tensor(2.2593), tensor(2.5342), tensor(1.5305), tensor(1.5457), tensor(1.4968), tensor(1.5016), tensor(1.4823), tensor(1.4769), tensor(2.1912), tensor(1.4724), tensor(1.4780), tensor(1.5143), tensor(2.3423), tensor(1.4576), tensor(1.5034), tensor(1.5029), tensor(1.4867), tensor(1.4736), tensor(2.2897), tensor(1.9121), tensor(1.5168), tensor(1.5200), tensor(1.5384), tensor(1.4985), tensor(2.4922), tensor(1.6484), tensor(1.6992), tensor(2.0970), tensor(1.4784), tensor(1.5909), tensor(1.4690), tensor(1.5394), tensor(1.4732), tensor(1.4962), tensor(1.5303), tensor(1.4907), tensor(1.4899), tensor(1.4684), tensor(1.5422), tensor(2.2626), tensor(1.6673), tensor(1.4931), tensor(1.4921), tensor(1.5473), tensor(2.2428), tensor(1.5140), tensor(1.5849), tensor(1.4534), tensor(1.5139), tensor(1.4963), tensor(1.4862), tensor(1.5208), tensor(1.5127), tensor(1.4874), tensor(1.4684), tensor(1.5091), tensor(1.5087), tensor(1.5260), tensor(1.5096), tensor(1.4828), tensor(1.5608), tensor(1.4969), tensor(2.4682), tensor(1.9929), tensor(1.4968), tensor(1.6354), tensor(1.5598), tensor(1.4746), tensor(1.6263), tensor(1.5235), tensor(1.4761), tensor(2.1263), tensor(1.4878), tensor(1.5155), tensor(1.5197), tensor(1.4645), tensor(1.5173), tensor(1.5008), tensor(1.5611), tensor(1.4719), tensor(1.4509), tensor(1.5143), tensor(1.4533), tensor(1.5966), tensor(1.4633), tensor(1.5681), tensor(1.5167), tensor(1.4718), tensor(2.6006), tensor(1.4612), tensor(1.5448), tensor(1.4857), tensor(2.6502), tensor(1.4857), tensor(1.5147), tensor(2.3094), tensor(2.0415), tensor(1.7555), tensor(2.8980), tensor(1.4776), tensor(2.6116), tensor(1.4781), tensor(1.5153), tensor(1.4980), tensor(1.4903), tensor(1.5199), tensor(1.4957), tensor(1.5201), tensor(2.0481), tensor(1.5046), tensor(1.4814), tensor(1.5520), tensor(1.4948), tensor(1.5948), tensor(1.6948), tensor(2.1452), tensor(1.4835), tensor(1.5069), tensor(1.5394), tensor(1.5232), tensor(1.5968), tensor(1.4538)]
09/23/2021 14:14:40 - INFO - __main__ - interference_scores=[tensor(-0.0024), tensor(-0.0073), tensor(-0.0156), tensor(-0.0079), tensor(-0.0231), tensor(-0.0048), tensor(-0.0197), tensor(-0.0531), tensor(-0.0178), tensor(-0.0144), tensor(-0.0172), tensor(-0.0339), tensor(-0.0855), tensor(-0.0174), tensor(-0.0011), tensor(-0.0469), tensor(-0.0244), tensor(0.0263), tensor(-0.0224), tensor(0.0022), tensor(-0.0179), tensor(-0.0178), tensor(-0.0273), tensor(-0.0114), tensor(-0.0098), tensor(-0.0079), tensor(-0.3181), tensor(0.0025), tensor(-0.0179), tensor(-0.0308), tensor(-0.0143), tensor(-0.0038), tensor(-0.0854), tensor(-0.0033), tensor(0.0058), tensor(-0.0296), tensor(-0.1054), tensor(0.0044), tensor(-0.0089), tensor(-0.0087), tensor(0.0664), tensor(0.0051), tensor(-0.0075), tensor(-0.0495), tensor(-0.1021), tensor(-0.0153), tensor(-0.0248), tensor(-0.0501), tensor(-0.0152), tensor(-0.0084), tensor(-0.0739), tensor(-0.0195), tensor(-0.0242), tensor(-0.0062), tensor(-0.0213), tensor(-0.1364), tensor(-0.1248), tensor(0.0323), tensor(-0.0205), tensor(-0.0101), tensor(-0.0226), tensor(-0.0152), tensor(-0.1609), tensor(-0.0145), tensor(-0.9413), tensor(-0.0146), tensor(0.0068), tensor(-0.0152), tensor(0.0073), tensor(0.0097), tensor(0.0296), tensor(-0.0620), tensor(0.3091), tensor(0.2562), tensor(-0.0271), tensor(-0.0116), tensor(0.0912), tensor(-0.0182), tensor(-0.0122), tensor(-0.1483), tensor(0.2038), tensor(-0.0044), tensor(-0.0093), tensor(-0.0775), tensor(0.0333), tensor(-0.0103), tensor(-0.1030), tensor(-0.0064), tensor(-0.0150), tensor(-0.0128), tensor(-0.0082), tensor(-0.0006), tensor(-0.0201), tensor(0.1911), tensor(-0.0936), tensor(-0.3076), tensor(-0.0213), tensor(-0.0038), tensor(0.0186), tensor(-0.0182), tensor(-0.0048), tensor(-0.0214), tensor(0.0980), tensor(-0.0178), tensor(-0.0281), tensor(0.0851), tensor(-0.0261), tensor(-0.0214), tensor(-0.0473), tensor(-0.0533), tensor(-0.0244), tensor(-0.0184), tensor(-0.0018), tensor(-0.0179), tensor(-0.0127), tensor(-0.0116), tensor(-0.0202), tensor(-0.1225), tensor(-0.0083), tensor(0.0092), tensor(-0.0118), tensor(-0.0213), tensor(-0.0189), tensor(-0.0133), tensor(0.1585), tensor(0.0202), tensor(-0.0146), tensor(-0.0486), tensor(-0.0029), tensor(0.2003), tensor(-0.0160), tensor(-0.0339), tensor(0.2769), tensor(-0.0857), tensor(-0.1889), tensor(0.0109), tensor(0.0080), tensor(-0.0148), tensor(-0.0075), tensor(-0.0070), tensor(-0.0100), tensor(-0.0190), tensor(0.6763), tensor(-0.0104), tensor(-0.0543), tensor(-0.0286), tensor(0.0908), tensor(-0.0093), tensor(-0.0307), tensor(-0.0286), tensor(-0.0145), tensor(-0.0084), tensor(0.0091), tensor(0.0207), tensor(-0.0249), tensor(-0.0083), tensor(-0.0112), tensor(-0.0065), tensor(0.6081), tensor(-0.1017), tensor(-0.0273), tensor(0.1415), tensor(-0.0088), tensor(-0.0072), tensor(-0.0015), tensor(0.0018), tensor(-0.0045), tensor(-0.0364), tensor(-0.0186), tensor(-0.0206), tensor(-0.0290), tensor(-0.0109), tensor(-0.0164), tensor(-0.0099), tensor(0.1092), tensor(-0.0175), tensor(-0.0028), tensor(-0.0052), tensor(0.0866), tensor(0.0078), tensor(-0.0185), tensor(-0.0032), tensor(-0.0259), tensor(-0.0170), tensor(-0.0068), tensor(-0.0256), tensor(-0.0247), tensor(-0.0220), tensor(-0.0145), tensor(-0.0331), tensor(-0.0010), tensor(0.0216), tensor(0.0086), tensor(-0.0155), tensor(0.0273), tensor(-0.0198), tensor(0.1749), tensor(0.0375), tensor(-0.0143), tensor(0.0711), tensor(-0.0055), tensor(-0.0067), tensor(0.0499), tensor(-0.0534), tensor(-0.0159), tensor(-0.0971), tensor(-0.0112), tensor(0.0138), tensor(0.0136), tensor(-0.0008), tensor(-0.0062), tensor(-0.0147), tensor(-0.0389), tensor(-0.0069), tensor(0.0038), tensor(-0.0094), tensor(-0.0101), tensor(-0.0102), tensor(-0.0240), tensor(-0.0832), tensor(-0.0232), tensor(-0.0085), tensor(0.2026), tensor(-0.0057), tensor(-0.0084), tensor(-0.0114), tensor(-0.2251), tensor(-0.0059), tensor(-0.0218), tensor(-0.0920), tensor(-0.0040), tensor(0.0221), tensor(0.4662), tensor(-0.0289), tensor(-0.0429), tensor(-0.0107), tensor(0.0174), tensor(-0.0230), tensor(-0.0277), tensor(-0.0708), tensor(-0.0195), tensor(-0.0217), tensor(0.3586), tensor(-0.0346), tensor(-0.0022), tensor(-0.1767), tensor(-0.0168), tensor(-0.0161), tensor(-0.0148), tensor(-0.0234), tensor(-0.0393), tensor(-0.0201), tensor(-0.0072), tensor(0.0235), tensor(0.0435), tensor(-0.0065)]
09/23/2021 14:14:40 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-8464', 'mrqa_triviaqa-validation-1423', 'mrqa_triviaqa-validation-622', 'mrqa_triviaqa-validation-5698', 'mrqa_triviaqa-validation-1954', 'mrqa_triviaqa-validation-6091', 'mrqa_triviaqa-validation-2551', 'mrqa_triviaqa-validation-3595', 'mrqa_triviaqa-validation-2368', 'mrqa_triviaqa-validation-1293', 'mrqa_naturalquestions-validation-2248', 'mrqa_triviaqa-validation-4731', 'mrqa_squad-validation-8739', 'mrqa_triviaqa-validation-4966', 'mrqa_triviaqa-validation-7371', 'mrqa_hotpotqa-validation-1099', 'mrqa_triviaqa-validation-227', 'mrqa_triviaqa-validation-1550', 'mrqa_naturalquestions-validation-5017', 'mrqa_naturalquestions-validation-3474', 'mrqa_triviaqa-validation-1995', 'mrqa_squad-validation-5273', 'mrqa_naturalquestions-validation-1135', 'mrqa_squad-validation-7447', 'mrqa_naturalquestions-validation-390', 'mrqa_triviaqa-validation-3716', 'mrqa_naturalquestions-validation-5538', 'mrqa_triviaqa-validation-365', 'mrqa_triviaqa-validation-1034', 'mrqa_hotpotqa-validation-499', 'mrqa_triviaqa-validation-2007', 'mrqa_naturalquestions-validation-5769']
09/23/2021 14:14:40 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:14:40 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=60) .... Timecode: 28
09/23/2021 14:14:53 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:14:53 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 28
09/23/2021 14:14:57 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 14:14:57 - INFO - __main__ - Instant Fixing Rate: 0.9285714285714286
09/23/2021 14:14:57 - INFO - __main__ - Instant Retention Rate: 0.7499999981250001
09/23/2021 14:14:59 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_029.pt.
09/23/2021 14:14:59 - INFO - __main__ - Saving the current error examples (len=28) to the memory.
09/23/2021 14:14:59 - INFO - __main__ - Current memory size: 742.
09/23/2021 14:14:59 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:14:59 - INFO - __main__ - Finished.
09/23/2021 14:14:59 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:14:59 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:14:59 - INFO - __main__ - Evaluating to get errors .... Timecode: 29
09/23/2021 14:15:02 - INFO - __main__ - Before Error Fixing: 0.125
09/23/2021 14:15:02 - INFO - __main__ - Found 28 errors.
09/23/2021 14:15:02 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:15:02 - INFO - __main__ - Current memory size: 742.
09/23/2021 14:15:02 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:15:26 - INFO - __main__ - before_losses=[tensor(1.4865), tensor(1.4948), tensor(1.5600), tensor(1.5779), tensor(1.5961), tensor(1.4983), tensor(1.5156), tensor(1.4979), tensor(1.4957), tensor(1.4909), tensor(1.5089), tensor(1.5162), tensor(1.5032), tensor(1.9694), tensor(1.4527), tensor(2.4074), tensor(1.4839), tensor(1.6186), tensor(1.5394), tensor(1.4817), tensor(1.5103), tensor(1.4641), tensor(1.6655), tensor(1.5603), tensor(1.4738), tensor(1.4607), tensor(2.0275), tensor(1.5764), tensor(2.6387), tensor(1.5755), tensor(2.5124), tensor(1.5384), tensor(1.5839), tensor(1.8794), tensor(1.4988), tensor(1.6000), tensor(1.5216), tensor(3.2304), tensor(1.5012), tensor(2.5242), tensor(1.5077), tensor(1.4843), tensor(1.5040), tensor(1.4685), tensor(1.5796), tensor(1.4939), tensor(1.7702), tensor(1.6832), tensor(1.5717), tensor(1.5659), tensor(1.4512), tensor(1.9213), tensor(1.5176), tensor(1.6105), tensor(1.5439), tensor(1.4847), tensor(1.4753), tensor(1.4621), tensor(2.3545), tensor(1.5277), tensor(2.0637), tensor(1.5515), tensor(1.4950), tensor(1.6454), tensor(1.5522), tensor(1.5244), tensor(1.5927), tensor(1.5005), tensor(1.4688), tensor(1.8250), tensor(1.6040), tensor(1.5608), tensor(1.4863), tensor(1.5616), tensor(1.5045), tensor(3.0014), tensor(1.5996), tensor(1.4947), tensor(1.4807), tensor(1.4876), tensor(1.4642), tensor(1.5074), tensor(1.5227), tensor(1.4534), tensor(1.4896), tensor(1.5709), tensor(1.5212), tensor(1.5598), tensor(1.5407), tensor(1.5254), tensor(1.4700), tensor(1.5190), tensor(1.4904), tensor(1.7634), tensor(1.4792), tensor(1.4876), tensor(1.4706), tensor(1.5796), tensor(1.4785), tensor(1.6087), tensor(1.5657), tensor(1.4868), tensor(1.5013), tensor(2.4089), tensor(1.5433), tensor(1.5002), tensor(1.4672), tensor(1.5365), tensor(1.5047), tensor(1.4842), tensor(2.2258), tensor(1.5138), tensor(1.5323), tensor(1.5099), tensor(1.5878), tensor(1.5986), tensor(1.5327), tensor(1.4500), tensor(1.7388), tensor(1.4864), tensor(1.8759), tensor(2.7101), tensor(1.5703), tensor(1.5801), tensor(1.5434), tensor(1.4870), tensor(1.4886), tensor(1.5710), tensor(1.5353), tensor(1.5442), tensor(1.8021), tensor(1.5538), tensor(1.5178), tensor(1.5561), tensor(1.5069), tensor(1.5507), tensor(1.4675), tensor(1.5914), tensor(2.6681), tensor(1.5512), tensor(1.5214), tensor(1.5276), tensor(1.5407), tensor(1.5129), tensor(1.4899), tensor(1.5331), tensor(1.5310), tensor(1.6049), tensor(1.5104), tensor(1.4787), tensor(1.6266), tensor(1.5017), tensor(1.4987), tensor(2.2475), tensor(1.4540), tensor(1.5249), tensor(1.5387), tensor(1.4880), tensor(1.5225), tensor(2.1453), tensor(1.4971), tensor(1.5521), tensor(1.4902), tensor(1.4750), tensor(1.4761), tensor(1.5333), tensor(1.5370), tensor(2.3754), tensor(1.5345), tensor(1.4776), tensor(1.7553), tensor(1.4607), tensor(1.6660), tensor(1.4801), tensor(1.5816), tensor(1.5585), tensor(1.5097), tensor(1.5965), tensor(1.5886), tensor(1.9859), tensor(2.2312), tensor(1.4714), tensor(1.4910), tensor(2.7986), tensor(1.4886), tensor(1.5108), tensor(2.7103), tensor(1.5550), tensor(1.5490), tensor(1.7962), tensor(1.9012), tensor(1.6236), tensor(1.5543), tensor(1.5259), tensor(1.5370), tensor(1.5304), tensor(1.5841), tensor(1.5713), tensor(1.5079), tensor(1.5008), tensor(1.5292), tensor(1.4859), tensor(1.5153), tensor(1.8120), tensor(1.7594), tensor(1.4703), tensor(1.5652), tensor(1.6064), tensor(1.5063), tensor(1.5189), tensor(1.6014), tensor(4.9843), tensor(1.6706), tensor(1.4933), tensor(1.5630), tensor(1.5030), tensor(1.5675), tensor(1.4964), tensor(1.4681), tensor(1.5143), tensor(1.8843), tensor(1.6166), tensor(1.6681), tensor(1.5339), tensor(1.5314), tensor(1.7699), tensor(1.4799), tensor(1.8876), tensor(1.7314), tensor(1.5005), tensor(1.4619), tensor(1.4903), tensor(1.4849), tensor(1.4828), tensor(1.4717), tensor(1.5053), tensor(1.4778), tensor(1.5306), tensor(1.4889), tensor(1.8314), tensor(1.5326), tensor(1.8336), tensor(1.5148), tensor(1.4677), tensor(1.5215), tensor(1.4810), tensor(1.5179), tensor(2.1341), tensor(1.5631), tensor(1.4995), tensor(1.5633), tensor(1.5109), tensor(1.5028), tensor(1.5037), tensor(1.4882), tensor(1.9953)]
09/23/2021 14:15:26 - INFO - __main__ - after_losses=[tensor(1.4708), tensor(1.4749), tensor(1.5410), tensor(1.5892), tensor(1.6069), tensor(1.4865), tensor(1.4888), tensor(1.4892), tensor(1.4727), tensor(1.5090), tensor(1.4879), tensor(1.4874), tensor(1.4923), tensor(1.9909), tensor(1.4440), tensor(2.3719), tensor(1.4873), tensor(1.6064), tensor(1.5977), tensor(1.4710), tensor(1.4854), tensor(1.4529), tensor(1.6034), tensor(2.0176), tensor(1.4699), tensor(1.4463), tensor(1.9471), tensor(1.5663), tensor(2.7106), tensor(1.5542), tensor(2.3722), tensor(1.7770), tensor(1.5761), tensor(1.9412), tensor(1.4878), tensor(1.6178), tensor(1.5189), tensor(3.2096), tensor(1.4852), tensor(2.3870), tensor(1.4776), tensor(1.4766), tensor(1.5032), tensor(1.4846), tensor(1.6321), tensor(1.4713), tensor(1.8597), tensor(1.9555), tensor(1.5265), tensor(1.5451), tensor(1.4421), tensor(1.9742), tensor(1.5180), tensor(1.5832), tensor(1.5366), tensor(1.4721), tensor(1.4670), tensor(1.4520), tensor(2.2562), tensor(1.5144), tensor(1.4969), tensor(1.5345), tensor(1.4931), tensor(1.6870), tensor(1.5267), tensor(1.5126), tensor(1.8312), tensor(1.4844), tensor(1.4573), tensor(1.8618), tensor(1.5538), tensor(1.5602), tensor(1.4712), tensor(1.5580), tensor(1.4885), tensor(3.1589), tensor(1.5680), tensor(1.4767), tensor(1.4697), tensor(1.4706), tensor(1.4531), tensor(1.5129), tensor(1.5080), tensor(1.4415), tensor(1.4742), tensor(1.5467), tensor(1.5171), tensor(1.5183), tensor(1.5014), tensor(1.4998), tensor(1.4595), tensor(1.5012), tensor(1.4745), tensor(1.7152), tensor(1.4587), tensor(1.4863), tensor(1.4854), tensor(1.6322), tensor(1.4622), tensor(1.6006), tensor(1.5459), tensor(1.4676), tensor(1.4803), tensor(2.2695), tensor(1.5403), tensor(1.4969), tensor(1.4462), tensor(1.5860), tensor(1.4976), tensor(1.4636), tensor(2.1004), tensor(1.5001), tensor(1.5309), tensor(1.5043), tensor(2.2302), tensor(1.5752), tensor(1.5217), tensor(1.4439), tensor(2.2563), tensor(1.4725), tensor(1.9202), tensor(2.5436), tensor(1.5415), tensor(1.5450), tensor(1.5298), tensor(1.4800), tensor(1.4902), tensor(1.5674), tensor(1.5226), tensor(1.5175), tensor(1.7532), tensor(1.5327), tensor(1.5114), tensor(1.4949), tensor(1.4933), tensor(1.5373), tensor(1.4573), tensor(1.5664), tensor(2.5832), tensor(1.5428), tensor(1.5016), tensor(1.5013), tensor(1.5574), tensor(1.4954), tensor(1.4727), tensor(1.5105), tensor(1.5137), tensor(1.6595), tensor(1.4736), tensor(1.4608), tensor(1.6056), tensor(1.4730), tensor(1.4693), tensor(2.5595), tensor(1.4580), tensor(1.4994), tensor(1.5175), tensor(1.4785), tensor(1.5587), tensor(2.2339), tensor(1.4917), tensor(1.5435), tensor(1.4840), tensor(1.4641), tensor(1.4680), tensor(1.5158), tensor(1.5310), tensor(2.2442), tensor(1.5160), tensor(1.4627), tensor(1.7607), tensor(1.4518), tensor(1.6776), tensor(1.4730), tensor(1.5189), tensor(1.5212), tensor(1.4944), tensor(1.5485), tensor(1.5594), tensor(1.9719), tensor(1.7213), tensor(1.4746), tensor(1.4962), tensor(2.6913), tensor(1.4745), tensor(1.5039), tensor(2.6870), tensor(1.6267), tensor(1.5376), tensor(1.5881), tensor(2.0045), tensor(1.7316), tensor(1.5360), tensor(1.5520), tensor(1.5107), tensor(1.4990), tensor(1.5688), tensor(1.5477), tensor(1.4893), tensor(1.4791), tensor(1.5016), tensor(1.4898), tensor(1.7649), tensor(1.8345), tensor(1.7767), tensor(1.4672), tensor(1.5674), tensor(1.5742), tensor(1.4830), tensor(1.4983), tensor(1.6286), tensor(4.6788), tensor(1.7582), tensor(1.4804), tensor(1.5517), tensor(1.4826), tensor(1.5834), tensor(1.4838), tensor(1.4582), tensor(1.5086), tensor(2.2060), tensor(1.5945), tensor(1.7281), tensor(1.5161), tensor(1.5168), tensor(1.7193), tensor(1.4705), tensor(1.9326), tensor(1.6530), tensor(1.4926), tensor(1.4630), tensor(1.4756), tensor(1.4716), tensor(1.4796), tensor(1.4672), tensor(1.4987), tensor(1.4689), tensor(1.6118), tensor(1.4780), tensor(1.6680), tensor(1.5125), tensor(1.8132), tensor(1.4983), tensor(1.4440), tensor(1.5006), tensor(1.4727), tensor(1.7643), tensor(2.1869), tensor(1.5175), tensor(1.4734), tensor(1.4939), tensor(1.4900), tensor(1.4926), tensor(1.4969), tensor(1.4623), tensor(2.0302)]
09/23/2021 14:15:26 - INFO - __main__ - interference_scores=[tensor(-0.0157), tensor(-0.0200), tensor(-0.0190), tensor(0.0114), tensor(0.0108), tensor(-0.0119), tensor(-0.0267), tensor(-0.0087), tensor(-0.0230), tensor(0.0181), tensor(-0.0210), tensor(-0.0288), tensor(-0.0109), tensor(0.0215), tensor(-0.0088), tensor(-0.0355), tensor(0.0034), tensor(-0.0122), tensor(0.0583), tensor(-0.0107), tensor(-0.0249), tensor(-0.0111), tensor(-0.0621), tensor(0.4572), tensor(-0.0039), tensor(-0.0145), tensor(-0.0805), tensor(-0.0101), tensor(0.0719), tensor(-0.0213), tensor(-0.1402), tensor(0.2386), tensor(-0.0078), tensor(0.0618), tensor(-0.0110), tensor(0.0178), tensor(-0.0027), tensor(-0.0208), tensor(-0.0160), tensor(-0.1373), tensor(-0.0302), tensor(-0.0077), tensor(-0.0008), tensor(0.0161), tensor(0.0525), tensor(-0.0226), tensor(0.0895), tensor(0.2723), tensor(-0.0452), tensor(-0.0208), tensor(-0.0092), tensor(0.0530), tensor(0.0004), tensor(-0.0273), tensor(-0.0073), tensor(-0.0125), tensor(-0.0083), tensor(-0.0101), tensor(-0.0983), tensor(-0.0133), tensor(-0.5668), tensor(-0.0170), tensor(-0.0019), tensor(0.0415), tensor(-0.0255), tensor(-0.0118), tensor(0.2385), tensor(-0.0162), tensor(-0.0114), tensor(0.0368), tensor(-0.0501), tensor(-0.0007), tensor(-0.0150), tensor(-0.0037), tensor(-0.0161), tensor(0.1575), tensor(-0.0316), tensor(-0.0181), tensor(-0.0110), tensor(-0.0170), tensor(-0.0111), tensor(0.0055), tensor(-0.0146), tensor(-0.0119), tensor(-0.0153), tensor(-0.0242), tensor(-0.0041), tensor(-0.0415), tensor(-0.0393), tensor(-0.0255), tensor(-0.0105), tensor(-0.0177), tensor(-0.0158), tensor(-0.0482), tensor(-0.0204), tensor(-0.0013), tensor(0.0148), tensor(0.0526), tensor(-0.0163), tensor(-0.0081), tensor(-0.0198), tensor(-0.0192), tensor(-0.0210), tensor(-0.1394), tensor(-0.0031), tensor(-0.0033), tensor(-0.0209), tensor(0.0495), tensor(-0.0071), tensor(-0.0206), tensor(-0.1254), tensor(-0.0137), tensor(-0.0014), tensor(-0.0056), tensor(0.6424), tensor(-0.0234), tensor(-0.0110), tensor(-0.0061), tensor(0.5175), tensor(-0.0139), tensor(0.0443), tensor(-0.1665), tensor(-0.0288), tensor(-0.0351), tensor(-0.0135), tensor(-0.0070), tensor(0.0016), tensor(-0.0036), tensor(-0.0127), tensor(-0.0268), tensor(-0.0490), tensor(-0.0211), tensor(-0.0064), tensor(-0.0612), tensor(-0.0137), tensor(-0.0134), tensor(-0.0101), tensor(-0.0250), tensor(-0.0849), tensor(-0.0084), tensor(-0.0198), tensor(-0.0262), tensor(0.0167), tensor(-0.0176), tensor(-0.0171), tensor(-0.0226), tensor(-0.0172), tensor(0.0545), tensor(-0.0369), tensor(-0.0178), tensor(-0.0210), tensor(-0.0287), tensor(-0.0294), tensor(0.3120), tensor(0.0040), tensor(-0.0254), tensor(-0.0212), tensor(-0.0095), tensor(0.0362), tensor(0.0886), tensor(-0.0053), tensor(-0.0085), tensor(-0.0061), tensor(-0.0109), tensor(-0.0081), tensor(-0.0175), tensor(-0.0060), tensor(-0.1312), tensor(-0.0184), tensor(-0.0149), tensor(0.0055), tensor(-0.0089), tensor(0.0117), tensor(-0.0071), tensor(-0.0626), tensor(-0.0374), tensor(-0.0153), tensor(-0.0481), tensor(-0.0292), tensor(-0.0139), tensor(-0.5099), tensor(0.0033), tensor(0.0052), tensor(-0.1073), tensor(-0.0141), tensor(-0.0069), tensor(-0.0232), tensor(0.0717), tensor(-0.0114), tensor(-0.2082), tensor(0.1033), tensor(0.1079), tensor(-0.0184), tensor(0.0261), tensor(-0.0263), tensor(-0.0314), tensor(-0.0153), tensor(-0.0236), tensor(-0.0185), tensor(-0.0217), tensor(-0.0276), tensor(0.0038), tensor(0.2496), tensor(0.0226), tensor(0.0173), tensor(-0.0031), tensor(0.0022), tensor(-0.0322), tensor(-0.0233), tensor(-0.0206), tensor(0.0272), tensor(-0.3055), tensor(0.0876), tensor(-0.0130), tensor(-0.0113), tensor(-0.0205), tensor(0.0159), tensor(-0.0126), tensor(-0.0099), tensor(-0.0057), tensor(0.3217), tensor(-0.0221), tensor(0.0600), tensor(-0.0178), tensor(-0.0146), tensor(-0.0505), tensor(-0.0093), tensor(0.0450), tensor(-0.0783), tensor(-0.0080), tensor(0.0011), tensor(-0.0147), tensor(-0.0133), tensor(-0.0032), tensor(-0.0045), tensor(-0.0066), tensor(-0.0089), tensor(0.0812), tensor(-0.0109), tensor(-0.1634), tensor(-0.0201), tensor(-0.0203), tensor(-0.0166), tensor(-0.0236), tensor(-0.0209), tensor(-0.0082), tensor(0.2464), tensor(0.0528), tensor(-0.0456), tensor(-0.0261), tensor(-0.0694), tensor(-0.0209), tensor(-0.0102), tensor(-0.0069), tensor(-0.0259), tensor(0.0349)]
09/23/2021 14:15:26 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-1423', 'mrqa_squad-validation-7447', 'mrqa_squad-validation-8739', 'mrqa_triviaqa-validation-2530', 'mrqa_triviaqa-validation-2179', 'mrqa_triviaqa-validation-7304', 'mrqa_squad-validation-2313', 'mrqa_hotpotqa-validation-3757', 'mrqa_hotpotqa-validation-4812', 'mrqa_triviaqa-validation-1451', 'mrqa_naturalquestions-validation-98', 'mrqa_triviaqa-validation-4856', 'mrqa_triviaqa-validation-2376', 'mrqa_triviaqa-validation-7336', 'mrqa_triviaqa-validation-6689', 'mrqa_triviaqa-validation-3901', 'mrqa_naturalquestions-validation-8744', 'mrqa_squad-validation-4417', 'mrqa_squad-validation-1914', 'mrqa_squad-validation-6811', 'mrqa_triviaqa-validation-1306', 'mrqa_squad-validation-392', 'mrqa_triviaqa-validation-7100', 'mrqa_squad-validation-3181', 'mrqa_triviaqa-validation-316', 'mrqa_triviaqa-validation-314', 'mrqa_squad-validation-9214', 'mrqa_triviaqa-validation-6699', 'mrqa_triviaqa-validation-2454', 'mrqa_squad-validation-3558', 'mrqa_triviaqa-validation-866', 'mrqa_squad-validation-7149']
09/23/2021 14:15:26 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:15:26 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=60) .... Timecode: 29
09/23/2021 14:15:39 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:15:39 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 29
09/23/2021 14:15:42 - INFO - __main__ - After Error Fixing: 0.9375
09/23/2021 14:15:42 - INFO - __main__ - Instant Fixing Rate: 0.9642857142857143
09/23/2021 14:15:42 - INFO - __main__ - Instant Retention Rate: 0.7499999981250001
09/23/2021 14:15:44 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_030.pt.
09/23/2021 14:15:44 - INFO - __main__ - Saving the current error examples (len=28) to the memory.
09/23/2021 14:15:44 - INFO - __main__ - Current memory size: 742.
09/23/2021 14:15:44 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:15:44 - INFO - __main__ - Finished.
09/23/2021 14:15:44 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:15:44 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:15:44 - INFO - __main__ - Evaluating to get errors .... Timecode: 30
09/23/2021 14:15:48 - INFO - __main__ - Before Error Fixing: 0.125
09/23/2021 14:15:48 - INFO - __main__ - Found 28 errors.
09/23/2021 14:15:48 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:15:48 - INFO - __main__ - Current memory size: 770.
09/23/2021 14:15:48 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:16:11 - INFO - __main__ - before_losses=[tensor(1.7171), tensor(1.4958), tensor(1.9181), tensor(1.4819), tensor(1.5074), tensor(1.7445), tensor(1.5145), tensor(1.4900), tensor(1.4600), tensor(1.4933), tensor(1.4783), tensor(1.4613), tensor(1.4735), tensor(1.4959), tensor(1.5124), tensor(1.5406), tensor(1.5228), tensor(1.7832), tensor(1.4841), tensor(1.5408), tensor(1.4568), tensor(1.4928), tensor(1.4941), tensor(1.4717), tensor(1.5294), tensor(1.5099), tensor(1.5105), tensor(1.5420), tensor(1.5396), tensor(1.4659), tensor(1.4699), tensor(1.4825), tensor(1.5747), tensor(1.7272), tensor(1.5154), tensor(1.4632), tensor(1.4887), tensor(1.4652), tensor(1.4994), tensor(1.5731), tensor(1.5350), tensor(1.5257), tensor(1.5205), tensor(1.4905), tensor(1.5355), tensor(1.5317), tensor(1.7375), tensor(1.5603), tensor(1.5119), tensor(1.5449), tensor(1.4810), tensor(1.4704), tensor(1.4968), tensor(1.5542), tensor(1.5024), tensor(1.4852), tensor(1.4948), tensor(1.5355), tensor(1.5008), tensor(1.4607), tensor(2.4302), tensor(1.5132), tensor(1.5243), tensor(1.5465), tensor(2.0547), tensor(1.4899), tensor(2.8343), tensor(1.5212), tensor(1.5062), tensor(2.5263), tensor(1.5063), tensor(1.4774), tensor(1.4612), tensor(1.9491), tensor(1.6510), tensor(1.5489), tensor(1.4972), tensor(1.5047), tensor(1.4985), tensor(1.4989), tensor(1.5564), tensor(1.5044), tensor(1.6878), tensor(1.5926), tensor(1.7185), tensor(1.5203), tensor(1.5942), tensor(1.8732), tensor(1.4723), tensor(1.4928), tensor(1.5232), tensor(2.4489), tensor(1.4886), tensor(1.4926), tensor(1.4664), tensor(1.4798), tensor(1.5402), tensor(1.4884), tensor(2.3455), tensor(2.6077), tensor(1.4792), tensor(1.5174), tensor(1.4825), tensor(1.4693), tensor(1.5132), tensor(1.4618), tensor(1.4891), tensor(1.5071), tensor(1.6405), tensor(1.4979), tensor(2.0199), tensor(1.4866), tensor(1.4726), tensor(1.4970), tensor(1.7091), tensor(1.5447), tensor(1.6441), tensor(1.4878), tensor(2.0410), tensor(1.4822), tensor(2.1688), tensor(1.6187), tensor(1.5161), tensor(1.5517), tensor(1.5324), tensor(1.8881), tensor(1.4737), tensor(1.5649), tensor(1.4721), tensor(1.4981), tensor(2.3804), tensor(1.4896), tensor(1.4838), tensor(1.4929), tensor(1.4837), tensor(1.5051), tensor(1.4926), tensor(1.5055), tensor(1.5020), tensor(1.5256), tensor(1.7914), tensor(1.6654), tensor(2.9349), tensor(1.5005), tensor(1.5410), tensor(1.8088), tensor(1.4748), tensor(1.4570), tensor(1.5101), tensor(1.9576), tensor(1.4895), tensor(1.8970), tensor(1.4745), tensor(3.3571), tensor(1.4448), tensor(1.5413), tensor(1.5103), tensor(1.6366), tensor(1.5244), tensor(1.5598), tensor(1.5587), tensor(1.8547), tensor(1.4807), tensor(3.8329), tensor(1.4696), tensor(1.5653), tensor(2.5134), tensor(1.5172), tensor(1.4710), tensor(1.4576), tensor(1.6085), tensor(1.4794), tensor(1.4994), tensor(1.4673), tensor(1.5578), tensor(1.8364), tensor(1.4808), tensor(1.4916), tensor(1.7842), tensor(2.9405), tensor(1.4648), tensor(1.5597), tensor(1.5184), tensor(1.4717), tensor(2.2937), tensor(1.6603), tensor(1.5077), tensor(1.5035), tensor(1.5066), tensor(1.5936), tensor(1.6187), tensor(1.5216), tensor(1.5047), tensor(2.0635), tensor(1.4930), tensor(1.7900), tensor(1.4979), tensor(1.5522), tensor(1.4681), tensor(1.4721), tensor(1.5636), tensor(1.5304), tensor(1.5178), tensor(1.4970), tensor(2.3868), tensor(1.4509), tensor(1.5317), tensor(1.9834), tensor(1.5559), tensor(1.5006), tensor(1.6116), tensor(1.5140), tensor(1.5319), tensor(1.4813), tensor(1.7546), tensor(1.6802), tensor(1.5116), tensor(2.2871), tensor(1.6352), tensor(1.5147), tensor(1.4830), tensor(1.4840), tensor(2.6148), tensor(1.5380), tensor(1.4899), tensor(1.4789), tensor(1.6266), tensor(1.4769), tensor(1.5681), tensor(1.4629), tensor(1.5455), tensor(1.4810), tensor(1.5545), tensor(1.5620), tensor(1.4742), tensor(1.6294), tensor(1.4659), tensor(1.4958), tensor(1.5795), tensor(1.5370), tensor(1.4816), tensor(1.8252), tensor(1.5787), tensor(1.5532), tensor(1.5174), tensor(1.8242), tensor(1.4851), tensor(1.6282), tensor(2.1854), tensor(1.4864), tensor(2.2556), tensor(1.4919), tensor(1.5255), tensor(1.4959), tensor(1.4906), tensor(1.5262)]
09/23/2021 14:16:11 - INFO - __main__ - after_losses=[tensor(1.7173), tensor(1.4997), tensor(2.0861), tensor(1.4801), tensor(1.4986), tensor(1.6742), tensor(1.5267), tensor(1.4895), tensor(1.4710), tensor(1.4910), tensor(1.4770), tensor(1.4550), tensor(1.4697), tensor(1.5079), tensor(1.5019), tensor(1.5583), tensor(1.5550), tensor(1.8713), tensor(1.4810), tensor(1.5482), tensor(1.4615), tensor(1.4922), tensor(1.4842), tensor(1.4712), tensor(1.5458), tensor(1.5050), tensor(1.5128), tensor(1.5470), tensor(1.5464), tensor(1.4776), tensor(1.4615), tensor(1.4861), tensor(1.5538), tensor(1.7188), tensor(1.5092), tensor(1.4592), tensor(1.4918), tensor(1.4668), tensor(1.4987), tensor(1.8128), tensor(1.5391), tensor(1.5497), tensor(1.6247), tensor(1.4824), tensor(1.5633), tensor(1.5454), tensor(1.6096), tensor(1.5340), tensor(1.5061), tensor(1.5407), tensor(1.4793), tensor(1.4818), tensor(1.5272), tensor(1.5674), tensor(1.6679), tensor(1.4860), tensor(1.4894), tensor(1.5008), tensor(1.4920), tensor(1.4578), tensor(2.5032), tensor(1.5379), tensor(1.5340), tensor(1.5527), tensor(1.9930), tensor(1.4895), tensor(2.4532), tensor(1.5282), tensor(1.4978), tensor(2.3731), tensor(1.4865), tensor(1.4785), tensor(1.4648), tensor(2.0507), tensor(1.6343), tensor(1.8494), tensor(1.5024), tensor(1.5029), tensor(1.5044), tensor(1.5904), tensor(1.5350), tensor(1.5248), tensor(1.7374), tensor(1.5705), tensor(1.7534), tensor(1.5291), tensor(1.5942), tensor(1.9140), tensor(1.4687), tensor(1.4873), tensor(1.5527), tensor(2.1839), tensor(1.4977), tensor(1.4913), tensor(1.4721), tensor(1.4748), tensor(1.5313), tensor(1.4824), tensor(2.2069), tensor(2.6538), tensor(1.4930), tensor(1.5576), tensor(1.4827), tensor(1.4671), tensor(1.5067), tensor(1.4598), tensor(1.4785), tensor(1.5287), tensor(1.5139), tensor(1.4888), tensor(1.9049), tensor(1.5006), tensor(1.4809), tensor(1.4946), tensor(1.8726), tensor(1.5068), tensor(1.6538), tensor(1.4832), tensor(2.0509), tensor(1.4862), tensor(2.5220), tensor(1.6515), tensor(1.5155), tensor(1.5894), tensor(1.5020), tensor(2.0417), tensor(1.4779), tensor(1.5620), tensor(1.4798), tensor(1.5288), tensor(1.6512), tensor(1.4835), tensor(1.4856), tensor(1.4852), tensor(1.4811), tensor(1.5064), tensor(1.4944), tensor(1.6002), tensor(1.4974), tensor(1.5220), tensor(2.0783), tensor(1.8547), tensor(2.7433), tensor(1.5095), tensor(1.5326), tensor(1.8358), tensor(1.4758), tensor(1.4667), tensor(1.4987), tensor(1.4848), tensor(1.4881), tensor(1.6340), tensor(1.5049), tensor(3.0646), tensor(1.4520), tensor(1.5319), tensor(1.4911), tensor(1.5962), tensor(1.5145), tensor(1.5055), tensor(1.5539), tensor(1.6853), tensor(1.4653), tensor(3.5387), tensor(1.4762), tensor(1.5574), tensor(2.6134), tensor(1.5026), tensor(1.4857), tensor(1.4621), tensor(1.6550), tensor(1.4945), tensor(1.5025), tensor(1.4696), tensor(1.5353), tensor(2.0559), tensor(1.4815), tensor(1.4848), tensor(2.0552), tensor(2.9855), tensor(1.4698), tensor(1.6225), tensor(1.5216), tensor(1.4686), tensor(2.4289), tensor(1.7497), tensor(1.5181), tensor(1.4721), tensor(1.5413), tensor(1.6698), tensor(1.5344), tensor(1.5002), tensor(1.5177), tensor(1.9830), tensor(1.5158), tensor(1.7470), tensor(1.4875), tensor(1.5505), tensor(1.4646), tensor(1.4708), tensor(1.5505), tensor(1.5147), tensor(1.5158), tensor(1.4978), tensor(2.2064), tensor(1.4630), tensor(1.5858), tensor(1.9881), tensor(1.6566), tensor(1.5242), tensor(1.5397), tensor(1.5070), tensor(1.5690), tensor(1.4908), tensor(1.5465), tensor(1.7593), tensor(1.5046), tensor(2.4352), tensor(1.5015), tensor(1.6094), tensor(1.4901), tensor(1.4843), tensor(2.8238), tensor(1.5228), tensor(1.4764), tensor(1.4793), tensor(1.6558), tensor(1.4759), tensor(1.6765), tensor(1.4693), tensor(1.5497), tensor(1.4776), tensor(1.5497), tensor(1.5888), tensor(1.4729), tensor(1.7481), tensor(1.4957), tensor(1.4968), tensor(1.6215), tensor(1.5353), tensor(1.4771), tensor(2.2478), tensor(1.5413), tensor(1.5812), tensor(1.5174), tensor(2.8432), tensor(1.4915), tensor(1.5154), tensor(2.2707), tensor(1.4792), tensor(2.1703), tensor(1.4908), tensor(1.5209), tensor(1.5067), tensor(1.4980), tensor(1.5467)]
09/23/2021 14:16:12 - INFO - __main__ - interference_scores=[tensor(0.0001), tensor(0.0039), tensor(0.1679), tensor(-0.0018), tensor(-0.0089), tensor(-0.0703), tensor(0.0122), tensor(-0.0005), tensor(0.0109), tensor(-0.0023), tensor(-0.0013), tensor(-0.0063), tensor(-0.0037), tensor(0.0120), tensor(-0.0105), tensor(0.0177), tensor(0.0322), tensor(0.0881), tensor(-0.0031), tensor(0.0074), tensor(0.0047), tensor(-0.0006), tensor(-0.0099), tensor(-0.0006), tensor(0.0164), tensor(-0.0049), tensor(0.0024), tensor(0.0050), tensor(0.0067), tensor(0.0117), tensor(-0.0084), tensor(0.0036), tensor(-0.0209), tensor(-0.0084), tensor(-0.0062), tensor(-0.0040), tensor(0.0031), tensor(0.0017), tensor(-0.0007), tensor(0.2397), tensor(0.0041), tensor(0.0241), tensor(0.1042), tensor(-0.0081), tensor(0.0278), tensor(0.0138), tensor(-0.1279), tensor(-0.0262), tensor(-0.0058), tensor(-0.0042), tensor(-0.0017), tensor(0.0114), tensor(0.0304), tensor(0.0132), tensor(0.1655), tensor(0.0008), tensor(-0.0053), tensor(-0.0348), tensor(-0.0088), tensor(-0.0029), tensor(0.0729), tensor(0.0248), tensor(0.0097), tensor(0.0062), tensor(-0.0618), tensor(-0.0005), tensor(-0.3811), tensor(0.0070), tensor(-0.0084), tensor(-0.1532), tensor(-0.0199), tensor(0.0010), tensor(0.0036), tensor(0.1016), tensor(-0.0167), tensor(0.3005), tensor(0.0052), tensor(-0.0018), tensor(0.0059), tensor(0.0915), tensor(-0.0213), tensor(0.0204), tensor(0.0495), tensor(-0.0221), tensor(0.0349), tensor(0.0088), tensor(-4.5538e-05), tensor(0.0407), tensor(-0.0036), tensor(-0.0056), tensor(0.0295), tensor(-0.2650), tensor(0.0091), tensor(-0.0013), tensor(0.0057), tensor(-0.0049), tensor(-0.0088), tensor(-0.0060), tensor(-0.1386), tensor(0.0462), tensor(0.0138), tensor(0.0402), tensor(0.0002), tensor(-0.0022), tensor(-0.0065), tensor(-0.0020), tensor(-0.0107), tensor(0.0217), tensor(-0.1266), tensor(-0.0091), tensor(-0.1150), tensor(0.0141), tensor(0.0083), tensor(-0.0025), tensor(0.1635), tensor(-0.0379), tensor(0.0097), tensor(-0.0047), tensor(0.0099), tensor(0.0040), tensor(0.3532), tensor(0.0328), tensor(-0.0006), tensor(0.0377), tensor(-0.0304), tensor(0.1536), tensor(0.0042), tensor(-0.0029), tensor(0.0077), tensor(0.0308), tensor(-0.7292), tensor(-0.0061), tensor(0.0018), tensor(-0.0077), tensor(-0.0025), tensor(0.0013), tensor(0.0017), tensor(0.0947), tensor(-0.0047), tensor(-0.0036), tensor(0.2869), tensor(0.1893), tensor(-0.1916), tensor(0.0090), tensor(-0.0084), tensor(0.0271), tensor(0.0010), tensor(0.0098), tensor(-0.0114), tensor(-0.4728), tensor(-0.0015), tensor(-0.2631), tensor(0.0305), tensor(-0.2925), tensor(0.0072), tensor(-0.0094), tensor(-0.0192), tensor(-0.0404), tensor(-0.0099), tensor(-0.0543), tensor(-0.0047), tensor(-0.1694), tensor(-0.0153), tensor(-0.2942), tensor(0.0065), tensor(-0.0079), tensor(0.0999), tensor(-0.0146), tensor(0.0147), tensor(0.0045), tensor(0.0465), tensor(0.0151), tensor(0.0031), tensor(0.0023), tensor(-0.0225), tensor(0.2194), tensor(0.0007), tensor(-0.0068), tensor(0.2710), tensor(0.0450), tensor(0.0050), tensor(0.0628), tensor(0.0032), tensor(-0.0032), tensor(0.1352), tensor(0.0894), tensor(0.0103), tensor(-0.0315), tensor(0.0347), tensor(0.0762), tensor(-0.0843), tensor(-0.0214), tensor(0.0130), tensor(-0.0804), tensor(0.0228), tensor(-0.0430), tensor(-0.0105), tensor(-0.0016), tensor(-0.0036), tensor(-0.0012), tensor(-0.0131), tensor(-0.0157), tensor(-0.0020), tensor(0.0008), tensor(-0.1804), tensor(0.0120), tensor(0.0541), tensor(0.0048), tensor(0.1008), tensor(0.0235), tensor(-0.0719), tensor(-0.0071), tensor(0.0371), tensor(0.0095), tensor(-0.2081), tensor(0.0791), tensor(-0.0070), tensor(0.1481), tensor(-0.1337), tensor(0.0946), tensor(0.0070), tensor(0.0003), tensor(0.2090), tensor(-0.0152), tensor(-0.0135), tensor(0.0004), tensor(0.0292), tensor(-0.0010), tensor(0.1084), tensor(0.0064), tensor(0.0041), tensor(-0.0034), tensor(-0.0048), tensor(0.0269), tensor(-0.0014), tensor(0.1187), tensor(0.0297), tensor(0.0010), tensor(0.0420), tensor(-0.0017), tensor(-0.0044), tensor(0.4226), tensor(-0.0374), tensor(0.0280), tensor(-7.9870e-06), tensor(1.0190), tensor(0.0063), tensor(-0.1128), tensor(0.0853), tensor(-0.0072), tensor(-0.0854), tensor(-0.0012), tensor(-0.0046), tensor(0.0108), tensor(0.0075), tensor(0.0205)]
09/23/2021 14:16:12 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-814', 'mrqa_triviaqa-validation-1259', 'mrqa_triviaqa-validation-3847', 'mrqa_triviaqa-validation-1764', 'mrqa_squad-validation-6947', 'mrqa_triviaqa-validation-4383', 'mrqa_squad-validation-7149', 'mrqa_triviaqa-validation-2961', 'mrqa_squad-validation-5249', 'mrqa_triviaqa-validation-6881', 'mrqa_triviaqa-validation-1588', 'mrqa_triviaqa-validation-7332', 'mrqa_triviaqa-validation-2980', 'mrqa_squad-validation-8979', 'mrqa_triviaqa-validation-6331', 'mrqa_naturalquestions-validation-390', 'mrqa_triviaqa-validation-7506', 'mrqa_triviaqa-validation-3420', 'mrqa_squad-validation-8876', 'mrqa_triviaqa-validation-2368', 'mrqa_squad-validation-9214', 'mrqa_naturalquestions-validation-1279', 'mrqa_triviaqa-validation-287', 'mrqa_naturalquestions-validation-10680', 'mrqa_triviaqa-validation-2007', 'mrqa_triviaqa-validation-7369', 'mrqa_triviaqa-validation-1671', 'mrqa_triviaqa-validation-6872', 'mrqa_squad-validation-4637', 'mrqa_triviaqa-validation-7153', 'mrqa_squad-validation-6924', 'mrqa_triviaqa-validation-314']
09/23/2021 14:16:12 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:16:12 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=60) .... Timecode: 30
09/23/2021 14:16:25 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:16:25 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 30
09/23/2021 14:16:28 - INFO - __main__ - After Error Fixing: 0.875
09/23/2021 14:16:28 - INFO - __main__ - Instant Fixing Rate: 0.9285714285714286
09/23/2021 14:16:28 - INFO - __main__ - Instant Retention Rate: 0.49999999875
09/23/2021 14:16:30 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_031.pt.
09/23/2021 14:16:30 - INFO - __main__ - Saving the current error examples (len=28) to the memory.
09/23/2021 14:16:30 - INFO - __main__ - Current memory size: 770.
09/23/2021 14:16:30 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:16:30 - INFO - __main__ - Finished.
09/23/2021 14:16:30 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:16:30 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:16:30 - INFO - __main__ - Evaluating to get errors .... Timecode: 31
09/23/2021 14:16:34 - INFO - __main__ - Before Error Fixing: 0.125
09/23/2021 14:16:34 - INFO - __main__ - Found 28 errors.
09/23/2021 14:16:34 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:16:34 - INFO - __main__ - Current memory size: 798.
09/23/2021 14:16:34 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:16:57 - INFO - __main__ - before_losses=[tensor(1.4834), tensor(1.4918), tensor(1.5135), tensor(1.5082), tensor(1.5258), tensor(1.5236), tensor(1.5435), tensor(1.4847), tensor(1.5108), tensor(1.8007), tensor(1.5557), tensor(1.4785), tensor(1.6436), tensor(2.6890), tensor(1.4804), tensor(1.5108), tensor(1.5396), tensor(1.5451), tensor(1.7140), tensor(2.5801), tensor(1.5213), tensor(1.6920), tensor(1.5306), tensor(1.4793), tensor(2.4630), tensor(1.5497), tensor(1.4862), tensor(1.6093), tensor(1.6182), tensor(1.5214), tensor(1.5238), tensor(1.5267), tensor(2.3209), tensor(1.4826), tensor(1.4898), tensor(1.4539), tensor(1.5079), tensor(3.2758), tensor(1.4754), tensor(2.1040), tensor(1.5388), tensor(1.4911), tensor(1.5467), tensor(1.4985), tensor(1.4854), tensor(2.8316), tensor(1.4932), tensor(1.5156), tensor(1.5190), tensor(1.4961), tensor(1.4816), tensor(1.5320), tensor(1.5060), tensor(1.5591), tensor(1.4749), tensor(1.4772), tensor(1.4812), tensor(1.4914), tensor(1.4872), tensor(1.9500), tensor(1.4644), tensor(3.0570), tensor(1.6116), tensor(1.4749), tensor(2.1831), tensor(1.9144), tensor(1.5330), tensor(1.5101), tensor(1.5441), tensor(1.5138), tensor(1.6528), tensor(1.5123), tensor(1.4680), tensor(1.4981), tensor(1.5042), tensor(1.5108), tensor(1.7181), tensor(1.7615), tensor(1.4857), tensor(1.4643), tensor(1.4694), tensor(1.6229), tensor(3.2406), tensor(1.5217), tensor(1.4964), tensor(1.5031), tensor(2.1251), tensor(1.4761), tensor(1.6391), tensor(1.5110), tensor(1.4993), tensor(1.4694), tensor(1.5360), tensor(1.5532), tensor(1.5148), tensor(1.4967), tensor(1.5405), tensor(1.6232), tensor(1.5065), tensor(1.5000), tensor(1.5862), tensor(1.4981), tensor(1.4661), tensor(1.4795), tensor(1.4949), tensor(1.4871), tensor(1.5116), tensor(1.4869), tensor(1.4582), tensor(1.9645), tensor(1.4783), tensor(1.5536), tensor(1.4887), tensor(1.5078), tensor(3.8037), tensor(2.2664), tensor(1.4972), tensor(1.4930), tensor(1.5225), tensor(1.5017), tensor(1.5814), tensor(1.5090), tensor(1.5695), tensor(1.6550), tensor(1.5346), tensor(1.5030), tensor(1.5168), tensor(3.4666), tensor(2.0909), tensor(2.9040), tensor(1.4824), tensor(1.5207), tensor(1.6086), tensor(1.5124), tensor(1.5064), tensor(1.5865), tensor(1.4906), tensor(1.5943), tensor(1.5267), tensor(1.5142), tensor(1.6352), tensor(1.5024), tensor(1.5260), tensor(1.4524), tensor(1.9782), tensor(1.4844), tensor(1.9443), tensor(1.8005), tensor(1.5289), tensor(1.4769), tensor(1.4732), tensor(1.5389), tensor(1.5046), tensor(1.5409), tensor(1.5296), tensor(1.7346), tensor(2.4667), tensor(1.4923), tensor(1.4825), tensor(2.3182), tensor(1.7335), tensor(1.4475), tensor(1.6038), tensor(1.4918), tensor(1.7008), tensor(1.4862), tensor(1.4791), tensor(1.5150), tensor(1.5415), tensor(1.6476), tensor(1.4794), tensor(1.9587), tensor(1.5539), tensor(1.5295), tensor(1.5727), tensor(1.4784), tensor(5.0473), tensor(1.6584), tensor(1.5010), tensor(1.5285), tensor(1.4781), tensor(1.4699), tensor(1.4907), tensor(1.8096), tensor(1.4646), tensor(1.4852), tensor(2.5875), tensor(1.5080), tensor(1.5211), tensor(1.7015), tensor(1.5026), tensor(1.4620), tensor(1.5842), tensor(1.5373), tensor(1.4596), tensor(1.4457), tensor(1.5118), tensor(1.5162), tensor(1.4732), tensor(1.5521), tensor(1.5602), tensor(1.5453), tensor(1.5195), tensor(2.4971), tensor(1.5566), tensor(2.1500), tensor(1.4998), tensor(1.4859), tensor(1.5632), tensor(1.5117), tensor(1.5693), tensor(1.5056), tensor(1.5243), tensor(1.4608), tensor(2.7589), tensor(1.4923), tensor(1.6059), tensor(1.5379), tensor(2.6702), tensor(1.5638), tensor(1.4912), tensor(2.3840), tensor(1.4889), tensor(1.5011), tensor(2.2344), tensor(3.4061), tensor(1.5660), tensor(1.4802), tensor(1.4841), tensor(1.5107), tensor(1.5061), tensor(1.5448), tensor(1.7202), tensor(1.4661), tensor(1.4765), tensor(1.5280), tensor(1.4787), tensor(1.5070), tensor(3.0923), tensor(1.4452), tensor(1.5290), tensor(2.2826), tensor(1.4706), tensor(1.5779), tensor(1.4994), tensor(1.5001), tensor(1.8634), tensor(1.5290), tensor(1.5698), tensor(1.8826), tensor(1.5689), tensor(1.5123), tensor(1.4944), tensor(1.5884), tensor(1.5179), tensor(1.5619)]
09/23/2021 14:16:57 - INFO - __main__ - after_losses=[tensor(1.4694), tensor(1.4950), tensor(1.4971), tensor(1.4822), tensor(1.5007), tensor(1.4936), tensor(1.5426), tensor(1.4937), tensor(1.5063), tensor(1.7375), tensor(1.6256), tensor(1.4625), tensor(1.6052), tensor(2.6769), tensor(1.4623), tensor(1.5078), tensor(1.5206), tensor(1.6042), tensor(1.7478), tensor(2.6394), tensor(1.5667), tensor(1.7043), tensor(1.5166), tensor(1.4698), tensor(2.4754), tensor(1.5369), tensor(1.4764), tensor(1.5937), tensor(1.5731), tensor(1.5288), tensor(1.5092), tensor(1.4869), tensor(2.3284), tensor(1.4641), tensor(1.5219), tensor(1.4479), tensor(1.4908), tensor(3.4051), tensor(1.4664), tensor(2.1923), tensor(1.5136), tensor(1.5021), tensor(1.5302), tensor(1.4859), tensor(1.4644), tensor(1.6799), tensor(1.4808), tensor(1.5157), tensor(1.5052), tensor(1.4744), tensor(1.4828), tensor(1.5766), tensor(1.4871), tensor(1.5333), tensor(1.4997), tensor(1.4885), tensor(1.4615), tensor(1.4659), tensor(1.4758), tensor(2.2027), tensor(1.4674), tensor(1.7774), tensor(1.7221), tensor(1.4647), tensor(2.0875), tensor(1.8362), tensor(1.6073), tensor(1.4772), tensor(1.5359), tensor(1.5025), tensor(1.5380), tensor(1.5046), tensor(1.4614), tensor(1.4827), tensor(1.4854), tensor(1.4972), tensor(1.7655), tensor(1.7408), tensor(1.4756), tensor(1.4732), tensor(1.4599), tensor(1.6803), tensor(3.0949), tensor(1.4991), tensor(1.4848), tensor(1.4839), tensor(2.0742), tensor(1.4585), tensor(1.5437), tensor(1.4975), tensor(1.4882), tensor(1.4530), tensor(1.5523), tensor(1.5252), tensor(1.5006), tensor(1.4745), tensor(1.5618), tensor(1.8696), tensor(1.4876), tensor(1.4719), tensor(1.5483), tensor(1.4799), tensor(1.4537), tensor(1.4576), tensor(1.4904), tensor(1.4731), tensor(1.5678), tensor(1.4763), tensor(1.4471), tensor(1.7316), tensor(1.4592), tensor(1.5241), tensor(1.4914), tensor(1.4971), tensor(3.8480), tensor(2.3145), tensor(1.4850), tensor(1.4512), tensor(1.5048), tensor(1.4789), tensor(1.5439), tensor(1.4898), tensor(1.5731), tensor(1.6543), tensor(1.4982), tensor(1.5069), tensor(1.5019), tensor(3.6700), tensor(1.9936), tensor(1.9066), tensor(1.4686), tensor(1.5086), tensor(1.5940), tensor(1.5026), tensor(1.5058), tensor(1.5613), tensor(1.4800), tensor(1.7002), tensor(1.5342), tensor(1.4881), tensor(1.6247), tensor(1.4821), tensor(1.4921), tensor(1.4396), tensor(1.8538), tensor(1.4826), tensor(2.0674), tensor(1.6346), tensor(1.5699), tensor(1.4758), tensor(1.4822), tensor(1.5487), tensor(1.5136), tensor(1.5013), tensor(1.5135), tensor(1.6175), tensor(2.2459), tensor(1.4766), tensor(1.4790), tensor(2.4904), tensor(1.8828), tensor(1.4415), tensor(1.5504), tensor(1.4753), tensor(1.8191), tensor(1.4599), tensor(1.4660), tensor(1.5044), tensor(1.5434), tensor(1.5545), tensor(1.6915), tensor(1.9136), tensor(1.5585), tensor(1.5137), tensor(1.5014), tensor(1.4606), tensor(5.1812), tensor(1.7177), tensor(1.4985), tensor(1.4904), tensor(1.4659), tensor(1.4574), tensor(1.4707), tensor(1.8775), tensor(1.4580), tensor(1.4622), tensor(3.0073), tensor(1.5002), tensor(1.5053), tensor(1.5982), tensor(1.4905), tensor(1.4381), tensor(1.5731), tensor(1.5495), tensor(1.4515), tensor(1.4394), tensor(1.4855), tensor(1.4786), tensor(1.4551), tensor(1.5430), tensor(1.5785), tensor(1.5154), tensor(1.4954), tensor(2.1276), tensor(1.5290), tensor(2.1227), tensor(1.4867), tensor(1.4798), tensor(1.6257), tensor(1.4970), tensor(1.5367), tensor(1.4872), tensor(1.5093), tensor(1.4612), tensor(2.6775), tensor(1.4743), tensor(1.5808), tensor(1.5099), tensor(2.5696), tensor(2.2227), tensor(1.4779), tensor(1.5170), tensor(1.4737), tensor(1.4705), tensor(2.2120), tensor(3.2188), tensor(1.5389), tensor(1.4574), tensor(1.4670), tensor(1.4929), tensor(1.5207), tensor(1.5215), tensor(1.7123), tensor(1.4582), tensor(1.4644), tensor(1.5115), tensor(1.4622), tensor(1.4849), tensor(3.1531), tensor(1.4497), tensor(1.5174), tensor(1.9251), tensor(1.4553), tensor(1.5341), tensor(1.4700), tensor(1.4800), tensor(1.8924), tensor(1.5204), tensor(1.5453), tensor(1.8177), tensor(1.6593), tensor(1.4997), tensor(1.4827), tensor(1.5999), tensor(1.4954), tensor(1.5336)]
09/23/2021 14:16:57 - INFO - __main__ - interference_scores=[tensor(-0.0140), tensor(0.0033), tensor(-0.0164), tensor(-0.0260), tensor(-0.0251), tensor(-0.0301), tensor(-0.0009), tensor(0.0090), tensor(-0.0046), tensor(-0.0632), tensor(0.0699), tensor(-0.0161), tensor(-0.0384), tensor(-0.0120), tensor(-0.0181), tensor(-0.0030), tensor(-0.0189), tensor(0.0591), tensor(0.0338), tensor(0.0593), tensor(0.0455), tensor(0.0122), tensor(-0.0140), tensor(-0.0094), tensor(0.0124), tensor(-0.0128), tensor(-0.0098), tensor(-0.0156), tensor(-0.0451), tensor(0.0074), tensor(-0.0146), tensor(-0.0398), tensor(0.0075), tensor(-0.0185), tensor(0.0321), tensor(-0.0060), tensor(-0.0171), tensor(0.1293), tensor(-0.0090), tensor(0.0884), tensor(-0.0251), tensor(0.0110), tensor(-0.0165), tensor(-0.0126), tensor(-0.0210), tensor(-1.1518), tensor(-0.0124), tensor(9.0599e-05), tensor(-0.0138), tensor(-0.0218), tensor(0.0012), tensor(0.0446), tensor(-0.0189), tensor(-0.0258), tensor(0.0248), tensor(0.0113), tensor(-0.0197), tensor(-0.0254), tensor(-0.0113), tensor(0.2527), tensor(0.0030), tensor(-1.2796), tensor(0.1105), tensor(-0.0101), tensor(-0.0956), tensor(-0.0781), tensor(0.0743), tensor(-0.0328), tensor(-0.0081), tensor(-0.0113), tensor(-0.1148), tensor(-0.0077), tensor(-0.0066), tensor(-0.0154), tensor(-0.0188), tensor(-0.0136), tensor(0.0474), tensor(-0.0207), tensor(-0.0101), tensor(0.0089), tensor(-0.0095), tensor(0.0574), tensor(-0.1458), tensor(-0.0225), tensor(-0.0116), tensor(-0.0192), tensor(-0.0509), tensor(-0.0175), tensor(-0.0953), tensor(-0.0135), tensor(-0.0110), tensor(-0.0164), tensor(0.0162), tensor(-0.0280), tensor(-0.0142), tensor(-0.0222), tensor(0.0213), tensor(0.2464), tensor(-0.0189), tensor(-0.0281), tensor(-0.0379), tensor(-0.0181), tensor(-0.0124), tensor(-0.0219), tensor(-0.0045), tensor(-0.0140), tensor(0.0562), tensor(-0.0106), tensor(-0.0111), tensor(-0.2329), tensor(-0.0192), tensor(-0.0295), tensor(0.0026), tensor(-0.0107), tensor(0.0443), tensor(0.0481), tensor(-0.0122), tensor(-0.0418), tensor(-0.0177), tensor(-0.0228), tensor(-0.0375), tensor(-0.0191), tensor(0.0036), tensor(-0.0007), tensor(-0.0364), tensor(0.0039), tensor(-0.0149), tensor(0.2034), tensor(-0.0974), tensor(-0.9974), tensor(-0.0138), tensor(-0.0121), tensor(-0.0146), tensor(-0.0098), tensor(-0.0006), tensor(-0.0251), tensor(-0.0105), tensor(0.1060), tensor(0.0074), tensor(-0.0261), tensor(-0.0105), tensor(-0.0203), tensor(-0.0339), tensor(-0.0128), tensor(-0.1244), tensor(-0.0018), tensor(0.1231), tensor(-0.1659), tensor(0.0410), tensor(-0.0010), tensor(0.0090), tensor(0.0098), tensor(0.0089), tensor(-0.0396), tensor(-0.0162), tensor(-0.1171), tensor(-0.2208), tensor(-0.0157), tensor(-0.0035), tensor(0.1722), tensor(0.1493), tensor(-0.0060), tensor(-0.0533), tensor(-0.0165), tensor(0.1183), tensor(-0.0263), tensor(-0.0131), tensor(-0.0105), tensor(0.0019), tensor(-0.0931), tensor(0.2121), tensor(-0.0451), tensor(0.0046), tensor(-0.0158), tensor(-0.0713), tensor(-0.0178), tensor(0.1339), tensor(0.0593), tensor(-0.0024), tensor(-0.0381), tensor(-0.0122), tensor(-0.0125), tensor(-0.0200), tensor(0.0680), tensor(-0.0066), tensor(-0.0230), tensor(0.4197), tensor(-0.0078), tensor(-0.0158), tensor(-0.1033), tensor(-0.0121), tensor(-0.0239), tensor(-0.0110), tensor(0.0122), tensor(-0.0081), tensor(-0.0064), tensor(-0.0263), tensor(-0.0376), tensor(-0.0181), tensor(-0.0091), tensor(0.0183), tensor(-0.0299), tensor(-0.0241), tensor(-0.3695), tensor(-0.0277), tensor(-0.0273), tensor(-0.0131), tensor(-0.0061), tensor(0.0625), tensor(-0.0147), tensor(-0.0326), tensor(-0.0184), tensor(-0.0150), tensor(0.0003), tensor(-0.0814), tensor(-0.0180), tensor(-0.0251), tensor(-0.0280), tensor(-0.1006), tensor(0.6589), tensor(-0.0134), tensor(-0.8670), tensor(-0.0153), tensor(-0.0305), tensor(-0.0224), tensor(-0.1873), tensor(-0.0271), tensor(-0.0228), tensor(-0.0171), tensor(-0.0178), tensor(0.0146), tensor(-0.0234), tensor(-0.0080), tensor(-0.0080), tensor(-0.0120), tensor(-0.0165), tensor(-0.0166), tensor(-0.0220), tensor(0.0608), tensor(0.0045), tensor(-0.0116), tensor(-0.3575), tensor(-0.0154), tensor(-0.0439), tensor(-0.0294), tensor(-0.0201), tensor(0.0290), tensor(-0.0086), tensor(-0.0245), tensor(-0.0649), tensor(0.0905), tensor(-0.0126), tensor(-0.0117), tensor(0.0115), tensor(-0.0225), tensor(-0.0284)]
09/23/2021 14:16:57 - INFO - __main__ - retrieved candidates ids = ['mrqa_naturalquestions-validation-390', 'mrqa_squad-validation-7572', 'mrqa_triviaqa-validation-6772', 'mrqa_triviaqa-validation-1578', 'mrqa_hotpotqa-validation-2399', 'mrqa_triviaqa-validation-7032', 'mrqa_naturalquestions-validation-1282', 'mrqa_triviaqa-validation-866', 'mrqa_triviaqa-validation-2442', 'mrqa_triviaqa-validation-751', 'mrqa_triviaqa-validation-5239', 'mrqa_triviaqa-validation-3595', 'mrqa_triviaqa-validation-4308', 'mrqa_triviaqa-validation-5078', 'mrqa_hotpotqa-validation-3978', 'mrqa_triviaqa-validation-6781', 'mrqa_naturalquestions-validation-6019', 'mrqa_triviaqa-validation-2376', 'mrqa_naturalquestions-validation-866', 'mrqa_triviaqa-validation-1550', 'mrqa_triviaqa-validation-6689', 'mrqa_naturalquestions-validation-9011', 'mrqa_triviaqa-validation-3472', 'mrqa_hotpotqa-validation-2434', 'mrqa_squad-validation-4181', 'mrqa_squad-validation-1705', 'mrqa_squad-validation-10148', 'mrqa_hotpotqa-validation-57', 'mrqa_hotpotqa-validation-1001', 'mrqa_triviaqa-validation-1034', 'mrqa_triviaqa-validation-5304', 'mrqa_hotpotqa-validation-1864']
09/23/2021 14:16:57 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:16:57 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=60) .... Timecode: 31
09/23/2021 14:17:10 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:17:10 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 31
09/23/2021 14:17:13 - INFO - __main__ - After Error Fixing: 0.96875
09/23/2021 14:17:13 - INFO - __main__ - Instant Fixing Rate: 1.0
09/23/2021 14:17:13 - INFO - __main__ - Instant Retention Rate: 0.7499999981250001
09/23/2021 14:17:15 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_032.pt.
09/23/2021 14:17:15 - INFO - __main__ - Saving the current error examples (len=28) to the memory.
09/23/2021 14:17:15 - INFO - __main__ - Current memory size: 798.
09/23/2021 14:17:15 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:17:15 - INFO - __main__ - Finished.
09/23/2021 14:17:15 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:17:15 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:17:15 - INFO - __main__ - Evaluating to get errors .... Timecode: 32
09/23/2021 14:17:19 - INFO - __main__ - Before Error Fixing: 0.1875
09/23/2021 14:17:19 - INFO - __main__ - Found 26 errors.
09/23/2021 14:17:19 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:17:19 - INFO - __main__ - Current memory size: 826.
09/23/2021 14:17:19 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:17:42 - INFO - __main__ - before_losses=[tensor(1.5944), tensor(1.4853), tensor(1.8547), tensor(1.8112), tensor(1.6349), tensor(2.5325), tensor(1.7357), tensor(1.5663), tensor(1.5359), tensor(1.5127), tensor(1.5844), tensor(1.5527), tensor(1.5067), tensor(1.5234), tensor(1.5723), tensor(1.5686), tensor(1.5249), tensor(1.5509), tensor(1.5114), tensor(1.6029), tensor(1.4716), tensor(2.1007), tensor(1.5945), tensor(1.5621), tensor(2.4983), tensor(1.5507), tensor(1.5176), tensor(1.4695), tensor(1.5735), tensor(1.5118), tensor(1.5276), tensor(1.7532), tensor(1.5117), tensor(1.6302), tensor(1.5054), tensor(1.4999), tensor(1.5244), tensor(1.4957), tensor(1.6116), tensor(1.4991), tensor(1.4888), tensor(1.8895), tensor(2.2966), tensor(1.5268), tensor(1.5104), tensor(1.5282), tensor(1.5054), tensor(1.5470), tensor(1.5337), tensor(1.5334), tensor(1.4810), tensor(1.5056), tensor(1.8172), tensor(1.5227), tensor(1.5066), tensor(1.5966), tensor(1.7198), tensor(1.4872), tensor(1.5060), tensor(1.7060), tensor(1.5387), tensor(1.5087), tensor(1.5320), tensor(2.0445), tensor(2.1800), tensor(1.4921), tensor(1.5288), tensor(1.8114), tensor(1.5074), tensor(1.4940), tensor(1.5002), tensor(1.5585), tensor(1.5947), tensor(1.5739), tensor(1.5181), tensor(1.4816), tensor(1.5862), tensor(1.4882), tensor(1.4887), tensor(1.5359), tensor(1.5863), tensor(1.5128), tensor(1.4791), tensor(1.4814), tensor(1.6235), tensor(1.8100), tensor(1.5183), tensor(1.5886), tensor(1.5021), tensor(1.5168), tensor(1.4829), tensor(1.4832), tensor(1.5084), tensor(1.5275), tensor(1.6120), tensor(1.9122), tensor(1.5091), tensor(1.6308), tensor(2.6200), tensor(1.4795), tensor(1.6129), tensor(1.5650), tensor(1.5083), tensor(2.2185), tensor(1.4624), tensor(1.5940), tensor(2.1565), tensor(1.4986), tensor(1.5332), tensor(1.4817), tensor(1.5135), tensor(1.4946), tensor(1.5095), tensor(1.4981), tensor(1.4711), tensor(1.4786), tensor(1.4687), tensor(1.5548), tensor(1.6887), tensor(1.4810), tensor(1.4889), tensor(1.5322), tensor(1.4967), tensor(1.6549), tensor(1.5257), tensor(1.4793), tensor(1.4761), tensor(1.4728), tensor(1.4916), tensor(1.5069), tensor(2.3330), tensor(1.4938), tensor(1.7063), tensor(1.5091), tensor(1.5273), tensor(1.5127), tensor(1.4597), tensor(2.2717), tensor(1.5385), tensor(1.5476), tensor(1.9656), tensor(1.4626), tensor(1.5033), tensor(1.5335), tensor(1.5026), tensor(1.4494), tensor(1.4736), tensor(2.0874), tensor(1.5402), tensor(1.7350), tensor(1.4875), tensor(1.6510), tensor(1.4660), tensor(1.5391), tensor(1.5115), tensor(2.8463), tensor(1.4945), tensor(1.5931), tensor(1.5192), tensor(1.4740), tensor(1.6109), tensor(1.7504), tensor(1.5119), tensor(1.6238), tensor(1.5059), tensor(1.6213), tensor(1.5013), tensor(1.4866), tensor(1.4929), tensor(1.5717), tensor(1.5281), tensor(1.4571), tensor(1.5383), tensor(1.7436), tensor(1.6256), tensor(1.5285), tensor(1.4782), tensor(1.5742), tensor(1.5923), tensor(1.4953), tensor(1.5703), tensor(1.5249), tensor(1.5054), tensor(1.6047), tensor(1.6400), tensor(1.6768), tensor(1.4754), tensor(1.5070), tensor(1.5023), tensor(2.3691), tensor(1.4783), tensor(2.0244), tensor(1.4742), tensor(1.7278), tensor(1.5730), tensor(1.6768), tensor(1.4847), tensor(1.5006), tensor(1.6042), tensor(1.6286), tensor(1.5394), tensor(1.5529), tensor(1.8780), tensor(1.5115), tensor(1.5773), tensor(1.4659), tensor(1.4794), tensor(1.5627), tensor(1.4953), tensor(1.5239), tensor(1.5816), tensor(1.5249), tensor(1.4841), tensor(1.5474), tensor(1.5130), tensor(1.5295), tensor(1.9445), tensor(1.4958), tensor(1.5579), tensor(1.4879), tensor(1.4993), tensor(1.4991), tensor(1.4785), tensor(1.5007), tensor(1.5833), tensor(1.5343), tensor(1.5033), tensor(1.5098), tensor(1.5563), tensor(1.4925), tensor(1.5019), tensor(1.8354), tensor(1.6545), tensor(1.6503), tensor(1.6311), tensor(1.4572), tensor(1.7741), tensor(1.5045), tensor(1.5476), tensor(1.4571), tensor(1.5215), tensor(1.4492), tensor(1.5097), tensor(1.4823), tensor(1.5491), tensor(1.4873), tensor(1.5486), tensor(1.4911), tensor(2.1356), tensor(1.4679), tensor(1.4427), tensor(1.4742), tensor(1.6183), tensor(1.5105), tensor(1.4934), tensor(1.4718)]
09/23/2021 14:17:43 - INFO - __main__ - after_losses=[tensor(1.7014), tensor(1.5036), tensor(2.0522), tensor(1.6915), tensor(1.6500), tensor(2.5734), tensor(1.6982), tensor(1.5735), tensor(1.5389), tensor(1.5765), tensor(1.5547), tensor(1.5681), tensor(1.5236), tensor(1.5208), tensor(1.7079), tensor(1.5446), tensor(1.5381), tensor(1.5609), tensor(1.5453), tensor(1.9212), tensor(1.4789), tensor(2.1557), tensor(1.5798), tensor(1.5669), tensor(2.4170), tensor(1.5443), tensor(1.6428), tensor(1.4774), tensor(1.5457), tensor(1.5243), tensor(1.5141), tensor(1.7688), tensor(1.5209), tensor(1.7242), tensor(1.4890), tensor(1.4938), tensor(1.5203), tensor(1.5051), tensor(1.6217), tensor(1.4979), tensor(1.5010), tensor(1.7628), tensor(2.3062), tensor(1.5462), tensor(1.5166), tensor(1.5291), tensor(1.5389), tensor(1.5951), tensor(1.5516), tensor(1.5482), tensor(1.4889), tensor(1.4943), tensor(1.9830), tensor(1.6034), tensor(1.5129), tensor(1.5845), tensor(1.6948), tensor(1.5239), tensor(1.4938), tensor(1.7207), tensor(1.6209), tensor(1.5009), tensor(1.5353), tensor(1.8991), tensor(2.3262), tensor(1.4950), tensor(1.5240), tensor(1.9505), tensor(1.5278), tensor(1.4874), tensor(1.5359), tensor(1.5529), tensor(1.5959), tensor(1.5858), tensor(1.5242), tensor(1.4859), tensor(1.5797), tensor(1.5140), tensor(1.5149), tensor(1.5156), tensor(1.6378), tensor(1.5227), tensor(1.4815), tensor(1.5053), tensor(1.5971), tensor(1.8102), tensor(1.5303), tensor(1.5735), tensor(1.5057), tensor(1.5116), tensor(1.4863), tensor(1.4816), tensor(1.5167), tensor(1.5293), tensor(1.6690), tensor(2.0353), tensor(1.5017), tensor(1.5963), tensor(2.6719), tensor(1.4830), tensor(1.6145), tensor(1.5529), tensor(1.4997), tensor(2.3489), tensor(1.4660), tensor(1.5934), tensor(1.9984), tensor(1.5069), tensor(1.5306), tensor(1.4835), tensor(1.4888), tensor(1.5065), tensor(1.4926), tensor(1.5121), tensor(1.4860), tensor(1.4811), tensor(1.4808), tensor(1.5574), tensor(1.7144), tensor(1.4827), tensor(1.5079), tensor(1.5345), tensor(1.6910), tensor(1.6282), tensor(1.5148), tensor(1.4878), tensor(1.4687), tensor(1.4751), tensor(1.4955), tensor(1.4935), tensor(2.4032), tensor(1.4928), tensor(1.6818), tensor(1.5027), tensor(1.5135), tensor(1.5214), tensor(1.4697), tensor(2.3904), tensor(1.5358), tensor(1.5551), tensor(1.9858), tensor(1.4606), tensor(1.5218), tensor(1.6055), tensor(1.5246), tensor(1.4551), tensor(1.4617), tensor(2.0284), tensor(1.5524), tensor(1.6185), tensor(1.4967), tensor(1.4941), tensor(1.4615), tensor(1.5693), tensor(1.5113), tensor(2.4632), tensor(1.4866), tensor(1.6082), tensor(1.5233), tensor(1.4706), tensor(1.7938), tensor(1.7225), tensor(1.5033), tensor(1.5786), tensor(1.5253), tensor(1.6624), tensor(1.4940), tensor(1.4766), tensor(1.4788), tensor(1.5933), tensor(1.5374), tensor(1.4712), tensor(1.5327), tensor(1.6350), tensor(1.5483), tensor(1.5337), tensor(1.4840), tensor(1.5870), tensor(1.7288), tensor(1.4990), tensor(1.5664), tensor(1.5397), tensor(1.5003), tensor(1.6057), tensor(1.7177), tensor(1.6071), tensor(1.4748), tensor(1.5043), tensor(1.5014), tensor(2.5449), tensor(1.4857), tensor(2.0046), tensor(1.4732), tensor(1.7993), tensor(1.5563), tensor(1.7078), tensor(1.4871), tensor(1.5107), tensor(1.6031), tensor(1.7099), tensor(1.5414), tensor(1.5625), tensor(1.9817), tensor(1.5058), tensor(1.5376), tensor(1.4702), tensor(1.4932), tensor(1.5383), tensor(1.5096), tensor(1.5261), tensor(1.6121), tensor(1.5293), tensor(1.5435), tensor(1.5590), tensor(1.5128), tensor(1.5332), tensor(2.0021), tensor(1.5015), tensor(1.7388), tensor(1.4852), tensor(1.4944), tensor(1.4941), tensor(1.4737), tensor(1.5007), tensor(1.5408), tensor(1.5410), tensor(1.5083), tensor(1.5195), tensor(1.5581), tensor(1.4894), tensor(1.4811), tensor(1.9324), tensor(1.6244), tensor(1.6829), tensor(1.6588), tensor(1.4530), tensor(1.7191), tensor(1.4977), tensor(1.5861), tensor(1.4570), tensor(1.5130), tensor(1.4506), tensor(1.5252), tensor(1.4775), tensor(1.5561), tensor(1.5046), tensor(1.5711), tensor(1.5081), tensor(2.1836), tensor(1.4650), tensor(1.4493), tensor(1.4704), tensor(1.5780), tensor(1.4945), tensor(1.4788), tensor(1.4918)]
09/23/2021 14:17:43 - INFO - __main__ - interference_scores=[tensor(0.1070), tensor(0.0183), tensor(0.1975), tensor(-0.1197), tensor(0.0152), tensor(0.0409), tensor(-0.0375), tensor(0.0072), tensor(0.0030), tensor(0.0638), tensor(-0.0297), tensor(0.0154), tensor(0.0169), tensor(-0.0027), tensor(0.1355), tensor(-0.0240), tensor(0.0131), tensor(0.0101), tensor(0.0339), tensor(0.3184), tensor(0.0072), tensor(0.0550), tensor(-0.0148), tensor(0.0048), tensor(-0.0812), tensor(-0.0065), tensor(0.1252), tensor(0.0079), tensor(-0.0277), tensor(0.0125), tensor(-0.0135), tensor(0.0155), tensor(0.0093), tensor(0.0940), tensor(-0.0164), tensor(-0.0061), tensor(-0.0041), tensor(0.0094), tensor(0.0100), tensor(-0.0012), tensor(0.0122), tensor(-0.1267), tensor(0.0096), tensor(0.0194), tensor(0.0063), tensor(0.0009), tensor(0.0336), tensor(0.0480), tensor(0.0179), tensor(0.0148), tensor(0.0080), tensor(-0.0114), tensor(0.1658), tensor(0.0807), tensor(0.0063), tensor(-0.0121), tensor(-0.0250), tensor(0.0367), tensor(-0.0122), tensor(0.0148), tensor(0.0821), tensor(-0.0077), tensor(0.0033), tensor(-0.1454), tensor(0.1462), tensor(0.0029), tensor(-0.0047), tensor(0.1390), tensor(0.0204), tensor(-0.0066), tensor(0.0357), tensor(-0.0056), tensor(0.0012), tensor(0.0119), tensor(0.0062), tensor(0.0044), tensor(-0.0065), tensor(0.0258), tensor(0.0262), tensor(-0.0202), tensor(0.0515), tensor(0.0099), tensor(0.0023), tensor(0.0239), tensor(-0.0264), tensor(0.0002), tensor(0.0120), tensor(-0.0152), tensor(0.0036), tensor(-0.0052), tensor(0.0034), tensor(-0.0016), tensor(0.0083), tensor(0.0019), tensor(0.0571), tensor(0.1232), tensor(-0.0074), tensor(-0.0345), tensor(0.0519), tensor(0.0035), tensor(0.0016), tensor(-0.0121), tensor(-0.0087), tensor(0.1303), tensor(0.0035), tensor(-0.0006), tensor(-0.1581), tensor(0.0083), tensor(-0.0025), tensor(0.0018), tensor(-0.0247), tensor(0.0119), tensor(-0.0170), tensor(0.0140), tensor(0.0150), tensor(0.0025), tensor(0.0122), tensor(0.0026), tensor(0.0257), tensor(0.0017), tensor(0.0191), tensor(0.0023), tensor(0.1943), tensor(-0.0267), tensor(-0.0109), tensor(0.0084), tensor(-0.0074), tensor(0.0023), tensor(0.0039), tensor(-0.0134), tensor(0.0702), tensor(-0.0011), tensor(-0.0245), tensor(-0.0064), tensor(-0.0138), tensor(0.0087), tensor(0.0100), tensor(0.1187), tensor(-0.0027), tensor(0.0075), tensor(0.0203), tensor(-0.0019), tensor(0.0184), tensor(0.0720), tensor(0.0220), tensor(0.0057), tensor(-0.0120), tensor(-0.0591), tensor(0.0123), tensor(-0.1165), tensor(0.0092), tensor(-0.1569), tensor(-0.0045), tensor(0.0302), tensor(-0.0002), tensor(-0.3831), tensor(-0.0079), tensor(0.0151), tensor(0.0041), tensor(-0.0034), tensor(0.1829), tensor(-0.0279), tensor(-0.0086), tensor(-0.0452), tensor(0.0195), tensor(0.0411), tensor(-0.0073), tensor(-0.0100), tensor(-0.0141), tensor(0.0215), tensor(0.0093), tensor(0.0141), tensor(-0.0056), tensor(-0.1086), tensor(-0.0773), tensor(0.0052), tensor(0.0057), tensor(0.0128), tensor(0.1365), tensor(0.0037), tensor(-0.0039), tensor(0.0148), tensor(-0.0051), tensor(0.0010), tensor(0.0777), tensor(-0.0696), tensor(-0.0005), tensor(-0.0027), tensor(-0.0009), tensor(0.1758), tensor(0.0074), tensor(-0.0198), tensor(-0.0010), tensor(0.0716), tensor(-0.0167), tensor(0.0310), tensor(0.0024), tensor(0.0101), tensor(-0.0012), tensor(0.0813), tensor(0.0020), tensor(0.0096), tensor(0.1037), tensor(-0.0057), tensor(-0.0397), tensor(0.0043), tensor(0.0138), tensor(-0.0243), tensor(0.0143), tensor(0.0022), tensor(0.0305), tensor(0.0044), tensor(0.0594), tensor(0.0116), tensor(-0.0002), tensor(0.0037), tensor(0.0576), tensor(0.0057), tensor(0.1809), tensor(-0.0027), tensor(-0.0050), tensor(-0.0050), tensor(-0.0048), tensor(2.1935e-05), tensor(-0.0426), tensor(0.0067), tensor(0.0050), tensor(0.0097), tensor(0.0019), tensor(-0.0031), tensor(-0.0208), tensor(0.0969), tensor(-0.0301), tensor(0.0327), tensor(0.0277), tensor(-0.0042), tensor(-0.0550), tensor(-0.0068), tensor(0.0385), tensor(-0.0001), tensor(-0.0085), tensor(0.0015), tensor(0.0155), tensor(-0.0048), tensor(0.0070), tensor(0.0173), tensor(0.0225), tensor(0.0170), tensor(0.0480), tensor(-0.0030), tensor(0.0066), tensor(-0.0038), tensor(-0.0403), tensor(-0.0161), tensor(-0.0146), tensor(0.0200)]
09/23/2021 14:17:43 - INFO - __main__ - retrieved candidates ids = ['mrqa_hotpotqa-validation-3976', 'mrqa_triviaqa-validation-2709', 'mrqa_squad-validation-6128', 'mrqa_triviaqa-validation-3420', 'mrqa_triviaqa-validation-1034', 'mrqa_triviaqa-validation-1736', 'mrqa_squad-validation-4118', 'mrqa_triviaqa-validation-6331', 'mrqa_triviaqa-validation-6632', 'mrqa_triviaqa-validation-3945', 'mrqa_squad-validation-10015', 'mrqa_triviaqa-validation-5261', 'mrqa_squad-validation-2656', 'mrqa_triviaqa-validation-5962', 'mrqa_triviaqa-validation-3915', 'mrqa_squad-validation-6947', 'mrqa_squad-validation-7799', 'mrqa_triviaqa-validation-2376', 'mrqa_triviaqa-validation-4852', 'mrqa_triviaqa-validation-2899', 'mrqa_squad-validation-3344', 'mrqa_triviaqa-validation-4768', 'mrqa_triviaqa-validation-2683', 'mrqa_squad-validation-3106', 'mrqa_squad-validation-5407', 'mrqa_squad-validation-5184', 'mrqa_triviaqa-validation-1954', 'mrqa_hotpotqa-validation-613', 'mrqa_squad-validation-2584', 'mrqa_triviaqa-validation-2179', 'mrqa_naturalquestions-validation-4127', 'mrqa_triviaqa-validation-3393']
09/23/2021 14:17:43 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:17:43 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=58) .... Timecode: 32
09/23/2021 14:17:56 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:17:56 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 32
09/23/2021 14:17:59 - INFO - __main__ - After Error Fixing: 0.9375
09/23/2021 14:17:59 - INFO - __main__ - Instant Fixing Rate: 0.9615384615384616
09/23/2021 14:17:59 - INFO - __main__ - Instant Retention Rate: 0.8333333319444445
09/23/2021 14:18:01 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_033.pt.
09/23/2021 14:18:01 - INFO - __main__ - Saving the current error examples (len=26) to the memory.
09/23/2021 14:18:01 - INFO - __main__ - Current memory size: 826.
09/23/2021 14:18:01 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:18:01 - INFO - __main__ - Finished.
09/23/2021 14:18:01 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:18:01 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:18:01 - INFO - __main__ - Evaluating to get errors .... Timecode: 33
09/23/2021 14:18:04 - INFO - __main__ - Before Error Fixing: 0.15625
09/23/2021 14:18:04 - INFO - __main__ - Found 27 errors.
09/23/2021 14:18:04 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:18:04 - INFO - __main__ - Current memory size: 852.
09/23/2021 14:18:04 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:18:27 - INFO - __main__ - before_losses=[tensor(1.4767), tensor(1.4773), tensor(1.4391), tensor(1.5398), tensor(1.5162), tensor(1.5558), tensor(2.6598), tensor(1.4887), tensor(1.5047), tensor(1.5549), tensor(1.5293), tensor(1.4750), tensor(1.5784), tensor(1.5123), tensor(1.5137), tensor(1.4759), tensor(1.4943), tensor(1.5332), tensor(1.5573), tensor(1.4983), tensor(1.4999), tensor(1.4906), tensor(1.4559), tensor(2.3837), tensor(1.5140), tensor(1.5615), tensor(1.9631), tensor(2.4450), tensor(1.5927), tensor(1.5913), tensor(1.7887), tensor(1.4588), tensor(1.7190), tensor(1.5120), tensor(1.6775), tensor(1.4896), tensor(2.6012), tensor(1.4913), tensor(1.4970), tensor(1.4749), tensor(1.5193), tensor(1.5365), tensor(1.5215), tensor(1.8147), tensor(1.4757), tensor(2.0514), tensor(1.6000), tensor(1.9035), tensor(1.5315), tensor(1.4976), tensor(1.5054), tensor(1.5209), tensor(1.4620), tensor(1.5905), tensor(2.7264), tensor(1.5111), tensor(2.1028), tensor(1.5171), tensor(1.4610), tensor(1.5108), tensor(1.9919), tensor(1.6453), tensor(1.4817), tensor(1.5422), tensor(3.5691), tensor(3.4553), tensor(1.4808), tensor(1.5068), tensor(1.6487), tensor(1.4596), tensor(1.4920), tensor(1.5544), tensor(1.5208), tensor(1.4706), tensor(1.5013), tensor(1.4966), tensor(1.5075), tensor(1.5821), tensor(1.4550), tensor(1.7116), tensor(1.5402), tensor(1.5823), tensor(1.6234), tensor(1.5299), tensor(1.5779), tensor(2.0332), tensor(1.4788), tensor(1.5424), tensor(1.9112), tensor(1.7315), tensor(1.4945), tensor(2.2402), tensor(1.8040), tensor(2.6339), tensor(1.4690), tensor(1.6104), tensor(1.5246), tensor(1.4969), tensor(1.4629), tensor(1.5226), tensor(1.4976), tensor(1.4658), tensor(1.4919), tensor(1.5206), tensor(1.5284), tensor(1.4870), tensor(1.4813), tensor(1.4835), tensor(1.5185), tensor(1.4530), tensor(1.8719), tensor(1.5445), tensor(1.4654), tensor(1.5017), tensor(1.7314), tensor(1.4950), tensor(1.5167), tensor(1.5376), tensor(1.4786), tensor(1.4849), tensor(1.4868), tensor(2.0445), tensor(1.5389), tensor(1.4628), tensor(1.5564), tensor(1.5482), tensor(1.5905), tensor(2.9752), tensor(1.6178), tensor(1.5318), tensor(1.5529), tensor(1.5192), tensor(1.4949), tensor(1.5828), tensor(1.6329), tensor(2.2730), tensor(1.7132), tensor(1.4612), tensor(1.5163), tensor(1.5820), tensor(2.0245), tensor(1.5201), tensor(1.5119), tensor(1.5313), tensor(1.5280), tensor(1.5480), tensor(1.6508), tensor(1.5227), tensor(1.4566), tensor(1.5072), tensor(1.4783), tensor(1.4613), tensor(1.4812), tensor(1.8276), tensor(1.5342), tensor(1.5329), tensor(1.5336), tensor(2.1876), tensor(1.5014), tensor(1.4782), tensor(1.4723), tensor(2.1401), tensor(1.5490), tensor(2.2383), tensor(1.6954), tensor(1.4647), tensor(1.5326), tensor(1.5978), tensor(1.4940), tensor(1.4739), tensor(1.4721), tensor(1.4890), tensor(1.4743), tensor(1.4757), tensor(1.5009), tensor(1.5018), tensor(1.6934), tensor(1.4891), tensor(1.4660), tensor(1.5189), tensor(1.7546), tensor(1.5549), tensor(2.3251), tensor(1.5555), tensor(1.5152), tensor(1.9889), tensor(1.7349), tensor(1.5771), tensor(1.5330), tensor(1.5209), tensor(1.8051), tensor(1.4854), tensor(1.5854), tensor(1.5032), tensor(5.1280), tensor(1.4730), tensor(1.5797), tensor(1.8951), tensor(1.4834), tensor(1.4886), tensor(1.5992), tensor(1.6809), tensor(1.4835), tensor(1.8318), tensor(1.5159), tensor(1.4897), tensor(2.4699), tensor(1.4845), tensor(1.5473), tensor(1.5199), tensor(1.7983), tensor(1.4660), tensor(1.4963), tensor(2.3987), tensor(1.5603), tensor(1.4957), tensor(1.5503), tensor(1.4854), tensor(3.3194), tensor(1.4749), tensor(1.4949), tensor(1.4778), tensor(1.6837), tensor(1.4974), tensor(2.0493), tensor(2.4061), tensor(1.4823), tensor(1.5231), tensor(1.4838), tensor(1.4721), tensor(1.6202), tensor(1.5125), tensor(1.4958), tensor(1.5256), tensor(1.4809), tensor(1.6623), tensor(1.5061), tensor(1.8123), tensor(1.5141), tensor(1.5335), tensor(1.5033), tensor(1.5642), tensor(1.4769), tensor(1.6060), tensor(1.5512), tensor(1.4882), tensor(1.7266), tensor(1.6162), tensor(1.5510), tensor(1.5195), tensor(1.5342), tensor(1.4843), tensor(1.4726), tensor(1.5320), tensor(1.4808), tensor(1.4634)]
09/23/2021 14:18:27 - INFO - __main__ - after_losses=[tensor(1.4711), tensor(1.4742), tensor(1.4348), tensor(1.5491), tensor(1.4729), tensor(1.6765), tensor(1.4739), tensor(1.4771), tensor(1.4894), tensor(1.5311), tensor(1.5291), tensor(1.5328), tensor(1.5530), tensor(1.5397), tensor(1.6788), tensor(1.5194), tensor(1.4713), tensor(1.4762), tensor(1.5909), tensor(1.4823), tensor(1.4710), tensor(1.4711), tensor(1.4487), tensor(2.4368), tensor(1.4958), tensor(1.4807), tensor(1.9304), tensor(2.3309), tensor(1.5758), tensor(1.5468), tensor(1.8158), tensor(1.4527), tensor(1.7697), tensor(1.4999), tensor(1.6129), tensor(1.5079), tensor(2.3626), tensor(1.4767), tensor(1.4792), tensor(1.4734), tensor(1.4946), tensor(1.5159), tensor(1.5015), tensor(1.9500), tensor(1.5002), tensor(2.0801), tensor(1.5788), tensor(1.9703), tensor(1.5349), tensor(1.4674), tensor(1.4712), tensor(1.5550), tensor(1.4450), tensor(1.5639), tensor(2.6800), tensor(1.4842), tensor(2.1889), tensor(1.4902), tensor(1.4510), tensor(1.4880), tensor(2.0333), tensor(1.6454), tensor(1.4694), tensor(1.5198), tensor(3.5727), tensor(3.2789), tensor(1.4570), tensor(1.5118), tensor(1.5302), tensor(1.4523), tensor(1.4703), tensor(1.5249), tensor(1.4968), tensor(1.4716), tensor(1.4747), tensor(1.5040), tensor(1.5255), tensor(1.5413), tensor(1.4437), tensor(1.7489), tensor(1.5357), tensor(1.5394), tensor(1.6794), tensor(1.4978), tensor(1.5334), tensor(2.0192), tensor(1.4653), tensor(1.5131), tensor(1.9018), tensor(1.6978), tensor(1.4779), tensor(1.4606), tensor(1.9465), tensor(2.6352), tensor(1.4696), tensor(1.5763), tensor(1.5188), tensor(1.4796), tensor(1.4493), tensor(1.4929), tensor(1.4835), tensor(1.4637), tensor(1.4665), tensor(1.5061), tensor(1.5025), tensor(1.5033), tensor(1.4690), tensor(1.4930), tensor(1.5553), tensor(1.4431), tensor(2.0003), tensor(1.6971), tensor(1.4711), tensor(1.5436), tensor(1.5769), tensor(1.4716), tensor(1.5197), tensor(1.5033), tensor(1.4628), tensor(1.4782), tensor(1.4679), tensor(2.0277), tensor(1.5864), tensor(1.4511), tensor(1.5549), tensor(1.5038), tensor(1.6081), tensor(2.9519), tensor(1.5789), tensor(1.5766), tensor(1.5904), tensor(1.5119), tensor(1.5386), tensor(1.5409), tensor(1.8733), tensor(2.0716), tensor(1.6171), tensor(1.4471), tensor(1.5019), tensor(1.5366), tensor(1.9626), tensor(1.4923), tensor(1.4944), tensor(1.5341), tensor(1.5300), tensor(1.4815), tensor(1.6254), tensor(1.4922), tensor(1.4507), tensor(1.5448), tensor(1.4572), tensor(1.4499), tensor(1.4696), tensor(1.8435), tensor(1.5062), tensor(1.4894), tensor(1.5188), tensor(2.0303), tensor(1.4910), tensor(1.5105), tensor(1.4579), tensor(1.8942), tensor(1.5226), tensor(2.2074), tensor(1.6333), tensor(1.4972), tensor(1.5003), tensor(1.5884), tensor(1.4828), tensor(1.4626), tensor(1.4756), tensor(1.4732), tensor(1.4716), tensor(1.4871), tensor(1.4679), tensor(1.4746), tensor(1.8010), tensor(1.4581), tensor(1.4493), tensor(1.4960), tensor(1.7938), tensor(1.7077), tensor(2.2188), tensor(1.5519), tensor(1.4730), tensor(1.9235), tensor(1.4793), tensor(1.5892), tensor(1.5052), tensor(1.5218), tensor(1.8448), tensor(1.4754), tensor(1.7027), tensor(1.5440), tensor(5.2033), tensor(1.4620), tensor(2.2014), tensor(1.8675), tensor(1.4699), tensor(1.5428), tensor(1.5281), tensor(1.6422), tensor(1.4729), tensor(1.9444), tensor(1.4999), tensor(1.4729), tensor(2.2298), tensor(1.4774), tensor(1.5618), tensor(1.6021), tensor(1.9305), tensor(1.4572), tensor(1.5832), tensor(2.3321), tensor(1.5515), tensor(1.4687), tensor(1.5050), tensor(1.4707), tensor(3.1134), tensor(1.4632), tensor(1.5115), tensor(1.4470), tensor(1.7979), tensor(1.4975), tensor(2.1329), tensor(2.5014), tensor(1.4869), tensor(1.5213), tensor(1.4704), tensor(1.4658), tensor(1.7285), tensor(1.4868), tensor(1.4814), tensor(1.5205), tensor(1.4638), tensor(1.5835), tensor(1.4764), tensor(1.7306), tensor(1.5435), tensor(1.5187), tensor(1.4903), tensor(1.5650), tensor(1.4618), tensor(2.7338), tensor(1.6060), tensor(1.4719), tensor(1.6877), tensor(2.1997), tensor(1.5297), tensor(1.5159), tensor(1.5310), tensor(1.4718), tensor(1.4594), tensor(1.4940), tensor(1.4908), tensor(1.4502)]
09/23/2021 14:18:27 - INFO - __main__ - interference_scores=[tensor(-0.0057), tensor(-0.0030), tensor(-0.0044), tensor(0.0093), tensor(-0.0433), tensor(0.1207), tensor(-1.1859), tensor(-0.0116), tensor(-0.0153), tensor(-0.0238), tensor(-0.0002), tensor(0.0578), tensor(-0.0255), tensor(0.0274), tensor(0.1651), tensor(0.0435), tensor(-0.0230), tensor(-0.0570), tensor(0.0336), tensor(-0.0160), tensor(-0.0290), tensor(-0.0195), tensor(-0.0072), tensor(0.0531), tensor(-0.0181), tensor(-0.0808), tensor(-0.0327), tensor(-0.1141), tensor(-0.0169), tensor(-0.0445), tensor(0.0271), tensor(-0.0061), tensor(0.0507), tensor(-0.0122), tensor(-0.0646), tensor(0.0183), tensor(-0.2386), tensor(-0.0146), tensor(-0.0178), tensor(-0.0015), tensor(-0.0247), tensor(-0.0206), tensor(-0.0200), tensor(0.1354), tensor(0.0245), tensor(0.0287), tensor(-0.0211), tensor(0.0668), tensor(0.0034), tensor(-0.0302), tensor(-0.0342), tensor(0.0341), tensor(-0.0170), tensor(-0.0266), tensor(-0.0463), tensor(-0.0269), tensor(0.0861), tensor(-0.0269), tensor(-0.0100), tensor(-0.0228), tensor(0.0414), tensor(0.0001), tensor(-0.0123), tensor(-0.0224), tensor(0.0036), tensor(-0.1764), tensor(-0.0238), tensor(0.0050), tensor(-0.1185), tensor(-0.0073), tensor(-0.0217), tensor(-0.0295), tensor(-0.0240), tensor(0.0010), tensor(-0.0267), tensor(0.0075), tensor(0.0180), tensor(-0.0408), tensor(-0.0113), tensor(0.0372), tensor(-0.0044), tensor(-0.0429), tensor(0.0559), tensor(-0.0320), tensor(-0.0445), tensor(-0.0140), tensor(-0.0135), tensor(-0.0294), tensor(-0.0095), tensor(-0.0337), tensor(-0.0166), tensor(-0.7795), tensor(0.1425), tensor(0.0013), tensor(0.0006), tensor(-0.0341), tensor(-0.0058), tensor(-0.0173), tensor(-0.0137), tensor(-0.0297), tensor(-0.0141), tensor(-0.0021), tensor(-0.0254), tensor(-0.0144), tensor(-0.0259), tensor(0.0163), tensor(-0.0123), tensor(0.0095), tensor(0.0368), tensor(-0.0100), tensor(0.1284), tensor(0.1526), tensor(0.0057), tensor(0.0419), tensor(-0.1545), tensor(-0.0234), tensor(0.0030), tensor(-0.0343), tensor(-0.0158), tensor(-0.0067), tensor(-0.0188), tensor(-0.0168), tensor(0.0475), tensor(-0.0117), tensor(-0.0015), tensor(-0.0444), tensor(0.0176), tensor(-0.0233), tensor(-0.0389), tensor(0.0447), tensor(0.0375), tensor(-0.0073), tensor(0.0437), tensor(-0.0419), tensor(0.2404), tensor(-0.2013), tensor(-0.0961), tensor(-0.0141), tensor(-0.0144), tensor(-0.0454), tensor(-0.0619), tensor(-0.0279), tensor(-0.0175), tensor(0.0028), tensor(0.0020), tensor(-0.0665), tensor(-0.0254), tensor(-0.0305), tensor(-0.0059), tensor(0.0376), tensor(-0.0211), tensor(-0.0114), tensor(-0.0116), tensor(0.0159), tensor(-0.0280), tensor(-0.0435), tensor(-0.0147), tensor(-0.1573), tensor(-0.0104), tensor(0.0323), tensor(-0.0144), tensor(-0.2459), tensor(-0.0264), tensor(-0.0309), tensor(-0.0620), tensor(0.0326), tensor(-0.0323), tensor(-0.0095), tensor(-0.0112), tensor(-0.0113), tensor(0.0035), tensor(-0.0158), tensor(-0.0027), tensor(0.0114), tensor(-0.0330), tensor(-0.0271), tensor(0.1076), tensor(-0.0309), tensor(-0.0166), tensor(-0.0229), tensor(0.0392), tensor(0.1528), tensor(-0.1063), tensor(-0.0036), tensor(-0.0422), tensor(-0.0654), tensor(-0.2556), tensor(0.0121), tensor(-0.0278), tensor(0.0008), tensor(0.0397), tensor(-0.0099), tensor(0.1174), tensor(0.0408), tensor(0.0753), tensor(-0.0110), tensor(0.6217), tensor(-0.0276), tensor(-0.0136), tensor(0.0541), tensor(-0.0710), tensor(-0.0387), tensor(-0.0106), tensor(0.1127), tensor(-0.0161), tensor(-0.0168), tensor(-0.2401), tensor(-0.0071), tensor(0.0145), tensor(0.0821), tensor(0.1322), tensor(-0.0088), tensor(0.0869), tensor(-0.0665), tensor(-0.0088), tensor(-0.0269), tensor(-0.0453), tensor(-0.0147), tensor(-0.2060), tensor(-0.0117), tensor(0.0166), tensor(-0.0308), tensor(0.1143), tensor(4.7088e-05), tensor(0.0836), tensor(0.0953), tensor(0.0046), tensor(-0.0017), tensor(-0.0134), tensor(-0.0063), tensor(0.1083), tensor(-0.0257), tensor(-0.0144), tensor(-0.0051), tensor(-0.0171), tensor(-0.0788), tensor(-0.0297), tensor(-0.0818), tensor(0.0294), tensor(-0.0148), tensor(-0.0130), tensor(0.0009), tensor(-0.0151), tensor(1.1278), tensor(0.0547), tensor(-0.0163), tensor(-0.0389), tensor(0.5834), tensor(-0.0213), tensor(-0.0036), tensor(-0.0031), tensor(-0.0125), tensor(-0.0133), tensor(-0.0380), tensor(0.0101), tensor(-0.0133)]
09/23/2021 14:18:27 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-3887', 'mrqa_triviaqa-validation-743', 'mrqa_triviaqa-validation-5261', 'mrqa_triviaqa-validation-6331', 'mrqa_triviaqa-validation-3920', 'mrqa_triviaqa-validation-2210', 'mrqa_triviaqa-validation-3420', 'mrqa_triviaqa-validation-5071', 'mrqa_triviaqa-validation-3340', 'mrqa_triviaqa-validation-2368', 'mrqa_triviaqa-validation-1921', 'mrqa_squad-validation-10403', 'mrqa_triviaqa-validation-6699', 'mrqa_squad-validation-2313', 'mrqa_triviaqa-validation-3760', 'mrqa_triviaqa-validation-2091', 'mrqa_hotpotqa-validation-3333', 'mrqa_squad-validation-6837', 'mrqa_triviaqa-validation-227', 'mrqa_triviaqa-validation-1671', 'mrqa_squad-validation-3558', 'mrqa_squad-validation-10459', 'mrqa_triviaqa-validation-3515', 'mrqa_squad-validation-772', 'mrqa_naturalquestions-validation-9650', 'mrqa_hotpotqa-validation-945', 'mrqa_triviaqa-validation-4966', 'mrqa_naturalquestions-validation-430', 'mrqa_squad-validation-1276', 'mrqa_triviaqa-validation-1423', 'mrqa_squad-validation-4169', 'mrqa_triviaqa-validation-46']
09/23/2021 14:18:27 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:18:27 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=59) .... Timecode: 33
09/23/2021 14:18:40 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:18:40 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 33
09/23/2021 14:18:44 - INFO - __main__ - After Error Fixing: 0.96875
09/23/2021 14:18:44 - INFO - __main__ - Instant Fixing Rate: 0.9629629629629629
09/23/2021 14:18:44 - INFO - __main__ - Instant Retention Rate: 0.9999999980000001
09/23/2021 14:18:46 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_034.pt.
09/23/2021 14:18:46 - INFO - __main__ - Saving the current error examples (len=27) to the memory.
09/23/2021 14:18:46 - INFO - __main__ - Current memory size: 852.
09/23/2021 14:18:46 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:18:46 - INFO - __main__ - Finished.
09/23/2021 14:18:46 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:18:46 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:18:46 - INFO - __main__ - Evaluating to get errors .... Timecode: 34
09/23/2021 14:18:49 - INFO - __main__ - Before Error Fixing: 0.09375
09/23/2021 14:18:49 - INFO - __main__ - Found 29 errors.
09/23/2021 14:18:49 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:18:49 - INFO - __main__ - Current memory size: 879.
09/23/2021 14:18:49 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:19:12 - INFO - __main__ - before_losses=[tensor(1.5460), tensor(1.5129), tensor(1.5066), tensor(1.7242), tensor(1.6113), tensor(1.4951), tensor(1.5761), tensor(1.4860), tensor(1.5501), tensor(1.5476), tensor(1.5820), tensor(1.4978), tensor(1.5447), tensor(1.6850), tensor(1.5402), tensor(1.5620), tensor(1.9063), tensor(1.5221), tensor(1.5167), tensor(1.4764), tensor(1.5235), tensor(1.5415), tensor(1.4851), tensor(1.5711), tensor(1.4797), tensor(1.8260), tensor(1.5242), tensor(1.6394), tensor(1.6128), tensor(1.4841), tensor(1.5303), tensor(1.5181), tensor(1.5267), tensor(1.5105), tensor(1.6406), tensor(1.4774), tensor(1.5769), tensor(1.4829), tensor(1.4806), tensor(1.5732), tensor(1.5231), tensor(1.5079), tensor(1.5690), tensor(1.5465), tensor(1.4806), tensor(1.5454), tensor(1.4993), tensor(1.5607), tensor(1.5565), tensor(1.4812), tensor(1.5409), tensor(2.5409), tensor(1.5008), tensor(1.5307), tensor(1.5130), tensor(1.5031), tensor(1.7760), tensor(1.5082), tensor(1.5553), tensor(2.3126), tensor(3.1257), tensor(1.4801), tensor(1.5902), tensor(1.5548), tensor(1.5455), tensor(1.4973), tensor(1.5050), tensor(1.4830), tensor(1.4918), tensor(1.4868), tensor(1.5972), tensor(1.5133), tensor(1.4628), tensor(1.5304), tensor(1.9188), tensor(1.4852), tensor(1.4740), tensor(1.5157), tensor(1.5196), tensor(1.5499), tensor(1.8563), tensor(1.5436), tensor(1.6069), tensor(1.9171), tensor(1.5681), tensor(2.7942), tensor(1.5004), tensor(1.5637), tensor(1.5399), tensor(1.9816), tensor(1.6685), tensor(1.5097), tensor(1.4767), tensor(1.5654), tensor(1.4833), tensor(1.5976), tensor(1.6630), tensor(1.5206), tensor(1.5067), tensor(1.5077), tensor(1.4984), tensor(1.5824), tensor(1.4855), tensor(1.7213), tensor(1.5194), tensor(1.5346), tensor(1.4858), tensor(1.5475), tensor(1.5638), tensor(1.5174), tensor(1.5098), tensor(1.5218), tensor(1.5173), tensor(1.4608), tensor(1.5408), tensor(1.5323), tensor(1.5782), tensor(1.5344), tensor(1.4641), tensor(2.2204), tensor(1.4720), tensor(1.5091), tensor(1.4900), tensor(1.5590), tensor(1.4893), tensor(1.4905), tensor(1.4849), tensor(1.4897), tensor(1.7741), tensor(1.5209), tensor(1.5451), tensor(1.5051), tensor(1.4931), tensor(2.3820), tensor(1.5327), tensor(1.6317), tensor(1.8196), tensor(1.5188), tensor(1.4910), tensor(1.4781), tensor(1.5624), tensor(1.5012), tensor(1.5157), tensor(1.7492), tensor(1.8837), tensor(1.5449), tensor(1.9652), tensor(2.6660), tensor(1.5413), tensor(1.5350), tensor(1.8644), tensor(1.5009), tensor(1.5356), tensor(1.5591), tensor(1.5614), tensor(1.5528), tensor(1.5455), tensor(1.5378), tensor(1.5954), tensor(1.5313), tensor(1.4981), tensor(1.5327), tensor(1.5073), tensor(1.5680), tensor(1.8056), tensor(1.4887), tensor(1.4750), tensor(2.0243), tensor(1.5558), tensor(1.5000), tensor(1.5239), tensor(1.4983), tensor(1.4779), tensor(1.4950), tensor(1.5286), tensor(1.6025), tensor(2.1366), tensor(1.5355), tensor(1.5832), tensor(1.5353), tensor(1.5118), tensor(2.1293), tensor(1.5024), tensor(1.6192), tensor(2.7908), tensor(1.4747), tensor(2.4329), tensor(2.4030), tensor(1.5622), tensor(1.7648), tensor(1.4583), tensor(1.6222), tensor(1.5237), tensor(1.5807), tensor(1.5792), tensor(1.5343), tensor(1.5601), tensor(1.4793), tensor(1.5236), tensor(1.5071), tensor(2.0014), tensor(1.4931), tensor(5.2560), tensor(1.5854), tensor(1.4625), tensor(1.5365), tensor(1.5075), tensor(1.5509), tensor(1.5073), tensor(1.5327), tensor(1.5972), tensor(1.5272), tensor(1.7140), tensor(1.5076), tensor(1.5909), tensor(1.5718), tensor(1.5076), tensor(1.5679), tensor(1.5039), tensor(1.5417), tensor(1.4776), tensor(2.1391), tensor(2.1498), tensor(1.5082), tensor(1.9870), tensor(1.4882), tensor(1.5624), tensor(1.6211), tensor(1.5250), tensor(1.5009), tensor(1.4829), tensor(1.5102), tensor(1.7442), tensor(3.6357), tensor(1.4985), tensor(1.8787), tensor(1.5375), tensor(1.5198), tensor(1.5053), tensor(1.7165), tensor(1.4792), tensor(2.2641), tensor(1.4960), tensor(1.7919), tensor(1.5545), tensor(1.7941), tensor(1.5150), tensor(1.5486), tensor(1.4680), tensor(1.4880), tensor(1.5687), tensor(2.2069), tensor(1.5149), tensor(1.4964), tensor(1.4812), tensor(1.5373)]
09/23/2021 14:19:12 - INFO - __main__ - after_losses=[tensor(1.5789), tensor(1.4970), tensor(1.4927), tensor(1.6885), tensor(1.5639), tensor(1.4804), tensor(1.5806), tensor(1.4728), tensor(1.5306), tensor(1.5361), tensor(1.5590), tensor(1.4860), tensor(1.5164), tensor(1.6286), tensor(1.5256), tensor(1.5378), tensor(1.8821), tensor(1.4964), tensor(2.2551), tensor(1.4560), tensor(1.4956), tensor(1.5833), tensor(1.4626), tensor(1.5810), tensor(1.4760), tensor(1.8449), tensor(1.5028), tensor(1.5965), tensor(1.6637), tensor(1.4789), tensor(1.5158), tensor(1.5012), tensor(1.5047), tensor(1.4942), tensor(1.6879), tensor(1.4835), tensor(1.5379), tensor(1.4644), tensor(1.4699), tensor(1.5257), tensor(1.5019), tensor(1.4958), tensor(1.6412), tensor(1.5454), tensor(1.4650), tensor(1.5682), tensor(1.4668), tensor(1.5504), tensor(1.5393), tensor(1.4763), tensor(1.5173), tensor(2.3852), tensor(1.4660), tensor(1.5388), tensor(1.4983), tensor(1.6609), tensor(1.9408), tensor(1.4875), tensor(1.5613), tensor(2.3788), tensor(3.1544), tensor(1.4673), tensor(1.6969), tensor(1.5371), tensor(1.5213), tensor(1.4872), tensor(1.5837), tensor(1.4722), tensor(1.4884), tensor(1.4902), tensor(1.5672), tensor(1.5003), tensor(1.4627), tensor(1.4984), tensor(2.1686), tensor(1.4684), tensor(1.4508), tensor(1.5018), tensor(1.5707), tensor(1.5319), tensor(1.8062), tensor(1.5286), tensor(1.6346), tensor(2.1916), tensor(1.5337), tensor(2.9852), tensor(1.5028), tensor(1.5418), tensor(1.5315), tensor(2.0061), tensor(1.6932), tensor(1.4994), tensor(1.4678), tensor(1.5144), tensor(1.4708), tensor(1.5363), tensor(1.6448), tensor(1.4972), tensor(1.4975), tensor(1.4879), tensor(1.4605), tensor(1.5843), tensor(1.4731), tensor(1.6510), tensor(1.4939), tensor(1.5222), tensor(1.4946), tensor(1.5264), tensor(1.5498), tensor(1.5276), tensor(1.5060), tensor(1.5035), tensor(1.4993), tensor(1.4500), tensor(1.5120), tensor(1.5102), tensor(1.6244), tensor(1.5028), tensor(1.4581), tensor(1.5166), tensor(1.4628), tensor(1.4881), tensor(1.4810), tensor(1.5319), tensor(1.4768), tensor(1.4749), tensor(1.4754), tensor(1.4829), tensor(1.7725), tensor(1.5058), tensor(1.5418), tensor(1.4894), tensor(1.4718), tensor(1.5317), tensor(1.5086), tensor(1.6803), tensor(1.7845), tensor(1.5114), tensor(1.4795), tensor(1.4780), tensor(1.5476), tensor(1.4741), tensor(1.5064), tensor(1.7761), tensor(1.9769), tensor(1.5317), tensor(1.8663), tensor(2.4503), tensor(1.5136), tensor(1.5067), tensor(1.8916), tensor(1.4635), tensor(1.5000), tensor(1.5142), tensor(1.5288), tensor(1.5315), tensor(1.5646), tensor(1.5015), tensor(1.7384), tensor(1.4940), tensor(1.4793), tensor(1.5107), tensor(1.5016), tensor(1.5528), tensor(1.9263), tensor(1.4718), tensor(1.4836), tensor(2.2566), tensor(1.5787), tensor(1.5108), tensor(1.5013), tensor(1.4892), tensor(1.5088), tensor(1.4840), tensor(1.5026), tensor(1.6951), tensor(2.1247), tensor(1.5014), tensor(1.5615), tensor(1.5173), tensor(1.4835), tensor(2.2535), tensor(1.4653), tensor(1.7700), tensor(2.9940), tensor(1.4676), tensor(2.6932), tensor(2.3505), tensor(1.5348), tensor(1.7281), tensor(1.4592), tensor(1.5941), tensor(1.5005), tensor(1.5680), tensor(1.5520), tensor(1.4962), tensor(1.5604), tensor(1.4605), tensor(1.4952), tensor(1.4756), tensor(1.6435), tensor(1.4862), tensor(4.9376), tensor(1.5516), tensor(1.4499), tensor(1.5092), tensor(1.5024), tensor(1.5172), tensor(1.4668), tensor(1.5360), tensor(1.5526), tensor(1.5007), tensor(1.5219), tensor(1.5011), tensor(1.5699), tensor(1.6375), tensor(1.4903), tensor(1.5367), tensor(1.5284), tensor(1.5330), tensor(1.4461), tensor(2.1761), tensor(2.2149), tensor(1.4909), tensor(1.5048), tensor(1.4751), tensor(1.5486), tensor(1.5809), tensor(1.4998), tensor(1.5025), tensor(1.4697), tensor(1.4769), tensor(1.8216), tensor(3.3658), tensor(1.4772), tensor(1.8954), tensor(1.5159), tensor(1.4993), tensor(1.4792), tensor(1.7003), tensor(1.4819), tensor(2.4699), tensor(1.4849), tensor(1.7177), tensor(1.5417), tensor(1.6476), tensor(1.4816), tensor(1.5286), tensor(1.4559), tensor(1.4816), tensor(1.5349), tensor(2.0910), tensor(1.5034), tensor(1.4811), tensor(1.4853), tensor(1.5288)]
09/23/2021 14:19:12 - INFO - __main__ - interference_scores=[tensor(0.0329), tensor(-0.0159), tensor(-0.0139), tensor(-0.0357), tensor(-0.0475), tensor(-0.0147), tensor(0.0045), tensor(-0.0132), tensor(-0.0195), tensor(-0.0115), tensor(-0.0230), tensor(-0.0119), tensor(-0.0283), tensor(-0.0564), tensor(-0.0146), tensor(-0.0242), tensor(-0.0242), tensor(-0.0257), tensor(0.7384), tensor(-0.0204), tensor(-0.0279), tensor(0.0419), tensor(-0.0226), tensor(0.0099), tensor(-0.0037), tensor(0.0189), tensor(-0.0214), tensor(-0.0429), tensor(0.0509), tensor(-0.0052), tensor(-0.0145), tensor(-0.0169), tensor(-0.0220), tensor(-0.0163), tensor(0.0473), tensor(0.0061), tensor(-0.0390), tensor(-0.0184), tensor(-0.0107), tensor(-0.0475), tensor(-0.0212), tensor(-0.0121), tensor(0.0722), tensor(-0.0011), tensor(-0.0156), tensor(0.0228), tensor(-0.0324), tensor(-0.0103), tensor(-0.0173), tensor(-0.0049), tensor(-0.0236), tensor(-0.1557), tensor(-0.0348), tensor(0.0082), tensor(-0.0147), tensor(0.1578), tensor(0.1648), tensor(-0.0207), tensor(0.0059), tensor(0.0662), tensor(0.0287), tensor(-0.0128), tensor(0.1067), tensor(-0.0177), tensor(-0.0241), tensor(-0.0100), tensor(0.0787), tensor(-0.0108), tensor(-0.0034), tensor(0.0034), tensor(-0.0301), tensor(-0.0130), tensor(-4.1485e-05), tensor(-0.0319), tensor(0.2498), tensor(-0.0168), tensor(-0.0232), tensor(-0.0138), tensor(0.0512), tensor(-0.0180), tensor(-0.0501), tensor(-0.0150), tensor(0.0277), tensor(0.2745), tensor(-0.0344), tensor(0.1910), tensor(0.0024), tensor(-0.0219), tensor(-0.0083), tensor(0.0245), tensor(0.0248), tensor(-0.0103), tensor(-0.0089), tensor(-0.0510), tensor(-0.0125), tensor(-0.0613), tensor(-0.0182), tensor(-0.0234), tensor(-0.0091), tensor(-0.0197), tensor(-0.0379), tensor(0.0020), tensor(-0.0124), tensor(-0.0704), tensor(-0.0255), tensor(-0.0124), tensor(0.0088), tensor(-0.0211), tensor(-0.0140), tensor(0.0102), tensor(-0.0038), tensor(-0.0183), tensor(-0.0180), tensor(-0.0108), tensor(-0.0288), tensor(-0.0221), tensor(0.0462), tensor(-0.0316), tensor(-0.0060), tensor(-0.7038), tensor(-0.0092), tensor(-0.0210), tensor(-0.0090), tensor(-0.0271), tensor(-0.0125), tensor(-0.0156), tensor(-0.0096), tensor(-0.0068), tensor(-0.0016), tensor(-0.0151), tensor(-0.0033), tensor(-0.0157), tensor(-0.0214), tensor(-0.8503), tensor(-0.0241), tensor(0.0486), tensor(-0.0351), tensor(-0.0074), tensor(-0.0115), tensor(-9.1434e-05), tensor(-0.0148), tensor(-0.0271), tensor(-0.0093), tensor(0.0270), tensor(0.0932), tensor(-0.0132), tensor(-0.0988), tensor(-0.2158), tensor(-0.0277), tensor(-0.0283), tensor(0.0272), tensor(-0.0374), tensor(-0.0355), tensor(-0.0448), tensor(-0.0326), tensor(-0.0212), tensor(0.0191), tensor(-0.0363), tensor(0.1431), tensor(-0.0374), tensor(-0.0188), tensor(-0.0219), tensor(-0.0057), tensor(-0.0153), tensor(0.1207), tensor(-0.0169), tensor(0.0086), tensor(0.2322), tensor(0.0229), tensor(0.0108), tensor(-0.0227), tensor(-0.0091), tensor(0.0310), tensor(-0.0110), tensor(-0.0259), tensor(0.0926), tensor(-0.0119), tensor(-0.0341), tensor(-0.0217), tensor(-0.0180), tensor(-0.0283), tensor(0.1243), tensor(-0.0371), tensor(0.1508), tensor(0.2032), tensor(-0.0071), tensor(0.2603), tensor(-0.0525), tensor(-0.0274), tensor(-0.0368), tensor(0.0009), tensor(-0.0281), tensor(-0.0231), tensor(-0.0126), tensor(-0.0273), tensor(-0.0380), tensor(0.0003), tensor(-0.0188), tensor(-0.0284), tensor(-0.0314), tensor(-0.3578), tensor(-0.0069), tensor(-0.3184), tensor(-0.0339), tensor(-0.0126), tensor(-0.0273), tensor(-0.0050), tensor(-0.0337), tensor(-0.0405), tensor(0.0033), tensor(-0.0446), tensor(-0.0265), tensor(-0.1922), tensor(-0.0066), tensor(-0.0209), tensor(0.0657), tensor(-0.0172), tensor(-0.0312), tensor(0.0245), tensor(-0.0087), tensor(-0.0315), tensor(0.0370), tensor(0.0650), tensor(-0.0173), tensor(-0.4822), tensor(-0.0131), tensor(-0.0138), tensor(-0.0403), tensor(-0.0252), tensor(0.0016), tensor(-0.0133), tensor(-0.0333), tensor(0.0774), tensor(-0.2699), tensor(-0.0214), tensor(0.0167), tensor(-0.0216), tensor(-0.0205), tensor(-0.0261), tensor(-0.0161), tensor(0.0027), tensor(0.2058), tensor(-0.0112), tensor(-0.0742), tensor(-0.0128), tensor(-0.1465), tensor(-0.0334), tensor(-0.0199), tensor(-0.0121), tensor(-0.0063), tensor(-0.0337), tensor(-0.1160), tensor(-0.0115), tensor(-0.0153), tensor(0.0041), tensor(-0.0085)]
09/23/2021 14:19:12 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-1671', 'mrqa_triviaqa-validation-6107', 'mrqa_squad-validation-3463', 'mrqa_squad-validation-5249', 'mrqa_triviaqa-validation-3847', 'mrqa_squad-validation-2812', 'mrqa_squad-validation-6677', 'mrqa_squad-validation-6924', 'mrqa_squad-validation-4506', 'mrqa_triviaqa-validation-1437', 'mrqa_triviaqa-validation-2635', 'mrqa_squad-validation-5184', 'mrqa_squad-validation-2884', 'mrqa_triviaqa-validation-4729', 'mrqa_triviaqa-validation-6692', 'mrqa_squad-validation-5407', 'mrqa_triviaqa-validation-4583', 'mrqa_triviaqa-validation-3714', 'mrqa_squad-validation-932', 'mrqa_triviaqa-validation-792', 'mrqa_triviaqa-validation-313', 'mrqa_triviaqa-validation-6721', 'mrqa_triviaqa-validation-5877', 'mrqa_triviaqa-validation-4209', 'mrqa_triviaqa-validation-2980', 'mrqa_triviaqa-validation-5698', 'mrqa_triviaqa-validation-2368', 'mrqa_squad-validation-6128', 'mrqa_triviaqa-validation-1578', 'mrqa_naturalquestions-validation-10490', 'mrqa_hotpotqa-validation-2970', 'mrqa_hotpotqa-validation-3333']
09/23/2021 14:19:12 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:19:12 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=61) .... Timecode: 34
09/23/2021 14:19:25 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:19:25 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 34
09/23/2021 14:19:29 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 14:19:29 - INFO - __main__ - Instant Fixing Rate: 0.896551724137931
09/23/2021 14:19:29 - INFO - __main__ - Instant Retention Rate: 0.3333333322222222
09/23/2021 14:19:30 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_035.pt.
09/23/2021 14:19:30 - INFO - __main__ - Saving the current error examples (len=29) to the memory.
09/23/2021 14:19:30 - INFO - __main__ - Current memory size: 879.
09/23/2021 14:19:30 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:19:30 - INFO - __main__ - Finished.
09/23/2021 14:19:30 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:19:30 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:19:30 - INFO - __main__ - Evaluating to get errors .... Timecode: 35
09/23/2021 14:19:34 - INFO - __main__ - Before Error Fixing: 0.15625
09/23/2021 14:19:34 - INFO - __main__ - Found 27 errors.
09/23/2021 14:19:34 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:19:34 - INFO - __main__ - Current memory size: 908.
09/23/2021 14:19:34 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:19:57 - INFO - __main__ - before_losses=[tensor(1.5221), tensor(1.5524), tensor(1.5581), tensor(1.9999), tensor(3.3067), tensor(2.1982), tensor(3.2402), tensor(1.4885), tensor(2.0654), tensor(1.4789), tensor(1.4921), tensor(1.4666), tensor(1.5844), tensor(1.4505), tensor(1.5739), tensor(1.5201), tensor(1.5634), tensor(1.4726), tensor(2.3333), tensor(1.5311), tensor(1.4875), tensor(1.5311), tensor(1.4791), tensor(1.4663), tensor(1.5499), tensor(1.6606), tensor(1.7266), tensor(1.4871), tensor(1.5057), tensor(1.6151), tensor(2.2137), tensor(1.4911), tensor(1.4698), tensor(1.5039), tensor(1.5689), tensor(1.8456), tensor(1.7024), tensor(1.4970), tensor(1.4773), tensor(1.5011), tensor(1.4889), tensor(1.5197), tensor(1.4874), tensor(2.1943), tensor(1.6115), tensor(2.0658), tensor(1.5525), tensor(1.5392), tensor(1.4593), tensor(1.4702), tensor(1.4823), tensor(1.6570), tensor(1.5260), tensor(1.8181), tensor(2.1193), tensor(1.4482), tensor(1.4736), tensor(1.4943), tensor(1.5431), tensor(1.5580), tensor(1.5115), tensor(1.5214), tensor(1.4886), tensor(1.5537), tensor(1.5105), tensor(1.6807), tensor(2.6418), tensor(1.4880), tensor(1.7439), tensor(1.4769), tensor(1.5488), tensor(1.5555), tensor(1.5064), tensor(1.5457), tensor(1.5015), tensor(1.5482), tensor(1.4887), tensor(1.6674), tensor(1.4800), tensor(1.4847), tensor(1.4766), tensor(1.4807), tensor(1.5199), tensor(1.5889), tensor(1.5030), tensor(1.4844), tensor(1.4928), tensor(1.8502), tensor(1.5348), tensor(1.5479), tensor(1.5038), tensor(1.4839), tensor(1.4744), tensor(1.5237), tensor(1.4888), tensor(1.9091), tensor(1.5613), tensor(1.5367), tensor(1.4720), tensor(2.3774), tensor(1.4426), tensor(1.9164), tensor(1.4932), tensor(1.4968), tensor(1.4592), tensor(1.5016), tensor(3.6976), tensor(1.5037), tensor(1.5471), tensor(1.5215), tensor(1.9658), tensor(1.4895), tensor(1.5078), tensor(1.5156), tensor(1.5517), tensor(1.5205), tensor(1.6070), tensor(1.5111), tensor(1.4946), tensor(1.4846), tensor(2.4653), tensor(1.5091), tensor(1.4551), tensor(1.5454), tensor(1.5008), tensor(1.5321), tensor(2.2412), tensor(1.4732), tensor(1.5121), tensor(1.6203), tensor(1.5251), tensor(2.2976), tensor(1.8845), tensor(1.5342), tensor(1.7564), tensor(1.5300), tensor(1.4803), tensor(1.9666), tensor(1.5583), tensor(1.7559), tensor(2.6180), tensor(1.5601), tensor(1.5240), tensor(2.3386), tensor(1.5566), tensor(3.4493), tensor(1.6261), tensor(1.4784), tensor(1.5911), tensor(1.6468), tensor(1.8198), tensor(1.6709), tensor(1.5106), tensor(1.5336), tensor(1.5347), tensor(1.9750), tensor(1.5582), tensor(1.6287), tensor(2.3726), tensor(2.0587), tensor(1.7958), tensor(1.6086), tensor(1.5299), tensor(1.4651), tensor(1.5516), tensor(1.8264), tensor(1.5677), tensor(1.4604), tensor(1.5277), tensor(1.5950), tensor(1.5658), tensor(1.5781), tensor(1.9796), tensor(1.5429), tensor(2.4053), tensor(1.4917), tensor(1.4900), tensor(1.4727), tensor(1.5581), tensor(1.5272), tensor(1.6267), tensor(2.0067), tensor(1.4921), tensor(1.6029), tensor(1.6244), tensor(1.5189), tensor(1.5234), tensor(1.4766), tensor(1.5384), tensor(1.4800), tensor(1.5521), tensor(2.0447), tensor(1.4626), tensor(1.5148), tensor(1.9973), tensor(1.5035), tensor(1.4925), tensor(1.5236), tensor(1.4810), tensor(1.6295), tensor(1.5311), tensor(1.5315), tensor(1.5655), tensor(1.4826), tensor(1.5650), tensor(1.4901), tensor(1.4813), tensor(1.5960), tensor(2.8206), tensor(1.6075), tensor(1.5369), tensor(1.5303), tensor(1.4940), tensor(1.4906), tensor(1.4794), tensor(1.4609), tensor(1.5402), tensor(1.5026), tensor(1.5824), tensor(2.8563), tensor(1.5838), tensor(1.5010), tensor(1.4888), tensor(2.0118), tensor(1.4742), tensor(2.1398), tensor(1.4788), tensor(1.5783), tensor(1.6815), tensor(1.4594), tensor(1.5424), tensor(1.5160), tensor(1.5709), tensor(1.5434), tensor(2.1604), tensor(1.5029), tensor(1.7695), tensor(1.5538), tensor(1.6431), tensor(2.4759), tensor(1.5654), tensor(2.0659), tensor(1.5708), tensor(1.5263), tensor(1.5297), tensor(1.5421), tensor(1.5146), tensor(1.5280), tensor(1.5970), tensor(1.5021), tensor(1.9977), tensor(1.4923), tensor(1.5413), tensor(1.5422), tensor(1.5109), tensor(1.4913)]
09/23/2021 14:19:57 - INFO - __main__ - after_losses=[tensor(1.4996), tensor(1.5653), tensor(1.5131), tensor(1.8871), tensor(3.2377), tensor(2.1438), tensor(3.2306), tensor(1.4930), tensor(2.2749), tensor(1.4651), tensor(1.4702), tensor(1.4935), tensor(1.5635), tensor(1.4416), tensor(1.5456), tensor(1.4908), tensor(1.5420), tensor(1.4753), tensor(1.6654), tensor(1.5088), tensor(1.4731), tensor(1.4932), tensor(1.4771), tensor(1.4546), tensor(1.5186), tensor(1.7364), tensor(1.5530), tensor(1.4594), tensor(1.5015), tensor(1.5133), tensor(2.3996), tensor(1.4701), tensor(1.4649), tensor(1.4882), tensor(1.5053), tensor(1.8239), tensor(1.6850), tensor(1.5994), tensor(1.5723), tensor(1.4821), tensor(1.4853), tensor(1.5176), tensor(1.4778), tensor(1.9802), tensor(1.5850), tensor(2.0665), tensor(1.7019), tensor(1.5331), tensor(1.4542), tensor(1.4734), tensor(1.4627), tensor(1.9887), tensor(1.5110), tensor(1.8537), tensor(2.0688), tensor(1.4378), tensor(1.4575), tensor(1.4556), tensor(1.5063), tensor(1.5246), tensor(1.5476), tensor(1.5314), tensor(1.4800), tensor(1.5843), tensor(1.4835), tensor(1.6095), tensor(2.2869), tensor(1.5019), tensor(1.5382), tensor(1.4793), tensor(1.8384), tensor(1.9881), tensor(1.5307), tensor(1.7132), tensor(1.4682), tensor(1.5312), tensor(1.4761), tensor(1.5568), tensor(1.4578), tensor(1.4857), tensor(1.4892), tensor(1.4561), tensor(1.5067), tensor(1.7541), tensor(1.4778), tensor(1.4585), tensor(1.4717), tensor(1.6278), tensor(1.5204), tensor(1.5047), tensor(1.4887), tensor(1.4833), tensor(1.4810), tensor(1.4998), tensor(1.4719), tensor(1.9846), tensor(1.7689), tensor(1.5504), tensor(1.4652), tensor(2.1282), tensor(1.4351), tensor(2.0071), tensor(1.4891), tensor(1.4917), tensor(1.4384), tensor(1.5503), tensor(3.5395), tensor(1.4865), tensor(1.5203), tensor(1.4872), tensor(1.8682), tensor(1.4782), tensor(1.4835), tensor(1.5222), tensor(1.5159), tensor(1.5017), tensor(1.6512), tensor(1.4706), tensor(1.4905), tensor(1.4689), tensor(2.2256), tensor(1.4811), tensor(1.4588), tensor(1.5749), tensor(1.5696), tensor(1.5347), tensor(1.6286), tensor(1.4579), tensor(1.4761), tensor(1.7348), tensor(1.4983), tensor(2.0855), tensor(1.9412), tensor(1.4931), tensor(1.7219), tensor(1.6454), tensor(1.4600), tensor(1.7503), tensor(1.5270), tensor(1.8047), tensor(2.6570), tensor(1.5330), tensor(1.5017), tensor(2.3407), tensor(1.5681), tensor(3.2744), tensor(1.5093), tensor(1.4556), tensor(1.5622), tensor(1.8028), tensor(1.7572), tensor(1.5972), tensor(1.4906), tensor(1.4940), tensor(1.8110), tensor(1.7647), tensor(1.8004), tensor(1.6930), tensor(2.0761), tensor(1.5011), tensor(1.8020), tensor(1.6103), tensor(1.5020), tensor(1.4527), tensor(1.4892), tensor(1.6416), tensor(1.6053), tensor(1.4492), tensor(1.5057), tensor(1.5545), tensor(1.5495), tensor(1.5071), tensor(1.7680), tensor(1.5814), tensor(2.4148), tensor(1.4773), tensor(1.5670), tensor(1.4656), tensor(1.5396), tensor(1.5050), tensor(1.5752), tensor(1.8881), tensor(1.4628), tensor(1.5684), tensor(1.6676), tensor(1.4949), tensor(1.5156), tensor(1.4692), tensor(1.5105), tensor(1.4644), tensor(1.5100), tensor(1.8977), tensor(1.4398), tensor(1.5023), tensor(2.1085), tensor(1.4825), tensor(1.6763), tensor(1.5381), tensor(1.4833), tensor(1.5792), tensor(1.5121), tensor(1.5188), tensor(1.5885), tensor(1.4651), tensor(1.5283), tensor(1.4939), tensor(1.4964), tensor(1.5902), tensor(2.8969), tensor(1.6271), tensor(1.5404), tensor(1.4961), tensor(1.5081), tensor(1.4819), tensor(1.4599), tensor(1.4484), tensor(1.5228), tensor(1.4827), tensor(1.5981), tensor(2.8017), tensor(1.5337), tensor(1.4806), tensor(1.4737), tensor(1.9604), tensor(1.4577), tensor(1.9944), tensor(1.4648), tensor(2.2204), tensor(1.6208), tensor(1.4502), tensor(1.5401), tensor(1.4910), tensor(1.5175), tensor(1.5431), tensor(1.8745), tensor(1.4776), tensor(1.7194), tensor(1.5216), tensor(1.6451), tensor(2.3973), tensor(1.5240), tensor(1.9727), tensor(1.5221), tensor(1.5262), tensor(1.5270), tensor(1.5090), tensor(1.5072), tensor(1.5079), tensor(2.0165), tensor(1.4780), tensor(2.0541), tensor(1.4909), tensor(1.5528), tensor(1.4939), tensor(1.5085), tensor(1.4717)]
09/23/2021 14:19:57 - INFO - __main__ - interference_scores=[tensor(-0.0225), tensor(0.0130), tensor(-0.0450), tensor(-0.1128), tensor(-0.0691), tensor(-0.0544), tensor(-0.0096), tensor(0.0045), tensor(0.2095), tensor(-0.0139), tensor(-0.0219), tensor(0.0269), tensor(-0.0209), tensor(-0.0089), tensor(-0.0282), tensor(-0.0293), tensor(-0.0214), tensor(0.0026), tensor(-0.6679), tensor(-0.0223), tensor(-0.0144), tensor(-0.0379), tensor(-0.0020), tensor(-0.0117), tensor(-0.0314), tensor(0.0758), tensor(-0.1736), tensor(-0.0277), tensor(-0.0042), tensor(-0.1018), tensor(0.1859), tensor(-0.0210), tensor(-0.0050), tensor(-0.0157), tensor(-0.0635), tensor(-0.0217), tensor(-0.0173), tensor(0.1024), tensor(0.0950), tensor(-0.0190), tensor(-0.0037), tensor(-0.0021), tensor(-0.0097), tensor(-0.2141), tensor(-0.0265), tensor(0.0006), tensor(0.1494), tensor(-0.0061), tensor(-0.0051), tensor(0.0032), tensor(-0.0196), tensor(0.3318), tensor(-0.0149), tensor(0.0357), tensor(-0.0505), tensor(-0.0104), tensor(-0.0161), tensor(-0.0388), tensor(-0.0368), tensor(-0.0334), tensor(0.0361), tensor(0.0100), tensor(-0.0086), tensor(0.0306), tensor(-0.0270), tensor(-0.0712), tensor(-0.3549), tensor(0.0140), tensor(-0.2058), tensor(0.0024), tensor(0.2896), tensor(0.4326), tensor(0.0243), tensor(0.1675), tensor(-0.0333), tensor(-0.0169), tensor(-0.0126), tensor(-0.1106), tensor(-0.0222), tensor(0.0009), tensor(0.0126), tensor(-0.0246), tensor(-0.0132), tensor(0.1653), tensor(-0.0252), tensor(-0.0259), tensor(-0.0211), tensor(-0.2223), tensor(-0.0145), tensor(-0.0432), tensor(-0.0151), tensor(-0.0006), tensor(0.0066), tensor(-0.0239), tensor(-0.0169), tensor(0.0756), tensor(0.2076), tensor(0.0137), tensor(-0.0068), tensor(-0.2492), tensor(-0.0075), tensor(0.0907), tensor(-0.0041), tensor(-0.0050), tensor(-0.0209), tensor(0.0487), tensor(-0.1581), tensor(-0.0172), tensor(-0.0268), tensor(-0.0343), tensor(-0.0977), tensor(-0.0112), tensor(-0.0243), tensor(0.0065), tensor(-0.0357), tensor(-0.0188), tensor(0.0443), tensor(-0.0405), tensor(-0.0041), tensor(-0.0156), tensor(-0.2398), tensor(-0.0280), tensor(0.0037), tensor(0.0295), tensor(0.0688), tensor(0.0025), tensor(-0.6126), tensor(-0.0153), tensor(-0.0360), tensor(0.1145), tensor(-0.0268), tensor(-0.2121), tensor(0.0567), tensor(-0.0411), tensor(-0.0345), tensor(0.1155), tensor(-0.0203), tensor(-0.2163), tensor(-0.0312), tensor(0.0489), tensor(0.0390), tensor(-0.0271), tensor(-0.0223), tensor(0.0021), tensor(0.0115), tensor(-0.1749), tensor(-0.1168), tensor(-0.0227), tensor(-0.0288), tensor(0.1560), tensor(-0.0626), tensor(-0.0737), tensor(-0.0200), tensor(-0.0397), tensor(0.2763), tensor(-0.2103), tensor(0.2421), tensor(0.0643), tensor(-0.2965), tensor(-0.5576), tensor(0.0062), tensor(0.0017), tensor(-0.0279), tensor(-0.0125), tensor(-0.0623), tensor(-0.1848), tensor(0.0376), tensor(-0.0112), tensor(-0.0219), tensor(-0.0405), tensor(-0.0163), tensor(-0.0711), tensor(-0.2115), tensor(0.0384), tensor(0.0095), tensor(-0.0144), tensor(0.0770), tensor(-0.0070), tensor(-0.0186), tensor(-0.0223), tensor(-0.0515), tensor(-0.1186), tensor(-0.0293), tensor(-0.0344), tensor(0.0432), tensor(-0.0240), tensor(-0.0079), tensor(-0.0074), tensor(-0.0279), tensor(-0.0156), tensor(-0.0422), tensor(-0.1470), tensor(-0.0229), tensor(-0.0125), tensor(0.1111), tensor(-0.0210), tensor(0.1838), tensor(0.0145), tensor(0.0023), tensor(-0.0502), tensor(-0.0190), tensor(-0.0126), tensor(0.0230), tensor(-0.0175), tensor(-0.0367), tensor(0.0038), tensor(0.0151), tensor(-0.0057), tensor(0.0763), tensor(0.0196), tensor(0.0034), tensor(-0.0342), tensor(0.0142), tensor(-0.0088), tensor(-0.0195), tensor(-0.0126), tensor(-0.0174), tensor(-0.0198), tensor(0.0157), tensor(-0.0547), tensor(-0.0501), tensor(-0.0204), tensor(-0.0151), tensor(-0.0514), tensor(-0.0166), tensor(-0.1454), tensor(-0.0140), tensor(0.6420), tensor(-0.0608), tensor(-0.0092), tensor(-0.0023), tensor(-0.0250), tensor(-0.0533), tensor(-0.0003), tensor(-0.2859), tensor(-0.0252), tensor(-0.0501), tensor(-0.0322), tensor(0.0020), tensor(-0.0786), tensor(-0.0414), tensor(-0.0932), tensor(-0.0487), tensor(-7.8797e-05), tensor(-0.0027), tensor(-0.0331), tensor(-0.0074), tensor(-0.0201), tensor(0.4195), tensor(-0.0242), tensor(0.0564), tensor(-0.0014), tensor(0.0115), tensor(-0.0483), tensor(-0.0025), tensor(-0.0195)]
09/23/2021 14:19:57 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-5249', 'mrqa_triviaqa-validation-935', 'mrqa_squad-validation-4228', 'mrqa_triviaqa-validation-2376', 'mrqa_hotpotqa-validation-501', 'mrqa_triviaqa-validation-2368', 'mrqa_triviaqa-validation-4856', 'mrqa_triviaqa-validation-1521', 'mrqa_triviaqa-validation-7684', 'mrqa_triviaqa-validation-2961', 'mrqa_triviaqa-validation-2007', 'mrqa_triviaqa-validation-7153', 'mrqa_triviaqa-validation-4308', 'mrqa_triviaqa-validation-5406', 'mrqa_hotpotqa-validation-4463', 'mrqa_hotpotqa-validation-3161', 'mrqa_naturalquestions-validation-9171', 'mrqa_triviaqa-validation-2802', 'mrqa_triviaqa-validation-6683', 'mrqa_squad-validation-358', 'mrqa_squad-validation-932', 'mrqa_hotpotqa-validation-1210', 'mrqa_squad-validation-2010', 'mrqa_hotpotqa-validation-4720', 'mrqa_squad-validation-8464', 'mrqa_triviaqa-validation-1085', 'mrqa_squad-validation-608', 'mrqa_triviaqa-validation-6950', 'mrqa_naturalquestions-validation-9852', 'mrqa_naturalquestions-validation-1085', 'mrqa_squad-validation-8223', 'mrqa_naturalquestions-validation-4309']
09/23/2021 14:19:57 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:19:57 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=59) .... Timecode: 35
09/23/2021 14:20:10 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:20:10 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 35
09/23/2021 14:20:13 - INFO - __main__ - After Error Fixing: 0.9375
09/23/2021 14:20:13 - INFO - __main__ - Instant Fixing Rate: 1.0
09/23/2021 14:20:13 - INFO - __main__ - Instant Retention Rate: 0.5999999988
09/23/2021 14:20:15 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_036.pt.
09/23/2021 14:20:15 - INFO - __main__ - Saving the current error examples (len=27) to the memory.
09/23/2021 14:20:15 - INFO - __main__ - Current memory size: 908.
09/23/2021 14:20:15 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:20:15 - INFO - __main__ - Finished.
09/23/2021 14:20:15 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:20:15 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:20:15 - INFO - __main__ - Evaluating to get errors .... Timecode: 36
09/23/2021 14:20:18 - INFO - __main__ - Before Error Fixing: 0.3125
09/23/2021 14:20:18 - INFO - __main__ - Found 22 errors.
09/23/2021 14:20:18 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:20:18 - INFO - __main__ - Current memory size: 909.
09/23/2021 14:20:18 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:20:41 - INFO - __main__ - before_losses=[tensor(1.6960), tensor(1.5399), tensor(1.4516), tensor(1.6581), tensor(1.4860), tensor(1.5547), tensor(1.4899), tensor(1.6526), tensor(1.9686), tensor(1.5458), tensor(2.1618), tensor(1.9670), tensor(1.4564), tensor(1.5080), tensor(1.5285), tensor(1.8466), tensor(1.4847), tensor(1.5079), tensor(1.5732), tensor(1.4556), tensor(1.5445), tensor(1.9923), tensor(1.4797), tensor(1.5300), tensor(2.5747), tensor(1.4703), tensor(1.4988), tensor(1.8266), tensor(1.4780), tensor(1.4644), tensor(1.4864), tensor(1.5349), tensor(1.4885), tensor(1.4863), tensor(1.5035), tensor(1.4780), tensor(1.5062), tensor(1.6231), tensor(1.5021), tensor(1.4526), tensor(2.1089), tensor(1.5093), tensor(2.0604), tensor(1.4805), tensor(1.5452), tensor(2.4554), tensor(2.1719), tensor(2.4639), tensor(1.4972), tensor(1.6613), tensor(1.6975), tensor(2.3476), tensor(1.6562), tensor(1.5285), tensor(1.5867), tensor(1.5322), tensor(1.8340), tensor(1.5047), tensor(1.5826), tensor(3.9389), tensor(1.4501), tensor(1.4798), tensor(1.4511), tensor(1.6094), tensor(1.5346), tensor(1.4825), tensor(1.5766), tensor(1.5286), tensor(1.4882), tensor(2.0708), tensor(1.5335), tensor(1.5038), tensor(2.2291), tensor(1.5070), tensor(1.5336), tensor(1.8394), tensor(1.5365), tensor(1.4716), tensor(1.4841), tensor(1.5184), tensor(1.4833), tensor(1.5270), tensor(2.0924), tensor(1.5097), tensor(1.5314), tensor(1.5273), tensor(1.8258), tensor(1.5295), tensor(1.5918), tensor(3.5973), tensor(1.6040), tensor(1.6678), tensor(1.6707), tensor(1.8644), tensor(1.5534), tensor(1.6012), tensor(1.7678), tensor(3.2303), tensor(1.5105), tensor(1.4901), tensor(1.4668), tensor(2.3064), tensor(1.5870), tensor(2.1436), tensor(1.9674), tensor(1.4742), tensor(1.4818), tensor(1.5470), tensor(1.5261), tensor(1.4738), tensor(1.5105), tensor(2.5591), tensor(1.6704), tensor(2.2984), tensor(1.5009), tensor(1.6893), tensor(1.5456), tensor(1.5408), tensor(1.4713), tensor(1.5123), tensor(1.4954), tensor(1.7846), tensor(1.5176), tensor(1.5578), tensor(1.4745), tensor(1.4876), tensor(2.4426), tensor(1.4600), tensor(1.5222), tensor(2.4850), tensor(1.6408), tensor(1.8327), tensor(1.4985), tensor(1.5039), tensor(1.5223), tensor(2.3339), tensor(1.6578), tensor(1.6060), tensor(1.6375), tensor(1.4497), tensor(1.4988), tensor(1.4926), tensor(1.4759), tensor(1.7163), tensor(1.5204), tensor(1.6167), tensor(2.4363), tensor(2.2153), tensor(1.4846), tensor(1.5100), tensor(2.6841), tensor(1.9987), tensor(2.3833), tensor(1.4946), tensor(1.5182), tensor(1.4920), tensor(2.1838), tensor(1.4831), tensor(1.4820), tensor(1.5445), tensor(1.5333), tensor(1.5110), tensor(1.4857), tensor(1.5447), tensor(1.6558), tensor(1.4794), tensor(1.5039), tensor(1.4878), tensor(1.5382), tensor(1.5157), tensor(1.5156), tensor(1.5262), tensor(1.5665), tensor(1.4803), tensor(1.4912), tensor(1.6615), tensor(1.5584), tensor(1.5019), tensor(1.5514), tensor(1.4979), tensor(1.5205), tensor(1.4878), tensor(1.5600), tensor(1.5435), tensor(1.5466), tensor(1.5012), tensor(1.4968), tensor(2.7551), tensor(1.4972), tensor(1.5011), tensor(1.5934), tensor(2.2205), tensor(1.4956), tensor(2.3004), tensor(1.5396), tensor(1.6393), tensor(1.7632), tensor(2.3396), tensor(1.5523), tensor(1.4765), tensor(1.7059), tensor(1.5092), tensor(2.0686), tensor(2.1330), tensor(1.4827), tensor(1.5782), tensor(1.4675), tensor(1.5074), tensor(2.4293), tensor(1.5964), tensor(1.4909), tensor(1.4680), tensor(1.7070), tensor(1.9157), tensor(1.5816), tensor(1.5561), tensor(1.7795), tensor(3.2893), tensor(1.4700), tensor(1.5133), tensor(1.5300), tensor(1.4849), tensor(1.4913), tensor(1.5386), tensor(1.5370), tensor(1.5480), tensor(1.6416), tensor(1.5356), tensor(1.5528), tensor(1.5558), tensor(1.4698), tensor(1.5028), tensor(1.5041), tensor(1.8252), tensor(1.5276), tensor(1.7742), tensor(1.4825), tensor(1.6120), tensor(1.4576), tensor(1.5043), tensor(1.8379), tensor(1.5058), tensor(1.5190), tensor(1.5192), tensor(1.8345), tensor(1.6142), tensor(1.4918), tensor(1.5035), tensor(1.8937), tensor(1.5359), tensor(1.6056), tensor(1.4817), tensor(1.5046), tensor(1.5218), tensor(1.5270), tensor(1.5785)]
09/23/2021 14:20:41 - INFO - __main__ - after_losses=[tensor(1.7581), tensor(1.5426), tensor(1.4437), tensor(1.7017), tensor(1.4567), tensor(1.5514), tensor(1.4649), tensor(1.5843), tensor(1.9157), tensor(1.5269), tensor(2.0475), tensor(1.8330), tensor(1.4402), tensor(1.4790), tensor(1.4786), tensor(1.6971), tensor(1.4573), tensor(2.4850), tensor(1.5329), tensor(1.4370), tensor(1.4729), tensor(1.9285), tensor(1.4570), tensor(1.4893), tensor(3.1095), tensor(1.4481), tensor(1.4456), tensor(1.5856), tensor(1.4588), tensor(1.4450), tensor(1.4557), tensor(1.4986), tensor(1.4674), tensor(1.4663), tensor(1.4845), tensor(1.4720), tensor(1.4706), tensor(1.5014), tensor(1.4735), tensor(1.4559), tensor(1.9030), tensor(1.4791), tensor(1.5852), tensor(1.4621), tensor(1.4867), tensor(2.3194), tensor(1.7419), tensor(2.4215), tensor(1.4681), tensor(1.5113), tensor(1.6269), tensor(2.7801), tensor(1.5216), tensor(1.4956), tensor(1.5312), tensor(1.4758), tensor(1.6562), tensor(1.4792), tensor(1.5154), tensor(3.8436), tensor(1.4462), tensor(1.4649), tensor(1.4432), tensor(1.5487), tensor(1.5054), tensor(1.4683), tensor(1.5679), tensor(1.4846), tensor(1.4656), tensor(2.0957), tensor(1.4807), tensor(1.4771), tensor(3.0157), tensor(1.4943), tensor(1.5025), tensor(1.5901), tensor(1.4937), tensor(1.4535), tensor(1.4609), tensor(1.4786), tensor(1.4597), tensor(1.4916), tensor(2.3177), tensor(1.4783), tensor(1.5080), tensor(1.4888), tensor(1.5273), tensor(1.5020), tensor(1.5658), tensor(3.8387), tensor(1.6117), tensor(1.6005), tensor(1.5349), tensor(1.6011), tensor(1.5100), tensor(1.6880), tensor(1.8157), tensor(3.5135), tensor(1.4690), tensor(1.4582), tensor(1.4581), tensor(2.1999), tensor(1.5402), tensor(2.0188), tensor(2.1075), tensor(1.4528), tensor(1.4458), tensor(1.5315), tensor(1.5046), tensor(1.4579), tensor(1.4993), tensor(2.2727), tensor(1.5598), tensor(2.1907), tensor(1.5135), tensor(1.6173), tensor(1.5676), tensor(1.5068), tensor(1.4494), tensor(1.4593), tensor(1.4660), tensor(1.6153), tensor(1.4843), tensor(1.5043), tensor(1.4572), tensor(1.4669), tensor(2.2609), tensor(1.4482), tensor(1.4803), tensor(2.5449), tensor(1.6240), tensor(2.0450), tensor(1.4673), tensor(1.4810), tensor(1.4818), tensor(2.4324), tensor(1.5900), tensor(1.5741), tensor(1.5511), tensor(1.4494), tensor(1.4692), tensor(1.4685), tensor(1.4592), tensor(1.6677), tensor(1.4868), tensor(1.6007), tensor(2.2259), tensor(1.5608), tensor(1.4596), tensor(1.4718), tensor(2.6526), tensor(1.9969), tensor(2.3152), tensor(1.5460), tensor(1.4795), tensor(1.4667), tensor(2.5382), tensor(1.4931), tensor(1.4581), tensor(1.4871), tensor(1.4616), tensor(1.4848), tensor(1.4548), tensor(1.5009), tensor(1.5498), tensor(1.4551), tensor(1.4674), tensor(1.4657), tensor(1.5203), tensor(1.4734), tensor(1.4790), tensor(1.4725), tensor(1.5189), tensor(1.4603), tensor(1.4631), tensor(1.5077), tensor(1.4961), tensor(1.4861), tensor(1.5218), tensor(1.4728), tensor(1.4999), tensor(1.4589), tensor(1.5130), tensor(1.5269), tensor(1.5098), tensor(1.4796), tensor(1.4825), tensor(2.8637), tensor(1.4719), tensor(1.4905), tensor(1.5897), tensor(2.0438), tensor(1.4694), tensor(1.8337), tensor(1.5026), tensor(1.6237), tensor(1.6506), tensor(2.0582), tensor(1.5140), tensor(1.4442), tensor(1.7223), tensor(1.4764), tensor(2.1757), tensor(2.2669), tensor(1.4787), tensor(1.5327), tensor(1.4537), tensor(1.5632), tensor(2.1006), tensor(1.5016), tensor(1.4757), tensor(1.4521), tensor(1.5759), tensor(1.9664), tensor(1.5316), tensor(1.4974), tensor(1.7677), tensor(2.0560), tensor(1.4615), tensor(1.5090), tensor(1.5482), tensor(1.4591), tensor(1.4684), tensor(1.5697), tensor(1.4873), tensor(1.5150), tensor(1.6438), tensor(1.5062), tensor(1.5059), tensor(1.4877), tensor(1.4572), tensor(1.4857), tensor(1.4739), tensor(1.6757), tensor(1.4807), tensor(1.7884), tensor(1.4751), tensor(1.6323), tensor(1.4444), tensor(1.4870), tensor(1.8788), tensor(1.4728), tensor(1.4868), tensor(1.4810), tensor(1.6612), tensor(1.6880), tensor(1.4677), tensor(1.4725), tensor(2.2121), tensor(1.4902), tensor(1.4668), tensor(1.4754), tensor(1.4672), tensor(1.4877), tensor(1.5061), tensor(1.5489)]
09/23/2021 14:20:42 - INFO - __main__ - interference_scores=[tensor(0.0621), tensor(0.0026), tensor(-0.0079), tensor(0.0436), tensor(-0.0293), tensor(-0.0033), tensor(-0.0249), tensor(-0.0683), tensor(-0.0529), tensor(-0.0188), tensor(-0.1143), tensor(-0.1341), tensor(-0.0162), tensor(-0.0290), tensor(-0.0498), tensor(-0.1495), tensor(-0.0274), tensor(0.9770), tensor(-0.0403), tensor(-0.0185), tensor(-0.0716), tensor(-0.0638), tensor(-0.0226), tensor(-0.0407), tensor(0.5348), tensor(-0.0222), tensor(-0.0532), tensor(-0.2410), tensor(-0.0192), tensor(-0.0194), tensor(-0.0307), tensor(-0.0364), tensor(-0.0211), tensor(-0.0200), tensor(-0.0191), tensor(-0.0061), tensor(-0.0356), tensor(-0.1217), tensor(-0.0287), tensor(0.0033), tensor(-0.2059), tensor(-0.0302), tensor(-0.4752), tensor(-0.0184), tensor(-0.0585), tensor(-0.1360), tensor(-0.4300), tensor(-0.0424), tensor(-0.0291), tensor(-0.1500), tensor(-0.0706), tensor(0.4325), tensor(-0.1345), tensor(-0.0329), tensor(-0.0555), tensor(-0.0564), tensor(-0.1778), tensor(-0.0255), tensor(-0.0672), tensor(-0.0953), tensor(-0.0039), tensor(-0.0150), tensor(-0.0078), tensor(-0.0607), tensor(-0.0292), tensor(-0.0142), tensor(-0.0087), tensor(-0.0440), tensor(-0.0226), tensor(0.0249), tensor(-0.0528), tensor(-0.0267), tensor(0.7866), tensor(-0.0127), tensor(-0.0311), tensor(-0.2493), tensor(-0.0428), tensor(-0.0181), tensor(-0.0231), tensor(-0.0397), tensor(-0.0236), tensor(-0.0353), tensor(0.2253), tensor(-0.0314), tensor(-0.0234), tensor(-0.0385), tensor(-0.2985), tensor(-0.0276), tensor(-0.0260), tensor(0.2414), tensor(0.0077), tensor(-0.0674), tensor(-0.1359), tensor(-0.2633), tensor(-0.0434), tensor(0.0869), tensor(0.0479), tensor(0.2833), tensor(-0.0415), tensor(-0.0319), tensor(-0.0087), tensor(-0.1065), tensor(-0.0467), tensor(-0.1248), tensor(0.1402), tensor(-0.0214), tensor(-0.0360), tensor(-0.0155), tensor(-0.0215), tensor(-0.0158), tensor(-0.0113), tensor(-0.2864), tensor(-0.1106), tensor(-0.1077), tensor(0.0126), tensor(-0.0720), tensor(0.0220), tensor(-0.0339), tensor(-0.0218), tensor(-0.0530), tensor(-0.0294), tensor(-0.1692), tensor(-0.0332), tensor(-0.0535), tensor(-0.0173), tensor(-0.0207), tensor(-0.1817), tensor(-0.0117), tensor(-0.0419), tensor(0.0599), tensor(-0.0168), tensor(0.2123), tensor(-0.0312), tensor(-0.0229), tensor(-0.0404), tensor(0.0984), tensor(-0.0678), tensor(-0.0319), tensor(-0.0864), tensor(-0.0003), tensor(-0.0297), tensor(-0.0241), tensor(-0.0167), tensor(-0.0486), tensor(-0.0336), tensor(-0.0160), tensor(-0.2103), tensor(-0.6545), tensor(-0.0250), tensor(-0.0382), tensor(-0.0315), tensor(-0.0018), tensor(-0.0681), tensor(0.0514), tensor(-0.0387), tensor(-0.0253), tensor(0.3544), tensor(0.0100), tensor(-0.0239), tensor(-0.0573), tensor(-0.0717), tensor(-0.0262), tensor(-0.0309), tensor(-0.0439), tensor(-0.1059), tensor(-0.0243), tensor(-0.0364), tensor(-0.0222), tensor(-0.0179), tensor(-0.0423), tensor(-0.0366), tensor(-0.0536), tensor(-0.0476), tensor(-0.0199), tensor(-0.0281), tensor(-0.1538), tensor(-0.0623), tensor(-0.0158), tensor(-0.0296), tensor(-0.0251), tensor(-0.0206), tensor(-0.0289), tensor(-0.0470), tensor(-0.0165), tensor(-0.0368), tensor(-0.0216), tensor(-0.0143), tensor(0.1087), tensor(-0.0253), tensor(-0.0106), tensor(-0.0037), tensor(-0.1767), tensor(-0.0262), tensor(-0.4666), tensor(-0.0370), tensor(-0.0156), tensor(-0.1125), tensor(-0.2814), tensor(-0.0383), tensor(-0.0323), tensor(0.0164), tensor(-0.0329), tensor(0.1072), tensor(0.1340), tensor(-0.0040), tensor(-0.0455), tensor(-0.0138), tensor(0.0558), tensor(-0.3287), tensor(-0.0948), tensor(-0.0152), tensor(-0.0159), tensor(-0.1311), tensor(0.0507), tensor(-0.0500), tensor(-0.0587), tensor(-0.0118), tensor(-1.2333), tensor(-0.0085), tensor(-0.0043), tensor(0.0182), tensor(-0.0258), tensor(-0.0229), tensor(0.0311), tensor(-0.0497), tensor(-0.0329), tensor(0.0023), tensor(-0.0293), tensor(-0.0469), tensor(-0.0682), tensor(-0.0126), tensor(-0.0171), tensor(-0.0303), tensor(-0.1495), tensor(-0.0469), tensor(0.0142), tensor(-0.0074), tensor(0.0203), tensor(-0.0131), tensor(-0.0173), tensor(0.0409), tensor(-0.0330), tensor(-0.0323), tensor(-0.0382), tensor(-0.1733), tensor(0.0738), tensor(-0.0241), tensor(-0.0310), tensor(0.3184), tensor(-0.0457), tensor(-0.1387), tensor(-0.0063), tensor(-0.0374), tensor(-0.0341), tensor(-0.0209), tensor(-0.0296)]
09/23/2021 14:20:42 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-2368', 'mrqa_triviaqa-validation-5996', 'mrqa_triviaqa-validation-751', 'mrqa_squad-validation-8280', 'mrqa_naturalquestions-validation-7704', 'mrqa_triviaqa-validation-7394', 'mrqa_triviaqa-validation-7156', 'mrqa_squad-validation-6733', 'mrqa_squad-validation-5249', 'mrqa_triviaqa-validation-4681', 'mrqa_squad-validation-8589', 'mrqa_squad-validation-7571', 'mrqa_squad-validation-814', 'mrqa_squad-validation-8617', 'mrqa_squad-validation-6673', 'mrqa_hotpotqa-validation-4676', 'mrqa_squad-validation-7447', 'mrqa_triviaqa-validation-116', 'mrqa_squad-validation-8966', 'mrqa_squad-validation-1750', 'mrqa_hotpotqa-validation-1032', 'mrqa_squad-validation-3181', 'mrqa_triviaqa-validation-1616', 'mrqa_squad-validation-6961', 'mrqa_triviaqa-validation-1130', 'mrqa_hotpotqa-validation-3072', 'mrqa_squad-validation-6924', 'mrqa_hotpotqa-validation-3031', 'mrqa_hotpotqa-validation-1566', 'mrqa_naturalquestions-validation-954', 'mrqa_triviaqa-validation-3420', 'mrqa_naturalquestions-validation-8653']
09/23/2021 14:20:42 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:20:42 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=54) .... Timecode: 36
09/23/2021 14:20:53 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:20:53 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 36
09/23/2021 14:20:56 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 14:20:56 - INFO - __main__ - Instant Fixing Rate: 0.9545454545454546
09/23/2021 14:20:56 - INFO - __main__ - Instant Retention Rate: 0.5999999993999999
09/23/2021 14:20:58 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_037.pt.
09/23/2021 14:20:58 - INFO - __main__ - Saving the current error examples (len=22) to the memory.
09/23/2021 14:20:58 - INFO - __main__ - Current memory size: 909.
09/23/2021 14:20:58 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:20:58 - INFO - __main__ - Finished.
09/23/2021 14:20:58 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:20:58 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:20:58 - INFO - __main__ - Evaluating to get errors .... Timecode: 37
09/23/2021 14:21:02 - INFO - __main__ - Before Error Fixing: 0.25
09/23/2021 14:21:02 - INFO - __main__ - Found 24 errors.
09/23/2021 14:21:02 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:21:02 - INFO - __main__ - Current memory size: 931.
09/23/2021 14:21:02 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:21:26 - INFO - __main__ - before_losses=[tensor(3.0481), tensor(1.5609), tensor(1.4502), tensor(1.4913), tensor(1.5148), tensor(2.1663), tensor(1.4502), tensor(1.5005), tensor(1.4939), tensor(1.4830), tensor(1.5180), tensor(1.5381), tensor(1.5201), tensor(1.5372), tensor(1.4810), tensor(1.5512), tensor(1.5030), tensor(2.4303), tensor(1.4775), tensor(1.5320), tensor(1.5212), tensor(1.5403), tensor(1.5973), tensor(1.5093), tensor(1.4708), tensor(1.4962), tensor(1.5128), tensor(1.4916), tensor(1.5485), tensor(2.1129), tensor(1.5183), tensor(1.4683), tensor(1.9104), tensor(1.4915), tensor(1.5143), tensor(1.5196), tensor(1.5713), tensor(1.4987), tensor(1.5048), tensor(1.4721), tensor(1.4839), tensor(1.4789), tensor(1.5019), tensor(1.5178), tensor(1.8330), tensor(1.5067), tensor(1.5019), tensor(1.5321), tensor(1.5341), tensor(1.5886), tensor(1.5627), tensor(2.2035), tensor(1.4865), tensor(1.5279), tensor(1.5921), tensor(1.9630), tensor(1.4915), tensor(2.2529), tensor(1.8724), tensor(1.5212), tensor(1.4793), tensor(1.4934), tensor(1.6805), tensor(1.4737), tensor(1.7907), tensor(1.4559), tensor(2.4973), tensor(1.7009), tensor(1.5292), tensor(1.7618), tensor(1.4911), tensor(2.0884), tensor(1.5217), tensor(1.4785), tensor(1.4616), tensor(1.4993), tensor(1.4734), tensor(1.5034), tensor(1.9448), tensor(1.4817), tensor(1.5116), tensor(1.5986), tensor(1.5033), tensor(1.6163), tensor(2.3078), tensor(1.4646), tensor(1.6015), tensor(1.5410), tensor(1.5018), tensor(1.5260), tensor(1.4699), tensor(1.5640), tensor(1.5090), tensor(3.7429), tensor(1.5838), tensor(1.4885), tensor(1.4564), tensor(1.5093), tensor(1.6426), tensor(1.7143), tensor(2.6137), tensor(1.6668), tensor(1.5622), tensor(1.4954), tensor(1.6556), tensor(2.6482), tensor(1.5089), tensor(1.6106), tensor(1.4732), tensor(1.5436), tensor(1.4885), tensor(1.6256), tensor(2.0656), tensor(1.5728), tensor(1.6722), tensor(1.4863), tensor(1.4758), tensor(1.9462), tensor(1.5922), tensor(2.2914), tensor(2.6951), tensor(1.6445), tensor(1.4723), tensor(2.6893), tensor(1.5019), tensor(1.5058), tensor(1.7433), tensor(1.5412), tensor(1.5073), tensor(1.5082), tensor(1.5787), tensor(1.5176), tensor(1.4987), tensor(1.5051), tensor(1.5493), tensor(1.5388), tensor(1.5005), tensor(1.9540), tensor(2.3397), tensor(1.5849), tensor(1.4985), tensor(1.5244), tensor(1.4664), tensor(1.5849), tensor(1.4789), tensor(2.2694), tensor(1.5390), tensor(2.0365), tensor(1.5015), tensor(1.5807), tensor(1.5814), tensor(1.5223), tensor(1.5887), tensor(1.4825), tensor(1.4871), tensor(1.4946), tensor(1.4722), tensor(1.5420), tensor(1.5731), tensor(1.4948), tensor(1.4841), tensor(1.5389), tensor(1.4752), tensor(1.4735), tensor(1.8338), tensor(1.4583), tensor(1.4958), tensor(1.4839), tensor(1.6212), tensor(2.1224), tensor(2.7270), tensor(1.5030), tensor(1.4953), tensor(1.4853), tensor(2.1078), tensor(1.5232), tensor(1.5233), tensor(1.5804), tensor(1.5502), tensor(2.9765), tensor(1.5988), tensor(1.5048), tensor(1.5715), tensor(1.5165), tensor(1.5040), tensor(1.5426), tensor(1.5082), tensor(1.6977), tensor(1.4640), tensor(1.4967), tensor(1.7576), tensor(1.5181), tensor(1.8129), tensor(1.4660), tensor(1.6232), tensor(1.4884), tensor(1.7733), tensor(1.7812), tensor(1.4848), tensor(1.5591), tensor(1.7275), tensor(1.9955), tensor(2.0077), tensor(1.5192), tensor(1.4875), tensor(1.4772), tensor(1.4992), tensor(1.5473), tensor(1.4740), tensor(1.5222), tensor(1.5645), tensor(3.3032), tensor(1.4842), tensor(1.4859), tensor(2.9543), tensor(1.4644), tensor(1.6834), tensor(1.4977), tensor(1.5228), tensor(1.5346), tensor(1.5446), tensor(1.5768), tensor(1.4805), tensor(1.5868), tensor(1.5002), tensor(1.5690), tensor(1.5640), tensor(1.4740), tensor(1.4754), tensor(1.7504), tensor(1.5028), tensor(1.5078), tensor(2.3217), tensor(1.4862), tensor(1.4504), tensor(2.2132), tensor(1.4860), tensor(2.7623), tensor(1.7262), tensor(1.4953), tensor(1.4861), tensor(1.7546), tensor(1.5630), tensor(1.5893), tensor(1.4775), tensor(1.4893), tensor(1.5286), tensor(1.4590), tensor(1.8035), tensor(1.4591), tensor(1.6003), tensor(1.6289), tensor(1.7676), tensor(1.5341), tensor(1.4813), tensor(1.4751)]
09/23/2021 14:21:26 - INFO - __main__ - after_losses=[tensor(3.1971), tensor(1.5347), tensor(1.4520), tensor(1.4736), tensor(1.5864), tensor(2.2445), tensor(1.4432), tensor(1.5047), tensor(1.4828), tensor(1.4752), tensor(1.5035), tensor(1.5449), tensor(1.4986), tensor(1.5250), tensor(1.4726), tensor(1.5400), tensor(1.4950), tensor(2.1868), tensor(1.4785), tensor(1.5095), tensor(1.5106), tensor(1.5341), tensor(1.5245), tensor(1.4852), tensor(1.4635), tensor(1.4898), tensor(1.5113), tensor(1.4715), tensor(1.5391), tensor(2.1260), tensor(1.5559), tensor(1.4697), tensor(1.8594), tensor(1.4950), tensor(1.5083), tensor(1.5040), tensor(1.5648), tensor(1.4969), tensor(1.4891), tensor(1.4616), tensor(1.4733), tensor(1.4712), tensor(1.4916), tensor(1.4647), tensor(1.8244), tensor(1.5080), tensor(1.4677), tensor(1.4718), tensor(1.5162), tensor(1.6182), tensor(1.8362), tensor(2.2454), tensor(1.4789), tensor(1.5284), tensor(1.5821), tensor(1.9714), tensor(1.4903), tensor(2.0695), tensor(1.9378), tensor(1.5068), tensor(1.4781), tensor(1.4737), tensor(1.6236), tensor(1.4635), tensor(1.7507), tensor(1.4466), tensor(1.4676), tensor(1.5991), tensor(1.5218), tensor(1.6675), tensor(1.5010), tensor(2.1945), tensor(1.6421), tensor(1.4748), tensor(1.4747), tensor(1.6295), tensor(1.4858), tensor(1.5009), tensor(1.9533), tensor(1.4744), tensor(1.5090), tensor(1.4597), tensor(1.4929), tensor(1.6308), tensor(2.4248), tensor(1.4647), tensor(1.6381), tensor(1.5526), tensor(1.5097), tensor(1.5387), tensor(1.4591), tensor(1.5460), tensor(1.4904), tensor(3.6428), tensor(1.5512), tensor(1.4752), tensor(1.4521), tensor(1.5046), tensor(1.5695), tensor(1.6253), tensor(1.7666), tensor(1.7979), tensor(1.5462), tensor(1.4677), tensor(1.6724), tensor(2.4577), tensor(1.5397), tensor(1.6141), tensor(1.4610), tensor(1.5618), tensor(1.4771), tensor(1.5753), tensor(2.0942), tensor(1.5489), tensor(1.6703), tensor(1.4796), tensor(1.4838), tensor(1.8963), tensor(1.6269), tensor(2.2501), tensor(2.2655), tensor(1.6810), tensor(1.4644), tensor(2.7557), tensor(1.5472), tensor(1.5107), tensor(2.9192), tensor(1.5246), tensor(1.4797), tensor(1.4918), tensor(1.5904), tensor(1.5240), tensor(1.4830), tensor(1.5549), tensor(1.5504), tensor(1.5205), tensor(1.4871), tensor(1.9739), tensor(2.3809), tensor(1.5495), tensor(1.5002), tensor(1.5189), tensor(1.4618), tensor(1.5651), tensor(1.4692), tensor(2.3434), tensor(1.5262), tensor(2.0672), tensor(1.5032), tensor(1.5464), tensor(1.5380), tensor(1.5062), tensor(1.6062), tensor(1.4727), tensor(1.5038), tensor(1.4724), tensor(1.4682), tensor(1.5554), tensor(1.5719), tensor(1.4805), tensor(1.4800), tensor(1.5354), tensor(1.4794), tensor(1.4588), tensor(1.4865), tensor(1.4516), tensor(1.4818), tensor(1.4804), tensor(1.7438), tensor(2.1087), tensor(2.8173), tensor(1.4788), tensor(1.4747), tensor(1.4775), tensor(2.1151), tensor(1.5158), tensor(1.5270), tensor(1.5715), tensor(1.5251), tensor(1.7449), tensor(1.6271), tensor(1.4849), tensor(1.5372), tensor(1.5058), tensor(1.5167), tensor(1.5186), tensor(1.5134), tensor(1.8280), tensor(1.4589), tensor(1.5048), tensor(1.7499), tensor(1.4982), tensor(1.7667), tensor(1.4609), tensor(1.5775), tensor(1.4909), tensor(1.7048), tensor(1.8361), tensor(1.4787), tensor(1.5436), tensor(1.7728), tensor(1.9510), tensor(1.4616), tensor(1.5425), tensor(1.4984), tensor(1.4762), tensor(1.4868), tensor(1.5394), tensor(1.4715), tensor(1.5115), tensor(1.5445), tensor(3.8635), tensor(1.4664), tensor(1.4886), tensor(3.0073), tensor(1.4593), tensor(2.1021), tensor(1.4906), tensor(1.5124), tensor(1.6477), tensor(1.5257), tensor(1.5334), tensor(1.4666), tensor(1.5987), tensor(1.4887), tensor(1.5926), tensor(1.5172), tensor(1.4814), tensor(1.4718), tensor(1.7691), tensor(1.4846), tensor(1.5678), tensor(2.1639), tensor(1.4781), tensor(1.4455), tensor(2.2342), tensor(1.4787), tensor(2.9504), tensor(1.8680), tensor(1.4883), tensor(1.4874), tensor(1.5405), tensor(1.5416), tensor(1.8404), tensor(1.4706), tensor(1.4865), tensor(1.6297), tensor(1.4608), tensor(1.9601), tensor(1.4444), tensor(1.6065), tensor(1.6003), tensor(2.2006), tensor(1.5198), tensor(1.4711), tensor(1.4678)]
09/23/2021 14:21:26 - INFO - __main__ - interference_scores=[tensor(0.1490), tensor(-0.0262), tensor(0.0017), tensor(-0.0177), tensor(0.0715), tensor(0.0782), tensor(-0.0070), tensor(0.0042), tensor(-0.0110), tensor(-0.0079), tensor(-0.0145), tensor(0.0068), tensor(-0.0215), tensor(-0.0121), tensor(-0.0085), tensor(-0.0112), tensor(-0.0079), tensor(-0.2434), tensor(0.0009), tensor(-0.0226), tensor(-0.0106), tensor(-0.0062), tensor(-0.0728), tensor(-0.0241), tensor(-0.0073), tensor(-0.0065), tensor(-0.0015), tensor(-0.0201), tensor(-0.0094), tensor(0.0131), tensor(0.0376), tensor(0.0014), tensor(-0.0510), tensor(0.0035), tensor(-0.0059), tensor(-0.0156), tensor(-0.0066), tensor(-0.0018), tensor(-0.0157), tensor(-0.0105), tensor(-0.0106), tensor(-0.0077), tensor(-0.0103), tensor(-0.0531), tensor(-0.0086), tensor(0.0013), tensor(-0.0342), tensor(-0.0603), tensor(-0.0180), tensor(0.0296), tensor(0.2735), tensor(0.0419), tensor(-0.0076), tensor(0.0005), tensor(-0.0100), tensor(0.0084), tensor(-0.0012), tensor(-0.1834), tensor(0.0654), tensor(-0.0144), tensor(-0.0012), tensor(-0.0198), tensor(-0.0568), tensor(-0.0102), tensor(-0.0400), tensor(-0.0093), tensor(-1.0297), tensor(-0.1018), tensor(-0.0074), tensor(-0.0943), tensor(0.0099), tensor(0.1060), tensor(0.1205), tensor(-0.0037), tensor(0.0131), tensor(0.1302), tensor(0.0124), tensor(-0.0026), tensor(0.0085), tensor(-0.0073), tensor(-0.0026), tensor(-0.1389), tensor(-0.0104), tensor(0.0145), tensor(0.1170), tensor(7.8321e-05), tensor(0.0366), tensor(0.0117), tensor(0.0079), tensor(0.0127), tensor(-0.0108), tensor(-0.0180), tensor(-0.0186), tensor(-0.1001), tensor(-0.0326), tensor(-0.0134), tensor(-0.0043), tensor(-0.0047), tensor(-0.0731), tensor(-0.0890), tensor(-0.8471), tensor(0.1310), tensor(-0.0160), tensor(-0.0278), tensor(0.0168), tensor(-0.1905), tensor(0.0308), tensor(0.0035), tensor(-0.0123), tensor(0.0182), tensor(-0.0114), tensor(-0.0503), tensor(0.0285), tensor(-0.0239), tensor(-0.0018), tensor(-0.0067), tensor(0.0080), tensor(-0.0500), tensor(0.0347), tensor(-0.0414), tensor(-0.4296), tensor(0.0365), tensor(-0.0079), tensor(0.0664), tensor(0.0453), tensor(0.0049), tensor(1.1759), tensor(-0.0166), tensor(-0.0275), tensor(-0.0164), tensor(0.0117), tensor(0.0064), tensor(-0.0157), tensor(0.0498), tensor(0.0011), tensor(-0.0182), tensor(-0.0134), tensor(0.0199), tensor(0.0412), tensor(-0.0354), tensor(0.0016), tensor(-0.0055), tensor(-0.0046), tensor(-0.0199), tensor(-0.0097), tensor(0.0740), tensor(-0.0129), tensor(0.0307), tensor(0.0017), tensor(-0.0343), tensor(-0.0434), tensor(-0.0162), tensor(0.0175), tensor(-0.0098), tensor(0.0167), tensor(-0.0222), tensor(-0.0040), tensor(0.0134), tensor(-0.0012), tensor(-0.0143), tensor(-0.0041), tensor(-0.0035), tensor(0.0041), tensor(-0.0148), tensor(-0.3473), tensor(-0.0067), tensor(-0.0140), tensor(-0.0034), tensor(0.1226), tensor(-0.0137), tensor(0.0903), tensor(-0.0241), tensor(-0.0206), tensor(-0.0078), tensor(0.0073), tensor(-0.0074), tensor(0.0037), tensor(-0.0089), tensor(-0.0250), tensor(-1.2316), tensor(0.0284), tensor(-0.0199), tensor(-0.0343), tensor(-0.0107), tensor(0.0127), tensor(-0.0240), tensor(0.0052), tensor(0.1303), tensor(-0.0051), tensor(0.0081), tensor(-0.0077), tensor(-0.0200), tensor(-0.0462), tensor(-0.0051), tensor(-0.0457), tensor(0.0025), tensor(-0.0685), tensor(0.0549), tensor(-0.0061), tensor(-0.0154), tensor(0.0452), tensor(-0.0444), tensor(-0.5461), tensor(0.0233), tensor(0.0109), tensor(-0.0010), tensor(-0.0124), tensor(-0.0079), tensor(-0.0025), tensor(-0.0107), tensor(-0.0200), tensor(0.5603), tensor(-0.0178), tensor(0.0027), tensor(0.0530), tensor(-0.0050), tensor(0.4188), tensor(-0.0071), tensor(-0.0104), tensor(0.1132), tensor(-0.0189), tensor(-0.0433), tensor(-0.0140), tensor(0.0119), tensor(-0.0115), tensor(0.0236), tensor(-0.0468), tensor(0.0074), tensor(-0.0036), tensor(0.0187), tensor(-0.0182), tensor(0.0601), tensor(-0.1577), tensor(-0.0080), tensor(-0.0049), tensor(0.0210), tensor(-0.0072), tensor(0.1881), tensor(0.1417), tensor(-0.0069), tensor(0.0013), tensor(-0.2141), tensor(-0.0214), tensor(0.2511), tensor(-0.0069), tensor(-0.0028), tensor(0.1011), tensor(0.0018), tensor(0.1566), tensor(-0.0147), tensor(0.0062), tensor(-0.0285), tensor(0.4330), tensor(-0.0143), tensor(-0.0102), tensor(-0.0073)]
09/23/2021 14:21:26 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-8280', 'mrqa_squad-validation-2291', 'mrqa_triviaqa-validation-1616', 'mrqa_triviaqa-validation-2202', 'mrqa_triviaqa-validation-3920', 'mrqa_squad-validation-3181', 'mrqa_squad-validation-772', 'mrqa_triviaqa-validation-2091', 'mrqa_triviaqa-validation-622', 'mrqa_squad-validation-2629', 'mrqa_triviaqa-validation-83', 'mrqa_naturalquestions-validation-5437', 'mrqa_squad-validation-1249', 'mrqa_squad-validation-10322', 'mrqa_triviaqa-validation-2802', 'mrqa_triviaqa-validation-5810', 'mrqa_naturalquestions-validation-2893', 'mrqa_triviaqa-validation-2465', 'mrqa_triviaqa-validation-6901', 'mrqa_triviaqa-validation-3393', 'mrqa_squad-validation-2987', 'mrqa_naturalquestions-validation-3208', 'mrqa_triviaqa-validation-7578', 'mrqa_squad-validation-10312', 'mrqa_triviaqa-validation-4731', 'mrqa_naturalquestions-validation-2222', 'mrqa_triviaqa-validation-2715', 'mrqa_triviaqa-validation-6902', 'mrqa_naturalquestions-validation-56', 'mrqa_hotpotqa-validation-1444', 'mrqa_squad-validation-8095', 'mrqa_squad-validation-9984']
09/23/2021 14:21:26 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:21:26 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=56) .... Timecode: 37
09/23/2021 14:21:38 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:21:38 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 37
09/23/2021 14:21:42 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 14:21:42 - INFO - __main__ - Instant Fixing Rate: 0.9166666666666666
09/23/2021 14:21:42 - INFO - __main__ - Instant Retention Rate: 0.8749999989062499
09/23/2021 14:21:43 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_038.pt.
09/23/2021 14:21:43 - INFO - __main__ - Saving the current error examples (len=24) to the memory.
09/23/2021 14:21:43 - INFO - __main__ - Current memory size: 931.
09/23/2021 14:21:43 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:21:43 - INFO - __main__ - Finished.
09/23/2021 14:21:43 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:21:43 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:21:43 - INFO - __main__ - Evaluating to get errors .... Timecode: 38
09/23/2021 14:21:47 - INFO - __main__ - Before Error Fixing: 0.25
09/23/2021 14:21:47 - INFO - __main__ - Found 24 errors.
09/23/2021 14:21:47 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:21:47 - INFO - __main__ - Current memory size: 955.
09/23/2021 14:21:47 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:22:10 - INFO - __main__ - before_losses=[tensor(2.0117), tensor(1.6732), tensor(1.4860), tensor(1.5126), tensor(1.5183), tensor(1.5375), tensor(1.4927), tensor(1.5010), tensor(1.7742), tensor(1.9380), tensor(1.5137), tensor(1.6229), tensor(1.4775), tensor(1.4893), tensor(1.6080), tensor(2.6021), tensor(1.5224), tensor(1.5121), tensor(1.4815), tensor(1.5412), tensor(5.4277), tensor(1.4830), tensor(1.7417), tensor(1.5539), tensor(1.4549), tensor(1.5085), tensor(1.5976), tensor(1.4942), tensor(1.4670), tensor(1.5461), tensor(1.5570), tensor(1.5718), tensor(1.4660), tensor(1.4871), tensor(1.5035), tensor(1.5140), tensor(1.5452), tensor(1.5743), tensor(1.4927), tensor(1.5254), tensor(1.5411), tensor(1.8627), tensor(1.9469), tensor(2.1612), tensor(1.4768), tensor(1.5011), tensor(1.5699), tensor(1.5474), tensor(1.6397), tensor(1.4615), tensor(2.0442), tensor(2.8428), tensor(1.8035), tensor(1.5351), tensor(1.6349), tensor(1.5388), tensor(1.7531), tensor(1.4624), tensor(1.4731), tensor(1.5446), tensor(1.4873), tensor(1.6003), tensor(1.5607), tensor(1.4995), tensor(1.5335), tensor(2.0141), tensor(1.5115), tensor(1.5095), tensor(1.5006), tensor(1.7297), tensor(1.4985), tensor(1.4629), tensor(1.5871), tensor(1.8688), tensor(1.4899), tensor(1.5066), tensor(1.4867), tensor(1.5324), tensor(3.6485), tensor(1.5076), tensor(1.6132), tensor(1.5023), tensor(1.4765), tensor(1.4621), tensor(1.4882), tensor(1.7529), tensor(1.6302), tensor(1.5064), tensor(1.5047), tensor(2.2786), tensor(1.4859), tensor(1.5853), tensor(1.4726), tensor(1.6371), tensor(1.5185), tensor(1.4886), tensor(1.5013), tensor(1.5310), tensor(1.5025), tensor(1.5128), tensor(1.5727), tensor(1.5067), tensor(1.6249), tensor(1.4939), tensor(1.4953), tensor(1.4975), tensor(2.0463), tensor(1.4926), tensor(1.4755), tensor(1.5623), tensor(1.5525), tensor(1.4913), tensor(1.6160), tensor(1.4932), tensor(1.8907), tensor(1.5020), tensor(1.8484), tensor(1.9704), tensor(1.5298), tensor(1.5338), tensor(1.5922), tensor(1.5952), tensor(1.4654), tensor(1.5589), tensor(1.5448), tensor(1.5290), tensor(1.4857), tensor(1.5247), tensor(1.5273), tensor(1.5086), tensor(1.5553), tensor(1.5738), tensor(1.7025), tensor(1.4892), tensor(1.5107), tensor(1.4598), tensor(1.5317), tensor(1.5123), tensor(2.3236), tensor(1.5249), tensor(1.5431), tensor(1.4811), tensor(1.5100), tensor(1.8408), tensor(1.9732), tensor(1.5135), tensor(1.5192), tensor(1.4459), tensor(1.4907), tensor(1.6294), tensor(1.5183), tensor(1.4874), tensor(1.5252), tensor(1.7572), tensor(1.6322), tensor(1.9682), tensor(1.4968), tensor(1.5225), tensor(1.4947), tensor(1.5561), tensor(1.5750), tensor(1.5568), tensor(1.5262), tensor(1.4758), tensor(1.5151), tensor(1.4968), tensor(1.6057), tensor(1.5118), tensor(1.5176), tensor(1.4872), tensor(1.4868), tensor(1.5152), tensor(1.5619), tensor(1.5354), tensor(1.5146), tensor(1.9337), tensor(2.0429), tensor(1.4867), tensor(1.8736), tensor(2.0928), tensor(1.5224), tensor(1.5443), tensor(1.5202), tensor(1.5194), tensor(1.5104), tensor(1.5370), tensor(1.5473), tensor(1.5104), tensor(1.5900), tensor(1.5871), tensor(1.5419), tensor(1.5644), tensor(1.5316), tensor(1.5694), tensor(1.4602), tensor(1.5426), tensor(1.4923), tensor(1.5012), tensor(1.5146), tensor(1.5416), tensor(2.2391), tensor(2.7762), tensor(1.6618), tensor(1.4674), tensor(1.5773), tensor(1.5978), tensor(1.5019), tensor(1.5307), tensor(1.4459), tensor(1.4869), tensor(4.2417), tensor(1.4657), tensor(2.2713), tensor(1.5292), tensor(1.5099), tensor(1.6047), tensor(1.5244), tensor(1.5350), tensor(1.6963), tensor(1.5757), tensor(1.4809), tensor(1.4611), tensor(1.4722), tensor(1.4930), tensor(1.5863), tensor(1.5123), tensor(1.5389), tensor(1.9242), tensor(1.6191), tensor(1.5249), tensor(1.5014), tensor(1.7262), tensor(1.7035), tensor(1.5402), tensor(1.5083), tensor(1.5837), tensor(1.6020), tensor(1.5038), tensor(1.4735), tensor(1.4757), tensor(1.5149), tensor(1.5379), tensor(1.7561), tensor(1.5501), tensor(1.7296), tensor(1.6990), tensor(1.4991), tensor(1.5024), tensor(1.4856), tensor(1.5820), tensor(1.4731), tensor(1.4761), tensor(1.4886), tensor(2.0426), tensor(1.5465), tensor(1.5074)]
09/23/2021 14:22:10 - INFO - __main__ - after_losses=[tensor(2.0513), tensor(1.7658), tensor(1.4875), tensor(1.4974), tensor(1.4974), tensor(1.5472), tensor(1.5328), tensor(1.4848), tensor(3.1868), tensor(1.9068), tensor(1.4924), tensor(1.7909), tensor(1.4622), tensor(1.5011), tensor(1.9093), tensor(2.8040), tensor(1.5002), tensor(1.5401), tensor(1.4727), tensor(1.5307), tensor(4.7564), tensor(1.4758), tensor(1.7241), tensor(1.5340), tensor(1.4577), tensor(1.4904), tensor(1.5668), tensor(1.4852), tensor(1.4682), tensor(1.5379), tensor(1.6088), tensor(1.5502), tensor(1.4631), tensor(1.4753), tensor(1.4835), tensor(1.5048), tensor(1.5544), tensor(1.5417), tensor(1.4875), tensor(1.4998), tensor(1.5222), tensor(1.8872), tensor(1.8457), tensor(2.1144), tensor(1.4718), tensor(1.4760), tensor(1.5087), tensor(1.5265), tensor(1.6308), tensor(1.4861), tensor(2.2726), tensor(2.7715), tensor(1.7945), tensor(1.5272), tensor(1.6145), tensor(1.5329), tensor(1.6683), tensor(1.4503), tensor(1.4856), tensor(1.6049), tensor(1.4815), tensor(1.5385), tensor(1.5361), tensor(1.4760), tensor(1.5190), tensor(1.7983), tensor(1.5452), tensor(1.5048), tensor(1.5337), tensor(1.8010), tensor(1.4967), tensor(1.4593), tensor(1.6607), tensor(1.9093), tensor(1.4750), tensor(1.4867), tensor(1.4719), tensor(1.5236), tensor(3.4578), tensor(1.4770), tensor(1.5936), tensor(1.4796), tensor(1.4676), tensor(1.4613), tensor(1.4862), tensor(1.8781), tensor(1.5948), tensor(1.5052), tensor(1.5472), tensor(2.3533), tensor(1.4707), tensor(1.5975), tensor(1.4678), tensor(1.6891), tensor(1.5307), tensor(1.4682), tensor(1.4882), tensor(1.5139), tensor(1.4965), tensor(1.5038), tensor(1.5337), tensor(1.5169), tensor(1.7432), tensor(1.4855), tensor(1.4922), tensor(1.4821), tensor(1.9081), tensor(1.4886), tensor(1.4761), tensor(1.5428), tensor(1.5478), tensor(1.4761), tensor(1.6175), tensor(1.4777), tensor(1.5086), tensor(1.5074), tensor(1.8890), tensor(1.8622), tensor(1.5934), tensor(1.5121), tensor(1.5456), tensor(1.6207), tensor(1.4670), tensor(1.5333), tensor(1.5181), tensor(1.5167), tensor(1.4848), tensor(1.4813), tensor(1.5668), tensor(1.4933), tensor(1.5299), tensor(1.5488), tensor(1.8308), tensor(1.4767), tensor(1.6090), tensor(1.4640), tensor(1.5146), tensor(1.5182), tensor(2.3614), tensor(1.5074), tensor(1.5402), tensor(1.4663), tensor(1.4967), tensor(2.2940), tensor(1.7366), tensor(1.4992), tensor(1.5199), tensor(1.4425), tensor(1.4797), tensor(1.6285), tensor(1.4862), tensor(1.4782), tensor(1.5147), tensor(1.7159), tensor(1.6333), tensor(1.9543), tensor(1.4885), tensor(1.5118), tensor(1.4832), tensor(1.6044), tensor(1.5444), tensor(1.4748), tensor(1.5065), tensor(1.4582), tensor(1.4952), tensor(1.4986), tensor(1.6417), tensor(1.5003), tensor(1.5024), tensor(1.4764), tensor(1.4730), tensor(1.5197), tensor(1.5473), tensor(1.5465), tensor(1.4980), tensor(1.8471), tensor(2.1788), tensor(1.5307), tensor(1.8220), tensor(2.0408), tensor(1.5067), tensor(1.5506), tensor(1.5088), tensor(1.5059), tensor(1.6048), tensor(1.5186), tensor(1.5657), tensor(1.4753), tensor(1.5757), tensor(1.5484), tensor(1.5973), tensor(1.5190), tensor(1.5735), tensor(1.5634), tensor(1.4690), tensor(1.5193), tensor(1.4905), tensor(1.5052), tensor(1.4693), tensor(1.5109), tensor(2.3734), tensor(2.7186), tensor(1.6291), tensor(1.4721), tensor(1.5487), tensor(1.5480), tensor(1.4917), tensor(1.5210), tensor(1.4416), tensor(1.4795), tensor(3.7699), tensor(1.4494), tensor(2.2997), tensor(1.5176), tensor(1.5061), tensor(1.6803), tensor(1.5110), tensor(1.5378), tensor(1.8643), tensor(1.5006), tensor(1.4616), tensor(1.4605), tensor(1.4695), tensor(1.4783), tensor(1.5612), tensor(1.5253), tensor(1.5143), tensor(1.8242), tensor(1.6219), tensor(1.5072), tensor(1.4774), tensor(1.6197), tensor(1.8334), tensor(1.5167), tensor(1.4949), tensor(1.5705), tensor(1.5849), tensor(1.4839), tensor(1.4704), tensor(1.4610), tensor(1.4914), tensor(1.5253), tensor(1.6743), tensor(1.6277), tensor(1.6088), tensor(1.6743), tensor(1.4974), tensor(1.4969), tensor(1.4796), tensor(1.6279), tensor(1.4718), tensor(1.5089), tensor(1.4939), tensor(1.5951), tensor(1.5423), tensor(1.4957)]
09/23/2021 14:22:10 - INFO - __main__ - interference_scores=[tensor(0.0396), tensor(0.0926), tensor(0.0015), tensor(-0.0152), tensor(-0.0209), tensor(0.0097), tensor(0.0401), tensor(-0.0162), tensor(1.4126), tensor(-0.0312), tensor(-0.0214), tensor(0.1681), tensor(-0.0153), tensor(0.0118), tensor(0.3013), tensor(0.2018), tensor(-0.0222), tensor(0.0280), tensor(-0.0088), tensor(-0.0105), tensor(-0.6714), tensor(-0.0072), tensor(-0.0176), tensor(-0.0198), tensor(0.0029), tensor(-0.0181), tensor(-0.0309), tensor(-0.0090), tensor(0.0012), tensor(-0.0083), tensor(0.0519), tensor(-0.0216), tensor(-0.0029), tensor(-0.0118), tensor(-0.0200), tensor(-0.0092), tensor(0.0092), tensor(-0.0326), tensor(-0.0052), tensor(-0.0256), tensor(-0.0189), tensor(0.0245), tensor(-0.1013), tensor(-0.0467), tensor(-0.0050), tensor(-0.0251), tensor(-0.0612), tensor(-0.0209), tensor(-0.0089), tensor(0.0246), tensor(0.2283), tensor(-0.0713), tensor(-0.0090), tensor(-0.0079), tensor(-0.0204), tensor(-0.0059), tensor(-0.0849), tensor(-0.0120), tensor(0.0125), tensor(0.0603), tensor(-0.0058), tensor(-0.0618), tensor(-0.0247), tensor(-0.0234), tensor(-0.0145), tensor(-0.2158), tensor(0.0337), tensor(-0.0047), tensor(0.0331), tensor(0.0713), tensor(-0.0018), tensor(-0.0036), tensor(0.0736), tensor(0.0406), tensor(-0.0148), tensor(-0.0199), tensor(-0.0148), tensor(-0.0088), tensor(-0.1906), tensor(-0.0305), tensor(-0.0196), tensor(-0.0226), tensor(-0.0089), tensor(-0.0009), tensor(-0.0020), tensor(0.1252), tensor(-0.0355), tensor(-0.0012), tensor(0.0425), tensor(0.0748), tensor(-0.0151), tensor(0.0122), tensor(-0.0048), tensor(0.0520), tensor(0.0122), tensor(-0.0204), tensor(-0.0131), tensor(-0.0171), tensor(-0.0059), tensor(-0.0091), tensor(-0.0390), tensor(0.0102), tensor(0.1183), tensor(-0.0084), tensor(-0.0031), tensor(-0.0154), tensor(-0.1383), tensor(-0.0040), tensor(0.0006), tensor(-0.0194), tensor(-0.0046), tensor(-0.0152), tensor(0.0016), tensor(-0.0154), tensor(-0.3821), tensor(0.0054), tensor(0.0406), tensor(-0.1083), tensor(0.0636), tensor(-0.0217), tensor(-0.0466), tensor(0.0254), tensor(0.0016), tensor(-0.0257), tensor(-0.0268), tensor(-0.0124), tensor(-0.0010), tensor(-0.0434), tensor(0.0396), tensor(-0.0154), tensor(-0.0254), tensor(-0.0250), tensor(0.1283), tensor(-0.0125), tensor(0.0983), tensor(0.0042), tensor(-0.0170), tensor(0.0059), tensor(0.0377), tensor(-0.0175), tensor(-0.0030), tensor(-0.0148), tensor(-0.0133), tensor(0.4532), tensor(-0.2366), tensor(-0.0143), tensor(0.0007), tensor(-0.0034), tensor(-0.0110), tensor(-0.0008), tensor(-0.0321), tensor(-0.0093), tensor(-0.0105), tensor(-0.0413), tensor(0.0010), tensor(-0.0139), tensor(-0.0083), tensor(-0.0107), tensor(-0.0115), tensor(0.0483), tensor(-0.0306), tensor(-0.0820), tensor(-0.0198), tensor(-0.0176), tensor(-0.0199), tensor(0.0019), tensor(0.0360), tensor(-0.0115), tensor(-0.0152), tensor(-0.0108), tensor(-0.0139), tensor(0.0045), tensor(-0.0145), tensor(0.0111), tensor(-0.0165), tensor(-0.0866), tensor(0.1359), tensor(0.0441), tensor(-0.0515), tensor(-0.0520), tensor(-0.0157), tensor(0.0062), tensor(-0.0114), tensor(-0.0134), tensor(0.0944), tensor(-0.0184), tensor(0.0184), tensor(-0.0351), tensor(-0.0143), tensor(-0.0386), tensor(0.0555), tensor(-0.0454), tensor(0.0419), tensor(-0.0060), tensor(0.0087), tensor(-0.0234), tensor(-0.0017), tensor(0.0040), tensor(-0.0453), tensor(-0.0308), tensor(0.1343), tensor(-0.0576), tensor(-0.0327), tensor(0.0047), tensor(-0.0286), tensor(-0.0497), tensor(-0.0102), tensor(-0.0097), tensor(-0.0043), tensor(-0.0074), tensor(-0.4718), tensor(-0.0162), tensor(0.0285), tensor(-0.0115), tensor(-0.0038), tensor(0.0756), tensor(-0.0134), tensor(0.0028), tensor(0.1680), tensor(-0.0751), tensor(-0.0194), tensor(-0.0006), tensor(-0.0027), tensor(-0.0147), tensor(-0.0252), tensor(0.0130), tensor(-0.0246), tensor(-0.1000), tensor(0.0028), tensor(-0.0177), tensor(-0.0240), tensor(-0.1065), tensor(0.1299), tensor(-0.0234), tensor(-0.0134), tensor(-0.0132), tensor(-0.0170), tensor(-0.0198), tensor(-0.0030), tensor(-0.0147), tensor(-0.0235), tensor(-0.0127), tensor(-0.0818), tensor(0.0776), tensor(-0.1208), tensor(-0.0247), tensor(-0.0017), tensor(-0.0056), tensor(-0.0060), tensor(0.0459), tensor(-0.0013), tensor(0.0328), tensor(0.0053), tensor(-0.4476), tensor(-0.0042), tensor(-0.0117)]
09/23/2021 14:22:10 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-622', 'mrqa_triviaqa-validation-1293', 'mrqa_squad-validation-3181', 'mrqa_triviaqa-validation-4055', 'mrqa_squad-validation-6759', 'mrqa_squad-validation-769', 'mrqa_triviaqa-validation-5997', 'mrqa_triviaqa-validation-3901', 'mrqa_triviaqa-validation-3134', 'mrqa_triviaqa-validation-1034', 'mrqa_triviaqa-validation-5266', 'mrqa_triviaqa-validation-7703', 'mrqa_triviaqa-validation-316', 'mrqa_hotpotqa-validation-1818', 'mrqa_naturalquestions-validation-8617', 'mrqa_triviaqa-validation-5996', 'mrqa_hotpotqa-validation-5651', 'mrqa_hotpotqa-validation-3440', 'mrqa_triviaqa-validation-6781', 'mrqa_squad-validation-5345', 'mrqa_triviaqa-validation-2098', 'mrqa_hotpotqa-validation-3780', 'mrqa_triviaqa-validation-5936', 'mrqa_triviaqa-validation-444', 'mrqa_triviaqa-validation-2530', 'mrqa_triviaqa-validation-5795', 'mrqa_triviaqa-validation-2812', 'mrqa_squad-validation-2709', 'mrqa_hotpotqa-validation-5513', 'mrqa_hotpotqa-validation-3789', 'mrqa_triviaqa-validation-287', 'mrqa_triviaqa-validation-1736']
09/23/2021 14:22:10 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:22:10 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=56) .... Timecode: 38
09/23/2021 14:22:23 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:22:23 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 38
09/23/2021 14:22:27 - INFO - __main__ - After Error Fixing: 0.78125
09/23/2021 14:22:27 - INFO - __main__ - Instant Fixing Rate: 0.875
09/23/2021 14:22:27 - INFO - __main__ - Instant Retention Rate: 0.49999999937499995
09/23/2021 14:22:29 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_039.pt.
09/23/2021 14:22:29 - INFO - __main__ - Saving the current error examples (len=24) to the memory.
09/23/2021 14:22:29 - INFO - __main__ - Current memory size: 955.
09/23/2021 14:22:29 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:22:29 - INFO - __main__ - Finished.
09/23/2021 14:22:29 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:22:29 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:22:29 - INFO - __main__ - Evaluating to get errors .... Timecode: 39
09/23/2021 14:22:33 - INFO - __main__ - Before Error Fixing: 0.3125
09/23/2021 14:22:33 - INFO - __main__ - Found 22 errors.
09/23/2021 14:22:33 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:22:33 - INFO - __main__ - Current memory size: 979.
09/23/2021 14:22:33 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:22:56 - INFO - __main__ - before_losses=[tensor(2.3837), tensor(1.5029), tensor(1.5785), tensor(1.4987), tensor(1.5726), tensor(2.1380), tensor(1.5545), tensor(1.4774), tensor(1.5202), tensor(1.4814), tensor(1.8171), tensor(1.6239), tensor(1.5516), tensor(1.6562), tensor(2.3885), tensor(1.4858), tensor(1.4707), tensor(1.4572), tensor(1.5448), tensor(1.4884), tensor(1.5404), tensor(1.4884), tensor(1.6868), tensor(1.5442), tensor(1.4796), tensor(1.5132), tensor(1.5122), tensor(1.4915), tensor(1.5257), tensor(1.4831), tensor(1.4928), tensor(1.6358), tensor(1.5407), tensor(1.4836), tensor(1.5394), tensor(1.4692), tensor(1.5382), tensor(1.4662), tensor(1.5668), tensor(1.5903), tensor(1.5227), tensor(1.4760), tensor(1.9929), tensor(1.5351), tensor(1.5676), tensor(1.4716), tensor(1.5480), tensor(1.5201), tensor(1.4657), tensor(1.5668), tensor(1.5301), tensor(1.6399), tensor(1.5271), tensor(1.5349), tensor(1.5570), tensor(1.4906), tensor(1.5012), tensor(1.4733), tensor(1.5447), tensor(1.4918), tensor(1.4644), tensor(1.5281), tensor(1.5292), tensor(1.5322), tensor(1.5128), tensor(2.0704), tensor(1.4935), tensor(1.6564), tensor(1.7469), tensor(1.4788), tensor(1.7052), tensor(2.6358), tensor(1.4940), tensor(1.5401), tensor(1.4611), tensor(1.4865), tensor(1.4566), tensor(1.4522), tensor(1.4638), tensor(1.5151), tensor(1.4856), tensor(1.4742), tensor(2.1481), tensor(1.5690), tensor(2.0163), tensor(1.4673), tensor(1.5261), tensor(1.5354), tensor(1.4728), tensor(1.5971), tensor(1.7043), tensor(1.4577), tensor(1.5324), tensor(1.5297), tensor(1.5749), tensor(1.5263), tensor(1.5407), tensor(1.4784), tensor(1.6913), tensor(1.5149), tensor(1.5948), tensor(1.4689), tensor(1.4683), tensor(2.0811), tensor(1.5705), tensor(1.5074), tensor(1.5466), tensor(1.4831), tensor(1.4787), tensor(1.4863), tensor(1.7002), tensor(1.4990), tensor(1.4968), tensor(1.5125), tensor(1.5109), tensor(1.5133), tensor(1.5981), tensor(5.3698), tensor(2.5318), tensor(1.5261), tensor(1.6038), tensor(1.4866), tensor(1.4943), tensor(1.5114), tensor(1.7754), tensor(1.4974), tensor(1.5598), tensor(1.5465), tensor(1.5423), tensor(1.4828), tensor(1.4902), tensor(1.4780), tensor(1.4996), tensor(1.5659), tensor(1.4695), tensor(1.5743), tensor(1.5276), tensor(1.6540), tensor(1.4786), tensor(1.5163), tensor(1.5341), tensor(1.4469), tensor(1.4936), tensor(1.8963), tensor(1.5643), tensor(1.9281), tensor(1.4770), tensor(1.5192), tensor(1.5313), tensor(1.4690), tensor(1.5326), tensor(1.5706), tensor(1.4652), tensor(1.5275), tensor(1.4769), tensor(1.4985), tensor(1.5009), tensor(1.5113), tensor(3.6085), tensor(1.4977), tensor(1.5251), tensor(1.5408), tensor(1.7915), tensor(1.5598), tensor(1.4602), tensor(1.4923), tensor(1.7075), tensor(1.5290), tensor(1.5194), tensor(1.5031), tensor(1.4817), tensor(1.4891), tensor(1.5527), tensor(1.4757), tensor(1.5272), tensor(1.4737), tensor(1.5488), tensor(1.5003), tensor(1.4918), tensor(1.4972), tensor(4.1414), tensor(1.5255), tensor(1.5270), tensor(1.5877), tensor(1.5933), tensor(1.7838), tensor(1.5775), tensor(1.4753), tensor(1.4508), tensor(1.5533), tensor(1.7472), tensor(1.5200), tensor(1.4763), tensor(1.4547), tensor(1.8836), tensor(1.5116), tensor(1.5219), tensor(1.4743), tensor(1.5098), tensor(1.5134), tensor(1.5414), tensor(1.5105), tensor(1.4859), tensor(1.4731), tensor(1.4516), tensor(1.4838), tensor(1.9124), tensor(1.4832), tensor(1.5778), tensor(1.7583), tensor(1.4563), tensor(1.5035), tensor(1.5943), tensor(1.4639), tensor(2.5165), tensor(1.5073), tensor(1.4992), tensor(1.8506), tensor(1.5658), tensor(1.5445), tensor(1.4913), tensor(1.5220), tensor(1.4951), tensor(1.5247), tensor(1.5445), tensor(1.7003), tensor(1.6665), tensor(1.5588), tensor(1.7454), tensor(1.5392), tensor(1.5036), tensor(1.5530), tensor(1.4653), tensor(1.5179), tensor(1.4892), tensor(1.6895), tensor(1.4745), tensor(2.0073), tensor(2.0021), tensor(1.5223), tensor(1.5018), tensor(1.5241), tensor(1.5029), tensor(1.4993), tensor(1.4724), tensor(1.7611), tensor(1.4850), tensor(1.5011), tensor(1.5881), tensor(1.5157), tensor(1.6933), tensor(1.5104), tensor(1.4555), tensor(1.6073), tensor(1.5644), tensor(1.4960)]
09/23/2021 14:22:56 - INFO - __main__ - after_losses=[tensor(2.3664), tensor(1.4749), tensor(1.6374), tensor(1.4868), tensor(1.5178), tensor(2.3718), tensor(1.6213), tensor(1.4759), tensor(1.5168), tensor(1.4644), tensor(2.4274), tensor(1.5682), tensor(1.5588), tensor(1.6296), tensor(2.2017), tensor(1.4817), tensor(1.4661), tensor(1.4575), tensor(1.5553), tensor(1.4818), tensor(1.5229), tensor(1.5506), tensor(1.7865), tensor(1.5087), tensor(1.4881), tensor(1.5083), tensor(1.5244), tensor(1.4843), tensor(1.5085), tensor(1.5132), tensor(1.4927), tensor(1.6073), tensor(1.5328), tensor(1.4768), tensor(2.1630), tensor(1.4637), tensor(1.5218), tensor(1.4633), tensor(1.7107), tensor(1.9247), tensor(1.4724), tensor(1.4790), tensor(2.0569), tensor(1.5194), tensor(1.6170), tensor(1.4864), tensor(1.5211), tensor(1.5082), tensor(1.4518), tensor(1.5602), tensor(1.5158), tensor(2.2455), tensor(1.5046), tensor(1.5368), tensor(1.5414), tensor(1.4753), tensor(1.4875), tensor(1.4719), tensor(1.5388), tensor(1.4884), tensor(1.4573), tensor(1.5325), tensor(1.4798), tensor(1.5034), tensor(1.5077), tensor(2.3974), tensor(1.4746), tensor(1.7472), tensor(1.8272), tensor(1.4660), tensor(1.6695), tensor(2.7464), tensor(1.4782), tensor(1.5494), tensor(1.4660), tensor(1.4933), tensor(1.4495), tensor(1.4500), tensor(1.4517), tensor(1.5862), tensor(1.4812), tensor(1.4608), tensor(2.1523), tensor(1.5428), tensor(2.0396), tensor(1.5392), tensor(1.4975), tensor(1.5174), tensor(1.4595), tensor(1.5593), tensor(1.7967), tensor(1.4611), tensor(1.5332), tensor(1.5017), tensor(1.6824), tensor(1.5272), tensor(1.5414), tensor(1.4850), tensor(1.7833), tensor(1.4930), tensor(1.8162), tensor(1.4665), tensor(1.4869), tensor(2.3198), tensor(1.6427), tensor(2.4412), tensor(1.5721), tensor(1.4842), tensor(1.4660), tensor(1.4697), tensor(1.9010), tensor(1.4951), tensor(1.4772), tensor(1.4935), tensor(1.4977), tensor(1.6647), tensor(1.8702), tensor(5.1228), tensor(2.6619), tensor(1.5405), tensor(1.5775), tensor(1.4769), tensor(1.4855), tensor(1.5011), tensor(1.8587), tensor(1.5947), tensor(1.5243), tensor(1.5378), tensor(1.5303), tensor(1.4747), tensor(1.6053), tensor(1.4639), tensor(1.4875), tensor(1.5119), tensor(1.4532), tensor(1.5463), tensor(1.4721), tensor(1.5993), tensor(1.4800), tensor(1.5052), tensor(1.5483), tensor(1.4763), tensor(1.4767), tensor(1.9010), tensor(1.5352), tensor(1.8895), tensor(1.4665), tensor(1.5058), tensor(1.5291), tensor(1.5392), tensor(1.5249), tensor(1.6466), tensor(1.4576), tensor(1.5814), tensor(1.4719), tensor(1.4884), tensor(1.4903), tensor(1.4871), tensor(3.4188), tensor(1.5517), tensor(1.5363), tensor(1.5229), tensor(1.8276), tensor(1.6378), tensor(1.4468), tensor(1.4790), tensor(1.7556), tensor(1.5376), tensor(1.5298), tensor(1.5011), tensor(1.4805), tensor(1.4738), tensor(1.5325), tensor(1.4941), tensor(1.5041), tensor(1.4754), tensor(1.5272), tensor(1.4817), tensor(1.4686), tensor(1.4782), tensor(4.2130), tensor(1.5372), tensor(1.5301), tensor(1.5782), tensor(1.7200), tensor(1.7127), tensor(1.7603), tensor(1.4695), tensor(1.4442), tensor(1.5637), tensor(1.8108), tensor(1.4835), tensor(1.5627), tensor(1.4421), tensor(1.8014), tensor(1.4990), tensor(1.5057), tensor(1.4725), tensor(1.5024), tensor(1.5075), tensor(1.5332), tensor(1.5324), tensor(1.4719), tensor(1.4537), tensor(1.4498), tensor(1.4712), tensor(2.0956), tensor(1.4779), tensor(1.6895), tensor(1.9791), tensor(1.4557), tensor(1.4957), tensor(1.5417), tensor(1.4512), tensor(2.1938), tensor(1.4815), tensor(1.4909), tensor(1.8408), tensor(1.6554), tensor(1.5324), tensor(1.4836), tensor(1.5065), tensor(1.4465), tensor(1.5153), tensor(1.5408), tensor(1.6293), tensor(1.7961), tensor(1.5484), tensor(1.9064), tensor(1.4922), tensor(1.4994), tensor(1.5564), tensor(1.4591), tensor(1.5045), tensor(1.4856), tensor(1.6411), tensor(1.4600), tensor(1.9331), tensor(1.9276), tensor(1.4938), tensor(1.4783), tensor(1.5026), tensor(1.4953), tensor(1.4797), tensor(1.4792), tensor(1.8410), tensor(1.4702), tensor(1.5481), tensor(1.5749), tensor(1.4997), tensor(1.6484), tensor(1.5670), tensor(1.4556), tensor(1.5557), tensor(1.5736), tensor(1.5663)]
09/23/2021 14:22:56 - INFO - __main__ - interference_scores=[tensor(-0.0173), tensor(-0.0280), tensor(0.0589), tensor(-0.0119), tensor(-0.0548), tensor(0.2338), tensor(0.0668), tensor(-0.0015), tensor(-0.0035), tensor(-0.0170), tensor(0.6104), tensor(-0.0557), tensor(0.0072), tensor(-0.0266), tensor(-0.1869), tensor(-0.0042), tensor(-0.0046), tensor(0.0004), tensor(0.0105), tensor(-0.0066), tensor(-0.0176), tensor(0.0621), tensor(0.0997), tensor(-0.0355), tensor(0.0086), tensor(-0.0049), tensor(0.0123), tensor(-0.0071), tensor(-0.0172), tensor(0.0301), tensor(-0.0001), tensor(-0.0285), tensor(-0.0078), tensor(-0.0068), tensor(0.6236), tensor(-0.0055), tensor(-0.0164), tensor(-0.0029), tensor(0.1439), tensor(0.3343), tensor(-0.0504), tensor(0.0030), tensor(0.0640), tensor(-0.0156), tensor(0.0493), tensor(0.0148), tensor(-0.0268), tensor(-0.0119), tensor(-0.0139), tensor(-0.0066), tensor(-0.0143), tensor(0.6056), tensor(-0.0225), tensor(0.0020), tensor(-0.0156), tensor(-0.0153), tensor(-0.0138), tensor(-0.0014), tensor(-0.0059), tensor(-0.0034), tensor(-0.0071), tensor(0.0044), tensor(-0.0494), tensor(-0.0287), tensor(-0.0051), tensor(0.3270), tensor(-0.0189), tensor(0.0908), tensor(0.0803), tensor(-0.0127), tensor(-0.0357), tensor(0.1106), tensor(-0.0158), tensor(0.0093), tensor(0.0048), tensor(0.0068), tensor(-0.0071), tensor(-0.0023), tensor(-0.0121), tensor(0.0711), tensor(-0.0043), tensor(-0.0134), tensor(0.0042), tensor(-0.0262), tensor(0.0233), tensor(0.0720), tensor(-0.0286), tensor(-0.0180), tensor(-0.0134), tensor(-0.0378), tensor(0.0923), tensor(0.0035), tensor(0.0008), tensor(-0.0280), tensor(0.1076), tensor(0.0008), tensor(0.0007), tensor(0.0066), tensor(0.0919), tensor(-0.0219), tensor(0.2214), tensor(-0.0024), tensor(0.0186), tensor(0.2387), tensor(0.0722), tensor(0.9338), tensor(0.0255), tensor(0.0010), tensor(-0.0127), tensor(-0.0166), tensor(0.2008), tensor(-0.0039), tensor(-0.0196), tensor(-0.0190), tensor(-0.0132), tensor(0.1513), tensor(0.2720), tensor(-0.2470), tensor(0.1301), tensor(0.0143), tensor(-0.0263), tensor(-0.0097), tensor(-0.0088), tensor(-0.0103), tensor(0.0833), tensor(0.0974), tensor(-0.0354), tensor(-0.0087), tensor(-0.0120), tensor(-0.0081), tensor(0.1151), tensor(-0.0141), tensor(-0.0122), tensor(-0.0540), tensor(-0.0163), tensor(-0.0280), tensor(-0.0554), tensor(-0.0547), tensor(0.0015), tensor(-0.0111), tensor(0.0143), tensor(0.0294), tensor(-0.0169), tensor(0.0047), tensor(-0.0290), tensor(-0.0387), tensor(-0.0105), tensor(-0.0135), tensor(-0.0022), tensor(0.0702), tensor(-0.0077), tensor(0.0760), tensor(-0.0076), tensor(0.0539), tensor(-0.0049), tensor(-0.0101), tensor(-0.0106), tensor(-0.0242), tensor(-0.1897), tensor(0.0541), tensor(0.0112), tensor(-0.0179), tensor(0.0361), tensor(0.0780), tensor(-0.0133), tensor(-0.0133), tensor(0.0481), tensor(0.0086), tensor(0.0104), tensor(-0.0020), tensor(-0.0012), tensor(-0.0153), tensor(-0.0202), tensor(0.0184), tensor(-0.0231), tensor(0.0016), tensor(-0.0216), tensor(-0.0186), tensor(-0.0232), tensor(-0.0190), tensor(0.0716), tensor(0.0118), tensor(0.0030), tensor(-0.0095), tensor(0.1267), tensor(-0.0711), tensor(0.1828), tensor(-0.0058), tensor(-0.0066), tensor(0.0104), tensor(0.0636), tensor(-0.0365), tensor(0.0864), tensor(-0.0126), tensor(-0.0821), tensor(-0.0125), tensor(-0.0162), tensor(-0.0017), tensor(-0.0073), tensor(-0.0059), tensor(-0.0081), tensor(0.0220), tensor(-0.0140), tensor(-0.0194), tensor(-0.0018), tensor(-0.0126), tensor(0.1833), tensor(-0.0053), tensor(0.1117), tensor(0.2207), tensor(-0.0007), tensor(-0.0078), tensor(-0.0526), tensor(-0.0127), tensor(-0.3227), tensor(-0.0258), tensor(-0.0083), tensor(-0.0098), tensor(0.0896), tensor(-0.0121), tensor(-0.0077), tensor(-0.0154), tensor(-0.0486), tensor(-0.0094), tensor(-0.0036), tensor(-0.0710), tensor(0.1295), tensor(-0.0104), tensor(0.1610), tensor(-0.0469), tensor(-0.0042), tensor(0.0034), tensor(-0.0062), tensor(-0.0134), tensor(-0.0035), tensor(-0.0484), tensor(-0.0146), tensor(-0.0741), tensor(-0.0745), tensor(-0.0285), tensor(-0.0234), tensor(-0.0215), tensor(-0.0077), tensor(-0.0196), tensor(0.0068), tensor(0.0800), tensor(-0.0147), tensor(0.0471), tensor(-0.0132), tensor(-0.0160), tensor(-0.0448), tensor(0.0566), tensor(0.0002), tensor(-0.0516), tensor(0.0092), tensor(0.0702)]
09/23/2021 14:22:56 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-444', 'mrqa_triviaqa-validation-5050', 'mrqa_hotpotqa-validation-1929', 'mrqa_triviaqa-validation-7332', 'mrqa_triviaqa-validation-3808', 'mrqa_triviaqa-validation-2324', 'mrqa_hotpotqa-validation-1906', 'mrqa_squad-validation-1750', 'mrqa_squad-validation-9400', 'mrqa_triviaqa-validation-5962', 'mrqa_squad-validation-5702', 'mrqa_triviaqa-validation-6250', 'mrqa_triviaqa-validation-7090', 'mrqa_triviaqa-validation-3408', 'mrqa_squad-validation-8739', 'mrqa_hotpotqa-validation-3623', 'mrqa_triviaqa-validation-7134', 'mrqa_squad-validation-10312', 'mrqa_triviaqa-validation-1451', 'mrqa_triviaqa-validation-5362', 'mrqa_triviaqa-validation-5036', 'mrqa_triviaqa-validation-6438', 'mrqa_squad-validation-3887', 'mrqa_triviaqa-validation-2683', 'mrqa_naturalquestions-validation-2212', 'mrqa_triviaqa-validation-2091', 'mrqa_triviaqa-validation-1293', 'mrqa_triviaqa-validation-314', 'mrqa_naturalquestions-validation-7346', 'mrqa_squad-validation-8850', 'mrqa_triviaqa-validation-893', 'mrqa_triviaqa-validation-7336']
09/23/2021 14:22:56 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:22:56 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=54) .... Timecode: 39
09/23/2021 14:23:08 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:23:08 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 39
09/23/2021 14:23:11 - INFO - __main__ - After Error Fixing: 0.9375
09/23/2021 14:23:11 - INFO - __main__ - Instant Fixing Rate: 0.9090909090909091
09/23/2021 14:23:11 - INFO - __main__ - Instant Retention Rate: 0.9999999989999999
09/23/2021 14:23:13 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_040.pt.
09/23/2021 14:23:13 - INFO - __main__ - Saving the current error examples (len=22) to the memory.
09/23/2021 14:23:13 - INFO - __main__ - Current memory size: 979.
09/23/2021 14:23:13 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:23:13 - INFO - __main__ - Finished.
09/23/2021 14:23:13 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:23:13 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:23:13 - INFO - __main__ - Evaluating to get errors .... Timecode: 40
09/23/2021 14:23:17 - INFO - __main__ - Before Error Fixing: 0.28125
09/23/2021 14:23:17 - INFO - __main__ - Found 23 errors.
09/23/2021 14:23:17 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:23:17 - INFO - __main__ - Current memory size: 979.
09/23/2021 14:23:17 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:23:40 - INFO - __main__ - before_losses=[tensor(1.5458), tensor(2.3000), tensor(1.4481), tensor(1.4631), tensor(1.4755), tensor(1.5887), tensor(1.5715), tensor(1.5184), tensor(1.5437), tensor(1.4776), tensor(1.4697), tensor(1.4642), tensor(1.9216), tensor(1.6952), tensor(1.9748), tensor(1.4723), tensor(1.8312), tensor(1.4694), tensor(1.6039), tensor(1.4900), tensor(1.5613), tensor(1.5027), tensor(1.5145), tensor(1.5502), tensor(2.4013), tensor(1.4919), tensor(1.5179), tensor(2.2824), tensor(1.9119), tensor(1.5562), tensor(1.8578), tensor(1.5964), tensor(1.4813), tensor(1.4824), tensor(1.5031), tensor(1.9676), tensor(1.4957), tensor(1.4985), tensor(1.5128), tensor(1.5885), tensor(1.5907), tensor(1.4424), tensor(1.5534), tensor(1.5151), tensor(1.8912), tensor(1.4820), tensor(1.4665), tensor(1.9804), tensor(1.7151), tensor(2.8299), tensor(1.6652), tensor(1.5320), tensor(1.5020), tensor(2.4031), tensor(1.5370), tensor(1.4985), tensor(2.0911), tensor(1.4781), tensor(1.6414), tensor(1.4849), tensor(1.4807), tensor(1.4718), tensor(1.4701), tensor(1.5019), tensor(1.4625), tensor(1.4894), tensor(1.5150), tensor(1.5113), tensor(1.4712), tensor(1.4919), tensor(1.5244), tensor(1.4608), tensor(1.4617), tensor(1.4657), tensor(1.5471), tensor(1.5383), tensor(1.5104), tensor(1.5233), tensor(2.5067), tensor(1.5793), tensor(1.5064), tensor(1.4908), tensor(1.5467), tensor(1.4835), tensor(1.5891), tensor(1.5089), tensor(1.6998), tensor(2.5131), tensor(1.7023), tensor(1.5045), tensor(1.4681), tensor(1.5339), tensor(1.5303), tensor(2.6372), tensor(1.5623), tensor(1.8500), tensor(1.7077), tensor(1.4947), tensor(1.9253), tensor(1.7343), tensor(1.5390), tensor(1.7378), tensor(1.5795), tensor(1.5299), tensor(1.5189), tensor(1.8661), tensor(1.5332), tensor(1.4467), tensor(1.4924), tensor(1.5561), tensor(1.4887), tensor(1.5164), tensor(1.4856), tensor(1.5409), tensor(1.5333), tensor(1.8697), tensor(1.5453), tensor(1.5292), tensor(1.4682), tensor(1.4985), tensor(1.5054), tensor(3.2885), tensor(1.8593), tensor(1.5008), tensor(3.2575), tensor(1.5123), tensor(1.4849), tensor(1.5782), tensor(1.6660), tensor(1.5371), tensor(1.4889), tensor(1.5515), tensor(1.4911), tensor(1.6121), tensor(1.5535), tensor(1.4925), tensor(2.2189), tensor(1.4596), tensor(1.4732), tensor(1.5075), tensor(1.5435), tensor(1.4699), tensor(1.6575), tensor(1.5325), tensor(1.5017), tensor(1.5310), tensor(1.5511), tensor(1.4722), tensor(1.5085), tensor(1.4607), tensor(1.4700), tensor(1.5231), tensor(1.4460), tensor(1.5228), tensor(2.3787), tensor(1.5154), tensor(1.5233), tensor(1.5239), tensor(1.5463), tensor(1.4560), tensor(1.4887), tensor(1.4944), tensor(1.9955), tensor(2.6911), tensor(1.5426), tensor(2.5170), tensor(1.8818), tensor(2.1944), tensor(2.6864), tensor(1.4852), tensor(1.4819), tensor(1.5156), tensor(1.4825), tensor(2.1883), tensor(1.5271), tensor(1.6417), tensor(1.5019), tensor(1.5963), tensor(1.9220), tensor(1.5656), tensor(1.5721), tensor(1.5012), tensor(1.4525), tensor(1.4802), tensor(1.4725), tensor(1.8913), tensor(1.5464), tensor(1.5653), tensor(1.4762), tensor(2.9332), tensor(1.4812), tensor(1.4659), tensor(1.9803), tensor(1.6052), tensor(1.5409), tensor(1.8140), tensor(1.6401), tensor(1.5257), tensor(1.4906), tensor(2.5918), tensor(1.4882), tensor(1.5286), tensor(1.5809), tensor(1.4662), tensor(1.7154), tensor(1.9138), tensor(1.5054), tensor(1.5191), tensor(1.5108), tensor(1.4639), tensor(1.4824), tensor(1.4544), tensor(1.5080), tensor(1.5314), tensor(1.6094), tensor(1.4914), tensor(1.5064), tensor(1.5042), tensor(1.5687), tensor(1.4825), tensor(1.5765), tensor(1.5113), tensor(1.4514), tensor(1.5199), tensor(1.5359), tensor(1.4609), tensor(1.4981), tensor(1.4637), tensor(1.5432), tensor(1.4790), tensor(1.5230), tensor(1.4804), tensor(1.5049), tensor(1.4690), tensor(1.5388), tensor(1.4888), tensor(2.0019), tensor(1.5230), tensor(1.5090), tensor(1.5171), tensor(1.5590), tensor(1.4978), tensor(1.5177), tensor(1.5682), tensor(1.6212), tensor(1.5399), tensor(1.5170), tensor(1.5339), tensor(1.6061), tensor(1.5351), tensor(1.5457), tensor(1.4894), tensor(1.5215), tensor(1.7059), tensor(1.6389), tensor(1.4440)]
09/23/2021 14:23:40 - INFO - __main__ - after_losses=[tensor(1.5468), tensor(2.4962), tensor(1.4400), tensor(1.4630), tensor(1.4845), tensor(1.5707), tensor(1.5526), tensor(1.5023), tensor(1.5260), tensor(1.4713), tensor(1.4719), tensor(1.4467), tensor(1.8137), tensor(1.7008), tensor(1.8690), tensor(1.4653), tensor(1.7413), tensor(1.4539), tensor(1.7344), tensor(1.4888), tensor(1.5650), tensor(1.5000), tensor(1.5018), tensor(1.5448), tensor(2.4541), tensor(1.4902), tensor(1.5162), tensor(2.2652), tensor(1.9672), tensor(1.5087), tensor(1.8449), tensor(1.5728), tensor(1.4866), tensor(1.4942), tensor(1.4838), tensor(2.1760), tensor(1.4849), tensor(1.5150), tensor(1.5036), tensor(1.5834), tensor(1.7625), tensor(1.4430), tensor(1.5386), tensor(1.4841), tensor(1.5351), tensor(1.4943), tensor(1.4607), tensor(2.0140), tensor(1.6865), tensor(2.9703), tensor(1.7311), tensor(1.5324), tensor(1.5019), tensor(2.4582), tensor(1.5316), tensor(1.5381), tensor(2.1150), tensor(1.4635), tensor(1.5744), tensor(1.4892), tensor(1.4796), tensor(1.4442), tensor(1.4771), tensor(1.4867), tensor(1.4567), tensor(1.4834), tensor(1.4871), tensor(1.5541), tensor(1.4624), tensor(1.4779), tensor(1.5150), tensor(1.4574), tensor(1.4590), tensor(1.4512), tensor(1.5057), tensor(1.5350), tensor(1.4986), tensor(1.5142), tensor(2.1923), tensor(1.5567), tensor(1.4652), tensor(1.4896), tensor(1.5230), tensor(1.4794), tensor(1.5782), tensor(1.4928), tensor(1.7945), tensor(1.4846), tensor(1.7741), tensor(1.4969), tensor(1.4608), tensor(1.5510), tensor(1.5167), tensor(2.7335), tensor(1.5866), tensor(1.8148), tensor(1.7614), tensor(1.4717), tensor(2.0485), tensor(1.8604), tensor(1.5255), tensor(1.9106), tensor(1.6000), tensor(1.4983), tensor(1.5061), tensor(1.7668), tensor(1.5177), tensor(1.4504), tensor(1.4774), tensor(1.6165), tensor(1.4757), tensor(1.7571), tensor(1.4821), tensor(1.5281), tensor(1.5522), tensor(1.8963), tensor(1.6247), tensor(1.5223), tensor(1.4678), tensor(1.4830), tensor(1.5216), tensor(3.1562), tensor(1.6438), tensor(1.4865), tensor(3.3696), tensor(1.5228), tensor(1.4679), tensor(1.7830), tensor(1.9035), tensor(1.5199), tensor(1.4838), tensor(1.6097), tensor(1.4860), tensor(1.6055), tensor(1.5389), tensor(1.4826), tensor(2.1940), tensor(1.4516), tensor(1.4674), tensor(1.4801), tensor(1.5090), tensor(1.4664), tensor(1.6449), tensor(1.5252), tensor(1.5390), tensor(1.5048), tensor(1.5623), tensor(1.4612), tensor(1.4994), tensor(1.4598), tensor(1.4628), tensor(1.5188), tensor(1.4416), tensor(1.4922), tensor(2.5362), tensor(1.4969), tensor(1.5525), tensor(1.5328), tensor(1.5392), tensor(1.5389), tensor(1.4699), tensor(1.4878), tensor(1.9628), tensor(2.4689), tensor(1.5314), tensor(2.2910), tensor(1.8696), tensor(2.0899), tensor(2.8931), tensor(1.4757), tensor(1.4688), tensor(1.4882), tensor(1.4710), tensor(1.6607), tensor(1.5178), tensor(1.6412), tensor(1.4871), tensor(1.6008), tensor(2.0329), tensor(1.5607), tensor(1.6098), tensor(1.4742), tensor(1.4483), tensor(1.4662), tensor(1.4709), tensor(1.8834), tensor(1.5378), tensor(1.5921), tensor(1.4707), tensor(2.9667), tensor(1.4830), tensor(1.4591), tensor(1.8323), tensor(1.5502), tensor(1.5414), tensor(1.6947), tensor(1.6387), tensor(1.5280), tensor(1.4739), tensor(2.5655), tensor(1.4823), tensor(1.5407), tensor(1.5732), tensor(1.4607), tensor(1.6526), tensor(1.8738), tensor(1.5108), tensor(1.5090), tensor(1.5087), tensor(1.4580), tensor(1.4757), tensor(1.4508), tensor(1.4935), tensor(1.5053), tensor(1.5745), tensor(1.4823), tensor(1.4921), tensor(1.4896), tensor(1.5235), tensor(1.4819), tensor(1.5615), tensor(1.4698), tensor(1.4446), tensor(1.5006), tensor(1.5316), tensor(1.4537), tensor(1.5248), tensor(1.4591), tensor(1.5041), tensor(1.4746), tensor(1.4753), tensor(1.4779), tensor(1.5028), tensor(1.4610), tensor(1.4890), tensor(1.4811), tensor(2.0689), tensor(1.5114), tensor(1.5014), tensor(1.5074), tensor(1.5229), tensor(1.4913), tensor(2.8939), tensor(1.5461), tensor(1.5952), tensor(1.5238), tensor(1.4881), tensor(1.5522), tensor(1.5506), tensor(1.5242), tensor(1.5311), tensor(1.4812), tensor(1.5661), tensor(1.6293), tensor(1.7469), tensor(1.4406)]
09/23/2021 14:23:40 - INFO - __main__ - interference_scores=[tensor(0.0011), tensor(0.1962), tensor(-0.0082), tensor(-9.6440e-05), tensor(0.0090), tensor(-0.0181), tensor(-0.0189), tensor(-0.0161), tensor(-0.0177), tensor(-0.0063), tensor(0.0022), tensor(-0.0175), tensor(-0.1078), tensor(0.0055), tensor(-0.1058), tensor(-0.0070), tensor(-0.0900), tensor(-0.0155), tensor(0.1305), tensor(-0.0012), tensor(0.0037), tensor(-0.0027), tensor(-0.0127), tensor(-0.0054), tensor(0.0528), tensor(-0.0017), tensor(-0.0017), tensor(-0.0172), tensor(0.0553), tensor(-0.0475), tensor(-0.0129), tensor(-0.0236), tensor(0.0053), tensor(0.0118), tensor(-0.0194), tensor(0.2083), tensor(-0.0108), tensor(0.0164), tensor(-0.0093), tensor(-0.0051), tensor(0.1719), tensor(0.0006), tensor(-0.0148), tensor(-0.0310), tensor(-0.3561), tensor(0.0123), tensor(-0.0057), tensor(0.0336), tensor(-0.0286), tensor(0.1405), tensor(0.0660), tensor(0.0004), tensor(-0.0002), tensor(0.0550), tensor(-0.0054), tensor(0.0396), tensor(0.0239), tensor(-0.0146), tensor(-0.0669), tensor(0.0043), tensor(-0.0010), tensor(-0.0277), tensor(0.0070), tensor(-0.0152), tensor(-0.0059), tensor(-0.0060), tensor(-0.0279), tensor(0.0428), tensor(-0.0088), tensor(-0.0141), tensor(-0.0094), tensor(-0.0034), tensor(-0.0027), tensor(-0.0145), tensor(-0.0414), tensor(-0.0033), tensor(-0.0118), tensor(-0.0090), tensor(-0.3144), tensor(-0.0226), tensor(-0.0412), tensor(-0.0012), tensor(-0.0237), tensor(-0.0041), tensor(-0.0110), tensor(-0.0160), tensor(0.0948), tensor(-1.0285), tensor(0.0718), tensor(-0.0075), tensor(-0.0073), tensor(0.0171), tensor(-0.0135), tensor(0.0962), tensor(0.0243), tensor(-0.0352), tensor(0.0538), tensor(-0.0230), tensor(0.1231), tensor(0.1261), tensor(-0.0135), tensor(0.1728), tensor(0.0204), tensor(-0.0315), tensor(-0.0128), tensor(-0.0993), tensor(-0.0156), tensor(0.0037), tensor(-0.0149), tensor(0.0605), tensor(-0.0131), tensor(0.2408), tensor(-0.0034), tensor(-0.0128), tensor(0.0190), tensor(0.0266), tensor(0.0794), tensor(-0.0069), tensor(-0.0004), tensor(-0.0155), tensor(0.0163), tensor(-0.1323), tensor(-0.2155), tensor(-0.0143), tensor(0.1122), tensor(0.0105), tensor(-0.0169), tensor(0.2048), tensor(0.2375), tensor(-0.0173), tensor(-0.0050), tensor(0.0582), tensor(-0.0051), tensor(-0.0066), tensor(-0.0146), tensor(-0.0099), tensor(-0.0249), tensor(-0.0080), tensor(-0.0057), tensor(-0.0274), tensor(-0.0345), tensor(-0.0035), tensor(-0.0126), tensor(-0.0073), tensor(0.0373), tensor(-0.0262), tensor(0.0112), tensor(-0.0110), tensor(-0.0091), tensor(-0.0009), tensor(-0.0073), tensor(-0.0043), tensor(-0.0044), tensor(-0.0306), tensor(0.1574), tensor(-0.0185), tensor(0.0292), tensor(0.0088), tensor(-0.0071), tensor(0.0829), tensor(-0.0188), tensor(-0.0065), tensor(-0.0327), tensor(-0.2222), tensor(-0.0112), tensor(-0.2261), tensor(-0.0123), tensor(-0.1045), tensor(0.2068), tensor(-0.0095), tensor(-0.0132), tensor(-0.0275), tensor(-0.0115), tensor(-0.5275), tensor(-0.0093), tensor(-0.0005), tensor(-0.0148), tensor(0.0045), tensor(0.1108), tensor(-0.0049), tensor(0.0378), tensor(-0.0271), tensor(-0.0043), tensor(-0.0140), tensor(-0.0016), tensor(-0.0078), tensor(-0.0086), tensor(0.0269), tensor(-0.0054), tensor(0.0335), tensor(0.0019), tensor(-0.0068), tensor(-0.1480), tensor(-0.0551), tensor(0.0005), tensor(-0.1194), tensor(-0.0013), tensor(0.0024), tensor(-0.0166), tensor(-0.0263), tensor(-0.0058), tensor(0.0121), tensor(-0.0077), tensor(-0.0056), tensor(-0.0628), tensor(-0.0399), tensor(0.0055), tensor(-0.0101), tensor(-0.0021), tensor(-0.0059), tensor(-0.0067), tensor(-0.0035), tensor(-0.0145), tensor(-0.0261), tensor(-0.0349), tensor(-0.0091), tensor(-0.0143), tensor(-0.0146), tensor(-0.0452), tensor(-0.0006), tensor(-0.0150), tensor(-0.0415), tensor(-0.0068), tensor(-0.0194), tensor(-0.0043), tensor(-0.0072), tensor(0.0267), tensor(-0.0046), tensor(-0.0391), tensor(-0.0045), tensor(-0.0477), tensor(-0.0024), tensor(-0.0021), tensor(-0.0081), tensor(-0.0497), tensor(-0.0077), tensor(0.0670), tensor(-0.0116), tensor(-0.0076), tensor(-0.0097), tensor(-0.0361), tensor(-0.0065), tensor(1.3761), tensor(-0.0221), tensor(-0.0260), tensor(-0.0161), tensor(-0.0288), tensor(0.0182), tensor(-0.0555), tensor(-0.0110), tensor(-0.0146), tensor(-0.0083), tensor(0.0446), tensor(-0.0766), tensor(0.1080), tensor(-0.0034)]
09/23/2021 14:23:40 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-3887', 'mrqa_squad-validation-2966', 'mrqa_triviaqa-validation-6423', 'mrqa_naturalquestions-validation-7704', 'mrqa_triviaqa-validation-4824', 'mrqa_triviaqa-validation-2096', 'mrqa_squad-validation-7382', 'mrqa_triviaqa-validation-7669', 'mrqa_hotpotqa-validation-3971', 'mrqa_squad-validation-3463', 'mrqa_squad-validation-6915', 'mrqa_squad-validation-8280', 'mrqa_triviaqa-validation-1521', 'mrqa_squad-validation-2885', 'mrqa_naturalquestions-validation-1375', 'mrqa_triviaqa-validation-1935', 'mrqa_triviaqa-validation-1280', 'mrqa_triviaqa-validation-751', 'mrqa_triviaqa-validation-7506', 'mrqa_squad-validation-194', 'mrqa_naturalquestions-validation-3027', 'mrqa_triviaqa-validation-1578', 'mrqa_squad-validation-3558', 'mrqa_triviaqa-validation-1550', 'mrqa_triviaqa-validation-7336', 'mrqa_squad-validation-3708', 'mrqa_squad-validation-3539', 'mrqa_triviaqa-validation-5258', 'mrqa_squad-validation-608', 'mrqa_triviaqa-validation-6224', 'mrqa_squad-validation-7572', 'mrqa_triviaqa-validation-6385']
09/23/2021 14:23:40 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:23:40 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=55) .... Timecode: 40
09/23/2021 14:23:53 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:23:53 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 40
09/23/2021 14:23:56 - INFO - __main__ - After Error Fixing: 0.96875
09/23/2021 14:23:56 - INFO - __main__ - Instant Fixing Rate: 1.0
09/23/2021 14:23:56 - INFO - __main__ - Instant Retention Rate: 0.8888888879012344
09/23/2021 14:23:58 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_041.pt.
09/23/2021 14:23:58 - INFO - __main__ - Saving the current error examples (len=23) to the memory.
09/23/2021 14:23:58 - INFO - __main__ - Current memory size: 979.
09/23/2021 14:23:58 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:23:58 - INFO - __main__ - Finished.
09/23/2021 14:23:58 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:23:58 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:23:58 - INFO - __main__ - Evaluating to get errors .... Timecode: 41
09/23/2021 14:24:00 - INFO - __main__ - Before Error Fixing: 0.125
09/23/2021 14:24:00 - INFO - __main__ - Found 28 errors.
09/23/2021 14:24:00 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:24:00 - INFO - __main__ - Current memory size: 1002.
09/23/2021 14:24:00 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:24:23 - INFO - __main__ - before_losses=[tensor(1.4642), tensor(2.1162), tensor(1.5033), tensor(1.4985), tensor(1.5206), tensor(1.5535), tensor(1.4902), tensor(1.4926), tensor(1.9278), tensor(1.6547), tensor(1.5209), tensor(1.5128), tensor(1.4857), tensor(1.6644), tensor(1.5795), tensor(1.5313), tensor(1.4768), tensor(1.4806), tensor(1.4650), tensor(1.8990), tensor(1.4931), tensor(2.2127), tensor(2.6100), tensor(1.4769), tensor(1.5526), tensor(1.5177), tensor(1.5477), tensor(1.8048), tensor(1.5177), tensor(1.5795), tensor(2.0458), tensor(1.5738), tensor(1.5102), tensor(1.5402), tensor(1.4996), tensor(1.5066), tensor(1.4844), tensor(1.5275), tensor(1.5438), tensor(1.4656), tensor(1.5271), tensor(2.2414), tensor(1.4973), tensor(1.5198), tensor(1.5284), tensor(2.1627), tensor(1.7838), tensor(1.5209), tensor(1.5499), tensor(1.8320), tensor(1.4750), tensor(1.5661), tensor(1.5282), tensor(1.7378), tensor(1.5108), tensor(1.5308), tensor(1.5168), tensor(1.4852), tensor(1.5071), tensor(1.5644), tensor(1.4938), tensor(1.4890), tensor(1.5553), tensor(1.5135), tensor(1.5123), tensor(1.6486), tensor(2.5823), tensor(1.5067), tensor(1.4946), tensor(1.5181), tensor(1.5497), tensor(1.4602), tensor(1.6260), tensor(1.4828), tensor(1.5168), tensor(1.4803), tensor(1.5113), tensor(1.4832), tensor(1.5084), tensor(1.8865), tensor(1.5199), tensor(1.5302), tensor(1.5270), tensor(1.4809), tensor(1.8811), tensor(1.5469), tensor(1.6661), tensor(2.1901), tensor(2.1443), tensor(1.5225), tensor(1.4987), tensor(1.5495), tensor(1.6348), tensor(1.4627), tensor(1.4979), tensor(1.5271), tensor(1.5070), tensor(1.5317), tensor(1.4742), tensor(1.5605), tensor(1.6332), tensor(1.5380), tensor(1.5494), tensor(1.5973), tensor(1.4781), tensor(1.5225), tensor(1.5506), tensor(1.4983), tensor(1.6118), tensor(1.4503), tensor(2.0326), tensor(1.5469), tensor(1.5651), tensor(1.5901), tensor(1.5505), tensor(1.4868), tensor(1.5638), tensor(1.5356), tensor(1.5287), tensor(1.5892), tensor(1.5144), tensor(1.6455), tensor(2.2917), tensor(3.3468), tensor(1.5553), tensor(1.6267), tensor(2.9844), tensor(1.4767), tensor(1.4883), tensor(1.9121), tensor(1.6175), tensor(1.4989), tensor(1.6373), tensor(2.1993), tensor(1.4570), tensor(1.5311), tensor(1.4626), tensor(1.5448), tensor(1.4891), tensor(1.5001), tensor(1.6147), tensor(1.5358), tensor(1.6754), tensor(1.4983), tensor(1.5484), tensor(1.5632), tensor(1.5184), tensor(2.0179), tensor(1.4903), tensor(1.5170), tensor(1.4910), tensor(1.5633), tensor(1.4759), tensor(1.5316), tensor(2.4067), tensor(1.5463), tensor(1.5328), tensor(1.5757), tensor(1.4964), tensor(1.7671), tensor(2.7178), tensor(1.5245), tensor(1.5212), tensor(2.1678), tensor(1.5566), tensor(1.5043), tensor(1.5721), tensor(1.6292), tensor(1.5659), tensor(2.1864), tensor(1.4973), tensor(1.6082), tensor(1.5163), tensor(1.4703), tensor(1.4867), tensor(1.5193), tensor(1.4783), tensor(1.8419), tensor(1.8973), tensor(2.0451), tensor(1.5171), tensor(1.5794), tensor(1.5458), tensor(1.5073), tensor(1.4815), tensor(1.4872), tensor(1.7158), tensor(1.5527), tensor(1.6031), tensor(1.5370), tensor(1.5038), tensor(1.4763), tensor(1.4875), tensor(1.4916), tensor(1.5721), tensor(1.8558), tensor(1.5298), tensor(1.5796), tensor(1.5470), tensor(1.4550), tensor(1.6773), tensor(1.5126), tensor(1.9127), tensor(1.5332), tensor(1.5138), tensor(2.3252), tensor(1.4674), tensor(2.1929), tensor(1.8477), tensor(1.5267), tensor(1.5256), tensor(1.4855), tensor(1.5214), tensor(1.5799), tensor(1.4927), tensor(1.5572), tensor(1.5794), tensor(1.5285), tensor(1.5163), tensor(1.5679), tensor(1.5005), tensor(1.4920), tensor(1.8088), tensor(1.5172), tensor(1.4653), tensor(1.5046), tensor(1.4949), tensor(1.4810), tensor(1.5422), tensor(1.5651), tensor(2.8370), tensor(1.5604), tensor(1.4693), tensor(1.6003), tensor(1.5225), tensor(1.4965), tensor(1.5327), tensor(1.5421), tensor(1.5081), tensor(1.5259), tensor(1.4776), tensor(1.5309), tensor(1.5515), tensor(1.5354), tensor(1.5173), tensor(1.5210), tensor(1.5257), tensor(1.5093), tensor(1.5418), tensor(1.5452), tensor(1.6036), tensor(1.5244), tensor(1.4639), tensor(1.5413), tensor(1.5554), tensor(1.5392)]
09/23/2021 14:24:23 - INFO - __main__ - after_losses=[tensor(1.4618), tensor(2.0809), tensor(1.4944), tensor(1.4844), tensor(1.4980), tensor(1.5459), tensor(1.4822), tensor(1.4948), tensor(2.0343), tensor(1.7629), tensor(1.5550), tensor(1.5015), tensor(1.4874), tensor(1.5963), tensor(1.5834), tensor(1.5201), tensor(1.4679), tensor(1.4765), tensor(1.4586), tensor(1.8423), tensor(1.4801), tensor(2.2428), tensor(2.4772), tensor(1.4554), tensor(1.5276), tensor(1.5464), tensor(1.5340), tensor(1.5879), tensor(1.7620), tensor(1.5578), tensor(1.9554), tensor(1.5740), tensor(1.4790), tensor(1.4937), tensor(1.5006), tensor(1.4728), tensor(1.4960), tensor(1.5179), tensor(1.5712), tensor(1.4600), tensor(1.5150), tensor(2.2752), tensor(1.4867), tensor(1.4945), tensor(1.5061), tensor(1.9961), tensor(1.9655), tensor(1.5218), tensor(1.5401), tensor(1.8655), tensor(1.4695), tensor(1.5472), tensor(1.5588), tensor(2.0137), tensor(1.4971), tensor(1.5199), tensor(1.5092), tensor(1.4810), tensor(1.5012), tensor(1.5807), tensor(1.4858), tensor(1.5056), tensor(1.5845), tensor(1.5169), tensor(1.4877), tensor(1.6324), tensor(2.4846), tensor(1.4644), tensor(1.4847), tensor(1.5148), tensor(1.5410), tensor(1.4471), tensor(1.5758), tensor(1.4687), tensor(1.4937), tensor(1.6453), tensor(1.5102), tensor(1.4869), tensor(1.5012), tensor(2.3380), tensor(1.5748), tensor(1.5183), tensor(1.5086), tensor(1.4635), tensor(2.1547), tensor(1.5014), tensor(1.7143), tensor(1.9959), tensor(2.0870), tensor(1.5133), tensor(1.4965), tensor(1.5382), tensor(1.5345), tensor(1.4567), tensor(1.4879), tensor(1.5060), tensor(1.4927), tensor(1.5861), tensor(1.4636), tensor(1.5323), tensor(1.5533), tensor(1.5024), tensor(1.5184), tensor(1.5415), tensor(1.4702), tensor(1.5006), tensor(1.5720), tensor(1.4788), tensor(1.5474), tensor(1.4424), tensor(1.5327), tensor(1.5315), tensor(1.5359), tensor(1.7933), tensor(1.5411), tensor(1.4738), tensor(1.5259), tensor(1.5246), tensor(1.5178), tensor(1.5448), tensor(1.5086), tensor(1.6602), tensor(2.2770), tensor(3.5116), tensor(1.5335), tensor(1.6332), tensor(1.7989), tensor(1.4808), tensor(1.4800), tensor(1.8352), tensor(1.5851), tensor(1.5024), tensor(1.5896), tensor(3.0254), tensor(1.4536), tensor(1.5342), tensor(1.4555), tensor(1.5383), tensor(1.4867), tensor(1.4899), tensor(1.6340), tensor(1.9318), tensor(1.7089), tensor(1.4925), tensor(1.5503), tensor(1.5326), tensor(1.5053), tensor(2.0435), tensor(1.4753), tensor(1.4876), tensor(1.4756), tensor(1.5318), tensor(1.4613), tensor(1.5179), tensor(1.7016), tensor(1.5223), tensor(1.5234), tensor(1.6397), tensor(1.4689), tensor(1.6943), tensor(2.5753), tensor(1.4706), tensor(1.5003), tensor(1.8267), tensor(1.5376), tensor(1.4937), tensor(1.5554), tensor(1.5472), tensor(1.5440), tensor(2.1179), tensor(1.4927), tensor(1.7669), tensor(1.4957), tensor(1.4753), tensor(1.4844), tensor(1.5062), tensor(1.4738), tensor(1.9239), tensor(1.6110), tensor(1.5156), tensor(1.4858), tensor(1.5740), tensor(1.5723), tensor(1.6144), tensor(1.4703), tensor(1.4721), tensor(1.5913), tensor(1.5311), tensor(1.5735), tensor(1.5264), tensor(1.5070), tensor(1.4664), tensor(1.4724), tensor(1.6293), tensor(1.5483), tensor(1.7620), tensor(1.5032), tensor(1.5682), tensor(1.5123), tensor(2.5303), tensor(1.6327), tensor(1.4890), tensor(2.5250), tensor(1.5257), tensor(1.5038), tensor(2.2740), tensor(1.4611), tensor(2.2100), tensor(1.8965), tensor(1.5352), tensor(1.4978), tensor(1.4816), tensor(1.5131), tensor(1.5819), tensor(1.4764), tensor(1.5363), tensor(1.5431), tensor(1.5120), tensor(1.5177), tensor(1.5665), tensor(1.4745), tensor(1.4816), tensor(1.7405), tensor(1.5010), tensor(1.4582), tensor(1.4945), tensor(1.4804), tensor(1.4719), tensor(1.5541), tensor(1.5459), tensor(2.7336), tensor(1.5064), tensor(1.4681), tensor(1.5617), tensor(1.4918), tensor(1.4828), tensor(1.5164), tensor(1.5301), tensor(1.5013), tensor(1.5333), tensor(1.4666), tensor(1.5159), tensor(1.5315), tensor(1.5196), tensor(1.5011), tensor(1.5143), tensor(1.5250), tensor(1.4949), tensor(1.5073), tensor(1.5241), tensor(1.5866), tensor(1.4828), tensor(1.4696), tensor(1.5391), tensor(1.5493), tensor(1.5217)]
09/23/2021 14:24:23 - INFO - __main__ - interference_scores=[tensor(-0.0023), tensor(-0.0353), tensor(-0.0089), tensor(-0.0141), tensor(-0.0226), tensor(-0.0076), tensor(-0.0079), tensor(0.0022), tensor(0.1065), tensor(0.1082), tensor(0.0341), tensor(-0.0112), tensor(0.0018), tensor(-0.0681), tensor(0.0038), tensor(-0.0112), tensor(-0.0089), tensor(-0.0041), tensor(-0.0065), tensor(-0.0567), tensor(-0.0130), tensor(0.0301), tensor(-0.1328), tensor(-0.0215), tensor(-0.0250), tensor(0.0287), tensor(-0.0138), tensor(-0.2168), tensor(0.2443), tensor(-0.0217), tensor(-0.0904), tensor(0.0003), tensor(-0.0312), tensor(-0.0466), tensor(0.0010), tensor(-0.0338), tensor(0.0115), tensor(-0.0096), tensor(0.0274), tensor(-0.0056), tensor(-0.0121), tensor(0.0338), tensor(-0.0105), tensor(-0.0253), tensor(-0.0224), tensor(-0.1666), tensor(0.1818), tensor(0.0009), tensor(-0.0098), tensor(0.0335), tensor(-0.0054), tensor(-0.0189), tensor(0.0307), tensor(0.2759), tensor(-0.0137), tensor(-0.0109), tensor(-0.0077), tensor(-0.0042), tensor(-0.0059), tensor(0.0163), tensor(-0.0079), tensor(0.0166), tensor(0.0293), tensor(0.0034), tensor(-0.0246), tensor(-0.0163), tensor(-0.0977), tensor(-0.0423), tensor(-0.0099), tensor(-0.0033), tensor(-0.0087), tensor(-0.0132), tensor(-0.0503), tensor(-0.0141), tensor(-0.0231), tensor(0.1650), tensor(-0.0011), tensor(0.0037), tensor(-0.0072), tensor(0.4515), tensor(0.0549), tensor(-0.0119), tensor(-0.0183), tensor(-0.0174), tensor(0.2737), tensor(-0.0455), tensor(0.0482), tensor(-0.1942), tensor(-0.0573), tensor(-0.0091), tensor(-0.0022), tensor(-0.0113), tensor(-0.1004), tensor(-0.0060), tensor(-0.0100), tensor(-0.0211), tensor(-0.0143), tensor(0.0544), tensor(-0.0106), tensor(-0.0282), tensor(-0.0799), tensor(-0.0356), tensor(-0.0309), tensor(-0.0558), tensor(-0.0078), tensor(-0.0220), tensor(0.0214), tensor(-0.0195), tensor(-0.0644), tensor(-0.0078), tensor(-0.4999), tensor(-0.0153), tensor(-0.0293), tensor(0.2033), tensor(-0.0094), tensor(-0.0130), tensor(-0.0379), tensor(-0.0109), tensor(-0.0110), tensor(-0.0444), tensor(-0.0059), tensor(0.0146), tensor(-0.0147), tensor(0.1649), tensor(-0.0219), tensor(0.0066), tensor(-1.1855), tensor(0.0041), tensor(-0.0083), tensor(-0.0768), tensor(-0.0325), tensor(0.0035), tensor(-0.0478), tensor(0.8261), tensor(-0.0034), tensor(0.0031), tensor(-0.0071), tensor(-0.0065), tensor(-0.0024), tensor(-0.0102), tensor(0.0194), tensor(0.3960), tensor(0.0335), tensor(-0.0058), tensor(0.0019), tensor(-0.0307), tensor(-0.0130), tensor(0.0256), tensor(-0.0150), tensor(-0.0294), tensor(-0.0154), tensor(-0.0315), tensor(-0.0146), tensor(-0.0137), tensor(-0.7051), tensor(-0.0240), tensor(-0.0094), tensor(0.0640), tensor(-0.0275), tensor(-0.0728), tensor(-0.1425), tensor(-0.0538), tensor(-0.0209), tensor(-0.3411), tensor(-0.0190), tensor(-0.0106), tensor(-0.0167), tensor(-0.0820), tensor(-0.0220), tensor(-0.0684), tensor(-0.0047), tensor(0.1587), tensor(-0.0206), tensor(0.0049), tensor(-0.0022), tensor(-0.0130), tensor(-0.0044), tensor(0.0820), tensor(-0.2863), tensor(-0.5295), tensor(-0.0313), tensor(-0.0055), tensor(0.0264), tensor(0.1071), tensor(-0.0112), tensor(-0.0151), tensor(-0.1245), tensor(-0.0216), tensor(-0.0296), tensor(-0.0107), tensor(0.0033), tensor(-0.0100), tensor(-0.0150), tensor(0.1377), tensor(-0.0238), tensor(-0.0938), tensor(-0.0266), tensor(-0.0114), tensor(-0.0347), tensor(1.0753), tensor(-0.0446), tensor(-0.0236), tensor(0.6123), tensor(-0.0075), tensor(-0.0100), tensor(-0.0512), tensor(-0.0063), tensor(0.0171), tensor(0.0487), tensor(0.0085), tensor(-0.0278), tensor(-0.0039), tensor(-0.0082), tensor(0.0020), tensor(-0.0163), tensor(-0.0209), tensor(-0.0363), tensor(-0.0164), tensor(0.0014), tensor(-0.0015), tensor(-0.0260), tensor(-0.0105), tensor(-0.0683), tensor(-0.0162), tensor(-0.0070), tensor(-0.0102), tensor(-0.0144), tensor(-0.0091), tensor(0.0119), tensor(-0.0191), tensor(-0.1034), tensor(-0.0539), tensor(-0.0012), tensor(-0.0386), tensor(-0.0307), tensor(-0.0137), tensor(-0.0163), tensor(-0.0120), tensor(-0.0068), tensor(0.0074), tensor(-0.0110), tensor(-0.0150), tensor(-0.0200), tensor(-0.0157), tensor(-0.0161), tensor(-0.0067), tensor(-0.0008), tensor(-0.0145), tensor(-0.0345), tensor(-0.0211), tensor(-0.0170), tensor(-0.0416), tensor(0.0057), tensor(-0.0022), tensor(-0.0062), tensor(-0.0175)]
09/23/2021 14:24:23 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-7382', 'mrqa_triviaqa-validation-751', 'mrqa_squad-validation-2966', 'mrqa_naturalquestions-validation-4664', 'mrqa_squad-validation-6248', 'mrqa_triviaqa-validation-4054', 'mrqa_squad-validation-1249', 'mrqa_triviaqa-validation-1578', 'mrqa_triviaqa-validation-6107', 'mrqa_triviaqa-validation-1423', 'mrqa_triviaqa-validation-2609', 'mrqa_squad-validation-2428', 'mrqa_squad-validation-8247', 'mrqa_triviaqa-validation-4069', 'mrqa_triviaqa-validation-6772', 'mrqa_naturalquestions-validation-142', 'mrqa_squad-validation-8966', 'mrqa_triviaqa-validation-3933', 'mrqa_hotpotqa-validation-2559', 'mrqa_triviaqa-validation-3324', 'mrqa_hotpotqa-validation-1289', 'mrqa_triviaqa-validation-6556', 'mrqa_triviaqa-validation-6259', 'mrqa_triviaqa-validation-2802', 'mrqa_naturalquestions-validation-5017', 'mrqa_triviaqa-validation-1276', 'mrqa_triviaqa-validation-4852', 'mrqa_naturalquestions-validation-7356', 'mrqa_squad-validation-1003', 'mrqa_squad-validation-9827', 'mrqa_triviaqa-validation-3552', 'mrqa_naturalquestions-validation-2448']
09/23/2021 14:24:23 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:24:23 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=60) .... Timecode: 41
09/23/2021 14:24:37 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:24:37 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 41
09/23/2021 14:24:40 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 14:24:40 - INFO - __main__ - Instant Fixing Rate: 0.8571428571428571
09/23/2021 14:24:40 - INFO - __main__ - Instant Retention Rate: 0.7499999981250001
09/23/2021 14:24:43 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_042.pt.
09/23/2021 14:24:43 - INFO - __main__ - Saving the current error examples (len=28) to the memory.
09/23/2021 14:24:43 - INFO - __main__ - Current memory size: 1002.
09/23/2021 14:24:43 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:24:43 - INFO - __main__ - Finished.
09/23/2021 14:24:43 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:24:43 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:24:43 - INFO - __main__ - Evaluating to get errors .... Timecode: 42
09/23/2021 14:24:47 - INFO - __main__ - Before Error Fixing: 0.15625
09/23/2021 14:24:47 - INFO - __main__ - Found 27 errors.
09/23/2021 14:24:47 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:24:47 - INFO - __main__ - Current memory size: 1030.
09/23/2021 14:24:47 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:25:10 - INFO - __main__ - before_losses=[tensor(1.5295), tensor(1.5029), tensor(1.5024), tensor(1.5828), tensor(1.5126), tensor(1.5658), tensor(1.4934), tensor(1.5439), tensor(1.6964), tensor(1.5495), tensor(1.5714), tensor(1.5579), tensor(1.5118), tensor(1.4958), tensor(1.4752), tensor(1.4718), tensor(1.4996), tensor(1.5557), tensor(1.5754), tensor(1.4925), tensor(1.5235), tensor(1.4736), tensor(1.6980), tensor(1.7353), tensor(1.5214), tensor(1.4724), tensor(1.5502), tensor(1.4432), tensor(1.5649), tensor(1.5222), tensor(2.2370), tensor(2.0549), tensor(2.2294), tensor(1.5809), tensor(1.5642), tensor(1.5491), tensor(1.5922), tensor(1.5143), tensor(1.4729), tensor(1.5078), tensor(1.4509), tensor(1.5448), tensor(3.2497), tensor(1.5009), tensor(1.4751), tensor(1.4726), tensor(1.5041), tensor(1.4793), tensor(1.4803), tensor(1.5179), tensor(1.4673), tensor(2.4737), tensor(1.5354), tensor(2.6112), tensor(1.4552), tensor(1.9830), tensor(1.5456), tensor(1.5574), tensor(1.5474), tensor(1.5211), tensor(1.5000), tensor(1.5073), tensor(1.5263), tensor(1.5564), tensor(2.2644), tensor(2.2846), tensor(1.6680), tensor(1.6093), tensor(1.5369), tensor(1.9948), tensor(2.3494), tensor(1.5234), tensor(1.5234), tensor(1.4864), tensor(1.4837), tensor(1.5492), tensor(1.4787), tensor(2.0727), tensor(1.5130), tensor(1.5283), tensor(1.7181), tensor(2.2869), tensor(1.4819), tensor(1.5197), tensor(1.5127), tensor(1.6331), tensor(1.6009), tensor(2.2514), tensor(1.7515), tensor(1.4810), tensor(1.5385), tensor(1.9908), tensor(1.4689), tensor(1.5707), tensor(1.4936), tensor(1.8207), tensor(1.7363), tensor(2.1196), tensor(1.5226), tensor(1.4798), tensor(1.5585), tensor(1.5019), tensor(1.4597), tensor(1.5223), tensor(1.5749), tensor(1.4922), tensor(1.4819), tensor(1.5017), tensor(2.3305), tensor(1.5089), tensor(1.5204), tensor(1.5435), tensor(1.5206), tensor(1.4913), tensor(1.5171), tensor(1.8270), tensor(2.8462), tensor(3.6936), tensor(2.0734), tensor(1.8438), tensor(1.5302), tensor(1.6204), tensor(1.4890), tensor(1.5201), tensor(1.4759), tensor(1.6009), tensor(1.5156), tensor(1.6083), tensor(1.5127), tensor(2.6777), tensor(1.4985), tensor(1.4694), tensor(1.4798), tensor(1.4717), tensor(1.5508), tensor(1.4670), tensor(1.4924), tensor(1.5661), tensor(1.5279), tensor(1.4868), tensor(1.7861), tensor(1.4994), tensor(1.5242), tensor(1.4630), tensor(1.5071), tensor(1.7519), tensor(1.5575), tensor(1.5516), tensor(1.5912), tensor(1.4878), tensor(1.5507), tensor(1.6699), tensor(1.5206), tensor(1.4713), tensor(1.4713), tensor(1.4875), tensor(1.5369), tensor(1.5372), tensor(1.7700), tensor(1.5197), tensor(1.5168), tensor(1.4870), tensor(1.4958), tensor(1.5977), tensor(1.5011), tensor(1.5385), tensor(1.4690), tensor(1.5438), tensor(1.5179), tensor(1.4888), tensor(1.6451), tensor(1.4987), tensor(1.4808), tensor(1.5291), tensor(1.4802), tensor(1.4779), tensor(2.4317), tensor(1.4699), tensor(1.6192), tensor(1.5303), tensor(1.4542), tensor(1.5235), tensor(1.4976), tensor(1.9101), tensor(1.8941), tensor(1.4773), tensor(1.4763), tensor(2.6852), tensor(1.5130), tensor(1.4668), tensor(1.5036), tensor(1.5280), tensor(1.5094), tensor(1.4570), tensor(1.4835), tensor(1.5274), tensor(1.4725), tensor(1.4630), tensor(3.0678), tensor(1.6097), tensor(1.5184), tensor(2.7600), tensor(1.5051), tensor(1.5054), tensor(1.5049), tensor(1.5139), tensor(1.8387), tensor(1.5228), tensor(1.4841), tensor(1.5404), tensor(1.6209), tensor(1.5571), tensor(1.6602), tensor(1.5130), tensor(1.4919), tensor(1.4651), tensor(1.5568), tensor(1.5334), tensor(1.9405), tensor(1.4740), tensor(1.4758), tensor(1.9143), tensor(1.5624), tensor(1.4662), tensor(1.5341), tensor(1.4935), tensor(1.6863), tensor(1.4847), tensor(2.0546), tensor(1.4663), tensor(1.4861), tensor(1.4856), tensor(1.5203), tensor(1.5802), tensor(1.5485), tensor(1.8146), tensor(1.5288), tensor(1.7370), tensor(1.5600), tensor(2.4126), tensor(1.8451), tensor(1.4954), tensor(1.5034), tensor(1.9207), tensor(1.4768), tensor(1.7601), tensor(1.5771), tensor(1.5295), tensor(1.5305), tensor(1.6035), tensor(1.5338), tensor(1.5728), tensor(1.4928), tensor(1.5214), tensor(1.7880), tensor(1.9524)]
09/23/2021 14:25:10 - INFO - __main__ - after_losses=[tensor(1.5664), tensor(1.5230), tensor(1.4919), tensor(1.5926), tensor(1.5051), tensor(1.5336), tensor(1.4897), tensor(1.5517), tensor(3.6107), tensor(1.5157), tensor(1.5854), tensor(1.7685), tensor(1.5081), tensor(1.4752), tensor(1.4530), tensor(1.4743), tensor(1.4886), tensor(1.5719), tensor(1.6739), tensor(1.4757), tensor(1.6066), tensor(1.4618), tensor(1.6225), tensor(1.6754), tensor(1.5109), tensor(1.4737), tensor(1.5630), tensor(1.4403), tensor(1.5482), tensor(1.5159), tensor(2.3146), tensor(1.6073), tensor(1.8669), tensor(1.5826), tensor(1.5016), tensor(1.5172), tensor(1.5672), tensor(1.5453), tensor(1.4478), tensor(1.4880), tensor(1.4530), tensor(1.5395), tensor(3.2593), tensor(1.4953), tensor(1.4631), tensor(1.4687), tensor(1.5015), tensor(1.4563), tensor(1.4699), tensor(1.5704), tensor(1.4552), tensor(2.5498), tensor(1.5104), tensor(2.4046), tensor(1.4494), tensor(2.1799), tensor(1.5283), tensor(1.5531), tensor(1.5211), tensor(1.5348), tensor(1.5019), tensor(1.5311), tensor(1.4954), tensor(1.5208), tensor(2.0016), tensor(2.4221), tensor(1.5437), tensor(1.6583), tensor(1.5329), tensor(2.0655), tensor(2.0509), tensor(1.4944), tensor(1.5114), tensor(1.4904), tensor(1.4782), tensor(1.5397), tensor(1.4913), tensor(2.1907), tensor(1.5023), tensor(1.5226), tensor(1.5589), tensor(2.2224), tensor(1.4750), tensor(1.5249), tensor(1.5124), tensor(1.8344), tensor(1.5864), tensor(2.1986), tensor(1.7272), tensor(1.4705), tensor(1.5318), tensor(2.0176), tensor(1.4512), tensor(1.5526), tensor(1.4947), tensor(1.8205), tensor(1.6550), tensor(2.1381), tensor(1.5030), tensor(1.4629), tensor(1.5356), tensor(1.4997), tensor(1.4504), tensor(1.5027), tensor(1.5432), tensor(1.4760), tensor(1.4639), tensor(1.4822), tensor(2.2700), tensor(1.4968), tensor(1.5197), tensor(1.5241), tensor(1.5126), tensor(1.4628), tensor(1.4983), tensor(2.2930), tensor(2.5837), tensor(3.7380), tensor(2.0516), tensor(1.9547), tensor(1.6010), tensor(1.7028), tensor(1.4742), tensor(1.5534), tensor(1.4776), tensor(1.5921), tensor(1.4983), tensor(1.5642), tensor(1.5063), tensor(2.9573), tensor(1.4969), tensor(1.4595), tensor(1.4603), tensor(1.4507), tensor(1.5164), tensor(1.4502), tensor(1.4599), tensor(1.5446), tensor(1.5061), tensor(1.4657), tensor(1.7956), tensor(1.5102), tensor(1.5145), tensor(1.4465), tensor(1.4946), tensor(1.6037), tensor(1.5390), tensor(1.5664), tensor(1.5894), tensor(1.4732), tensor(1.5500), tensor(1.6464), tensor(1.5255), tensor(1.4670), tensor(1.4676), tensor(1.4658), tensor(1.5263), tensor(1.5462), tensor(1.6852), tensor(1.5046), tensor(1.5357), tensor(1.4868), tensor(1.4864), tensor(1.6215), tensor(1.4808), tensor(1.5338), tensor(1.4677), tensor(1.5162), tensor(1.5211), tensor(1.4773), tensor(1.5731), tensor(1.5041), tensor(1.4986), tensor(1.5287), tensor(1.4680), tensor(1.4750), tensor(1.5563), tensor(1.4552), tensor(1.5128), tensor(1.5373), tensor(1.4423), tensor(1.4926), tensor(1.5033), tensor(1.8775), tensor(1.8403), tensor(1.4656), tensor(1.4645), tensor(2.6494), tensor(1.5004), tensor(1.4617), tensor(1.4970), tensor(1.5258), tensor(1.5050), tensor(1.4576), tensor(1.4878), tensor(1.5318), tensor(1.4658), tensor(1.4479), tensor(2.7667), tensor(1.6746), tensor(1.4511), tensor(2.5546), tensor(1.4878), tensor(1.6178), tensor(1.4760), tensor(1.4954), tensor(3.5603), tensor(1.5465), tensor(1.4673), tensor(1.5327), tensor(1.6265), tensor(1.5533), tensor(1.6155), tensor(1.4886), tensor(1.4533), tensor(1.4678), tensor(1.5520), tensor(1.5128), tensor(1.9033), tensor(1.4645), tensor(1.4724), tensor(2.1025), tensor(1.5444), tensor(1.4697), tensor(1.5370), tensor(1.4779), tensor(1.5815), tensor(1.4663), tensor(1.9332), tensor(1.4690), tensor(1.4734), tensor(1.4824), tensor(1.4823), tensor(1.5720), tensor(1.5700), tensor(2.4867), tensor(1.5200), tensor(1.7642), tensor(1.7092), tensor(2.2546), tensor(1.6615), tensor(1.4901), tensor(1.4876), tensor(1.9411), tensor(1.4663), tensor(1.7414), tensor(1.5518), tensor(1.5276), tensor(1.5255), tensor(1.5275), tensor(1.5216), tensor(1.6136), tensor(1.4563), tensor(1.5251), tensor(1.6522), tensor(2.0263)]
09/23/2021 14:25:10 - INFO - __main__ - interference_scores=[tensor(0.0369), tensor(0.0201), tensor(-0.0105), tensor(0.0098), tensor(-0.0076), tensor(-0.0322), tensor(-0.0037), tensor(0.0078), tensor(1.9143), tensor(-0.0338), tensor(0.0140), tensor(0.2106), tensor(-0.0037), tensor(-0.0207), tensor(-0.0222), tensor(0.0025), tensor(-0.0111), tensor(0.0162), tensor(0.0984), tensor(-0.0168), tensor(0.0831), tensor(-0.0118), tensor(-0.0755), tensor(-0.0600), tensor(-0.0105), tensor(0.0013), tensor(0.0129), tensor(-0.0029), tensor(-0.0167), tensor(-0.0063), tensor(0.0776), tensor(-0.4477), tensor(-0.3625), tensor(0.0017), tensor(-0.0626), tensor(-0.0319), tensor(-0.0250), tensor(0.0310), tensor(-0.0252), tensor(-0.0198), tensor(0.0021), tensor(-0.0052), tensor(0.0096), tensor(-0.0056), tensor(-0.0120), tensor(-0.0039), tensor(-0.0026), tensor(-0.0229), tensor(-0.0103), tensor(0.0525), tensor(-0.0121), tensor(0.0760), tensor(-0.0250), tensor(-0.2066), tensor(-0.0058), tensor(0.1969), tensor(-0.0173), tensor(-0.0043), tensor(-0.0263), tensor(0.0137), tensor(0.0019), tensor(0.0238), tensor(-0.0309), tensor(-0.0356), tensor(-0.2628), tensor(0.1375), tensor(-0.1243), tensor(0.0490), tensor(-0.0041), tensor(0.0707), tensor(-0.2985), tensor(-0.0289), tensor(-0.0120), tensor(0.0040), tensor(-0.0055), tensor(-0.0095), tensor(0.0126), tensor(0.1180), tensor(-0.0108), tensor(-0.0057), tensor(-0.1593), tensor(-0.0645), tensor(-0.0069), tensor(0.0051), tensor(-0.0003), tensor(0.2013), tensor(-0.0145), tensor(-0.0527), tensor(-0.0243), tensor(-0.0105), tensor(-0.0067), tensor(0.0268), tensor(-0.0177), tensor(-0.0181), tensor(0.0011), tensor(-0.0002), tensor(-0.0813), tensor(0.0186), tensor(-0.0195), tensor(-0.0169), tensor(-0.0229), tensor(-0.0022), tensor(-0.0093), tensor(-0.0196), tensor(-0.0317), tensor(-0.0163), tensor(-0.0180), tensor(-0.0195), tensor(-0.0606), tensor(-0.0121), tensor(-0.0007), tensor(-0.0193), tensor(-0.0080), tensor(-0.0284), tensor(-0.0187), tensor(0.4661), tensor(-0.2625), tensor(0.0444), tensor(-0.0218), tensor(0.1109), tensor(0.0708), tensor(0.0824), tensor(-0.0148), tensor(0.0334), tensor(0.0017), tensor(-0.0088), tensor(-0.0173), tensor(-0.0440), tensor(-0.0064), tensor(0.2796), tensor(-0.0017), tensor(-0.0099), tensor(-0.0196), tensor(-0.0211), tensor(-0.0345), tensor(-0.0168), tensor(-0.0326), tensor(-0.0215), tensor(-0.0218), tensor(-0.0211), tensor(0.0095), tensor(0.0109), tensor(-0.0096), tensor(-0.0165), tensor(-0.0125), tensor(-0.1482), tensor(-0.0185), tensor(0.0148), tensor(-0.0019), tensor(-0.0146), tensor(-0.0007), tensor(-0.0236), tensor(0.0049), tensor(-0.0043), tensor(-0.0037), tensor(-0.0217), tensor(-0.0106), tensor(0.0090), tensor(-0.0848), tensor(-0.0151), tensor(0.0189), tensor(-0.0002), tensor(-0.0094), tensor(0.0238), tensor(-0.0203), tensor(-0.0047), tensor(-0.0013), tensor(-0.0276), tensor(0.0031), tensor(-0.0115), tensor(-0.0720), tensor(0.0054), tensor(0.0179), tensor(-0.0004), tensor(-0.0121), tensor(-0.0029), tensor(-0.8754), tensor(-0.0147), tensor(-0.1064), tensor(0.0070), tensor(-0.0119), tensor(-0.0309), tensor(0.0057), tensor(-0.0326), tensor(-0.0538), tensor(-0.0118), tensor(-0.0118), tensor(-0.0358), tensor(-0.0126), tensor(-0.0051), tensor(-0.0066), tensor(-0.0022), tensor(-0.0044), tensor(0.0006), tensor(0.0043), tensor(0.0044), tensor(-0.0067), tensor(-0.0151), tensor(-0.3011), tensor(0.0648), tensor(-0.0674), tensor(-0.2054), tensor(-0.0173), tensor(0.1124), tensor(-0.0289), tensor(-0.0184), tensor(1.7215), tensor(0.0237), tensor(-0.0168), tensor(-0.0076), tensor(0.0057), tensor(-0.0038), tensor(-0.0447), tensor(-0.0244), tensor(-0.0385), tensor(0.0027), tensor(-0.0048), tensor(-0.0206), tensor(-0.0372), tensor(-0.0095), tensor(-0.0033), tensor(0.1882), tensor(-0.0179), tensor(0.0035), tensor(0.0029), tensor(-0.0157), tensor(-0.1048), tensor(-0.0184), tensor(-0.1214), tensor(0.0027), tensor(-0.0127), tensor(-0.0032), tensor(-0.0380), tensor(-0.0082), tensor(0.0216), tensor(0.6720), tensor(-0.0088), tensor(0.0272), tensor(0.1492), tensor(-0.1580), tensor(-0.1836), tensor(-0.0053), tensor(-0.0157), tensor(0.0204), tensor(-0.0105), tensor(-0.0187), tensor(-0.0253), tensor(-0.0019), tensor(-0.0050), tensor(-0.0760), tensor(-0.0122), tensor(0.0408), tensor(-0.0365), tensor(0.0037), tensor(-0.1358), tensor(0.0739)]
09/23/2021 14:25:10 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-1249', 'mrqa_squad-validation-2966', 'mrqa_triviaqa-validation-751', 'mrqa_triviaqa-validation-1578', 'mrqa_triviaqa-validation-3134', 'mrqa_squad-validation-10428', 'mrqa_squad-validation-8739', 'mrqa_triviaqa-validation-6872', 'mrqa_triviaqa-validation-5996', 'mrqa_naturalquestions-validation-7058', 'mrqa_triviaqa-validation-1995', 'mrqa_squad-validation-1750', 'mrqa_triviaqa-validation-457', 'mrqa_triviaqa-validation-1293', 'mrqa_triviaqa-validation-314', 'mrqa_triviaqa-validation-7156', 'mrqa_squad-validation-4181', 'mrqa_triviaqa-validation-1521', 'mrqa_squad-validation-4637', 'mrqa_triviaqa-validation-7512', 'mrqa_triviaqa-validation-7134', 'mrqa_hotpotqa-validation-2377', 'mrqa_squad-validation-9405', 'mrqa_triviaqa-validation-1764', 'mrqa_triviaqa-validation-5698', 'mrqa_triviaqa-validation-6902', 'mrqa_squad-validation-6328', 'mrqa_naturalquestions-validation-4837', 'mrqa_triviaqa-validation-3595', 'mrqa_naturalquestions-validation-4544', 'mrqa_triviaqa-validation-579', 'mrqa_triviaqa-validation-5810']
09/23/2021 14:25:10 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:25:10 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=59) .... Timecode: 42
09/23/2021 14:25:23 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:25:23 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 42
09/23/2021 14:25:27 - INFO - __main__ - After Error Fixing: 0.84375
09/23/2021 14:25:27 - INFO - __main__ - Instant Fixing Rate: 0.8888888888888888
09/23/2021 14:25:27 - INFO - __main__ - Instant Retention Rate: 0.5999999988
09/23/2021 14:25:29 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_043.pt.
09/23/2021 14:25:29 - INFO - __main__ - Saving the current error examples (len=27) to the memory.
09/23/2021 14:25:29 - INFO - __main__ - Current memory size: 1030.
09/23/2021 14:25:29 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:25:29 - INFO - __main__ - Finished.
09/23/2021 14:25:29 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:25:29 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:25:29 - INFO - __main__ - Evaluating to get errors .... Timecode: 43
09/23/2021 14:25:33 - INFO - __main__ - Before Error Fixing: 0.21875
09/23/2021 14:25:33 - INFO - __main__ - Found 25 errors.
09/23/2021 14:25:33 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:25:33 - INFO - __main__ - Current memory size: 1057.
09/23/2021 14:25:33 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:25:57 - INFO - __main__ - before_losses=[tensor(1.5549), tensor(1.5896), tensor(1.9894), tensor(2.6034), tensor(2.2771), tensor(1.4944), tensor(1.4886), tensor(1.4998), tensor(1.5663), tensor(1.5266), tensor(1.4983), tensor(1.5277), tensor(1.4918), tensor(1.4866), tensor(3.4437), tensor(1.5135), tensor(1.4883), tensor(1.5415), tensor(1.5115), tensor(1.4953), tensor(1.5764), tensor(1.6218), tensor(1.4776), tensor(1.4907), tensor(1.4845), tensor(1.5198), tensor(1.5252), tensor(1.5173), tensor(2.8479), tensor(1.4892), tensor(1.4838), tensor(1.5424), tensor(1.5026), tensor(1.4843), tensor(1.5215), tensor(1.5632), tensor(1.5415), tensor(1.4697), tensor(1.5078), tensor(1.4939), tensor(3.6517), tensor(1.5003), tensor(1.4705), tensor(1.5171), tensor(1.4562), tensor(1.4669), tensor(1.5340), tensor(1.6601), tensor(1.4911), tensor(1.4915), tensor(1.4628), tensor(1.7832), tensor(1.5112), tensor(1.5235), tensor(1.4936), tensor(1.5629), tensor(1.5315), tensor(1.5325), tensor(1.4934), tensor(1.5164), tensor(1.4950), tensor(1.5162), tensor(1.4673), tensor(1.5586), tensor(1.4998), tensor(1.5021), tensor(1.5224), tensor(1.5736), tensor(1.5282), tensor(1.4914), tensor(2.8174), tensor(1.4903), tensor(2.1241), tensor(3.0207), tensor(1.8798), tensor(1.6361), tensor(1.4892), tensor(1.5381), tensor(1.4699), tensor(1.5334), tensor(1.4882), tensor(2.0594), tensor(1.4947), tensor(1.9551), tensor(2.3462), tensor(1.5450), tensor(1.5129), tensor(1.5227), tensor(1.6390), tensor(1.4725), tensor(1.5337), tensor(1.5191), tensor(1.5519), tensor(1.6169), tensor(1.7139), tensor(1.6177), tensor(1.5471), tensor(3.1112), tensor(1.6769), tensor(1.5701), tensor(1.4631), tensor(1.5616), tensor(1.5949), tensor(1.4659), tensor(1.4961), tensor(1.5076), tensor(1.6431), tensor(1.5557), tensor(2.9314), tensor(1.4881), tensor(1.5177), tensor(1.5372), tensor(1.4914), tensor(1.5589), tensor(1.5433), tensor(1.6488), tensor(1.7088), tensor(1.4784), tensor(1.6313), tensor(1.8576), tensor(2.3661), tensor(1.4839), tensor(1.4822), tensor(1.5191), tensor(1.5447), tensor(1.5204), tensor(1.6638), tensor(1.4920), tensor(1.5083), tensor(1.5001), tensor(1.5129), tensor(1.5344), tensor(1.4924), tensor(1.5804), tensor(1.5364), tensor(1.4648), tensor(1.5567), tensor(2.0119), tensor(2.2672), tensor(1.5474), tensor(1.4861), tensor(1.5344), tensor(1.6420), tensor(1.5131), tensor(1.4729), tensor(1.5449), tensor(1.5194), tensor(1.5507), tensor(1.5691), tensor(1.5738), tensor(1.5133), tensor(1.5700), tensor(1.5146), tensor(1.5537), tensor(1.5182), tensor(1.5041), tensor(1.6225), tensor(1.6280), tensor(1.5390), tensor(1.8193), tensor(1.5578), tensor(1.5868), tensor(1.5431), tensor(2.9161), tensor(3.7118), tensor(1.4970), tensor(1.5128), tensor(1.7822), tensor(1.5411), tensor(1.4865), tensor(1.5229), tensor(1.4860), tensor(1.5042), tensor(1.4677), tensor(1.5132), tensor(1.5230), tensor(1.4925), tensor(1.5597), tensor(3.4151), tensor(1.6576), tensor(1.5390), tensor(1.5345), tensor(2.3650), tensor(1.4962), tensor(1.4991), tensor(1.6484), tensor(1.5280), tensor(1.6537), tensor(1.4752), tensor(1.8238), tensor(1.6450), tensor(1.4529), tensor(1.6438), tensor(1.5520), tensor(1.4835), tensor(1.5443), tensor(2.6009), tensor(1.6654), tensor(1.5583), tensor(1.5488), tensor(1.4771), tensor(1.4936), tensor(1.5097), tensor(2.8514), tensor(1.4573), tensor(1.5253), tensor(1.4808), tensor(1.5126), tensor(1.6544), tensor(1.4878), tensor(1.5591), tensor(1.7056), tensor(1.5337), tensor(1.6492), tensor(1.4986), tensor(1.5981), tensor(1.5632), tensor(1.7391), tensor(1.5230), tensor(1.5122), tensor(1.6238), tensor(2.3806), tensor(1.8880), tensor(1.5213), tensor(1.9674), tensor(2.0924), tensor(1.5222), tensor(1.5960), tensor(1.4610), tensor(1.4849), tensor(1.4868), tensor(1.5032), tensor(1.6650), tensor(1.5447), tensor(1.5624), tensor(1.5203), tensor(1.5550), tensor(1.4931), tensor(1.6048), tensor(1.9149), tensor(1.5125), tensor(1.4837), tensor(1.5470), tensor(1.4780), tensor(1.4783), tensor(2.2201), tensor(1.5595), tensor(1.5642), tensor(1.4672), tensor(2.3470), tensor(1.4629), tensor(1.4941), tensor(1.4913), tensor(1.5127), tensor(1.4735), tensor(1.5181)]
09/23/2021 14:25:57 - INFO - __main__ - after_losses=[tensor(1.5553), tensor(1.5525), tensor(2.0084), tensor(1.9397), tensor(2.2423), tensor(1.5100), tensor(1.4545), tensor(1.4991), tensor(1.5545), tensor(1.5325), tensor(1.5088), tensor(1.5059), tensor(1.4754), tensor(1.4767), tensor(3.1966), tensor(1.5531), tensor(1.4713), tensor(1.5136), tensor(1.7303), tensor(1.4957), tensor(1.5596), tensor(1.5293), tensor(1.4633), tensor(1.4915), tensor(1.4828), tensor(1.5031), tensor(1.5044), tensor(1.5143), tensor(2.7790), tensor(1.4800), tensor(1.4780), tensor(1.5161), tensor(1.4983), tensor(1.5318), tensor(1.5508), tensor(1.5440), tensor(1.5243), tensor(1.4648), tensor(1.5992), tensor(1.4803), tensor(3.4160), tensor(1.5063), tensor(1.4640), tensor(1.5141), tensor(1.4510), tensor(1.4511), tensor(1.5062), tensor(1.6658), tensor(1.4856), tensor(1.4930), tensor(1.4431), tensor(1.7526), tensor(1.4955), tensor(1.5143), tensor(1.4800), tensor(1.5355), tensor(1.5328), tensor(1.5087), tensor(1.4784), tensor(1.5120), tensor(1.4796), tensor(1.4984), tensor(1.4651), tensor(1.5529), tensor(1.4844), tensor(1.4871), tensor(1.5100), tensor(1.5250), tensor(1.5206), tensor(1.4866), tensor(2.8305), tensor(1.4961), tensor(2.0775), tensor(1.7262), tensor(1.8687), tensor(1.5731), tensor(1.4879), tensor(1.5127), tensor(1.4548), tensor(1.5155), tensor(1.4650), tensor(2.0207), tensor(1.4891), tensor(1.7820), tensor(2.3635), tensor(1.5459), tensor(1.5009), tensor(1.5140), tensor(1.6160), tensor(1.4689), tensor(1.5358), tensor(1.5064), tensor(1.5431), tensor(1.6124), tensor(1.7028), tensor(1.5963), tensor(1.5438), tensor(2.0991), tensor(1.6739), tensor(1.5526), tensor(1.4541), tensor(1.5132), tensor(1.5672), tensor(1.4538), tensor(1.4933), tensor(1.4853), tensor(1.6203), tensor(1.5411), tensor(2.9523), tensor(1.4853), tensor(1.5080), tensor(1.5163), tensor(1.4719), tensor(1.5590), tensor(1.5285), tensor(1.5941), tensor(1.4773), tensor(1.4718), tensor(1.5676), tensor(1.6871), tensor(2.4980), tensor(1.4721), tensor(1.4705), tensor(1.5061), tensor(1.5436), tensor(1.5036), tensor(1.9423), tensor(1.4734), tensor(1.4897), tensor(1.4898), tensor(1.5065), tensor(1.5373), tensor(1.4864), tensor(1.5650), tensor(1.5490), tensor(1.4585), tensor(1.5174), tensor(2.1150), tensor(2.2410), tensor(1.5391), tensor(1.4649), tensor(1.5279), tensor(1.6507), tensor(1.5025), tensor(1.4756), tensor(1.4927), tensor(1.5164), tensor(1.5386), tensor(1.7549), tensor(1.5141), tensor(1.4960), tensor(1.5595), tensor(1.5075), tensor(1.5468), tensor(1.5099), tensor(1.5083), tensor(1.5843), tensor(1.6098), tensor(1.5469), tensor(1.8644), tensor(1.5163), tensor(1.5077), tensor(1.6199), tensor(2.7904), tensor(3.4327), tensor(1.4934), tensor(1.4962), tensor(1.8754), tensor(1.5294), tensor(1.4857), tensor(1.5157), tensor(1.4745), tensor(1.4953), tensor(1.4531), tensor(1.4951), tensor(1.5195), tensor(1.4875), tensor(1.5424), tensor(3.3397), tensor(1.6174), tensor(1.5154), tensor(1.5360), tensor(2.5074), tensor(1.4884), tensor(1.4936), tensor(1.7052), tensor(1.5118), tensor(1.5888), tensor(1.4728), tensor(1.7734), tensor(1.6041), tensor(1.4432), tensor(1.6179), tensor(1.5253), tensor(1.4954), tensor(1.5604), tensor(2.2779), tensor(1.6129), tensor(1.5454), tensor(1.5287), tensor(1.4721), tensor(1.4803), tensor(1.5016), tensor(3.1997), tensor(1.4732), tensor(1.5195), tensor(1.4714), tensor(1.4996), tensor(1.6524), tensor(1.4978), tensor(1.6403), tensor(1.9485), tensor(1.5025), tensor(1.5607), tensor(1.5057), tensor(1.5824), tensor(1.5535), tensor(1.7547), tensor(1.5216), tensor(1.4703), tensor(1.5361), tensor(2.2849), tensor(1.8219), tensor(1.5088), tensor(2.3272), tensor(1.9420), tensor(1.7477), tensor(1.5703), tensor(1.4604), tensor(1.4630), tensor(1.4707), tensor(1.4922), tensor(1.7722), tensor(1.5861), tensor(1.5417), tensor(1.5092), tensor(1.5512), tensor(1.4934), tensor(1.5406), tensor(1.9177), tensor(1.5032), tensor(1.4732), tensor(1.5096), tensor(1.4863), tensor(1.4707), tensor(2.2437), tensor(1.5534), tensor(1.5359), tensor(1.4601), tensor(2.3971), tensor(1.4657), tensor(1.4796), tensor(1.4960), tensor(1.5035), tensor(1.4571), tensor(1.5076)]
09/23/2021 14:25:57 - INFO - __main__ - interference_scores=[tensor(0.0004), tensor(-0.0371), tensor(0.0190), tensor(-0.6637), tensor(-0.0347), tensor(0.0156), tensor(-0.0341), tensor(-0.0007), tensor(-0.0118), tensor(0.0059), tensor(0.0105), tensor(-0.0218), tensor(-0.0164), tensor(-0.0099), tensor(-0.2471), tensor(0.0396), tensor(-0.0169), tensor(-0.0279), tensor(0.2187), tensor(0.0004), tensor(-0.0168), tensor(-0.0925), tensor(-0.0143), tensor(0.0009), tensor(-0.0017), tensor(-0.0168), tensor(-0.0207), tensor(-0.0030), tensor(-0.0689), tensor(-0.0092), tensor(-0.0058), tensor(-0.0263), tensor(-0.0042), tensor(0.0475), tensor(0.0293), tensor(-0.0192), tensor(-0.0172), tensor(-0.0049), tensor(0.0914), tensor(-0.0135), tensor(-0.2357), tensor(0.0059), tensor(-0.0065), tensor(-0.0030), tensor(-0.0053), tensor(-0.0159), tensor(-0.0278), tensor(0.0057), tensor(-0.0055), tensor(0.0015), tensor(-0.0197), tensor(-0.0306), tensor(-0.0157), tensor(-0.0092), tensor(-0.0136), tensor(-0.0274), tensor(0.0014), tensor(-0.0238), tensor(-0.0150), tensor(-0.0044), tensor(-0.0154), tensor(-0.0179), tensor(-0.0022), tensor(-0.0057), tensor(-0.0154), tensor(-0.0150), tensor(-0.0124), tensor(-0.0486), tensor(-0.0076), tensor(-0.0048), tensor(0.0131), tensor(0.0058), tensor(-0.0466), tensor(-1.2945), tensor(-0.0112), tensor(-0.0630), tensor(-0.0013), tensor(-0.0254), tensor(-0.0151), tensor(-0.0178), tensor(-0.0232), tensor(-0.0387), tensor(-0.0056), tensor(-0.1731), tensor(0.0173), tensor(0.0009), tensor(-0.0119), tensor(-0.0087), tensor(-0.0229), tensor(-0.0036), tensor(0.0021), tensor(-0.0128), tensor(-0.0088), tensor(-0.0044), tensor(-0.0111), tensor(-0.0215), tensor(-0.0033), tensor(-1.0121), tensor(-0.0029), tensor(-0.0175), tensor(-0.0090), tensor(-0.0484), tensor(-0.0277), tensor(-0.0121), tensor(-0.0028), tensor(-0.0224), tensor(-0.0228), tensor(-0.0146), tensor(0.0209), tensor(-0.0028), tensor(-0.0097), tensor(-0.0209), tensor(-0.0195), tensor(9.0599e-05), tensor(-0.0148), tensor(-0.0547), tensor(-0.2315), tensor(-0.0065), tensor(-0.0637), tensor(-0.1706), tensor(0.1318), tensor(-0.0118), tensor(-0.0117), tensor(-0.0130), tensor(-0.0011), tensor(-0.0168), tensor(0.2785), tensor(-0.0185), tensor(-0.0186), tensor(-0.0103), tensor(-0.0064), tensor(0.0029), tensor(-0.0060), tensor(-0.0154), tensor(0.0126), tensor(-0.0063), tensor(-0.0393), tensor(0.1031), tensor(-0.0262), tensor(-0.0083), tensor(-0.0212), tensor(-0.0065), tensor(0.0088), tensor(-0.0106), tensor(0.0027), tensor(-0.0523), tensor(-0.0029), tensor(-0.0120), tensor(0.1858), tensor(-0.0597), tensor(-0.0173), tensor(-0.0105), tensor(-0.0070), tensor(-0.0069), tensor(-0.0083), tensor(0.0041), tensor(-0.0382), tensor(-0.0182), tensor(0.0079), tensor(0.0451), tensor(-0.0414), tensor(-0.0791), tensor(0.0768), tensor(-0.1257), tensor(-0.2791), tensor(-0.0036), tensor(-0.0165), tensor(0.0932), tensor(-0.0116), tensor(-0.0008), tensor(-0.0072), tensor(-0.0114), tensor(-0.0089), tensor(-0.0146), tensor(-0.0180), tensor(-0.0035), tensor(-0.0050), tensor(-0.0172), tensor(-0.0754), tensor(-0.0402), tensor(-0.0236), tensor(0.0015), tensor(0.1424), tensor(-0.0078), tensor(-0.0055), tensor(0.0568), tensor(-0.0161), tensor(-0.0649), tensor(-0.0024), tensor(-0.0504), tensor(-0.0408), tensor(-0.0097), tensor(-0.0259), tensor(-0.0267), tensor(0.0119), tensor(0.0161), tensor(-0.3230), tensor(-0.0525), tensor(-0.0129), tensor(-0.0200), tensor(-0.0050), tensor(-0.0133), tensor(-0.0080), tensor(0.3483), tensor(0.0159), tensor(-0.0058), tensor(-0.0094), tensor(-0.0130), tensor(-0.0020), tensor(0.0100), tensor(0.0812), tensor(0.2429), tensor(-0.0312), tensor(-0.0884), tensor(0.0071), tensor(-0.0157), tensor(-0.0097), tensor(0.0156), tensor(-0.0014), tensor(-0.0419), tensor(-0.0878), tensor(-0.0958), tensor(-0.0661), tensor(-0.0125), tensor(0.3598), tensor(-0.1504), tensor(0.2255), tensor(-0.0258), tensor(-0.0006), tensor(-0.0219), tensor(-0.0161), tensor(-0.0109), tensor(0.1072), tensor(0.0414), tensor(-0.0207), tensor(-0.0111), tensor(-0.0038), tensor(0.0002), tensor(-0.0642), tensor(0.0028), tensor(-0.0093), tensor(-0.0105), tensor(-0.0374), tensor(0.0083), tensor(-0.0076), tensor(0.0236), tensor(-0.0061), tensor(-0.0283), tensor(-0.0071), tensor(0.0501), tensor(0.0028), tensor(-0.0146), tensor(0.0047), tensor(-0.0092), tensor(-0.0163), tensor(-0.0105)]
09/23/2021 14:25:57 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-1521', 'mrqa_squad-validation-5702', 'mrqa_triviaqa-validation-4681', 'mrqa_triviaqa-validation-1995', 'mrqa_hotpotqa-validation-303', 'mrqa_triviaqa-validation-1578', 'mrqa_triviaqa-validation-4966', 'mrqa_squad-validation-814', 'mrqa_squad-validation-6677', 'mrqa_triviaqa-validation-6969', 'mrqa_triviaqa-validation-3716', 'mrqa_squad-validation-4506', 'mrqa_squad-validation-8456', 'mrqa_hotpotqa-validation-4812', 'mrqa_triviaqa-validation-5698', 'mrqa_triviaqa-validation-83', 'mrqa_naturalquestions-validation-3490', 'mrqa_triviaqa-validation-1085', 'mrqa_triviaqa-validation-5937', 'mrqa_triviaqa-validation-2722', 'mrqa_squad-validation-2372', 'mrqa_triviaqa-validation-192', 'mrqa_squad-validation-9063', 'mrqa_squad-validation-3106', 'mrqa_triviaqa-validation-946', 'mrqa_naturalquestions-validation-98', 'mrqa_squad-validation-1994', 'mrqa_hotpotqa-validation-5513', 'mrqa_naturalquestions-validation-8180', 'mrqa_hotpotqa-validation-5762', 'mrqa_triviaqa-validation-2961', 'mrqa_squad-validation-7720']
09/23/2021 14:25:57 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:25:57 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=57) .... Timecode: 43
09/23/2021 14:26:10 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:26:10 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 43
09/23/2021 14:26:14 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 14:26:14 - INFO - __main__ - Instant Fixing Rate: 0.96
09/23/2021 14:26:14 - INFO - __main__ - Instant Retention Rate: 0.7142857132653061
09/23/2021 14:26:16 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_044.pt.
09/23/2021 14:26:16 - INFO - __main__ - Saving the current error examples (len=25) to the memory.
09/23/2021 14:26:16 - INFO - __main__ - Current memory size: 1057.
09/23/2021 14:26:16 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:26:16 - INFO - __main__ - Finished.
09/23/2021 14:26:16 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:26:16 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:26:16 - INFO - __main__ - Evaluating to get errors .... Timecode: 44
09/23/2021 14:26:20 - INFO - __main__ - Before Error Fixing: 0.125
09/23/2021 14:26:20 - INFO - __main__ - Found 28 errors.
09/23/2021 14:26:20 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:26:20 - INFO - __main__ - Current memory size: 1082.
09/23/2021 14:26:20 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:26:42 - INFO - __main__ - before_losses=[tensor(1.5477), tensor(1.5311), tensor(1.4779), tensor(2.0666), tensor(1.5553), tensor(1.4982), tensor(1.5396), tensor(1.6041), tensor(1.7125), tensor(1.4985), tensor(2.0720), tensor(1.4946), tensor(1.5523), tensor(1.5410), tensor(1.5508), tensor(1.4733), tensor(1.5417), tensor(1.4518), tensor(1.4844), tensor(1.4981), tensor(1.4847), tensor(1.5222), tensor(1.5315), tensor(1.5360), tensor(2.9169), tensor(1.5100), tensor(1.5450), tensor(1.8516), tensor(1.5678), tensor(1.5254), tensor(1.6339), tensor(1.5137), tensor(1.5209), tensor(1.4922), tensor(1.5528), tensor(1.4871), tensor(1.4746), tensor(1.5128), tensor(1.6723), tensor(1.5165), tensor(1.5005), tensor(1.5362), tensor(1.5314), tensor(1.5988), tensor(1.5373), tensor(1.5811), tensor(1.5550), tensor(1.7774), tensor(1.5497), tensor(1.5273), tensor(1.4685), tensor(1.5483), tensor(1.4756), tensor(1.5316), tensor(1.5355), tensor(1.6464), tensor(1.5863), tensor(1.5364), tensor(1.4977), tensor(1.7753), tensor(1.8798), tensor(1.5155), tensor(1.5126), tensor(1.5079), tensor(1.5344), tensor(1.4942), tensor(2.3974), tensor(2.2458), tensor(1.4833), tensor(1.4805), tensor(1.8097), tensor(1.4840), tensor(1.5525), tensor(2.3727), tensor(1.5207), tensor(1.7315), tensor(1.5534), tensor(1.5543), tensor(1.4806), tensor(1.5584), tensor(1.5027), tensor(1.5044), tensor(1.5443), tensor(1.5422), tensor(1.4992), tensor(1.5267), tensor(1.5147), tensor(1.5350), tensor(1.4751), tensor(1.5574), tensor(1.5297), tensor(1.4783), tensor(1.5354), tensor(1.5152), tensor(1.5504), tensor(1.5236), tensor(1.7826), tensor(1.5722), tensor(1.5165), tensor(1.5319), tensor(1.4957), tensor(1.5397), tensor(1.5116), tensor(1.8120), tensor(1.5624), tensor(1.5098), tensor(1.5009), tensor(1.6126), tensor(1.5646), tensor(1.4860), tensor(1.5192), tensor(1.5031), tensor(1.4999), tensor(1.5419), tensor(1.5623), tensor(1.8186), tensor(1.4847), tensor(1.4721), tensor(1.4953), tensor(1.5715), tensor(3.0186), tensor(1.6688), tensor(1.4984), tensor(2.1530), tensor(1.7561), tensor(1.6674), tensor(1.5316), tensor(1.4794), tensor(1.5522), tensor(1.5344), tensor(1.7520), tensor(1.5191), tensor(1.5001), tensor(1.5341), tensor(1.5656), tensor(1.4919), tensor(1.6396), tensor(1.5202), tensor(1.8466), tensor(1.5465), tensor(1.5048), tensor(1.4961), tensor(1.5148), tensor(1.5119), tensor(1.9146), tensor(1.4685), tensor(1.5162), tensor(1.7397), tensor(1.4911), tensor(1.4915), tensor(1.5419), tensor(1.5574), tensor(1.5148), tensor(2.2544), tensor(1.5394), tensor(1.4993), tensor(1.9897), tensor(1.5118), tensor(1.5080), tensor(1.5064), tensor(1.5021), tensor(1.4870), tensor(1.5738), tensor(1.8844), tensor(2.5665), tensor(1.5342), tensor(1.5735), tensor(1.5039), tensor(1.6801), tensor(1.5036), tensor(1.5588), tensor(1.6732), tensor(1.5906), tensor(1.4871), tensor(1.4955), tensor(2.1724), tensor(1.4675), tensor(1.6326), tensor(1.7535), tensor(2.0055), tensor(1.4927), tensor(1.6067), tensor(1.5007), tensor(1.5123), tensor(1.5373), tensor(1.4927), tensor(1.5575), tensor(1.5572), tensor(1.4775), tensor(1.7430), tensor(1.5113), tensor(5.4920), tensor(1.5043), tensor(1.6216), tensor(1.7300), tensor(1.5479), tensor(1.4986), tensor(1.5152), tensor(2.4177), tensor(1.5135), tensor(1.5017), tensor(1.5005), tensor(1.5290), tensor(1.6614), tensor(1.5574), tensor(1.4825), tensor(2.3956), tensor(1.5771), tensor(1.8888), tensor(1.8581), tensor(1.5293), tensor(1.5050), tensor(1.5699), tensor(1.5505), tensor(2.1126), tensor(1.5042), tensor(2.1319), tensor(1.5635), tensor(1.5163), tensor(1.5543), tensor(2.1818), tensor(1.5264), tensor(1.5680), tensor(1.4869), tensor(1.5865), tensor(1.5112), tensor(1.5405), tensor(1.4739), tensor(3.6427), tensor(1.7182), tensor(1.5324), tensor(2.1466), tensor(1.8627), tensor(1.5317), tensor(1.5263), tensor(1.5140), tensor(1.5968), tensor(1.5119), tensor(1.5135), tensor(1.5472), tensor(1.6206), tensor(1.5506), tensor(1.5517), tensor(1.5900), tensor(1.5848), tensor(1.5062), tensor(1.5579), tensor(1.5334), tensor(1.4819), tensor(2.2497), tensor(1.5296), tensor(2.1817), tensor(1.5458), tensor(1.4805), tensor(1.5377), tensor(1.5496)]
09/23/2021 14:26:42 - INFO - __main__ - after_losses=[tensor(1.5585), tensor(1.5208), tensor(1.4694), tensor(2.1506), tensor(1.5304), tensor(1.4922), tensor(1.5245), tensor(1.4731), tensor(2.2292), tensor(1.4940), tensor(1.8867), tensor(1.4830), tensor(1.5415), tensor(1.5204), tensor(1.5251), tensor(1.4668), tensor(1.5341), tensor(1.4500), tensor(1.4786), tensor(1.4824), tensor(1.4824), tensor(1.5172), tensor(1.5211), tensor(1.5076), tensor(3.2473), tensor(1.5021), tensor(1.5470), tensor(2.0280), tensor(1.5534), tensor(1.4985), tensor(1.5625), tensor(1.4941), tensor(1.5199), tensor(1.4904), tensor(1.5381), tensor(1.4674), tensor(1.4684), tensor(1.5158), tensor(1.6211), tensor(1.4691), tensor(1.5187), tensor(1.4993), tensor(1.5502), tensor(1.5547), tensor(1.5474), tensor(1.5845), tensor(1.5436), tensor(1.6920), tensor(1.5206), tensor(1.5116), tensor(1.4668), tensor(1.5406), tensor(1.4791), tensor(1.5222), tensor(1.5624), tensor(1.6625), tensor(1.5309), tensor(1.5085), tensor(1.4878), tensor(1.8657), tensor(1.7428), tensor(1.5098), tensor(1.5025), tensor(1.4963), tensor(1.4778), tensor(1.4873), tensor(1.9186), tensor(2.5968), tensor(1.4782), tensor(1.4833), tensor(1.7008), tensor(1.4846), tensor(1.5410), tensor(2.2962), tensor(1.4785), tensor(1.8127), tensor(1.5577), tensor(1.5280), tensor(1.4687), tensor(1.5552), tensor(1.4934), tensor(1.4981), tensor(1.5116), tensor(1.5326), tensor(1.4864), tensor(1.5109), tensor(1.5011), tensor(1.5227), tensor(1.4575), tensor(1.5113), tensor(1.5032), tensor(1.4748), tensor(1.5026), tensor(1.5020), tensor(1.5324), tensor(1.5169), tensor(1.6325), tensor(1.5589), tensor(1.4988), tensor(1.5344), tensor(1.5008), tensor(1.5283), tensor(1.4832), tensor(1.9646), tensor(1.5703), tensor(1.5011), tensor(1.4882), tensor(1.5899), tensor(1.4888), tensor(1.4714), tensor(1.5014), tensor(1.4907), tensor(1.4809), tensor(1.5368), tensor(1.5338), tensor(1.8164), tensor(1.4700), tensor(1.4587), tensor(1.4842), tensor(1.6297), tensor(2.8981), tensor(1.7059), tensor(1.4877), tensor(1.9270), tensor(2.0004), tensor(1.6283), tensor(1.4816), tensor(1.4827), tensor(1.6168), tensor(1.5635), tensor(1.7643), tensor(1.5137), tensor(1.4992), tensor(1.4876), tensor(1.5629), tensor(1.5007), tensor(1.6134), tensor(1.4846), tensor(1.5198), tensor(1.5110), tensor(1.4959), tensor(1.5258), tensor(1.5165), tensor(1.5096), tensor(1.8091), tensor(1.4647), tensor(1.5175), tensor(1.6266), tensor(1.4584), tensor(1.4914), tensor(1.5284), tensor(1.5776), tensor(1.5008), tensor(2.1994), tensor(1.5372), tensor(1.4969), tensor(2.0305), tensor(1.5149), tensor(1.4978), tensor(1.4917), tensor(1.4827), tensor(1.4858), tensor(1.5593), tensor(2.0509), tensor(2.6018), tensor(1.5031), tensor(1.5351), tensor(1.4948), tensor(1.6282), tensor(1.4837), tensor(1.5991), tensor(1.5801), tensor(1.5721), tensor(1.4694), tensor(1.5009), tensor(2.0173), tensor(1.4678), tensor(1.5530), tensor(1.6519), tensor(1.9658), tensor(1.5216), tensor(1.5436), tensor(1.4937), tensor(1.5030), tensor(1.5295), tensor(1.4652), tensor(1.5639), tensor(1.5406), tensor(1.4799), tensor(1.7267), tensor(1.4999), tensor(5.6393), tensor(1.4860), tensor(1.7128), tensor(1.6255), tensor(1.5353), tensor(1.4973), tensor(1.5573), tensor(2.2453), tensor(1.4880), tensor(1.4903), tensor(1.4926), tensor(1.5307), tensor(1.5426), tensor(1.5302), tensor(1.4699), tensor(2.3755), tensor(1.5729), tensor(1.7238), tensor(1.8216), tensor(1.5016), tensor(1.4888), tensor(1.5624), tensor(1.5132), tensor(1.7759), tensor(1.4954), tensor(2.0903), tensor(1.5493), tensor(1.4967), tensor(1.5240), tensor(2.5877), tensor(1.5189), tensor(1.5659), tensor(1.4781), tensor(1.5531), tensor(1.5180), tensor(1.5482), tensor(1.4569), tensor(3.7973), tensor(1.7271), tensor(1.5360), tensor(2.1923), tensor(2.1353), tensor(1.5224), tensor(1.4991), tensor(1.5069), tensor(1.5924), tensor(1.5015), tensor(1.4999), tensor(1.5429), tensor(1.6067), tensor(1.5326), tensor(1.5123), tensor(1.6047), tensor(1.5854), tensor(1.5037), tensor(1.5257), tensor(1.5958), tensor(1.4734), tensor(2.1904), tensor(1.5191), tensor(2.0131), tensor(1.5382), tensor(1.4856), tensor(1.4887), tensor(1.5335)]
09/23/2021 14:26:43 - INFO - __main__ - interference_scores=[tensor(0.0108), tensor(-0.0104), tensor(-0.0085), tensor(0.0840), tensor(-0.0249), tensor(-0.0060), tensor(-0.0151), tensor(-0.1310), tensor(0.5167), tensor(-0.0045), tensor(-0.1853), tensor(-0.0116), tensor(-0.0108), tensor(-0.0206), tensor(-0.0257), tensor(-0.0065), tensor(-0.0076), tensor(-0.0018), tensor(-0.0058), tensor(-0.0157), tensor(-0.0023), tensor(-0.0051), tensor(-0.0104), tensor(-0.0285), tensor(0.3304), tensor(-0.0078), tensor(0.0020), tensor(0.1765), tensor(-0.0144), tensor(-0.0269), tensor(-0.0713), tensor(-0.0196), tensor(-0.0010), tensor(-0.0018), tensor(-0.0146), tensor(-0.0198), tensor(-0.0061), tensor(0.0030), tensor(-0.0513), tensor(-0.0473), tensor(0.0182), tensor(-0.0369), tensor(0.0189), tensor(-0.0441), tensor(0.0101), tensor(0.0034), tensor(-0.0114), tensor(-0.0854), tensor(-0.0291), tensor(-0.0157), tensor(-0.0017), tensor(-0.0077), tensor(0.0035), tensor(-0.0094), tensor(0.0269), tensor(0.0160), tensor(-0.0554), tensor(-0.0279), tensor(-0.0099), tensor(0.0904), tensor(-0.1370), tensor(-0.0057), tensor(-0.0101), tensor(-0.0115), tensor(-0.0566), tensor(-0.0068), tensor(-0.4788), tensor(0.3510), tensor(-0.0051), tensor(0.0028), tensor(-0.1089), tensor(0.0006), tensor(-0.0115), tensor(-0.0765), tensor(-0.0422), tensor(0.0812), tensor(0.0043), tensor(-0.0263), tensor(-0.0119), tensor(-0.0031), tensor(-0.0093), tensor(-0.0063), tensor(-0.0327), tensor(-0.0096), tensor(-0.0127), tensor(-0.0158), tensor(-0.0136), tensor(-0.0123), tensor(-0.0176), tensor(-0.0461), tensor(-0.0266), tensor(-0.0035), tensor(-0.0327), tensor(-0.0132), tensor(-0.0180), tensor(-0.0067), tensor(-0.1501), tensor(-0.0133), tensor(-0.0178), tensor(0.0025), tensor(0.0051), tensor(-0.0114), tensor(-0.0284), tensor(0.1527), tensor(0.0079), tensor(-0.0087), tensor(-0.0127), tensor(-0.0227), tensor(-0.0758), tensor(-0.0146), tensor(-0.0178), tensor(-0.0124), tensor(-0.0190), tensor(-0.0051), tensor(-0.0285), tensor(-0.0021), tensor(-0.0147), tensor(-0.0133), tensor(-0.0111), tensor(0.0583), tensor(-0.1205), tensor(0.0371), tensor(-0.0108), tensor(-0.2260), tensor(0.2442), tensor(-0.0391), tensor(-0.0500), tensor(0.0033), tensor(0.0646), tensor(0.0291), tensor(0.0123), tensor(-0.0054), tensor(-0.0009), tensor(-0.0465), tensor(-0.0027), tensor(0.0088), tensor(-0.0263), tensor(-0.0356), tensor(-0.3269), tensor(-0.0355), tensor(-0.0090), tensor(0.0296), tensor(0.0017), tensor(-0.0023), tensor(-0.1055), tensor(-0.0037), tensor(0.0013), tensor(-0.1131), tensor(-0.0327), tensor(-6.0797e-05), tensor(-0.0135), tensor(0.0202), tensor(-0.0140), tensor(-0.0550), tensor(-0.0023), tensor(-0.0024), tensor(0.0408), tensor(0.0030), tensor(-0.0102), tensor(-0.0147), tensor(-0.0194), tensor(-0.0012), tensor(-0.0145), tensor(0.1665), tensor(0.0353), tensor(-0.0312), tensor(-0.0385), tensor(-0.0091), tensor(-0.0519), tensor(-0.0199), tensor(0.0403), tensor(-0.0931), tensor(-0.0185), tensor(-0.0177), tensor(0.0055), tensor(-0.1551), tensor(0.0003), tensor(-0.0796), tensor(-0.1016), tensor(-0.0396), tensor(0.0289), tensor(-0.0630), tensor(-0.0070), tensor(-0.0093), tensor(-0.0078), tensor(-0.0275), tensor(0.0064), tensor(-0.0166), tensor(0.0024), tensor(-0.0162), tensor(-0.0115), tensor(0.1473), tensor(-0.0183), tensor(0.0912), tensor(-0.1046), tensor(-0.0126), tensor(-0.0012), tensor(0.0421), tensor(-0.1724), tensor(-0.0256), tensor(-0.0114), tensor(-0.0080), tensor(0.0017), tensor(-0.1188), tensor(-0.0272), tensor(-0.0126), tensor(-0.0200), tensor(-0.0042), tensor(-0.1650), tensor(-0.0365), tensor(-0.0277), tensor(-0.0162), tensor(-0.0076), tensor(-0.0373), tensor(-0.3367), tensor(-0.0088), tensor(-0.0416), tensor(-0.0142), tensor(-0.0196), tensor(-0.0303), tensor(0.4059), tensor(-0.0075), tensor(-0.0021), tensor(-0.0088), tensor(-0.0334), tensor(0.0069), tensor(0.0077), tensor(-0.0170), tensor(0.1546), tensor(0.0089), tensor(0.0036), tensor(0.0457), tensor(0.2726), tensor(-0.0093), tensor(-0.0272), tensor(-0.0071), tensor(-0.0044), tensor(-0.0104), tensor(-0.0136), tensor(-0.0043), tensor(-0.0140), tensor(-0.0180), tensor(-0.0394), tensor(0.0147), tensor(0.0006), tensor(-0.0025), tensor(-0.0323), tensor(0.0624), tensor(-0.0085), tensor(-0.0593), tensor(-0.0105), tensor(-0.1686), tensor(-0.0076), tensor(0.0051), tensor(-0.0490), tensor(-0.0161)]
09/23/2021 14:26:43 - INFO - __main__ - retrieved candidates ids = ['mrqa_naturalquestions-validation-4664', 'mrqa_naturalquestions-validation-7641', 'mrqa_triviaqa-validation-6902', 'mrqa_squad-validation-2966', 'mrqa_squad-validation-2249', 'mrqa_squad-validation-6442', 'mrqa_squad-validation-608', 'mrqa_naturalquestions-validation-866', 'mrqa_triviaqa-validation-3803', 'mrqa_squad-validation-5622', 'mrqa_triviaqa-validation-3515', 'mrqa_triviaqa-validation-571', 'mrqa_triviaqa-validation-3134', 'mrqa_triviaqa-validation-5810', 'mrqa_squad-validation-3539', 'mrqa_squad-validation-10502', 'mrqa_triviaqa-validation-866', 'mrqa_naturalquestions-validation-5272', 'mrqa_squad-validation-8464', 'mrqa_naturalquestions-validation-7144', 'mrqa_squad-validation-2885', 'mrqa_triviaqa-validation-6887', 'mrqa_squad-validation-4181', 'mrqa_triviaqa-validation-5496', 'mrqa_triviaqa-validation-1770', 'mrqa_squad-validation-8617', 'mrqa_triviaqa-validation-5936', 'mrqa_naturalquestions-validation-6610', 'mrqa_squad-validation-2660', 'mrqa_naturalquestions-validation-3559', 'mrqa_triviaqa-validation-1575', 'mrqa_triviaqa-validation-2959']
09/23/2021 14:26:43 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:26:43 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=60) .... Timecode: 44
09/23/2021 14:26:56 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:26:56 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 44
09/23/2021 14:27:00 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 14:27:00 - INFO - __main__ - Instant Fixing Rate: 0.9642857142857143
09/23/2021 14:27:00 - INFO - __main__ - Instant Retention Rate: 0.49999999875
09/23/2021 14:27:02 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_045.pt.
09/23/2021 14:27:02 - INFO - __main__ - Saving the current error examples (len=28) to the memory.
09/23/2021 14:27:02 - INFO - __main__ - Current memory size: 1082.
09/23/2021 14:27:02 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:27:02 - INFO - __main__ - Finished.
09/23/2021 14:27:02 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:27:02 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:27:02 - INFO - __main__ - Evaluating to get errors .... Timecode: 45
09/23/2021 14:27:06 - INFO - __main__ - Before Error Fixing: 0.125
09/23/2021 14:27:06 - INFO - __main__ - Found 28 errors.
09/23/2021 14:27:06 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:27:06 - INFO - __main__ - Current memory size: 1082.
09/23/2021 14:27:06 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:27:30 - INFO - __main__ - before_losses=[tensor(1.5433), tensor(2.4030), tensor(1.6183), tensor(1.4999), tensor(1.6053), tensor(1.5757), tensor(1.4916), tensor(1.4932), tensor(1.7269), tensor(1.5226), tensor(1.8350), tensor(1.5653), tensor(1.4913), tensor(1.5943), tensor(1.5204), tensor(1.5375), tensor(1.5005), tensor(1.6951), tensor(1.5267), tensor(1.5008), tensor(2.3504), tensor(1.8004), tensor(1.4938), tensor(1.4857), tensor(1.4963), tensor(1.5053), tensor(1.5436), tensor(1.5426), tensor(1.4803), tensor(3.0720), tensor(1.4672), tensor(1.5586), tensor(1.5350), tensor(1.4992), tensor(1.4891), tensor(1.5298), tensor(1.4624), tensor(1.6305), tensor(3.1089), tensor(1.6847), tensor(1.4959), tensor(3.1288), tensor(1.5653), tensor(1.5451), tensor(1.4678), tensor(1.5182), tensor(1.5070), tensor(1.5672), tensor(1.6087), tensor(1.5535), tensor(1.5274), tensor(1.4826), tensor(1.9946), tensor(1.5178), tensor(1.5046), tensor(1.7564), tensor(1.4912), tensor(1.5310), tensor(1.4938), tensor(1.5065), tensor(1.5012), tensor(2.0130), tensor(1.5107), tensor(1.5294), tensor(1.5149), tensor(1.4882), tensor(1.5337), tensor(1.7185), tensor(1.4783), tensor(2.3246), tensor(1.5143), tensor(1.6220), tensor(1.5101), tensor(1.5454), tensor(1.4444), tensor(1.7595), tensor(1.4918), tensor(1.5271), tensor(1.5739), tensor(1.5962), tensor(1.4945), tensor(1.9279), tensor(2.0070), tensor(1.7013), tensor(1.4764), tensor(1.5034), tensor(1.5942), tensor(3.2709), tensor(2.2531), tensor(1.5278), tensor(1.5443), tensor(1.5143), tensor(1.5672), tensor(1.5311), tensor(1.5909), tensor(1.4756), tensor(1.4771), tensor(1.4657), tensor(1.6028), tensor(1.4829), tensor(1.8749), tensor(2.4244), tensor(1.4700), tensor(1.5062), tensor(1.9265), tensor(1.5129), tensor(1.5015), tensor(1.4618), tensor(1.5260), tensor(1.5402), tensor(1.5135), tensor(1.4931), tensor(1.5284), tensor(1.4805), tensor(1.4783), tensor(1.5615), tensor(1.4929), tensor(1.5773), tensor(1.5523), tensor(1.5125), tensor(1.4901), tensor(2.0489), tensor(1.5236), tensor(1.5173), tensor(1.5198), tensor(1.8725), tensor(1.5524), tensor(1.4500), tensor(1.6454), tensor(1.5218), tensor(1.5140), tensor(3.5930), tensor(1.6216), tensor(1.5417), tensor(1.5495), tensor(1.5904), tensor(1.5251), tensor(1.5677), tensor(1.5055), tensor(1.5150), tensor(1.5518), tensor(1.4885), tensor(1.5313), tensor(1.4736), tensor(1.6942), tensor(1.5719), tensor(1.5555), tensor(1.5357), tensor(1.4776), tensor(1.5372), tensor(1.5179), tensor(1.8849), tensor(1.4910), tensor(1.5082), tensor(1.4880), tensor(1.4610), tensor(1.5599), tensor(1.5688), tensor(1.5440), tensor(1.4708), tensor(1.4840), tensor(1.4792), tensor(1.5569), tensor(1.5847), tensor(1.5458), tensor(2.1958), tensor(1.5522), tensor(2.5794), tensor(1.5063), tensor(1.5387), tensor(3.5896), tensor(1.4742), tensor(1.4643), tensor(1.5690), tensor(1.5224), tensor(1.4820), tensor(1.5358), tensor(2.4547), tensor(1.4911), tensor(1.4643), tensor(1.6036), tensor(1.4850), tensor(1.4756), tensor(1.5236), tensor(1.5177), tensor(4.8849), tensor(1.5421), tensor(1.5563), tensor(1.5204), tensor(1.5822), tensor(1.5381), tensor(1.6045), tensor(1.4711), tensor(1.4737), tensor(1.4953), tensor(1.4843), tensor(1.5252), tensor(1.5447), tensor(1.4885), tensor(1.4742), tensor(1.5269), tensor(1.4950), tensor(1.5064), tensor(1.8531), tensor(1.4848), tensor(2.0624), tensor(1.5103), tensor(1.4815), tensor(1.4889), tensor(1.4819), tensor(1.5196), tensor(1.4721), tensor(2.3412), tensor(1.6073), tensor(1.4747), tensor(1.4916), tensor(1.4713), tensor(1.5430), tensor(1.6001), tensor(1.4768), tensor(1.6573), tensor(2.0026), tensor(1.4838), tensor(1.4998), tensor(1.5701), tensor(1.4942), tensor(1.8736), tensor(1.5199), tensor(1.8988), tensor(1.4869), tensor(1.5266), tensor(2.0098), tensor(1.6786), tensor(1.4769), tensor(1.4991), tensor(2.0029), tensor(1.5192), tensor(1.4969), tensor(1.4712), tensor(1.5107), tensor(1.4909), tensor(1.4797), tensor(1.6510), tensor(1.5239), tensor(1.5311), tensor(1.5182), tensor(1.5016), tensor(1.4938), tensor(1.4885), tensor(1.4629), tensor(1.5353), tensor(1.4857), tensor(1.5028), tensor(2.2408), tensor(1.4513), tensor(1.4862)]
09/23/2021 14:27:30 - INFO - __main__ - after_losses=[tensor(1.5443), tensor(2.4450), tensor(1.6599), tensor(1.4992), tensor(1.6643), tensor(1.5303), tensor(1.4917), tensor(1.5182), tensor(1.7268), tensor(1.5287), tensor(1.8569), tensor(1.5964), tensor(1.5069), tensor(1.6530), tensor(1.5379), tensor(1.5680), tensor(1.4912), tensor(1.7074), tensor(1.5131), tensor(1.5112), tensor(2.3692), tensor(1.7110), tensor(1.4946), tensor(1.4855), tensor(1.5216), tensor(1.4988), tensor(1.5450), tensor(1.5268), tensor(1.4843), tensor(2.5682), tensor(1.5101), tensor(1.4960), tensor(1.5449), tensor(1.5013), tensor(1.4832), tensor(1.5144), tensor(1.4604), tensor(1.8303), tensor(3.0477), tensor(1.6913), tensor(1.4954), tensor(1.4859), tensor(1.5036), tensor(1.5553), tensor(1.4795), tensor(1.5029), tensor(1.5282), tensor(1.5231), tensor(1.7187), tensor(1.5641), tensor(1.5350), tensor(1.4837), tensor(1.9898), tensor(1.4941), tensor(1.5006), tensor(1.7196), tensor(1.4912), tensor(1.7416), tensor(1.4971), tensor(1.4972), tensor(1.5211), tensor(1.7922), tensor(1.5082), tensor(1.5014), tensor(1.5287), tensor(1.4945), tensor(1.5444), tensor(1.7189), tensor(1.4705), tensor(2.0756), tensor(1.5127), tensor(1.6086), tensor(1.4826), tensor(1.5508), tensor(1.4568), tensor(1.8029), tensor(1.4955), tensor(1.5401), tensor(1.5632), tensor(1.5710), tensor(1.4867), tensor(1.9365), tensor(1.6987), tensor(2.1486), tensor(1.4892), tensor(1.4930), tensor(1.5971), tensor(3.2036), tensor(2.1768), tensor(1.5221), tensor(1.5775), tensor(1.5086), tensor(1.5281), tensor(1.5148), tensor(1.6776), tensor(1.4773), tensor(1.4778), tensor(1.4711), tensor(1.5313), tensor(1.4967), tensor(1.8958), tensor(2.2511), tensor(1.4726), tensor(1.5107), tensor(2.0162), tensor(1.5167), tensor(1.4955), tensor(1.4537), tensor(1.5235), tensor(1.5474), tensor(1.4916), tensor(1.5346), tensor(1.5646), tensor(1.4966), tensor(1.4775), tensor(1.5741), tensor(1.4882), tensor(1.6131), tensor(1.5471), tensor(1.4870), tensor(1.5037), tensor(2.1580), tensor(1.5701), tensor(1.5230), tensor(1.5158), tensor(2.0598), tensor(1.5448), tensor(1.4438), tensor(1.7483), tensor(1.5187), tensor(1.5432), tensor(3.6456), tensor(1.6206), tensor(1.5436), tensor(1.5469), tensor(1.5697), tensor(1.5438), tensor(1.5623), tensor(1.5204), tensor(1.5162), tensor(1.5744), tensor(1.4908), tensor(1.5314), tensor(1.4591), tensor(1.6498), tensor(1.5560), tensor(1.5418), tensor(1.5299), tensor(1.4793), tensor(1.5412), tensor(1.5332), tensor(2.0030), tensor(1.4895), tensor(1.5005), tensor(1.4770), tensor(1.4573), tensor(1.5889), tensor(1.5572), tensor(1.5371), tensor(1.4551), tensor(1.4666), tensor(1.4924), tensor(1.5387), tensor(1.5598), tensor(1.5503), tensor(2.2106), tensor(1.5565), tensor(2.5332), tensor(1.5269), tensor(1.5639), tensor(3.3838), tensor(1.4867), tensor(1.4749), tensor(1.5537), tensor(1.5553), tensor(1.5008), tensor(1.5618), tensor(2.2697), tensor(1.4958), tensor(1.4611), tensor(1.5562), tensor(1.4808), tensor(1.4686), tensor(1.5236), tensor(1.5034), tensor(2.3340), tensor(1.5307), tensor(1.5675), tensor(1.5088), tensor(1.6096), tensor(1.5849), tensor(1.6376), tensor(1.4710), tensor(1.4691), tensor(1.5192), tensor(1.5236), tensor(1.5453), tensor(1.4624), tensor(1.4960), tensor(1.4716), tensor(1.5015), tensor(1.4890), tensor(1.5099), tensor(2.3083), tensor(1.4771), tensor(2.1742), tensor(1.5348), tensor(1.4918), tensor(1.5581), tensor(1.4866), tensor(1.5186), tensor(1.4688), tensor(2.3431), tensor(1.6944), tensor(1.4674), tensor(1.5047), tensor(1.4727), tensor(1.5415), tensor(1.6358), tensor(1.4798), tensor(1.6413), tensor(1.9406), tensor(1.4865), tensor(1.5105), tensor(1.6130), tensor(1.5088), tensor(1.8373), tensor(1.5071), tensor(1.9619), tensor(1.5123), tensor(1.5562), tensor(2.3806), tensor(1.6724), tensor(1.4744), tensor(1.5024), tensor(1.9971), tensor(1.5224), tensor(1.4899), tensor(1.4855), tensor(1.5289), tensor(1.4933), tensor(1.4734), tensor(1.6974), tensor(1.5326), tensor(1.5231), tensor(1.5268), tensor(1.5359), tensor(1.4985), tensor(1.5005), tensor(1.4628), tensor(1.5137), tensor(1.4957), tensor(1.7304), tensor(2.3292), tensor(1.4573), tensor(1.4689)]
09/23/2021 14:27:30 - INFO - __main__ - interference_scores=[tensor(0.0010), tensor(0.0420), tensor(0.0416), tensor(-0.0008), tensor(0.0590), tensor(-0.0454), tensor(0.0002), tensor(0.0249), tensor(-6.1035e-05), tensor(0.0061), tensor(0.0219), tensor(0.0312), tensor(0.0156), tensor(0.0588), tensor(0.0175), tensor(0.0305), tensor(-0.0093), tensor(0.0123), tensor(-0.0136), tensor(0.0104), tensor(0.0188), tensor(-0.0895), tensor(0.0008), tensor(-0.0001), tensor(0.0254), tensor(-0.0065), tensor(0.0014), tensor(-0.0157), tensor(0.0040), tensor(-0.5038), tensor(0.0429), tensor(-0.0626), tensor(0.0099), tensor(0.0021), tensor(-0.0058), tensor(-0.0155), tensor(-0.0020), tensor(0.1997), tensor(-0.0612), tensor(0.0066), tensor(-0.0005), tensor(-1.6429), tensor(-0.0617), tensor(0.0102), tensor(0.0117), tensor(-0.0153), tensor(0.0211), tensor(-0.0441), tensor(0.1101), tensor(0.0106), tensor(0.0075), tensor(0.0010), tensor(-0.0048), tensor(-0.0237), tensor(-0.0040), tensor(-0.0369), tensor(-3.3855e-05), tensor(0.2106), tensor(0.0033), tensor(-0.0093), tensor(0.0199), tensor(-0.2208), tensor(-0.0025), tensor(-0.0280), tensor(0.0138), tensor(0.0063), tensor(0.0107), tensor(0.0004), tensor(-0.0078), tensor(-0.2490), tensor(-0.0015), tensor(-0.0134), tensor(-0.0275), tensor(0.0053), tensor(0.0123), tensor(0.0434), tensor(0.0037), tensor(0.0130), tensor(-0.0107), tensor(-0.0253), tensor(-0.0078), tensor(0.0086), tensor(-0.3083), tensor(0.4473), tensor(0.0128), tensor(-0.0104), tensor(0.0029), tensor(-0.0674), tensor(-0.0763), tensor(-0.0057), tensor(0.0332), tensor(-0.0057), tensor(-0.0391), tensor(-0.0163), tensor(0.0867), tensor(0.0017), tensor(0.0007), tensor(0.0054), tensor(-0.0715), tensor(0.0138), tensor(0.0210), tensor(-0.1733), tensor(0.0026), tensor(0.0045), tensor(0.0896), tensor(0.0037), tensor(-0.0060), tensor(-0.0081), tensor(-0.0025), tensor(0.0073), tensor(-0.0218), tensor(0.0414), tensor(0.0362), tensor(0.0161), tensor(-0.0008), tensor(0.0126), tensor(-0.0047), tensor(0.0358), tensor(-0.0052), tensor(-0.0255), tensor(0.0136), tensor(0.1091), tensor(0.0464), tensor(0.0057), tensor(-0.0041), tensor(0.1873), tensor(-0.0075), tensor(-0.0063), tensor(0.1029), tensor(-0.0031), tensor(0.0292), tensor(0.0526), tensor(-0.0010), tensor(0.0020), tensor(-0.0026), tensor(-0.0207), tensor(0.0187), tensor(-0.0053), tensor(0.0148), tensor(0.0012), tensor(0.0226), tensor(0.0024), tensor(9.0122e-05), tensor(-0.0145), tensor(-0.0443), tensor(-0.0159), tensor(-0.0137), tensor(-0.0058), tensor(0.0017), tensor(0.0040), tensor(0.0153), tensor(0.1180), tensor(-0.0015), tensor(-0.0077), tensor(-0.0110), tensor(-0.0037), tensor(0.0290), tensor(-0.0116), tensor(-0.0070), tensor(-0.0156), tensor(-0.0174), tensor(0.0131), tensor(-0.0182), tensor(-0.0250), tensor(0.0045), tensor(0.0148), tensor(0.0043), tensor(-0.0462), tensor(0.0206), tensor(0.0252), tensor(-0.2058), tensor(0.0126), tensor(0.0105), tensor(-0.0153), tensor(0.0328), tensor(0.0188), tensor(0.0260), tensor(-0.1849), tensor(0.0047), tensor(-0.0032), tensor(-0.0473), tensor(-0.0043), tensor(-0.0071), tensor(3.3855e-05), tensor(-0.0144), tensor(-2.5510), tensor(-0.0114), tensor(0.0112), tensor(-0.0116), tensor(0.0275), tensor(0.0468), tensor(0.0331), tensor(-0.0002), tensor(-0.0046), tensor(0.0239), tensor(0.0393), tensor(0.0201), tensor(-0.0823), tensor(0.0075), tensor(-0.0027), tensor(-0.0254), tensor(-0.0059), tensor(0.0035), tensor(0.4552), tensor(-0.0077), tensor(0.1118), tensor(0.0245), tensor(0.0104), tensor(0.0692), tensor(0.0047), tensor(-0.0011), tensor(-0.0032), tensor(0.0019), tensor(0.0871), tensor(-0.0072), tensor(0.0131), tensor(0.0013), tensor(-0.0014), tensor(0.0356), tensor(0.0030), tensor(-0.0161), tensor(-0.0620), tensor(0.0028), tensor(0.0107), tensor(0.0429), tensor(0.0146), tensor(-0.0363), tensor(-0.0128), tensor(0.0631), tensor(0.0254), tensor(0.0296), tensor(0.3707), tensor(-0.0062), tensor(-0.0025), tensor(0.0033), tensor(-0.0058), tensor(0.0032), tensor(-0.0069), tensor(0.0142), tensor(0.0182), tensor(0.0024), tensor(-0.0063), tensor(0.0463), tensor(0.0087), tensor(-0.0081), tensor(0.0087), tensor(0.0343), tensor(0.0046), tensor(0.0120), tensor(-6.2227e-05), tensor(-0.0216), tensor(0.0100), tensor(0.2276), tensor(0.0884), tensor(0.0060), tensor(-0.0173)]
09/23/2021 14:27:30 - INFO - __main__ - retrieved candidates ids = ['mrqa_naturalquestions-validation-7641', 'mrqa_triviaqa-validation-4829', 'mrqa_squad-validation-4506', 'mrqa_hotpotqa-validation-680', 'mrqa_squad-validation-6403', 'mrqa_triviaqa-validation-7100', 'mrqa_triviaqa-validation-866', 'mrqa_triviaqa-validation-6331', 'mrqa_squad-validation-6924', 'mrqa_squad-validation-9036', 'mrqa_naturalquestions-validation-5017', 'mrqa_triviaqa-validation-305', 'mrqa_triviaqa-validation-3066', 'mrqa_naturalquestions-validation-1085', 'mrqa_squad-validation-2629', 'mrqa_triviaqa-validation-1764', 'mrqa_hotpotqa-validation-2936', 'mrqa_squad-validation-6835', 'mrqa_hotpotqa-validation-1142', 'mrqa_squad-validation-7746', 'mrqa_squad-validation-2733', 'mrqa_hotpotqa-validation-62', 'mrqa_naturalquestions-validation-4309', 'mrqa_squad-validation-6248', 'mrqa_squad-validation-10180', 'mrqa_hotpotqa-validation-3241', 'mrqa_naturalquestions-validation-3209', 'mrqa_squad-validation-1003', 'mrqa_triviaqa-validation-7179', 'mrqa_naturalquestions-validation-9218', 'mrqa_squad-validation-772', 'mrqa_squad-validation-7799']
09/23/2021 14:27:30 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:27:30 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=60) .... Timecode: 45
09/23/2021 14:27:43 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:27:43 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 45
09/23/2021 14:27:46 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 14:27:46 - INFO - __main__ - Instant Fixing Rate: 0.8928571428571429
09/23/2021 14:27:46 - INFO - __main__ - Instant Retention Rate: 0.9999999975
09/23/2021 14:27:48 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_046.pt.
09/23/2021 14:27:49 - INFO - __main__ - Saving the current error examples (len=28) to the memory.
09/23/2021 14:27:49 - INFO - __main__ - Current memory size: 1082.
09/23/2021 14:27:49 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:27:49 - INFO - __main__ - Finished.
09/23/2021 14:27:49 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:27:49 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:27:49 - INFO - __main__ - Evaluating to get errors .... Timecode: 46
09/23/2021 14:27:52 - INFO - __main__ - Before Error Fixing: 0.21875
09/23/2021 14:27:52 - INFO - __main__ - Found 25 errors.
09/23/2021 14:27:52 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:27:52 - INFO - __main__ - Current memory size: 1083.
09/23/2021 14:27:52 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:28:15 - INFO - __main__ - before_losses=[tensor(1.4857), tensor(1.5092), tensor(2.1821), tensor(1.5030), tensor(1.4721), tensor(1.4471), tensor(1.5227), tensor(1.5294), tensor(1.4681), tensor(2.4760), tensor(1.8806), tensor(1.5002), tensor(1.5263), tensor(1.5011), tensor(1.5412), tensor(1.4755), tensor(1.4847), tensor(1.5070), tensor(1.4945), tensor(1.5224), tensor(1.4960), tensor(1.4732), tensor(1.4775), tensor(1.7120), tensor(1.4831), tensor(1.5726), tensor(1.4896), tensor(1.6372), tensor(1.4549), tensor(1.4993), tensor(1.4885), tensor(1.6501), tensor(1.4689), tensor(1.4964), tensor(1.5070), tensor(1.4995), tensor(1.6440), tensor(2.1549), tensor(1.5384), tensor(1.4885), tensor(1.5205), tensor(1.5566), tensor(1.5376), tensor(1.5650), tensor(1.4601), tensor(1.4974), tensor(1.4931), tensor(1.7398), tensor(1.4859), tensor(1.5698), tensor(1.5705), tensor(1.5333), tensor(1.4825), tensor(1.4839), tensor(1.4969), tensor(1.5247), tensor(1.4973), tensor(1.5665), tensor(1.5355), tensor(1.6186), tensor(1.7712), tensor(1.5669), tensor(1.4915), tensor(1.5151), tensor(1.5270), tensor(1.5940), tensor(1.4686), tensor(1.7633), tensor(1.4474), tensor(1.6499), tensor(1.4704), tensor(1.4639), tensor(1.7267), tensor(1.5609), tensor(1.5608), tensor(1.4898), tensor(1.6127), tensor(1.5361), tensor(1.6915), tensor(1.7624), tensor(1.5992), tensor(1.4690), tensor(1.4965), tensor(1.4850), tensor(1.4980), tensor(1.4646), tensor(1.5323), tensor(1.4971), tensor(1.4648), tensor(1.5326), tensor(1.5381), tensor(1.4831), tensor(1.5044), tensor(1.4764), tensor(1.4877), tensor(1.9168), tensor(1.5594), tensor(1.7560), tensor(1.5064), tensor(1.5040), tensor(1.5004), tensor(1.9674), tensor(1.7298), tensor(1.4496), tensor(1.4807), tensor(1.5486), tensor(1.5030), tensor(1.5328), tensor(1.5306), tensor(1.4921), tensor(1.5342), tensor(1.4490), tensor(1.6704), tensor(1.4788), tensor(1.4498), tensor(1.4651), tensor(1.6107), tensor(1.5463), tensor(1.5573), tensor(2.0116), tensor(1.7116), tensor(1.5020), tensor(1.5940), tensor(1.4977), tensor(1.8572), tensor(1.6333), tensor(1.4982), tensor(1.4841), tensor(1.9489), tensor(1.5887), tensor(1.5849), tensor(1.5268), tensor(1.5496), tensor(1.4937), tensor(1.4966), tensor(1.5324), tensor(1.7945), tensor(1.5051), tensor(1.4627), tensor(1.6002), tensor(1.5265), tensor(2.2420), tensor(1.6193), tensor(1.4922), tensor(1.4805), tensor(2.5656), tensor(1.4981), tensor(3.6779), tensor(1.4945), tensor(1.5174), tensor(1.5025), tensor(1.7356), tensor(1.4829), tensor(1.5530), tensor(1.5201), tensor(1.5529), tensor(1.4898), tensor(1.4816), tensor(1.4436), tensor(1.5482), tensor(1.6473), tensor(1.4847), tensor(1.5023), tensor(1.5065), tensor(1.5063), tensor(1.4828), tensor(1.5084), tensor(2.6834), tensor(1.9211), tensor(1.5165), tensor(1.4880), tensor(1.4844), tensor(2.4484), tensor(3.2057), tensor(1.5133), tensor(1.5525), tensor(1.8622), tensor(1.5381), tensor(1.5067), tensor(1.4843), tensor(1.5433), tensor(1.5475), tensor(1.5082), tensor(2.2445), tensor(1.5216), tensor(1.5416), tensor(1.5506), tensor(2.4044), tensor(1.6530), tensor(2.4117), tensor(1.5130), tensor(1.6236), tensor(1.4863), tensor(3.2283), tensor(1.4858), tensor(1.6736), tensor(1.4788), tensor(1.5066), tensor(1.5434), tensor(1.4565), tensor(1.9540), tensor(1.5275), tensor(1.9053), tensor(1.4938), tensor(1.5435), tensor(1.8657), tensor(1.4586), tensor(2.1002), tensor(1.4842), tensor(1.4633), tensor(1.4651), tensor(2.2850), tensor(1.4765), tensor(1.5902), tensor(1.5251), tensor(1.6633), tensor(1.4922), tensor(1.4970), tensor(1.5403), tensor(1.4508), tensor(1.6811), tensor(1.5044), tensor(1.8025), tensor(1.5963), tensor(1.4754), tensor(1.5042), tensor(1.5000), tensor(1.6863), tensor(1.5214), tensor(1.4952), tensor(1.6262), tensor(1.4925), tensor(1.4910), tensor(1.4598), tensor(3.0030), tensor(1.4684), tensor(1.5673), tensor(1.8109), tensor(1.5333), tensor(1.5664), tensor(1.5390), tensor(1.5168), tensor(1.5800), tensor(1.5114), tensor(1.5200), tensor(1.4837), tensor(1.4943), tensor(1.7914), tensor(1.5200), tensor(1.5713), tensor(1.6159), tensor(1.4783), tensor(1.5412), tensor(1.4536), tensor(1.4959), tensor(1.8707)]
09/23/2021 14:28:15 - INFO - __main__ - after_losses=[tensor(1.4897), tensor(1.5162), tensor(2.3558), tensor(1.4725), tensor(1.5096), tensor(1.4423), tensor(1.5143), tensor(1.5162), tensor(1.4533), tensor(2.3645), tensor(1.7979), tensor(1.5009), tensor(1.5217), tensor(1.4608), tensor(1.5381), tensor(1.4651), tensor(1.4655), tensor(1.4930), tensor(1.4741), tensor(1.4738), tensor(1.4757), tensor(1.4642), tensor(1.4721), tensor(1.5268), tensor(1.4722), tensor(1.5359), tensor(1.4856), tensor(1.5619), tensor(1.4598), tensor(1.5067), tensor(1.4718), tensor(1.6687), tensor(1.4402), tensor(1.4781), tensor(1.4915), tensor(1.5810), tensor(1.7141), tensor(2.3965), tensor(1.5910), tensor(1.4835), tensor(1.4853), tensor(1.5073), tensor(1.6098), tensor(1.8210), tensor(1.4735), tensor(1.4854), tensor(1.4732), tensor(2.0080), tensor(1.4667), tensor(1.5350), tensor(1.6030), tensor(1.5046), tensor(1.4665), tensor(1.4764), tensor(1.4708), tensor(1.5189), tensor(1.4791), tensor(1.5292), tensor(1.5489), tensor(1.5721), tensor(1.7934), tensor(1.5057), tensor(1.4832), tensor(1.4796), tensor(1.4980), tensor(1.6801), tensor(1.4620), tensor(1.8480), tensor(1.4359), tensor(1.7734), tensor(1.4541), tensor(1.4567), tensor(1.6202), tensor(1.5338), tensor(1.5230), tensor(1.4646), tensor(1.6204), tensor(1.5797), tensor(1.6207), tensor(1.7665), tensor(1.5824), tensor(1.4699), tensor(1.5106), tensor(1.4974), tensor(1.4661), tensor(1.4513), tensor(1.5769), tensor(1.6534), tensor(1.4485), tensor(1.5877), tensor(1.5250), tensor(1.4848), tensor(1.4828), tensor(1.4628), tensor(1.4659), tensor(2.1475), tensor(1.5246), tensor(1.9944), tensor(1.4944), tensor(1.4693), tensor(1.4644), tensor(2.0081), tensor(1.7571), tensor(1.4548), tensor(1.5171), tensor(1.5410), tensor(1.4837), tensor(1.4940), tensor(1.5065), tensor(1.4693), tensor(1.5122), tensor(1.4328), tensor(1.7201), tensor(1.4649), tensor(1.4454), tensor(1.4483), tensor(1.7520), tensor(1.5505), tensor(1.6289), tensor(1.9545), tensor(1.8030), tensor(1.5157), tensor(1.5397), tensor(1.4910), tensor(1.7603), tensor(1.6255), tensor(1.5416), tensor(1.4728), tensor(2.2091), tensor(1.6035), tensor(1.5715), tensor(1.5247), tensor(1.5314), tensor(1.4709), tensor(1.4797), tensor(1.5079), tensor(1.8712), tensor(1.5688), tensor(1.4565), tensor(1.6118), tensor(1.5148), tensor(2.3083), tensor(1.6381), tensor(1.4685), tensor(1.4896), tensor(2.6653), tensor(1.4949), tensor(2.4950), tensor(1.4878), tensor(1.5055), tensor(1.4813), tensor(2.1502), tensor(1.4720), tensor(1.5755), tensor(1.5258), tensor(1.5798), tensor(1.4924), tensor(1.4701), tensor(1.4549), tensor(1.5152), tensor(1.7418), tensor(1.4672), tensor(1.4961), tensor(1.4942), tensor(1.9035), tensor(1.4823), tensor(1.4888), tensor(2.4348), tensor(2.0380), tensor(1.5104), tensor(1.5054), tensor(1.4796), tensor(2.6665), tensor(3.3603), tensor(1.5075), tensor(1.5437), tensor(1.6073), tensor(1.5255), tensor(1.4916), tensor(1.4705), tensor(1.5239), tensor(1.5360), tensor(1.4762), tensor(2.4654), tensor(1.5042), tensor(1.5213), tensor(1.5260), tensor(2.3977), tensor(1.6542), tensor(2.1871), tensor(1.4766), tensor(1.5847), tensor(1.4773), tensor(3.0560), tensor(1.5013), tensor(1.7248), tensor(1.4547), tensor(1.4971), tensor(1.7456), tensor(1.4474), tensor(2.0645), tensor(1.5844), tensor(1.9388), tensor(1.4640), tensor(1.5196), tensor(1.8725), tensor(1.4667), tensor(2.0896), tensor(1.4770), tensor(1.4664), tensor(1.4492), tensor(2.1334), tensor(1.4650), tensor(1.5681), tensor(1.5035), tensor(1.6319), tensor(1.4998), tensor(1.4894), tensor(1.5254), tensor(1.4659), tensor(1.7425), tensor(1.4651), tensor(1.8453), tensor(1.5791), tensor(1.4688), tensor(1.4966), tensor(1.4972), tensor(1.4700), tensor(1.4947), tensor(1.5461), tensor(1.5256), tensor(1.4826), tensor(1.4738), tensor(1.4372), tensor(2.8090), tensor(1.4663), tensor(1.6789), tensor(1.9711), tensor(1.5173), tensor(1.6864), tensor(1.6805), tensor(1.5162), tensor(1.5544), tensor(1.6013), tensor(1.5377), tensor(1.4920), tensor(1.4716), tensor(2.3537), tensor(1.4759), tensor(1.6493), tensor(1.6837), tensor(1.4844), tensor(1.5051), tensor(1.4484), tensor(1.4577), tensor(1.8056)]
09/23/2021 14:28:15 - INFO - __main__ - interference_scores=[tensor(0.0040), tensor(0.0069), tensor(0.1737), tensor(-0.0305), tensor(0.0375), tensor(-0.0048), tensor(-0.0084), tensor(-0.0131), tensor(-0.0148), tensor(-0.1115), tensor(-0.0828), tensor(0.0008), tensor(-0.0045), tensor(-0.0403), tensor(-0.0031), tensor(-0.0104), tensor(-0.0192), tensor(-0.0140), tensor(-0.0205), tensor(-0.0486), tensor(-0.0203), tensor(-0.0090), tensor(-0.0054), tensor(-0.1853), tensor(-0.0108), tensor(-0.0367), tensor(-0.0040), tensor(-0.0753), tensor(0.0049), tensor(0.0074), tensor(-0.0167), tensor(0.0185), tensor(-0.0287), tensor(-0.0183), tensor(-0.0155), tensor(0.0815), tensor(0.0701), tensor(0.2416), tensor(0.0526), tensor(-0.0050), tensor(-0.0352), tensor(-0.0493), tensor(0.0721), tensor(0.2560), tensor(0.0134), tensor(-0.0120), tensor(-0.0199), tensor(0.2683), tensor(-0.0192), tensor(-0.0348), tensor(0.0325), tensor(-0.0287), tensor(-0.0160), tensor(-0.0075), tensor(-0.0261), tensor(-0.0059), tensor(-0.0181), tensor(-0.0373), tensor(0.0134), tensor(-0.0465), tensor(0.0222), tensor(-0.0612), tensor(-0.0083), tensor(-0.0355), tensor(-0.0290), tensor(0.0861), tensor(-0.0066), tensor(0.0847), tensor(-0.0115), tensor(0.1236), tensor(-0.0163), tensor(-0.0071), tensor(-0.1065), tensor(-0.0272), tensor(-0.0377), tensor(-0.0252), tensor(0.0076), tensor(0.0436), tensor(-0.0708), tensor(0.0041), tensor(-0.0168), tensor(0.0009), tensor(0.0142), tensor(0.0124), tensor(-0.0319), tensor(-0.0132), tensor(0.0446), tensor(0.1562), tensor(-0.0163), tensor(0.0551), tensor(-0.0131), tensor(0.0017), tensor(-0.0216), tensor(-0.0135), tensor(-0.0218), tensor(0.2307), tensor(-0.0348), tensor(0.2384), tensor(-0.0120), tensor(-0.0347), tensor(-0.0361), tensor(0.0407), tensor(0.0273), tensor(0.0051), tensor(0.0365), tensor(-0.0076), tensor(-0.0193), tensor(-0.0388), tensor(-0.0241), tensor(-0.0228), tensor(-0.0220), tensor(-0.0162), tensor(0.0497), tensor(-0.0140), tensor(-0.0044), tensor(-0.0168), tensor(0.1413), tensor(0.0042), tensor(0.0716), tensor(-0.0572), tensor(0.0914), tensor(0.0137), tensor(-0.0543), tensor(-0.0068), tensor(-0.0969), tensor(-0.0078), tensor(0.0434), tensor(-0.0113), tensor(0.2601), tensor(0.0148), tensor(-0.0134), tensor(-0.0021), tensor(-0.0182), tensor(-0.0228), tensor(-0.0169), tensor(-0.0245), tensor(0.0767), tensor(0.0637), tensor(-0.0062), tensor(0.0116), tensor(-0.0116), tensor(0.0663), tensor(0.0188), tensor(-0.0237), tensor(0.0091), tensor(0.0997), tensor(-0.0032), tensor(-1.1830), tensor(-0.0067), tensor(-0.0119), tensor(-0.0212), tensor(0.4146), tensor(-0.0109), tensor(0.0225), tensor(0.0057), tensor(0.0269), tensor(0.0026), tensor(-0.0115), tensor(0.0114), tensor(-0.0330), tensor(0.0945), tensor(-0.0174), tensor(-0.0062), tensor(-0.0123), tensor(0.3971), tensor(-0.0005), tensor(-0.0196), tensor(-0.2485), tensor(0.1169), tensor(-0.0061), tensor(0.0174), tensor(-0.0048), tensor(0.2181), tensor(0.1547), tensor(-0.0059), tensor(-0.0088), tensor(-0.2549), tensor(-0.0126), tensor(-0.0150), tensor(-0.0138), tensor(-0.0194), tensor(-0.0115), tensor(-0.0319), tensor(0.2209), tensor(-0.0174), tensor(-0.0203), tensor(-0.0246), tensor(-0.0067), tensor(0.0012), tensor(-0.2246), tensor(-0.0364), tensor(-0.0389), tensor(-0.0090), tensor(-0.1723), tensor(0.0155), tensor(0.0512), tensor(-0.0241), tensor(-0.0094), tensor(0.2023), tensor(-0.0091), tensor(0.1105), tensor(0.0568), tensor(0.0335), tensor(-0.0298), tensor(-0.0239), tensor(0.0068), tensor(0.0081), tensor(-0.0106), tensor(-0.0072), tensor(0.0032), tensor(-0.0159), tensor(-0.1515), tensor(-0.0116), tensor(-0.0221), tensor(-0.0217), tensor(-0.0314), tensor(0.0076), tensor(-0.0077), tensor(-0.0149), tensor(0.0151), tensor(0.0614), tensor(-0.0393), tensor(0.0427), tensor(-0.0172), tensor(-0.0066), tensor(-0.0076), tensor(-0.0028), tensor(-0.2163), tensor(-0.0266), tensor(0.0509), tensor(-0.1007), tensor(-0.0100), tensor(-0.0172), tensor(-0.0226), tensor(-0.1941), tensor(-0.0021), tensor(0.1116), tensor(0.1602), tensor(-0.0160), tensor(0.1201), tensor(0.1415), tensor(-0.0005), tensor(-0.0256), tensor(0.0899), tensor(0.0177), tensor(0.0083), tensor(-0.0227), tensor(0.5623), tensor(-0.0441), tensor(0.0780), tensor(0.0679), tensor(0.0061), tensor(-0.0361), tensor(-0.0052), tensor(-0.0382), tensor(-0.0651)]
09/23/2021 14:28:15 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-7195', 'mrqa_triviaqa-validation-388', 'mrqa_triviaqa-validation-2007', 'mrqa_triviaqa-validation-5788', 'mrqa_triviaqa-validation-2803', 'mrqa_triviaqa-validation-2812', 'mrqa_triviaqa-validation-2961', 'mrqa_naturalquestions-validation-2085', 'mrqa_naturalquestions-validation-5017', 'mrqa_triviaqa-validation-3393', 'mrqa_triviaqa-validation-7482', 'mrqa_triviaqa-validation-2797', 'mrqa_naturalquestions-validation-7641', 'mrqa_triviaqa-validation-3280', 'mrqa_hotpotqa-validation-2680', 'mrqa_squad-validation-2966', 'mrqa_triviaqa-validation-7415', 'mrqa_triviaqa-validation-4383', 'mrqa_triviaqa-validation-6119', 'mrqa_triviaqa-validation-6772', 'mrqa_triviaqa-validation-3074', 'mrqa_triviaqa-validation-6579', 'mrqa_triviaqa-validation-4583', 'mrqa_triviaqa-validation-1079', 'mrqa_triviaqa-validation-2454', 'mrqa_triviaqa-validation-2530', 'mrqa_triviaqa-validation-7156', 'mrqa_triviaqa-validation-5022', 'mrqa_squad-validation-6442', 'mrqa_triviaqa-validation-6385', 'mrqa_triviaqa-validation-3647', 'mrqa_triviaqa-validation-6639']
09/23/2021 14:28:15 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:28:15 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=57) .... Timecode: 46
09/23/2021 14:28:28 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:28:28 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 46
09/23/2021 14:28:31 - INFO - __main__ - After Error Fixing: 0.96875
09/23/2021 14:28:31 - INFO - __main__ - Instant Fixing Rate: 0.96
09/23/2021 14:28:31 - INFO - __main__ - Instant Retention Rate: 0.9999999985714286
09/23/2021 14:28:33 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_047.pt.
09/23/2021 14:28:33 - INFO - __main__ - Saving the current error examples (len=25) to the memory.
09/23/2021 14:28:33 - INFO - __main__ - Current memory size: 1083.
09/23/2021 14:28:33 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:28:33 - INFO - __main__ - Finished.
09/23/2021 14:28:33 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:28:33 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:28:33 - INFO - __main__ - Evaluating to get errors .... Timecode: 47
09/23/2021 14:28:36 - INFO - __main__ - Before Error Fixing: 0.28125
09/23/2021 14:28:36 - INFO - __main__ - Found 23 errors.
09/23/2021 14:28:36 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:28:36 - INFO - __main__ - Current memory size: 1108.
09/23/2021 14:28:36 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:29:00 - INFO - __main__ - before_losses=[tensor(2.0074), tensor(1.5557), tensor(1.4912), tensor(1.4544), tensor(1.4770), tensor(1.4672), tensor(1.5120), tensor(1.4676), tensor(1.5433), tensor(1.5497), tensor(1.5489), tensor(1.4645), tensor(1.6494), tensor(2.0541), tensor(1.4961), tensor(1.6408), tensor(1.5810), tensor(1.5477), tensor(1.8391), tensor(1.4427), tensor(1.8239), tensor(1.4801), tensor(1.4966), tensor(1.4924), tensor(1.4887), tensor(1.5742), tensor(1.5270), tensor(1.4573), tensor(1.5151), tensor(1.5417), tensor(1.8124), tensor(1.4703), tensor(1.5289), tensor(1.5249), tensor(1.4885), tensor(1.4780), tensor(1.5897), tensor(1.5833), tensor(2.1873), tensor(2.3078), tensor(1.5531), tensor(1.4941), tensor(1.4778), tensor(1.5076), tensor(1.4569), tensor(1.5594), tensor(1.5372), tensor(1.7450), tensor(1.5141), tensor(1.4945), tensor(1.4870), tensor(1.5168), tensor(1.6293), tensor(1.5173), tensor(1.4750), tensor(1.5332), tensor(1.5044), tensor(2.2360), tensor(1.5286), tensor(2.3233), tensor(1.4771), tensor(1.4988), tensor(1.6018), tensor(1.4824), tensor(1.4667), tensor(1.5130), tensor(1.5265), tensor(1.5843), tensor(1.5178), tensor(1.5635), tensor(1.5916), tensor(2.4953), tensor(1.5314), tensor(1.4472), tensor(1.5981), tensor(1.4909), tensor(1.5019), tensor(1.4413), tensor(1.5160), tensor(2.5100), tensor(1.5917), tensor(1.5341), tensor(2.2921), tensor(1.7390), tensor(1.7277), tensor(1.4976), tensor(2.0847), tensor(1.8739), tensor(1.5293), tensor(1.4881), tensor(2.7895), tensor(1.5678), tensor(2.7159), tensor(1.4932), tensor(1.5176), tensor(1.6431), tensor(1.4912), tensor(1.4984), tensor(1.5211), tensor(1.5172), tensor(1.5726), tensor(1.4959), tensor(1.5674), tensor(2.0458), tensor(1.4757), tensor(1.6987), tensor(1.4897), tensor(1.5613), tensor(1.6693), tensor(1.4788), tensor(1.5235), tensor(1.7736), tensor(1.5123), tensor(1.4883), tensor(2.5453), tensor(1.5026), tensor(1.5039), tensor(1.5182), tensor(1.5021), tensor(1.6082), tensor(1.5372), tensor(1.4817), tensor(1.5339), tensor(1.6351), tensor(1.9227), tensor(1.5384), tensor(1.5399), tensor(1.5043), tensor(1.4653), tensor(1.5721), tensor(1.8184), tensor(1.4731), tensor(1.4764), tensor(1.6274), tensor(2.0065), tensor(1.5055), tensor(1.6350), tensor(1.5348), tensor(1.4675), tensor(1.4955), tensor(1.5502), tensor(1.5043), tensor(1.5001), tensor(1.5240), tensor(1.5240), tensor(1.4676), tensor(1.5062), tensor(1.4785), tensor(1.4756), tensor(1.5753), tensor(1.6056), tensor(1.5029), tensor(1.8085), tensor(1.4788), tensor(1.5504), tensor(1.4843), tensor(1.5053), tensor(1.5318), tensor(1.4865), tensor(1.5846), tensor(1.5526), tensor(2.9806), tensor(1.5102), tensor(1.4550), tensor(1.5380), tensor(1.4914), tensor(1.5468), tensor(1.5020), tensor(1.4681), tensor(2.3934), tensor(1.4998), tensor(1.5307), tensor(1.5523), tensor(1.4951), tensor(1.8148), tensor(1.5409), tensor(1.5746), tensor(1.4621), tensor(1.5089), tensor(1.4814), tensor(1.4985), tensor(1.5376), tensor(1.4557), tensor(1.5913), tensor(1.4660), tensor(1.5093), tensor(1.5013), tensor(3.0764), tensor(1.5132), tensor(2.4719), tensor(1.5110), tensor(1.4996), tensor(1.5746), tensor(1.5521), tensor(1.5283), tensor(1.5227), tensor(1.4391), tensor(1.5361), tensor(1.5839), tensor(1.4611), tensor(1.4640), tensor(4.1827), tensor(1.4662), tensor(1.5382), tensor(1.5317), tensor(1.4751), tensor(1.5853), tensor(1.5345), tensor(1.9963), tensor(1.4961), tensor(1.5088), tensor(1.5333), tensor(1.5569), tensor(1.5184), tensor(1.4440), tensor(1.5303), tensor(1.4466), tensor(1.5221), tensor(1.5566), tensor(1.5069), tensor(1.4697), tensor(1.5255), tensor(1.5281), tensor(1.4913), tensor(1.5354), tensor(1.4868), tensor(3.6264), tensor(2.2164), tensor(1.5698), tensor(2.3115), tensor(1.4806), tensor(1.5968), tensor(1.5482), tensor(1.5673), tensor(1.5880), tensor(1.5063), tensor(1.4564), tensor(2.2867), tensor(1.4739), tensor(1.4930), tensor(1.5558), tensor(1.5291), tensor(1.5772), tensor(1.5842), tensor(1.4739), tensor(2.1930), tensor(1.5696), tensor(1.5052), tensor(1.4979), tensor(1.4879), tensor(1.4883), tensor(1.4957), tensor(1.5377), tensor(1.5354), tensor(1.5705), tensor(2.2401)]
09/23/2021 14:29:00 - INFO - __main__ - after_losses=[tensor(2.5071), tensor(1.5214), tensor(1.4789), tensor(1.4442), tensor(1.4547), tensor(1.4590), tensor(1.4981), tensor(1.4644), tensor(1.6220), tensor(1.5136), tensor(1.5369), tensor(1.4659), tensor(1.5717), tensor(2.2715), tensor(1.4751), tensor(1.5777), tensor(1.5868), tensor(1.4947), tensor(1.8106), tensor(1.4369), tensor(2.0088), tensor(1.4617), tensor(1.4775), tensor(1.4863), tensor(1.4969), tensor(1.5764), tensor(1.4921), tensor(1.4483), tensor(1.4960), tensor(1.5213), tensor(1.9757), tensor(1.4606), tensor(1.6682), tensor(1.5131), tensor(1.4828), tensor(1.5067), tensor(1.5935), tensor(1.5845), tensor(2.2307), tensor(2.3071), tensor(1.5665), tensor(1.4864), tensor(1.4780), tensor(1.5480), tensor(1.4428), tensor(1.5290), tensor(1.5191), tensor(1.7396), tensor(1.4979), tensor(1.5267), tensor(1.4605), tensor(1.4935), tensor(1.8744), tensor(1.5017), tensor(1.4673), tensor(1.5341), tensor(1.4925), tensor(2.2854), tensor(1.5111), tensor(2.3611), tensor(1.4669), tensor(1.4801), tensor(1.5286), tensor(1.4722), tensor(1.4467), tensor(1.4915), tensor(1.4963), tensor(1.5589), tensor(1.4797), tensor(1.5977), tensor(1.5941), tensor(2.4450), tensor(1.5089), tensor(1.4425), tensor(1.5932), tensor(1.4713), tensor(1.4971), tensor(1.4383), tensor(1.4971), tensor(2.5261), tensor(1.6687), tensor(1.4944), tensor(2.3377), tensor(1.6754), tensor(1.9581), tensor(1.4764), tensor(2.0065), tensor(1.8545), tensor(1.5053), tensor(1.4824), tensor(1.8999), tensor(1.5307), tensor(1.5045), tensor(1.4827), tensor(1.4913), tensor(1.5441), tensor(1.4582), tensor(1.4962), tensor(1.5392), tensor(1.4954), tensor(1.5553), tensor(1.5482), tensor(1.5310), tensor(2.1347), tensor(1.4697), tensor(1.5227), tensor(1.4852), tensor(1.5412), tensor(1.5741), tensor(1.4750), tensor(1.5132), tensor(1.7385), tensor(1.4956), tensor(1.4816), tensor(2.5169), tensor(1.7030), tensor(1.4865), tensor(1.5023), tensor(1.4899), tensor(1.5924), tensor(1.6029), tensor(1.4693), tensor(1.5293), tensor(1.6129), tensor(1.9563), tensor(1.4906), tensor(1.4984), tensor(1.4759), tensor(1.4688), tensor(1.6309), tensor(1.7348), tensor(1.4621), tensor(1.4584), tensor(1.6310), tensor(1.7963), tensor(1.5069), tensor(1.8173), tensor(1.5247), tensor(1.4569), tensor(1.4933), tensor(1.5330), tensor(1.4916), tensor(1.4820), tensor(1.4961), tensor(1.4977), tensor(1.4573), tensor(1.4930), tensor(1.4707), tensor(1.4597), tensor(1.5617), tensor(1.6242), tensor(1.4817), tensor(1.7501), tensor(1.4961), tensor(1.5325), tensor(1.4723), tensor(1.4933), tensor(1.5194), tensor(1.5093), tensor(1.5935), tensor(1.5299), tensor(2.3367), tensor(1.4967), tensor(1.4533), tensor(1.5224), tensor(1.4814), tensor(1.5257), tensor(1.4797), tensor(1.4636), tensor(2.3130), tensor(1.4814), tensor(1.5080), tensor(1.6785), tensor(1.4857), tensor(1.8690), tensor(1.5068), tensor(1.6160), tensor(1.4472), tensor(1.4991), tensor(1.4734), tensor(1.4864), tensor(1.5061), tensor(1.4440), tensor(1.5646), tensor(1.4637), tensor(1.5168), tensor(1.4829), tensor(2.7133), tensor(1.4873), tensor(2.2621), tensor(1.4991), tensor(1.4854), tensor(1.6086), tensor(1.5467), tensor(1.5072), tensor(1.4822), tensor(1.4380), tensor(1.5347), tensor(1.5034), tensor(1.5158), tensor(1.4570), tensor(3.7358), tensor(1.4629), tensor(1.5032), tensor(1.5265), tensor(1.4750), tensor(1.5842), tensor(1.5141), tensor(2.0472), tensor(1.4740), tensor(1.4815), tensor(1.5049), tensor(1.5417), tensor(1.4972), tensor(1.4391), tensor(1.5332), tensor(1.4358), tensor(1.5220), tensor(1.5372), tensor(1.5118), tensor(1.4601), tensor(1.5396), tensor(1.5001), tensor(1.4799), tensor(1.4896), tensor(1.4771), tensor(3.7005), tensor(2.6280), tensor(1.5418), tensor(2.3328), tensor(1.4733), tensor(1.6325), tensor(1.6089), tensor(1.5245), tensor(1.7027), tensor(1.4884), tensor(1.4449), tensor(2.4259), tensor(1.4630), tensor(1.4772), tensor(1.5360), tensor(1.4905), tensor(1.5927), tensor(1.5384), tensor(1.4710), tensor(2.2344), tensor(1.5627), tensor(1.4949), tensor(1.4647), tensor(1.5256), tensor(1.4761), tensor(1.4889), tensor(1.5145), tensor(1.5188), tensor(1.5494), tensor(2.3837)]
09/23/2021 14:29:00 - INFO - __main__ - interference_scores=[tensor(0.4997), tensor(-0.0343), tensor(-0.0123), tensor(-0.0101), tensor(-0.0223), tensor(-0.0082), tensor(-0.0139), tensor(-0.0032), tensor(0.0787), tensor(-0.0361), tensor(-0.0120), tensor(0.0014), tensor(-0.0778), tensor(0.2173), tensor(-0.0210), tensor(-0.0631), tensor(0.0058), tensor(-0.0530), tensor(-0.0285), tensor(-0.0058), tensor(0.1848), tensor(-0.0183), tensor(-0.0191), tensor(-0.0061), tensor(0.0081), tensor(0.0023), tensor(-0.0349), tensor(-0.0090), tensor(-0.0191), tensor(-0.0204), tensor(0.1632), tensor(-0.0097), tensor(0.1393), tensor(-0.0118), tensor(-0.0058), tensor(0.0287), tensor(0.0038), tensor(0.0012), tensor(0.0435), tensor(-0.0007), tensor(0.0135), tensor(-0.0077), tensor(0.0002), tensor(0.0404), tensor(-0.0141), tensor(-0.0304), tensor(-0.0181), tensor(-0.0054), tensor(-0.0162), tensor(0.0323), tensor(-0.0266), tensor(-0.0232), tensor(0.2451), tensor(-0.0155), tensor(-0.0077), tensor(0.0008), tensor(-0.0118), tensor(0.0493), tensor(-0.0175), tensor(0.0377), tensor(-0.0102), tensor(-0.0187), tensor(-0.0732), tensor(-0.0102), tensor(-0.0200), tensor(-0.0215), tensor(-0.0302), tensor(-0.0254), tensor(-0.0381), tensor(0.0342), tensor(0.0025), tensor(-0.0504), tensor(-0.0225), tensor(-0.0048), tensor(-0.0049), tensor(-0.0196), tensor(-0.0048), tensor(-0.0030), tensor(-0.0189), tensor(0.0160), tensor(0.0771), tensor(-0.0397), tensor(0.0456), tensor(-0.0636), tensor(0.2304), tensor(-0.0212), tensor(-0.0781), tensor(-0.0194), tensor(-0.0240), tensor(-0.0057), tensor(-0.8897), tensor(-0.0370), tensor(-1.2114), tensor(-0.0105), tensor(-0.0263), tensor(-0.0990), tensor(-0.0330), tensor(-0.0021), tensor(0.0181), tensor(-0.0218), tensor(-0.0172), tensor(0.0524), tensor(-0.0363), tensor(0.0888), tensor(-0.0060), tensor(-0.1760), tensor(-0.0045), tensor(-0.0201), tensor(-0.0951), tensor(-0.0038), tensor(-0.0103), tensor(-0.0351), tensor(-0.0167), tensor(-0.0067), tensor(-0.0284), tensor(0.2004), tensor(-0.0174), tensor(-0.0159), tensor(-0.0122), tensor(-0.0158), tensor(0.0657), tensor(-0.0124), tensor(-0.0046), tensor(-0.0222), tensor(0.0336), tensor(-0.0478), tensor(-0.0415), tensor(-0.0285), tensor(0.0035), tensor(0.0588), tensor(-0.0836), tensor(-0.0110), tensor(-0.0181), tensor(0.0036), tensor(-0.2102), tensor(0.0014), tensor(0.1822), tensor(-0.0101), tensor(-0.0105), tensor(-0.0021), tensor(-0.0172), tensor(-0.0127), tensor(-0.0181), tensor(-0.0279), tensor(-0.0263), tensor(-0.0103), tensor(-0.0132), tensor(-0.0078), tensor(-0.0159), tensor(-0.0135), tensor(0.0186), tensor(-0.0211), tensor(-0.0584), tensor(0.0173), tensor(-0.0179), tensor(-0.0120), tensor(-0.0120), tensor(-0.0123), tensor(0.0228), tensor(0.0089), tensor(-0.0228), tensor(-0.6439), tensor(-0.0135), tensor(-0.0017), tensor(-0.0156), tensor(-0.0099), tensor(-0.0211), tensor(-0.0223), tensor(-0.0046), tensor(-0.0805), tensor(-0.0184), tensor(-0.0227), tensor(0.1261), tensor(-0.0093), tensor(0.0542), tensor(-0.0341), tensor(0.0415), tensor(-0.0149), tensor(-0.0097), tensor(-0.0080), tensor(-0.0120), tensor(-0.0315), tensor(-0.0117), tensor(-0.0268), tensor(-0.0023), tensor(0.0075), tensor(-0.0185), tensor(-0.3631), tensor(-0.0259), tensor(-0.2098), tensor(-0.0119), tensor(-0.0142), tensor(0.0340), tensor(-0.0054), tensor(-0.0212), tensor(-0.0405), tensor(-0.0011), tensor(-0.0015), tensor(-0.0805), tensor(0.0547), tensor(-0.0070), tensor(-0.4469), tensor(-0.0033), tensor(-0.0350), tensor(-0.0052), tensor(-0.0001), tensor(-0.0011), tensor(-0.0203), tensor(0.0509), tensor(-0.0221), tensor(-0.0273), tensor(-0.0284), tensor(-0.0152), tensor(-0.0212), tensor(-0.0050), tensor(0.0029), tensor(-0.0108), tensor(-0.0001), tensor(-0.0194), tensor(0.0049), tensor(-0.0096), tensor(0.0142), tensor(-0.0279), tensor(-0.0114), tensor(-0.0458), tensor(-0.0096), tensor(0.0741), tensor(0.4116), tensor(-0.0280), tensor(0.0214), tensor(-0.0073), tensor(0.0357), tensor(0.0607), tensor(-0.0428), tensor(0.1147), tensor(-0.0179), tensor(-0.0115), tensor(0.1392), tensor(-0.0108), tensor(-0.0158), tensor(-0.0198), tensor(-0.0386), tensor(0.0155), tensor(-0.0458), tensor(-0.0028), tensor(0.0414), tensor(-0.0069), tensor(-0.0102), tensor(-0.0332), tensor(0.0377), tensor(-0.0122), tensor(-0.0067), tensor(-0.0231), tensor(-0.0166), tensor(-0.0211), tensor(0.1436)]
09/23/2021 14:29:00 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-4197', 'mrqa_triviaqa-validation-6059', 'mrqa_triviaqa-validation-5587', 'mrqa_squad-validation-2249', 'mrqa_triviaqa-validation-3286', 'mrqa_triviaqa-validation-4069', 'mrqa_triviaqa-validation-3915', 'mrqa_triviaqa-validation-6091', 'mrqa_triviaqa-validation-743', 'mrqa_hotpotqa-validation-2971', 'mrqa_triviaqa-validation-2327', 'mrqa_triviaqa-validation-2181', 'mrqa_naturalquestions-validation-866', 'mrqa_triviaqa-validation-2368', 'mrqa_squad-validation-3558', 'mrqa_triviaqa-validation-4560', 'mrqa_squad-validation-9598', 'mrqa_triviaqa-validation-7032', 'mrqa_triviaqa-validation-7744', 'mrqa_hotpotqa-validation-1391', 'mrqa_triviaqa-validation-3066', 'mrqa_hotpotqa-validation-1626', 'mrqa_triviaqa-validation-5839', 'mrqa_triviaqa-validation-4955', 'mrqa_naturalquestions-validation-8728', 'mrqa_naturalquestions-validation-5017', 'mrqa_squad-validation-4506', 'mrqa_squad-validation-2584', 'mrqa_triviaqa-validation-5654', 'mrqa_triviaqa-validation-2551', 'mrqa_hotpotqa-validation-104', 'mrqa_squad-validation-9296']
09/23/2021 14:29:00 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:29:00 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=55) .... Timecode: 47
09/23/2021 14:29:12 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:29:12 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 47
09/23/2021 14:29:16 - INFO - __main__ - After Error Fixing: 0.875
09/23/2021 14:29:16 - INFO - __main__ - Instant Fixing Rate: 1.0
09/23/2021 14:29:16 - INFO - __main__ - Instant Retention Rate: 0.5555555549382716
09/23/2021 14:29:18 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_048.pt.
09/23/2021 14:29:18 - INFO - __main__ - Saving the current error examples (len=23) to the memory.
09/23/2021 14:29:18 - INFO - __main__ - Current memory size: 1108.
09/23/2021 14:29:18 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:29:18 - INFO - __main__ - Finished.
09/23/2021 14:29:18 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:29:18 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:29:18 - INFO - __main__ - Evaluating to get errors .... Timecode: 48
09/23/2021 14:29:22 - INFO - __main__ - Before Error Fixing: 0.21875
09/23/2021 14:29:22 - INFO - __main__ - Found 25 errors.
09/23/2021 14:29:22 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:29:22 - INFO - __main__ - Current memory size: 1108.
09/23/2021 14:29:22 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:29:45 - INFO - __main__ - before_losses=[tensor(1.4955), tensor(1.4784), tensor(1.5921), tensor(1.4614), tensor(1.4645), tensor(1.5412), tensor(1.4884), tensor(1.6579), tensor(1.4694), tensor(2.0135), tensor(1.4947), tensor(1.6301), tensor(1.4816), tensor(1.5140), tensor(1.4981), tensor(1.4741), tensor(1.5333), tensor(1.5260), tensor(1.5244), tensor(1.4786), tensor(1.5189), tensor(1.5015), tensor(1.6212), tensor(1.4665), tensor(2.1078), tensor(1.4692), tensor(1.4939), tensor(1.5718), tensor(2.0899), tensor(1.7007), tensor(1.4738), tensor(1.4596), tensor(1.4786), tensor(1.4841), tensor(1.4823), tensor(1.4758), tensor(1.4504), tensor(1.6011), tensor(1.6002), tensor(2.3202), tensor(1.6964), tensor(1.5817), tensor(1.5047), tensor(1.4937), tensor(1.5790), tensor(1.5158), tensor(1.4676), tensor(1.6415), tensor(1.4893), tensor(1.4738), tensor(2.6330), tensor(1.4799), tensor(1.5080), tensor(2.1557), tensor(1.4568), tensor(1.4985), tensor(1.6432), tensor(1.6518), tensor(1.6774), tensor(1.5354), tensor(1.5393), tensor(1.5777), tensor(1.5045), tensor(2.1081), tensor(1.5429), tensor(2.4932), tensor(1.6756), tensor(1.4933), tensor(1.8069), tensor(1.6199), tensor(1.4994), tensor(1.5002), tensor(1.5534), tensor(1.6093), tensor(1.7375), tensor(1.4674), tensor(1.4835), tensor(1.4970), tensor(1.4660), tensor(1.4860), tensor(1.6679), tensor(1.5801), tensor(1.5323), tensor(1.4963), tensor(1.5547), tensor(1.5653), tensor(1.5022), tensor(1.5189), tensor(1.4966), tensor(1.5267), tensor(1.4981), tensor(1.5822), tensor(1.5675), tensor(1.9745), tensor(1.5546), tensor(1.5010), tensor(1.6261), tensor(1.5459), tensor(2.3680), tensor(1.4908), tensor(1.5306), tensor(1.5443), tensor(1.4856), tensor(1.5067), tensor(1.5015), tensor(1.5454), tensor(1.7718), tensor(1.4542), tensor(1.5450), tensor(1.5546), tensor(1.5207), tensor(1.4849), tensor(1.4880), tensor(1.4952), tensor(1.4922), tensor(1.5019), tensor(1.5097), tensor(2.3592), tensor(1.5793), tensor(1.5134), tensor(1.4728), tensor(1.5500), tensor(1.6343), tensor(1.4498), tensor(1.5566), tensor(1.4882), tensor(1.5430), tensor(1.5332), tensor(1.5729), tensor(1.5023), tensor(2.4792), tensor(1.6103), tensor(1.4639), tensor(1.7441), tensor(2.9677), tensor(2.6872), tensor(1.5554), tensor(1.5017), tensor(1.8997), tensor(1.4924), tensor(1.6050), tensor(2.0316), tensor(2.4530), tensor(1.6017), tensor(1.5190), tensor(1.5255), tensor(1.5207), tensor(1.4491), tensor(2.2699), tensor(1.5073), tensor(2.0190), tensor(1.4919), tensor(1.5117), tensor(1.5057), tensor(1.4953), tensor(1.4592), tensor(1.5115), tensor(1.4885), tensor(1.4602), tensor(2.5381), tensor(1.5224), tensor(1.5695), tensor(1.5074), tensor(1.5449), tensor(1.4769), tensor(1.4838), tensor(2.0137), tensor(1.4561), tensor(1.4655), tensor(1.5787), tensor(1.4660), tensor(1.5173), tensor(1.6059), tensor(1.4958), tensor(1.5852), tensor(1.4883), tensor(1.5798), tensor(1.4942), tensor(1.5550), tensor(1.5232), tensor(1.4980), tensor(1.5076), tensor(1.5314), tensor(1.5064), tensor(1.4656), tensor(1.4905), tensor(1.5386), tensor(2.2227), tensor(1.5265), tensor(1.6086), tensor(1.5175), tensor(1.5327), tensor(1.5496), tensor(1.5357), tensor(2.2695), tensor(1.8189), tensor(2.2626), tensor(1.4491), tensor(1.5146), tensor(1.6127), tensor(1.4996), tensor(1.4941), tensor(1.4778), tensor(1.5568), tensor(1.5088), tensor(2.3236), tensor(1.4965), tensor(1.4942), tensor(1.4818), tensor(1.4853), tensor(1.5156), tensor(1.4462), tensor(1.5058), tensor(1.9671), tensor(1.6524), tensor(1.5532), tensor(2.0318), tensor(1.4773), tensor(1.8486), tensor(1.4976), tensor(1.5442), tensor(1.5449), tensor(1.5091), tensor(1.4997), tensor(1.5363), tensor(1.6175), tensor(1.5049), tensor(1.7153), tensor(1.5051), tensor(1.5992), tensor(1.4805), tensor(1.5467), tensor(1.5161), tensor(1.5720), tensor(1.4992), tensor(1.5734), tensor(1.5395), tensor(1.4898), tensor(1.5005), tensor(1.4708), tensor(1.5392), tensor(1.4650), tensor(1.5335), tensor(2.3754), tensor(1.6538), tensor(1.4603), tensor(1.5306), tensor(1.5491), tensor(1.5058), tensor(1.4902), tensor(1.4999), tensor(1.6195), tensor(1.4985), tensor(1.4906), tensor(1.5228), tensor(1.5669)]
09/23/2021 14:29:45 - INFO - __main__ - after_losses=[tensor(1.4533), tensor(1.4548), tensor(1.7512), tensor(1.4584), tensor(1.4538), tensor(1.5635), tensor(1.4657), tensor(1.5917), tensor(1.4617), tensor(1.9431), tensor(1.4675), tensor(1.7041), tensor(1.4790), tensor(1.4870), tensor(1.4803), tensor(1.4635), tensor(1.4901), tensor(1.5231), tensor(1.5022), tensor(1.4756), tensor(1.4876), tensor(1.4802), tensor(2.1986), tensor(1.4561), tensor(1.8609), tensor(1.4722), tensor(1.4673), tensor(1.5437), tensor(2.2144), tensor(1.7131), tensor(1.4717), tensor(1.4547), tensor(1.4684), tensor(1.4751), tensor(1.4535), tensor(1.4566), tensor(1.4474), tensor(1.5625), tensor(1.5324), tensor(2.0996), tensor(1.9674), tensor(1.5866), tensor(1.4970), tensor(1.4834), tensor(1.5319), tensor(1.4587), tensor(1.4730), tensor(1.7064), tensor(1.4774), tensor(1.4540), tensor(1.4913), tensor(1.4621), tensor(1.5995), tensor(2.4030), tensor(1.4518), tensor(1.4850), tensor(1.7469), tensor(1.5977), tensor(1.9243), tensor(1.5160), tensor(1.5449), tensor(1.5782), tensor(1.4865), tensor(2.5051), tensor(1.5217), tensor(2.6907), tensor(1.5632), tensor(1.4499), tensor(1.7459), tensor(1.5349), tensor(1.4844), tensor(1.4847), tensor(1.6379), tensor(1.6411), tensor(1.5547), tensor(1.4619), tensor(1.4952), tensor(1.4812), tensor(1.5174), tensor(1.4734), tensor(1.6485), tensor(1.6504), tensor(1.5042), tensor(1.4862), tensor(1.5386), tensor(1.5484), tensor(1.4990), tensor(1.4936), tensor(1.5131), tensor(1.4972), tensor(1.4963), tensor(1.5187), tensor(1.5401), tensor(1.9934), tensor(1.5007), tensor(1.4908), tensor(1.5669), tensor(1.5047), tensor(2.6958), tensor(1.4767), tensor(1.5246), tensor(1.4878), tensor(1.4931), tensor(1.4812), tensor(1.4846), tensor(1.5431), tensor(1.6950), tensor(1.4470), tensor(1.5183), tensor(1.5026), tensor(1.4825), tensor(1.5409), tensor(1.4732), tensor(1.4746), tensor(1.5088), tensor(1.5247), tensor(1.4892), tensor(2.1694), tensor(1.5467), tensor(1.4736), tensor(1.4640), tensor(1.5446), tensor(1.9099), tensor(1.4412), tensor(1.5409), tensor(1.4827), tensor(1.5573), tensor(1.5200), tensor(1.5481), tensor(1.4893), tensor(2.7217), tensor(1.5371), tensor(1.4642), tensor(1.6443), tensor(2.7031), tensor(2.4106), tensor(1.7080), tensor(1.4766), tensor(1.6715), tensor(1.5033), tensor(1.5650), tensor(2.2059), tensor(2.5348), tensor(1.7074), tensor(1.5014), tensor(1.5040), tensor(1.5502), tensor(1.4921), tensor(2.1907), tensor(1.4826), tensor(1.9507), tensor(1.4654), tensor(1.5859), tensor(1.4808), tensor(1.4891), tensor(1.4465), tensor(1.4907), tensor(1.4707), tensor(1.4745), tensor(2.5130), tensor(1.4992), tensor(1.5262), tensor(1.4893), tensor(1.5266), tensor(1.4597), tensor(1.5150), tensor(2.0188), tensor(1.4792), tensor(1.4622), tensor(1.5758), tensor(1.5538), tensor(1.5178), tensor(1.5828), tensor(1.4755), tensor(1.5427), tensor(1.4744), tensor(1.5757), tensor(1.4809), tensor(1.5162), tensor(1.4727), tensor(1.4733), tensor(1.4979), tensor(1.5159), tensor(1.4780), tensor(1.4583), tensor(1.4757), tensor(1.5082), tensor(2.1874), tensor(1.4971), tensor(1.7796), tensor(1.5128), tensor(1.5041), tensor(1.5599), tensor(1.5082), tensor(2.4006), tensor(1.7609), tensor(2.3201), tensor(1.4628), tensor(1.4786), tensor(1.5670), tensor(1.6144), tensor(1.4836), tensor(1.4629), tensor(1.5383), tensor(1.5077), tensor(1.5316), tensor(1.4752), tensor(1.4687), tensor(1.4751), tensor(1.5103), tensor(1.5009), tensor(1.4470), tensor(1.5141), tensor(1.4933), tensor(1.7405), tensor(1.5597), tensor(1.9172), tensor(1.4686), tensor(2.0058), tensor(1.4778), tensor(1.5127), tensor(1.5551), tensor(1.4706), tensor(1.4807), tensor(1.5073), tensor(1.5861), tensor(1.4925), tensor(2.0697), tensor(1.4951), tensor(1.5026), tensor(1.4573), tensor(1.6064), tensor(1.7039), tensor(1.5407), tensor(1.4808), tensor(1.5429), tensor(1.5617), tensor(1.4625), tensor(1.4718), tensor(1.4535), tensor(1.5943), tensor(1.4654), tensor(1.5155), tensor(2.4280), tensor(1.6263), tensor(1.4433), tensor(1.5042), tensor(1.5319), tensor(1.5041), tensor(1.5359), tensor(1.5018), tensor(1.7844), tensor(1.4890), tensor(1.4760), tensor(1.5216), tensor(1.5276)]
09/23/2021 14:29:45 - INFO - __main__ - interference_scores=[tensor(-0.0422), tensor(-0.0236), tensor(0.1591), tensor(-0.0030), tensor(-0.0107), tensor(0.0223), tensor(-0.0227), tensor(-0.0661), tensor(-0.0077), tensor(-0.0704), tensor(-0.0272), tensor(0.0740), tensor(-0.0026), tensor(-0.0269), tensor(-0.0178), tensor(-0.0106), tensor(-0.0432), tensor(-0.0029), tensor(-0.0222), tensor(-0.0030), tensor(-0.0313), tensor(-0.0213), tensor(0.5775), tensor(-0.0104), tensor(-0.2469), tensor(0.0030), tensor(-0.0266), tensor(-0.0282), tensor(0.1245), tensor(0.0124), tensor(-0.0021), tensor(-0.0049), tensor(-0.0102), tensor(-0.0090), tensor(-0.0289), tensor(-0.0193), tensor(-0.0030), tensor(-0.0385), tensor(-0.0677), tensor(-0.2207), tensor(0.2710), tensor(0.0049), tensor(-0.0077), tensor(-0.0104), tensor(-0.0472), tensor(-0.0571), tensor(0.0053), tensor(0.0649), tensor(-0.0119), tensor(-0.0198), tensor(-1.1417), tensor(-0.0179), tensor(0.0915), tensor(0.2473), tensor(-0.0050), tensor(-0.0135), tensor(0.1037), tensor(-0.0541), tensor(0.2468), tensor(-0.0194), tensor(0.0056), tensor(0.0005), tensor(-0.0180), tensor(0.3970), tensor(-0.0212), tensor(0.1974), tensor(-0.1124), tensor(-0.0434), tensor(-0.0610), tensor(-0.0849), tensor(-0.0150), tensor(-0.0155), tensor(0.0846), tensor(0.0318), tensor(-0.1829), tensor(-0.0055), tensor(0.0117), tensor(-0.0159), tensor(0.0514), tensor(-0.0126), tensor(-0.0194), tensor(0.0703), tensor(-0.0281), tensor(-0.0101), tensor(-0.0161), tensor(-0.0169), tensor(-0.0031), tensor(-0.0253), tensor(0.0165), tensor(-0.0295), tensor(-0.0018), tensor(-0.0636), tensor(-0.0274), tensor(0.0189), tensor(-0.0539), tensor(-0.0102), tensor(-0.0592), tensor(-0.0411), tensor(0.3278), tensor(-0.0141), tensor(-0.0060), tensor(-0.0565), tensor(0.0074), tensor(-0.0255), tensor(-0.0169), tensor(-0.0023), tensor(-0.0767), tensor(-0.0072), tensor(-0.0267), tensor(-0.0521), tensor(-0.0382), tensor(0.0560), tensor(-0.0148), tensor(-0.0207), tensor(0.0166), tensor(0.0227), tensor(-0.0205), tensor(-0.1898), tensor(-0.0326), tensor(-0.0398), tensor(-0.0088), tensor(-0.0054), tensor(0.2756), tensor(-0.0085), tensor(-0.0157), tensor(-0.0056), tensor(0.0144), tensor(-0.0132), tensor(-0.0248), tensor(-0.0131), tensor(0.2425), tensor(-0.0732), tensor(0.0003), tensor(-0.0998), tensor(-0.2646), tensor(-0.2765), tensor(0.1526), tensor(-0.0252), tensor(-0.2282), tensor(0.0109), tensor(-0.0400), tensor(0.1743), tensor(0.0817), tensor(0.1057), tensor(-0.0176), tensor(-0.0215), tensor(0.0295), tensor(0.0430), tensor(-0.0792), tensor(-0.0248), tensor(-0.0683), tensor(-0.0265), tensor(0.0741), tensor(-0.0249), tensor(-0.0062), tensor(-0.0126), tensor(-0.0208), tensor(-0.0178), tensor(0.0143), tensor(-0.0251), tensor(-0.0232), tensor(-0.0433), tensor(-0.0181), tensor(-0.0183), tensor(-0.0172), tensor(0.0312), tensor(0.0051), tensor(0.0231), tensor(-0.0033), tensor(-0.0029), tensor(0.0878), tensor(0.0006), tensor(-0.0231), tensor(-0.0203), tensor(-0.0425), tensor(-0.0139), tensor(-0.0041), tensor(-0.0133), tensor(-0.0388), tensor(-0.0504), tensor(-0.0248), tensor(-0.0097), tensor(-0.0154), tensor(-0.0284), tensor(-0.0073), tensor(-0.0147), tensor(-0.0304), tensor(-0.0353), tensor(-0.0294), tensor(0.1709), tensor(-0.0046), tensor(-0.0286), tensor(0.0103), tensor(-0.0275), tensor(0.1312), tensor(-0.0579), tensor(0.0575), tensor(0.0137), tensor(-0.0360), tensor(-0.0457), tensor(0.1148), tensor(-0.0105), tensor(-0.0149), tensor(-0.0185), tensor(-0.0010), tensor(-0.7921), tensor(-0.0214), tensor(-0.0255), tensor(-0.0068), tensor(0.0250), tensor(-0.0148), tensor(0.0008), tensor(0.0083), tensor(-0.4738), tensor(0.0881), tensor(0.0065), tensor(-0.1146), tensor(-0.0086), tensor(0.1572), tensor(-0.0198), tensor(-0.0314), tensor(0.0102), tensor(-0.0385), tensor(-0.0191), tensor(-0.0289), tensor(-0.0314), tensor(-0.0123), tensor(0.3543), tensor(-0.0100), tensor(-0.0966), tensor(-0.0232), tensor(0.0597), tensor(0.1878), tensor(-0.0313), tensor(-0.0184), tensor(-0.0305), tensor(0.0222), tensor(-0.0272), tensor(-0.0287), tensor(-0.0174), tensor(0.0551), tensor(0.0004), tensor(-0.0180), tensor(0.0526), tensor(-0.0275), tensor(-0.0171), tensor(-0.0265), tensor(-0.0172), tensor(-0.0016), tensor(0.0457), tensor(0.0019), tensor(0.1649), tensor(-0.0095), tensor(-0.0146), tensor(-0.0012), tensor(-0.0392)]
09/23/2021 14:29:45 - INFO - __main__ - retrieved candidates ids = ['mrqa_squad-validation-6287', 'mrqa_triviaqa-validation-1975', 'mrqa_naturalquestions-validation-8728', 'mrqa_triviaqa-validation-2873', 'mrqa_triviaqa-validation-2952', 'mrqa_triviaqa-validation-5160', 'mrqa_squad-validation-6737', 'mrqa_triviaqa-validation-5071', 'mrqa_squad-validation-2966', 'mrqa_triviaqa-validation-7538', 'mrqa_triviaqa-validation-6499', 'mrqa_triviaqa-validation-4731', 'mrqa_triviaqa-validation-2890', 'mrqa_triviaqa-validation-1353', 'mrqa_triviaqa-validation-1079', 'mrqa_triviaqa-validation-2368', 'mrqa_triviaqa-validation-3617', 'mrqa_triviaqa-validation-3803', 'mrqa_squad-validation-3539', 'mrqa_triviaqa-validation-671', 'mrqa_triviaqa-validation-5746', 'mrqa_triviaqa-validation-6639', 'mrqa_squad-validation-9984', 'mrqa_triviaqa-validation-6935', 'mrqa_squad-validation-4108', 'mrqa_triviaqa-validation-6579', 'mrqa_naturalquestions-validation-1282', 'mrqa_squad-validation-6878', 'mrqa_hotpotqa-validation-3618', 'mrqa_hotpotqa-validation-3497', 'mrqa_squad-validation-677', 'mrqa_triviaqa-validation-1280']
09/23/2021 14:29:45 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:29:45 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=57) .... Timecode: 48
09/23/2021 14:29:58 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:29:58 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 48
09/23/2021 14:30:02 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 14:30:02 - INFO - __main__ - Instant Fixing Rate: 0.96
09/23/2021 14:30:02 - INFO - __main__ - Instant Retention Rate: 0.7142857132653061
09/23/2021 14:30:04 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_049.pt.
09/23/2021 14:30:04 - INFO - __main__ - Saving the current error examples (len=25) to the memory.
09/23/2021 14:30:04 - INFO - __main__ - Current memory size: 1108.
09/23/2021 14:30:04 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:30:04 - INFO - __main__ - Finished.
09/23/2021 14:30:04 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:30:04 - INFO - __main__ - Not doing replay-based eval.
09/23/2021 14:30:04 - INFO - __main__ - Evaluating to get errors .... Timecode: 49
09/23/2021 14:30:08 - INFO - __main__ - Before Error Fixing: 0.09375
09/23/2021 14:30:08 - INFO - __main__ - Found 29 errors.
09/23/2021 14:30:08 - INFO - __main__ - Triggering Sampling from Memory and starting to replay.
09/23/2021 14:30:08 - INFO - __main__ - Current memory size: 1133.
09/23/2021 14:30:08 - INFO - __main__ - get_top_interfered_examples: len(candidate_examples)=256;
09/23/2021 14:30:31 - INFO - __main__ - before_losses=[tensor(1.6489), tensor(1.4807), tensor(1.5308), tensor(1.4941), tensor(1.5108), tensor(1.5388), tensor(1.5627), tensor(1.5101), tensor(1.6000), tensor(1.4810), tensor(1.4583), tensor(1.4871), tensor(1.5118), tensor(2.5676), tensor(1.5336), tensor(1.4900), tensor(1.5754), tensor(1.4971), tensor(1.5217), tensor(1.9457), tensor(1.5133), tensor(1.5045), tensor(1.5492), tensor(1.4956), tensor(1.4581), tensor(1.4642), tensor(1.4935), tensor(1.4671), tensor(1.5139), tensor(1.5146), tensor(2.5806), tensor(1.4707), tensor(1.4422), tensor(1.8971), tensor(1.5258), tensor(2.1546), tensor(1.5154), tensor(2.2905), tensor(1.4643), tensor(1.5360), tensor(1.5552), tensor(1.4631), tensor(1.5620), tensor(1.5947), tensor(1.5522), tensor(1.5476), tensor(1.6665), tensor(1.4666), tensor(1.5419), tensor(1.6864), tensor(1.9663), tensor(1.5413), tensor(1.5052), tensor(1.5116), tensor(1.4572), tensor(2.2833), tensor(1.5128), tensor(1.5272), tensor(1.4945), tensor(1.5184), tensor(1.4744), tensor(1.4587), tensor(1.5415), tensor(1.5579), tensor(1.6430), tensor(1.5266), tensor(1.5328), tensor(1.4839), tensor(1.4943), tensor(1.5378), tensor(1.4749), tensor(2.0347), tensor(1.4908), tensor(1.5329), tensor(1.5368), tensor(1.4526), tensor(1.5797), tensor(1.8822), tensor(1.5000), tensor(1.4874), tensor(1.5699), tensor(1.4785), tensor(3.0432), tensor(1.5281), tensor(1.7341), tensor(1.5220), tensor(1.9000), tensor(1.5727), tensor(1.6694), tensor(1.4871), tensor(4.0059), tensor(1.5333), tensor(1.4827), tensor(1.5709), tensor(1.4852), tensor(1.4977), tensor(1.5376), tensor(1.4535), tensor(1.4991), tensor(1.4608), tensor(1.5406), tensor(1.5987), tensor(1.5538), tensor(1.8779), tensor(1.4741), tensor(1.4940), tensor(1.5291), tensor(1.4426), tensor(1.5171), tensor(2.4066), tensor(1.5274), tensor(1.4907), tensor(1.7309), tensor(3.7594), tensor(1.5468), tensor(1.5189), tensor(1.5179), tensor(1.5083), tensor(1.5338), tensor(1.5010), tensor(1.7075), tensor(1.6696), tensor(1.5088), tensor(1.4987), tensor(1.9962), tensor(1.6282), tensor(1.4959), tensor(1.4881), tensor(2.3429), tensor(1.4610), tensor(1.5235), tensor(1.4919), tensor(1.5150), tensor(1.6115), tensor(1.6225), tensor(1.5618), tensor(2.0012), tensor(1.4779), tensor(1.4949), tensor(1.5040), tensor(3.4711), tensor(1.5287), tensor(1.5367), tensor(3.7340), tensor(1.5154), tensor(1.5042), tensor(1.5011), tensor(1.4893), tensor(2.2459), tensor(1.5876), tensor(1.4881), tensor(1.4968), tensor(1.5166), tensor(1.7152), tensor(1.4895), tensor(1.4809), tensor(1.5134), tensor(1.4888), tensor(1.5295), tensor(1.5377), tensor(1.4856), tensor(1.4913), tensor(1.5088), tensor(1.6117), tensor(1.5837), tensor(1.5472), tensor(1.5326), tensor(1.5189), tensor(1.4583), tensor(1.6647), tensor(1.4504), tensor(1.4714), tensor(1.5022), tensor(2.0902), tensor(1.6152), tensor(1.6995), tensor(2.4889), tensor(1.5183), tensor(1.5025), tensor(1.4835), tensor(1.7068), tensor(1.5278), tensor(1.5376), tensor(1.4675), tensor(1.5133), tensor(1.4844), tensor(2.0963), tensor(1.4610), tensor(1.5492), tensor(1.8470), tensor(1.5203), tensor(1.4662), tensor(1.4677), tensor(1.5389), tensor(1.5496), tensor(2.9224), tensor(1.4766), tensor(1.6350), tensor(1.6273), tensor(1.5835), tensor(1.4484), tensor(1.4853), tensor(1.6192), tensor(1.5007), tensor(1.5113), tensor(1.4761), tensor(1.4762), tensor(1.5120), tensor(1.6931), tensor(1.8814), tensor(1.4905), tensor(1.5721), tensor(2.3213), tensor(1.5173), tensor(1.5237), tensor(1.4972), tensor(1.4780), tensor(1.8539), tensor(1.5099), tensor(1.5233), tensor(1.6192), tensor(2.3866), tensor(1.5048), tensor(1.5027), tensor(2.4193), tensor(1.4734), tensor(1.5317), tensor(1.5927), tensor(1.4804), tensor(1.5131), tensor(1.5646), tensor(1.4765), tensor(1.4809), tensor(1.5234), tensor(1.4787), tensor(1.6006), tensor(2.1775), tensor(1.5893), tensor(1.5116), tensor(1.4912), tensor(1.5233), tensor(1.6387), tensor(1.5028), tensor(1.6267), tensor(1.4516), tensor(1.4603), tensor(1.4800), tensor(1.5338), tensor(1.5442), tensor(1.5089), tensor(1.5415), tensor(1.5415), tensor(1.5252), tensor(1.4828), tensor(1.5801), tensor(1.5463)]
09/23/2021 14:30:32 - INFO - __main__ - after_losses=[tensor(1.8053), tensor(1.4573), tensor(1.5083), tensor(1.4727), tensor(1.4907), tensor(1.5815), tensor(1.5320), tensor(1.6212), tensor(1.6340), tensor(1.4707), tensor(1.4482), tensor(1.4651), tensor(1.4930), tensor(2.5979), tensor(1.5315), tensor(1.4790), tensor(1.5370), tensor(1.4791), tensor(1.5673), tensor(1.8824), tensor(1.4821), tensor(1.4868), tensor(1.5111), tensor(1.4828), tensor(1.4484), tensor(1.5059), tensor(1.4683), tensor(1.4560), tensor(1.4964), tensor(1.5228), tensor(2.5505), tensor(1.4454), tensor(1.4455), tensor(2.0921), tensor(1.5612), tensor(2.4406), tensor(1.5281), tensor(2.3032), tensor(1.4545), tensor(1.5176), tensor(1.5355), tensor(1.4458), tensor(1.5138), tensor(1.5204), tensor(1.6768), tensor(1.5143), tensor(1.5447), tensor(1.4537), tensor(1.5064), tensor(1.6131), tensor(1.8941), tensor(1.4984), tensor(1.4899), tensor(1.4877), tensor(1.4524), tensor(2.3981), tensor(1.5206), tensor(1.5035), tensor(1.4920), tensor(1.5492), tensor(1.4945), tensor(1.4493), tensor(1.5053), tensor(1.5155), tensor(1.6676), tensor(1.4965), tensor(1.5198), tensor(1.5429), tensor(1.5053), tensor(1.5152), tensor(1.4954), tensor(1.9582), tensor(1.4645), tensor(1.5151), tensor(1.6176), tensor(1.4434), tensor(1.6272), tensor(1.7641), tensor(1.4830), tensor(1.4598), tensor(1.5188), tensor(1.4587), tensor(2.9432), tensor(1.5201), tensor(1.6784), tensor(1.6254), tensor(1.7516), tensor(1.5227), tensor(1.6399), tensor(1.4670), tensor(3.9797), tensor(1.4899), tensor(1.4557), tensor(1.5470), tensor(1.4741), tensor(1.5101), tensor(1.5612), tensor(1.4352), tensor(1.4952), tensor(1.4587), tensor(1.4914), tensor(1.5598), tensor(1.5547), tensor(1.7935), tensor(1.4593), tensor(1.4709), tensor(1.5056), tensor(1.4602), tensor(1.4936), tensor(2.4053), tensor(1.5114), tensor(1.5203), tensor(1.5264), tensor(3.0078), tensor(1.5746), tensor(1.5086), tensor(1.5150), tensor(1.4889), tensor(1.4830), tensor(1.4804), tensor(1.5783), tensor(1.4913), tensor(1.4999), tensor(1.4722), tensor(1.8150), tensor(1.6338), tensor(1.4922), tensor(1.4835), tensor(2.3706), tensor(1.4620), tensor(1.5065), tensor(1.4762), tensor(1.4677), tensor(1.7476), tensor(1.5460), tensor(1.7060), tensor(2.1524), tensor(1.4619), tensor(1.4814), tensor(1.4944), tensor(3.1659), tensor(1.4889), tensor(1.6571), tensor(3.8635), tensor(1.4907), tensor(1.5112), tensor(1.4539), tensor(1.5149), tensor(2.0285), tensor(1.5638), tensor(1.4704), tensor(1.4787), tensor(1.5491), tensor(1.6230), tensor(1.4934), tensor(1.4588), tensor(1.4981), tensor(1.4700), tensor(1.5027), tensor(1.5089), tensor(1.4671), tensor(1.4715), tensor(1.4798), tensor(1.6486), tensor(1.5462), tensor(1.5292), tensor(1.6170), tensor(1.5049), tensor(1.4471), tensor(1.6386), tensor(1.4403), tensor(1.4640), tensor(1.4923), tensor(2.2062), tensor(1.4542), tensor(1.5981), tensor(2.3236), tensor(1.5688), tensor(1.4766), tensor(1.4617), tensor(1.7505), tensor(1.5163), tensor(1.5189), tensor(1.4585), tensor(1.4882), tensor(1.4669), tensor(2.0653), tensor(1.4503), tensor(1.6032), tensor(1.7508), tensor(1.5225), tensor(1.4528), tensor(1.4531), tensor(1.5838), tensor(1.5224), tensor(3.1138), tensor(1.4704), tensor(1.8121), tensor(1.5637), tensor(1.7031), tensor(1.4398), tensor(1.4617), tensor(1.5597), tensor(2.2516), tensor(1.4947), tensor(1.4671), tensor(1.4631), tensor(1.4794), tensor(1.7040), tensor(1.8094), tensor(1.4995), tensor(1.5473), tensor(2.4516), tensor(1.4997), tensor(1.4692), tensor(1.4763), tensor(1.4629), tensor(2.1780), tensor(1.4901), tensor(1.5098), tensor(1.5132), tensor(2.8251), tensor(1.5303), tensor(1.4952), tensor(2.1326), tensor(1.4585), tensor(1.4715), tensor(1.5402), tensor(1.4629), tensor(1.4919), tensor(1.5904), tensor(1.4705), tensor(1.4654), tensor(1.5120), tensor(1.4709), tensor(1.7147), tensor(2.1060), tensor(1.4798), tensor(1.4573), tensor(1.4679), tensor(1.4740), tensor(1.5975), tensor(1.4728), tensor(1.5491), tensor(1.4439), tensor(1.4649), tensor(1.4636), tensor(1.5282), tensor(1.5041), tensor(1.4900), tensor(1.4984), tensor(1.5517), tensor(1.5061), tensor(1.4784), tensor(1.6282), tensor(1.6318)]
09/23/2021 14:30:32 - INFO - __main__ - interference_scores=[tensor(0.1563), tensor(-0.0234), tensor(-0.0225), tensor(-0.0214), tensor(-0.0201), tensor(0.0427), tensor(-0.0308), tensor(0.1110), tensor(0.0340), tensor(-0.0103), tensor(-0.0101), tensor(-0.0220), tensor(-0.0188), tensor(0.0303), tensor(-0.0021), tensor(-0.0110), tensor(-0.0384), tensor(-0.0180), tensor(0.0456), tensor(-0.0633), tensor(-0.0312), tensor(-0.0177), tensor(-0.0381), tensor(-0.0128), tensor(-0.0097), tensor(0.0417), tensor(-0.0252), tensor(-0.0111), tensor(-0.0175), tensor(0.0082), tensor(-0.0300), tensor(-0.0252), tensor(0.0033), tensor(0.1950), tensor(0.0355), tensor(0.2861), tensor(0.0127), tensor(0.0127), tensor(-0.0097), tensor(-0.0184), tensor(-0.0197), tensor(-0.0173), tensor(-0.0482), tensor(-0.0743), tensor(0.1245), tensor(-0.0333), tensor(-0.1219), tensor(-0.0129), tensor(-0.0355), tensor(-0.0733), tensor(-0.0723), tensor(-0.0429), tensor(-0.0153), tensor(-0.0240), tensor(-0.0048), tensor(0.1149), tensor(0.0078), tensor(-0.0237), tensor(-0.0025), tensor(0.0308), tensor(0.0201), tensor(-0.0094), tensor(-0.0362), tensor(-0.0424), tensor(0.0247), tensor(-0.0301), tensor(-0.0130), tensor(0.0590), tensor(0.0110), tensor(-0.0226), tensor(0.0205), tensor(-0.0765), tensor(-0.0263), tensor(-0.0178), tensor(0.0808), tensor(-0.0093), tensor(0.0475), tensor(-0.1181), tensor(-0.0171), tensor(-0.0275), tensor(-0.0512), tensor(-0.0198), tensor(-0.1001), tensor(-0.0080), tensor(-0.0557), tensor(0.1034), tensor(-0.1485), tensor(-0.0500), tensor(-0.0295), tensor(-0.0201), tensor(-0.0262), tensor(-0.0434), tensor(-0.0271), tensor(-0.0239), tensor(-0.0111), tensor(0.0125), tensor(0.0235), tensor(-0.0183), tensor(-0.0039), tensor(-0.0020), tensor(-0.0492), tensor(-0.0389), tensor(0.0009), tensor(-0.0844), tensor(-0.0148), tensor(-0.0231), tensor(-0.0235), tensor(0.0176), tensor(-0.0235), tensor(-0.0013), tensor(-0.0160), tensor(0.0296), tensor(-0.2045), tensor(-0.7516), tensor(0.0278), tensor(-0.0103), tensor(-0.0029), tensor(-0.0194), tensor(-0.0508), tensor(-0.0206), tensor(-0.1292), tensor(-0.1783), tensor(-0.0089), tensor(-0.0265), tensor(-0.1812), tensor(0.0056), tensor(-0.0037), tensor(-0.0046), tensor(0.0276), tensor(0.0010), tensor(-0.0170), tensor(-0.0158), tensor(-0.0473), tensor(0.1361), tensor(-0.0765), tensor(0.1442), tensor(0.1512), tensor(-0.0160), tensor(-0.0135), tensor(-0.0096), tensor(-0.3053), tensor(-0.0397), tensor(0.1204), tensor(0.1295), tensor(-0.0247), tensor(0.0069), tensor(-0.0472), tensor(0.0256), tensor(-0.2174), tensor(-0.0237), tensor(-0.0177), tensor(-0.0181), tensor(0.0325), tensor(-0.0922), tensor(0.0038), tensor(-0.0221), tensor(-0.0153), tensor(-0.0188), tensor(-0.0268), tensor(-0.0288), tensor(-0.0184), tensor(-0.0198), tensor(-0.0290), tensor(0.0368), tensor(-0.0375), tensor(-0.0180), tensor(0.0844), tensor(-0.0140), tensor(-0.0112), tensor(-0.0260), tensor(-0.0102), tensor(-0.0074), tensor(-0.0099), tensor(0.1160), tensor(-0.1610), tensor(-0.1014), tensor(-0.1654), tensor(0.0505), tensor(-0.0259), tensor(-0.0218), tensor(0.0438), tensor(-0.0115), tensor(-0.0187), tensor(-0.0089), tensor(-0.0251), tensor(-0.0175), tensor(-0.0310), tensor(-0.0107), tensor(0.0540), tensor(-0.0962), tensor(0.0022), tensor(-0.0133), tensor(-0.0145), tensor(0.0450), tensor(-0.0272), tensor(0.1915), tensor(-0.0062), tensor(0.1771), tensor(-0.0636), tensor(0.1196), tensor(-0.0086), tensor(-0.0237), tensor(-0.0595), tensor(0.7509), tensor(-0.0166), tensor(-0.0090), tensor(-0.0131), tensor(-0.0326), tensor(0.0109), tensor(-0.0720), tensor(0.0090), tensor(-0.0248), tensor(0.1303), tensor(-0.0177), tensor(-0.0545), tensor(-0.0209), tensor(-0.0151), tensor(0.3241), tensor(-0.0198), tensor(-0.0135), tensor(-0.1060), tensor(0.4385), tensor(0.0255), tensor(-0.0074), tensor(-0.2867), tensor(-0.0149), tensor(-0.0602), tensor(-0.0525), tensor(-0.0174), tensor(-0.0212), tensor(0.0258), tensor(-0.0061), tensor(-0.0155), tensor(-0.0114), tensor(-0.0078), tensor(0.1141), tensor(-0.0715), tensor(-0.1096), tensor(-0.0542), tensor(-0.0232), tensor(-0.0493), tensor(-0.0411), tensor(-0.0300), tensor(-0.0776), tensor(-0.0077), tensor(0.0046), tensor(-0.0164), tensor(-0.0055), tensor(-0.0401), tensor(-0.0190), tensor(-0.0432), tensor(0.0102), tensor(-0.0191), tensor(-0.0044), tensor(0.0481), tensor(0.0855)]
09/23/2021 14:30:32 - INFO - __main__ - retrieved candidates ids = ['mrqa_triviaqa-validation-3803', 'mrqa_triviaqa-validation-3479', 'mrqa_triviaqa-validation-6186', 'mrqa_naturalquestions-validation-2248', 'mrqa_triviaqa-validation-4137', 'mrqa_triviaqa-validation-7032', 'mrqa_triviaqa-validation-6224', 'mrqa_triviaqa-validation-2914', 'mrqa_triviaqa-validation-945', 'mrqa_naturalquestions-validation-9054', 'mrqa_hotpotqa-validation-452', 'mrqa_triviaqa-validation-4584', 'mrqa_squad-validation-10428', 'mrqa_hotpotqa-validation-3872', 'mrqa_triviaqa-validation-3335', 'mrqa_triviaqa-validation-1550', 'mrqa_squad-validation-7278', 'mrqa_squad-validation-3887', 'mrqa_squad-validation-3385', 'mrqa_triviaqa-validation-6125', 'mrqa_naturalquestions-validation-1364', 'mrqa_squad-validation-9036', 'mrqa_triviaqa-validation-2091', 'mrqa_triviaqa-validation-2812', 'mrqa_naturalquestions-validation-1725', 'mrqa_triviaqa-validation-105', 'mrqa_hotpotqa-validation-4649', 'mrqa_triviaqa-validation-192', 'mrqa_squad-validation-6588', 'mrqa_triviaqa-validation-5239', 'mrqa_hotpotqa-validation-400', 'mrqa_triviaqa-validation-866']
09/23/2021 14:30:32 - INFO - __main__ - Mixed the retrieved examples (len=32) to the current batch for training.
09/23/2021 14:30:32 - INFO - __main__ - Start bug-fixing (len(examples_to_train)=61) .... Timecode: 49
09/23/2021 14:30:45 - INFO - __main__ - Start bug-fixing .... Done!
09/23/2021 14:30:45 - INFO - __main__ - Evaluating again to analyze the performance .... Timecode: 49
09/23/2021 14:30:49 - INFO - __main__ - After Error Fixing: 0.90625
09/23/2021 14:30:49 - INFO - __main__ - Instant Fixing Rate: 0.896551724137931
09/23/2021 14:30:49 - INFO - __main__ - Instant Retention Rate: 0.9999999966666667
09/23/2021 14:30:51 - INFO - __main__ - Model saved to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/model_ckpt_050.pt.
09/23/2021 14:30:51 - INFO - __main__ - Saving the current error examples (len=29) to the memory.
09/23/2021 14:30:51 - INFO - __main__ - Current memory size: 1133.
09/23/2021 14:30:51 - INFO - __main__ - Using randomly generated memory keys for ER and MIR.
09/23/2021 14:30:51 - INFO - __main__ - Finished.
09/23/2021 14:30:51 - INFO - __main__ - --------------------------------------------------
09/23/2021 14:30:51 - INFO - __main__ - Start the final evaluation.
09/23/2021 14:30:51 - INFO - __main__ - Test the in-stream examples overall.
09/23/2021 14:33:50 - INFO - __main__ - Test the upstream forgetting eval.
09/23/2021 14:35:41 - INFO - __main__ - Finish the final evaluation.
09/23/2021 14:35:41 - INFO - __main__ - Saving the memory to exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_ckpts/memory_dict.pkl
09/23/2021 14:35:41 - INFO - __main__ - Finished. Results saved to exp_results/dynamic_stream/memory_based/results/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=LA_result.json
