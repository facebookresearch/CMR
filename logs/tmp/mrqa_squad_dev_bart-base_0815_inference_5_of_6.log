08/16/2021 13:38:08 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa_naturalquestions', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=4, output_dir='out/mrqa_naturalquestions_bart-base_0617v4', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
08/16/2021 13:38:08 - INFO - __main__ - dataset_size=10474, num_shards=6, local_shard_id=5
/private/home/yuchenlin/.conda/envs/bartqa/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  return array(a, dtype, copy=False, order=order)
08/16/2021 13:38:09 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
08/16/2021 13:38:09 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
08/16/2021 13:38:09 - INFO - __main__ - Start tokenizing ... 1745 instances
08/16/2021 13:38:09 - INFO - __main__ - Printing 3 examples
08/16/2021 13:38:09 - INFO - __main__ - Context: Endosymbiotic gene transfer is how we know about the lost chloroplasts in many chromalveolate lineages. Even if a chloroplast is eventually lost, the genes it donated to the former host's nucleus persist, providing evidence for the lost chloroplast's existence. For example, while diatoms (a heterokontophyte) now have a red algal derived chloroplast, the presence of many green algal genes in the diatom nucleus provide evidence that the diatom ancestor (probably the ancestor of all chromalveolates too) had a green algal derived chloroplast at some point, which was subsequently replaced by the red chloroplast. | Question: What shows us lost chloroplasts?
08/16/2021 13:38:09 - INFO - __main__ - ['Endosymbiotic gene transfer']
08/16/2021 13:38:09 - INFO - __main__ - Context: Endosymbiotic gene transfer is how we know about the lost chloroplasts in many chromalveolate lineages. Even if a chloroplast is eventually lost, the genes it donated to the former host's nucleus persist, providing evidence for the lost chloroplast's existence. For example, while diatoms (a heterokontophyte) now have a red algal derived chloroplast, the presence of many green algal genes in the diatom nucleus provide evidence that the diatom ancestor (probably the ancestor of all chromalveolates too) had a green algal derived chloroplast at some point, which was subsequently replaced by the red chloroplast. | Question: What do donated genes give evidence of?
08/16/2021 13:38:09 - INFO - __main__ - ["the lost chloroplast's existence", "for the lost chloroplast's existence"]
08/16/2021 13:38:09 - INFO - __main__ - Context: Endosymbiotic gene transfer is how we know about the lost chloroplasts in many chromalveolate lineages. Even if a chloroplast is eventually lost, the genes it donated to the former host's nucleus persist, providing evidence for the lost chloroplast's existence. For example, while diatoms (a heterokontophyte) now have a red algal derived chloroplast, the presence of many green algal genes in the diatom nucleus provide evidence that the diatom ancestor (probably the ancestor of all chromalveolates too) had a green algal derived chloroplast at some point, which was subsequently replaced by the red chloroplast. | Question: What kind of chloroplasts do diatoms have?
08/16/2021 13:38:09 - INFO - __main__ - ['red algal derived', 'red algal', 'a red algal derived chloroplast']
08/16/2021 13:38:09 - INFO - __main__ - Tokenizing Input ...
08/16/2021 13:38:11 - INFO - __main__ - Tokenizing Input ... Done!
08/16/2021 13:38:11 - INFO - __main__ - Tokenizing Output ...
08/16/2021 13:38:12 - INFO - __main__ - Tokenizing Output ... Done!
08/16/2021 13:38:12 - INFO - __main__ - Loaded 1745 examples from dev data
08/16/2021 13:38:12 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt ....
08/16/2021 13:38:12 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
08/16/2021 13:38:12 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

08/16/2021 13:38:13 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
08/16/2021 13:38:17 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt .... Done!
08/16/2021 13:38:21 - INFO - __main__ - Starting inference ...
Infernece:   0%|          | 0/28 [00:00<?, ?it/s]Infernece:   4%|▎         | 1/28 [00:04<02:10,  4.82s/it]Infernece:   7%|▋         | 2/28 [00:09<02:07,  4.92s/it]Infernece:  11%|█         | 3/28 [00:14<02:02,  4.89s/it]Infernece:  14%|█▍        | 4/28 [00:19<01:54,  4.76s/it]Infernece:  18%|█▊        | 5/28 [00:23<01:48,  4.72s/it]Infernece:  21%|██▏       | 6/28 [00:28<01:44,  4.74s/it]Infernece:  25%|██▌       | 7/28 [00:33<01:40,  4.80s/it]Infernece:  29%|██▊       | 8/28 [00:39<01:39,  4.98s/it]Infernece:  32%|███▏      | 9/28 [00:43<01:31,  4.84s/it]Infernece:  36%|███▌      | 10/28 [00:48<01:25,  4.74s/it]Infernece:  39%|███▉      | 11/28 [00:52<01:20,  4.76s/it]Infernece:  43%|████▎     | 12/28 [00:57<01:15,  4.74s/it]Infernece:  46%|████▋     | 13/28 [01:02<01:10,  4.73s/it]Infernece:  50%|█████     | 14/28 [01:07<01:07,  4.82s/it]Infernece:  54%|█████▎    | 15/28 [01:11<01:01,  4.73s/it]Infernece:  57%|█████▋    | 16/28 [01:16<00:56,  4.70s/it]Infernece:  61%|██████    | 17/28 [01:20<00:50,  4.63s/it]Infernece:  64%|██████▍   | 18/28 [01:25<00:46,  4.62s/it]Infernece:  68%|██████▊   | 19/28 [01:30<00:41,  4.65s/it]Infernece:  71%|███████▏  | 20/28 [01:35<00:38,  4.76s/it]Infernece:  75%|███████▌  | 21/28 [01:40<00:33,  4.85s/it]Infernece:  79%|███████▊  | 22/28 [01:45<00:29,  4.91s/it]Infernece:  82%|████████▏ | 23/28 [01:50<00:24,  4.95s/it]Infernece:  86%|████████▌ | 24/28 [01:55<00:19,  4.90s/it]Infernece:  89%|████████▉ | 25/28 [01:59<00:14,  4.82s/it]Infernece:  93%|█████████▎| 26/28 [02:05<00:09,  4.98s/it]Infernece:  96%|█████████▋| 27/28 [02:10<00:05,  5.04s/it]Infernece: 100%|██████████| 28/28 [02:12<00:00,  4.08s/it]Infernece: 100%|██████████| 28/28 [02:12<00:00,  4.72s/it]
08/16/2021 13:40:33 - INFO - __main__ - Starting inference ... Done
