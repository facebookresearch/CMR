08/16/2021 13:33:25 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa_naturalquestions', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=4, output_dir='out/mrqa_naturalquestions_bart-base_0617v4', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
08/16/2021 13:33:25 - INFO - __main__ - dataset_size=7784, num_shards=6, local_shard_id=4
/private/home/yuchenlin/.conda/envs/bartqa/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  return array(a, dtype, copy=False, order=order)
08/16/2021 13:33:26 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
08/16/2021 13:33:26 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
08/16/2021 13:33:26 - INFO - __main__ - Start tokenizing ... 1297 instances
08/16/2021 13:33:26 - INFO - __main__ - Printing 3 examples
08/16/2021 13:33:26 - INFO - __main__ - Context: Kenny Everett - The best possible way to remember a true ...Kenny Everett - The best possible way to remember a true pioneer | The Independent  Kenny Everett - The best possible way to remember a true pioneer  A biopic of Kenny Everett should be compelling TV, says James Rampton  Sunday 30 September 2012 23:00 BST  Click to follow  The Independent Culture  Kenny Everett  In 1984, Kenny Everett was the biggest star on British TV. His anarchic, groundbreaking BBC1 programme, The Kenny Everett Television Show, was pulling in audiences of some 20 million, a third of the British population, and his catchphrases were parroted in pubs and playgrounds up and down the land.  So when he contemplated coming out to the general public that year and admitting that he was living in a ménage à trois with two men called Pepe and Nikolai, his overwhelming fear was that he would be totalling his own career. A Catholic who once contemplated becoming a priest, he was so petrified about coming out that nine years earlier he had even attempted suicide.  Oliver Lansley, who plays the iconoclastic DJ in a compelling new BBC4 biopic, Best Possible Taste: The Kenny Everett Story, says: "He was terrified of what the press would say if he came out. He thought everyone would turn on him. But when he came out with the now celebrated announcement that 'two husbands are so much better than one', nobody was fussed."  In Best Possible Taste: The Kenny Everett Story, Katherine Kelly portrays Lee Middleton, his long-suffering wife, who remained devoted to him long after the break-up of their 12-year marriage. She agrees that, "He was really frightened about coming out, which is why he didn't do it for so long.  "But when he finally plucked up the courage to do it, nobody cared. His ratings didn't go down. His talent was so sublime that it didn't matter what was going on in his private life. That shows how flipping gifted he was!"  Tim Whitnall's drama, partly based on the biography by The Independent's David Lister, In the Best Possible Taste: The Crazy Life of Kenny Everett, shows Everett as the radio and TV pioneer he was, constantly reinventing those media. "He was an astonishing sonic innovator," says Luke Franklin, associate producer on the drama.  "He was a virtuoso when it came to running a radio studio and putting jingles together. When he started on the pirate station Radio London, all the jingles were imported pre-made from the US. Kenny very quickly started to make his own – he was the first person to do that." Everett also had eclectic musical taste and was the first DJ to play both "Strawberry Fields Forever" and "Bohemian Rhapsody" – which he once aired 36 times in one day.  That same questing spirit, however, frequently landed Everett in trouble. Pathologically averse to authority, he was fired from several jobs for failing to obey instructions. Most notoriously, he also caused an almighty row at the Tory Party Conference in 1983 when he ranted "Let's bomb Russia!"  During all this mayhem, the one constant in his life was his amazingly durable relationship with Lee. Kelly says that, "the problem in their relationship was that Kenny was gay. He loved her so much, but fantasised about Burt Reynolds."  Lansley is hopeful that Best Possible Taste will help bring the unique talents of the man who has influenced an entire generation of broadcasters to a whole new audience. "I hope it refreshes people's memories of what a dazzling performer he was. He should be mentioned in every conversation about comedy greats and in the same breath as Tony Hancock, Spike Milligan and Monty Python. He's right up there."  'Best Possible Taste: The Kenny Everett Story' is on BBC4 at 9pm on Wed 3 Oct  More about:  Rick Jones the 20th Century - RebelDogRick Jones the 20th Century  In use during the 1900's, this farewell remark was rendered in Chinese-sounding  Pidgin English | Question: Which comedian had the catch phrase 'It's all done in the best possible taste!'?
08/16/2021 13:33:26 - INFO - __main__ - ['kenny everett']
08/16/2021 13:33:26 - INFO - __main__ - Context: Facts about Bletchley Park – the WW2 code-breaking centre ...Facts about Bletchley Park – the WW2 code-breaking centre that made the D-Day landings possible and spawned the computer age | History Extra  BBC History Magazine - 5 issues for £5  The registration room in hut 6 at Bletchley Park, Buckinghamshire. (Getty Images)  Here, writing for History Extra, Sinclair McKay, the author of  The Lost World Of Bletchley Park  (Aurum Press) and The Secret Listeners, tells you everything you need to know about the famous decryption centre.  Q: What role did Bletchley Park play during the Second World War?  A: Bletchley Park was the Buckinghamshire house and estate that played host to Britain’s code-breaking triumphs – from the success in breaking the ‘insoluble’ Nazi Enigma codes to its incredible work on Japanese cryptology.  Its role was so secret – everyone who worked there signed the Official Secrets Act for life – that we only really started to learn about its achievements decades afterwards. And there is still much to emerge.  Q: When and how was the code-breaking centre established?  A: Before it was evacuated to Bletchley, the Government Code and Cypher School – as it was then known – operated from a building near Westminster. There had been a special code-breaking department since the First World War – then, it was referred to as ‘Room 40’.  In the interwar years, they analysed Soviet traffic, among other things. When it seemed obvious that war was coming once more, many of the expert cryptanalysts of Room 40 set up the new establishment at Bletchley in 1938. They recruited crucial new blood, including brilliant young mathematician Alan Turing, as well as lecturers and undergraduates from universities all over the country.  Q: How many code-breakers worked there? What did an average day consist of?  A: Numbers rose as the war went on; from a relatively small team in 1938 to something like 10,000 people – code-breakers, Wrens, WAAFs, posh debutantes working on the cross-indexing system etc.  Obviously, they didn’t live at the house. All these people, mainly young, were billeted around the town and nearby villages. Bletchley locals were disconcerted by some of the eccentric code-breakers, and they formed a theory that the secret establishment was in fact a special lunatic asylum.  There was no average day. Code-breaking went on round the clock, in a three-shift system, and in different huts – very often the work was done in plain wooden huts. There could either be months of agonising paralysis (as there was in the battle to crack the nightmarishly complex Naval Enigma) or nights of giddying triumph, as when 20-year-old Mavis Batey broke the code that led to British victory in the Battle of Cape Matapan.  Bletchley Park pictured in 1926. (Evening Standard/Getty Images)  Q: Roughly how many ciphers and codes were decrypted in total?  A: It is simply impossible to know – largely because Bletchley was so mind-bogglingly successful. They read messages from the German army, navy, air force, secret service… even messages from the desk of Hitler himself. Countless thousands upon thousands of communications.  They cracked Italian and Japanese cyphers, and the operation was spread across the whole wide world. In posts from Cairo to Murmansk, dedicated secret listeners for the Y Service intercepted all the secret coded radio messages – and relayed them back to England, and ultimately back to this incredible code-breaking factory.  The other miracle was the secrecy; the fact that the Germans never really guessed adds to this astonishing success.  Q: Is it true that the 'Ultra' intelligence produced at Bletchley Park shortened the war by two to four years?  A: President Eisenhower credited the work of ‘BP’, as it was called, with having shortened the war by two years. Think for a moment of how many lives those two years might represent; the countless people saved simply by the ending of the conflict | Question: Which English county is Bletchley Park, the WW II code-breaking centre?
08/16/2021 13:33:26 - INFO - __main__ - ['buckinghamshire']
08/16/2021 13:33:26 - INFO - __main__ - Context: Kir Drink Recipe - Cocktail - Bar None DrinksKir Drink Recipe - Cocktail  dash(es) Creme de Cassis (more Creme de Cassis drinks )  Instructions  In a white wine glass, pour the cassis first, then the wine.  Serve chilled. Very popular drink in France.  Kir is a popular cocktail that is created when the blackcurrant-flavored crï¿½me liquor Crï¿½me de cassis is combined with a white wine of the drinkerï¿½s choice.  To make it the even more well-known Kir Royal, champagne is added instead of white wine.  The origins of the drink are quite interesting.  Kir is named after Felix Kir, also referred to as Canon Kir, who ruled as Dijonï¿½s mayor between 1945 and 1968.  Kir enjoyed drinking the mixture of white wine and blackcurrant liquor so much, that he often offered the sweet concoction to visiting delegations at receptions.  His love for the drink was so popular that it was soon being served at the receptions as their official aperitif.  In addition, Kir allowed Lejay Lagoute, a producer of blackcurrant liquor, the use of his name during promotions for their liquor, Crï¿½me de Cassis de Dijon.  Today, when in a bar, those who ask for a Kir will more than likely be given a drink made with blackcurrant liquor and white wine.  The drink is quite refreshing, but doesnï¿½t have the sparkling kick that many people enjoy.  To truly enjoy all that a Kir has to offer, make sure to ask for a Kir Royal.  The bartender will replace the white wine with champagne to give it the higher end taste that connoisseurs truly love.  Of course, when ordering a Kir Royal, expect it to cost a little more than an average Kir since the more expensive champagne is being used.  But for a couple extra bucks, itï¿½s worth it!  Credit  Kir and Kir Royale: Two Iconic French Wine Cocktails | The ...Kir and Kir Royale: Two Iconic French Wine Cocktails | The Kitchn  Kir and Kir Royale: Two Iconic French Wine Cocktails  Kir and Kir Royale: Two Iconic French Wine Cocktails  Email  Cocktail Week at The Kitchn immediately brings to mind two iconic and much loved French wine cocktails, namely Kir and its more salubrious cousin Kir Royale.  I was introduced to Kir many years ago while living in France. Every Sunday, before a fabulously long lunch, friends, family and neighbors would gather for the apéritif' hour to chat, laugh, comment on the state of the nation and nibble on a wide selection of savory, salty snacks. Typically, Kir was the drink of choice and on special occasions Kir Royale.  The History of Kir  Kir originated in Burgundy, France. It is named after the priest Canon Félix Kir, who was a hero in the French Resistance during the Second World War, and also the Mayor of the Burgundian town Dijon from 1945 to 1968. He was much revered, and history tells us that he was also a big fan of local products and hence created the drink by mixing the local white wine made from the Aligoté grape (bone dry, acidic, with fairly neutral aromas and flavors and unoaked) with the local blackcurrant liqueur 'Crème de Cassis'.  Crème de Cassis is a deeply colored, viscous, sweet liqueur, which is made by macerating crushed blackcurrants in eau-de-vie. Once the maceration is complete, the liqueur is drawn off the blackcurrant skins and seeds and then sugar is added. The resulting Crème de Cassis liqueur is 20% abv and has about 400g/l residual sugar.  Canon Felix's recipe was delicious and a huge success, because the sweetness and flavor of the liqueur provided the perfect foil and balance for the austerity and acidity of the Aligoté wine. So successful was the combination it flourished and became famous not just in Burgundy but all over France, where it was often adapted to include other regional wines and liqueurs, as well as internationally all over the world.  Kir Royale, differs from Kir in that it is made using Champagne, rather than the Aligoté white wine. Hence it is more expensive | Question: What is added to white wine to make a Kir?
08/16/2021 13:33:26 - INFO - __main__ - ['cassis']
08/16/2021 13:33:26 - INFO - __main__ - Tokenizing Input ...
08/16/2021 13:33:36 - INFO - __main__ - Tokenizing Input ... Done!
08/16/2021 13:33:36 - INFO - __main__ - Tokenizing Output ...
08/16/2021 13:33:36 - INFO - __main__ - Tokenizing Output ... Done!
08/16/2021 13:33:36 - INFO - __main__ - Loaded 1297 examples from dev data
08/16/2021 13:33:36 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt ....
08/16/2021 13:33:40 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
08/16/2021 13:33:40 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

08/16/2021 13:33:40 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
08/16/2021 13:33:45 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt .... Done!
08/16/2021 13:33:50 - INFO - __main__ - Starting inference ...
Infernece:   0%|          | 0/21 [00:00<?, ?it/s]Infernece:   5%|▍         | 1/21 [00:07<02:22,  7.13s/it]Infernece:  10%|▉         | 2/21 [00:14<02:14,  7.09s/it]Infernece:  14%|█▍        | 3/21 [00:20<02:06,  7.02s/it]Infernece:  19%|█▉        | 4/21 [00:27<01:57,  6.93s/it]Infernece:  24%|██▍       | 5/21 [00:34<01:51,  6.95s/it]Infernece:  29%|██▊       | 6/21 [00:41<01:44,  6.98s/it]Infernece:  33%|███▎      | 7/21 [00:48<01:38,  7.01s/it]Infernece:  38%|███▊      | 8/21 [00:55<01:30,  6.93s/it]Infernece:  43%|████▎     | 9/21 [01:02<01:22,  6.85s/it]Infernece:  48%|████▊     | 10/21 [01:08<01:14,  6.81s/it]Infernece:  52%|█████▏    | 11/21 [01:16<01:09,  6.90s/it]Infernece:  57%|█████▋    | 12/21 [01:22<01:01,  6.87s/it]Infernece:  62%|██████▏   | 13/21 [01:29<00:55,  6.92s/it]Infernece:  67%|██████▋   | 14/21 [01:36<00:48,  6.87s/it]Infernece:  71%|███████▏  | 15/21 [01:44<00:42,  7.02s/it]Infernece:  76%|███████▌  | 16/21 [01:51<00:35,  7.03s/it]Infernece:  81%|████████  | 17/21 [01:58<00:28,  7.00s/it]Infernece:  86%|████████▌ | 18/21 [02:04<00:20,  6.91s/it]Infernece:  90%|█████████ | 19/21 [02:11<00:13,  6.86s/it]Infernece:  95%|█████████▌| 20/21 [02:18<00:06,  6.81s/it]Infernece: 100%|██████████| 21/21 [02:20<00:00,  5.33s/it]Infernece: 100%|██████████| 21/21 [02:20<00:00,  6.67s/it]
08/16/2021 13:36:10 - INFO - __main__ - Starting inference ... Done
