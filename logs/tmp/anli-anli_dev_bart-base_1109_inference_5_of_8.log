11/09/2021 19:01:46 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:01:46 - INFO - __main__ - dataset_size=6400, num_shards=8, local_shard_id=5
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:01:47 - INFO - __main__ - Start tokenizing ... 800 instances
11/09/2021 19:01:47 - INFO - __main__ - Printing 3 examples
11/09/2021 19:01:47 - INFO - __main__ - Premise: Richard A. Ludwin (born May 27, 1948) is an American television executive and former vice president at NBC Television. He is notable as the executive who backed Jerry Seinfeld's series "Seinfeld", which went on to become one of the most popular and successful television sitcoms of all time. He was also the head of NBC's late night programming during the Conan O'Brien and Jay Leno conflict in 2010. </s> Hypothesis:  He was also the head of NBC's late night programming during the Conan O'Brien and Jay Leno conflict in 2010 and was responsible for the conflict.
11/09/2021 19:01:47 - INFO - __main__ - ['neutral']
11/09/2021 19:01:47 - INFO - __main__ - Premise: Laurence Alma-Tadema (born Laurense Tadema, 1865–1940), was an English novelist and poet of the late 19th and early 20th centuries who worked in many genres. Eldest daughter of the Dutch painter Lawrence Alma-Tadema (1836–1912) and his first wife Marie-Pauline Gressin Dumoulin, she was born in Brussels. </s> Hypothesis: Alma-Tadema was born 16 years after 1850.
11/09/2021 19:01:47 - INFO - __main__ - ['contradiction']
11/09/2021 19:01:47 - INFO - __main__ - Premise: Operation Zahnarzt (literally "Dentist") was a plan by the Germans to eliminate the Third Army during World War II. The plan of Operation Zahnarzt was to immediately come after Operation Nordwind. The plan was to initiate a pincer movement to encircle and destroy the 3rd US Army. </s> Hypothesis: Operation Zahnarzt was part of WWI
11/09/2021 19:01:47 - INFO - __main__ - ['contradiction']
11/09/2021 19:01:47 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:01:49 - INFO - __main__ - Loaded 800 examples from dev data
11/09/2021 19:01:49 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:01:49 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:01:49 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:01:49 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:01:55 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:02:01 - INFO - __main__ - Starting inference ...
Infernece:   0%|          | 0/13 [00:00<?, ?it/s]Infernece:   8%|▊         | 1/13 [00:04<00:48,  4.05s/it]Infernece:  15%|█▌        | 2/13 [00:08<00:44,  4.08s/it]Infernece:  23%|██▎       | 3/13 [00:12<00:42,  4.26s/it]Infernece:  31%|███       | 4/13 [00:16<00:37,  4.14s/it]Infernece:  38%|███▊      | 5/13 [00:20<00:32,  4.05s/it]Infernece:  46%|████▌     | 6/13 [00:24<00:27,  3.95s/it]Infernece:  54%|█████▍    | 7/13 [00:27<00:22,  3.82s/it]Infernece:  62%|██████▏   | 8/13 [00:31<00:18,  3.64s/it]Infernece:  69%|██████▉   | 9/13 [00:35<00:14,  3.75s/it]Infernece:  77%|███████▋  | 10/13 [00:38<00:11,  3.74s/it]Infernece:  85%|████████▍ | 11/13 [00:42<00:07,  3.88s/it]Infernece:  92%|█████████▏| 12/13 [00:45<00:03,  3.41s/it]Infernece: 100%|██████████| 13/13 [00:47<00:00,  2.90s/it]Infernece: 100%|██████████| 13/13 [00:47<00:00,  3.62s/it]
11/09/2021 19:02:48 - INFO - __main__ - Starting inference ... Done
