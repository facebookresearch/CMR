08/16/2021 13:36:17 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa_naturalquestions', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=4, output_dir='out/mrqa_naturalquestions_bart-base_0617v4', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
08/16/2021 13:36:18 - INFO - __main__ - dataset_size=5901, num_shards=6, local_shard_id=3
/private/home/yuchenlin/.conda/envs/bartqa/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  return array(a, dtype, copy=False, order=order)
08/16/2021 13:36:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
08/16/2021 13:36:18 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
08/16/2021 13:36:18 - INFO - __main__ - Start tokenizing ... 983 instances
08/16/2021 13:36:18 - INFO - __main__ - Printing 3 examples
08/16/2021 13:36:18 - INFO - __main__ - Context: Admiral Clarey Bridge  Admiral Clarey Bridge, also known as the Ford Island Bridge, is a pontoon bridge, commonly called a floating concrete drawbridge, providing access to Ford Island, a United States Navy installation situated in the middle of Pearl Harbor.  The bridge provides access to Ford Island's historic sites to the public via tour bus and provides access to O'ahu for US military families housed on the island.  Before the completion of the bridge, the island's residents were required to use ferry boats operated by Naval personnel that operated on an hourly basis.  The bridge is one of only a few floating bridges and its floating moveable span is the largest worldwide.  Its namesake, Admiral Bernard A. Clarey, was one of the Navy's most decorated officers.   Admiral Clarey Bridge  Admiral Clarey Bridge, also known as the Ford Island Bridge, is a pontoon bridge, commonly called a floating concrete drawbridge, providing access to Ford Island, a United States Navy installation situated in the middle of Pearl Harbor.  The bridge provides access to Ford Island's historic sites to the public via tour bus and provides access to O'ahu for US military families housed on the island.  Before the completion of the bridge, the island's residents were required to use ferry boats operated by Naval personnel that operated on an hourly basis.  The bridge is one of only a few floating bridges and its floating moveable span is the largest worldwide.  Its namesake, Admiral Bernard A. Clarey, was one of the Navy's most decorated officers.   Ford Island  Ford Island (Hawaiian: ' ) is an islet in the center of Pearl Harbor, Oahu, in the U.S. state of Hawaii.  It has been known as Rabbit Island, Marín's Island, and Little Goats Island, and its native Hawaiian name is Mokuʻ umeʻ ume.  The island had an area of 334 acres when it was surveyed in 1825, which was increased during the 1930s to 441 acres with fill dredged out of Pearl Harbor by the United States Navy to accommodate battleships. | Question: What island in the center of Pearl Harbor is connected to the O'ahu by a floating bridge?
08/16/2021 13:36:18 - INFO - __main__ - ['Ford Island']
08/16/2021 13:36:18 - INFO - __main__ - Context: Michael Standing (footballer)  Michael John Standing (born 20 March 1981) is an English former professional footballer who played as a midfielder.  Since terminating his playing career, Standing has become an agent for former teammate and long-term friend Gareth Barry.  He has also played part-time for his hometown club, Shoreham.   Gareth Barry  Gareth Barry (born 23 February 1981) is an English professional footballer who plays as a midfielder for West Bromwich Albion.  He is the player with the highest number of appearances in the Premier League. | Question: Michael Standing was long term friends with which professional midfielder from West Brom?
08/16/2021 13:36:18 - INFO - __main__ - ['Gareth Barry']
08/16/2021 13:36:18 - INFO - __main__ - Context: Cacareco  Cacareco (1954-1962) was a female black rhinoceros exhibited in Brazilian zoos.  She became famous as a candidate for the 1958 São Paulo city council elections with the intention of protesting against political corruption.  Electoral officials did not accept Cacareco's candidacy, but she eventually won 100,000 votes, more than any other party in that same election (which was also marked by rampant absenteeism).  Today, the term "voto Cacareco" (Cacareco vote) is commonly used to describe protest votes in Brazil.  Cacareco's candidacy inspired the Rhinoceros Party of Canada, nominally led by the rhinoceros Cornelius the First.   Rhinoceros Party of Canada (1963–93)  The Rhinoceros Party (French: "Parti Rhinocéros") was a registered political party in Canada from the 1960s to the 1990s.  Operating within the tradition of political satire, the Rhinoceros Party's basic credo, their so-called primal promise, was "a promise to keep none of our promises".  They then promised outlandishly impossible schemes designed to amuse and entertain the voting public. | Question: When did the party inspired by Cacareco begin in Canada?
08/16/2021 13:36:18 - INFO - __main__ - ['1960s']
08/16/2021 13:36:18 - INFO - __main__ - Tokenizing Input ...
08/16/2021 13:36:21 - INFO - __main__ - Tokenizing Input ... Done!
08/16/2021 13:36:21 - INFO - __main__ - Tokenizing Output ...
08/16/2021 13:36:21 - INFO - __main__ - Tokenizing Output ... Done!
08/16/2021 13:36:21 - INFO - __main__ - Loaded 983 examples from dev data
08/16/2021 13:36:21 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt ....
08/16/2021 13:36:22 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
08/16/2021 13:36:22 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

08/16/2021 13:36:22 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
08/16/2021 13:36:27 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt .... Done!
08/16/2021 13:36:31 - INFO - __main__ - Starting inference ...
Infernece:   0%|          | 0/16 [00:00<?, ?it/s]Infernece:   6%|▋         | 1/16 [00:06<01:34,  6.30s/it]Infernece:  12%|█▎        | 2/16 [00:12<01:29,  6.40s/it]Infernece:  19%|█▉        | 3/16 [00:19<01:22,  6.38s/it]Infernece:  25%|██▌       | 4/16 [00:25<01:16,  6.38s/it]Infernece:  31%|███▏      | 5/16 [00:32<01:10,  6.40s/it]Infernece:  38%|███▊      | 6/16 [00:37<01:01,  6.18s/it]Infernece:  44%|████▍     | 7/16 [00:43<00:54,  6.11s/it]Infernece:  50%|█████     | 8/16 [00:49<00:47,  5.97s/it]Infernece:  56%|█████▋    | 9/16 [00:55<00:41,  5.99s/it]Infernece:  62%|██████▎   | 10/16 [01:01<00:36,  6.13s/it]Infernece:  69%|██████▉   | 11/16 [01:08<00:31,  6.25s/it]Infernece:  75%|███████▌  | 12/16 [01:15<00:25,  6.40s/it]Infernece:  81%|████████▏ | 13/16 [01:21<00:18,  6.30s/it]Infernece:  88%|████████▊ | 14/16 [01:26<00:12,  6.14s/it]Infernece:  94%|█████████▍| 15/16 [01:32<00:05,  5.99s/it]Infernece: 100%|██████████| 16/16 [01:33<00:00,  4.60s/it]Infernece: 100%|██████████| 16/16 [01:33<00:00,  5.87s/it]
08/16/2021 13:38:05 - INFO - __main__ - Starting inference ... Done
