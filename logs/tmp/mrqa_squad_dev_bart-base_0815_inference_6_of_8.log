10/29/2021 11:44:30 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_naturalquestions_bart-base_1028_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 11:44:30 - INFO - __main__ - dataset_size=10474, num_shards=8, local_shard_id=6
10/29/2021 11:44:31 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 11:44:31 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 11:44:31 - INFO - __main__ - Start tokenizing ... 1309 instances
10/29/2021 11:44:31 - INFO - __main__ - Printing 3 examples
10/29/2021 11:44:31 - INFO - __main__ - Question: Who is also known at the father of the hydrogen bomb? </s> Context: Notable faculty in physics have included the speed of light calculator A. A. Michelson, elementary charge calculator Robert A. Millikan, discoverer of the Compton Effect Arthur H. Compton, the creator of the first nuclear reactor Enrico Fermi, "the father of the hydrogen bomb" Edward Teller, "one of the most brilliant and productive experimental physicists of the twentieth century" Luis Walter Alvarez, Murray Gell-Mann who introduced the quark, second female Nobel laureate Maria Goeppert-Mayer, the youngest American winner of the Nobel Prize Tsung-Dao Lee, and astrophysicist Subrahmanyan Chandrasekhar.
10/29/2021 11:44:31 - INFO - __main__ - ['Edward Teller']
10/29/2021 11:44:31 - INFO - __main__ - Question: Who was the second female Nobel laureate ? </s> Context: Notable faculty in physics have included the speed of light calculator A. A. Michelson, elementary charge calculator Robert A. Millikan, discoverer of the Compton Effect Arthur H. Compton, the creator of the first nuclear reactor Enrico Fermi, "the father of the hydrogen bomb" Edward Teller, "one of the most brilliant and productive experimental physicists of the twentieth century" Luis Walter Alvarez, Murray Gell-Mann who introduced the quark, second female Nobel laureate Maria Goeppert-Mayer, the youngest American winner of the Nobel Prize Tsung-Dao Lee, and astrophysicist Subrahmanyan Chandrasekhar.
10/29/2021 11:44:31 - INFO - __main__ - ['Maria Goeppert-Mayer']
10/29/2021 11:44:31 - INFO - __main__ - Question: What was the name of the Doctor Who special created for Comic Relief? </s> Context: In 1999, another special, Doctor Who and the Curse of Fatal Death, was made for Comic Relief and later released on VHS. An affectionate parody of the television series, it was split into four segments, mimicking the traditional serial format, complete with cliffhangers, and running down the same corridor several times when being chased (the version released on video was split into only two episodes). In the story, the Doctor (Rowan Atkinson) encounters both the Master (Jonathan Pryce) and the Daleks. During the special the Doctor is forced to regenerate several times, with his subsequent incarnations played by, in order, Richard E. Grant, Jim Broadbent, Hugh Grant and Joanna Lumley. The script was written by Steven Moffat, later to be head writer and executive producer to the revived series.
10/29/2021 11:44:31 - INFO - __main__ - ['Curse of Fatal Death,', 'Doctor Who and the Curse of Fatal Death']
10/29/2021 11:44:31 - INFO - __main__ - Tokenizing Input ...
10/29/2021 11:44:33 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 11:44:33 - INFO - __main__ - Tokenizing Output ...
10/29/2021 11:44:34 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 11:44:34 - INFO - __main__ - Loaded 1309 examples from dev data
10/29/2021 11:44:34 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt ....
10/29/2021 11:44:34 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 11:44:34 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 11:44:34 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 11:44:42 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt .... Done!
10/29/2021 11:44:48 - INFO - __main__ - Starting inference ...
Infernece:   0%|          | 0/21 [00:00<?, ?it/s]Infernece:   5%|▍         | 1/21 [00:04<01:38,  4.90s/it]Infernece:  10%|▉         | 2/21 [00:09<01:33,  4.90s/it]Infernece:  14%|█▍        | 3/21 [00:14<01:25,  4.76s/it]Infernece:  19%|█▉        | 4/21 [00:19<01:21,  4.81s/it]Infernece:  24%|██▍       | 5/21 [00:22<01:12,  4.51s/it]Infernece:  29%|██▊       | 6/21 [00:28<01:11,  4.74s/it]Infernece:  33%|███▎      | 7/21 [00:33<01:07,  4.82s/it]Infernece:  38%|███▊      | 8/21 [00:38<01:03,  4.90s/it]Infernece:  43%|████▎     | 9/21 [00:43<00:59,  4.98s/it]Infernece:  48%|████▊     | 10/21 [00:47<00:50,  4.56s/it]Infernece:  52%|█████▏    | 11/21 [00:49<00:39,  3.98s/it]Infernece:  57%|█████▋    | 12/21 [00:53<00:34,  3.79s/it]Infernece:  62%|██████▏   | 13/21 [00:56<00:29,  3.72s/it]Infernece:  67%|██████▋   | 14/21 [00:59<00:25,  3.61s/it]Infernece:  71%|███████▏  | 15/21 [01:03<00:21,  3.56s/it]Infernece:  76%|███████▌  | 16/21 [01:06<00:17,  3.52s/it]Infernece:  81%|████████  | 17/21 [01:09<00:12,  3.16s/it]Infernece:  86%|████████▌ | 18/21 [01:12<00:09,  3.08s/it]Infernece:  90%|█████████ | 19/21 [01:14<00:06,  3.04s/it]Infernece:  95%|█████████▌| 20/21 [01:18<00:03,  3.07s/it]Infernece: 100%|██████████| 21/21 [01:20<00:00,  2.71s/it]Infernece: 100%|██████████| 21/21 [01:20<00:00,  3.81s/it]
10/29/2021 11:46:08 - INFO - __main__ - Starting inference ... Done
