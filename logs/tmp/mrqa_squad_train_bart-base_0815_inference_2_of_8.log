10/29/2021 17:15:04 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 17:15:04 - INFO - __main__ - dataset_size=86420, num_shards=8, local_shard_id=2
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 17:15:05 - INFO - __main__ - Start tokenizing ... 10803 instances
10/29/2021 17:15:05 - INFO - __main__ - Printing 3 examples
10/29/2021 17:15:05 - INFO - __main__ - Question: Who did Norman say "turned her back"? </s> Context: The book was never published. Instead the Normans whirled off to New York. Norman published the gist of his planned travel book curiously mixed with vituperation against the Ottoman Empire in an article in June, 1896, in Scribner's Magazine. The empire had descended from an enlightened civilization ruling over barbarians for their own good to something considerably less. The difference was the Hamidian Massacres, which were being conducted even as the couple traveled the Balkans. According to Norman now, the empire had been established by "the Moslem horde" from Asia, which was stopped by "intrepid Hungary." Furthermore, "Greece shook off the turbaned destroyer of her people" and so on. The Russians were suddenly liberators of oppressed Balkan states. Having portrayed the Armenians as revolutionaries in the name of freedom with the expectation of being rescued by the intervention of Christian Europe, he states "but her hope was vain." England had "turned her back." Norman concluded his exhortation with "In the Balkans one learns to hate the Turk." Norman made sure that Gladstone read the article. Prince Nicolas of Montenegro wrote a letter thanking him for his article.
10/29/2021 17:15:05 - INFO - __main__ - ['England']
10/29/2021 17:15:05 - INFO - __main__ - Question: Who works closely with the definition of the Near East? </s> Context: Working closely in conjunction with the definition of the Near East provided by the State Department is the Near East South Asia Center for Strategic Studies (NESA), an educational institution of the United States Department of Defense. It teaches courses and holds seminars and workshops for government officials and military officers who will work or are working within its region. As the name indicates, that region is a combination of State Department regions; however, NESA is careful to identify the State Department region. As its Near East is not different from the State Department's it does not appear in the table. Its name, however, is not entirely accurate. For example, its region includes Mauritania, a member of the State Department's Africa (Sub-Sahara).
10/29/2021 17:15:05 - INFO - __main__ - ['the Near East South Asia Center for Strategic Studies (NESA)']
10/29/2021 17:15:05 - INFO - __main__ - Question: What is NESA? </s> Context: Working closely in conjunction with the definition of the Near East provided by the State Department is the Near East South Asia Center for Strategic Studies (NESA), an educational institution of the United States Department of Defense. It teaches courses and holds seminars and workshops for government officials and military officers who will work or are working within its region. As the name indicates, that region is a combination of State Department regions; however, NESA is careful to identify the State Department region. As its Near East is not different from the State Department's it does not appear in the table. Its name, however, is not entirely accurate. For example, its region includes Mauritania, a member of the State Department's Africa (Sub-Sahara).
10/29/2021 17:15:05 - INFO - __main__ - ['an educational institution of the United States Department of Defense']
10/29/2021 17:15:05 - INFO - __main__ - Tokenizing Input ...
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Output ...
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 17:15:16 - INFO - __main__ - Loaded 10803 examples from dev data
10/29/2021 17:15:16 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
10/29/2021 17:15:17 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 17:15:17 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 17:15:17 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 17:15:20 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
10/29/2021 17:15:24 - INFO - __main__ - Starting inference ...
Infernece:   0%|          | 0/169 [00:00<?, ?it/s]Infernece:   1%|          | 1/169 [00:02<06:19,  2.26s/it]Infernece:   1%|          | 2/169 [00:04<06:00,  2.16s/it]Infernece:   2%|▏         | 3/169 [00:06<06:03,  2.19s/it]Infernece:   2%|▏         | 4/169 [00:09<06:36,  2.40s/it]Infernece:   3%|▎         | 5/169 [00:11<06:39,  2.44s/it]Infernece:   4%|▎         | 6/169 [00:14<06:26,  2.37s/it]Infernece:   4%|▍         | 7/169 [00:16<06:33,  2.43s/it]Infernece:   5%|▍         | 8/169 [00:19<06:41,  2.49s/it]Infernece:   5%|▌         | 9/169 [00:21<06:40,  2.50s/it]Infernece:   6%|▌         | 10/169 [00:23<05:45,  2.18s/it]Infernece:   7%|▋         | 11/169 [00:26<06:17,  2.39s/it]Infernece:   7%|▋         | 12/169 [00:27<05:34,  2.13s/it]Infernece:   8%|▊         | 13/169 [00:29<05:09,  1.99s/it]Infernece:   8%|▊         | 14/169 [00:32<05:45,  2.23s/it]Infernece:   9%|▉         | 15/169 [00:34<05:30,  2.15s/it]Infernece:   9%|▉         | 16/169 [00:36<05:29,  2.15s/it]Infernece:  10%|█         | 17/169 [00:38<05:40,  2.24s/it]Infernece:  11%|█         | 18/169 [00:40<05:31,  2.19s/it]Infernece:  11%|█         | 19/169 [00:42<05:13,  2.09s/it]Infernece:  12%|█▏        | 20/169 [00:45<05:45,  2.32s/it]Infernece:  12%|█▏        | 21/169 [00:47<05:15,  2.13s/it]Infernece:  13%|█▎        | 22/169 [00:49<05:32,  2.26s/it]Infernece:  14%|█▎        | 23/169 [00:51<05:25,  2.23s/it]Infernece:  14%|█▍        | 24/169 [00:54<05:26,  2.25s/it]Infernece:  15%|█▍        | 25/169 [00:55<05:01,  2.09s/it]Infernece:  15%|█▌        | 26/169 [00:57<04:40,  1.96s/it]Infernece:  16%|█▌        | 27/169 [00:59<04:46,  2.02s/it]Infernece:  17%|█▋        | 28/169 [01:01<04:35,  1.95s/it]Infernece:  17%|█▋        | 29/169 [01:03<04:18,  1.85s/it]Infernece:  18%|█▊        | 30/169 [01:04<04:19,  1.87s/it]Infernece:  18%|█▊        | 31/169 [01:07<04:38,  2.02s/it]Infernece:  19%|█▉        | 32/169 [01:09<04:52,  2.14s/it]Infernece:  20%|█▉        | 33/169 [01:12<05:23,  2.38s/it]Infernece:  20%|██        | 34/169 [01:15<05:36,  2.49s/it]Infernece:  21%|██        | 35/169 [01:17<05:20,  2.39s/it]Infernece:  21%|██▏       | 36/169 [01:19<05:09,  2.33s/it]Infernece:  22%|██▏       | 37/169 [01:21<04:55,  2.24s/it]Infernece:  22%|██▏       | 38/169 [01:23<04:36,  2.11s/it]Infernece:  23%|██▎       | 39/169 [01:25<04:08,  1.91s/it]Infernece:  24%|██▎       | 40/169 [01:26<04:05,  1.90s/it]Infernece:  24%|██▍       | 41/169 [01:28<03:53,  1.82s/it]Infernece:  25%|██▍       | 42/169 [01:30<03:51,  1.83s/it]Infernece:  25%|██▌       | 43/169 [01:33<04:34,  2.18s/it]Infernece:  26%|██▌       | 44/169 [01:35<04:29,  2.16s/it]Infernece:  27%|██▋       | 45/169 [01:37<04:22,  2.12s/it]Infernece:  27%|██▋       | 46/169 [01:39<04:27,  2.17s/it]Infernece:  28%|██▊       | 47/169 [01:41<04:22,  2.15s/it]Infernece:  28%|██▊       | 48/169 [01:44<04:51,  2.41s/it]Infernece:  29%|██▉       | 49/169 [01:46<04:17,  2.15s/it]Infernece:  30%|██▉       | 50/169 [01:48<04:08,  2.09s/it]Infernece:  30%|███       | 51/169 [01:51<04:25,  2.25s/it]Infernece:  31%|███       | 52/169 [01:53<04:17,  2.20s/it]Infernece:  31%|███▏      | 53/169 [01:55<04:15,  2.21s/it]Infernece:  32%|███▏      | 54/169 [01:58<04:35,  2.40s/it]Infernece:  33%|███▎      | 55/169 [02:00<04:15,  2.25s/it]Infernece:  33%|███▎      | 56/169 [02:02<04:03,  2.15s/it]Infernece:  34%|███▎      | 57/169 [02:04<03:58,  2.13s/it]Infernece:  34%|███▍      | 58/169 [02:06<04:01,  2.17s/it]Infernece:  35%|███▍      | 59/169 [02:09<04:16,  2.34s/it]Infernece:  36%|███▌      | 60/169 [02:11<04:13,  2.32s/it]Infernece:  36%|███▌      | 61/169 [02:13<04:04,  2.27s/it]Infernece:  37%|███▋      | 62/169 [02:15<04:02,  2.27s/it]Infernece:  37%|███▋      | 63/169 [02:18<04:06,  2.33s/it]Infernece:  38%|███▊      | 64/169 [02:20<04:13,  2.41s/it]Infernece:  38%|███▊      | 65/169 [02:22<04:00,  2.31s/it]Infernece:  39%|███▉      | 66/169 [02:25<03:57,  2.30s/it]Infernece:  40%|███▉      | 67/169 [02:27<03:50,  2.26s/it]Infernece:  40%|████      | 68/169 [02:30<03:57,  2.36s/it]Infernece:  41%|████      | 69/169 [02:32<03:45,  2.25s/it]Infernece:  41%|████▏     | 70/169 [02:34<03:45,  2.28s/it]Infernece:  42%|████▏     | 71/169 [02:36<03:34,  2.19s/it]Infernece:  43%|████▎     | 72/169 [02:38<03:30,  2.17s/it]Infernece:  43%|████▎     | 73/169 [02:40<03:29,  2.18s/it]Infernece:  44%|████▍     | 74/169 [02:43<03:39,  2.31s/it]Infernece:  44%|████▍     | 75/169 [02:45<03:34,  2.28s/it]Infernece:  45%|████▍     | 76/169 [02:48<03:53,  2.51s/it]Infernece:  46%|████▌     | 77/169 [02:51<03:58,  2.60s/it]Infernece:  46%|████▌     | 78/169 [02:53<03:52,  2.55s/it]Infernece:  47%|████▋     | 79/169 [02:55<03:31,  2.35s/it]Infernece:  47%|████▋     | 80/169 [02:57<03:22,  2.27s/it]Infernece:  48%|████▊     | 81/169 [02:59<03:15,  2.23s/it]Infernece:  49%|████▊     | 82/169 [03:02<03:27,  2.39s/it]Infernece:  49%|████▉     | 83/169 [03:04<03:18,  2.31s/it]Infernece:  50%|████▉     | 84/169 [03:07<03:17,  2.32s/it]Infernece:  50%|█████     | 85/169 [03:09<03:06,  2.22s/it]Infernece:  51%|█████     | 86/169 [03:11<03:10,  2.30s/it]Infernece:  51%|█████▏    | 87/169 [03:13<02:56,  2.15s/it]Infernece:  52%|█████▏    | 88/169 [03:15<02:57,  2.19s/it]Infernece:  53%|█████▎    | 89/169 [03:18<03:06,  2.33s/it]Infernece:  53%|█████▎    | 90/169 [03:20<03:05,  2.34s/it]Infernece:  54%|█████▍    | 91/169 [03:23<03:09,  2.43s/it]Infernece:  54%|█████▍    | 92/169 [03:26<03:17,  2.57s/it]Infernece:  55%|█████▌    | 93/169 [03:29<03:27,  2.73s/it]Infernece:  56%|█████▌    | 94/169 [03:31<03:06,  2.49s/it]Infernece:  56%|█████▌    | 95/169 [03:33<03:02,  2.47s/it]Infernece:  57%|█████▋    | 96/169 [03:36<02:59,  2.46s/it]Infernece:  57%|█████▋    | 97/169 [03:39<03:11,  2.66s/it]Infernece:  58%|█████▊    | 98/169 [03:41<03:06,  2.63s/it]Infernece:  59%|█████▊    | 99/169 [03:43<02:42,  2.32s/it]Infernece:  59%|█████▉    | 100/169 [03:45<02:38,  2.29s/it]Infernece:  60%|█████▉    | 101/169 [03:48<02:43,  2.41s/it]Infernece:  60%|██████    | 102/169 [03:50<02:38,  2.36s/it]Infernece:  61%|██████    | 103/169 [03:52<02:26,  2.22s/it]Infernece:  62%|██████▏   | 104/169 [03:54<02:26,  2.25s/it]Infernece:  62%|██████▏   | 105/169 [03:57<02:25,  2.27s/it]Infernece:  63%|██████▎   | 106/169 [03:59<02:23,  2.28s/it]Infernece:  63%|██████▎   | 107/169 [04:01<02:17,  2.22s/it]Infernece:  64%|██████▍   | 108/169 [04:03<02:14,  2.20s/it]Infernece:  64%|██████▍   | 109/169 [04:06<02:18,  2.31s/it]Infernece:  65%|██████▌   | 110/169 [04:08<02:18,  2.35s/it]Infernece:  66%|██████▌   | 111/169 [04:11<02:20,  2.42s/it]Infernece:  66%|██████▋   | 112/169 [04:12<02:05,  2.20s/it]Infernece:  67%|██████▋   | 113/169 [04:14<01:59,  2.14s/it]Infernece:  67%|██████▋   | 114/169 [04:16<01:52,  2.05s/it]Infernece:  68%|██████▊   | 115/169 [04:18<01:48,  2.01s/it]Infernece:  69%|██████▊   | 116/169 [04:20<01:48,  2.05s/it]Infernece:  69%|██████▉   | 117/169 [04:23<01:57,  2.26s/it]Infernece:  70%|██████▉   | 118/169 [04:26<02:05,  2.46s/it]Infernece:  70%|███████   | 119/169 [04:29<02:03,  2.48s/it]Infernece:  71%|███████   | 120/169 [04:31<01:58,  2.42s/it]Infernece:  72%|███████▏  | 121/169 [04:33<01:52,  2.33s/it]Infernece:  72%|███████▏  | 122/169 [04:35<01:51,  2.38s/it]Infernece:  73%|███████▎  | 123/169 [04:38<01:53,  2.46s/it]Infernece:  73%|███████▎  | 124/169 [04:41<02:01,  2.70s/it]Infernece:  74%|███████▍  | 125/169 [04:44<01:53,  2.59s/it]Infernece:  75%|███████▍  | 126/169 [04:47<01:59,  2.78s/it]Infernece:  75%|███████▌  | 127/169 [04:49<01:51,  2.66s/it]Infernece:  76%|███████▌  | 128/169 [04:52<01:55,  2.83s/it]Infernece:  76%|███████▋  | 129/169 [04:55<01:50,  2.76s/it]Infernece:  77%|███████▋  | 130/169 [04:58<01:46,  2.73s/it]Infernece:  78%|███████▊  | 131/169 [05:01<01:46,  2.81s/it]Infernece:  78%|███████▊  | 132/169 [05:03<01:36,  2.61s/it]Infernece:  79%|███████▊  | 133/169 [05:05<01:28,  2.45s/it]Infernece:  79%|███████▉  | 134/169 [05:08<01:29,  2.54s/it]Infernece:  80%|███████▉  | 135/169 [05:10<01:24,  2.48s/it]Infernece:  80%|████████  | 136/169 [05:12<01:20,  2.45s/it]Infernece:  81%|████████  | 137/169 [05:15<01:20,  2.51s/it]Infernece:  82%|████████▏ | 138/169 [05:18<01:21,  2.62s/it]Infernece:  82%|████████▏ | 139/169 [05:20<01:11,  2.37s/it]Infernece:  83%|████████▎ | 140/169 [05:22<01:05,  2.27s/it]Infernece:  83%|████████▎ | 141/169 [05:24<01:02,  2.24s/it]Infernece:  84%|████████▍ | 142/169 [05:26<00:56,  2.10s/it]Infernece:  85%|████████▍ | 143/169 [05:28<00:52,  2.03s/it]Infernece:  85%|████████▌ | 144/169 [05:30<00:55,  2.23s/it]Infernece:  86%|████████▌ | 145/169 [05:32<00:50,  2.09s/it]Infernece:  86%|████████▋ | 146/169 [05:34<00:48,  2.12s/it]Infernece:  87%|████████▋ | 147/169 [05:36<00:47,  2.15s/it]Infernece:  88%|████████▊ | 148/169 [05:38<00:44,  2.10s/it]Infernece:  88%|████████▊ | 149/169 [05:41<00:44,  2.22s/it]Infernece:  89%|████████▉ | 150/169 [05:44<00:44,  2.32s/it]Infernece:  89%|████████▉ | 151/169 [05:46<00:41,  2.31s/it]Infernece:  90%|████████▉ | 152/169 [05:48<00:39,  2.30s/it]Infernece:  91%|█████████ | 153/169 [05:50<00:35,  2.23s/it]Infernece:  91%|█████████ | 154/169 [05:52<00:32,  2.18s/it]Infernece:  92%|█████████▏| 155/169 [05:55<00:31,  2.25s/it]Infernece:  92%|█████████▏| 156/169 [05:58<00:33,  2.55s/it]Infernece:  93%|█████████▎| 157/169 [06:00<00:29,  2.45s/it]Infernece:  93%|█████████▎| 158/169 [06:03<00:28,  2.57s/it]Infernece:  94%|█████████▍| 159/169 [06:05<00:23,  2.39s/it]Infernece:  95%|█████████▍| 160/169 [06:07<00:20,  2.33s/it]Infernece:  95%|█████████▌| 161/169 [06:09<00:17,  2.21s/it]Infernece:  96%|█████████▌| 162/169 [06:12<00:16,  2.33s/it]Infernece:  96%|█████████▋| 163/169 [06:14<00:14,  2.47s/it]Infernece:  97%|█████████▋| 164/169 [06:16<00:11,  2.31s/it]Infernece:  98%|█████████▊| 165/169 [06:19<00:09,  2.32s/it]Infernece:  98%|█████████▊| 166/169 [06:21<00:06,  2.18s/it]Infernece:  99%|█████████▉| 167/169 [06:23<00:04,  2.14s/it]Infernece:  99%|█████████▉| 168/169 [06:25<00:02,  2.12s/it]Infernece: 100%|██████████| 169/169 [06:26<00:00,  1.94s/it]Infernece: 100%|██████████| 169/169 [06:26<00:00,  2.29s/it]
10/29/2021 17:21:51 - INFO - __main__ - Starting inference ... Done
