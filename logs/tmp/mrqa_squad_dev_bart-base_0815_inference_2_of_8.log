08/15/2021 15:49:55 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa_naturalquestions', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=4, output_dir='out/mrqa_naturalquestions_bart-base_0617v4', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
08/15/2021 15:49:55 - INFO - __main__ - dataset_size=10474, num_shards=8, local_shard_id=2
/private/home/yuchenlin/.conda/envs/bartqa/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  return array(a, dtype, copy=False, order=order)
08/15/2021 15:49:56 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
08/15/2021 15:49:56 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
08/15/2021 15:49:56 - INFO - __main__ - Start tokenizing ... 1309 instances
08/15/2021 15:49:56 - INFO - __main__ - Printing 3 examples
08/15/2021 15:49:56 - INFO - __main__ - Context: Southern California is home to many major business districts. Central business districts (CBD) include Downtown Los Angeles, Downtown San Diego, Downtown San Bernardino, Downtown Bakersfield, South Coast Metro and Downtown Riverside. | Question: What does CBD stand for?
08/15/2021 15:49:56 - INFO - __main__ - ['Central business districts']
08/15/2021 15:49:56 - INFO - __main__ - Context: Southern California is home to many major business districts. Central business districts (CBD) include Downtown Los Angeles, Downtown San Diego, Downtown San Bernardino, Downtown Bakersfield, South Coast Metro and Downtown Riverside. | Question: What is the only district in the CBD to not have "downtown" in it's name?
08/15/2021 15:49:56 - INFO - __main__ - ['South Coast Metro']
08/15/2021 15:49:56 - INFO - __main__ - Context: Southern California includes the heavily built-up urban area stretching along the Pacific coast from Ventura, through the Greater Los Angeles Area and the Inland Empire, and down to Greater San Diego. Southern California's population encompasses seven metropolitan areas, or MSAs: the Los Angeles metropolitan area, consisting of Los Angeles and Orange counties; the Inland Empire, consisting of Riverside and San Bernardino counties; the San Diego metropolitan area; the Oxnard–Thousand Oaks–Ventura metropolitan area; the Santa Barbara metro area; the San Luis Obispo metropolitan area; and the El Centro area. Out of these, three are heavy populated areas: the Los Angeles area with over 12 million inhabitants, the Riverside-San Bernardino area with over four million inhabitants, and the San Diego area with over 3 million inhabitants. For CSA metropolitan purposes, the five counties of Los Angeles, Orange, Riverside, San Bernardino, and Ventura are all combined to make up the Greater Los Angeles Area with over 17.5 million people. With over 22 million people, southern California contains roughly 60 percent of California's population. | Question: Which coastline does Southern California touch?
08/15/2021 15:49:56 - INFO - __main__ - ['Pacific']
08/15/2021 15:49:56 - INFO - __main__ - Tokenizing Input ...
08/15/2021 15:49:58 - INFO - __main__ - Tokenizing Input ... Done!
08/15/2021 15:49:58 - INFO - __main__ - Tokenizing Output ...
08/15/2021 15:49:58 - INFO - __main__ - Tokenizing Output ... Done!
08/15/2021 15:49:59 - INFO - __main__ - Loaded 1309 examples from dev data
08/15/2021 15:49:59 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt ....
08/15/2021 15:49:59 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
08/15/2021 15:49:59 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

08/15/2021 15:49:59 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
08/15/2021 15:50:07 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt .... Done!
08/15/2021 15:50:12 - INFO - __main__ - Starting inference ...
Infernece:   0%|          | 0/21 [00:00<?, ?it/s]Infernece:   5%|▍         | 1/21 [00:04<01:21,  4.09s/it]Infernece:  10%|▉         | 2/21 [00:07<01:16,  4.01s/it]Infernece:  14%|█▍        | 3/21 [00:11<01:12,  4.00s/it]Infernece:  19%|█▉        | 4/21 [00:15<01:08,  4.01s/it]Infernece:  24%|██▍       | 5/21 [00:20<01:04,  4.05s/it]Infernece:  29%|██▊       | 6/21 [00:24<01:00,  4.02s/it]Infernece:  33%|███▎      | 7/21 [00:28<00:56,  4.06s/it]Infernece:  38%|███▊      | 8/21 [00:32<00:52,  4.01s/it]Infernece:  43%|████▎     | 9/21 [00:36<00:48,  4.08s/it]Infernece:  48%|████▊     | 10/21 [00:40<00:45,  4.15s/it]Infernece:  52%|█████▏    | 11/21 [00:44<00:40,  4.09s/it]Infernece:  57%|█████▋    | 12/21 [00:48<00:36,  4.07s/it]Infernece:  62%|██████▏   | 13/21 [00:52<00:32,  4.08s/it]Infernece:  67%|██████▋   | 14/21 [00:57<00:29,  4.15s/it]Infernece:  71%|███████▏  | 15/21 [01:00<00:24,  4.09s/it]Infernece:  76%|███████▌  | 16/21 [01:04<00:19,  4.00s/it]Infernece:  81%|████████  | 17/21 [01:08<00:16,  4.02s/it]Infernece:  86%|████████▌ | 18/21 [01:12<00:12,  4.01s/it]Infernece:  90%|█████████ | 19/21 [01:16<00:08,  4.03s/it]Infernece:  95%|█████████▌| 20/21 [01:21<00:04,  4.10s/it]Infernece: 100%|██████████| 21/21 [01:23<00:00,  3.50s/it]Infernece: 100%|██████████| 21/21 [01:23<00:00,  3.96s/it]
08/15/2021 15:51:35 - INFO - __main__ - Starting inference ... Done
