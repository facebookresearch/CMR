08/15/2021 15:49:55 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa_naturalquestions', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=4, output_dir='out/mrqa_naturalquestions_bart-base_0617v4', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
08/15/2021 15:49:55 - INFO - __main__ - dataset_size=10474, num_shards=8, local_shard_id=1
/private/home/yuchenlin/.conda/envs/bartqa/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  return array(a, dtype, copy=False, order=order)
08/15/2021 15:49:56 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
08/15/2021 15:49:56 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
08/15/2021 15:49:56 - INFO - __main__ - Start tokenizing ... 1310 instances
08/15/2021 15:49:56 - INFO - __main__ - Printing 3 examples
08/15/2021 15:49:56 - INFO - __main__ - Context: In 1875, Tesla enrolled at Austrian Polytechnic in Graz, Austria, on a Military Frontier scholarship. During his first year, Tesla never missed a lecture, earned the highest grades possible, passed nine exams (nearly twice as many required), started a Serbian culture club, and even received a letter of commendation from the dean of the technical faculty to his father, which stated, "Your son is a star of first rank." Tesla claimed that he worked from 3 a.m. to 11 p.m., no Sundays or holidays excepted. He was "mortified when [his] father made light of [those] hard won honors." After his father's death in 1879, Tesla found a package of letters from his professors to his father, warning that unless he were removed from the school, Tesla would be killed through overwork. During his second year, Tesla came into conflict with Professor Poeschl over the Gramme dynamo, when Tesla suggested that commutators weren't necessary. At the end of his second year, Tesla lost his scholarship and became addicted to gambling. During his third year, Tesla gambled away his allowance and his tuition money, later gambling back his initial losses and returning the balance to his family. Tesla said that he "conquered [his] passion then and there," but later he was known to play billiards in the US. When exam time came, Tesla was unprepared and asked for an extension to study, but was denied. He never graduated from the university and did not receive grades for the last semester. | Question: When did Tesla enroll in Austrian Polytechnic?
08/15/2021 15:49:56 - INFO - __main__ - ['In 1875', '1875']
08/15/2021 15:49:56 - INFO - __main__ - Context: In 1875, Tesla enrolled at Austrian Polytechnic in Graz, Austria, on a Military Frontier scholarship. During his first year, Tesla never missed a lecture, earned the highest grades possible, passed nine exams (nearly twice as many required), started a Serbian culture club, and even received a letter of commendation from the dean of the technical faculty to his father, which stated, "Your son is a star of first rank." Tesla claimed that he worked from 3 a.m. to 11 p.m., no Sundays or holidays excepted. He was "mortified when [his] father made light of [those] hard won honors." After his father's death in 1879, Tesla found a package of letters from his professors to his father, warning that unless he were removed from the school, Tesla would be killed through overwork. During his second year, Tesla came into conflict with Professor Poeschl over the Gramme dynamo, when Tesla suggested that commutators weren't necessary. At the end of his second year, Tesla lost his scholarship and became addicted to gambling. During his third year, Tesla gambled away his allowance and his tuition money, later gambling back his initial losses and returning the balance to his family. Tesla said that he "conquered [his] passion then and there," but later he was known to play billiards in the US. When exam time came, Tesla was unprepared and asked for an extension to study, but was denied. He never graduated from the university and did not receive grades for the last semester. | Question: When did Tesla's father die?
08/15/2021 15:49:56 - INFO - __main__ - ['in 1879', '1879']
08/15/2021 15:49:56 - INFO - __main__ - Context: In 1875, Tesla enrolled at Austrian Polytechnic in Graz, Austria, on a Military Frontier scholarship. During his first year, Tesla never missed a lecture, earned the highest grades possible, passed nine exams (nearly twice as many required), started a Serbian culture club, and even received a letter of commendation from the dean of the technical faculty to his father, which stated, "Your son is a star of first rank." Tesla claimed that he worked from 3 a.m. to 11 p.m., no Sundays or holidays excepted. He was "mortified when [his] father made light of [those] hard won honors." After his father's death in 1879, Tesla found a package of letters from his professors to his father, warning that unless he were removed from the school, Tesla would be killed through overwork. During his second year, Tesla came into conflict with Professor Poeschl over the Gramme dynamo, when Tesla suggested that commutators weren't necessary. At the end of his second year, Tesla lost his scholarship and became addicted to gambling. During his third year, Tesla gambled away his allowance and his tuition money, later gambling back his initial losses and returning the balance to his family. Tesla said that he "conquered [his] passion then and there," but later he was known to play billiards in the US. When exam time came, Tesla was unprepared and asked for an extension to study, but was denied. He never graduated from the university and did not receive grades for the last semester. | Question: How did Tesla lose his tuition money?
08/15/2021 15:49:56 - INFO - __main__ - ['gambled', 'gambling']
08/15/2021 15:49:56 - INFO - __main__ - Tokenizing Input ...
08/15/2021 15:49:58 - INFO - __main__ - Tokenizing Input ... Done!
08/15/2021 15:49:58 - INFO - __main__ - Tokenizing Output ...
08/15/2021 15:49:58 - INFO - __main__ - Tokenizing Output ... Done!
08/15/2021 15:49:58 - INFO - __main__ - Loaded 1310 examples from dev data
08/15/2021 15:49:58 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt ....
08/15/2021 15:49:59 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
08/15/2021 15:49:59 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

08/15/2021 15:49:59 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
08/15/2021 15:50:07 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt .... Done!
08/15/2021 15:50:11 - INFO - __main__ - Starting inference ...
Infernece:   0%|          | 0/21 [00:00<?, ?it/s]Infernece:   5%|▍         | 1/21 [00:04<01:30,  4.51s/it]Infernece:  10%|▉         | 2/21 [00:08<01:24,  4.44s/it]Infernece:  14%|█▍        | 3/21 [00:13<01:19,  4.39s/it]Infernece:  19%|█▉        | 4/21 [00:17<01:12,  4.27s/it]Infernece:  24%|██▍       | 5/21 [00:20<01:05,  4.09s/it]Infernece:  29%|██▊       | 6/21 [00:24<01:00,  4.00s/it]Infernece:  33%|███▎      | 7/21 [00:28<00:56,  4.03s/it]Infernece:  38%|███▊      | 8/21 [00:32<00:50,  3.89s/it]Infernece:  43%|████▎     | 9/21 [00:35<00:46,  3.86s/it]Infernece:  48%|████▊     | 10/21 [00:40<00:43,  3.97s/it]Infernece:  52%|█████▏    | 11/21 [00:44<00:39,  3.99s/it]Infernece:  57%|█████▋    | 12/21 [00:48<00:36,  4.01s/it]Infernece:  62%|██████▏   | 13/21 [00:52<00:31,  3.99s/it]Infernece:  67%|██████▋   | 14/21 [00:56<00:28,  4.08s/it]Infernece:  71%|███████▏  | 15/21 [01:01<00:25,  4.27s/it]Infernece:  76%|███████▌  | 16/21 [01:05<00:20,  4.20s/it]Infernece:  81%|████████  | 17/21 [01:09<00:16,  4.18s/it]Infernece:  86%|████████▌ | 18/21 [01:13<00:12,  4.29s/it]Infernece:  90%|█████████ | 19/21 [01:18<00:08,  4.29s/it]Infernece:  95%|█████████▌| 20/21 [01:22<00:04,  4.26s/it]Infernece: 100%|██████████| 21/21 [01:24<00:00,  3.51s/it]Infernece: 100%|██████████| 21/21 [01:24<00:00,  4.01s/it]
08/15/2021 15:51:35 - INFO - __main__ - Starting inference ... Done
