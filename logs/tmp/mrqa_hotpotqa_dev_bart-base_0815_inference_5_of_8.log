10/29/2021 17:11:46 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 17:11:46 - INFO - __main__ - dataset_size=5901, num_shards=8, local_shard_id=5
10/29/2021 17:11:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 17:11:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 17:11:47 - INFO - __main__ - Start tokenizing ... 737 instances
10/29/2021 17:11:47 - INFO - __main__ - Printing 3 examples
10/29/2021 17:11:47 - INFO - __main__ - Question: Which terrorist group was the cause of the battle in which Mark E. Michell won an award for his actions? </s> Context: Mark E. Mitchell  Mark E. Mitchell is the current Acting Assistant Secretary of Defense for Special Operations and Low-Intensity Conflict of the United States Department of Defense.  A retired Colonel, Mitchell was the first member of the United States Army to be awarded the Distinguished Service Cross during the War in Afghanistan and was the first to receive the award since the Vietnam War.  In 2003, he received the award for his actions during the Battle of Qala-i-Jangi, which took place in late November – early December 2001.   Battle of Qala-i-Jangi  The Battle of Qala-i-Jangi (also incorrectly referred to as the "Battle of Mazar-i-Sharif") was a prisoner-of-war camp uprising that took place between November 25 and December 1, 2001, in northern Afghanistan, following the armed intervention by United States-led coalition forces to try to overthrow the Taliban's Islamic Emirate of Afghanistan, which it had accused of harboring al-Qaeda operatives.
10/29/2021 17:11:47 - INFO - __main__ - ['al-Qaeda']
10/29/2021 17:11:47 - INFO - __main__ - Question: What kind of music did Firework's artist make before getting into pop? </s> Context: Firework (song)  "Firework" is a song by American singer Katy Perry from her third studio album, "Teenage Dream" (2010).  Perry co-wrote the song with Ester Dean and its producers StarGate and Sandy Vee.  It is a dance-pop self-empowerment anthem with inspirational lyrics, and Perry felt it was an important song for her on "Teenage Dream".  Capitol Records released it as the album's third single on October 26, 2010.   Katy Perry  Katheryn Elizabeth Hudson (born October 25, 1984), known professionally as Katy Perry, is an American singer and songwriter.  After singing in church during her childhood, she pursued a career in gospel music as a teenager.  Perry signed with Red Hill Records and released her debut studio album "Katy Hudson" under her birth name in 2001, which was commercially unsuccessful.  She moved to Los Angeles the following year to venture into secular music after Red Hill ceased operations and she subsequently began working with producers Glen Ballard, Dr. Luke, and Max Martin.  After adopting the stage name Katy Perry and being dropped by The Island Def Jam Music Group and Columbia Records, she signed a recording contract with Capitol Records in April 2007.
10/29/2021 17:11:47 - INFO - __main__ - ['gospel']
10/29/2021 17:11:47 - INFO - __main__ - Question: What is this fictional scientifically and technologically advanced militaristic alien race, from which Sequoia descends, is called? </s> Context: Sequoia (comics)  Sequoia, (AKA Quoi and Q) is a Marvel Comics character.  He is the son of Mantis and the elder member of the Cotati contingent which was transplanted from the Kree home planet Hala to Vietnam on Earth.   Kree  The Kree, briefly known as the Ruul, are a fictional scientifically and technologically advanced militaristic alien race appearing in American comic books published by Marvel Comics.  They are native to the planet Hala in the Large Magellanic Cloud.
10/29/2021 17:11:47 - INFO - __main__ - ['the Ruul']
10/29/2021 17:11:47 - INFO - __main__ - Tokenizing Input ...
10/29/2021 17:11:48 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 17:11:48 - INFO - __main__ - Tokenizing Output ...
10/29/2021 17:11:48 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 17:11:48 - INFO - __main__ - Loaded 737 examples from dev data
10/29/2021 17:11:48 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
10/29/2021 17:11:49 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 17:11:49 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 17:11:49 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 17:11:53 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
10/29/2021 17:11:58 - INFO - __main__ - Starting inference ...
Infernece:   0%|          | 0/12 [00:00<?, ?it/s]Infernece:   8%|▊         | 1/12 [00:03<00:34,  3.17s/it]Infernece:  17%|█▋        | 2/12 [00:07<00:34,  3.46s/it]Infernece:  25%|██▌       | 3/12 [00:09<00:28,  3.13s/it]Infernece:  33%|███▎      | 4/12 [00:13<00:27,  3.44s/it]Infernece:  42%|████▏     | 5/12 [00:17<00:25,  3.63s/it]Infernece:  50%|█████     | 6/12 [00:20<00:20,  3.36s/it]Infernece:  58%|█████▊    | 7/12 [00:23<00:16,  3.23s/it]Infernece:  67%|██████▋   | 8/12 [00:26<00:12,  3.19s/it]Infernece:  75%|███████▌  | 9/12 [00:30<00:10,  3.46s/it]Infernece:  83%|████████▎ | 10/12 [00:33<00:06,  3.36s/it]Infernece:  92%|█████████▏| 11/12 [00:36<00:03,  3.10s/it]Infernece: 100%|██████████| 12/12 [00:37<00:00,  2.65s/it]Infernece: 100%|██████████| 12/12 [00:37<00:00,  3.16s/it]
10/29/2021 17:12:36 - INFO - __main__ - Starting inference ... Done
