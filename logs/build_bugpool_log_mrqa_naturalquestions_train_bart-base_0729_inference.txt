07/29/2021 10:32:15 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa_naturalquestions', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=4, output_dir='out/mrqa_naturalquestions_bart-base_0617v4', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
07/29/2021 10:32:15 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa_naturalquestions', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=4, output_dir='out/mrqa_naturalquestions_bart-base_0617v4', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
07/29/2021 10:32:16 - INFO - __main__ - dataset_size=88251, num_shards=2, local_shard_id=1
07/29/2021 10:32:16 - INFO - __main__ - dataset_size=88251, num_shards=2, local_shard_id=0
07/29/2021 10:32:17 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/29/2021 10:32:17 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/29/2021 10:32:17 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/29/2021 10:32:17 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/29/2021 10:32:17 - INFO - __main__ - Start tokenizing ... 44125 instances
07/29/2021 10:32:17 - INFO - __main__ - Printing 3 examples
07/29/2021 10:32:17 - INFO - __main__ - Context: In the United States , the episode drew 105.97 million total viewers and a total audience of 121.6 million , more than both Super Bowl XVII and the Roots miniseries . The episode surpassed the single - episode ratings record that had been set by the Dallas episode that resolved the `` Who Shot J.R. ? '' cliffhanger . From 1983 until 2010 , Goodbye , Farewell and Amen remained the most watched television broadcast in American history , passed only in total viewership ( but not in ratings or share ) in February 2010 by Super Bowl XLIV . It still stands as the most - watched finale of any television series , as well as the most - watched episode . | Question: how many viewers watched the last episode of mash ?
07/29/2021 10:32:17 - INFO - __main__ - ['105.97 million']
07/29/2021 10:32:17 - INFO - __main__ - Context: CSS was first proposed by Håkon Wium Lie on October 10 , 1994 . At the time , Lie was working with Tim Berners - Lee at CERN . Several other style sheet languages for the web were proposed around the same time , and discussions on public mailing lists and inside World Wide Web Consortium resulted in the first W3C CSS Recommendation ( CSS1 ) being released in 1996 . In particular , Bert Bos ' proposal was influential ; he became co-author of CSS1 and is regarded as co-creator of CSS . | Question: when was css first proposed as a standard by the w3c ?
07/29/2021 10:32:17 - INFO - __main__ - ['October 10 , 1994']
07/29/2021 10:32:17 - INFO - __main__ - Context: Real Madrid is the most successful club with 33 titles . The most recent club other than Real Madrid and Barcelona to win the league is Atlético Madrid in the 2013 -- 14 season . With their 30 May Copa del Rey defeat of Athletic Bilbao , Barcelona has won the Spanish version of The Double the most times , having won the league and cup in the same year six times in its history , breaking its tie with Athletic 's five . Barcelona is the only Spanish team that has won the Treble , which includes the UEFA Champions League along with the league and Copa del Rey , and the only UEFA club to have won the treble twice after accomplishing that feat in 2015 . The current champions are Real Madrid , who won the 2016 -- 17 competition . | Question: who has won the most la liga trophies ?
07/29/2021 10:32:17 - INFO - __main__ - ['Real Madrid']
07/29/2021 10:32:17 - INFO - __main__ - Start tokenizing ... 44126 instances
07/29/2021 10:32:17 - INFO - __main__ - Printing 3 examples
07/29/2021 10:32:17 - INFO - __main__ - Context: CorelDraw ( styled CorelDRAW ) is a vector graphics editor developed and marketed by Corel Corporation . It is also the name of Corel 's Graphics Suite , which bundles CorelDraw with bitmap - image editor Corel Photo - Paint as well as other graphics - related programs ( see below ) . The latest version is marketed as Graphics Suite 2017 ( equivalent to version 19 ) , and was released in April 2017 . CorelDraw is designed to edit two - dimensional images such as logos and posters . | Question: which is the latest version of corel draw ?
07/29/2021 10:32:17 - INFO - __main__ - ['April 2017']
07/29/2021 10:32:17 - INFO - __main__ - Context: `` Fore ! '' , originally an Australian interjection , is used to warn anyone standing or moving in the flight of a golf ball . The mention of the term in an 1881 Australian Golf Museum indicates that the term was in use at least as early as that period . | Question: what does the word fore mean in golf ?
07/29/2021 10:32:17 - INFO - __main__ - ['to warn anyone standing or moving in the flight of a golf ball']
07/29/2021 10:32:17 - INFO - __main__ - Context: Home Town is an American television series starring husband and wife team Ben and Erin Napier that premiered on March 21 , 2017 on HGTV . The married couple restores Southern homes in Laurel , Mississippi . | Question: where is the home town show on hgtv filmed ?
07/29/2021 10:32:17 - INFO - __main__ - ['Laurel , Mississippi']
07/29/2021 10:32:17 - INFO - __main__ - Tokenizing Input ...
07/29/2021 10:32:17 - INFO - __main__ - Tokenizing Input ...
07/29/2021 10:32:47 - INFO - __main__ - Tokenizing Input ... Done!
07/29/2021 10:32:47 - INFO - __main__ - Tokenizing Output ...
07/29/2021 10:32:49 - INFO - __main__ - Tokenizing Input ... Done!
07/29/2021 10:32:49 - INFO - __main__ - Tokenizing Output ...
07/29/2021 10:32:53 - INFO - __main__ - Tokenizing Output ... Done!
07/29/2021 10:32:54 - INFO - __main__ - Loaded 44125 examples from dev data
07/29/2021 10:32:54 - INFO - __main__ - Tokenizing Output ... Done!
07/29/2021 10:32:54 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt ....
07/29/2021 10:32:55 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/29/2021 10:32:55 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/29/2021 10:32:55 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/29/2021 10:32:55 - INFO - __main__ - Loaded 44126 examples from dev data
07/29/2021 10:32:56 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt ....
07/29/2021 10:32:56 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/29/2021 10:32:56 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/29/2021 10:32:56 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/29/2021 10:32:59 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt .... Done!
07/29/2021 10:33:00 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt .... Done!
07/29/2021 10:33:02 - INFO - __main__ - Starting inference ...
07/29/2021 10:33:03 - INFO - __main__ - Starting inference ...
07/29/2021 10:41:53 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa_naturalquestions', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=4, output_dir='out/mrqa_naturalquestions_bart-base_0617v4', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
07/29/2021 10:41:53 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa_naturalquestions', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=4, output_dir='out/mrqa_naturalquestions_bart-base_0617v4', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
07/29/2021 10:41:53 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa_naturalquestions', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=4, output_dir='out/mrqa_naturalquestions_bart-base_0617v4', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
07/29/2021 10:41:53 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa_naturalquestions', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=4, output_dir='out/mrqa_naturalquestions_bart-base_0617v4', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
07/29/2021 10:41:53 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa_naturalquestions', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=4, output_dir='out/mrqa_naturalquestions_bart-base_0617v4', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
07/29/2021 10:41:53 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa_naturalquestions', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=4, output_dir='out/mrqa_naturalquestions_bart-base_0617v4', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
07/29/2021 10:41:53 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa_naturalquestions', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=4, output_dir='out/mrqa_naturalquestions_bart-base_0617v4', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', prefix='', quiet=False, seed=42, train_file='')
07/29/2021 10:41:53 - INFO - __main__ - dataset_size=88251, num_shards=7, local_shard_id=6
07/29/2021 10:41:53 - INFO - __main__ - dataset_size=88251, num_shards=7, local_shard_id=3
07/29/2021 10:41:53 - INFO - __main__ - dataset_size=88251, num_shards=7, local_shard_id=4
07/29/2021 10:41:54 - INFO - __main__ - dataset_size=88251, num_shards=7, local_shard_id=0
07/29/2021 10:41:54 - INFO - __main__ - dataset_size=88251, num_shards=7, local_shard_id=2
07/29/2021 10:41:54 - INFO - __main__ - dataset_size=88251, num_shards=7, local_shard_id=5
07/29/2021 10:41:54 - INFO - __main__ - dataset_size=88251, num_shards=7, local_shard_id=1
07/29/2021 10:41:54 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/29/2021 10:41:54 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/29/2021 10:41:54 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/29/2021 10:41:54 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/29/2021 10:41:54 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/29/2021 10:41:54 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/29/2021 10:41:54 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/29/2021 10:41:54 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/29/2021 10:41:54 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/29/2021 10:41:54 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/29/2021 10:41:54 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/29/2021 10:41:54 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/29/2021 10:41:54 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/29/2021 10:41:54 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/29/2021 10:41:54 - INFO - __main__ - Start tokenizing ... 12607 instances
07/29/2021 10:41:54 - INFO - __main__ - Printing 3 examples
07/29/2021 10:41:54 - INFO - __main__ - Context: Floyd Mayweather Jr. vs. Conor McGregor , also known as `` The Money Fight '' and `` The Biggest Fight in Combat Sports History '' , was a professional boxing match between undefeated eleven - time five - division boxing world champion Floyd Mayweather Jr. and two - division mixed martial arts ( MMA ) world champion and at - the - time current UFC Lightweight Champion Conor McGregor . The match took place at the T - Mobile Arena in Paradise , Nevada , on August 26 , 2017 at the light - middleweight weight class ( 154 lbs ; 69.9 kgs ) . It was scheduled for twelve rounds . | Question: where is floyd mayweather and conor mcgregor fighting at ?
07/29/2021 10:41:54 - INFO - __main__ - ['T - Mobile Arena in Paradise , Nevada']
07/29/2021 10:41:54 - INFO - __main__ - Context: India 's Rohit Sharma and New Zealand 's Colin Munro lead the list with three T20I centuries , followed by Munro 's compatriots Martin Guptill and Brendon McCullum , West Indians Chris Gayle and Evin Lewis , India 's K.L. Rahul and Australia 's Glenn Maxwell and Aaron Finch with two each . Lewis ' first century came during the 2016 series against India at the Central Broward Regional Park in Lauderhill , Florida . In reply , India 's Rahul finished on 110 not out , the only occasion where two T20I centuries were scored in the same match . Rahul 's innings was one of the ten instances where a batsman scored a century in the second innings of a T20I match . In July 2018 , Finch posted 172 from 76 balls against Zimbabwe during the 2018 Zimbabwe Tri-Nation Series to break his own record for the highest score in a T20I match , elipsing the 156 he set in August 2013 . Rohit Sharma and David Miller of South Africa share the record for the fastest century , both reaching the milestone from 35 deliveries . Miller was also the first player to score a T20I century batting at number five or lower . | Question: who scored the fastest century in international t20 cricket ?
07/29/2021 10:41:54 - INFO - __main__ - ['Rohit Sharma', 'David Miller']
07/29/2021 10:41:54 - INFO - __main__ - Context: It is also a placement next to God in Heaven , in the traditional place of honor , mentioned in the New Testament as the place of Christ at Mark 16 : 19 , Luke 22 : 69 , Matthew 22 : 44 and 26 : 64 , Acts 2 : 34 and 7 : 55 , 1 Peter 3 : 22 and elsewhere . These uses reflect use of the phrase in the Old Testament , for example in Psalms 63 : 8 and 110 : 1 . The implications of this anthropomorphic phrasing have been discussed at length by theologians , including Saint Thomas Aquinas . | Question: who is seated at the right hand of god ?
07/29/2021 10:41:54 - INFO - __main__ - ['Christ']
07/29/2021 10:41:54 - INFO - __main__ - Start tokenizing ... 12607 instances
07/29/2021 10:41:54 - INFO - __main__ - Printing 3 examples
07/29/2021 10:41:54 - INFO - __main__ - Context: Linda McFly ( portrayed by Wendie Jo Sperber ) is the middle child and only daughter of George and Lorraine McFly . In 1985 before Marty went to 1955 , Linda is having boy trouble and it is unknown if she is in college or has a job . In 1985 after Marty went to 1955 , Linda works in a boutique and has gained the attention of many boys . | Question: who plays marty 's sister in back to the future ?
07/29/2021 10:41:54 - INFO - __main__ - ['Wendie Jo Sperber']
07/29/2021 10:41:54 - INFO - __main__ - Context: First century BC Romans used sprung wagons for overland journeys . It is likely that Roman carriages employed some form of suspension on chains or leather straps , as indicated by carriage parts found in excavations . | Question: when was the first horse drawn carriage invented ?
07/29/2021 10:41:54 - INFO - __main__ - ['First century BC']
07/29/2021 10:41:54 - INFO - __main__ - Context: The function of the radioulnar joint is to lift and maneuver weight load from the distal radioulnar joint to be distributed across the forearm 's radius and ulna as a load - bearing joint . Supination of the radioulnar joint can move from 0 degrees neutral to approximately 80 - 90 degrees where Pronation of the Radioulnar Joint can move from 0 degrees neutral to approximately 70 - 90 degrees . Supination ( palms facing up ) vs. pronation ( palms facing down ) . Muscles that contribute to function are all supinator ( Biceps Brachii , Brachioradialis , and Supinator ) and pronator muscles ( Brachioradialis , Pronator Quadratus , and Pronator Teres ) . | Question: what kind of joint is the radioulnar joint ?
07/29/2021 10:41:54 - INFO - __main__ - ['load - bearing joint']
07/29/2021 10:41:54 - INFO - __main__ - Tokenizing Input ...
07/29/2021 10:41:54 - INFO - __main__ - Tokenizing Input ...
07/29/2021 10:41:54 - INFO - __main__ - Start tokenizing ... 12607 instances
07/29/2021 10:41:54 - INFO - __main__ - Printing 3 examples
07/29/2021 10:41:54 - INFO - __main__ - Context: On March 23 , 2017 , CBS renewed the series for the currently airing eighth season . As of March 30 , 2018 , 172 episodes of Blue Bloods have aired . | Question: what season of blue bloods are we in ?
07/29/2021 10:41:54 - INFO - __main__ - ['eighth season']
07/29/2021 10:41:54 - INFO - __main__ - Context: `` The Way You Look Tonight '' is a song from the film Swing Time , written by Dorothy Fields and Jerome Kern , and originally performed by Fred Astaire . It won the Academy Award for Best Original Song in 1936 . In 2004 the Astaire version finished at # 43 in AFI 's 100 Years ... 100 Songs survey of top tunes in American cinema . | Question: who sang the way you look tonight first ?
07/29/2021 10:41:54 - INFO - __main__ - ['Fred Astaire']
07/29/2021 10:41:54 - INFO - __main__ - Context: The winner of the first season of RuPaul 's Drag Race All Stars was Chad Michaels , with Raven being the runner - up . | Question: who won season 1 of rupaul 's drag race all stars ?
07/29/2021 10:41:54 - INFO - __main__ - ['Chad Michaels']
07/29/2021 10:41:54 - INFO - __main__ - Start tokenizing ... 12607 instances
07/29/2021 10:41:54 - INFO - __main__ - Printing 3 examples
07/29/2021 10:41:54 - INFO - __main__ - Context: `` The Fighter '' is a duet between New Zealand - born Australian country music singer Keith Urban and American country music singer Carrie Underwood . It was released on 6 February 2017 as the fifth single from Urban 's ninth studio album , Ripcord , through Hit Red and Capitol Nashville . Urban co-wrote and co-produced the song with busbee . | Question: when did the song the fighter come out ?
07/29/2021 10:41:54 - INFO - __main__ - ['6 February 2017']
07/29/2021 10:41:54 - INFO - __main__ - Context: Filtration is any of various mechanical , physical or biological operations that separate solids from fluids ( liquids or gases ) by adding a medium through which only the fluid can pass . The fluid that passes through is called the filtrate . In physical filters oversize solids in the fluid are retained and in biological filters particulates are trapped and ingested and metabolites are retained and removed . However , the separation is not complete ; solids will be contaminated with some fluid and filtrate will contain fine particles ( depending on the pore size , filter thickness and biological activity ) . Filtration occurs both in nature and in engineered systems ; there are biological , geological , and industrial forms . For example , in animals ( including humans ) , renal filtration removes wastes from the blood , and in water treatment and sewage treatment , undesirable constituents are removed by absorption into a biological film grown on or in the filter medium , as in slow sand filtration . | Question: property of earth to allow water to pass through it is known as ?
07/29/2021 10:41:54 - INFO - __main__ - ['Filtration']
07/29/2021 10:41:54 - INFO - __main__ - Context: Ethos ( / ˈiːθɒs / or US : / ˈiːθoʊs / ) is a Greek word meaning `` character '' that is used to describe the guiding beliefs or ideals that characterize a community , nation , or ideology . The Greeks also used this word to refer to the power of music to influence emotions , behaviours , and even morals . Early Greek stories of Orpheus exhibit this idea in a compelling way . The word 's use in rhetoric is closely based on the Greek terminology used by Aristotle in his concept of the three artistic proofs . | Question: ethics comes from the greek word ' ethos ' which means ?
07/29/2021 10:41:54 - INFO - __main__ - ["`` character ''"]
07/29/2021 10:41:54 - INFO - __main__ - Start tokenizing ... 12608 instances
07/29/2021 10:41:54 - INFO - __main__ - Printing 3 examples
07/29/2021 10:41:54 - INFO - __main__ - Context: CorelDraw ( styled CorelDRAW ) is a vector graphics editor developed and marketed by Corel Corporation . It is also the name of Corel 's Graphics Suite , which bundles CorelDraw with bitmap - image editor Corel Photo - Paint as well as other graphics - related programs ( see below ) . The latest version is marketed as Graphics Suite 2017 ( equivalent to version 19 ) , and was released in April 2017 . CorelDraw is designed to edit two - dimensional images such as logos and posters . | Question: which is the latest version of corel draw ?
07/29/2021 10:41:54 - INFO - __main__ - ['April 2017']
07/29/2021 10:41:54 - INFO - __main__ - Context: `` Fore ! '' , originally an Australian interjection , is used to warn anyone standing or moving in the flight of a golf ball . The mention of the term in an 1881 Australian Golf Museum indicates that the term was in use at least as early as that period . | Question: what does the word fore mean in golf ?
07/29/2021 10:41:54 - INFO - __main__ - ['to warn anyone standing or moving in the flight of a golf ball']
07/29/2021 10:41:54 - INFO - __main__ - Context: Home Town is an American television series starring husband and wife team Ben and Erin Napier that premiered on March 21 , 2017 on HGTV . The married couple restores Southern homes in Laurel , Mississippi . | Question: where is the home town show on hgtv filmed ?
07/29/2021 10:41:54 - INFO - __main__ - ['Laurel , Mississippi']
07/29/2021 10:41:54 - INFO - __main__ - Tokenizing Input ...
07/29/2021 10:41:54 - INFO - __main__ - Tokenizing Input ...
07/29/2021 10:41:54 - INFO - __main__ - Tokenizing Input ...
07/29/2021 10:41:54 - INFO - __main__ - Start tokenizing ... 12607 instances
07/29/2021 10:41:54 - INFO - __main__ - Printing 3 examples
07/29/2021 10:41:54 - INFO - __main__ - Context: Sloths are arboreal mammals noted for slowness of movement and for spending most of their lives hanging upside down in the trees of the tropical rainforests of South America and Central America . The six species are in two families : two - toed sloths and three - toed sloths . In spite of this traditional naming , all sloths actually have three toes . The two - toed sloths have two digits , or fingers , on each forelimb . | Question: where do sloths spend most of their time ?
07/29/2021 10:41:54 - INFO - __main__ - ['trees']
07/29/2021 10:41:54 - INFO - __main__ - Context: Geology ( from the Ancient Greek γῆ , gē , i.e. `` earth '' and - λoγία , - logia , i.e. `` study of , discourse '' ) is an earth science concerned with the solid Earth , the rocks of which it is composed , and the processes by which they change over time . Geology can also refer to the study of the solid features of any terrestrial planet or natural satellite , ( such as Mars or the Moon ) . | Question: what is used in nature to provide change over time ?
07/29/2021 10:41:54 - INFO - __main__ - ['Geology']
07/29/2021 10:41:54 - INFO - __main__ - Context: The United Kingdom is a constitutional monarchy in which the reigning monarch ( that is , the King or Queen who is the Head of State at any given time ) does not make any open political decisions . All political decisions are taken by the government and Parliament . This constitutional state of affairs is the result of a long history of constraining and reducing the political power of the monarch , beginning with the Magna Carta in 1215 . | Question: what is the type of government in england ?
07/29/2021 10:41:54 - INFO - __main__ - ['constitutional monarchy']
07/29/2021 10:41:54 - INFO - __main__ - Start tokenizing ... 12608 instances
07/29/2021 10:41:54 - INFO - __main__ - Printing 3 examples
07/29/2021 10:41:54 - INFO - __main__ - Context: The Greatest Showman is a 2017 American musical film directed by Michael Gracey in his directorial debut , written by Jenny Bicks and Bill Condon and starring Hugh Jackman , Zac Efron , Michelle Williams , Rebecca Ferguson , and Zendaya . The film is inspired by the story of P.T. Barnum 's creation of the Barnum & Bailey Circus and the lives of its star attractions . | Question: who was the greatest showman based off of ?
07/29/2021 10:41:54 - INFO - __main__ - ['P.T. Barnum']
07/29/2021 10:41:54 - INFO - __main__ - Context: Originally released to theaters on January 25 , 1961 , by Buena Vista Distribution , One Hundred and One Dalmatians was a box office success , pulling the studio out of the financial slump caused by Sleeping Beauty , a costlier production released two years prior . Aside from its box office revenue , its commercial success was due to the employment of inexpensive animation techniques -- such as using xerography during the process of inking and painting traditional animation cels -- that kept production costs down . It was reissued to cinemas four times in 1969 , 1979 , 1985 and 1991 . The 1991 reissue was the twentieth highest earning film of the year for domestic earnings . It was remade into a live action film in 1996 . | Question: when did the movie 101 dalmatians come out ?
07/29/2021 10:41:54 - INFO - __main__ - ['January 25 , 1961']
07/29/2021 10:41:54 - INFO - __main__ - Context: `` Separation of church and state '' is paraphrased from Thomas Jefferson and used by others in expressing an understanding of the intent and function of the Establishment Clause and Free Exercise Clause of the First Amendment to the Constitution of the United States which reads : `` Congress shall make no law respecting an establishment of religion , or prohibiting the free exercise thereof ... '' | Question: the right to choose your own religions or no religion at all is part of the -- amendment ?
07/29/2021 10:41:54 - INFO - __main__ - ['First']
07/29/2021 10:41:55 - INFO - __main__ - Tokenizing Input ...
07/29/2021 10:41:55 - INFO - __main__ - Tokenizing Input ...
07/29/2021 10:42:08 - INFO - __main__ - Tokenizing Input ... Done!
07/29/2021 10:42:08 - INFO - __main__ - Tokenizing Output ...
07/29/2021 10:42:08 - INFO - __main__ - Tokenizing Input ... Done!
07/29/2021 10:42:08 - INFO - __main__ - Tokenizing Output ...
07/29/2021 10:42:08 - INFO - __main__ - Tokenizing Input ... Done!
07/29/2021 10:42:08 - INFO - __main__ - Tokenizing Output ...
07/29/2021 10:42:08 - INFO - __main__ - Tokenizing Input ... Done!
07/29/2021 10:42:08 - INFO - __main__ - Tokenizing Output ...
07/29/2021 10:42:08 - INFO - __main__ - Tokenizing Input ... Done!
07/29/2021 10:42:08 - INFO - __main__ - Tokenizing Output ...
07/29/2021 10:42:09 - INFO - __main__ - Tokenizing Input ... Done!
07/29/2021 10:42:09 - INFO - __main__ - Tokenizing Output ...
07/29/2021 10:42:09 - INFO - __main__ - Tokenizing Input ... Done!
07/29/2021 10:42:09 - INFO - __main__ - Tokenizing Output ...
07/29/2021 10:42:10 - INFO - __main__ - Tokenizing Output ... Done!
07/29/2021 10:42:10 - INFO - __main__ - Tokenizing Output ... Done!
07/29/2021 10:42:10 - INFO - __main__ - Tokenizing Output ... Done!
07/29/2021 10:42:10 - INFO - __main__ - Tokenizing Output ... Done!
07/29/2021 10:42:10 - INFO - __main__ - Tokenizing Output ... Done!
07/29/2021 10:42:10 - INFO - __main__ - Loaded 12607 examples from dev data
07/29/2021 10:42:10 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt ....
07/29/2021 10:42:10 - INFO - __main__ - Loaded 12608 examples from dev data
07/29/2021 10:42:10 - INFO - __main__ - Tokenizing Output ... Done!
07/29/2021 10:42:10 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt ....
07/29/2021 10:42:10 - INFO - __main__ - Loaded 12607 examples from dev data
07/29/2021 10:42:10 - INFO - __main__ - Loaded 12607 examples from dev data
07/29/2021 10:42:10 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt ....
07/29/2021 10:42:11 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt ....
07/29/2021 10:42:11 - INFO - __main__ - Loaded 12607 examples from dev data
07/29/2021 10:42:11 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt ....
07/29/2021 10:42:11 - INFO - __main__ - Tokenizing Output ... Done!
07/29/2021 10:42:11 - INFO - __main__ - Loaded 12607 examples from dev data
07/29/2021 10:42:11 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt ....
07/29/2021 10:42:11 - INFO - __main__ - Loaded 12608 examples from dev data
07/29/2021 10:42:11 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt ....
07/29/2021 10:42:14 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/29/2021 10:42:14 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/29/2021 10:42:14 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/29/2021 10:42:14 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/29/2021 10:42:14 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/29/2021 10:42:14 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/29/2021 10:42:14 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/29/2021 10:42:14 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/29/2021 10:42:14 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/29/2021 10:42:14 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/29/2021 10:42:14 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/29/2021 10:42:14 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/29/2021 10:42:14 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/29/2021 10:42:14 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/29/2021 10:42:14 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/29/2021 10:42:14 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/29/2021 10:42:14 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/29/2021 10:42:14 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/29/2021 10:42:14 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/29/2021 10:42:14 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/29/2021 10:42:14 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/29/2021 10:42:21 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt .... Done!
07/29/2021 10:42:22 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt .... Done!
07/29/2021 10:42:22 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt .... Done!
07/29/2021 10:42:23 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt .... Done!
07/29/2021 10:42:23 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt .... Done!
07/29/2021 10:42:23 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt .... Done!
07/29/2021 10:42:23 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt .... Done!
07/29/2021 10:42:28 - INFO - __main__ - Starting inference ...
07/29/2021 10:42:28 - INFO - __main__ - Starting inference ...
07/29/2021 10:42:28 - INFO - __main__ - Starting inference ...
07/29/2021 10:42:28 - INFO - __main__ - Starting inference ...
07/29/2021 10:42:28 - INFO - __main__ - Starting inference ...
07/29/2021 10:42:28 - INFO - __main__ - Starting inference ...
07/29/2021 10:42:28 - INFO - __main__ - Starting inference ...
07/29/2021 11:02:36 - INFO - __main__ - Starting inference ... Done
07/29/2021 11:02:39 - INFO - __main__ - Starting inference ... Done
07/29/2021 11:02:44 - INFO - __main__ - Starting inference ... Done
07/29/2021 11:02:54 - INFO - __main__ - Starting inference ... Done
07/29/2021 11:03:01 - INFO - __main__ - Starting inference ... Done
07/29/2021 11:03:41 - INFO - __main__ - Starting inference ... Done
07/29/2021 11:03:45 - INFO - __main__ - Starting inference ... Done
07/29/2021 11:10:43 - INFO - root - Evaluation results: {'EM': 0.859344370035467, 'QA-F1': 0.9337433033424515}
