10/29/2021 17:15:04 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 17:15:04 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 17:15:04 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 17:15:04 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 17:15:04 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 17:15:04 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 17:15:04 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 17:15:04 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 17:15:04 - INFO - __main__ - dataset_size=86420, num_shards=8, local_shard_id=6
10/29/2021 17:15:04 - INFO - __main__ - dataset_size=86420, num_shards=8, local_shard_id=5
10/29/2021 17:15:04 - INFO - __main__ - dataset_size=86420, num_shards=8, local_shard_id=0
10/29/2021 17:15:04 - INFO - __main__ - dataset_size=86420, num_shards=8, local_shard_id=4
10/29/2021 17:15:04 - INFO - __main__ - dataset_size=86420, num_shards=8, local_shard_id=7
10/29/2021 17:15:04 - INFO - __main__ - dataset_size=86420, num_shards=8, local_shard_id=2
10/29/2021 17:15:04 - INFO - __main__ - dataset_size=86420, num_shards=8, local_shard_id=1
10/29/2021 17:15:05 - INFO - __main__ - dataset_size=86420, num_shards=8, local_shard_id=3
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 17:15:05 - INFO - __main__ - Start tokenizing ... 10802 instances
10/29/2021 17:15:05 - INFO - __main__ - Printing 3 examples
10/29/2021 17:15:05 - INFO - __main__ - Question: About how long will a TB patient receiving effective treatment stay contagious? </s> Context: People with prolonged, frequent, or close contact with people with TB are at particularly high risk of becoming infected, with an estimated 22% infection rate. A person with active but untreated tuberculosis may infect 10–15 (or more) other people per year. Transmission should occur from only people with active TB – those with latent infection are not thought to be contagious. The probability of transmission from one person to another depends upon several factors, including the number of infectious droplets expelled by the carrier, the effectiveness of ventilation, the duration of exposure, the virulence of the M. tuberculosis strain, the level of immunity in the uninfected person, and others. The cascade of person-to-person spread can be circumvented by segregating those with active ("overt") TB and putting them on anti-TB drug regimens. After about two weeks of effective treatment, subjects with nonresistant active infections generally do not remain contagious to others. If someone does become infected, it typically takes three to four weeks before the newly infected person becomes infectious enough to transmit the disease to others.
10/29/2021 17:15:05 - INFO - __main__ - ['two weeks']
10/29/2021 17:15:05 - INFO - __main__ - Question: If you contracted tuberculosis today, what's the estimated gestation period before you could spread the infection to others? </s> Context: People with prolonged, frequent, or close contact with people with TB are at particularly high risk of becoming infected, with an estimated 22% infection rate. A person with active but untreated tuberculosis may infect 10–15 (or more) other people per year. Transmission should occur from only people with active TB – those with latent infection are not thought to be contagious. The probability of transmission from one person to another depends upon several factors, including the number of infectious droplets expelled by the carrier, the effectiveness of ventilation, the duration of exposure, the virulence of the M. tuberculosis strain, the level of immunity in the uninfected person, and others. The cascade of person-to-person spread can be circumvented by segregating those with active ("overt") TB and putting them on anti-TB drug regimens. After about two weeks of effective treatment, subjects with nonresistant active infections generally do not remain contagious to others. If someone does become infected, it typically takes three to four weeks before the newly infected person becomes infectious enough to transmit the disease to others.
10/29/2021 17:15:05 - INFO - __main__ - ['three to four weeks']
10/29/2021 17:15:05 - INFO - __main__ - Question: What term is interchangeable with "active" when talking about tuberculosis infection? </s> Context: People with prolonged, frequent, or close contact with people with TB are at particularly high risk of becoming infected, with an estimated 22% infection rate. A person with active but untreated tuberculosis may infect 10–15 (or more) other people per year. Transmission should occur from only people with active TB – those with latent infection are not thought to be contagious. The probability of transmission from one person to another depends upon several factors, including the number of infectious droplets expelled by the carrier, the effectiveness of ventilation, the duration of exposure, the virulence of the M. tuberculosis strain, the level of immunity in the uninfected person, and others. The cascade of person-to-person spread can be circumvented by segregating those with active ("overt") TB and putting them on anti-TB drug regimens. After about two weeks of effective treatment, subjects with nonresistant active infections generally do not remain contagious to others. If someone does become infected, it typically takes three to four weeks before the newly infected person becomes infectious enough to transmit the disease to others.
10/29/2021 17:15:05 - INFO - __main__ - ['overt']
10/29/2021 17:15:05 - INFO - __main__ - Start tokenizing ... 10802 instances
10/29/2021 17:15:05 - INFO - __main__ - Printing 3 examples
10/29/2021 17:15:05 - INFO - __main__ - Question: Which group created the legalism, the supervision of the orders of the gods, and the demand for moderation and harmony? </s> Context: From his eastern origin Apollo brought the art of inspection of "symbols and omina" (σημεία και τέρατα : semeia kai terata), and of the observation of the omens of the days. The inspiration oracular-cult was probably introduced from Anatolia. The ritualism belonged to Apollo from the beginning. The Greeks created the legalism, the supervision of the orders of the gods, and the demand for moderation and harmony. Apollo became the god of shining youth, the protector of music, spiritual-life, moderation and perceptible order. The improvement of the old Anatolian god, and his elevation to an intellectual sphere, may be considered an achievement of the Greek people.
10/29/2021 17:15:05 - INFO - __main__ - ['The Greeks']
10/29/2021 17:15:05 - INFO - __main__ - Question: Who is Apollo's sister? </s> Context: Apollo and his sister Artemis can bring death with their arrows. The conception that diseases and death come from invisible shots sent by supernatural beings, or magicians is common in Germanic and Norse mythology. In Greek mythology Artemis was the leader (ἡγεμών, "hegemon") of the nymphs, who had similar functions with the Nordic Elves. The "elf-shot" originally indicated disease or death attributed to the elves, but it was later attested denoting stone arrow-heads which were used by witches to harm people, and also for healing rituals.
10/29/2021 17:15:05 - INFO - __main__ - ['Artemis']
10/29/2021 17:15:05 - INFO - __main__ - Question: It was believed that this woman could bring death with her arrows. ? </s> Context: Apollo and his sister Artemis can bring death with their arrows. The conception that diseases and death come from invisible shots sent by supernatural beings, or magicians is common in Germanic and Norse mythology. In Greek mythology Artemis was the leader (ἡγεμών, "hegemon") of the nymphs, who had similar functions with the Nordic Elves. The "elf-shot" originally indicated disease or death attributed to the elves, but it was later attested denoting stone arrow-heads which were used by witches to harm people, and also for healing rituals.
10/29/2021 17:15:05 - INFO - __main__ - ['Artemis']
10/29/2021 17:15:05 - INFO - __main__ - Tokenizing Input ...
10/29/2021 17:15:05 - INFO - __main__ - Start tokenizing ... 10803 instances
10/29/2021 17:15:05 - INFO - __main__ - Printing 3 examples
10/29/2021 17:15:05 - INFO - __main__ - Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? </s> Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.
10/29/2021 17:15:05 - INFO - __main__ - ['Saint Bernadette Soubirous']
10/29/2021 17:15:05 - INFO - __main__ - Question: What is in front of the Notre Dame Main Building? </s> Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.
10/29/2021 17:15:05 - INFO - __main__ - ['a copper statue of Christ']
10/29/2021 17:15:05 - INFO - __main__ - Question: The Basilica of the Sacred heart at Notre Dame is beside to which structure? </s> Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.
10/29/2021 17:15:05 - INFO - __main__ - ['the Main Building']
10/29/2021 17:15:05 - INFO - __main__ - Tokenizing Input ...
10/29/2021 17:15:05 - INFO - __main__ - Tokenizing Input ...
10/29/2021 17:15:05 - INFO - __main__ - Start tokenizing ... 10802 instances
10/29/2021 17:15:05 - INFO - __main__ - Printing 3 examples
10/29/2021 17:15:05 - INFO - __main__ - Question: In what county is New Haven located in? </s> Context: New Haven (local /nuː ˈheɪvən/, noo-HAY-vən), in the U.S. state of Connecticut, is the principal municipality in Greater New Haven, which had a total population of 862,477 in 2010. It is located on New Haven Harbor on the northern shore of the Long Island Sound in New Haven County, Connecticut, which in turn comprises the outer limits of the New York metropolitan area. It is the second-largest city in Connecticut (after Bridgeport), with a population of 129,779 people as of the 2010 United States Census. According to a census of 1 July 2012, by the Census Bureau, the city had a population of 130,741.
10/29/2021 17:15:05 - INFO - __main__ - ['New Haven County,']
10/29/2021 17:15:05 - INFO - __main__ - Question: The population of the Greater New Haven in 2010? </s> Context: New Haven (local /nuː ˈheɪvən/, noo-HAY-vən), in the U.S. state of Connecticut, is the principal municipality in Greater New Haven, which had a total population of 862,477 in 2010. It is located on New Haven Harbor on the northern shore of the Long Island Sound in New Haven County, Connecticut, which in turn comprises the outer limits of the New York metropolitan area. It is the second-largest city in Connecticut (after Bridgeport), with a population of 129,779 people as of the 2010 United States Census. According to a census of 1 July 2012, by the Census Bureau, the city had a population of 130,741.
10/29/2021 17:15:05 - INFO - __main__ - ['862,477']
10/29/2021 17:15:05 - INFO - __main__ - Question: What is the metropolitan next to New Haven County, Connecticut? </s> Context: New Haven (local /nuː ˈheɪvən/, noo-HAY-vən), in the U.S. state of Connecticut, is the principal municipality in Greater New Haven, which had a total population of 862,477 in 2010. It is located on New Haven Harbor on the northern shore of the Long Island Sound in New Haven County, Connecticut, which in turn comprises the outer limits of the New York metropolitan area. It is the second-largest city in Connecticut (after Bridgeport), with a population of 129,779 people as of the 2010 United States Census. According to a census of 1 July 2012, by the Census Bureau, the city had a population of 130,741.
10/29/2021 17:15:05 - INFO - __main__ - ['New York']
10/29/2021 17:15:05 - INFO - __main__ - Start tokenizing ... 10802 instances
10/29/2021 17:15:05 - INFO - __main__ - Printing 3 examples
10/29/2021 17:15:05 - INFO - __main__ - Question: What nature preserve is in Casas Adobes? </s> Context: The community of Casas Adobes is also on the Northwest Side, with the distinction of being Tucson's first suburb, established in the late 1940s. Casas Adobes is centered on the historic Casas Adobes Plaza (built in 1948). Casas Adobes is also home to Tohono Chul Park (a nature preserve) near the intersection of North Oracle Road and West Ina Road. The attempted assassination of Representative Gabrielle Giffords, and the murders of chief judge for the U.S. District Court for Arizona, John Roll and five other people on January 8, 2011, occurred at the La Toscana Village in Casas Adobes. The Foothills Mall is also located on the northwest side in Casas Adobes.
10/29/2021 17:15:05 - INFO - __main__ - ['Tohono Chul Park']
10/29/2021 17:15:05 - INFO - __main__ - Question: What mall is located in Casas Adobes? </s> Context: The community of Casas Adobes is also on the Northwest Side, with the distinction of being Tucson's first suburb, established in the late 1940s. Casas Adobes is centered on the historic Casas Adobes Plaza (built in 1948). Casas Adobes is also home to Tohono Chul Park (a nature preserve) near the intersection of North Oracle Road and West Ina Road. The attempted assassination of Representative Gabrielle Giffords, and the murders of chief judge for the U.S. District Court for Arizona, John Roll and five other people on January 8, 2011, occurred at the La Toscana Village in Casas Adobes. The Foothills Mall is also located on the northwest side in Casas Adobes.
10/29/2021 17:15:05 - INFO - __main__ - ['Foothills Mall']
10/29/2021 17:15:05 - INFO - __main__ - Question: When was the Casas Adobes Plaza built? </s> Context: The community of Casas Adobes is also on the Northwest Side, with the distinction of being Tucson's first suburb, established in the late 1940s. Casas Adobes is centered on the historic Casas Adobes Plaza (built in 1948). Casas Adobes is also home to Tohono Chul Park (a nature preserve) near the intersection of North Oracle Road and West Ina Road. The attempted assassination of Representative Gabrielle Giffords, and the murders of chief judge for the U.S. District Court for Arizona, John Roll and five other people on January 8, 2011, occurred at the La Toscana Village in Casas Adobes. The Foothills Mall is also located on the northwest side in Casas Adobes.
10/29/2021 17:15:05 - INFO - __main__ - ['1948']
10/29/2021 17:15:05 - INFO - __main__ - Tokenizing Input ...
10/29/2021 17:15:05 - INFO - __main__ - Tokenizing Input ...
10/29/2021 17:15:05 - INFO - __main__ - Start tokenizing ... 10803 instances
10/29/2021 17:15:05 - INFO - __main__ - Printing 3 examples
10/29/2021 17:15:05 - INFO - __main__ - Question: Who did Norman say "turned her back"? </s> Context: The book was never published. Instead the Normans whirled off to New York. Norman published the gist of his planned travel book curiously mixed with vituperation against the Ottoman Empire in an article in June, 1896, in Scribner's Magazine. The empire had descended from an enlightened civilization ruling over barbarians for their own good to something considerably less. The difference was the Hamidian Massacres, which were being conducted even as the couple traveled the Balkans. According to Norman now, the empire had been established by "the Moslem horde" from Asia, which was stopped by "intrepid Hungary." Furthermore, "Greece shook off the turbaned destroyer of her people" and so on. The Russians were suddenly liberators of oppressed Balkan states. Having portrayed the Armenians as revolutionaries in the name of freedom with the expectation of being rescued by the intervention of Christian Europe, he states "but her hope was vain." England had "turned her back." Norman concluded his exhortation with "In the Balkans one learns to hate the Turk." Norman made sure that Gladstone read the article. Prince Nicolas of Montenegro wrote a letter thanking him for his article.
10/29/2021 17:15:05 - INFO - __main__ - ['England']
10/29/2021 17:15:05 - INFO - __main__ - Question: Who works closely with the definition of the Near East? </s> Context: Working closely in conjunction with the definition of the Near East provided by the State Department is the Near East South Asia Center for Strategic Studies (NESA), an educational institution of the United States Department of Defense. It teaches courses and holds seminars and workshops for government officials and military officers who will work or are working within its region. As the name indicates, that region is a combination of State Department regions; however, NESA is careful to identify the State Department region. As its Near East is not different from the State Department's it does not appear in the table. Its name, however, is not entirely accurate. For example, its region includes Mauritania, a member of the State Department's Africa (Sub-Sahara).
10/29/2021 17:15:05 - INFO - __main__ - ['the Near East South Asia Center for Strategic Studies (NESA)']
10/29/2021 17:15:05 - INFO - __main__ - Question: What is NESA? </s> Context: Working closely in conjunction with the definition of the Near East provided by the State Department is the Near East South Asia Center for Strategic Studies (NESA), an educational institution of the United States Department of Defense. It teaches courses and holds seminars and workshops for government officials and military officers who will work or are working within its region. As the name indicates, that region is a combination of State Department regions; however, NESA is careful to identify the State Department region. As its Near East is not different from the State Department's it does not appear in the table. Its name, however, is not entirely accurate. For example, its region includes Mauritania, a member of the State Department's Africa (Sub-Sahara).
10/29/2021 17:15:05 - INFO - __main__ - ['an educational institution of the United States Department of Defense']
10/29/2021 17:15:05 - INFO - __main__ - Start tokenizing ... 10803 instances
10/29/2021 17:15:05 - INFO - __main__ - Printing 3 examples
10/29/2021 17:15:05 - INFO - __main__ - Question: What is the acronym for British Broadcasting Corporation? </s> Context: BBC Television is a service of the British Broadcasting Corporation. The corporation, which has operated in the United Kingdom under the terms of a Royal charter since 1927, has produced television programmes from its own since 1932, although the start of its regular service of television broadcasts is dated to 2 November 1936.
10/29/2021 17:15:05 - INFO - __main__ - ['BBC']
10/29/2021 17:15:05 - INFO - __main__ - Question: Under what auspices does the BBC exist? </s> Context: BBC Television is a service of the British Broadcasting Corporation. The corporation, which has operated in the United Kingdom under the terms of a Royal charter since 1927, has produced television programmes from its own since 1932, although the start of its regular service of television broadcasts is dated to 2 November 1936.
10/29/2021 17:15:05 - INFO - __main__ - ['Royal charter']
10/29/2021 17:15:05 - INFO - __main__ - Question: When did the BBC start creating its own programming? </s> Context: BBC Television is a service of the British Broadcasting Corporation. The corporation, which has operated in the United Kingdom under the terms of a Royal charter since 1927, has produced television programmes from its own since 1932, although the start of its regular service of television broadcasts is dated to 2 November 1936.
10/29/2021 17:15:05 - INFO - __main__ - ['1932']
10/29/2021 17:15:05 - INFO - __main__ - Tokenizing Input ...
10/29/2021 17:15:05 - INFO - __main__ - Tokenizing Input ...
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 17:15:05 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 17:15:05 - INFO - __main__ - Start tokenizing ... 10803 instances
10/29/2021 17:15:05 - INFO - __main__ - Printing 3 examples
10/29/2021 17:15:05 - INFO - __main__ - Question: How many divisions of the U.S. Army were in Europe? </s> Context: The end of World War II set the stage for the East–West confrontation known as the Cold War. With the outbreak of the Korean War, concerns over the defense of Western Europe rose. Two corps, V and VII, were reactivated under Seventh United States Army in 1950 and American strength in Europe rose from one division to four. Hundreds of thousands of U.S. troops remained stationed in West Germany, with others in Belgium, the Netherlands and the United Kingdom, until the 1990s in anticipation of a possible Soviet attack.
10/29/2021 17:15:05 - INFO - __main__ - ['four']
10/29/2021 17:15:05 - INFO - __main__ - Question: Who adopted the Total Force Policy? </s> Context: The Total Force Policy was adopted by Chief of Staff of the Army General Creighton Abrams in the aftermath of the Vietnam War and involves treating the three components of the army – the Regular Army, the Army National Guard and the Army Reserve as a single force. Believing that no U.S. president should be able to take the United States (and more specifically the U.S. Army) to war without the support of the American people, General Abrams intertwined the structure of the three components of the army in such a way as to make extended operations impossible, without the involvement of both the Army National Guard and the Army Reserve.
10/29/2021 17:15:05 - INFO - __main__ - ['General Creighton Abrams']
10/29/2021 17:15:05 - INFO - __main__ - Question: What war was responsible for the creation of the Total Force Policy? </s> Context: The Total Force Policy was adopted by Chief of Staff of the Army General Creighton Abrams in the aftermath of the Vietnam War and involves treating the three components of the army – the Regular Army, the Army National Guard and the Army Reserve as a single force. Believing that no U.S. president should be able to take the United States (and more specifically the U.S. Army) to war without the support of the American people, General Abrams intertwined the structure of the three components of the army in such a way as to make extended operations impossible, without the involvement of both the Army National Guard and the Army Reserve.
10/29/2021 17:15:05 - INFO - __main__ - ['Vietnam War']
10/29/2021 17:15:05 - INFO - __main__ - Tokenizing Input ...
10/29/2021 17:15:13 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 17:15:13 - INFO - __main__ - Tokenizing Output ...
10/29/2021 17:15:14 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 17:15:14 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 17:15:14 - INFO - __main__ - Tokenizing Output ...
10/29/2021 17:15:14 - INFO - __main__ - Loaded 10803 examples from dev data
10/29/2021 17:15:14 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
10/29/2021 17:15:14 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Output ...
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Output ...
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Output ...
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Output ...
10/29/2021 17:15:15 - INFO - __main__ - Loaded 10803 examples from dev data
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Output ...
10/29/2021 17:15:15 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Output ...
10/29/2021 17:15:15 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 17:15:15 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 17:15:15 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 17:15:15 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 17:15:16 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 17:15:16 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 17:15:16 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 17:15:16 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 17:15:16 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 17:15:16 - INFO - __main__ - Loaded 10802 examples from dev data
10/29/2021 17:15:16 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 17:15:16 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
10/29/2021 17:15:16 - INFO - __main__ - Loaded 10803 examples from dev data
10/29/2021 17:15:16 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 17:15:16 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
10/29/2021 17:15:16 - INFO - __main__ - Loaded 10802 examples from dev data
10/29/2021 17:15:16 - INFO - __main__ - Loaded 10802 examples from dev data
10/29/2021 17:15:16 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
10/29/2021 17:15:16 - INFO - __main__ - Loaded 10802 examples from dev data
10/29/2021 17:15:16 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
10/29/2021 17:15:16 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
10/29/2021 17:15:16 - INFO - __main__ - Loaded 10803 examples from dev data
10/29/2021 17:15:16 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
10/29/2021 17:15:17 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 17:15:17 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 17:15:17 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 17:15:17 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 17:15:17 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 17:15:17 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 17:15:17 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 17:15:17 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 17:15:17 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 17:15:17 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 17:15:17 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 17:15:17 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 17:15:17 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 17:15:17 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 17:15:17 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 17:15:17 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 17:15:17 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 17:15:17 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 17:15:18 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
10/29/2021 17:15:19 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
10/29/2021 17:15:20 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
10/29/2021 17:15:20 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
10/29/2021 17:15:21 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
10/29/2021 17:15:21 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
10/29/2021 17:15:21 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
10/29/2021 17:15:21 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
10/29/2021 17:15:23 - INFO - __main__ - Starting inference ...
10/29/2021 17:15:23 - INFO - __main__ - Starting inference ...
10/29/2021 17:15:24 - INFO - __main__ - Starting inference ...
10/29/2021 17:15:24 - INFO - __main__ - Starting inference ...
10/29/2021 17:15:24 - INFO - __main__ - Starting inference ...
10/29/2021 17:15:24 - INFO - __main__ - Starting inference ...
10/29/2021 17:15:24 - INFO - __main__ - Starting inference ...
10/29/2021 17:15:25 - INFO - __main__ - Starting inference ...
10/29/2021 17:21:16 - INFO - __main__ - Starting inference ... Done
10/29/2021 17:21:32 - INFO - __main__ - Starting inference ... Done
10/29/2021 17:21:51 - INFO - __main__ - Starting inference ... Done
10/29/2021 17:22:08 - INFO - __main__ - Starting inference ... Done
10/29/2021 17:22:09 - INFO - __main__ - Starting inference ... Done
10/29/2021 17:22:30 - INFO - __main__ - Starting inference ... Done
10/29/2021 17:22:36 - INFO - __main__ - Starting inference ... Done
10/29/2021 17:22:39 - INFO - __main__ - Starting inference ... Done
