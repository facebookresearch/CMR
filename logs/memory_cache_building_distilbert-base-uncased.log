07/15/2021 23:50:36 - INFO - __main__ - Namespace(batch_size=32, bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', memory_key_cache_path='bug_data/memory_key_cache.pkl', memory_key_encoder='distilbert-base-uncased', pass_pool_jsonl_path='bug_data/mrqa_naturalquestions_dev.sampled_pass.jsonl', sampled_upstream_json_path='bug_data/mrqa_naturalquestions.sampled_upstream.jsonl')
07/15/2021 23:50:37 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/15/2021 23:50:37 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/15/2021 23:50:37 - INFO - __main__ - All examples: 1300
07/15/2021 23:50:37 - INFO - __main__ - Starting to load the key encoder (distilbert-base-uncased) for the memory module.
07/15/2021 23:50:37 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
07/15/2021 23:50:38 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c
07/15/2021 23:50:38 - INFO - transformers.configuration_utils - Model config DistilBertConfig {
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "vocab_size": 30522
}

07/15/2021 23:50:38 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/distilbert-base-uncased-pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/ae9df7a8d658c4f3e1917a471a8a21cf678fa1d4cb91e7702dfe0598dbdcf354.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5
07/15/2021 23:50:42 - INFO - __main__ - Finished.
07/15/2021 23:50:42 - INFO - __main__ - all_key_vectors.shape: 1217 x 768
07/15/2021 23:50:43 - INFO - __main__ - Saved the cache to bug_data/memory_key_cache.pkl
07/15/2021 23:51:03 - INFO - __main__ - Namespace(batch_size=32, bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', memory_key_cache_path='bug_data/memory_key_cache.pkl', memory_key_encoder='distilbert-base-uncased', pass_pool_jsonl_path='bug_data/mrqa_naturalquestions_dev.sampled_pass.jsonl', sampled_upstream_json_path='bug_data/mrqa_naturalquestions.sampled_upstream.jsonl')
07/15/2021 23:51:03 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/15/2021 23:51:03 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/15/2021 23:51:03 - INFO - __main__ - All examples: 1300
07/15/2021 23:51:03 - INFO - __main__ - Starting to load the key encoder (distilbert-base-uncased) for the memory module.
07/15/2021 23:51:04 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
07/15/2021 23:51:04 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c
07/15/2021 23:51:04 - INFO - transformers.configuration_utils - Model config DistilBertConfig {
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "vocab_size": 30522
}

07/15/2021 23:51:04 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/distilbert-base-uncased-pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/ae9df7a8d658c4f3e1917a471a8a21cf678fa1d4cb91e7702dfe0598dbdcf354.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5
07/15/2021 23:51:08 - INFO - __main__ - Finished.
07/15/2021 23:51:09 - INFO - __main__ - all_key_vectors.shape: 1217 x 768
07/15/2021 23:51:09 - INFO - __main__ - Saved the cache to bug_data/memory_key_cache.pkl
