10/29/2021 12:26:58 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_naturalquestions_bart-base_1028_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 12:26:58 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_naturalquestions_bart-base_1028_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 12:26:59 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_naturalquestions_bart-base_1028_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 12:26:59 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_naturalquestions_bart-base_1028_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 12:26:59 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_naturalquestions_bart-base_1028_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 12:26:59 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_naturalquestions_bart-base_1028_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 12:26:59 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_naturalquestions_bart-base_1028_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 12:26:59 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_naturalquestions_bart-base_1028_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
10/29/2021 12:26:59 - INFO - __main__ - dataset_size=88251, num_shards=8, local_shard_id=3
10/29/2021 12:26:59 - INFO - __main__ - dataset_size=88251, num_shards=8, local_shard_id=4
10/29/2021 12:27:00 - INFO - __main__ - dataset_size=88251, num_shards=8, local_shard_id=5
10/29/2021 12:27:00 - INFO - __main__ - dataset_size=88251, num_shards=8, local_shard_id=6
10/29/2021 12:27:00 - INFO - __main__ - dataset_size=88251, num_shards=8, local_shard_id=7
10/29/2021 12:27:00 - INFO - __main__ - dataset_size=88251, num_shards=8, local_shard_id=2
10/29/2021 12:27:00 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 12:27:00 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 12:27:00 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 12:27:00 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 12:27:00 - INFO - __main__ - Start tokenizing ... 11031 instances
10/29/2021 12:27:00 - INFO - __main__ - Printing 3 examples
10/29/2021 12:27:00 - INFO - __main__ - Question: who played the cowardly lion in the wiz ? </s> Context: Theodore Ross Roberts ( June 30 , 1934 -- September 3 , 2002 ) , known as Ted Ross , was an American actor who was probably best known for his role as the Lion in The Wiz , an all - African American reinterpretation of The Wizard of Oz . He won a Tony Award for the original 1975 Broadway production , and went on to recreate the role in the 1978 film version which also starred Diana Ross , Michael Jackson and Nipsey Russell . Ross went on to appear in films including the role of Bitterman in the classic Arthur , and on the television sitcoms The Jeffersons , Benson , The Cosby Show , and its spin - off A Different World . His final role was in the 1991 movie The Fisher King .
10/29/2021 12:27:00 - INFO - __main__ - ['Theodore Ross Roberts']
10/29/2021 12:27:00 - INFO - __main__ - Question: who sings i could fall in love with you ? </s> Context: `` I Could Fall in Love '' is a song recorded by American Tejano singer Selena for her fifth studio album , Dreaming of You ( 1995 ) , released posthumously by EMI Latin on June 26 , 1995 . `` I Could Fall in Love '' and `` Tú Sólo Tú '' were the album 's lead promotional recordings , showcasing her musical transition from Spanish - to English - language songs . The lyrics explore feelings of heartbreak and despair and express the singer 's fear of rejection by a man she finds herself falling in love with . Composed by Keith Thomas , `` I Could Fall in Love '' is a pop ballad with R&B , soul and soft rock influences .
10/29/2021 12:27:00 - INFO - __main__ - ['Selena']
10/29/2021 12:27:00 - INFO - __main__ - Question: what character did john stamos play on general hospital ? </s> Context: John Phillip Stamos ( / ˈsteɪmoʊs / STAY - mohss ; born August 19 , 1963 ) is an American actor , producer , and musician . He first gained recognition for his contract role as Blackie Parrish on General Hospital for which he was nominated for the Daytime Emmy Award for Outstanding Supporting Actor in a Drama Series . He is known for his work in television , especially in his starring role as Jesse Katsopolis on the ABC sitcom Full House . Since the show 's finale in 1995 , Stamos has appeared in numerous TV films and series . Since 2005 , he has been the national spokesperson for Project Cuddle .
10/29/2021 12:27:00 - INFO - __main__ - ['Blackie Parrish']
10/29/2021 12:27:00 - INFO - __main__ - dataset_size=88251, num_shards=8, local_shard_id=0
10/29/2021 12:27:00 - INFO - __main__ - dataset_size=88251, num_shards=8, local_shard_id=1
10/29/2021 12:27:00 - INFO - __main__ - Tokenizing Input ...
10/29/2021 12:27:00 - INFO - __main__ - Start tokenizing ... 11031 instances
10/29/2021 12:27:00 - INFO - __main__ - Printing 3 examples
10/29/2021 12:27:00 - INFO - __main__ - Question: when was css first proposed as a standard by the w3c ? </s> Context: CSS was first proposed by Håkon Wium Lie on October 10 , 1994 . At the time , Lie was working with Tim Berners - Lee at CERN . Several other style sheet languages for the web were proposed around the same time , and discussions on public mailing lists and inside World Wide Web Consortium resulted in the first W3C CSS Recommendation ( CSS1 ) being released in 1996 . In particular , Bert Bos ' proposal was influential ; he became co-author of CSS1 and is regarded as co-creator of CSS .
10/29/2021 12:27:00 - INFO - __main__ - ['October 10 , 1994']
10/29/2021 12:27:00 - INFO - __main__ - Question: who has won the most la liga trophies ? </s> Context: Real Madrid is the most successful club with 33 titles . The most recent club other than Real Madrid and Barcelona to win the league is Atlético Madrid in the 2013 -- 14 season . With their 30 May Copa del Rey defeat of Athletic Bilbao , Barcelona has won the Spanish version of The Double the most times , having won the league and cup in the same year six times in its history , breaking its tie with Athletic 's five . Barcelona is the only Spanish team that has won the Treble , which includes the UEFA Champions League along with the league and Copa del Rey , and the only UEFA club to have won the treble twice after accomplishing that feat in 2015 . The current champions are Real Madrid , who won the 2016 -- 17 competition .
10/29/2021 12:27:00 - INFO - __main__ - ['Real Madrid']
10/29/2021 12:27:00 - INFO - __main__ - Question: who wrote music for chitty chitty bang bang ? </s> Context: The film was produced by Albert R. Broccoli , the regular co-producer of the James Bond series of films ( also based on Ian Fleming novels ) . John Stears supervised the special effects . Irwin Kostal supervised and conducted the music , while the musical numbers , written by Richard M. and Robert B. Sherman of Mary Poppins , were staged by Marc Breaux and Dee Dee Wood . The song `` Chitty Chitty Bang Bang '' was nominated for an Academy Award .
10/29/2021 12:27:00 - INFO - __main__ - ['Richard M. and Robert B. Sherman']
10/29/2021 12:27:00 - INFO - __main__ - Tokenizing Input ...
10/29/2021 12:27:00 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 12:27:00 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 12:27:00 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 12:27:00 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 12:27:01 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 12:27:01 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 12:27:01 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 12:27:01 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 12:27:01 - INFO - __main__ - Start tokenizing ... 11032 instances
10/29/2021 12:27:01 - INFO - __main__ - Printing 3 examples
10/29/2021 12:27:01 - INFO - __main__ - Question: where did dorothy from the wizard of oz live ? </s> Context: In the Oz books , Dorothy is an orphan raised by her aunt and uncle in the bleak landscape of a Kansas farm . Whether Aunt Em or Uncle Henry is Dorothy 's blood relative remains unclear . Uncle Henry makes reference to Dorothy 's mother in The Emerald City of Oz , possibly an indication that Henry is Dorothy 's blood relative . ( It is also possible that `` Aunt '' and `` Uncle '' are affectionate terms of a foster family and that Dorothy is not related to either of them , although Zeb in Dorothy and the Wizard in Oz claims to be Dorothy 's second cousin , related through Aunt Em. Little mention is made of what happened to Dorothy 's birth parents , other than a passing reference to her mother being dead . ) Along with her small black dog , Toto , Dorothy is swept away by a tornado to the Land of Oz and , much like Alice of Alice 's Adventures in Wonderland , they enter an alternative world filled with talking creatures . In many of the Oz books , Dorothy is the main heroine of the story . She is often seen with her best friend and the ruler of Oz , Princess Ozma . Her trademark blue and white gingham dress is admired by the Munchkins because blue is their favorite color and white is worn only by good witches and sorceresses , which indicates to them that Dorothy is a good witch .
10/29/2021 12:27:01 - INFO - __main__ - ['Kansas']
10/29/2021 12:27:01 - INFO - __main__ - Question: what was the purpose of the wagner act ? </s> Context: The National Labor Relations Act of 1935 ( 49 Stat. 449 ) 29 U.S.C. § 151 -- 169 ( also known as the Wagner Act after New York Senator Robert F. Wagner ) is a foundational statute of United States labor law which guarantees basic rights of private sector employees to organize into trade unions , engage in collective bargaining for better terms and conditions at work , and take collective action including strike if necessary . The act also created the National Labor Relations Board , which conducts elections that can expect employers to engage in collective bargaining with labor unions ( also known as trade unions ) . The Act does not apply to workers who are covered by the Railway Labor Act , agricultural employees , domestic employees , supervisors , federal , state or local government workers , independent contractors and some close relatives of individual employers .
10/29/2021 12:27:01 - INFO - __main__ - ['guarantees basic rights of private sector employees to organize into trade unions , engage in collective bargaining for better terms and conditions at work , and take collective action including strike if necessary', 'created the National Labor Relations Board']
10/29/2021 12:27:01 - INFO - __main__ - Question: when did banks stop printing their own money ? </s> Context: National Bank Notes were retired as a currency type by the U.S. government in the 1930s during the great depression as currency in the U.S. was consolidated into Federal Reserve Notes , United States Notes , and silver certificates ; privately issued banknotes were eliminated . The passage of the Gold Reserve Act created an accounting gain for the Treasury , part of which was used to provide funds to retire all bonds against which National Banks Notes could be issued .
10/29/2021 12:27:01 - INFO - __main__ - ['1930s']
10/29/2021 12:27:01 - INFO - __main__ - Tokenizing Input ...
10/29/2021 12:27:01 - INFO - __main__ - Start tokenizing ... 11031 instances
10/29/2021 12:27:01 - INFO - __main__ - Printing 3 examples
10/29/2021 12:27:01 - INFO - __main__ - Question: where is the statue of christ the redeemer located ? </s> Context: Christ the Redeemer ( Portuguese : Cristo Redentor , standard Brazilian Portuguese : ( ˈkɾistu ʁedẽˈtoʁ ) , local pronunciation : ( ˈkɾiɕtŭ̻ xe̞dẽ̞ˈtoɦ ) ) is an Art Deco statue of Jesus Christ in Rio de Janeiro , Brazil , created by French sculptor Paul Landowski and built by the Brazilian engineer Heitor da Silva Costa , in collaboration with the French engineer Albert Caquot . Romanian sculptor Gheorghe Leonida fashioned the face . Constructed between 1922 and 1931 , the statue is 30 metres ( 98 ft ) tall , excluding its 8 - metre ( 26 ft ) pedestal . The arms stretch 28 metres ( 92 ft ) wide .
10/29/2021 12:27:01 - INFO - __main__ - ['Rio de Janeiro , Brazil']
10/29/2021 12:27:01 - INFO - __main__ - Question: which two forms of energy do muscles produce ? </s> Context: Glycogen functions as one of two forms of long - term energy reserves , with the other form being triglyceride stores in adipose tissue ( i.e. , body fat ) . In humans , glycogen is made and stored primarily in the cells of the liver and skeletal muscle . In the liver , glycogen can make up from 5 -- 6 % of the organ 's fresh weight and the liver of an adult weighing 70 kg can store roughly 100 -- 120 grams of glycogen . In skeletal muscle , glycogen is found in a low concentration ( 1 -- 2 % of the muscle mass ) and the skeletal muscle of an adult weighing 70 kg can store roughly 400 grams of glycogen . The amount of glycogen stored in the body -- particularly within the muscles and liver -- mostly depends on physical training , basal metabolic rate , and eating habits . Small amounts of glycogen are also found in other tissues and cells , including the kidneys , red blood cells , white blood cells , and glial cells in the brain . The uterus also stores glycogen during pregnancy to nourish the embryo .
10/29/2021 12:27:01 - INFO - __main__ - ['Glycogen', 'triglyceride']
10/29/2021 12:27:01 - INFO - __main__ - Question: when does dbz wrath of the dragon take place ? </s> Context: Set after the events of the final battle with Majin Buu , the film focuses on the efforts of an evil magician , Hoi , to release the deadly monster Hirudegarn onto the Earth , forcing Goku and his friends to enlist the aid of a warrior named Tapion , who may be the only one capable of defeating the monster . Series creator Akira Toriyama designed the Tapion and Minoshia characters .
10/29/2021 12:27:01 - INFO - __main__ - ['after the events of the final battle with Majin Buu']
10/29/2021 12:27:01 - INFO - __main__ - Tokenizing Input ...
10/29/2021 12:27:01 - INFO - __main__ - Start tokenizing ... 11031 instances
10/29/2021 12:27:01 - INFO - __main__ - Printing 3 examples
10/29/2021 12:27:01 - INFO - __main__ - Question: when does summer start in the southern hemisphere ? </s> Context: During May , June , and July , the Northern Hemisphere is exposed to more direct sunlight because the hemisphere faces the Sun . The same is true of the Southern Hemisphere in November , December , and January . It is Earth 's axial tilt that causes the Sun to be higher in the sky during the summer months , which increases the solar flux . However , due to seasonal lag , June , July , and August are the warmest months in the Northern Hemisphere while December , January , and February are the warmest months in the Southern Hemisphere .
10/29/2021 12:27:01 - INFO - __main__ - ['December']
10/29/2021 12:27:01 - INFO - __main__ - Question: who did the populist party support in the 1896 election ? </s> Context: William Jennings Bryan , former U.S. representative ( Nebraska )   Seymour F. Norton from Illinois , writer
10/29/2021 12:27:01 - INFO - __main__ - ['William Jennings Bryan , former U.S. representative ( Nebraska )', 'Seymour F. Norton from Illinois , writer']
10/29/2021 12:27:01 - INFO - __main__ - Question: who has the authority to grant a pardon or clemency ? </s> Context: In the United States , the pardon power for offences against the United States is granted to the President of the United States under Article II , Section 2 of the United States Constitution which states that the President `` shall have power to grant reprieves and pardons for offenses against the United States , except in cases of impeachment '' . The U.S. Supreme Court has interpreted this language to include the power to grant pardons , conditional pardons , commutations of sentence , conditional commutations of sentence , remissions of fines and forfeitures , respites , and amnesties .
10/29/2021 12:27:01 - INFO - __main__ - ['the President of the United States']
10/29/2021 12:27:01 - INFO - __main__ - Tokenizing Input ...
10/29/2021 12:27:01 - INFO - __main__ - Start tokenizing ... 11031 instances
10/29/2021 12:27:01 - INFO - __main__ - Printing 3 examples
10/29/2021 12:27:01 - INFO - __main__ - Question: where does most digestion in the small intestine take place ? </s> Context: Absorption of the majority of nutrients takes place in the jejunum , with the following notable exceptions :
10/29/2021 12:27:01 - INFO - __main__ - ['the jejunum']
10/29/2021 12:27:01 - INFO - __main__ - Question: when did trinidad and tobago become an independent nation ? </s> Context: The island of Trinidad was a Spanish colony from the arrival of Christopher Columbus in 1498 until the capitulation of the Spanish Governor , Don José María Chacón , with the arrival of a British fleet of 18 warships on 18 February 1797 . During the same period , the island of Tobago changed hands among Spanish , British , French , Dutch and Courlander colonizers , more times than any other island in the Caribbean . Trinidad and Tobago ( remaining separate until 1889 ) were ceded to Britain in 1802 under the Treaty of Amiens . The country Trinidad and Tobago obtained independence in 1962 , becoming a republic in 1976 .
10/29/2021 12:27:01 - INFO - __main__ - ['1962']
10/29/2021 12:27:01 - INFO - __main__ - Question: what are the two chambers of the california state legislation ? </s> Context: The California State Legislature is the state legislature of the U.S. state of California . It is a bicameral body consisting of the lower house , the California State Assembly , with 80 members , and the upper house , the California State Senate , with 40 members . New legislators convene each new two - year session , to organize , in the Assembly and Senate Chambers , respectively , at noon on the first Monday in December following the election . After the organizational meeting , both houses are in recess until the first Monday in January , except when the first Monday is January 1 or January 1 is a Sunday , in which case they meet the following Wednesday . Aside from the recess , the legislature is in session year - round .
10/29/2021 12:27:01 - INFO - __main__ - ['the upper house , the California State Senate', 'the lower house , the California State Assembly']
10/29/2021 12:27:01 - INFO - __main__ - Tokenizing Input ...
10/29/2021 12:27:01 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 12:27:01 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 12:27:01 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
10/29/2021 12:27:01 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
10/29/2021 12:27:01 - INFO - __main__ - Start tokenizing ... 11032 instances
10/29/2021 12:27:01 - INFO - __main__ - Printing 3 examples
10/29/2021 12:27:01 - INFO - __main__ - Question: who is the most followed user of instagram ? </s> Context: This list contains the top 25 accounts with the most followers on the social photo - sharing platform Instagram . As of May 2018 , the most followed user is Instagram 's own account , with over 235 million followers . Selena Gomez is the most followed individual , with over 137 million followers . Ten accounts have exceeded 100 million followers on the site .
10/29/2021 12:27:01 - INFO - __main__ - ['Selena Gomez']
10/29/2021 12:27:01 - INFO - __main__ - Question: who was supposed to host the 1986 world cup ? </s> Context: The 1986 FIFA World Cup , the 13th FIFA World Cup , was held in Mexico from 31 May to 29 June 1986 . The tournament was the second to feature a 24 - team format . With European nations not allowed to host after the previous World Cup in Spain , Colombia had been originally chosen to host the competition by FIFA but , largely due to economic reasons , was not able to do so and officially resigned in 1982 . Mexico was selected as the new host in May 1983 . This was the third FIFA World Cup tournament in succession that was hosted by a Spanish - speaking country , after Argentina 1978 , and Spain 1982 .
10/29/2021 12:27:01 - INFO - __main__ - ['Colombia']
10/29/2021 12:27:01 - INFO - __main__ - Question: whats the song in the look at me video ? </s> Context: `` Look at Me '' ( stylized as `` Look at Me ! '' ) is the debut single by American rapper XXXTentacion . The song premiered on December 30 , 2015 on the SoundCloud account of Rojas , the song 's co-producer , before initially being released for digital download as a single on January 29 , 2016 , becoming a sleeper hit after its initial release until January 2017 , in which the single was later re-released for digital download again with a remastered and clean version of the single on February 20 , 2017 , by Empire Distribution . The song serves as the lead single from his debut commercial mixtape Revenge . The track was produced by Rojas and Jimmy Duval , and heavily samples the song `` Changes '' by British dubstep DJ and record producer Mala .
10/29/2021 12:27:01 - INFO - __main__ - ["`` Changes '' by British dubstep DJ and record producer Mala"]
10/29/2021 12:27:01 - INFO - __main__ - Start tokenizing ... 11032 instances
10/29/2021 12:27:01 - INFO - __main__ - Printing 3 examples
10/29/2021 12:27:01 - INFO - __main__ - Question: which is the latest version of corel draw ? </s> Context: CorelDraw ( styled CorelDRAW ) is a vector graphics editor developed and marketed by Corel Corporation . It is also the name of Corel 's Graphics Suite , which bundles CorelDraw with bitmap - image editor Corel Photo - Paint as well as other graphics - related programs ( see below ) . The latest version is marketed as Graphics Suite 2017 ( equivalent to version 19 ) , and was released in April 2017 . CorelDraw is designed to edit two - dimensional images such as logos and posters .
10/29/2021 12:27:01 - INFO - __main__ - ['April 2017']
10/29/2021 12:27:01 - INFO - __main__ - Question: what does the word fore mean in golf ? </s> Context: `` Fore ! '' , originally an Australian interjection , is used to warn anyone standing or moving in the flight of a golf ball . The mention of the term in an 1881 Australian Golf Museum indicates that the term was in use at least as early as that period .
10/29/2021 12:27:01 - INFO - __main__ - ['to warn anyone standing or moving in the flight of a golf ball']
10/29/2021 12:27:01 - INFO - __main__ - Question: where is the home town show on hgtv filmed ? </s> Context: Home Town is an American television series starring husband and wife team Ben and Erin Napier that premiered on March 21 , 2017 on HGTV . The married couple restores Southern homes in Laurel , Mississippi .
10/29/2021 12:27:01 - INFO - __main__ - ['Laurel , Mississippi']
10/29/2021 12:27:01 - INFO - __main__ - Tokenizing Input ...
10/29/2021 12:27:01 - INFO - __main__ - Tokenizing Input ...
10/29/2021 12:27:11 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 12:27:11 - INFO - __main__ - Tokenizing Output ...
10/29/2021 12:27:11 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 12:27:11 - INFO - __main__ - Tokenizing Output ...
10/29/2021 12:27:12 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 12:27:12 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 12:27:12 - INFO - __main__ - Loaded 11031 examples from dev data
10/29/2021 12:27:12 - INFO - __main__ - Loaded 11032 examples from dev data
10/29/2021 12:27:12 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt ....
10/29/2021 12:27:12 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt ....
10/29/2021 12:27:13 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 12:27:13 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 12:27:13 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 12:27:13 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 12:27:13 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 12:27:13 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 12:27:16 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 12:27:16 - INFO - __main__ - Tokenizing Output ...
10/29/2021 12:27:16 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 12:27:16 - INFO - __main__ - Tokenizing Output ...
10/29/2021 12:27:17 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 12:27:18 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 12:27:18 - INFO - __main__ - Loaded 11032 examples from dev data
10/29/2021 12:27:18 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt ....
10/29/2021 12:27:18 - INFO - __main__ - Loaded 11032 examples from dev data
10/29/2021 12:27:18 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 12:27:18 - INFO - __main__ - Tokenizing Output ...
10/29/2021 12:27:18 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt ....
10/29/2021 12:27:19 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 12:27:19 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 12:27:19 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 12:27:19 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 12:27:19 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 12:27:19 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 12:27:19 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 12:27:19 - INFO - __main__ - Tokenizing Output ...
10/29/2021 12:27:19 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 12:27:19 - INFO - __main__ - Tokenizing Output ...
10/29/2021 12:27:20 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 12:27:20 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt .... Done!
10/29/2021 12:27:20 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt .... Done!
10/29/2021 12:27:20 - INFO - __main__ - Loaded 11031 examples from dev data
10/29/2021 12:27:20 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt ....
10/29/2021 12:27:20 - INFO - __main__ - Tokenizing Input ... Done!
10/29/2021 12:27:20 - INFO - __main__ - Tokenizing Output ...
10/29/2021 12:27:21 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 12:27:21 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 12:27:21 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 12:27:21 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 12:27:21 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 12:27:21 - INFO - __main__ - Loaded 11031 examples from dev data
10/29/2021 12:27:21 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt ....
10/29/2021 12:27:22 - INFO - __main__ - Loaded 11031 examples from dev data
10/29/2021 12:27:22 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt ....
10/29/2021 12:27:22 - INFO - __main__ - Tokenizing Output ... Done!
10/29/2021 12:27:22 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 12:27:22 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 12:27:22 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 12:27:22 - INFO - __main__ - Loaded 11031 examples from dev data
10/29/2021 12:27:23 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt ....
10/29/2021 12:27:23 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 12:27:23 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 12:27:23 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 12:27:23 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
10/29/2021 12:27:23 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

10/29/2021 12:27:23 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
10/29/2021 12:27:24 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt .... Done!
10/29/2021 12:27:25 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt .... Done!
10/29/2021 12:27:25 - INFO - __main__ - Starting inference ...
10/29/2021 12:27:25 - INFO - __main__ - Starting inference ...
10/29/2021 12:27:27 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt .... Done!
10/29/2021 12:27:28 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt .... Done!
10/29/2021 12:27:29 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt .... Done!
10/29/2021 12:27:29 - INFO - __main__ - Starting inference ...
10/29/2021 12:27:30 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_1028_upstream_model//best-model.pt .... Done!
10/29/2021 12:27:30 - INFO - __main__ - Starting inference ...
10/29/2021 12:27:32 - INFO - __main__ - Starting inference ...
10/29/2021 12:27:33 - INFO - __main__ - Starting inference ...
10/29/2021 12:27:34 - INFO - __main__ - Starting inference ...
10/29/2021 12:27:34 - INFO - __main__ - Starting inference ...
10/29/2021 12:38:43 - INFO - __main__ - Starting inference ... Done
10/29/2021 12:38:50 - INFO - __main__ - Starting inference ... Done
10/29/2021 12:42:00 - INFO - __main__ - Starting inference ... Done
10/29/2021 12:42:02 - INFO - __main__ - Starting inference ... Done
10/29/2021 12:42:06 - INFO - __main__ - Starting inference ... Done
10/29/2021 12:42:07 - INFO - __main__ - Starting inference ... Done
10/29/2021 12:42:28 - INFO - __main__ - Starting inference ... Done
10/29/2021 12:42:36 - INFO - __main__ - Starting inference ... Done
