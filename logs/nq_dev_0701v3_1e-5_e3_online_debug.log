07/01/2021 08:25:52 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', current_thread_id=None, do_lowercase=False, freeze_embeds=False, gradient_accumulation_steps=1, learning_rate=1e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=None, num_beams=3, num_threads_eval=0, num_train_epochs=3.0, overtime_ckpt_dir='bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/', overtime_overall_bug_eval=0, pass_pool_jsonl_path='bug_data/mrqa_naturalquestions_dev.pass.jsonl', pass_sample_size=64, path_to_thread_result=None, predict_batch_size=16, prefix='nq_dev_0701v3_1e-5_e3', result_file='bug_data/output/nq_dev_0701v3_1e-5_e3_result.json', save_all_ckpts=1, seed=42, task_name='mrqa_naturalquestions', train_batch_size=8, weight_decay=0.01)
07/01/2021 08:25:52 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/01/2021 08:25:52 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/01/2021 08:25:57 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base .....
07/01/2021 08:26:01 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 08:26:01 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 08:26:01 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 08:26:06 - INFO - __main__ - Loading checkpoint from out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt for facebook/bart-base ..... Done!
07/01/2021 08:26:10 - INFO - __main__ - Moving to the GPUs.
07/01/2021 08:26:10 - INFO - __main__ - Debugger Setup ......
07/01/2021 08:26:10 - INFO - __main__ - debugger_args: Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, learning_rate=1e-05, max_grad_norm=0.1, num_epochs=3.0, overtime_ckpt_dir='bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/', overtime_overall_bug_eval=0, save_all_ckpts=1, total_steps=10000, warmup_steps=0, weight_decay=0.01) ......
07/01/2021 08:26:10 - INFO - __main__ - Debugger Setup ...... Done!
07/01/2021 08:26:10 - INFO - __main__ - Start Online Debugging
07/01/2021 08:26:10 - INFO - __main__ - Number of Batches of Bugs: 50
07/01/2021 08:26:10 - INFO - __main__ - Bug Batch Size: 20
07/01/2021 08:26:13 - INFO - __main__ - ----------Timecode: 0----------
07/01/2021 08:26:13 - INFO - __main__ - Before Bug-fixing the results on bug-batch-0 = {'EM': 0.0, 'QA-F1': 0.4711170762317054}
07/01/2021 08:26:18 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 1.0, 'QA-F1': 1.0}
07/01/2021 08:26:18 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:26:19 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:26:22 - INFO - __main__ - After Bug-fixing the results on bug-batch-0 = {'EM': 0.25, 'QA-F1': 0.6448880016150305}
07/01/2021 08:26:27 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.84375, 'QA-F1': 0.9080588426700344}
07/01/2021 08:26:27 - INFO - __main__ - Number of em_prefixed_bugs = 0; Number of f1_prefixed_bugs = 11
07/01/2021 08:26:27 - INFO - __main__ - Number of em_fixed_bugs = 5; Number of f1_fixed_bugs = 6
07/01/2021 08:26:27 - INFO - __main__ - Number of em_forgotten_passes = 10.
07/01/2021 08:26:29 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_001.pt.
07/01/2021 08:26:32 - INFO - __main__ - ----------Timecode: 1----------
07/01/2021 08:26:32 - INFO - __main__ - Before Bug-fixing the results on bug-batch-1 = {'EM': 0.05, 'QA-F1': 0.4986064562677465}
07/01/2021 08:26:32 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.84375, 'QA-F1': 0.9080588426700344}
07/01/2021 08:26:32 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:26:33 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:26:35 - INFO - __main__ - After Bug-fixing the results on bug-batch-1 = {'EM': 0.25, 'QA-F1': 0.6267964877981855}
07/01/2021 08:26:41 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.9375, 'QA-F1': 0.96703125}
07/01/2021 08:26:41 - INFO - __main__ - Number of em_prefixed_bugs = 1; Number of f1_prefixed_bugs = 9
07/01/2021 08:26:41 - INFO - __main__ - Number of em_fixed_bugs = 4; Number of f1_fixed_bugs = 5
07/01/2021 08:26:41 - INFO - __main__ - Number of em_forgotten_passes = 1.
07/01/2021 08:26:43 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_002.pt.
07/01/2021 08:26:45 - INFO - __main__ - ----------Timecode: 2----------
07/01/2021 08:26:45 - INFO - __main__ - Before Bug-fixing the results on bug-batch-2 = {'EM': 0.05, 'QA-F1': 0.4778502415458938}
07/01/2021 08:26:45 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.9375, 'QA-F1': 0.96703125}
07/01/2021 08:26:45 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:26:47 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:26:50 - INFO - __main__ - After Bug-fixing the results on bug-batch-2 = {'EM': 0.45, 'QA-F1': 0.6944444444444444}
07/01/2021 08:26:55 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.96875, 'QA-F1': 0.9872727272727273}
07/01/2021 08:26:55 - INFO - __main__ - Number of em_prefixed_bugs = 1; Number of f1_prefixed_bugs = 10
07/01/2021 08:26:55 - INFO - __main__ - Number of em_fixed_bugs = 8; Number of f1_fixed_bugs = 8
07/01/2021 08:26:55 - INFO - __main__ - Number of em_forgotten_passes = 1.
07/01/2021 08:26:56 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_003.pt.
07/01/2021 08:26:58 - INFO - __main__ - ----------Timecode: 3----------
07/01/2021 08:26:58 - INFO - __main__ - Before Bug-fixing the results on bug-batch-3 = {'EM': 0.1, 'QA-F1': 0.46128743300940106}
07/01/2021 08:26:58 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.96875, 'QA-F1': 0.9872727272727273}
07/01/2021 08:26:58 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:27:00 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:27:02 - INFO - __main__ - After Bug-fixing the results on bug-batch-3 = {'EM': 0.4, 'QA-F1': 0.635622656818309}
07/01/2021 08:27:08 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.90625, 'QA-F1': 0.9534094551282051}
07/01/2021 08:27:08 - INFO - __main__ - Number of em_prefixed_bugs = 2; Number of f1_prefixed_bugs = 9
07/01/2021 08:27:08 - INFO - __main__ - Number of em_fixed_bugs = 6; Number of f1_fixed_bugs = 6
07/01/2021 08:27:08 - INFO - __main__ - Number of em_forgotten_passes = 5.
07/01/2021 08:27:10 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_004.pt.
07/01/2021 08:27:12 - INFO - __main__ - ----------Timecode: 4----------
07/01/2021 08:27:12 - INFO - __main__ - Before Bug-fixing the results on bug-batch-4 = {'EM': 0.05, 'QA-F1': 0.45787330316742086}
07/01/2021 08:27:12 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.90625, 'QA-F1': 0.9534094551282051}
07/01/2021 08:27:12 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:27:13 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:27:15 - INFO - __main__ - After Bug-fixing the results on bug-batch-4 = {'EM': 0.25, 'QA-F1': 0.6255369509717337}
07/01/2021 08:27:21 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.9005808022995523}
07/01/2021 08:27:21 - INFO - __main__ - Number of em_prefixed_bugs = 1; Number of f1_prefixed_bugs = 9
07/01/2021 08:27:21 - INFO - __main__ - Number of em_fixed_bugs = 4; Number of f1_fixed_bugs = 5
07/01/2021 08:27:21 - INFO - __main__ - Number of em_forgotten_passes = 5.
07/01/2021 08:27:23 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_005.pt.
07/01/2021 08:27:25 - INFO - __main__ - ----------Timecode: 5----------
07/01/2021 08:27:25 - INFO - __main__ - Before Bug-fixing the results on bug-batch-5 = {'EM': 0.05, 'QA-F1': 0.4051111111111111}
07/01/2021 08:27:25 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.9005808022995523}
07/01/2021 08:27:25 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:27:26 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:27:28 - INFO - __main__ - After Bug-fixing the results on bug-batch-5 = {'EM': 0.25, 'QA-F1': 0.5540788202390033}
07/01/2021 08:27:33 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.84375, 'QA-F1': 0.9198880112942612}
07/01/2021 08:27:33 - INFO - __main__ - Number of em_prefixed_bugs = 1; Number of f1_prefixed_bugs = 7
07/01/2021 08:27:33 - INFO - __main__ - Number of em_fixed_bugs = 4; Number of f1_fixed_bugs = 6
07/01/2021 08:27:33 - INFO - __main__ - Number of em_forgotten_passes = 1.
07/01/2021 08:27:35 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_006.pt.
07/01/2021 08:27:37 - INFO - __main__ - ----------Timecode: 6----------
07/01/2021 08:27:37 - INFO - __main__ - Before Bug-fixing the results on bug-batch-6 = {'EM': 0.2, 'QA-F1': 0.5189897432787925}
07/01/2021 08:27:37 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.84375, 'QA-F1': 0.9198880112942612}
07/01/2021 08:27:37 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:27:38 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:27:40 - INFO - __main__ - After Bug-fixing the results on bug-batch-6 = {'EM': 0.4, 'QA-F1': 0.689688042682598}
07/01/2021 08:27:45 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.8125, 'QA-F1': 0.9019564636752136}
07/01/2021 08:27:45 - INFO - __main__ - Number of em_prefixed_bugs = 4; Number of f1_prefixed_bugs = 12
07/01/2021 08:27:45 - INFO - __main__ - Number of em_fixed_bugs = 4; Number of f1_fixed_bugs = 6
07/01/2021 08:27:45 - INFO - __main__ - Number of em_forgotten_passes = 3.
07/01/2021 08:27:48 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_007.pt.
07/01/2021 08:27:50 - INFO - __main__ - ----------Timecode: 7----------
07/01/2021 08:27:50 - INFO - __main__ - Before Bug-fixing the results on bug-batch-7 = {'EM': 0.05, 'QA-F1': 0.3842803030303029}
07/01/2021 08:27:50 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.8125, 'QA-F1': 0.9019564636752136}
07/01/2021 08:27:50 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:27:52 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:27:54 - INFO - __main__ - After Bug-fixing the results on bug-batch-7 = {'EM': 0.1, 'QA-F1': 0.4711907609933926}
07/01/2021 08:28:00 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.796875, 'QA-F1': 0.8937929258241758}
07/01/2021 08:28:00 - INFO - __main__ - Number of em_prefixed_bugs = 1; Number of f1_prefixed_bugs = 5
07/01/2021 08:28:00 - INFO - __main__ - Number of em_fixed_bugs = 1; Number of f1_fixed_bugs = 3
07/01/2021 08:28:00 - INFO - __main__ - Number of em_forgotten_passes = 1.
07/01/2021 08:28:03 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_008.pt.
07/01/2021 08:28:05 - INFO - __main__ - ----------Timecode: 8----------
07/01/2021 08:28:05 - INFO - __main__ - Before Bug-fixing the results on bug-batch-8 = {'EM': 0.1, 'QA-F1': 0.3763410597054786}
07/01/2021 08:28:05 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.796875, 'QA-F1': 0.8937929258241758}
07/01/2021 08:28:05 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:28:07 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:28:10 - INFO - __main__ - After Bug-fixing the results on bug-batch-8 = {'EM': 0.3, 'QA-F1': 0.5419887955182074}
07/01/2021 08:28:16 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8703554258241758}
07/01/2021 08:28:16 - INFO - __main__ - Number of em_prefixed_bugs = 2; Number of f1_prefixed_bugs = 7
07/01/2021 08:28:16 - INFO - __main__ - Number of em_fixed_bugs = 4; Number of f1_fixed_bugs = 4
07/01/2021 08:28:16 - INFO - __main__ - Number of em_forgotten_passes = 2.
07/01/2021 08:28:18 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_009.pt.
07/01/2021 08:28:20 - INFO - __main__ - ----------Timecode: 9----------
07/01/2021 08:28:20 - INFO - __main__ - Before Bug-fixing the results on bug-batch-9 = {'EM': 0.2, 'QA-F1': 0.49568839206770254}
07/01/2021 08:28:20 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8703554258241758}
07/01/2021 08:28:20 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:28:21 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:28:23 - INFO - __main__ - After Bug-fixing the results on bug-batch-9 = {'EM': 0.4, 'QA-F1': 0.6393411136151609}
07/01/2021 08:28:29 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.78125, 'QA-F1': 0.8804827406389906}
07/01/2021 08:28:29 - INFO - __main__ - Number of em_prefixed_bugs = 4; Number of f1_prefixed_bugs = 10
07/01/2021 08:28:29 - INFO - __main__ - Number of em_fixed_bugs = 4; Number of f1_fixed_bugs = 4
07/01/2021 08:28:29 - INFO - __main__ - Number of em_forgotten_passes = 0.
07/01/2021 08:28:31 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_010.pt.
07/01/2021 08:28:33 - INFO - __main__ - ----------Timecode: 10----------
07/01/2021 08:28:33 - INFO - __main__ - Before Bug-fixing the results on bug-batch-10 = {'EM': 0.05, 'QA-F1': 0.36886222888599807}
07/01/2021 08:28:33 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.78125, 'QA-F1': 0.8804827406389906}
07/01/2021 08:28:33 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:28:35 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:28:37 - INFO - __main__ - After Bug-fixing the results on bug-batch-10 = {'EM': 0.3, 'QA-F1': 0.5454415299612668}
07/01/2021 08:28:43 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.75, 'QA-F1': 0.858249349030599}
07/01/2021 08:28:43 - INFO - __main__ - Number of em_prefixed_bugs = 1; Number of f1_prefixed_bugs = 7
07/01/2021 08:28:43 - INFO - __main__ - Number of em_fixed_bugs = 5; Number of f1_fixed_bugs = 5
07/01/2021 08:28:43 - INFO - __main__ - Number of em_forgotten_passes = 2.
07/01/2021 08:28:45 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_011.pt.
07/01/2021 08:28:47 - INFO - __main__ - ----------Timecode: 11----------
07/01/2021 08:28:47 - INFO - __main__ - Before Bug-fixing the results on bug-batch-11 = {'EM': 0.1, 'QA-F1': 0.4324570739108092}
07/01/2021 08:28:47 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.75, 'QA-F1': 0.858249349030599}
07/01/2021 08:28:47 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:28:49 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:28:52 - INFO - __main__ - After Bug-fixing the results on bug-batch-11 = {'EM': 0.25, 'QA-F1': 0.5671245047522138}
07/01/2021 08:28:58 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.65625, 'QA-F1': 0.789533713853083}
07/01/2021 08:28:58 - INFO - __main__ - Number of em_prefixed_bugs = 2; Number of f1_prefixed_bugs = 8
07/01/2021 08:28:58 - INFO - __main__ - Number of em_fixed_bugs = 4; Number of f1_fixed_bugs = 4
07/01/2021 08:28:58 - INFO - __main__ - Number of em_forgotten_passes = 6.
07/01/2021 08:29:00 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_012.pt.
07/01/2021 08:29:02 - INFO - __main__ - ----------Timecode: 12----------
07/01/2021 08:29:02 - INFO - __main__ - Before Bug-fixing the results on bug-batch-12 = {'EM': 0.15, 'QA-F1': 0.4665067448576714}
07/01/2021 08:29:02 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.65625, 'QA-F1': 0.789533713853083}
07/01/2021 08:29:02 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:29:03 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:29:06 - INFO - __main__ - After Bug-fixing the results on bug-batch-12 = {'EM': 0.25, 'QA-F1': 0.5686297797175482}
07/01/2021 08:29:13 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.625, 'QA-F1': 0.7748555693236281}
07/01/2021 08:29:13 - INFO - __main__ - Number of em_prefixed_bugs = 3; Number of f1_prefixed_bugs = 10
07/01/2021 08:29:13 - INFO - __main__ - Number of em_fixed_bugs = 3; Number of f1_fixed_bugs = 4
07/01/2021 08:29:13 - INFO - __main__ - Number of em_forgotten_passes = 2.
07/01/2021 08:29:15 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_013.pt.
07/01/2021 08:29:18 - INFO - __main__ - ----------Timecode: 13----------
07/01/2021 08:29:18 - INFO - __main__ - Before Bug-fixing the results on bug-batch-13 = {'EM': 0.15, 'QA-F1': 0.46561696938525676}
07/01/2021 08:29:18 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.625, 'QA-F1': 0.7748555693236281}
07/01/2021 08:29:18 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:29:19 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:29:22 - INFO - __main__ - After Bug-fixing the results on bug-batch-13 = {'EM': 0.15, 'QA-F1': 0.53802229232114}
07/01/2021 08:29:28 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.65625, 'QA-F1': 0.7823994883335397}
07/01/2021 08:29:28 - INFO - __main__ - Number of em_prefixed_bugs = 3; Number of f1_prefixed_bugs = 9
07/01/2021 08:29:28 - INFO - __main__ - Number of em_fixed_bugs = 0; Number of f1_fixed_bugs = 2
07/01/2021 08:29:28 - INFO - __main__ - Number of em_forgotten_passes = 0.
07/01/2021 08:29:30 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_014.pt.
07/01/2021 08:29:32 - INFO - __main__ - ----------Timecode: 14----------
07/01/2021 08:29:32 - INFO - __main__ - Before Bug-fixing the results on bug-batch-14 = {'EM': 0.05, 'QA-F1': 0.34717808335455397}
07/01/2021 08:29:32 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.65625, 'QA-F1': 0.7823994883335397}
07/01/2021 08:29:32 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:29:34 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:29:36 - INFO - __main__ - After Bug-fixing the results on bug-batch-14 = {'EM': 0.05, 'QA-F1': 0.38017569002305124}
07/01/2021 08:29:42 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.671875, 'QA-F1': 0.7958638264148132}
07/01/2021 08:29:42 - INFO - __main__ - Number of em_prefixed_bugs = 1; Number of f1_prefixed_bugs = 5
07/01/2021 08:29:42 - INFO - __main__ - Number of em_fixed_bugs = 1; Number of f1_fixed_bugs = 2
07/01/2021 08:29:42 - INFO - __main__ - Number of em_forgotten_passes = 1.
07/01/2021 08:29:44 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_015.pt.
07/01/2021 08:29:46 - INFO - __main__ - ----------Timecode: 15----------
07/01/2021 08:29:46 - INFO - __main__ - Before Bug-fixing the results on bug-batch-15 = {'EM': 0.1, 'QA-F1': 0.2941007819598478}
07/01/2021 08:29:46 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.671875, 'QA-F1': 0.7958638264148132}
07/01/2021 08:29:46 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:29:48 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:29:50 - INFO - __main__ - After Bug-fixing the results on bug-batch-15 = {'EM': 0.25, 'QA-F1': 0.43703460853918524}
07/01/2021 08:29:56 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.71875, 'QA-F1': 0.8329761384448885}
07/01/2021 08:29:56 - INFO - __main__ - Number of em_prefixed_bugs = 2; Number of f1_prefixed_bugs = 6
07/01/2021 08:29:56 - INFO - __main__ - Number of em_fixed_bugs = 3; Number of f1_fixed_bugs = 4
07/01/2021 08:29:56 - INFO - __main__ - Number of em_forgotten_passes = 0.
07/01/2021 08:29:58 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_016.pt.
07/01/2021 08:30:01 - INFO - __main__ - ----------Timecode: 16----------
07/01/2021 08:30:01 - INFO - __main__ - Before Bug-fixing the results on bug-batch-16 = {'EM': 0.0, 'QA-F1': 0.2934568366796009}
07/01/2021 08:30:01 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.71875, 'QA-F1': 0.8329761384448885}
07/01/2021 08:30:01 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:30:02 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:30:05 - INFO - __main__ - After Bug-fixing the results on bug-batch-16 = {'EM': 0.15, 'QA-F1': 0.5878567673057588}
07/01/2021 08:30:11 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.703125, 'QA-F1': 0.8162959436396937}
07/01/2021 08:30:11 - INFO - __main__ - Number of em_prefixed_bugs = 0; Number of f1_prefixed_bugs = 11
07/01/2021 08:30:11 - INFO - __main__ - Number of em_fixed_bugs = 3; Number of f1_fixed_bugs = 7
07/01/2021 08:30:11 - INFO - __main__ - Number of em_forgotten_passes = 2.
07/01/2021 08:30:13 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_017.pt.
07/01/2021 08:30:16 - INFO - __main__ - ----------Timecode: 17----------
07/01/2021 08:30:16 - INFO - __main__ - Before Bug-fixing the results on bug-batch-17 = {'EM': 0.05, 'QA-F1': 0.28075424068166}
07/01/2021 08:30:16 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.703125, 'QA-F1': 0.8162959436396937}
07/01/2021 08:30:16 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:30:17 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:30:20 - INFO - __main__ - After Bug-fixing the results on bug-batch-17 = {'EM': 0.1, 'QA-F1': 0.35705937073679006}
07/01/2021 08:30:26 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.671875, 'QA-F1': 0.807057916042291}
07/01/2021 08:30:26 - INFO - __main__ - Number of em_prefixed_bugs = 1; Number of f1_prefixed_bugs = 5
07/01/2021 08:30:26 - INFO - __main__ - Number of em_fixed_bugs = 1; Number of f1_fixed_bugs = 2
07/01/2021 08:30:26 - INFO - __main__ - Number of em_forgotten_passes = 2.
07/01/2021 08:30:28 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_018.pt.
07/01/2021 08:30:31 - INFO - __main__ - ----------Timecode: 18----------
07/01/2021 08:30:31 - INFO - __main__ - Before Bug-fixing the results on bug-batch-18 = {'EM': 0.05, 'QA-F1': 0.2804212386216759}
07/01/2021 08:30:31 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.671875, 'QA-F1': 0.807057916042291}
07/01/2021 08:30:31 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:30:33 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:30:36 - INFO - __main__ - After Bug-fixing the results on bug-batch-18 = {'EM': 0.15, 'QA-F1': 0.3717408026755853}
07/01/2021 08:30:42 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.671875, 'QA-F1': 0.8101844191687941}
07/01/2021 08:30:42 - INFO - __main__ - Number of em_prefixed_bugs = 1; Number of f1_prefixed_bugs = 5
07/01/2021 08:30:42 - INFO - __main__ - Number of em_fixed_bugs = 2; Number of f1_fixed_bugs = 2
07/01/2021 08:30:42 - INFO - __main__ - Number of em_forgotten_passes = 0.
07/01/2021 08:30:44 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_019.pt.
07/01/2021 08:30:47 - INFO - __main__ - ----------Timecode: 19----------
07/01/2021 08:30:47 - INFO - __main__ - Before Bug-fixing the results on bug-batch-19 = {'EM': 0.1, 'QA-F1': 0.27151375938875943}
07/01/2021 08:30:47 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.671875, 'QA-F1': 0.8101844191687941}
07/01/2021 08:30:47 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:30:48 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:30:51 - INFO - __main__ - After Bug-fixing the results on bug-batch-19 = {'EM': 0.1, 'QA-F1': 0.3577597106052989}
07/01/2021 08:30:56 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.75, 'QA-F1': 0.8587893298830798}
07/01/2021 08:30:56 - INFO - __main__ - Number of em_prefixed_bugs = 2; Number of f1_prefixed_bugs = 6
07/01/2021 08:30:56 - INFO - __main__ - Number of em_fixed_bugs = 1; Number of f1_fixed_bugs = 4
07/01/2021 08:30:56 - INFO - __main__ - Number of em_forgotten_passes = 0.
07/01/2021 08:30:58 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_020.pt.
07/01/2021 08:31:01 - INFO - __main__ - ----------Timecode: 20----------
07/01/2021 08:31:01 - INFO - __main__ - Before Bug-fixing the results on bug-batch-20 = {'EM': 0.0, 'QA-F1': 0.15521997236954}
07/01/2021 08:31:01 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.75, 'QA-F1': 0.8587893298830798}
07/01/2021 08:31:01 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:31:02 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:31:05 - INFO - __main__ - After Bug-fixing the results on bug-batch-20 = {'EM': 0.1, 'QA-F1': 0.25709535046996346}
07/01/2021 08:31:10 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.859375, 'QA-F1': 0.9235576923076922}
07/01/2021 08:31:10 - INFO - __main__ - Number of em_prefixed_bugs = 0; Number of f1_prefixed_bugs = 3
07/01/2021 08:31:10 - INFO - __main__ - Number of em_fixed_bugs = 2; Number of f1_fixed_bugs = 2
07/01/2021 08:31:10 - INFO - __main__ - Number of em_forgotten_passes = 0.
07/01/2021 08:31:12 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_021.pt.
07/01/2021 08:31:15 - INFO - __main__ - ----------Timecode: 21----------
07/01/2021 08:31:15 - INFO - __main__ - Before Bug-fixing the results on bug-batch-21 = {'EM': 0.0, 'QA-F1': 0.19687364865845342}
07/01/2021 08:31:15 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.859375, 'QA-F1': 0.9235576923076922}
07/01/2021 08:31:15 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:31:16 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:31:19 - INFO - __main__ - After Bug-fixing the results on bug-batch-21 = {'EM': 0.2, 'QA-F1': 0.468656869080805}
07/01/2021 08:31:24 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.84375, 'QA-F1': 0.9199519230769231}
07/01/2021 08:31:24 - INFO - __main__ - Number of em_prefixed_bugs = 0; Number of f1_prefixed_bugs = 9
07/01/2021 08:31:24 - INFO - __main__ - Number of em_fixed_bugs = 4; Number of f1_fixed_bugs = 7
07/01/2021 08:31:24 - INFO - __main__ - Number of em_forgotten_passes = 1.
07/01/2021 08:31:26 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_022.pt.
07/01/2021 08:31:29 - INFO - __main__ - ----------Timecode: 22----------
07/01/2021 08:31:29 - INFO - __main__ - Before Bug-fixing the results on bug-batch-22 = {'EM': 0.1, 'QA-F1': 0.3079613960412241}
07/01/2021 08:31:29 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.84375, 'QA-F1': 0.9199519230769231}
07/01/2021 08:31:29 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:31:30 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:31:33 - INFO - __main__ - After Bug-fixing the results on bug-batch-22 = {'EM': 0.45, 'QA-F1': 0.5864774779125692}
07/01/2021 08:31:39 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.796875, 'QA-F1': 0.885615166083916}
07/01/2021 08:31:39 - INFO - __main__ - Number of em_prefixed_bugs = 2; Number of f1_prefixed_bugs = 11
07/01/2021 08:31:39 - INFO - __main__ - Number of em_fixed_bugs = 8; Number of f1_fixed_bugs = 8
07/01/2021 08:31:39 - INFO - __main__ - Number of em_forgotten_passes = 3.
07/01/2021 08:31:41 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_023.pt.
07/01/2021 08:31:43 - INFO - __main__ - ----------Timecode: 23----------
07/01/2021 08:31:43 - INFO - __main__ - Before Bug-fixing the results on bug-batch-23 = {'EM': 0.0, 'QA-F1': 0.1275568640614407}
07/01/2021 08:31:43 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.796875, 'QA-F1': 0.885615166083916}
07/01/2021 08:31:43 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:31:45 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:31:47 - INFO - __main__ - After Bug-fixing the results on bug-batch-23 = {'EM': 0.05, 'QA-F1': 0.33045615351107344}
07/01/2021 08:31:54 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.734375, 'QA-F1': 0.8461749188311689}
07/01/2021 08:31:54 - INFO - __main__ - Number of em_prefixed_bugs = 0; Number of f1_prefixed_bugs = 6
07/01/2021 08:31:54 - INFO - __main__ - Number of em_fixed_bugs = 1; Number of f1_fixed_bugs = 5
07/01/2021 08:31:54 - INFO - __main__ - Number of em_forgotten_passes = 4.
07/01/2021 08:31:56 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_024.pt.
07/01/2021 08:31:58 - INFO - __main__ - ----------Timecode: 24----------
07/01/2021 08:31:58 - INFO - __main__ - Before Bug-fixing the results on bug-batch-24 = {'EM': 0.0, 'QA-F1': 0.12933542329132403}
07/01/2021 08:31:58 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.734375, 'QA-F1': 0.8461749188311689}
07/01/2021 08:31:58 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:32:00 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:32:02 - INFO - __main__ - After Bug-fixing the results on bug-batch-24 = {'EM': 0.15, 'QA-F1': 0.2836720248869618}
07/01/2021 08:32:08 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8633624188311688}
07/01/2021 08:32:08 - INFO - __main__ - Number of em_prefixed_bugs = 0; Number of f1_prefixed_bugs = 4
07/01/2021 08:32:08 - INFO - __main__ - Number of em_fixed_bugs = 3; Number of f1_fixed_bugs = 4
07/01/2021 08:32:08 - INFO - __main__ - Number of em_forgotten_passes = 0.
07/01/2021 08:32:10 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_025.pt.
07/01/2021 08:32:13 - INFO - __main__ - ----------Timecode: 25----------
07/01/2021 08:32:13 - INFO - __main__ - Before Bug-fixing the results on bug-batch-25 = {'EM': 0.05, 'QA-F1': 0.28877992684826137}
07/01/2021 08:32:13 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8633624188311688}
07/01/2021 08:32:13 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:32:14 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:32:17 - INFO - __main__ - After Bug-fixing the results on bug-batch-25 = {'EM': 0.2, 'QA-F1': 0.4543245585008651}
07/01/2021 08:32:23 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.6875, 'QA-F1': 0.8096235795454545}
07/01/2021 08:32:23 - INFO - __main__ - Number of em_prefixed_bugs = 1; Number of f1_prefixed_bugs = 9
07/01/2021 08:32:23 - INFO - __main__ - Number of em_fixed_bugs = 3; Number of f1_fixed_bugs = 4
07/01/2021 08:32:23 - INFO - __main__ - Number of em_forgotten_passes = 5.
07/01/2021 08:32:25 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_026.pt.
07/01/2021 08:32:28 - INFO - __main__ - ----------Timecode: 26----------
07/01/2021 08:32:28 - INFO - __main__ - Before Bug-fixing the results on bug-batch-26 = {'EM': 0.15, 'QA-F1': 0.18354545454545454}
07/01/2021 08:32:28 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.6875, 'QA-F1': 0.8096235795454545}
07/01/2021 08:32:28 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:32:30 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:32:32 - INFO - __main__ - After Bug-fixing the results on bug-batch-26 = {'EM': 0.2, 'QA-F1': 0.26703571428571427}
07/01/2021 08:32:38 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8635169517982018}
07/01/2021 08:32:38 - INFO - __main__ - Number of em_prefixed_bugs = 3; Number of f1_prefixed_bugs = 4
07/01/2021 08:32:38 - INFO - __main__ - Number of em_fixed_bugs = 1; Number of f1_fixed_bugs = 1
07/01/2021 08:32:38 - INFO - __main__ - Number of em_forgotten_passes = 0.
07/01/2021 08:32:41 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_027.pt.
07/01/2021 08:32:42 - INFO - __main__ - ----------Timecode: 27----------
07/01/2021 08:32:42 - INFO - __main__ - Before Bug-fixing the results on bug-batch-27 = {'EM': 0.15, 'QA-F1': 0.2244047619047619}
07/01/2021 08:32:42 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8635169517982018}
07/01/2021 08:32:42 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:32:44 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:32:45 - INFO - __main__ - After Bug-fixing the results on bug-batch-27 = {'EM': 0.35, 'QA-F1': 0.4023809523809524}
07/01/2021 08:32:51 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.8947601877289377}
07/01/2021 08:32:51 - INFO - __main__ - Number of em_prefixed_bugs = 3; Number of f1_prefixed_bugs = 8
07/01/2021 08:32:51 - INFO - __main__ - Number of em_fixed_bugs = 4; Number of f1_fixed_bugs = 4
07/01/2021 08:32:51 - INFO - __main__ - Number of em_forgotten_passes = 0.
07/01/2021 08:32:52 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_028.pt.
07/01/2021 08:32:54 - INFO - __main__ - ----------Timecode: 28----------
07/01/2021 08:32:54 - INFO - __main__ - Before Bug-fixing the results on bug-batch-28 = {'EM': 0.1, 'QA-F1': 0.1287037037037037}
07/01/2021 08:32:54 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.8947601877289377}
07/01/2021 08:32:54 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:32:56 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:32:58 - INFO - __main__ - After Bug-fixing the results on bug-batch-28 = {'EM': 0.3, 'QA-F1': 0.3322751322751323}
07/01/2021 08:33:03 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.8948317307692308}
07/01/2021 08:33:03 - INFO - __main__ - Number of em_prefixed_bugs = 2; Number of f1_prefixed_bugs = 7
07/01/2021 08:33:03 - INFO - __main__ - Number of em_fixed_bugs = 4; Number of f1_fixed_bugs = 4
07/01/2021 08:33:03 - INFO - __main__ - Number of em_forgotten_passes = 1.
07/01/2021 08:33:05 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_029.pt.
07/01/2021 08:33:07 - INFO - __main__ - ----------Timecode: 29----------
07/01/2021 08:33:07 - INFO - __main__ - Before Bug-fixing the results on bug-batch-29 = {'EM': 0.1, 'QA-F1': 0.13076923076923075}
07/01/2021 08:33:07 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.8948317307692308}
07/01/2021 08:33:07 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:33:09 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:33:11 - INFO - __main__ - After Bug-fixing the results on bug-batch-29 = {'EM': 0.2, 'QA-F1': 0.24871794871794872}
07/01/2021 08:33:17 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.8970638736263736}
07/01/2021 08:33:17 - INFO - __main__ - Number of em_prefixed_bugs = 2; Number of f1_prefixed_bugs = 5
07/01/2021 08:33:17 - INFO - __main__ - Number of em_fixed_bugs = 2; Number of f1_fixed_bugs = 3
07/01/2021 08:33:17 - INFO - __main__ - Number of em_forgotten_passes = 1.
07/01/2021 08:33:18 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_030.pt.
07/01/2021 08:33:21 - INFO - __main__ - ----------Timecode: 30----------
07/01/2021 08:33:21 - INFO - __main__ - Before Bug-fixing the results on bug-batch-30 = {'EM': 0.05, 'QA-F1': 0.0875}
07/01/2021 08:33:21 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.8970638736263736}
07/01/2021 08:33:21 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:33:23 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:33:25 - INFO - __main__ - After Bug-fixing the results on bug-batch-30 = {'EM': 0.2, 'QA-F1': 0.2708333333333333}
07/01/2021 08:33:30 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.8954613095238095}
07/01/2021 08:33:30 - INFO - __main__ - Number of em_prefixed_bugs = 1; Number of f1_prefixed_bugs = 5
07/01/2021 08:33:30 - INFO - __main__ - Number of em_fixed_bugs = 3; Number of f1_fixed_bugs = 4
07/01/2021 08:33:30 - INFO - __main__ - Number of em_forgotten_passes = 0.
07/01/2021 08:33:32 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_031.pt.
07/01/2021 08:33:34 - INFO - __main__ - ----------Timecode: 31----------
07/01/2021 08:33:34 - INFO - __main__ - Before Bug-fixing the results on bug-batch-31 = {'EM': 0.2, 'QA-F1': 0.24500000000000002}
07/01/2021 08:33:34 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.8954613095238095}
07/01/2021 08:33:34 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:33:36 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:33:39 - INFO - __main__ - After Bug-fixing the results on bug-batch-31 = {'EM': 0.55, 'QA-F1': 0.5866477272727273}
07/01/2021 08:33:44 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.8954613095238095}
07/01/2021 08:33:44 - INFO - __main__ - Number of em_prefixed_bugs = 4; Number of f1_prefixed_bugs = 12
07/01/2021 08:33:44 - INFO - __main__ - Number of em_fixed_bugs = 7; Number of f1_fixed_bugs = 7
07/01/2021 08:33:44 - INFO - __main__ - Number of em_forgotten_passes = 0.
07/01/2021 08:33:46 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_032.pt.
07/01/2021 08:33:49 - INFO - __main__ - ----------Timecode: 32----------
07/01/2021 08:33:49 - INFO - __main__ - Before Bug-fixing the results on bug-batch-32 = {'EM': 0.15, 'QA-F1': 0.18218253968253967}
07/01/2021 08:33:49 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.8954613095238095}
07/01/2021 08:33:49 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:33:51 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:33:52 - INFO - __main__ - After Bug-fixing the results on bug-batch-32 = {'EM': 0.5, 'QA-F1': 0.5515873015873016}
07/01/2021 08:33:58 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.8966280743066457}
07/01/2021 08:33:58 - INFO - __main__ - Number of em_prefixed_bugs = 3; Number of f1_prefixed_bugs = 11
07/01/2021 08:33:58 - INFO - __main__ - Number of em_fixed_bugs = 7; Number of f1_fixed_bugs = 8
07/01/2021 08:33:58 - INFO - __main__ - Number of em_forgotten_passes = 2.
07/01/2021 08:34:01 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_033.pt.
07/01/2021 08:34:02 - INFO - __main__ - ----------Timecode: 33----------
07/01/2021 08:34:02 - INFO - __main__ - Before Bug-fixing the results on bug-batch-33 = {'EM': 0.15, 'QA-F1': 0.20833333333333331}
07/01/2021 08:34:02 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.8966280743066457}
07/01/2021 08:34:02 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:34:04 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:34:06 - INFO - __main__ - After Bug-fixing the results on bug-batch-33 = {'EM': 0.3, 'QA-F1': 0.397463768115942}
07/01/2021 08:34:11 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.8125, 'QA-F1': 0.8960271127681843}
07/01/2021 08:34:11 - INFO - __main__ - Number of em_prefixed_bugs = 3; Number of f1_prefixed_bugs = 8
07/01/2021 08:34:11 - INFO - __main__ - Number of em_fixed_bugs = 3; Number of f1_fixed_bugs = 4
07/01/2021 08:34:11 - INFO - __main__ - Number of em_forgotten_passes = 2.
07/01/2021 08:34:14 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_034.pt.
07/01/2021 08:34:16 - INFO - __main__ - ----------Timecode: 34----------
07/01/2021 08:34:16 - INFO - __main__ - Before Bug-fixing the results on bug-batch-34 = {'EM': 0.1, 'QA-F1': 0.125}
07/01/2021 08:34:16 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.8125, 'QA-F1': 0.8960271127681843}
07/01/2021 08:34:16 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:34:18 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:34:21 - INFO - __main__ - After Bug-fixing the results on bug-batch-34 = {'EM': 0.25, 'QA-F1': 0.2611455108359133}
07/01/2021 08:34:26 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8477632238792953}
07/01/2021 08:34:26 - INFO - __main__ - Number of em_prefixed_bugs = 2; Number of f1_prefixed_bugs = 5
07/01/2021 08:34:26 - INFO - __main__ - Number of em_fixed_bugs = 3; Number of f1_fixed_bugs = 2
07/01/2021 08:34:26 - INFO - __main__ - Number of em_forgotten_passes = 4.
07/01/2021 08:34:28 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_035.pt.
07/01/2021 08:34:31 - INFO - __main__ - ----------Timecode: 35----------
07/01/2021 08:34:31 - INFO - __main__ - Before Bug-fixing the results on bug-batch-35 = {'EM': 0.0, 'QA-F1': 0.09833333333333331}
07/01/2021 08:34:31 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8477632238792953}
07/01/2021 08:34:31 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:34:33 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:34:35 - INFO - __main__ - After Bug-fixing the results on bug-batch-35 = {'EM': 0.3, 'QA-F1': 0.5161111111111111}
07/01/2021 08:34:40 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8551191492039707}
07/01/2021 08:34:40 - INFO - __main__ - Number of em_prefixed_bugs = 0; Number of f1_prefixed_bugs = 12
07/01/2021 08:34:40 - INFO - __main__ - Number of em_fixed_bugs = 6; Number of f1_fixed_bugs = 9
07/01/2021 08:34:40 - INFO - __main__ - Number of em_forgotten_passes = 2.
07/01/2021 08:34:42 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_036.pt.
07/01/2021 08:34:45 - INFO - __main__ - ----------Timecode: 36----------
07/01/2021 08:34:45 - INFO - __main__ - Before Bug-fixing the results on bug-batch-36 = {'EM': 0.2, 'QA-F1': 0.29829545454545453}
07/01/2021 08:34:45 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8551191492039707}
07/01/2021 08:34:45 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:34:47 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:34:49 - INFO - __main__ - After Bug-fixing the results on bug-batch-36 = {'EM': 0.2, 'QA-F1': 0.336901554564598}
07/01/2021 08:34:54 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.75, 'QA-F1': 0.857778972077664}
07/01/2021 08:34:54 - INFO - __main__ - Number of em_prefixed_bugs = 4; Number of f1_prefixed_bugs = 7
07/01/2021 08:34:54 - INFO - __main__ - Number of em_fixed_bugs = 0; Number of f1_fixed_bugs = 1
07/01/2021 08:34:54 - INFO - __main__ - Number of em_forgotten_passes = 3.
07/01/2021 08:34:56 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_037.pt.
07/01/2021 08:34:58 - INFO - __main__ - ----------Timecode: 37----------
07/01/2021 08:34:58 - INFO - __main__ - Before Bug-fixing the results on bug-batch-37 = {'EM': 0.1, 'QA-F1': 0.2263688633348076}
07/01/2021 08:34:58 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.75, 'QA-F1': 0.857778972077664}
07/01/2021 08:34:58 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:35:00 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:35:02 - INFO - __main__ - After Bug-fixing the results on bug-batch-37 = {'EM': 0.3, 'QA-F1': 0.3837218045112782}
07/01/2021 08:35:08 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.734375, 'QA-F1': 0.8410689026332195}
07/01/2021 08:35:08 - INFO - __main__ - Number of em_prefixed_bugs = 2; Number of f1_prefixed_bugs = 7
07/01/2021 08:35:08 - INFO - __main__ - Number of em_fixed_bugs = 5; Number of f1_fixed_bugs = 4
07/01/2021 08:35:08 - INFO - __main__ - Number of em_forgotten_passes = 1.
07/01/2021 08:35:10 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_038.pt.
07/01/2021 08:35:12 - INFO - __main__ - ----------Timecode: 38----------
07/01/2021 08:35:12 - INFO - __main__ - Before Bug-fixing the results on bug-batch-38 = {'EM': 0.1, 'QA-F1': 0.12}
07/01/2021 08:35:12 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.734375, 'QA-F1': 0.8410689026332195}
07/01/2021 08:35:12 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:35:14 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:35:17 - INFO - __main__ - After Bug-fixing the results on bug-batch-38 = {'EM': 0.3, 'QA-F1': 0.3919047619047619}
07/01/2021 08:35:23 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.78125, 'QA-F1': 0.8548308462370962}
07/01/2021 08:35:23 - INFO - __main__ - Number of em_prefixed_bugs = 2; Number of f1_prefixed_bugs = 8
07/01/2021 08:35:23 - INFO - __main__ - Number of em_fixed_bugs = 4; Number of f1_fixed_bugs = 6
07/01/2021 08:35:23 - INFO - __main__ - Number of em_forgotten_passes = 1.
07/01/2021 08:35:25 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_039.pt.
07/01/2021 08:35:28 - INFO - __main__ - ----------Timecode: 39----------
07/01/2021 08:35:28 - INFO - __main__ - Before Bug-fixing the results on bug-batch-39 = {'EM': 0.1, 'QA-F1': 0.18429339477726575}
07/01/2021 08:35:28 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.78125, 'QA-F1': 0.8548308462370962}
07/01/2021 08:35:28 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:35:29 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:35:31 - INFO - __main__ - After Bug-fixing the results on bug-batch-39 = {'EM': 0.15, 'QA-F1': 0.25459770114942526}
07/01/2021 08:35:37 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.8125, 'QA-F1': 0.8829212454212454}
07/01/2021 08:35:37 - INFO - __main__ - Number of em_prefixed_bugs = 2; Number of f1_prefixed_bugs = 4
07/01/2021 08:35:37 - INFO - __main__ - Number of em_fixed_bugs = 1; Number of f1_fixed_bugs = 1
07/01/2021 08:35:37 - INFO - __main__ - Number of em_forgotten_passes = 2.
07/01/2021 08:35:39 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_040.pt.
07/01/2021 08:35:41 - INFO - __main__ - ----------Timecode: 40----------
07/01/2021 08:35:41 - INFO - __main__ - Before Bug-fixing the results on bug-batch-40 = {'EM': 0.0, 'QA-F1': 0.04}
07/01/2021 08:35:41 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.8125, 'QA-F1': 0.8829212454212454}
07/01/2021 08:35:41 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:35:44 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:35:46 - INFO - __main__ - After Bug-fixing the results on bug-batch-40 = {'EM': 0.1, 'QA-F1': 0.19545454545454546}
07/01/2021 08:35:52 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.84375, 'QA-F1': 0.8949404761904762}
07/01/2021 08:35:52 - INFO - __main__ - Number of em_prefixed_bugs = 0; Number of f1_prefixed_bugs = 4
07/01/2021 08:35:52 - INFO - __main__ - Number of em_fixed_bugs = 2; Number of f1_fixed_bugs = 3
07/01/2021 08:35:52 - INFO - __main__ - Number of em_forgotten_passes = 0.
07/01/2021 08:35:54 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_041.pt.
07/01/2021 08:35:56 - INFO - __main__ - ----------Timecode: 41----------
07/01/2021 08:35:56 - INFO - __main__ - Before Bug-fixing the results on bug-batch-41 = {'EM': 0.05, 'QA-F1': 0.09761904761904762}
07/01/2021 08:35:56 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.84375, 'QA-F1': 0.8949404761904762}
07/01/2021 08:35:56 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:35:57 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:35:59 - INFO - __main__ - After Bug-fixing the results on bug-batch-41 = {'EM': 0.2, 'QA-F1': 0.3780209038273554}
07/01/2021 08:36:05 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.8897321428571427}
07/01/2021 08:36:05 - INFO - __main__ - Number of em_prefixed_bugs = 1; Number of f1_prefixed_bugs = 9
07/01/2021 08:36:05 - INFO - __main__ - Number of em_fixed_bugs = 3; Number of f1_fixed_bugs = 7
07/01/2021 08:36:05 - INFO - __main__ - Number of em_forgotten_passes = 1.
07/01/2021 08:36:07 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_042.pt.
07/01/2021 08:36:09 - INFO - __main__ - ----------Timecode: 42----------
07/01/2021 08:36:09 - INFO - __main__ - Before Bug-fixing the results on bug-batch-42 = {'EM': 0.05, 'QA-F1': 0.16785714285714287}
07/01/2021 08:36:09 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.8897321428571427}
07/01/2021 08:36:09 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:36:10 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:36:12 - INFO - __main__ - After Bug-fixing the results on bug-batch-42 = {'EM': 0.3, 'QA-F1': 0.42857142857142855}
07/01/2021 08:36:18 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.8871279761904762}
07/01/2021 08:36:18 - INFO - __main__ - Number of em_prefixed_bugs = 1; Number of f1_prefixed_bugs = 9
07/01/2021 08:36:18 - INFO - __main__ - Number of em_fixed_bugs = 6; Number of f1_fixed_bugs = 6
07/01/2021 08:36:18 - INFO - __main__ - Number of em_forgotten_passes = 1.
07/01/2021 08:36:20 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_043.pt.
07/01/2021 08:36:22 - INFO - __main__ - ----------Timecode: 43----------
07/01/2021 08:36:22 - INFO - __main__ - Before Bug-fixing the results on bug-batch-43 = {'EM': 0.0, 'QA-F1': 0.005263157894736843}
07/01/2021 08:36:22 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.828125, 'QA-F1': 0.8871279761904762}
07/01/2021 08:36:22 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:36:24 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:36:25 - INFO - __main__ - After Bug-fixing the results on bug-batch-43 = {'EM': 0.25, 'QA-F1': 0.33444444444444443}
07/01/2021 08:36:30 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.8125, 'QA-F1': 0.8811183608058608}
07/01/2021 08:36:30 - INFO - __main__ - Number of em_prefixed_bugs = 0; Number of f1_prefixed_bugs = 7
07/01/2021 08:36:30 - INFO - __main__ - Number of em_fixed_bugs = 5; Number of f1_fixed_bugs = 7
07/01/2021 08:36:30 - INFO - __main__ - Number of em_forgotten_passes = 1.
07/01/2021 08:36:32 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_044.pt.
07/01/2021 08:36:34 - INFO - __main__ - ----------Timecode: 44----------
07/01/2021 08:36:34 - INFO - __main__ - Before Bug-fixing the results on bug-batch-44 = {'EM': 0.2, 'QA-F1': 0.2333333333333333}
07/01/2021 08:36:34 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.8125, 'QA-F1': 0.8811183608058608}
07/01/2021 08:36:34 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:36:36 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:36:37 - INFO - __main__ - After Bug-fixing the results on bug-batch-44 = {'EM': 0.4, 'QA-F1': 0.4466666666666666}
07/01/2021 08:36:43 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8612580128205127}
07/01/2021 08:36:43 - INFO - __main__ - Number of em_prefixed_bugs = 4; Number of f1_prefixed_bugs = 9
07/01/2021 08:36:43 - INFO - __main__ - Number of em_fixed_bugs = 4; Number of f1_fixed_bugs = 5
07/01/2021 08:36:43 - INFO - __main__ - Number of em_forgotten_passes = 3.
07/01/2021 08:36:45 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_045.pt.
07/01/2021 08:36:46 - INFO - __main__ - ----------Timecode: 45----------
07/01/2021 08:36:46 - INFO - __main__ - Before Bug-fixing the results on bug-batch-45 = {'EM': 0.1, 'QA-F1': 0.12222222222222223}
07/01/2021 08:36:46 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8612580128205127}
07/01/2021 08:36:46 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:36:48 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:36:50 - INFO - __main__ - After Bug-fixing the results on bug-batch-45 = {'EM': 0.2, 'QA-F1': 0.25130718954248366}
07/01/2021 08:36:56 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8610176282051283}
07/01/2021 08:36:56 - INFO - __main__ - Number of em_prefixed_bugs = 2; Number of f1_prefixed_bugs = 4
07/01/2021 08:36:56 - INFO - __main__ - Number of em_fixed_bugs = 2; Number of f1_fixed_bugs = 2
07/01/2021 08:36:56 - INFO - __main__ - Number of em_forgotten_passes = 2.
07/01/2021 08:36:58 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_046.pt.
07/01/2021 08:37:00 - INFO - __main__ - ----------Timecode: 46----------
07/01/2021 08:37:00 - INFO - __main__ - Before Bug-fixing the results on bug-batch-46 = {'EM': 0.05, 'QA-F1': 0.09761904761904762}
07/01/2021 08:37:00 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8610176282051283}
07/01/2021 08:37:00 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:37:02 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:37:04 - INFO - __main__ - After Bug-fixing the results on bug-batch-46 = {'EM': 0.3, 'QA-F1': 0.3476190476190476}
07/01/2021 08:37:10 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8572315705128205}
07/01/2021 08:37:10 - INFO - __main__ - Number of em_prefixed_bugs = 1; Number of f1_prefixed_bugs = 7
07/01/2021 08:37:10 - INFO - __main__ - Number of em_fixed_bugs = 5; Number of f1_fixed_bugs = 5
07/01/2021 08:37:10 - INFO - __main__ - Number of em_forgotten_passes = 1.
07/01/2021 08:37:12 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_047.pt.
07/01/2021 08:37:14 - INFO - __main__ - ----------Timecode: 47----------
07/01/2021 08:37:14 - INFO - __main__ - Before Bug-fixing the results on bug-batch-47 = {'EM': 0.1, 'QA-F1': 0.14285714285714285}
07/01/2021 08:37:14 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8572315705128205}
07/01/2021 08:37:14 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:37:16 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:37:18 - INFO - __main__ - After Bug-fixing the results on bug-batch-47 = {'EM': 0.15, 'QA-F1': 0.2197802197802198}
07/01/2021 08:37:23 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8664067158616577}
07/01/2021 08:37:23 - INFO - __main__ - Number of em_prefixed_bugs = 2; Number of f1_prefixed_bugs = 5
07/01/2021 08:37:23 - INFO - __main__ - Number of em_fixed_bugs = 1; Number of f1_fixed_bugs = 2
07/01/2021 08:37:23 - INFO - __main__ - Number of em_forgotten_passes = 1.
07/01/2021 08:37:25 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_048.pt.
07/01/2021 08:37:28 - INFO - __main__ - ----------Timecode: 48----------
07/01/2021 08:37:28 - INFO - __main__ - Before Bug-fixing the results on bug-batch-48 = {'EM': 0.0, 'QA-F1': 0.07727956254272045}
07/01/2021 08:37:28 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8664067158616577}
07/01/2021 08:37:28 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:37:29 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:37:31 - INFO - __main__ - After Bug-fixing the results on bug-batch-48 = {'EM': 0.15, 'QA-F1': 0.3058847237954743}
07/01/2021 08:37:37 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.75, 'QA-F1': 0.8478971004770424}
07/01/2021 08:37:37 - INFO - __main__ - Number of em_prefixed_bugs = 0; Number of f1_prefixed_bugs = 6
07/01/2021 08:37:37 - INFO - __main__ - Number of em_fixed_bugs = 3; Number of f1_fixed_bugs = 6
07/01/2021 08:37:37 - INFO - __main__ - Number of em_forgotten_passes = 2.
07/01/2021 08:37:39 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_049.pt.
07/01/2021 08:37:41 - INFO - __main__ - ----------Timecode: 49----------
07/01/2021 08:37:41 - INFO - __main__ - Before Bug-fixing the results on bug-batch-49 = {'EM': 0.15, 'QA-F1': 0.21356837606837606}
07/01/2021 08:37:41 - INFO - __main__ - Before Bug-fixing the results on the sampled pass cases = {'EM': 0.75, 'QA-F1': 0.8478971004770424}
07/01/2021 08:37:41 - INFO - __main__ - Start bug-fixing ....
07/01/2021 08:37:43 - INFO - __main__ - Start bug-fixing .... Done!
07/01/2021 08:37:45 - INFO - __main__ - After Bug-fixing the results on bug-batch-49 = {'EM': 0.5, 'QA-F1': 0.5434178743961352}
07/01/2021 08:37:51 - INFO - __main__ - After Bug-fixing the results on the sampled pass cases = {'EM': 0.765625, 'QA-F1': 0.8474964594514014}
07/01/2021 08:37:51 - INFO - __main__ - Number of em_prefixed_bugs = 3; Number of f1_prefixed_bugs = 10
07/01/2021 08:37:51 - INFO - __main__ - Number of em_fixed_bugs = 7; Number of f1_fixed_bugs = 6
07/01/2021 08:37:51 - INFO - __main__ - Number of em_forgotten_passes = 0.
07/01/2021 08:37:53 - INFO - __main__ - Model saved to bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_050.pt.
07/01/2021 08:39:31 - INFO - __main__ - Final Overall Bug-fixing Results = {'EM': 0.432, 'QA-F1': 0.5842150722302881}
07/01/2021 08:39:31 - INFO - __main__ - Finished. Results saved to bug_data/output/nq_dev_0701v3_1e-5_e3_result.json
07/01/2021 09:08:37 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', current_thread_id=3, do_lowercase=False, freeze_embeds=False, gradient_accumulation_steps=1, learning_rate=1e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, num_threads_eval=8, num_train_epochs=3.0, overtime_ckpt_dir='bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/', overtime_overall_bug_eval=0, pass_pool_jsonl_path='bug_data/mrqa_naturalquestions_dev.pass.jsonl', pass_sample_size=64, path_to_thread_result='bug_data/output/nq_dev_0701v3_1e-5_e3_offline_eval/thread_3_result.json', predict_batch_size=168, prefix='nq_dev_0701v3_1e-5_e3', result_file='bug_data/output/nq_dev_0701v3_1e-5_e3_result.json', save_all_ckpts=0, seed=42, task_name='mrqa_naturalquestions', train_batch_size=8, weight_decay=0.01)
07/01/2021 09:08:37 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', current_thread_id=1, do_lowercase=False, freeze_embeds=False, gradient_accumulation_steps=1, learning_rate=1e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, num_threads_eval=8, num_train_epochs=3.0, overtime_ckpt_dir='bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/', overtime_overall_bug_eval=0, pass_pool_jsonl_path='bug_data/mrqa_naturalquestions_dev.pass.jsonl', pass_sample_size=64, path_to_thread_result='bug_data/output/nq_dev_0701v3_1e-5_e3_offline_eval/thread_1_result.json', predict_batch_size=168, prefix='nq_dev_0701v3_1e-5_e3', result_file='bug_data/output/nq_dev_0701v3_1e-5_e3_result.json', save_all_ckpts=0, seed=42, task_name='mrqa_naturalquestions', train_batch_size=8, weight_decay=0.01)
07/01/2021 09:08:37 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', current_thread_id=4, do_lowercase=False, freeze_embeds=False, gradient_accumulation_steps=1, learning_rate=1e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, num_threads_eval=8, num_train_epochs=3.0, overtime_ckpt_dir='bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/', overtime_overall_bug_eval=0, pass_pool_jsonl_path='bug_data/mrqa_naturalquestions_dev.pass.jsonl', pass_sample_size=64, path_to_thread_result='bug_data/output/nq_dev_0701v3_1e-5_e3_offline_eval/thread_4_result.json', predict_batch_size=168, prefix='nq_dev_0701v3_1e-5_e3', result_file='bug_data/output/nq_dev_0701v3_1e-5_e3_result.json', save_all_ckpts=0, seed=42, task_name='mrqa_naturalquestions', train_batch_size=8, weight_decay=0.01)
07/01/2021 09:08:37 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', current_thread_id=0, do_lowercase=False, freeze_embeds=False, gradient_accumulation_steps=1, learning_rate=1e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, num_threads_eval=8, num_train_epochs=3.0, overtime_ckpt_dir='bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/', overtime_overall_bug_eval=0, pass_pool_jsonl_path='bug_data/mrqa_naturalquestions_dev.pass.jsonl', pass_sample_size=64, path_to_thread_result='bug_data/output/nq_dev_0701v3_1e-5_e3_offline_eval/thread_0_result.json', predict_batch_size=168, prefix='nq_dev_0701v3_1e-5_e3', result_file='bug_data/output/nq_dev_0701v3_1e-5_e3_result.json', save_all_ckpts=0, seed=42, task_name='mrqa_naturalquestions', train_batch_size=8, weight_decay=0.01)
07/01/2021 09:08:37 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', current_thread_id=6, do_lowercase=False, freeze_embeds=False, gradient_accumulation_steps=1, learning_rate=1e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, num_threads_eval=8, num_train_epochs=3.0, overtime_ckpt_dir='bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/', overtime_overall_bug_eval=0, pass_pool_jsonl_path='bug_data/mrqa_naturalquestions_dev.pass.jsonl', pass_sample_size=64, path_to_thread_result='bug_data/output/nq_dev_0701v3_1e-5_e3_offline_eval/thread_6_result.json', predict_batch_size=168, prefix='nq_dev_0701v3_1e-5_e3', result_file='bug_data/output/nq_dev_0701v3_1e-5_e3_result.json', save_all_ckpts=0, seed=42, task_name='mrqa_naturalquestions', train_batch_size=8, weight_decay=0.01)
07/01/2021 09:08:37 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', current_thread_id=7, do_lowercase=False, freeze_embeds=False, gradient_accumulation_steps=1, learning_rate=1e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, num_threads_eval=8, num_train_epochs=3.0, overtime_ckpt_dir='bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/', overtime_overall_bug_eval=0, pass_pool_jsonl_path='bug_data/mrqa_naturalquestions_dev.pass.jsonl', pass_sample_size=64, path_to_thread_result='bug_data/output/nq_dev_0701v3_1e-5_e3_offline_eval/thread_7_result.json', predict_batch_size=168, prefix='nq_dev_0701v3_1e-5_e3', result_file='bug_data/output/nq_dev_0701v3_1e-5_e3_result.json', save_all_ckpts=0, seed=42, task_name='mrqa_naturalquestions', train_batch_size=8, weight_decay=0.01)
07/01/2021 09:08:37 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', current_thread_id=2, do_lowercase=False, freeze_embeds=False, gradient_accumulation_steps=1, learning_rate=1e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, num_threads_eval=8, num_train_epochs=3.0, overtime_ckpt_dir='bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/', overtime_overall_bug_eval=0, pass_pool_jsonl_path='bug_data/mrqa_naturalquestions_dev.pass.jsonl', pass_sample_size=64, path_to_thread_result='bug_data/output/nq_dev_0701v3_1e-5_e3_offline_eval/thread_2_result.json', predict_batch_size=168, prefix='nq_dev_0701v3_1e-5_e3', result_file='bug_data/output/nq_dev_0701v3_1e-5_e3_result.json', save_all_ckpts=0, seed=42, task_name='mrqa_naturalquestions', train_batch_size=8, weight_decay=0.01)
07/01/2021 09:08:37 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', current_thread_id=5, do_lowercase=False, freeze_embeds=False, gradient_accumulation_steps=1, learning_rate=1e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, num_threads_eval=8, num_train_epochs=3.0, overtime_ckpt_dir='bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/', overtime_overall_bug_eval=0, pass_pool_jsonl_path='bug_data/mrqa_naturalquestions_dev.pass.jsonl', pass_sample_size=64, path_to_thread_result='bug_data/output/nq_dev_0701v3_1e-5_e3_offline_eval/thread_5_result.json', predict_batch_size=168, prefix='nq_dev_0701v3_1e-5_e3', result_file='bug_data/output/nq_dev_0701v3_1e-5_e3_result.json', save_all_ckpts=0, seed=42, task_name='mrqa_naturalquestions', train_batch_size=8, weight_decay=0.01)
07/01/2021 09:08:37 - INFO - __main__ - Namespace(adam_epsilon=1e-08, append_another_bos=1, base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', base_model_type='facebook/bart-base', bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', current_thread_id=8, do_lowercase=False, freeze_embeds=False, gradient_accumulation_steps=1, learning_rate=1e-05, max_grad_norm=0.1, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, num_threads_eval=8, num_train_epochs=3.0, overtime_ckpt_dir='bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/', overtime_overall_bug_eval=0, pass_pool_jsonl_path='bug_data/mrqa_naturalquestions_dev.pass.jsonl', pass_sample_size=64, path_to_thread_result='bug_data/output/nq_dev_0701v3_1e-5_e3_offline_eval/thread_8_result.json', predict_batch_size=168, prefix='nq_dev_0701v3_1e-5_e3', result_file='bug_data/output/nq_dev_0701v3_1e-5_e3_result.json', save_all_ckpts=0, seed=42, task_name='mrqa_naturalquestions', train_batch_size=8, weight_decay=0.01)
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
07/01/2021 09:08:38 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
07/01/2021 09:08:42 - INFO - __main__ - Starting the offline evaluation of 8
07/01/2021 09:08:42 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_008.pt for facebook/bart-base .....
07/01/2021 09:08:43 - INFO - __main__ - Starting the offline evaluation of 39
07/01/2021 09:08:43 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_039.pt for facebook/bart-base .....
07/01/2021 09:08:45 - INFO - __main__ - Starting the offline evaluation of 21
07/01/2021 09:08:45 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_021.pt for facebook/bart-base .....
07/01/2021 09:08:45 - INFO - __main__ - Starting the offline evaluation of 45
07/01/2021 09:08:45 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_045.pt for facebook/bart-base .....
07/01/2021 09:08:45 - INFO - __main__ - Starting the offline evaluation of 33
07/01/2021 09:08:45 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_033.pt for facebook/bart-base .....
07/01/2021 09:08:45 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:08:45 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:08:45 - INFO - __main__ - Starting the offline evaluation of 27
07/01/2021 09:08:45 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_027.pt for facebook/bart-base .....
07/01/2021 09:08:46 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:08:46 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:08:46 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:08:46 - INFO - __main__ - Starting the offline evaluation of 1
07/01/2021 09:08:46 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_001.pt for facebook/bart-base .....
07/01/2021 09:08:46 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:08:46 - INFO - __main__ - Starting the offline evaluation of 15
07/01/2021 09:08:46 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_015.pt for facebook/bart-base .....
07/01/2021 09:08:50 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:08:51 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:08:51 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:08:51 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:08:51 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:08:51 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:08:51 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:08:51 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:08:51 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:08:51 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:08:51 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:08:51 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:08:51 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_008.pt for facebook/bart-base ..... Done!
07/01/2021 09:08:51 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:08:51 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:08:51 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:08:51 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:08:51 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:08:51 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:08:52 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_039.pt for facebook/bart-base ..... Done!
07/01/2021 09:08:55 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:08:55 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:08:56 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_027.pt for facebook/bart-base ..... Done!
07/01/2021 09:08:59 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_015.pt for facebook/bart-base ..... Done!
07/01/2021 09:08:59 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_045.pt for facebook/bart-base ..... Done!
07/01/2021 09:08:59 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_021.pt for facebook/bart-base ..... Done!
07/01/2021 09:08:59 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_001.pt for facebook/bart-base ..... Done!
07/01/2021 09:09:00 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_033.pt for facebook/bart-base ..... Done!
07/01/2021 09:09:02 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:09:04 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:09:04 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:09:04 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:09:05 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:09:05 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:10:21 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.347, 'QA-F1': 0.5013910478487481} at Timecode=39
07/01/2021 09:10:21 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 39, "results": {"EM": 0.347, "QA-F1": 0.5013910478487481}}}
07/01/2021 09:10:21 - INFO - __main__ - Starting the offline evaluation of 40
07/01/2021 09:10:21 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_040.pt for facebook/bart-base .....
07/01/2021 09:10:24 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:10:24 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:10:24 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:10:29 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.221, 'QA-F1': 0.3474716343744247} at Timecode=21
07/01/2021 09:10:29 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 21, "results": {"EM": 0.221, "QA-F1": 0.3474716343744247}}}
07/01/2021 09:10:29 - INFO - __main__ - Starting the offline evaluation of 22
07/01/2021 09:10:29 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_022.pt for facebook/bart-base .....
07/01/2021 09:10:31 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_040.pt for facebook/bart-base ..... Done!
07/01/2021 09:10:31 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:10:31 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:10:31 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:10:32 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:10:39 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_022.pt for facebook/bart-base ..... Done!
07/01/2021 09:10:39 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:10:43 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.423, 'QA-F1': 0.56507003002967} at Timecode=45
07/01/2021 09:10:43 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 45, "results": {"EM": 0.423, "QA-F1": 0.56507003002967}}}
07/01/2021 09:10:43 - INFO - __main__ - Starting the offline evaluation of 46
07/01/2021 09:10:43 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_046.pt for facebook/bart-base .....
07/01/2021 09:10:43 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.119, 'QA-F1': 0.26337748014086576} at Timecode=8
07/01/2021 09:10:43 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 8, "results": {"EM": 0.119, "QA-F1": 0.26337748014086576}}}
07/01/2021 09:10:43 - INFO - __main__ - Starting the offline evaluation of 9
07/01/2021 09:10:43 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_009.pt for facebook/bart-base .....
07/01/2021 09:10:45 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.35, 'QA-F1': 0.47494510860502726} at Timecode=33
07/01/2021 09:10:45 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 33, "results": {"EM": 0.35, "QA-F1": 0.47494510860502726}}}
07/01/2021 09:10:45 - INFO - __main__ - Starting the offline evaluation of 34
07/01/2021 09:10:45 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_034.pt for facebook/bart-base .....
07/01/2021 09:10:45 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:10:45 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:10:45 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:10:46 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:10:46 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:10:46 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:10:48 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.227, 'QA-F1': 0.38297550659550456} at Timecode=27
07/01/2021 09:10:48 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 27, "results": {"EM": 0.227, "QA-F1": 0.38297550659550456}}}
07/01/2021 09:10:48 - INFO - __main__ - Starting the offline evaluation of 28
07/01/2021 09:10:48 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_028.pt for facebook/bart-base .....
07/01/2021 09:10:50 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.036, 'QA-F1': 0.19495468307512448} at Timecode=1
07/01/2021 09:10:50 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 1, "results": {"EM": 0.036, "QA-F1": 0.19495468307512448}}}
07/01/2021 09:10:50 - INFO - __main__ - Starting the offline evaluation of 2
07/01/2021 09:10:50 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_002.pt for facebook/bart-base .....
07/01/2021 09:10:50 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:10:50 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:10:50 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:10:51 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:10:52 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:10:52 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:10:52 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.152, 'QA-F1': 0.3109273852418538} at Timecode=15
07/01/2021 09:10:52 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 15, "results": {"EM": 0.152, "QA-F1": 0.3109273852418538}}}
07/01/2021 09:10:52 - INFO - __main__ - Starting the offline evaluation of 16
07/01/2021 09:10:52 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_016.pt for facebook/bart-base .....
07/01/2021 09:10:52 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_046.pt for facebook/bart-base ..... Done!
07/01/2021 09:10:53 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:10:53 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:10:53 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:10:53 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:10:54 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_009.pt for facebook/bart-base ..... Done!
07/01/2021 09:10:54 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:10:54 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:10:54 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:10:55 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:10:58 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_034.pt for facebook/bart-base ..... Done!
07/01/2021 09:10:58 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:10:58 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_028.pt for facebook/bart-base ..... Done!
07/01/2021 09:10:58 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:11:01 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_002.pt for facebook/bart-base ..... Done!
07/01/2021 09:11:01 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:11:02 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_016.pt for facebook/bart-base ..... Done!
07/01/2021 09:11:02 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:12:12 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.376, 'QA-F1': 0.5220242350831275} at Timecode=40
07/01/2021 09:12:12 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 40, "results": {"EM": 0.376, "QA-F1": 0.5220242350831275}}}
07/01/2021 09:12:12 - INFO - __main__ - Starting the offline evaluation of 41
07/01/2021 09:12:12 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_041.pt for facebook/bart-base .....
07/01/2021 09:12:14 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:12:14 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:12:15 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:12:17 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.427, 'QA-F1': 0.5738615466514181} at Timecode=46
07/01/2021 09:12:17 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 46, "results": {"EM": 0.427, "QA-F1": 0.5738615466514181}}}
07/01/2021 09:12:17 - INFO - __main__ - Starting the offline evaluation of 47
07/01/2021 09:12:17 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_047.pt for facebook/bart-base .....
07/01/2021 09:12:20 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:12:20 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:12:20 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:12:21 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.249, 'QA-F1': 0.363002385803717} at Timecode=22
07/01/2021 09:12:21 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 22, "results": {"EM": 0.249, "QA-F1": 0.363002385803717}}}
07/01/2021 09:12:21 - INFO - __main__ - Starting the offline evaluation of 23
07/01/2021 09:12:21 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_023.pt for facebook/bart-base .....
07/01/2021 09:12:21 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_041.pt for facebook/bart-base ..... Done!
07/01/2021 09:12:22 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:12:23 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:12:23 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:12:23 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:12:28 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.177, 'QA-F1': 0.3235562508621177} at Timecode=16
07/01/2021 09:12:28 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 16, "results": {"EM": 0.177, "QA-F1": 0.3235562508621177}}}
07/01/2021 09:12:28 - INFO - __main__ - Starting the offline evaluation of 17
07/01/2021 09:12:28 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_017.pt for facebook/bart-base .....
07/01/2021 09:12:28 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_047.pt for facebook/bart-base ..... Done!
07/01/2021 09:12:28 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:12:31 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:12:31 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:12:31 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:12:31 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_023.pt for facebook/bart-base ..... Done!
07/01/2021 09:12:31 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:12:37 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.36, 'QA-F1': 0.48759296286064} at Timecode=34
07/01/2021 09:12:37 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 34, "results": {"EM": 0.36, "QA-F1": 0.48759296286064}}}
07/01/2021 09:12:37 - INFO - __main__ - Starting the offline evaluation of 35
07/01/2021 09:12:37 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_035.pt for facebook/bart-base .....
07/01/2021 09:12:39 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_017.pt for facebook/bart-base ..... Done!
07/01/2021 09:12:39 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:12:40 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.133, 'QA-F1': 0.2762208756513926} at Timecode=9
07/01/2021 09:12:40 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 9, "results": {"EM": 0.133, "QA-F1": 0.2762208756513926}}}
07/01/2021 09:12:40 - INFO - __main__ - Starting the offline evaluation of 10
07/01/2021 09:12:40 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_010.pt for facebook/bart-base .....
07/01/2021 09:12:40 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:12:40 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:12:40 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:12:41 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.271, 'QA-F1': 0.41621733250047943} at Timecode=28
07/01/2021 09:12:41 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 28, "results": {"EM": 0.271, "QA-F1": 0.41621733250047943}}}
07/01/2021 09:12:41 - INFO - __main__ - Starting the offline evaluation of 29
07/01/2021 09:12:41 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_029.pt for facebook/bart-base .....
07/01/2021 09:12:42 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.057, 'QA-F1': 0.20778815774636128} at Timecode=2
07/01/2021 09:12:42 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 2, "results": {"EM": 0.057, "QA-F1": 0.20778815774636128}}}
07/01/2021 09:12:42 - INFO - __main__ - Starting the offline evaluation of 3
07/01/2021 09:12:42 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_003.pt for facebook/bart-base .....
07/01/2021 09:12:43 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:12:43 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:12:43 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:12:43 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:12:43 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:12:44 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:12:45 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:12:45 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:12:45 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:12:47 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_035.pt for facebook/bart-base ..... Done!
07/01/2021 09:12:48 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:12:51 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_010.pt for facebook/bart-base ..... Done!
07/01/2021 09:12:51 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:12:52 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_029.pt for facebook/bart-base ..... Done!
07/01/2021 09:12:52 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:12:53 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_003.pt for facebook/bart-base ..... Done!
07/01/2021 09:12:53 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:14:01 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.396, 'QA-F1': 0.5356794737199561} at Timecode=41
07/01/2021 09:14:01 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 41, "results": {"EM": 0.396, "QA-F1": 0.5356794737199561}}}
07/01/2021 09:14:01 - INFO - __main__ - Starting the offline evaluation of 42
07/01/2021 09:14:01 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_042.pt for facebook/bart-base .....
07/01/2021 09:14:02 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.216, 'QA-F1': 0.35474651500020415} at Timecode=23
07/01/2021 09:14:02 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 23, "results": {"EM": 0.216, "QA-F1": 0.35474651500020415}}}
07/01/2021 09:14:02 - INFO - __main__ - Starting the offline evaluation of 24
07/01/2021 09:14:02 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_024.pt for facebook/bart-base .....
07/01/2021 09:14:04 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:14:04 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:14:04 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:14:05 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:14:05 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:14:05 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:14:07 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.171, 'QA-F1': 0.32613875745707616} at Timecode=17
07/01/2021 09:14:07 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 17, "results": {"EM": 0.171, "QA-F1": 0.32613875745707616}}}
07/01/2021 09:14:07 - INFO - __main__ - Starting the offline evaluation of 18
07/01/2021 09:14:07 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_018.pt for facebook/bart-base .....
07/01/2021 09:14:09 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.422, 'QA-F1': 0.5736511590576684} at Timecode=47
07/01/2021 09:14:09 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 47, "results": {"EM": 0.422, "QA-F1": 0.5736511590576684}}}
07/01/2021 09:14:09 - INFO - __main__ - Starting the offline evaluation of 48
07/01/2021 09:14:09 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_048.pt for facebook/bart-base .....
07/01/2021 09:14:10 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:14:10 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:14:10 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:14:11 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_042.pt for facebook/bart-base ..... Done!
07/01/2021 09:14:11 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:14:12 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:14:12 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:14:12 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:14:13 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_024.pt for facebook/bart-base ..... Done!
07/01/2021 09:14:13 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:14:18 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_018.pt for facebook/bart-base ..... Done!
07/01/2021 09:14:18 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:14:20 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_048.pt for facebook/bart-base ..... Done!
07/01/2021 09:14:20 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:14:30 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.33, 'QA-F1': 0.47247254270904754} at Timecode=35
07/01/2021 09:14:30 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 35, "results": {"EM": 0.33, "QA-F1": 0.47247254270904754}}}
07/01/2021 09:14:30 - INFO - __main__ - Starting the offline evaluation of 36
07/01/2021 09:14:30 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_036.pt for facebook/bart-base .....
07/01/2021 09:14:32 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.295, 'QA-F1': 0.4345807855955725} at Timecode=29
07/01/2021 09:14:32 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 29, "results": {"EM": 0.295, "QA-F1": 0.4345807855955725}}}
07/01/2021 09:14:32 - INFO - __main__ - Starting the offline evaluation of 30
07/01/2021 09:14:32 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_030.pt for facebook/bart-base .....
07/01/2021 09:14:33 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:14:33 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:14:33 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:14:33 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.092, 'QA-F1': 0.22751278738221423} at Timecode=3
07/01/2021 09:14:33 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 3, "results": {"EM": 0.092, "QA-F1": 0.22751278738221423}}}
07/01/2021 09:14:33 - INFO - __main__ - Starting the offline evaluation of 4
07/01/2021 09:14:33 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_004.pt for facebook/bart-base .....
07/01/2021 09:14:34 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:14:34 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:14:35 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:14:36 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:14:36 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:14:36 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:14:40 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_036.pt for facebook/bart-base ..... Done!
07/01/2021 09:14:40 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:14:41 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.137, 'QA-F1': 0.2833270378091903} at Timecode=10
07/01/2021 09:14:41 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 10, "results": {"EM": 0.137, "QA-F1": 0.2833270378091903}}}
07/01/2021 09:14:41 - INFO - __main__ - Starting the offline evaluation of 11
07/01/2021 09:14:41 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_011.pt for facebook/bart-base .....
07/01/2021 09:14:43 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_030.pt for facebook/bart-base ..... Done!
07/01/2021 09:14:43 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:14:44 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_004.pt for facebook/bart-base ..... Done!
07/01/2021 09:14:44 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:14:44 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:14:44 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:14:44 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:14:52 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_011.pt for facebook/bart-base ..... Done!
07/01/2021 09:14:52 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:15:51 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.409, 'QA-F1': 0.5525479349642373} at Timecode=42
07/01/2021 09:15:51 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 42, "results": {"EM": 0.409, "QA-F1": 0.5525479349642373}}}
07/01/2021 09:15:51 - INFO - __main__ - Starting the offline evaluation of 43
07/01/2021 09:15:51 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_043.pt for facebook/bart-base .....
07/01/2021 09:15:52 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.42, 'QA-F1': 0.5787830692390254} at Timecode=48
07/01/2021 09:15:52 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 48, "results": {"EM": 0.42, "QA-F1": 0.5787830692390254}}}
07/01/2021 09:15:52 - INFO - __main__ - Starting the offline evaluation of 49
07/01/2021 09:15:52 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_049.pt for facebook/bart-base .....
07/01/2021 09:15:54 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:15:54 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:15:54 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:15:54 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:15:54 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:15:54 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:15:59 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.195, 'QA-F1': 0.35205553507456533} at Timecode=24
07/01/2021 09:15:59 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 24, "results": {"EM": 0.195, "QA-F1": 0.35205553507456533}}}
07/01/2021 09:15:59 - INFO - __main__ - Starting the offline evaluation of 25
07/01/2021 09:15:59 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_025.pt for facebook/bart-base .....
07/01/2021 09:15:59 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.158, 'QA-F1': 0.3174899578300378} at Timecode=18
07/01/2021 09:15:59 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 18, "results": {"EM": 0.158, "QA-F1": 0.3174899578300378}}}
07/01/2021 09:15:59 - INFO - __main__ - Starting the offline evaluation of 19
07/01/2021 09:15:59 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_019.pt for facebook/bart-base .....
07/01/2021 09:16:02 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_043.pt for facebook/bart-base ..... Done!
07/01/2021 09:16:02 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:16:02 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_049.pt for facebook/bart-base ..... Done!
07/01/2021 09:16:02 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:16:03 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:16:03 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:16:03 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:16:03 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:16:03 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:16:03 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:16:08 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.32, 'QA-F1': 0.453136962791937} at Timecode=30
07/01/2021 09:16:08 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 30, "results": {"EM": 0.32, "QA-F1": 0.453136962791937}}}
07/01/2021 09:16:08 - INFO - __main__ - Starting the offline evaluation of 31
07/01/2021 09:16:08 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_031.pt for facebook/bart-base .....
07/01/2021 09:16:10 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_019.pt for facebook/bart-base ..... Done!
07/01/2021 09:16:11 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:16:11 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_025.pt for facebook/bart-base ..... Done!
07/01/2021 09:16:11 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:16:11 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:16:11 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:16:11 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:16:19 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_031.pt for facebook/bart-base ..... Done!
07/01/2021 09:16:19 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:16:21 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.334, 'QA-F1': 0.48474051134796065} at Timecode=36
07/01/2021 09:16:21 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 36, "results": {"EM": 0.334, "QA-F1": 0.48474051134796065}}}
07/01/2021 09:16:21 - INFO - __main__ - Starting the offline evaluation of 37
07/01/2021 09:16:21 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_037.pt for facebook/bart-base .....
07/01/2021 09:16:23 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.089, 'QA-F1': 0.23328996135278152} at Timecode=4
07/01/2021 09:16:23 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 4, "results": {"EM": 0.089, "QA-F1": 0.23328996135278152}}}
07/01/2021 09:16:23 - INFO - __main__ - Starting the offline evaluation of 5
07/01/2021 09:16:23 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_005.pt for facebook/bart-base .....
07/01/2021 09:16:23 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:16:23 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:16:23 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:16:26 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:16:26 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:16:26 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:16:31 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_037.pt for facebook/bart-base ..... Done!
07/01/2021 09:16:31 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:16:34 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_005.pt for facebook/bart-base ..... Done!
07/01/2021 09:16:34 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:16:40 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.141, 'QA-F1': 0.290099660156078} at Timecode=11
07/01/2021 09:16:40 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 11, "results": {"EM": 0.141, "QA-F1": 0.290099660156078}}}
07/01/2021 09:16:40 - INFO - __main__ - Starting the offline evaluation of 12
07/01/2021 09:16:40 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_012.pt for facebook/bart-base .....
07/01/2021 09:16:43 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:16:43 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:16:43 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:16:51 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_012.pt for facebook/bart-base ..... Done!
07/01/2021 09:16:51 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:17:35 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.404, 'QA-F1': 0.5498337809421714} at Timecode=43
07/01/2021 09:17:35 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 43, "results": {"EM": 0.404, "QA-F1": 0.5498337809421714}}}
07/01/2021 09:17:35 - INFO - __main__ - Starting the offline evaluation of 44
07/01/2021 09:17:35 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_044.pt for facebook/bart-base .....
07/01/2021 09:17:38 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:17:38 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:17:38 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:17:42 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.416, 'QA-F1': 0.5768615102257829} at Timecode=49
07/01/2021 09:17:42 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 49, "results": {"EM": 0.416, "QA-F1": 0.5768615102257829}}}
07/01/2021 09:17:42 - INFO - __main__ - Starting the offline evaluation of 50
07/01/2021 09:17:42 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_050.pt for facebook/bart-base .....
07/01/2021 09:17:44 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.161, 'QA-F1': 0.32129959160727123} at Timecode=19
07/01/2021 09:17:44 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 19, "results": {"EM": 0.161, "QA-F1": 0.32129959160727123}}}
07/01/2021 09:17:44 - INFO - __main__ - Starting the offline evaluation of 20
07/01/2021 09:17:44 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_020.pt for facebook/bart-base .....
07/01/2021 09:17:45 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:17:45 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:17:45 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:17:46 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_044.pt for facebook/bart-base ..... Done!
07/01/2021 09:17:46 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:17:47 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:17:47 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:17:47 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:17:53 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_050.pt for facebook/bart-base ..... Done!
07/01/2021 09:17:53 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:17:54 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_020.pt for facebook/bart-base ..... Done!
07/01/2021 09:17:55 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:17:55 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.205, 'QA-F1': 0.3617561137959879} at Timecode=25
07/01/2021 09:17:55 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 25, "results": {"EM": 0.205, "QA-F1": 0.3617561137959879}}}
07/01/2021 09:17:55 - INFO - __main__ - Starting the offline evaluation of 26
07/01/2021 09:17:55 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_026.pt for facebook/bart-base .....
07/01/2021 09:17:58 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:17:58 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:17:58 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:18:00 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.338, 'QA-F1': 0.4653189153166277} at Timecode=31
07/01/2021 09:18:00 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 31, "results": {"EM": 0.338, "QA-F1": 0.4653189153166277}}}
07/01/2021 09:18:00 - INFO - __main__ - Starting the offline evaluation of 32
07/01/2021 09:18:00 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_032.pt for facebook/bart-base .....
07/01/2021 09:18:03 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:18:03 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:18:03 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:18:05 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_026.pt for facebook/bart-base ..... Done!
07/01/2021 09:18:06 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:18:11 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_032.pt for facebook/bart-base ..... Done!
07/01/2021 09:18:11 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:18:14 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.332, 'QA-F1': 0.4808502599333332} at Timecode=37
07/01/2021 09:18:14 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 37, "results": {"EM": 0.332, "QA-F1": 0.4808502599333332}}}
07/01/2021 09:18:14 - INFO - __main__ - Starting the offline evaluation of 38
07/01/2021 09:18:14 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_038.pt for facebook/bart-base .....
07/01/2021 09:18:16 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.088, 'QA-F1': 0.23864900935646352} at Timecode=5
07/01/2021 09:18:16 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 5, "results": {"EM": 0.088, "QA-F1": 0.23864900935646352}}}
07/01/2021 09:18:16 - INFO - __main__ - Starting the offline evaluation of 6
07/01/2021 09:18:16 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_006.pt for facebook/bart-base .....
07/01/2021 09:18:16 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:18:16 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:18:17 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:18:19 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:18:19 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:18:19 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:18:25 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_038.pt for facebook/bart-base ..... Done!
07/01/2021 09:18:25 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:18:27 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_006.pt for facebook/bart-base ..... Done!
07/01/2021 09:18:27 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:18:38 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.139, 'QA-F1': 0.2979212174823351} at Timecode=12
07/01/2021 09:18:38 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 12, "results": {"EM": 0.139, "QA-F1": 0.2979212174823351}}}
07/01/2021 09:18:38 - INFO - __main__ - Starting the offline evaluation of 13
07/01/2021 09:18:38 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_013.pt for facebook/bart-base .....
07/01/2021 09:18:40 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:18:40 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:18:41 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:18:48 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_013.pt for facebook/bart-base ..... Done!
07/01/2021 09:18:48 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:19:27 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.418, 'QA-F1': 0.5591583005684091} at Timecode=44
07/01/2021 09:19:27 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 44, "results": {"EM": 0.418, "QA-F1": 0.5591583005684091}}}
07/01/2021 09:19:34 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.189, 'QA-F1': 0.33739652334925596} at Timecode=20
07/01/2021 09:19:34 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 20, "results": {"EM": 0.189, "QA-F1": 0.33739652334925596}}}
07/01/2021 09:19:36 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.432, 'QA-F1': 0.5842150722302881} at Timecode=50
07/01/2021 09:19:36 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 50, "results": {"EM": 0.432, "QA-F1": 0.5842150722302881}}}
07/01/2021 09:19:36 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.343, 'QA-F1': 0.4713047959829166} at Timecode=32
07/01/2021 09:19:36 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 32, "results": {"EM": 0.343, "QA-F1": 0.4713047959829166}}}
07/01/2021 09:19:45 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.207, 'QA-F1': 0.36952645676089774} at Timecode=26
07/01/2021 09:19:45 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 26, "results": {"EM": 0.207, "QA-F1": 0.36952645676089774}}}
07/01/2021 09:19:54 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.105, 'QA-F1': 0.24697626756714966} at Timecode=6
07/01/2021 09:19:54 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 6, "results": {"EM": 0.105, "QA-F1": 0.24697626756714966}}}
07/01/2021 09:19:54 - INFO - __main__ - Starting the offline evaluation of 7
07/01/2021 09:19:54 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_007.pt for facebook/bart-base .....
07/01/2021 09:19:56 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:19:56 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:19:56 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:20:00 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_007.pt for facebook/bart-base ..... Done!
07/01/2021 09:20:00 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:20:04 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.319, 'QA-F1': 0.4815438438121441} at Timecode=38
07/01/2021 09:20:04 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 38, "results": {"EM": 0.319, "QA-F1": 0.4815438438121441}}}
07/01/2021 09:20:27 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.139, 'QA-F1': 0.30433679437875916} at Timecode=13
07/01/2021 09:20:27 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 13, "results": {"EM": 0.139, "QA-F1": 0.30433679437875916}}}
07/01/2021 09:20:27 - INFO - __main__ - Starting the offline evaluation of 14
07/01/2021 09:20:27 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_014.pt for facebook/bart-base .....
07/01/2021 09:20:30 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
07/01/2021 09:20:30 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

07/01/2021 09:20:30 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
07/01/2021 09:20:34 - INFO - __main__ - Loading checkpoint from bug_data/output/nq_dev_0701v3_1e-5_e3_ckpts/model_ckpt_014.pt for facebook/bart-base ..... Done!
07/01/2021 09:20:34 - INFO - __main__ - Moving to the GPUs.
07/01/2021 09:21:23 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.122, 'QA-F1': 0.26094704484728815} at Timecode=7
07/01/2021 09:21:23 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 7, "results": {"EM": 0.122, "QA-F1": 0.26094704484728815}}}
07/01/2021 09:22:00 - INFO - __main__ - Current Overall Bug-fixing Results = {'EM': 0.144, 'QA-F1': 0.3065520389998002} at Timecode=14
07/01/2021 09:22:00 - INFO - __main__ - Results: {"overtime_all_bug_eval": {"timecode": 14, "results": {"EM": 0.144, "QA-F1": 0.3065520389998002}}}
