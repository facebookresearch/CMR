11/09/2021 19:01:46 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:01:46 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:01:46 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:01:46 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:01:46 - INFO - __main__ - dataset_size=6400, num_shards=8, local_shard_id=0
11/09/2021 19:01:46 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:01:46 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:01:46 - INFO - __main__ - dataset_size=6400, num_shards=8, local_shard_id=7
11/09/2021 19:01:46 - INFO - __main__ - dataset_size=6400, num_shards=8, local_shard_id=4
11/09/2021 19:01:46 - INFO - __main__ - dataset_size=6400, num_shards=8, local_shard_id=1
11/09/2021 19:01:46 - INFO - __main__ - dataset_size=6400, num_shards=8, local_shard_id=6
11/09/2021 19:01:46 - INFO - __main__ - dataset_size=6400, num_shards=8, local_shard_id=3
11/09/2021 19:01:46 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:01:46 - INFO - __main__ - dataset_size=6400, num_shards=8, local_shard_id=2
11/09/2021 19:01:46 - INFO - __main__ - Config args: Namespace(append_another_bos=True, checkpoint='', dataset='mrqa', debug=False, dev_file='', do_lowercase=False, do_predict=True, do_train=False, max_input_length=888, max_output_length=50, model='facebook/bart-base', num_beams=3, output_dir='out/mrqa_squad_bart-base_1029_upstream_model/', predict_batch_size=64, predict_checkpoint='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', prefix='', quiet=False, seed=42, train_file='')
11/09/2021 19:01:46 - INFO - __main__ - dataset_size=6400, num_shards=8, local_shard_id=5
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:01:47 - INFO - __main__ - Start tokenizing ... 800 instances
11/09/2021 19:01:47 - INFO - __main__ - Printing 3 examples
11/09/2021 19:01:47 - INFO - __main__ - Premise: Ernest Jones is a British jeweller and watchmaker. Established in 1949, its first store was opened in Oxford Street, London. Ernest Jones specialises in diamonds and watches, stocking brands such as Gucci and Emporio Armani. Ernest Jones is part of the Signet Jewelers group. </s> Hypothesis: The first Ernest Jones store was opened on the continent of Europe.
11/09/2021 19:01:47 - INFO - __main__ - ['entailment']
11/09/2021 19:01:47 - INFO - __main__ - Premise: Old Trafford is a football stadium in Old Trafford, Greater Manchester, England, and the home of Manchester United. With a capacity of 75,643, it is the largest club football stadium in the United Kingdom, the second-largest football stadium, and the eleventh-largest in Europe. It is about 0.5 mi from Old Trafford Cricket Ground and the adjacent tram stop. </s> Hypothesis: There are only 10 larger football stadiums in Europe.
11/09/2021 19:01:47 - INFO - __main__ - ['entailment']
11/09/2021 19:01:47 - INFO - __main__ - Premise: Magnus is a Belgian joint dance project of Tom Barman (from the rock band dEUS) and CJ Bolland. Magnus' debut album, "The Body Gave You Everything", was released on March 29, 2004. Two of its tracks, "Summer's Here" and "Jumpneedle", were released as singles. </s> Hypothesis: "The body gave you everything" album was not released on March 28, 2003 but on March 29, 2004.
11/09/2021 19:01:47 - INFO - __main__ - ['entailment']
11/09/2021 19:01:47 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:01:47 - INFO - __main__ - Start tokenizing ... 800 instances
11/09/2021 19:01:47 - INFO - __main__ - Printing 3 examples
11/09/2021 19:01:47 - INFO - __main__ - Premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". </s> Hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
11/09/2021 19:01:47 - INFO - __main__ - ['contradiction']
11/09/2021 19:01:47 - INFO - __main__ - Premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. </s> Hypothesis: Franco Zeffirelli had a political career
11/09/2021 19:01:47 - INFO - __main__ - ['entailment']
11/09/2021 19:01:47 - INFO - __main__ - Premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. </s> Hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
11/09/2021 19:01:47 - INFO - __main__ - ['contradiction']
11/09/2021 19:01:47 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /private/home/yuchenlin/.cache/torch/transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
11/09/2021 19:01:47 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /private/home/yuchenlin/.cache/torch/transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
11/09/2021 19:01:47 - INFO - __main__ - Start tokenizing ... 800 instances
11/09/2021 19:01:47 - INFO - __main__ - Printing 3 examples
11/09/2021 19:01:47 - INFO - __main__ - Premise: How to remove mold odors from inside automobiles<br>Inspect the interior of the car. Check everywhere, even in places that are hidden from you view like under the floor mats and seats. Look for any traces of moisture or mildew. </s> Hypothesis: Check the back seats.
11/09/2021 19:01:47 - INFO - __main__ - ['entailment']
11/09/2021 19:01:47 - INFO - __main__ - Premise: How to use maca powder<br>Use maca as medicine. As a medicine, both maca root and powder has traditionally been used to treat anemia, chronic fatigue, and to boost energy. It also enhance both physical and sexual performance as well as male and female libido by balancing hormones. </s> Hypothesis: If you are losing stamina in the middle of the afternoon, you could use maca powder to uplift you.
11/09/2021 19:01:47 - INFO - __main__ - ['entailment']
11/09/2021 19:01:47 - INFO - __main__ - Premise: How to hoard food for an emergency<br>Make sure you have at least £ 300/$490/342 euro's (these are equivalent as of the seventh of may , 2011). Go to a shop that sells canned foods and drinks. Tinned and dried foods are best, as they last for a long time. </s> Hypothesis: Tinned and dried foods cost money
11/09/2021 19:01:47 - INFO - __main__ - ['entailment']
11/09/2021 19:01:47 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:01:47 - INFO - __main__ - Start tokenizing ... 800 instances
11/09/2021 19:01:47 - INFO - __main__ - Printing 3 examples
11/09/2021 19:01:47 - INFO - __main__ - Premise: The Magic Roundabout (released in France as Pollux - Le manège enchanté and redubbed in the United States as Doogal or The Lord of the Springs) is a 2005 French-British computer-animated adventure fantasy film based on the television series "The Magic Roundabout". </s> Hypothesis: They changed the name of The Magic Roundabout in America because people there do not know what a roundabout is.
11/09/2021 19:01:47 - INFO - __main__ - ['neutral']
11/09/2021 19:01:47 - INFO - __main__ - Premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". </s> Hypothesis: Zane was a thespian. 
11/09/2021 19:01:47 - INFO - __main__ - ['entailment']
11/09/2021 19:01:47 - INFO - __main__ - Premise: Gunby is a hamlet in the South Kesteven district of Lincolnshire, England. It is situated close to the borders with Leicestershire and Rutland, and 9 mi south from Grantham, and 2 mi west from the A1 road. It is in the civil parish of Gunby and Stainby. </s> Hypothesis: The A1 road runs 100 miles west-east.
11/09/2021 19:01:47 - INFO - __main__ - ['neutral']
11/09/2021 19:01:47 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:01:47 - INFO - __main__ - Start tokenizing ... 800 instances
11/09/2021 19:01:47 - INFO - __main__ - Printing 3 examples
11/09/2021 19:01:47 - INFO - __main__ - Premise: How to make the most of your time when studying<br>Develop a schedule that will allow you adequate time to study. Having a schedule makes you see that you have certain things to do and will give you an allocated time that you should be focusing only on your studying. To make the most of time when studying, select the times of day your brain is at its peak performance. </s> Hypothesis: Your brain performs worse during certain times of day.
11/09/2021 19:01:47 - INFO - __main__ - ['entailment']
11/09/2021 19:01:47 - INFO - __main__ - Premise: How to make candy corn<br>Combine the powdered sugar, salt, and powdered milk. Add the three ingredients to medium to large bowl and stir or whisk them together until evenly distributed. Set it to the side as you prepare the syrup. </s> Hypothesis: candy corn has a n
11/09/2021 19:01:47 - INFO - __main__ - ['entailment']
11/09/2021 19:01:47 - INFO - __main__ - Premise: How to receive happiness<br>Eat a healthier diet. Studies show that people who eat a " normal " american diet of refined and junk food suffer from more depression, anxiety, mood swings, and hyperactivity. Try to eat healthier if you want to boost your mood and live a happier life. </s> Hypothesis: Junk food causes more depression
11/09/2021 19:01:47 - INFO - __main__ - ['entailment']
11/09/2021 19:01:47 - INFO - __main__ - Start tokenizing ... 800 instances
11/09/2021 19:01:47 - INFO - __main__ - Printing 3 examples
11/09/2021 19:01:47 - INFO - __main__ - Premise: The 2017–18 Puebla season is the 70th professional season of Mexico's top-flight football league. The season is split into two tournaments—the Torneo Apertura and the Torneo Clausura—each with identical formats and each contested by the same eighteen teams.The Club will also play Copa MX.Rafael García Torres was named the club head coach on June 5, 2017, taking over for sacked coach José Cardozo. </s> Hypothesis: The 2017–18 Puebla season does not have two tournaments that have different formats.
11/09/2021 19:01:47 - INFO - __main__ - ['entailment']
11/09/2021 19:01:47 - INFO - __main__ - Premise: Genevieve LaCaze (born 4 August 1989) is an Australian athletics competitor who specialises in the 3000 metre steeplechase. She held an athletics scholarship at the University of Florida. She was selected to represent Australia at the 2012 Summer Olympics in London and Athletics at the 2016 Summer Olympics in Rio de Janeiro. LaCaze is of French, Italian and Spanish descent. </s> Hypothesis: Genevieve LaCaze completed her medical degree from the University of Florida while on an athletics scholarship. 
11/09/2021 19:01:47 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:01:47 - INFO - __main__ - ['neutral']
11/09/2021 19:01:47 - INFO - __main__ - Premise: EMP Merchandising also known as EMP Merchandising Handelsgesellschaft mbH, Large Popmerchandising, and Sweden Rock Shop is a German-based music mail order and merchandising store. The company distributes a quarterly catalog to customers. In a 2003 report the Osnabrück Chamber of Commerce considered the company to be the largest mail order business for Heavy Metal and Hard Rock music in Germany. </s> Hypothesis: EMP Merchandising starts with an A.
11/09/2021 19:01:47 - INFO - __main__ - ['contradiction']
11/09/2021 19:01:47 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:01:47 - INFO - __main__ - Start tokenizing ... 800 instances
11/09/2021 19:01:47 - INFO - __main__ - Printing 3 examples
11/09/2021 19:01:47 - INFO - __main__ - Premise: Johns Creek is a city located in Fulton County in the U.S. state of Georgia. According to the 2010 U.S. Census, the population was 76,728. The city is an affluent northeastern suburb of Atlanta. In 2017 Johns Creek ranked third on the "USA TODAY" list of "50 best cities to live in." </s> Hypothesis: Johns Creek has a population of 92,000.
11/09/2021 19:01:47 - INFO - __main__ - ['neutral']
11/09/2021 19:01:47 - INFO - __main__ - Premise: Club Deportivo Cajamadrid was a professional basketball and handball team in Spain. It was founded in 1979 and the basketball team played in Liga ACB from 1983 to 1986. The club was sponsored by Caja Madrid until 1991, when the bank decided to retire its support and continued as a different club called Juventud Alcalá. </s> Hypothesis: caja madrid started sponshorship of Club Deportivo Cajamadrid in 1979.
11/09/2021 19:01:47 - INFO - __main__ - ['entailment']
11/09/2021 19:01:47 - INFO - __main__ - Premise: Dennis Princewell Stehr (born 15 May 1984), better known by his stage name Mr Probz, is a Dutch singer, musician and actor. In 2013, he released the song "Waves", which was remixed in 2014 by Robin Schulz, becoming an international hit. He has released one album and featured in the film Bolletjes Blues. </s> Hypothesis: By the time Mr Probz was 30 years old, he had an international hit.
11/09/2021 19:01:47 - INFO - __main__ - ['entailment']
11/09/2021 19:01:47 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:01:47 - INFO - __main__ - Start tokenizing ... 800 instances
11/09/2021 19:01:47 - INFO - __main__ - Printing 3 examples
11/09/2021 19:01:47 - INFO - __main__ - Premise: Richard A. Ludwin (born May 27, 1948) is an American television executive and former vice president at NBC Television. He is notable as the executive who backed Jerry Seinfeld's series "Seinfeld", which went on to become one of the most popular and successful television sitcoms of all time. He was also the head of NBC's late night programming during the Conan O'Brien and Jay Leno conflict in 2010. </s> Hypothesis:  He was also the head of NBC's late night programming during the Conan O'Brien and Jay Leno conflict in 2010 and was responsible for the conflict.
11/09/2021 19:01:47 - INFO - __main__ - ['neutral']
11/09/2021 19:01:47 - INFO - __main__ - Premise: Laurence Alma-Tadema (born Laurense Tadema, 1865–1940), was an English novelist and poet of the late 19th and early 20th centuries who worked in many genres. Eldest daughter of the Dutch painter Lawrence Alma-Tadema (1836–1912) and his first wife Marie-Pauline Gressin Dumoulin, she was born in Brussels. </s> Hypothesis: Alma-Tadema was born 16 years after 1850.
11/09/2021 19:01:47 - INFO - __main__ - ['contradiction']
11/09/2021 19:01:47 - INFO - __main__ - Premise: Operation Zahnarzt (literally "Dentist") was a plan by the Germans to eliminate the Third Army during World War II. The plan of Operation Zahnarzt was to immediately come after Operation Nordwind. The plan was to initiate a pincer movement to encircle and destroy the 3rd US Army. </s> Hypothesis: Operation Zahnarzt was part of WWI
11/09/2021 19:01:47 - INFO - __main__ - ['contradiction']
11/09/2021 19:01:47 - INFO - __main__ - Tokenizing Input ...
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:01:48 - INFO - __main__ - Loaded 800 examples from dev data
11/09/2021 19:01:48 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:01:48 - INFO - __main__ - Loaded 800 examples from dev data
11/09/2021 19:01:48 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:01:48 - INFO - __main__ - Loaded 800 examples from dev data
11/09/2021 19:01:48 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:01:48 - INFO - __main__ - Loaded 800 examples from dev data
11/09/2021 19:01:48 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:01:48 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:01:48 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:01:48 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:01:49 - INFO - __main__ - Loaded 800 examples from dev data
11/09/2021 19:01:49 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:01:49 - INFO - __main__ - Loaded 800 examples from dev data
11/09/2021 19:01:49 - INFO - __main__ - Loaded 800 examples from dev data
11/09/2021 19:01:49 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:01:49 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:01:49 - INFO - __main__ - Tokenizing Input ... Done!
11/09/2021 19:01:49 - INFO - __main__ - Tokenizing Output ...
11/09/2021 19:01:49 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:01:49 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:01:49 - INFO - __main__ - Tokenizing Output ... Done!
11/09/2021 19:01:49 - INFO - __main__ - Loaded 800 examples from dev data
11/09/2021 19:01:49 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt ....
11/09/2021 19:01:49 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:01:49 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:01:49 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:01:49 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:01:49 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:01:49 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:01:49 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:01:49 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:01:49 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:01:49 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:01:49 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:01:49 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:01:49 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:01:49 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:01:49 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:01:49 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:01:50 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json from cache at /private/home/yuchenlin/.cache/torch/transformers/09f4fcaeaf785dd3b97b085d6e3510c7081f586ec8e75981683c6299c0f81d9d.e8d516ad807436d395effad8c2326786872659b7dd1210827ac67c761198a0eb
11/09/2021 19:01:50 - INFO - transformers.configuration_utils - Model config BartConfig {
  "activation_dropout": 0.1,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartModel",
    "BartForConditionalGeneration",
    "BartForSequenceClassification"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "d_model": 768,
  "decoder_attention_heads": 12,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "early_stopping": true,
  "encoder_attention_heads": 12,
  "encoder_ffn_dim": 3072,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 6,
  "eos_token_id": 2,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_position_embeddings": 1024,
  "model_type": "bart",
  "no_repeat_ngram_size": 3,
  "normalize_before": false,
  "normalize_embedding": true,
  "num_beams": 4,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "scale_embedding": false,
  "static_position_embeddings": false,
  "task_specific_params": {
    "summarization": {
      "length_penalty": 1.0,
      "max_length": 128,
      "min_length": 12,
      "num_beams": 4
    },
    "summarization_cnn": {
      "length_penalty": 2.0,
      "max_length": 142,
      "min_length": 56,
      "num_beams": 4
    },
    "summarization_xsum": {
      "length_penalty": 1.0,
      "max_length": 62,
      "min_length": 11,
      "num_beams": 6
    }
  },
  "vocab_size": 50265
}

11/09/2021 19:01:50 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:01:50 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/facebook/bart-base/pytorch_model.bin from cache at /private/home/yuchenlin/.cache/torch/transformers/566c05fb6983817e8ad7a4fa51e3099fe9caa3b31730f964bc5198d71c677523.0a3d95c18c1e434448941bc25accea7b122882be6526fb67c8e8fb6d5ebc711c
11/09/2021 19:01:54 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:01:55 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:01:56 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:01:57 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:01:57 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:01:57 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:01:57 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:01:57 - INFO - __main__ - Loading checkpoint from out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt .... Done!
11/09/2021 19:02:01 - INFO - __main__ - Starting inference ...
11/09/2021 19:02:01 - INFO - __main__ - Starting inference ...
11/09/2021 19:02:01 - INFO - __main__ - Starting inference ...
11/09/2021 19:02:01 - INFO - __main__ - Starting inference ...
11/09/2021 19:02:02 - INFO - __main__ - Starting inference ...
11/09/2021 19:02:02 - INFO - __main__ - Starting inference ...
11/09/2021 19:02:02 - INFO - __main__ - Starting inference ...
11/09/2021 19:02:02 - INFO - __main__ - Starting inference ...
11/09/2021 19:02:30 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:02:33 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:02:43 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:02:44 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:02:44 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:02:45 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:02:48 - INFO - __main__ - Starting inference ... Done
11/09/2021 19:02:48 - INFO - __main__ - Starting inference ... Done
