{"model_update_steps": 2545, "method_class": "index_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, index_rank_method='most_similar', indexing_args_path='exp_results/supervision_data/1012_dm_simple.train_args.json', indexing_method='bart_index', inference_query_size=1, init_memory_cache_path='exp_results/data_streams/bart_index.init_memory.pkl', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/index_based/ckpt_dir/1019_MixedAllErrors_T=100_index_M=U+I_rs=32_rq=3_rank=most_similar_mir=no(0)_seed=0212_ckpts/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/index_based/ckpt_dir/1019_MixedAllErrors_T=100_index_M=U+I_rs=32_rq=3_rank=most_similar_mir=no(0)_seed=0212_ckpts/', replay_candidate_size=0, replay_frequency=3, replay_size=32, save_all_ckpts=0, skip_instant_eval=True, total_steps=10000, use_mir=False, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, max_input_length=888, max_output_length=50, max_timecode=100, num_beams=4, pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.upstream_eval.jsonl', predict_batch_size=16, replay_stream_json_path='', sampled_upstream_json_path='exp_results/data_streams/mrqa.nq_train.memory.jsonl', task_name='mrqa_naturalquestions', train_batch_size=8, use_sampled_upstream=True)", "online_eval_results": [{"timecode": 0, "before_eval": {"predictions": ["Raymond Briggs' 1978 children's book 'The Snowman", "the acceleration due to gravity decreased as an inverse square law", "a marquetry commode by the \u00e9b\u00e9niste Jean Henri Riesener dated c1780", "a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work", "lymphocyte", "Chinghiz", "Doctor Who and the Daleks in the Seven Keys to Doomsday", "Super Bowl 50 Host Committee", "a satirical television comedy programme on BBC Television in 1962 and 1963", "dynasty", "Br'er Rabbit", "Bodhi Natural Health Products", "Amphitrite Goddess of the Sea", "the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ), the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho", "March 14 ( 3 / 14 in the month / day format ) since 3, 1, and 4 are the first three significant digits of \u03c0", "the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing", "Horace Rumpole", "combustion", "A Sunday Afternoon on the Island of La Grande Jatte", "nobody knows for sure how a do-over in golf came to be called a mulligan", "Captain Meriwether Lewis's 30th birthday", "An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses", "2011", "electric eels", "before the first year begins", "New Jerusalem", "Samantha Spiro", "2013", "Ronnie Biggs", "Steve Carell as Felonious Gru, the former villain turned Anti-Villain League agent, Margo, Edith, and Agnes'adoptive father, and Lucy's husband", "an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "the assassination of US President John F. Kennedy"], "metric_results": {"EM": 0.0, "QA-F1": 0.0812722081839729}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.4799999999999999, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.11764705882352942, 0.15384615384615383]}}, "error_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-10322", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-2376", "mrqa_squad-validation-6677", "mrqa_squad-validation-6303", "mrqa_squad-validation-7821", "mrqa_squad-validation-392", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9688", "mrqa_triviaqa-validation-1551", "mrqa_squad-validation-3478", "mrqa_triviaqa-validation-5937", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-2730", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-4185", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3241", "mrqa_triviaqa-validation-7369", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-validation-7017", "mrqa_squad-validation-7746"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 1, "before_eval": {"predictions": ["the town of Acolman, just north of Mexico City", "Iroquois", "philanthropy", "Catherine Zeta Jones", "Virginia Wade", "Crystal Gayle and Gary Morris", "to the anterolateral corner of the spinal cord", "1966", "radioisotope thermoelectric generator", "product or policy that is open and honest", "The Stock Market crash in New York", "New York Stadium", "norman Tebbit", "continental units", "john Forster", "Comptroller General of the Receipt and Issue of Her Majesty's Exchequer", "Sergio P\u00e9rez", "River Welland", "The Concubine", "1543", "quran", "Vigor", "a policeman who investigates a series of mysterious killings and illnesses", "glowed even when turned off", "Florence Nightingale", "Budweiser", "Charlie", "acmthompson", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object", "the check written by DC Comics to Jerry Siegel and Joe Shuster for the exclusive rights to their then-new character, Superman", "May and June 2010"], "metric_results": {"EM": 0.125, "QA-F1": 0.20247980505333446}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.23529411764705885, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.07407407407407407, 0.1904761904761905, 0.4]}}, "error_ids": ["mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-5144", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-5406", "mrqa_hotpotqa-validation-5899", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-validation-1364", "mrqa_hotpotqa-validation-14", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-3971", "mrqa_squad-validation-2629", "mrqa_triviaqa-validation-2210", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-5802", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-2096", "mrqa_naturalquestions-validation-1864", "mrqa_squad-validation-10410", "mrqa_triviaqa-validation-2367", "mrqa_hotpotqa-validation-3774"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 2, "before_eval": {"predictions": ["MS Kronprins Harald", "sports, among them cricket, rallying, football, rugby union and boxing", "Puritanism", "+, -, *, and / keys", "2009", "in different parts of the globe", "marioneth and Llantisilly Rail Traction Company Limited", "acetate", "John II Casimir Vasa", "marries Lord Darnley", "A55 North Wales Expressway", "phylum with relatively few species", "`` Everywhere '' is a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "sprawl leads to urban decay and a concentration of lower income residents in the inner city", "a student in the second year at a high school or college", "Bothtec", "Terry Reid", "information about climate change based on published sources", "for the first time is an English composer who has something to say", "North America", "Andr\u00e9 3000", "rookies", "Akhenaten", "President Theodore Roosevelt", "from 2010 to 2012", "four", "the Western Bloc ( the United States, its NATO allies and others )", "in the 1970s", "Georges Bizet", "Matt Winer", "1689", "the Pacific across the Amazonas Basin"], "metric_results": {"EM": 0.09375, "QA-F1": 0.30476993252728546}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.8235294117647058, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.0, 0.0, 0.4, 0.28571428571428575, 0.1, 0.3243243243243243, 0.19999999999999998, 1.0, 0.0, 0.2, 0.0, 0.4444444444444445, 0.4, 1.0, 0.0, 0.4, 0.8571428571428571, 1.0, 0.3636363636363636, 0.3333333333333333, 0.6666666666666666, 0.4, 0.0, 0.4]}}, "error_ids": ["mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-8542", "mrqa_squad-validation-7149", "mrqa_naturalquestions-validation-10364", "mrqa_squad-validation-4253", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-validation-1202", "mrqa_hotpotqa-validation-3632", "mrqa_triviaqa-validation-1935", "mrqa_hotpotqa-validation-1888", "mrqa_squad-validation-4456", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-8448", "mrqa_triviaqa-validation-2136", "mrqa_naturalquestions-validation-539", "mrqa_squad-validation-8513", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-validation-5502", "mrqa_hotpotqa-validation-2679", "mrqa_naturalquestions-validation-6896", "mrqa_naturalquestions-validation-2501", "mrqa_hotpotqa-validation-1376", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-683", "mrqa_triviaqa-validation-2722", "mrqa_hotpotqa-validation-4367", "mrqa_squad-validation-3113", "mrqa_squad-validation-4283"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 3, "before_eval": {"predictions": ["id", "is not involved", "c c + angle d = 180 degrees", "between 27 July and 7 August 2022", "New York", "casket", "2006 British Academy Television Award for Best Drama Series", "Least of the Great Powers", "usually restricted to the lower motor neurons, the efferent nerves that directly innervate muscles", "babbage", "boggling array of products that makes it almost impossible to finalise a choice without undertaking an inventory", "baze", "coronary thrombosis", "bollywood", "Overtime", "Sir Henry Cole", "trouble distinguishing between carbon dioxide and oxygen", "cactus", "cement City, Texas", "the Democratic Unionist Party (DUP )", "23 July 1989", "many educational institutions especially within the US", "gurus often exercising a great deal of control over the lives of their disciples", "control purposes", "bamboula", "Callability", "2.26 GHz quad - core Snapdragon 800 processor", "over 10,000 British and 2,000 old master works", "al - khimar", "proteins were more complex than DNA", "gallbladder", "berenice Abbott"], "metric_results": {"EM": 0.09375, "QA-F1": 0.14458411654135336}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.10526315789473685, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-5362", "mrqa_naturalquestions-validation-5647", "mrqa_squad-validation-3058", "mrqa_triviaqa-validation-7382", "mrqa_squad-validation-7816", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-2571", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2327", "mrqa_triviaqa-validation-365", "mrqa_squad-validation-1539", "mrqa_triviaqa-validation-3901", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-3189", "mrqa_squad-validation-8832", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-2150", "mrqa_squad-validation-2191", "mrqa_squad-validation-2069", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-6402", "mrqa_naturalquestions-validation-2385", "mrqa_squad-validation-5517", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-5818", "mrqa_triviaqa-validation-2530"], "retrieved_ids": ["mrqa_naturalquestions-train-66866", "mrqa_naturalquestions-train-37993", "mrqa_naturalquestions-train-47999", "mrqa_naturalquestions-train-83474", "mrqa_triviaqa-validation-2722", "mrqa_naturalquestions-train-64511", "mrqa_naturalquestions-train-80958", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-train-72133", "mrqa_naturalquestions-train-26710", "mrqa_naturalquestions-train-31611", "mrqa_naturalquestions-train-21309", "mrqa_naturalquestions-train-86408", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-train-66570", "mrqa_naturalquestions-train-59943", "mrqa_naturalquestions-train-65943", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-train-72496", "mrqa_naturalquestions-train-60822", "mrqa_naturalquestions-train-25755", "mrqa_naturalquestions-train-24615", "mrqa_naturalquestions-train-76391", "mrqa_naturalquestions-train-74958", "mrqa_naturalquestions-train-43791", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-train-52882", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-train-10736", "mrqa_naturalquestions-train-21281", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-train-14012"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 4, "before_eval": {"predictions": ["United Kingdom", "at Remagen", "December 9, 2016", "NASA discontinued the manned Block I program, using the Block I spacecraft only for unmanned Saturn V flights", "British progressive folk-rock band Gryphon", "16 April 1898", "month", "a shepherd", "at elevation 2 meters above sea level", "is a rare but often fatal disease that affects the central nervous system by causing painful muscular contractions", "bounding the time or space used by the algorithm", "max.", "Lieutenant Commander Steve McGarrett", "Eddie Leonski", "Jack", "a mixture of phencyclidine and cocaine", "bunker", "in the middle decade of the 19th century", "the Reverse - Flash", "All Souls'Day", "1968", "The Azerbaijan capital of Baku", "Catholics", "Mona Vanderwaal", "cricket", "Pyotr Ilyich Tchaikovsky", "2001 or 2010", "the English colonies of North America, and Quebec", "formulate language", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "steam turbine", "Splodgenessabounds"], "metric_results": {"EM": 0.125, "QA-F1": 0.26517857142857143}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.4, 0.33333333333333337, 0.5, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.6666666666666666, 0.4, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-6399", "mrqa_squad-validation-9358", "mrqa_naturalquestions-validation-5146", "mrqa_squad-validation-3971", "mrqa_squad-validation-5360", "mrqa_naturalquestions-validation-1277", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-3647", "mrqa_triviaqa-validation-7253", "mrqa_squad-validation-1688", "mrqa_triviaqa-validation-3808", "mrqa_hotpotqa-validation-1168", "mrqa_hotpotqa-validation-1289", "mrqa_hotpotqa-validation-2944", "mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-5654", "mrqa_squad-validation-3126", "mrqa_naturalquestions-validation-2900", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-3978", "mrqa_squad-validation-3021", "mrqa_naturalquestions-validation-3840", "mrqa_squad-validation-3467"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 5, "before_eval": {"predictions": ["lesterley Young", "F\u00e9d\u00e9ration Sportive F\u00e9minine Internationale (FSFI )", "a soft wool fabric with a colorful swirled pattern of curved shapes", "Paspahegh Indians", "a delay or obstruction along the pathway that electrical impulses travel in your heart to make it beat", "South Dakota", "2 : 44 p.m. EDT", "swanee or swannee whistle", "rapeseed plant", "to start fires, hunt, and bury their dead", "India", "Parietal cells ( also known as oxyntic or delomorphous cells )", "placental", "September 13, 1994", "june", "imperial rule", "1840", "make a defiant speech, or a speech explaining their actions", "Francis Marion Crawford, Robert Underwood Johnson, Stanford White, Fritz Lowenstein, George Scherff, and Kenneth Swezey", "kinks", "a greater tendency to take on debts", "entropy", "my mind is averse to wedlock because I daily expect the death of a heretic", "8.7 -- 9.2", "China", "2 November 1902", "present - day southeastern Texas", "May 7, 2018", "9 October 1940", "Selden", "structural collapses", "as a way of housing a fierce half-man, half-bull creature known as the Minotaur"], "metric_results": {"EM": 0.09375, "QA-F1": 0.1833444741532977}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5454545454545454, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.47058823529411764, 0.0, 1.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4725", "mrqa_hotpotqa-validation-2197", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-2020", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3625", "mrqa_naturalquestions-validation-4513", "mrqa_triviaqa-validation-1550", "mrqa_triviaqa-validation-5704", "mrqa_hotpotqa-validation-4621", "mrqa_squad-validation-6733", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-254", "mrqa_squad-validation-7554", "mrqa_squad-validation-10423", "mrqa_squad-validation-2757", "mrqa_naturalquestions-validation-2884", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-1489", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-5848", "mrqa_squad-validation-2531", "mrqa_triviaqa-validation-4560"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 6, "before_eval": {"predictions": ["zinnemann", "to prevent the flame from being blown out", "2005 to 2008", "1998", "city", "Menorca", "90-60's", "an aided or an unaided school", "dolph Camilli", "times sign", "BAFTA Television Award", "Juice Newton", "1960", "HTTP Secure ( HTTPS )", "late summer", "kansas", "monatomic", "Palm Springs", "june", "dads", "tanflies", "universal", "widow - maker infarction", "1.1 \u00d7 1011 metric tonnes", "dale", "leaf", "Indian club ATK", "land", "near Grande Comore, Comoros Islands", "rupees", "Norwegian", "burning of fossil fuels"], "metric_results": {"EM": 0.0625, "QA-F1": 0.12038690476190475}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-4165", "mrqa_hotpotqa-validation-5810", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-6887", "mrqa_squad-validation-2659", "mrqa_squad-validation-6947", "mrqa_triviaqa-validation-105", "mrqa_naturalquestions-validation-10356", "mrqa_squad-validation-7819", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-226", "mrqa_triviaqa-validation-5795", "mrqa_squad-validation-3463", "mrqa_squad-validation-2584", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-227", "mrqa_naturalquestions-validation-5582", "mrqa_squad-validation-4181", "mrqa_triviaqa-validation-4966", "mrqa_squad-validation-8821", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-10042", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6207", "mrqa_hotpotqa-validation-3919", "mrqa_naturalquestions-validation-6644"], "retrieved_ids": ["mrqa_naturalquestions-train-35002", "mrqa_triviaqa-validation-3901", "mrqa_naturalquestions-train-25341", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-train-60689", "mrqa_naturalquestions-train-38547", "mrqa_naturalquestions-train-5169", "mrqa_naturalquestions-train-25085", "mrqa_naturalquestions-train-28761", "mrqa_naturalquestions-train-22793", "mrqa_naturalquestions-train-35611", "mrqa_naturalquestions-train-46650", "mrqa_naturalquestions-train-73356", "mrqa_naturalquestions-train-21529", "mrqa_naturalquestions-train-21309", "mrqa_naturalquestions-train-77279", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-train-33527", "mrqa_naturalquestions-train-45778", "mrqa_naturalquestions-train-63031", "mrqa_naturalquestions-train-1290", "mrqa_naturalquestions-train-49927", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-26110", "mrqa_naturalquestions-train-10513", "mrqa_naturalquestions-train-14632", "mrqa_naturalquestions-train-8824", "mrqa_naturalquestions-train-56348", "mrqa_naturalquestions-train-16027", "mrqa_naturalquestions-train-83658", "mrqa_naturalquestions-train-72123"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 7, "before_eval": {"predictions": ["the wisdom and prudence of certain decisions of procurement", "The Q'eqchi '", "a few", "The U.S. Army Chaplain insignia", "Kairi", "the suburbs, who wanted more services and more control over the central city", "film", "near the Black Sea", "the last book accepted into the Christian biblical canon", "Beyonc\u00e9", "% IACS", "gallantry", "the most popular show", "1950s", "haulage", "1998", "a priest", "23.1", "18 - season career", "family member", "long-term environmental changes", "William Powell Lear", "the tangential force", "Terrell Suggs", "decide on all the motions and amendments that have been moved", "a voyage of adventure", "Abraham Gottlob Werner", "braves", "present-day Charleston", "quiescent", "Adam Karpel, Alex Baskin, Douglas Ross, Gregory Stewart, Scott Dunlop, Stephanie Boyriven and Andy Cohen", "panzer"], "metric_results": {"EM": 0.3125, "QA-F1": 0.3665543300653595}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.4444444444444445, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.23529411764705882, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5522", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-983", "mrqa_squad-validation-7296", "mrqa_triviaqa-validation-7415", "mrqa_naturalquestions-validation-859", "mrqa_squad-validation-108", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-1293", "mrqa_hotpotqa-validation-3898", "mrqa_naturalquestions-validation-98", "mrqa_squad-validation-358", "mrqa_squad-validation-3558", "mrqa_naturalquestions-validation-824", "mrqa_hotpotqa-validation-1142", "mrqa_naturalquestions-validation-3474", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-291", "mrqa_triviaqa-validation-781", "mrqa_squad-validation-3181", "mrqa_hotpotqa-validation-2902"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 8, "before_eval": {"predictions": ["computational complexity theory", "george Antonio 'Nino' Farina", "37.7", "about 5 nanometers across, arranged in rows 6.4 nanometers apart", "the eighth and eleventh episodes of the season", "Carl Michael Edwards II", "over 400 games", "the adrenal glands", "liberal arts", "Forest of Bowland in Lancashire", "Edward V, King of England and Richard of Shrewsbury, Duke of York", "St. Louis County, Missouri, United States, 17 mi southwest of St. Lawrence and 2 mi east of Eureka", "1868", "2018", "Steve Thompson", "law firm", "Pottawatomie County", "orangutan", "theory of general relativity (GR )", "The church tower", "walford", "Toronto", "wales", "110 miles (177 km ) from the East River in New York City, along the North Shore of Long Island, to the south", "Kona coast of the island of Hawai\u02bb i about 12 mi south of Kailua-Kona", "Liberal conservatism", "largest gold rushes the world has ever seen", "six degrees of freedom", "not guilty", "psychoanalysis", "Quentin Coldwater", "acidic bogs"], "metric_results": {"EM": 0.125, "QA-F1": 0.2634953268588137}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.4, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.5, 0.0, 0.0, 0.25, 0.375, 0.10526315789473684, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1904761904761905, 0.2666666666666667, 0.5, 0.4444444444444445, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "error_ids": ["mrqa_squad-validation-1705", "mrqa_triviaqa-validation-1521", "mrqa_naturalquestions-validation-6089", "mrqa_squad-validation-8869", "mrqa_naturalquestions-validation-856", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-3789", "mrqa_triviaqa-validation-7506", "mrqa_naturalquestions-validation-1360", "mrqa_hotpotqa-validation-3161", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5401", "mrqa_triviaqa-validation-7157", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-validation-3309", "mrqa_hotpotqa-validation-582", "mrqa_triviaqa-validation-4268", "mrqa_hotpotqa-validation-1021", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-4812", "mrqa_squad-validation-2987", "mrqa_naturalquestions-validation-7906", "mrqa_squad-validation-6915", "mrqa_triviaqa-validation-7767", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-187"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 9, "before_eval": {"predictions": ["I Seek You", "Argentinian", "a report, published in early February 2007 by the Ear Institute at the University College London, and Widex, a Danish hearing aid manufacturer", "but not actually find the right pan", "photosynthesis", "a wide range of society figures of the period", "george w. Bush", "the alt-right movement", "triplet", "water", "president", "all officeholders annually", "George, Margrave of Brandenburg-Ansbach", "a corruption of the Kamba version", "3D computer-animated comedy film", "Brown Square Station", "acting", "C. W. Grafton", "LED illuminated display", "Americans acting under orders", "iPod Classic", "My Sassy Girl", "prevent damage to the body", "Edge of Night", "non-combustible substances that corrode, such as iron", "pedagogy", "\u039c\u03ad\u03b3\u03b1\u03bd \u0399\u03b5\u03c1\u03cc\u03bd \u03a3\u03c5\u03bd\u03ad\u03ba\u03b4\u03b7\u03bc\u03bf\u03bd", "the root respiration", "organic carbon in all land - living organisms, both alive and dead, as well as carbon stored in soils", "heart disease", "medium and heavy- Duty diesel trucks", "testes"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2860813665501165}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.1, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.13333333333333333, 0.4, 0.0, 1.0, 0.18181818181818182, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 0.2857142857142857, 1.0, 0.0, 0.3333333333333333, 0.15384615384615383, 1.0, 0.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.0, 0.7272727272727272, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6844", "mrqa_squad-validation-5210", "mrqa_triviaqa-validation-3387", "mrqa_squad-validation-8924", "mrqa_squad-validation-5618", "mrqa_triviaqa-validation-2703", "mrqa_hotpotqa-validation-3428", "mrqa_squad-validation-3442", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-4520", "mrqa_squad-validation-8259", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-5128", "mrqa_naturalquestions-validation-754", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1436", "mrqa_naturalquestions-validation-1294", "mrqa_squad-validation-3490", "mrqa_squad-validation-1879", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-5823", "mrqa_naturalquestions-validation-3677"], "retrieved_ids": ["mrqa_naturalquestions-train-34489", "mrqa_naturalquestions-train-19660", "mrqa_naturalquestions-train-59801", "mrqa_squad-validation-4185", "mrqa_squad-validation-3181", "mrqa_naturalquestions-train-28437", "mrqa_naturalquestions-train-30204", "mrqa_naturalquestions-train-9081", "mrqa_naturalquestions-train-63004", "mrqa_naturalquestions-train-9712", "mrqa_naturalquestions-train-63031", "mrqa_naturalquestions-train-59591", "mrqa_naturalquestions-train-37019", "mrqa_naturalquestions-train-54176", "mrqa_naturalquestions-train-83473", "mrqa_hotpotqa-validation-1968", "mrqa_naturalquestions-train-68587", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-train-47739", "mrqa_naturalquestions-train-57692", "mrqa_hotpotqa-validation-5899", "mrqa_naturalquestions-train-13385", "mrqa_naturalquestions-train-45457", "mrqa_naturalquestions-train-45549", "mrqa_naturalquestions-train-30974", "mrqa_naturalquestions-train-79244", "mrqa_naturalquestions-train-67169", "mrqa_naturalquestions-train-7959", "mrqa_naturalquestions-train-24991", "mrqa_naturalquestions-train-215", "mrqa_naturalquestions-train-40945", "mrqa_naturalquestions-train-20644"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 10, "before_eval": {"predictions": ["1744", "yellow fever", "a particular geographical area", "Las Vegas", "a status line which includes the status code and reason message ( e.g., HTTP / 1.1 200 OK, which indicates that the client's request succeeded )", "globetrotters", "cruiserweight", "a bridge over the Merderet in the fictional town of Ramelle", "1987", "black Miserables", "vannes", "Victoria, Duchess of Kent", "7 ( 25 \u00b0 C )", "a log raft using ancient South American practices and sailed the Pacific Ocean from Peru to Polynesia", "the MGM Grand Garden Special Events Center", "Chicago History Museum", "Ronnie Hillman", "all-encompassing definition of the term", "1987", "more than 60 percent of the state's total land surface", "Eagle Ridge Mall", "Pel\u00e9", "reduce pressure on the public food supply", "Monastir / Tunisia / Africa", "fire", "Gomer Pyle", "at least 18 or 21 years old ( or have a legal guardian present )", "Ward", "poet", "Jamestown", "Monet", "tree growth stages"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2731060606060606}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.5, 1.0, 0.9333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9090909090909091, 0.0, 0.0, 0.18181818181818182, 0.0, 0.19999999999999998, 0.3333333333333333, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1860", "mrqa_hotpotqa-validation-3149", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-8006", "mrqa_hotpotqa-validation-4162", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-6389", "mrqa_triviaqa-validation-4791", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-10205", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-5682", "mrqa_triviaqa-validation-681", "mrqa_squad-validation-273", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-6119", "mrqa_squad-validation-3018", "mrqa_hotpotqa-validation-897", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-1451", "mrqa_hotpotqa-validation-3976", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-4803", "mrqa_squad-validation-4506"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 11, "before_eval": {"predictions": ["vincent Thomas \"Vince\" Lombardi", "Traumnovelle", "a Gender pay gap in favor of males in the labor market", "The TEU", "ice melting at 100 degrees", "the narrator driving a truck owned by his brother, who died in action in the United States Army", "joseph", "Abutment to abutment", "trams", "gun ban", "fripp", "Bulgars, and especially the Seljuk Turks", "died in battle", "Volkswagen Beetle", "joseph", "continental integration", "Queen Elizabeth I", "infection, irritation, or allergies", "The tower is the most - visited paid monument in the world. An average of 25,000 people ascend the tower every day which can result in long queues", "Town House Galleria", "catfish aquaculture", "a lustrous, purple - black metallic solid at standard conditions that sublimes readily to form a violet gas", "Evermoist", "Iraq", "a co-op of grape growers", "victor willsmeron", "joseph", "1952", "Los Angeles Lakers", "`` speed limit '' omitted and an additional panel stating the type of hazard ahead", "Jean F kernel ( 1497 -- 1558 ), a French physician", "chest, back, shoulders, torso and / or legs"], "metric_results": {"EM": 0.09375, "QA-F1": 0.20676762371615315}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.33333333333333337, 0.25, 0.23529411764705882, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.24000000000000002, 0.28571428571428575, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.09090909090909091, 0.22222222222222224, 0.25]}}, "error_ids": ["mrqa_triviaqa-validation-5078", "mrqa_squad-validation-7447", "mrqa_squad-validation-4169", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-5248", "mrqa_squad-validation-1003", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-validation-441", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-5184", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-9054", "mrqa_triviaqa-validation-7703", "mrqa_hotpotqa-validation-3921", "mrqa_triviaqa-validation-792", "mrqa_triviaqa-validation-5526", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6442"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 12, "before_eval": {"predictions": ["British rock group Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Joe Turano", "fencers", "Margaret Thatcher", "Kevin Harlan as play-by-play announcer, Boomer Esiason and Dan Fouts as color analysts, and James Lofton and Mark Malone as sideline reporters", "2014", "The stability, security, and predictability of British law and government", "Minoan or Mykenaian age", "1860", "quintero y Hermano", "norway", "New South Wales", "the French", "dandy", "ferric", "Orwell", "Czech Kingdom", "Gregg Popovich", "that not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence", "for creative reasons and `` not a reflection '' of the actress'performance", "adaptive immune system", "Mexican drug lord", "a musician", "nodel", "December 1, 1969", "american", "jK Rowling", "California State Automobile Association", "\"alone\"", "Cinderella", "The astronauts were asphyxiated before the hatch could be opened", "due to a lack of understanding of the legal ramifications"], "metric_results": {"EM": 0.125, "QA-F1": 0.2425783026885968}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false], "QA-F1": [0.15384615384615385, 0.0, 0.0, 1.0, 0.37037037037037035, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.11764705882352941, 1.0, 0.0, 0.23529411764705882, 0.4, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.16666666666666666, 0.8571428571428571]}}, "error_ids": ["mrqa_squad-validation-93", "mrqa_naturalquestions-validation-6167", "mrqa_triviaqa-validation-1671", "mrqa_squad-validation-568", "mrqa_hotpotqa-validation-2680", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-1758", "mrqa_hotpotqa-validation-4263", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-6902", "mrqa_hotpotqa-validation-1099", "mrqa_squad-validation-10202", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-2737", "mrqa_hotpotqa-validation-2886", "mrqa_squad-validation-9793", "mrqa_naturalquestions-validation-5175", "mrqa_squad-validation-6678", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-7371", "mrqa_squad-validation-2812", "mrqa_squad-validation-3935", "mrqa_squad-validation-6924"], "retrieved_ids": ["mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-41935", "mrqa_naturalquestions-train-24696", "mrqa_naturalquestions-train-67527", "mrqa_naturalquestions-train-26147", "mrqa_naturalquestions-train-34683", "mrqa_naturalquestions-train-82315", "mrqa_naturalquestions-train-3943", "mrqa_naturalquestions-train-66044", "mrqa_naturalquestions-train-68147", "mrqa_naturalquestions-train-34992", "mrqa_squad-validation-3181", "mrqa_triviaqa-validation-4856", "mrqa_squad-validation-3181", "mrqa_naturalquestions-train-8707", "mrqa_naturalquestions-train-60641", "mrqa_naturalquestions-train-55139", "mrqa_naturalquestions-train-74228", "mrqa_triviaqa-validation-1935", "mrqa_naturalquestions-train-2860", "mrqa_naturalquestions-train-39276", "mrqa_naturalquestions-train-85608", "mrqa_naturalquestions-train-28557", "mrqa_naturalquestions-train-7712", "mrqa_naturalquestions-train-87031", "mrqa_naturalquestions-train-6133", "mrqa_naturalquestions-train-40321", "mrqa_naturalquestions-train-59259", "mrqa_naturalquestions-train-20829", "mrqa_naturalquestions-train-13520", "mrqa_naturalquestions-train-55159", "mrqa_naturalquestions-train-76265"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 13, "before_eval": {"predictions": ["Sister, Sister ( 1982 film)", "president of Guggenheim Partners", "Craig T. Nelson as Bob Parr / Mr. Incredible, the patriarch of the Parr family, possessing super-strength and limited invulnerability", "Napoleon", "bakers", "3.7% of the entire student population", "increasing inequality", "maryland", "Tenacious D", "Jerry Ekandjo ( until February 2018 ), Erastus Utoni   Deputy : Agnes Tjongarero", "discipline problems with the Flight Director's orders during their flight", "maryland", "paddington", "amyotrophic lateral sclerosis ( ALS)", "odorama", "Swiss watchmaking industry", "mid 1970s", "Torah or Bible", "on the western coast of Italy", "first and only U.S. born world grand prix champion", "the quintessential New Orleans art form -- a jazz funeral without a body", "mid November", "Facebook", "mohnbeugel", "Rock Star ( 2001 film)", "Seattle", "King George's War", "cheated on Miley", "punk rock", "Fort Snelling, Minnesota", "daguerreotypes", "infrequent rain"], "metric_results": {"EM": 0.125, "QA-F1": 0.22671581890331888}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true], "QA-F1": [0.0, 0.4, 0.08333333333333333, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.33333333333333337, 0.3636363636363636, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2222222222222222, 0.5, 0.3333333333333333, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-1509", "mrqa_naturalquestions-validation-1135", "mrqa_squad-validation-830", "mrqa_triviaqa-validation-7638", "mrqa_squad-validation-7042", "mrqa_squad-validation-7351", "mrqa_triviaqa-validation-6944", "mrqa_hotpotqa-validation-4058", "mrqa_naturalquestions-validation-6445", "mrqa_squad-validation-4060", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-4486", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-2037", "mrqa_squad-validation-1850", "mrqa_naturalquestions-validation-150", "mrqa_hotpotqa-validation-800", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-2896", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-1932", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7310", "mrqa_hotpotqa-validation-3669", "mrqa_triviaqa-validation-6913"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 14, "before_eval": {"predictions": ["Beauty and the Breast", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "maryland", "garland Rose", "FX option", "electromagnetic waves", "caliphate", "garland", "Dimensions in Time", "Surveyor 3 unmanned lunar probe", "the end of January 1981", "gonadotropin - releasing hormone ( GnRH )", "baptism in the Small Catechism", "a Lutheran pastor in Hochfelden used a sermon to urge his parishioners to murder Jews", "garland garland", "slowing the vehicle", "Cheyenne rivers", "lateral changes in habitat (facies change in sedimentary strata )", "Hanna- Barbera", "Veneto region", "efficient and effective management of money ( funds )", "Latium in central Italy, 12 mi southeast of Rome, in the Alban Hills", "henryjewski a.k.a. whtmnk", "garland", "Alexander Hleb", "state sector", "February 1940", "weak government institutions", "a god of the Ammonites", "cornea (the transparent layer at the front of the eye )", "Uncle Fester", "Charles Whitman"], "metric_results": {"EM": 0.03125, "QA-F1": 0.1840229623317859}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.16666666666666669, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.4, 0.0, 0.4, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.5714285714285715, 0.18181818181818182, 0.5882352941176471, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.4615384615384615, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-970", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3253", "mrqa_triviaqa-validation-46", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10046", "mrqa_squad-validation-9751", "mrqa_triviaqa-validation-2442", "mrqa_squad-validation-7836", "mrqa_squad-validation-3999", "mrqa_naturalquestions-validation-5944", "mrqa_naturalquestions-validation-8180", "mrqa_squad-validation-2448", "mrqa_squad-validation-2509", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-116", "mrqa_squad-validation-5178", "mrqa_squad-validation-6023", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-validation-3302", "mrqa_hotpotqa-validation-1244", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-5191", "mrqa_naturalquestions-validation-6019", "mrqa_hotpotqa-validation-241", "mrqa_squad-validation-8444", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-5256"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 15, "before_eval": {"predictions": ["superhuman abilities", "Deathly Hallows", "the Flatbush section of Brooklyn, New York City", "San Antonio", "Akon, Christina Aguilera and Taio Cruz", "Jupiter (now known as the Galilean moons)", "a friend and publicist", "michael ondaatje", "masons'marks", "March 12, 1948", "Gateshead", "The horn line at the end is performed by the Phenix Horns from Earth, Wind & Fire", "the female cervix, uterus and uterine tubes", "1898", "Heavyweight", "Payaya Indians", "to steal the plans for the Death Star, the Galactic Empire's superweapon", "The Bells of St. Mary's", "a Curtiss JN-4 airplane", "art", "gorillas", "March 15, 1945", "absolute temperature", "hacking the private intelligence firm Stratfor and releasing the leaks through the whistle-blowing website", "American theoretical physicist and professor of physics at the University of California, Berkeley", "bicuspid", "his brother, Menelaus", "25 November 2015", "tallahassee", "prefabricated housing projects", "the British Press", "WOTV"], "metric_results": {"EM": 0.0625, "QA-F1": 0.08814102564102565}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.5714285714285715, 1.0, 0.0, 0.15384615384615383, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09523809523809525, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2612", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1479", "mrqa_hotpotqa-validation-335", "mrqa_triviaqa-validation-7512", "mrqa_squad-validation-1374", "mrqa_triviaqa-validation-5979", "mrqa_naturalquestions-validation-10439", "mrqa_hotpotqa-validation-1048", "mrqa_squad-validation-5249", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-56", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-validation-368", "mrqa_hotpotqa-validation-573", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-5877", "mrqa_triviaqa-validation-7578", "mrqa_hotpotqa-validation-5188", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-365", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-2957", "mrqa_triviaqa-validation-1736", "mrqa_naturalquestions-validation-9451", "mrqa_hotpotqa-validation-413", "mrqa_triviaqa-validation-1166", "mrqa_squad-validation-874", "mrqa_triviaqa-validation-1588", "mrqa_squad-validation-6091"], "retrieved_ids": ["mrqa_naturalquestions-train-5205", "mrqa_naturalquestions-train-21539", "mrqa_naturalquestions-train-49810", "mrqa_naturalquestions-train-40572", "mrqa_naturalquestions-train-47470", "mrqa_naturalquestions-train-58577", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-train-84845", "mrqa_naturalquestions-train-16956", "mrqa_naturalquestions-train-81971", "mrqa_naturalquestions-train-1557", "mrqa_naturalquestions-train-80159", "mrqa_naturalquestions-train-62048", "mrqa_naturalquestions-train-42542", "mrqa_hotpotqa-validation-5325", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-train-43906", "mrqa_naturalquestions-train-44254", "mrqa_naturalquestions-train-40409", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-85700", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-6119", "mrqa_naturalquestions-train-3222", "mrqa_triviaqa-validation-6692", "mrqa_hotpotqa-validation-3898", "mrqa_naturalquestions-validation-6896", "mrqa_naturalquestions-train-78579", "mrqa_naturalquestions-train-29696", "mrqa_naturalquestions-train-62065", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-6385"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 16, "before_eval": {"predictions": ["albin", "buder Basil", "blessed", "on the lateral side of the tibia", "ferguside", "the North Sea, through the former Meuse estuary, near Rotterdam", "the Kalahari Desert", "Colin Montgomerie", "October 29, 1985", "Amway", "secondary school study", "Thomas Sowell", "Speaker of the Lok Sabha or in his absence, the Deputy - Chairman of the Rajya Sabha", "Sinatra: London", "tANU", "Chad", "GMAT Sentence Correction (SC)", "an open work crown", "using a baby as bait, allowing a child to go through a torturous treatment to gain information", "Fulham", "French, English and Spanish", "michael Buerk", "U.S. Marshals", "What's Up", "supply chain management", "Mars", "Poland's Last King and English Culture", "polynomial algebra", "Michael J. Fox", "Japanese : \u4e09\u733f, Hepburn : san'en or sanzaru, alternatively \u4e09 \u5339 \u306e \u733f sanbiki no saru, literally `` three monkeys ''", "sheepskin", "Honolulu"], "metric_results": {"EM": 0.125, "QA-F1": 0.16279761904761905}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8095238095238095, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-2551", "mrqa_naturalquestions-validation-9218", "mrqa_triviaqa-validation-313", "mrqa_squad-validation-9319", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4681", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2771", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-3483", "mrqa_hotpotqa-validation-1566", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-4055", "mrqa_squad-validation-5407", "mrqa_naturalquestions-validation-7144", "mrqa_hotpotqa-validation-4164", "mrqa_triviaqa-validation-639", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-1001", "mrqa_triviaqa-validation-1770", "mrqa_triviaqa-validation-866", "mrqa_squad-validation-8223", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-validation-9087", "mrqa_hotpotqa-validation-2287"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 17, "before_eval": {"predictions": ["belgia", "Deadpool, X-Men: Apocalypse, Independence Day: Resurgence and Eddie the Eagle", "August 6, 1845", "stable, non-radioactive rubidium - 85", "James Zeebo", "sovereign states", "president of the United States", "The Discovery Institute (DI) is a politically conservative non-profit think tank based in Seattle, Washington, best known for its advocacy of the pseudoscientific principle of intelligent design (ID)", "Sam's captured parents", "Australian", "30 months and for women 18 months ( although in accordance with a temporary order from January 10, 1968, six additional months for men and 24 months for women respectively", "opportunities will vary by geographic area and subject taught", "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued)", "a private liberal arts college", "Roy Warren Spencer", "\"antiforms\"", "June 9, 2015", "V. Prakash Kumar", "Grace Nail Johnson", "Keith Richards", "prime number p with n < p < 2n \u2212 2, for any natural number n > 3", "Bangor International Airport", "teachers who specialize in one subject and who tend to be more knowledgeable in that one area than a teacher who teaches many subjects", "180th meridian in a 360 \u00b0 - system", "Adult Swim", "the Presiding Officer on the advice of the parliamentary bureau", "the Miami Heat of the National Basketball Association (NBA)", "33", "grapevines", "Annual Conference Cabinet", "field hockey player Hannah Macleod", "William Hartnell's poor health"], "metric_results": {"EM": 0.125, "QA-F1": 0.2641732357357357}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.08333333333333334, 0.0, 0.0, 0.0, 0.0, 0.43243243243243246, 0.19999999999999998, 0.16, 0.0, 0.8, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.5, 0.25, 0.08333333333333333, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1298", "mrqa_squad-validation-608", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3559", "mrqa_triviaqa-validation-314", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-3440", "mrqa_naturalquestions-validation-1090", "mrqa_squad-validation-2053", "mrqa_squad-validation-7572", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-4868", "mrqa_naturalquestions-validation-9171", "mrqa_squad-validation-8966", "mrqa_hotpotqa-validation-2782", "mrqa_squad-validation-1904", "mrqa_naturalquestions-validation-10347", "mrqa_hotpotqa-validation-3573", "mrqa_squad-validation-9405", "mrqa_hotpotqa-validation-613", "mrqa_hotpotqa-validation-4463", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-1130", "mrqa_squad-validation-7664"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 18, "before_eval": {"predictions": ["Rwanda genocide, also known as the genocide against the Tutsi", "two or more teachers working harmoniously to fulfill the needs of every student in the classroom", "400 metres", "Nasim Pedrad", "the entertainment division", "A to a point B", "12", "Great Exhibition of 1851", "King Edward I to Henry VIII", "the Chagos Archipelago", "dundee", "the person compelled to pay for reformist programs", "lyle darnley", "\"Grindhouse\" fake trailer", "davenport", "digital transmission", "the Swiss- Austrian border", "lithium-ion battery", "821", "Sky channels", "liquid", "Kim Hyun-ah", "the races of highest'social efficiency'", "the pitches of the scale of C major equally by a whole tone", "the \" King of Cool\"", "President Wilson and the American delegation from the Paris Peace Conference", "Proxenus", "the fifth season", "dave dors", "Hockey Club Davos", "Michael Patrick Smith", "Qutab Ud - Din - Aibak, founder of the Delhi Sultanate"], "metric_results": {"EM": 0.1875, "QA-F1": 0.32247023809523806}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, false], "QA-F1": [0.2, 0.13333333333333333, 0.5, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666665, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.4, 1.0, 0.6666666666666666]}}, "error_ids": ["mrqa_hotpotqa-validation-1441", "mrqa_squad-validation-1914", "mrqa_hotpotqa-validation-523", "mrqa_hotpotqa-validation-2145", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-3300", "mrqa_squad-validation-5257", "mrqa_hotpotqa-validation-2993", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-2683", "mrqa_hotpotqa-validation-2201", "mrqa_triviaqa-validation-2715", "mrqa_naturalquestions-validation-2222", "mrqa_squad-validation-9074", "mrqa_squad-validation-2862", "mrqa_triviaqa-validation-4279", "mrqa_hotpotqa-validation-1855", "mrqa_squad-validation-9841", "mrqa_naturalquestions-validation-4497", "mrqa_squad-validation-9827", "mrqa_triviaqa-validation-1764", "mrqa_hotpotqa-validation-3798", "mrqa_triviaqa-validation-7100", "mrqa_hotpotqa-validation-3446", "mrqa_naturalquestions-validation-10490"], "retrieved_ids": ["mrqa_naturalquestions-train-18797", "mrqa_naturalquestions-train-49719", "mrqa_naturalquestions-train-69860", "mrqa_naturalquestions-train-13512", "mrqa_naturalquestions-train-51350", "mrqa_naturalquestions-train-17900", "mrqa_naturalquestions-train-24742", "mrqa_naturalquestions-train-49719", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-781", "mrqa_naturalquestions-train-80958", "mrqa_naturalquestions-train-101", "mrqa_naturalquestions-train-60208", "mrqa_naturalquestions-train-57079", "mrqa_hotpotqa-validation-2937", "mrqa_naturalquestions-train-65495", "mrqa_naturalquestions-train-62665", "mrqa_naturalquestions-train-68895", "mrqa_naturalquestions-train-15636", "mrqa_naturalquestions-train-33207", "mrqa_naturalquestions-train-65495", "mrqa_naturalquestions-train-13346", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-train-51644", "mrqa_naturalquestions-train-61719", "mrqa_naturalquestions-train-81971", "mrqa_naturalquestions-train-27938", "mrqa_naturalquestions-train-36373", "mrqa_naturalquestions-train-16811", "mrqa_naturalquestions-train-75322", "mrqa_naturalquestions-train-43105", "mrqa_hotpotqa-validation-5899"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 19, "before_eval": {"predictions": ["Mauritian", "Norman Macdonnell", "aragon", "11.1", "the first trans-Pacific flight from the United States to Australia", "Sharman Joshi", "students normally have to sit in a classroom and do work, write lines or a punishment essay, or sit quietly", "Forster I, Forster II, and Forster III", "a divisor of p \u2212 1", "Ana", "Cherry Hill", "Season 4", "socrates", "socrates", "Six Degrees of Separation", "blackstar", "Indian", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "1974", "Nicki Minaj", "venzance", "Huguenot", "socrates", "socrates", "\"Drawn Together\"", "William the Conqueror", "Ben Gurion International Airport", "two", "the Corinthian and Saronic Gulfs", "blood", "Guinness World Records", "Sunset Publishing Corporation"], "metric_results": {"EM": 0.15625, "QA-F1": 0.24438795853269538}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.10526315789473684, 1.0, 0.3333333333333333, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2389", "mrqa_triviaqa-validation-6901", "mrqa_squad-validation-6963", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-303", "mrqa_squad-validation-1942", "mrqa_squad-validation-9214", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-9284", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-6935", "mrqa_triviaqa-validation-1995", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-8025", "mrqa_triviaqa-validation-2015", "mrqa_squad-validation-3151", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4068", "mrqa_hotpotqa-validation-57", "mrqa_naturalquestions-validation-1565", "mrqa_hotpotqa-validation-5061", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-3552", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-2627"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 20, "before_eval": {"predictions": ["reciprocating", "David Feldman", "the prints and architectural drawings", "black on Both Sides", "kalos", "British Columbia, in the Abbotsford, Vancouver and Langley areas", "AS-205", "ribosomal", "kingfisher", "six", "Scott Bakula as Dwayne `` King '' Cassius Pride, NCIS Special Agent", "'I Swear\"", "Cozonac", "Belfast West", "a heliocentric orbit", "Sulla", "Super Bowl LII, following the 2017 season", "Golden Globe", "Swahili", "trust God's word", "snow", "Pantone Matching System (PMS)", "Firoz Shah Tughlaq", "\" My Love from the Star\"", "San Jose", "sea wasp", "the Hawai\u02bbi House of Representatives", "a \"teleforce\" weapon", "Thunderbird of Native American tradition", "the most giving Super Bowl ever", "29.7", "casket specialist"], "metric_results": {"EM": 0.21875, "QA-F1": 0.30582611832611833}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, false, true, false, true, true, false], "QA-F1": [0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.48484848484848486, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-3368", "mrqa_hotpotqa-validation-779", "mrqa_hotpotqa-validation-3547", "mrqa_triviaqa-validation-5091", "mrqa_naturalquestions-validation-4590", "mrqa_squad-validation-3964", "mrqa_hotpotqa-validation-2434", "mrqa_triviaqa-validation-3486", "mrqa_hotpotqa-validation-1210", "mrqa_naturalquestions-validation-1279", "mrqa_hotpotqa-validation-4627", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-4852", "mrqa_hotpotqa-validation-3623", "mrqa_naturalquestions-validation-2448", "mrqa_squad-validation-8464", "mrqa_triviaqa-validation-6721", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-4015", "mrqa_squad-validation-315", "mrqa_triviaqa-validation-1453", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2064", "mrqa_triviaqa-validation-935"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 21, "before_eval": {"predictions": ["a Czech word, robota, meaning `` forced labor ''", "dust jacket", "various registries", "dog", "Yazoo", "22 April 1894", "black hole", "\"the soul does not sleep (anima non sic dormit) but wakes (sed vigilat) and experiences visions\"", "French chose to cede the former, but was able to negotiate the retention of Saint Pierre and Miquelon, two small islands in the Gulf of St. Lawrence", "Willie Nelson and Kris Kristofferson", "ill. (some col.)", "5 University of California campuses", "French pirate", "Lewis", "Charles Dickens", "a forum in which the Nobel Peace Laureates and the Peace Laureate Organizations could come together to address global issues with a view to encourage and support peace and human well being in the world", "proteins", "2001", "there must be infinitely many primes", "ITV News at Ten", "padlocking the gates and using sickles to deflate one of the large domes covering two satellite dishes", "1969", "huldra", "in western portions of the Great Lakes region (an area not directly subject to the conflict between the French and British) including the Huron, Mississauga, Ojibwa, Winnebago, and Potawatomi", "Orthodox Christians", "James Bond", "640 \u00d7 1136", "colt Teofilo", "eating both fish larvae and small crustaceans that would otherwise feed the adult fish", "Menace II Society", "quarterback", "trio"], "metric_results": {"EM": 0.15625, "QA-F1": 0.21727589229155503}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.14285714285714288, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.24390243902439027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4210526315789474, 1.0, 1.0, 0.0, 0.0, 0.23529411764705882, 1.0, 0.0, 0.43750000000000006, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.2222222222222222]}}, "error_ids": ["mrqa_naturalquestions-validation-3609", "mrqa_triviaqa-validation-5239", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-751", "mrqa_triviaqa-validation-4383", "mrqa_hotpotqa-validation-4430", "mrqa_squad-validation-2412", "mrqa_squad-validation-10502", "mrqa_hotpotqa-validation-5480", "mrqa_triviaqa-validation-2961", "mrqa_squad-validation-2709", "mrqa_hotpotqa-validation-4950", "mrqa_naturalquestions-validation-5439", "mrqa_squad-validation-5345", "mrqa_hotpotqa-validation-35", "mrqa_squad-validation-9020", "mrqa_triviaqa-validation-1954", "mrqa_squad-validation-6844", "mrqa_hotpotqa-validation-2399", "mrqa_squad-validation-10177", "mrqa_squad-validation-8456", "mrqa_triviaqa-validation-6950", "mrqa_naturalquestions-validation-6832", "mrqa_triviaqa-validation-1555", "mrqa_squad-validation-4648", "mrqa_naturalquestions-validation-2758", "mrqa_hotpotqa-validation-4676"], "retrieved_ids": ["mrqa_naturalquestions-train-75505", "mrqa_naturalquestions-train-75672", "mrqa_naturalquestions-train-10617", "mrqa_naturalquestions-train-3478", "mrqa_naturalquestions-train-23475", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-7415", "mrqa_naturalquestions-train-43497", "mrqa_naturalquestions-train-630", "mrqa_hotpotqa-validation-5899", "mrqa_naturalquestions-train-20355", "mrqa_hotpotqa-validation-1001", "mrqa_naturalquestions-train-2833", "mrqa_naturalquestions-train-79099", "mrqa_naturalquestions-train-60641", "mrqa_naturalquestions-train-68904", "mrqa_naturalquestions-train-86945", "mrqa_naturalquestions-train-45309", "mrqa_naturalquestions-train-22090", "mrqa_naturalquestions-train-28545", "mrqa_naturalquestions-train-8315", "mrqa_naturalquestions-train-76861", "mrqa_naturalquestions-train-13334", "mrqa_naturalquestions-train-76561", "mrqa_naturalquestions-train-52318", "mrqa_naturalquestions-train-63620", "mrqa_naturalquestions-train-69947", "mrqa_naturalquestions-train-34340", "mrqa_naturalquestions-train-46054", "mrqa_naturalquestions-train-1038", "mrqa_naturalquestions-train-22705", "mrqa_naturalquestions-train-8124"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 22, "before_eval": {"predictions": ["Alex Skuby", "England", "the first Thursday in May", "Mediterranean Shipping Company S.A.", "alison dale", "a European fairy tale in 1697", "Liberal Party of Australia", "Royalists", "any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "are ceremonially placed on the heads of Christians on Ash Wednesday, either by being sprinkled over their heads or, in English - speaking countries, more often by being marked on their foreheads as a visible cross", "death to spies", "20,000 leagues under the sea", "Augustus Waters", "1619", "Tony Blair", "\u2018 Often damaging\ufffd'", "June 11, 1973", "Kenya", "Timeline of Shakespeare criticism", "boudicca", "neutrality", "United Healthcare", "AMC Theatres", "\"The Gang\"", "3 October 1990", "March 1, 2018", "the exchange of the heavy W and Z bosons", "daedalus", "Martin Luther King III", "Development of Substitute Materials", "Chronicles of Barsetshire", "vast areas that have been left undeveloped"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3304757395382396}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false], "QA-F1": [0.0, 0.0, 0.4, 0.8571428571428571, 0.0, 0.0, 0.75, 0.33333333333333337, 0.1111111111111111, 0.16, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.4, 0.0, 0.0, 0.22222222222222224, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445]}}, "error_ids": ["mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1883", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-9789", "mrqa_triviaqa-validation-5698", "mrqa_naturalquestions-validation-7346", "mrqa_squad-validation-2884", "mrqa_hotpotqa-validation-2959", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-1578", "mrqa_triviaqa-validation-2797", "mrqa_naturalquestions-validation-3859", "mrqa_squad-validation-2769", "mrqa_hotpotqa-validation-482", "mrqa_squad-validation-8280", "mrqa_hotpotqa-validation-5655", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-190", "mrqa_squad-validation-10459", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-1929", "mrqa_squad-validation-2828"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 23, "before_eval": {"predictions": ["an additional $105 billion in growth to the country's economy over five years", "mono", "about two-thirds the size", "1934 Austin seven box saloon", "reigo Luna", "red", "Dreamland", "Best Animated Feature", "European Union institutions", "American record", "nine", "NASA's CAL IPSO satellite", "celandine", "U.S. ambassador's residence with New York Times columnist James `` Scotty '' Reston", "Ronald Ralph \"Ronnie\" Schell", "artemisinin-based therapy", "Mumbai, Maharashtra", "the east of Ireland", "1940", "2017 / 18 Divisional Round game against the New Orleans Saints", "possibly 1707", "on Fresno's far southeast side, bounded by Chestnut Avenue to the West", "midlands of the river Niger Benue trough to over 2,000 mm ( 78.7 in ) along the south western escarpment of the Jos Plateau", "Jane Seymour", "synovial joint", "bobby riggs", "Democritus", "Santa Clara Marriott", "benjamin barenboim", "political power generated by wealth", "log-space reductions", "Corey Brown"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3293569971694972}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, false, false], "QA-F1": [0.3076923076923077, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.6666666666666666, 0.25, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.3636363636363636, 0.6666666666666666, 0.0, 0.08, 0.0, 0.16666666666666669, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0]}}, "error_ids": ["mrqa_squad-validation-7389", "mrqa_triviaqa-validation-3716", "mrqa_squad-validation-8850", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-3933", "mrqa_hotpotqa-validation-4348", "mrqa_hotpotqa-validation-789", "mrqa_squad-validation-4118", "mrqa_hotpotqa-validation-2741", "mrqa_squad-validation-4228", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-validation-3714", "mrqa_hotpotqa-validation-1782", "mrqa_triviaqa-validation-3595", "mrqa_hotpotqa-validation-26", "mrqa_naturalquestions-validation-3052", "mrqa_squad-validation-2420", "mrqa_squad-validation-4662", "mrqa_naturalquestions-validation-2212", "mrqa_triviaqa-validation-571", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-3420", "mrqa_hotpotqa-validation-2340", "mrqa_triviaqa-validation-6781", "mrqa_squad-validation-1750", "mrqa_squad-validation-769"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 24, "before_eval": {"predictions": ["the 1965 -- 66 season", "photosystem I", "the Duke of Cumberland", "alchemy", "WBA title", "Moluccas", "Saturday", "Lear ( in the Quarto version ) or Edgar", "Arab oil producers linked any future policy changes to peace", "1971", "J.R. R. Tolkien", "John Elway", "Selena Gomez", "graduateuates from the polytechnics and colleges can then join the workforce and later obtain a specialised higher diploma qualification after a further one to two years of training", "bingo", "Eugene", "new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "primes of the form 2p + 1 with p prime", "letter", "Fa Ze YouTubers", "nine circles", "Mongols and a Muslim", "along the coast", "Friars Minor", "CD Castell\u00f3n", "1789, or 1798", "12\u20134", "ctenophores form an animal phylum that is more complex than sponges, about as complex as cnidarians (jellyfish, sea anemones, etc) and less complex than bilaterians", "Patrick Wachsberger and Erik Feig of Summit Entertainment produced with Adam Shankman and Jennifer Gibgot of Offspring Entertainment", "Mission Specialist for mission STS-51-L", "the Pennsylvania Dutch superstition that if a groundhog ( Deitsch : Grundsau, Grunddax, Dax ) emerging from its burrow on this day sees a shadow due to clear weather", "minister of external trade and tourism"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3623957337951903}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.2857142857142857, 0.08695652173913043, 1.0, 0.5714285714285715, 1.0, 0.0, 0.07142857142857142, 0.0, 0.0, 0.5, 0.7692307692307693, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.6785714285714286, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8444", "mrqa_triviaqa-validation-3678", "mrqa_hotpotqa-validation-4444", "mrqa_triviaqa-validation-3340", "mrqa_naturalquestions-validation-9011", "mrqa_squad-validation-3741", "mrqa_hotpotqa-validation-3247", "mrqa_naturalquestions-validation-10381", "mrqa_squad-validation-8473", "mrqa_triviaqa-validation-667", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-309", "mrqa_squad-validation-8979", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-3297", "mrqa_triviaqa-validation-3847", "mrqa_squad-validation-6204", "mrqa_triviaqa-validation-5304", "mrqa_hotpotqa-validation-5588", "mrqa_naturalquestions-validation-3504", "mrqa_squad-validation-4417", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-1814", "mrqa_naturalquestions-validation-6508", "mrqa_triviaqa-validation-1306"], "retrieved_ids": ["mrqa_triviaqa-validation-2959", "mrqa_naturalquestions-train-28391", "mrqa_naturalquestions-train-85803", "mrqa_naturalquestions-train-51094", "mrqa_triviaqa-validation-5091", "mrqa_naturalquestions-train-27314", "mrqa_naturalquestions-train-31102", "mrqa_naturalquestions-train-69860", "mrqa_naturalquestions-train-86944", "mrqa_naturalquestions-train-19223", "mrqa_naturalquestions-train-18064", "mrqa_hotpotqa-validation-2449", "mrqa_naturalquestions-train-7001", "mrqa_squad-validation-874", "mrqa_naturalquestions-train-32835", "mrqa_triviaqa-validation-2015", "mrqa_naturalquestions-train-29696", "mrqa_naturalquestions-train-24788", "mrqa_naturalquestions-train-63155", "mrqa_triviaqa-validation-7578", "mrqa_naturalquestions-train-42645", "mrqa_naturalquestions-train-30308", "mrqa_naturalquestions-train-36957", "mrqa_triviaqa-validation-3515", "mrqa_naturalquestions-train-1148", "mrqa_naturalquestions-train-47660", "mrqa_hotpotqa-validation-2262", "mrqa_naturalquestions-train-8357", "mrqa_naturalquestions-train-46200", "mrqa_triviaqa-validation-2722", "mrqa_naturalquestions-train-31830", "mrqa_naturalquestions-train-10053"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 25, "before_eval": {"predictions": ["alpine skiing", "slave", "over 50 million singles", "states'rights to expand slavery", "between 1923 and 1925", "Orlando\u2013Kissimmee\u2013 Sanford, Florida Metropolitan Statistical Area", "January 19, 1962", "Frigate", "Buck Barrow", "iteratively", "geese", "the move from the manufacturing sector to the service sector", "Brisbane River", "Sylvester McCoy", "August 14, 1848", "lower rates of health and social problems", "juveniles", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "the way they used `` rule '' and `` method '' to go about their religious affairs", "co-written the book of the musical \"A Chorus Line\"", "2,664 rooms and 220 suites", "iPhone 6 Plus", "a chute beneath his or her feet", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "cleaning, catering and security porterage business", "Symphony No. 7", "gironde", "1603", "ranked above the two personal physicians of the Emperor", "The Nutcracker", "We admitted we were powerless over alcohol -- that our lives had become unmanageable", "Chicago Cubs"], "metric_results": {"EM": 0.3125, "QA-F1": 0.49168643856143857}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, true, true, false], "QA-F1": [0.0, 0.0, 0.3333333333333333, 0.25, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.3333333333333333, 0.25, 0.33333333333333337, 0.4615384615384615, 0.923076923076923, 0.2857142857142857, 0.33333333333333337, 1.0, 1.0, 0.3636363636363636, 0.8, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-5788", "mrqa_hotpotqa-validation-1818", "mrqa_naturalquestions-validation-10169", "mrqa_naturalquestions-validation-4072", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-3977", "mrqa_triviaqa-validation-6091", "mrqa_squad-validation-7382", "mrqa_triviaqa-validation-7336", "mrqa_squad-validation-7677", "mrqa_hotpotqa-validation-4720", "mrqa_squad-validation-7301", "mrqa_squad-validation-4637", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-859", "mrqa_hotpotqa-validation-4282", "mrqa_naturalquestions-validation-7309", "mrqa_hotpotqa-validation-1414", "mrqa_squad-validation-6328", "mrqa_triviaqa-validation-1259"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 26, "before_eval": {"predictions": ["Jewish audiences, with the Romans serving as external arbiters on disputes concerning Jewish customs and law", "south", "Magic formula investing", "true history", "The women's events are the Vans Hawaiian Pro at Haleiwa Ali'i Beach Park", "1910", "non-teaching posts", "Catch Me Who Can", "jazz", "tennis", "4,000", "the founder of the Yuan dynasty", "Heathcliff", "canal", "spice", "Phil Hartman", "Raymond Unwin and the architect Barry Parker", "San Bernardino", "an extensive neoclassical centre referred to as Tyneside Classical largely developed in the 1830s by Richard Grainger and John Dobson, and recently extensively restored", "Albany High School for Educating People of Color", "Ram Bagh", "a non-commissioned officer in the United States Army's premier special operations unit, the 1st Special Forces Operational Detachment- Delta (1SFOD-D) or \" Delta Force\"", "Shaw", "seek jury nullification", "Cee - Lo", "Anglican", "Hattie McDaniel", "france", "magnetism", "sitting directly on the Gulf of Guinea on the Atlantic Ocean in Nigeria", "The economic impact of the former type of entrepreneurialism tends to be redistributive while the latter is expected to foster technological progress and thus have a more positive impact on economic growth", "January 11, 1755"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2757527195027195}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [0.2222222222222222, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.923076923076923, 0.0, 0.08333333333333334, 0.5454545454545454, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.7499999999999999]}}, "error_ids": ["mrqa_naturalquestions-validation-3027", "mrqa_hotpotqa-validation-2184", "mrqa_triviaqa-validation-6772", "mrqa_hotpotqa-validation-4553", "mrqa_squad-validation-7435", "mrqa_squad-validation-1994", "mrqa_triviaqa-validation-6689", "mrqa_hotpotqa-validation-5", "mrqa_squad-validation-6148", "mrqa_triviaqa-validation-3472", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6224", "mrqa_hotpotqa-validation-2436", "mrqa_naturalquestions-validation-2214", "mrqa_squad-validation-2428", "mrqa_squad-validation-5184", "mrqa_hotpotqa-validation-1446", "mrqa_naturalquestions-validation-800", "mrqa_hotpotqa-validation-4585", "mrqa_naturalquestions-validation-3658", "mrqa_squad-validation-6837", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-7581", "mrqa_triviaqa-validation-5050", "mrqa_triviaqa-validation-3393", "mrqa_squad-validation-7319", "mrqa_hotpotqa-validation-945"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 27, "before_eval": {"predictions": ["alsivar", "geldof", "berry", "belgium", "Big Mamie", "ocelots", "the \"eternal outsider, the sardonic drifter\" someone who rebels against the social structure", "It is a highly reactive substance and must be segregated from combustible materials", "they captured the Tower of London", "2009", "February 10, 1984", "inner mitochondria membrane", "aline Charigot", "events such as the popular Port Fairy Folk Festival, Queenscliff Music Festival, Bells Beach Surf Classic and the Bright Autumn Festival", "The channel which can get carriage on a suitable beam of a satellite", "the fourth season", "the weak and electromagnetic forces are expressions of a more fundamental electroweak interaction", "availability of skilled tradespeople", "diamond", "A simple iron boar crest", "polytechnics became new universities", "Jack Holloway", "James", "on kickoffs at the 25 - yard line", "Swedish astronomer Anders Celsius", "7,000", "lion, leopard, buffalo, rhinoceros, and elephant", "by faith", "ludwig", "possible combinations of two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "antwerp", "group"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3226048074732285}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.5333333333333333, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.0, 0.631578947368421, 0.8, 1.0, 1.0, 0.8, 0.0, 0.6060606060606061, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-348", "mrqa_triviaqa-validation-2899", "mrqa_triviaqa-validation-6331", "mrqa_hotpotqa-validation-2428", "mrqa_squad-validation-3539", "mrqa_triviaqa-validation-2980", "mrqa_hotpotqa-validation-4022", "mrqa_squad-validation-8739", "mrqa_triviaqa-validation-1423", "mrqa_squad-validation-2966", "mrqa_squad-validation-2733", "mrqa_hotpotqa-validation-158", "mrqa_squad-validation-10312", "mrqa_squad-validation-6855", "mrqa_triviaqa-validation-5962", "mrqa_triviaqa-validation-1034", "mrqa_hotpotqa-validation-3949", "mrqa_naturalquestions-validation-9105", "mrqa_naturalquestions-validation-3771", "mrqa_squad-validation-2313", "mrqa_triviaqa-validation-388", "mrqa_naturalquestions-validation-2893", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-1507"], "retrieved_ids": ["mrqa_naturalquestions-train-49121", "mrqa_naturalquestions-train-35943", "mrqa_naturalquestions-train-30083", "mrqa_naturalquestions-train-68952", "mrqa_naturalquestions-train-50915", "mrqa_naturalquestions-train-66924", "mrqa_triviaqa-validation-7336", "mrqa_naturalquestions-train-16061", "mrqa_naturalquestions-train-49957", "mrqa_naturalquestions-train-25772", "mrqa_naturalquestions-train-39862", "mrqa_naturalquestions-train-53137", "mrqa_naturalquestions-train-50473", "mrqa_naturalquestions-train-13675", "mrqa_naturalquestions-train-79078", "mrqa_naturalquestions-train-72595", "mrqa_naturalquestions-train-14788", "mrqa_naturalquestions-train-68441", "mrqa_naturalquestions-train-84830", "mrqa_naturalquestions-train-43105", "mrqa_naturalquestions-train-74251", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-train-58901", "mrqa_triviaqa-validation-1306", "mrqa_naturalquestions-train-7107", "mrqa_naturalquestions-train-60612", "mrqa_naturalquestions-train-47354", "mrqa_naturalquestions-train-74843", "mrqa_naturalquestions-train-73434", "mrqa_naturalquestions-train-72300", "mrqa_triviaqa-validation-3847", "mrqa_naturalquestions-train-82315"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 28, "before_eval": {"predictions": ["the Muslim faith as a tool of the devil, he was indifferent to its practice", "Chris Weidman", "jury nullification", "Harishchandra", "a blind Arab philosopher, poet, and writer", "Professor Eobard Thawne", "slivovitz", "a US$10 a week raise over Tesla's US$18 per week salary", "October 25, 1825", "member states", "clarinets", "McKinsey's offices in Silicon Valley and India and created its Internet practice", "gypsy", "living Doll", "Crohn's disease", "Ondemar Dias", "Raya Yarbrough", "Arizona", "cruiserweight", "businessmen Charles L. Hutchinson (trustee, treasurer and donor of Hutchinson Commons) Martin A. Ryerson", "Old Testament", "UPS", "local talent", "Football League", "tristan Farnon", "pacific region", "the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague", "mary surrattsville", "1349", "dodo bird", "people and their thoughts are both made from `` pure energy '', and that through the process of `` like energy attracting like energy '' a person can improve their own health, wealth and personal relationships", "on the Buses"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2821339994331066}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.5, 0.4444444444444445, 0.0, 0.2857142857142857, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 0.375, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.1, 0.0, 0.0, 1.0, 0.2040816326530612, 0.0]}}, "error_ids": ["mrqa_squad-validation-2291", "mrqa_hotpotqa-validation-1390", "mrqa_squad-validation-6835", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-5637", "mrqa_triviaqa-validation-4827", "mrqa_squad-validation-1276", "mrqa_hotpotqa-validation-4352", "mrqa_naturalquestions-validation-10495", "mrqa_triviaqa-validation-5237", "mrqa_hotpotqa-validation-5110", "mrqa_triviaqa-validation-3760", "mrqa_triviaqa-validation-6250", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-7110", "mrqa_triviaqa-validation-1347", "mrqa_squad-validation-8031", "mrqa_naturalquestions-validation-10687", "mrqa_hotpotqa-validation-1251", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-6881", "mrqa_squad-validation-4921", "mrqa_triviaqa-validation-5810", "mrqa_squad-validation-8190", "mrqa_naturalquestions-validation-7821", "mrqa_triviaqa-validation-4308"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 29, "before_eval": {"predictions": ["hollies", "886 AD", "used to finance his own projects with varying degrees of success", "Le Mans", "Kinect", "Tokyo", "safety Darian Stewart", "the parallelogram rule of vector addition", "j John raulston", "nine", "the reactor core", "Van Gogh", "the bore, and often the stroke, are increased in low-pressure cylinders resulting in larger cylinders", "performs six major functions ; support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "the local administrative structure of past Chinese dynasties", "Doctor Who Theme", "National Basketball Development League", "gillingham", "St. Mary's County", "Ted Ginn Jr.", "2,615 at the 2010 census", "Pyeongchang", "athlete Colin Kaepernick ( although he prefers \"Kap\")", "a password recovery tool for Microsoft Windows", "Homeless Man", "Charles and Ray Eames", "florida", "austin stingsworth", "the smallest subfield of a field F containing both 0 and 1", "heartburn", "53% in Botswana to -40% in Bahrain", "photosynthesis"], "metric_results": {"EM": 0.125, "QA-F1": 0.22130366569704804}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.0, 0.38095238095238093, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.058823529411764705, 0.6666666666666666, 0.28571428571428575, 0.0, 0.0, 0.33333333333333337, 1.0, 0.25, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4708", "mrqa_naturalquestions-validation-866", "mrqa_squad-validation-1249", "mrqa_hotpotqa-validation-3497", "mrqa_triviaqa-validation-6632", "mrqa_naturalquestions-validation-6610", "mrqa_squad-validation-814", "mrqa_squad-validation-10408", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-3103", "mrqa_naturalquestions-validation-8653", "mrqa_squad-validation-3344", "mrqa_naturalquestions-validation-5826", "mrqa_squad-validation-7914", "mrqa_triviaqa-validation-1280", "mrqa_naturalquestions-validation-5961", "mrqa_squad-validation-772", "mrqa_hotpotqa-validation-3031", "mrqa_naturalquestions-validation-9765", "mrqa_hotpotqa-validation-5383", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-7567", "mrqa_hotpotqa-validation-5709", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-2709", "mrqa_squad-validation-9036", "mrqa_squad-validation-7445", "mrqa_squad-validation-8873"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 30, "before_eval": {"predictions": ["the judiciary", "named for Frederick Louis, Prince of Wales, son of King George II", "the Mayor's son", "abraham lincoln", "john cleese and Connie Booth", "Erick Avari, Michael McKean, Amy D. Jacobson, Marty Ingels, Earl Boen, Jordana Capra, Dirk Benedict, and Rip Taylor", "BBC UKTV", "lester", "demographics and economic ties", "three or more", "The Kickoff Game", "narcolepsy", "arctic monkeys", "monza", "jurassic park", "all transmissions are in clear text, and usernames, passwords, commands and data can be read by anyone able to perform packet capture ( sniffing ) on the network", "instructions", "The centre-left Australian Labor Party (ALP) the centre-right Liberal Party, the rural-based National Party of Australia, and the environmentalist Australian Greens", "marduk", "hekla", "President Kenyatta's largest source of foreign direct investment", "pike", "South Pacific off the northeast coast of Australia", "Article 7, Paragraph 4", "Delaware to the southeast, Maryland to the south, West Virginia to the southwest, Ohio to the west, Lake Erie and the Canadian province of Ontario to the northwest, New York to the north, and New Jersey to the east", "Easy", "Sebastian Lund ( Rob Kerkovich ), a criminalist turned forensics agent and the team's newest member", "kamaraj", "National Lottery", "Apollo", "catherine of aragon", "in order to facilitate compliance with the Telephone Consumer Protection Act of 1991"], "metric_results": {"EM": 0.0625, "QA-F1": 0.16181396119718489}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.125, 0.22222222222222224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.14814814814814814, 0.0, 0.21052631578947367, 0.0, 0.0, 0.8571428571428571, 0.0, 0.4444444444444445, 0.4, 0.12121212121212122, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636]}}, "error_ids": ["mrqa_naturalquestions-validation-4645", "mrqa_hotpotqa-validation-2559", "mrqa_naturalquestions-validation-9608", "mrqa_triviaqa-validation-3920", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-4079", "mrqa_squad-validation-7793", "mrqa_triviaqa-validation-1921", "mrqa_squad-validation-2577", "mrqa_hotpotqa-validation-4578", "mrqa_naturalquestions-validation-3209", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-6888", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-9852", "mrqa_squad-validation-2885", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7090", "mrqa_squad-validation-8451", "mrqa_triviaqa-validation-7684", "mrqa_naturalquestions-validation-4710", "mrqa_squad-validation-6961", "mrqa_hotpotqa-validation-4604", "mrqa_hotpotqa-validation-3333", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-3134", "mrqa_hotpotqa-validation-5604", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-2091", "mrqa_naturalquestions-validation-10328"], "retrieved_ids": ["mrqa_triviaqa-validation-3945", "mrqa_naturalquestions-train-26521", "mrqa_naturalquestions-train-29696", "mrqa_triviaqa-validation-7578", "mrqa_naturalquestions-train-3268", "mrqa_naturalquestions-train-85399", "mrqa_triviaqa-validation-6901", "mrqa_naturalquestions-train-73184", "mrqa_naturalquestions-train-47764", "mrqa_naturalquestions-train-15537", "mrqa_naturalquestions-train-39682", "mrqa_naturalquestions-train-71368", "mrqa_triviaqa-validation-1280", "mrqa_naturalquestions-train-396", "mrqa_naturalquestions-train-79014", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-train-87942", "mrqa_naturalquestions-train-39439", "mrqa_naturalquestions-train-51350", "mrqa_triviaqa-validation-1935", "mrqa_naturalquestions-train-17830", "mrqa_naturalquestions-train-13914", "mrqa_naturalquestions-train-1327", "mrqa_naturalquestions-train-55313", "mrqa_naturalquestions-train-18614", "mrqa_naturalquestions-train-58634", "mrqa_naturalquestions-train-62271", "mrqa_triviaqa-validation-3420", "mrqa_hotpotqa-validation-1032", "mrqa_naturalquestions-train-84103", "mrqa_naturalquestions-train-82997", "mrqa_naturalquestions-train-25755"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 31, "before_eval": {"predictions": ["Yolanda Sald\u00edvar", "Peter", "boston", "kenneth wolstenholme", "the crossroads of the Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide", "androids", "15 hands", "a shopping mall located in Bloomington, Minnesota, United States ( a suburb of the Twin Cities )", "Some civil disobedience defendants choose to make a defiant speech, or a speech explaining their actions, in allocution", "DreamWorks Animation", "zigeunerbaron", "his own men", "emissions resulting from human activities", "arthur", "the RAF", "reduce growth in relatively poor countries but encourage growth", "Ishar-damu", "peterborough", "Polish-Jewish", "the Carlists", "alexander porter", "popham", "390 billion", "Washington Street between Boylston Street and Kneeland Street", "May 10, 1976", "six", "lohan", "frustration with the atmosphere in the group at that time -- namely, Paul McCartney's over-assertiveness and criticism of his guitar playing, John Lennon's lack of engagement with the project and dismissal of Harrison as a songwriter", "boston", "John Smith", "lusitania", "the variety of occupations necessary to sustain the community as distinct from the indigenous population"], "metric_results": {"EM": 0.0625, "QA-F1": 0.15321176984779927}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.23529411764705882, 0.0, 0.0, 0.15384615384615385, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.45000000000000007, 0.0, 1.0, 1.0, 0.13333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-1385", "mrqa_hotpotqa-validation-5585", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-1050", "mrqa_hotpotqa-validation-2762", "mrqa_triviaqa-validation-457", "mrqa_triviaqa-validation-5071", "mrqa_naturalquestions-validation-5604", "mrqa_squad-validation-6734", "mrqa_hotpotqa-validation-2564", "mrqa_triviaqa-validation-3300", "mrqa_squad-validation-6128", "mrqa_squad-validation-8589", "mrqa_triviaqa-validation-5513", "mrqa_naturalquestions-validation-954", "mrqa_squad-validation-7469", "mrqa_hotpotqa-validation-1444", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-2493", "mrqa_naturalquestions-validation-9825", "mrqa_triviaqa-validation-3286", "mrqa_triviaqa-validation-2609", "mrqa_squad-validation-4415", "mrqa_hotpotqa-validation-3233", "mrqa_hotpotqa-validation-3728", "mrqa_squad-validation-932", "mrqa_hotpotqa-validation-5307", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-743", "mrqa_squad-validation-3106"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 32, "before_eval": {"predictions": ["an orbital scientific instrument package", "Orthodox Christians", "Coca-Cola", "beer", "gender testing", "Matt Jones", "k2CO3", "in order to halt it following brake failure", "T cell receptor", "relatively low salaries", "fruit", "Heading Out to the Highway", "Moonraker", "$12.99", "Michael Oppenheimer", "England national team", "poverty", "No Night Today", "Convention", "5,922", "December 5, 1991", "2016", "Philadelphia 76ers", "Saint Nicholas", "Stern-Plaza", "WBC/WBA heavyweight champion Joe Frazier", "23 March 1991", "Monday", "Dealey Plaza", "Nairobi", "last Ice Age", "Anno 2053"], "metric_results": {"EM": 0.25, "QA-F1": 0.36458333333333337}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-572", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3066", "mrqa_naturalquestions-validation-4761", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-5510", "mrqa_squad-validation-6602", "mrqa_triviaqa-validation-7156", "mrqa_hotpotqa-validation-1119", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2245", "mrqa_squad-validation-8617", "mrqa_hotpotqa-validation-305", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-10311", "mrqa_hotpotqa-validation-2910", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-2985", "mrqa_triviaqa-validation-1079", "mrqa_hotpotqa-validation-4221", "mrqa_hotpotqa-validation-3557", "mrqa_naturalquestions-validation-5960"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 33, "before_eval": {"predictions": ["After the defeat of Napoleon", "\"Boston Herald\" Rumor Clinic", "1967", "Guigrysh", "the twelfth most populous city in the United States", "50 home run club", "tintin", "a cascade of events through phosphorylation of intracellular proteins that ultimately transmit ( `` transduce '' ) the extracellular signal to the nucleus, causing changes in gene expression", "lower rates of social goods (life expectancy by country, educational performance, social mobility, even numbers of patents issued)", "Bass", "Tevye", "New Orleans, Biloxi, Mississippi, Mobile, Alabama and small settlements in the Illinois Country, hugging the east side of the Mississippi River and its tributaries", "Japan", "bridge", "Yunnan- Fu", "Mumbai, India", "Broken Hill and Sydney", "2005", "all punishments and granted them salvation", "\"The Doctor's Daughter\"", "cooper: a person who made or repaired candles", "cole peyre", "The project must adhere to zoning and building code requirements. Constructing a project that fails to adhere to codes does not benefit the owner", "1879", "daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia", "stay with the same group of peers for all classes", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "enthusiasm and energy", "pasternak", "passenger space and amenities such as air conditioning, power steering, AM-FM radios, and even power windows and central locking without increasing the price of the vehicle", "Bill Clinton's national intelligence officer for East Asia at the National Intelligence Council (NIC) from 1997- 1998", "Buskerud and Telemark"], "metric_results": {"EM": 0.125, "QA-F1": 0.25277046806585346}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.11428571428571427, 0.09999999999999999, 0.0, 0.0, 0.12121212121212123, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.06896551724137931, 0.0, 0.11764705882352941, 0.8888888888888888, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.5]}}, "error_ids": ["mrqa_squad-validation-9984", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-2092", "mrqa_squad-validation-7278", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-9271", "mrqa_squad-validation-7571", "mrqa_hotpotqa-validation-650", "mrqa_naturalquestions-validation-3523", "mrqa_squad-validation-10180", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-6499", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-5727", "mrqa_squad-validation-2010", "mrqa_squad-validation-7741", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-6186", "mrqa_squad-validation-6878", "mrqa_naturalquestions-validation-7387", "mrqa_hotpotqa-validation-2588", "mrqa_squad-validation-1903", "mrqa_naturalquestions-validation-5297", "mrqa_squad-validation-2147", "mrqa_triviaqa-validation-2812", "mrqa_squad-validation-3733", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-1211"], "retrieved_ids": ["mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-26", "mrqa_naturalquestions-train-49261", "mrqa_naturalquestions-train-71415", "mrqa_naturalquestions-train-80827", "mrqa_triviaqa-validation-2007", "mrqa_naturalquestions-train-19752", "mrqa_naturalquestions-train-51582", "mrqa_naturalquestions-train-42530", "mrqa_naturalquestions-train-39157", "mrqa_naturalquestions-train-70375", "mrqa_naturalquestions-train-23444", "mrqa_triviaqa-validation-3595", "mrqa_naturalquestions-train-34003", "mrqa_hotpotqa-validation-1855", "mrqa_triviaqa-validation-1306", "mrqa_naturalquestions-train-54770", "mrqa_naturalquestions-train-78190", "mrqa_naturalquestions-train-9081", "mrqa_naturalquestions-validation-4590", "mrqa_naturalquestions-train-42134", "mrqa_naturalquestions-train-24980", "mrqa_triviaqa-validation-444", "mrqa_hotpotqa-validation-5899", "mrqa_naturalquestions-train-85867", "mrqa_naturalquestions-train-52050", "mrqa_naturalquestions-train-43791", "mrqa_naturalquestions-train-31848", "mrqa_naturalquestions-train-20981", "mrqa_naturalquestions-train-24162", "mrqa_triviaqa-validation-1280", "mrqa_naturalquestions-train-6559"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 34, "before_eval": {"predictions": ["Speaker", "p pearls", "Threatening government officials", "Veronica", "Victorian College of the Arts", "Britain", "onion", "0.2 inhabitants per square kilometre", "James David Graham Niven", "France", "Ian Paisley", "Bataan Death March", "euro", "London", "the United States", "Sherry Rowland", "1886", "Sam Bradford", "Stanwyck's bedroom window overlooks the night skyline of Manhattan", "Russell Crowe", "pole", "Johnny Darrell", "a waxy substance called plaque", "a Belgian law requiring all margarine to be in cube shaped packages", "Euler's totient function", "ear", "binary strings", "the second-busiest airport in the United States by passenger volume (see World's busiest airports by passenger traffic) and the third by international passenger volume", "red", "Toyota Corona", "Kurt Vonnegut", "princess"], "metric_results": {"EM": 0.0625, "QA-F1": 0.14890318015318016}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.14285714285714288, 0.6153846153846153, 1.0, 0.0, 0.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42857142857142855, 0.6666666666666666, 0.4444444444444445, 0.0909090909090909, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2962", "mrqa_triviaqa-validation-7248", "mrqa_squad-validation-6673", "mrqa_naturalquestions-validation-6787", "mrqa_hotpotqa-validation-3982", "mrqa_triviaqa-validation-5516", "mrqa_squad-validation-4369", "mrqa_triviaqa-validation-5496", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-3072", "mrqa_triviaqa-validation-1220", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-5740", "mrqa_naturalquestions-validation-4366", "mrqa_hotpotqa-validation-3002", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4288", "mrqa_triviaqa-validation-7592", "mrqa_squad-validation-4255", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-3408", "mrqa_squad-validation-1635", "mrqa_squad-validation-2751", "mrqa_triviaqa-validation-7184", "mrqa_squad-validation-3708", "mrqa_squad-validation-8034", "mrqa_triviaqa-validation-1198"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 35, "before_eval": {"predictions": ["up to 2% higher than during outbreaks of 13- and 17-year cicadas", "based on the interplay of supply and demand, which determines the prices of goods and services", "Emma Watson, Dan Stevens, Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Nathan Mack, Ian McKellen, and Emma Thompson", "in the brain, muscles, and liver", "rings", "Washington Redskins", "in the courtyard adjoining the Assembly Hall", "William Howard Ashton", "an oil reserve near a rock formation north of Casper", "promoting social dislocation, unrest and conflict", "Broward County", "Song Kang-ho, Lee Byung-hun, and Jung Woo-sung", "changing display or audio settings quickly", "Battle of Marston Moor", "from the spectroscopic notation for the associated atomic orbitals : sharp, principal, diffuse and fundamental, and then g which follows f in the alphabet", "if the income share of the top 20 percent (the rich) increases", "Beauty and the Beast", "South Africa", "Scotty Grainger Jr.", "Alamo", "a seal illegally", "the UMC", "Brian Liesegang", "Don Hahn", "Port Moresby", "2000", "National Association for the Advancement of Colored People", "1963\u20131989", "largest", "2000", "2000", "6500 - 1500 BC"], "metric_results": {"EM": 0.1875, "QA-F1": 0.31486950549450554}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false], "QA-F1": [0.5, 0.11111111111111112, 0.14285714285714288, 0.33333333333333337, 0.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.4615384615384615, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-9123", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-7704", "mrqa_triviaqa-validation-4829", "mrqa_naturalquestions-validation-4544", "mrqa_squad-validation-9400", "mrqa_triviaqa-validation-6423", "mrqa_squad-validation-7352", "mrqa_hotpotqa-validation-4447", "mrqa_hotpotqa-validation-2947", "mrqa_naturalquestions-validation-1587", "mrqa_triviaqa-validation-3767", "mrqa_naturalquestions-validation-585", "mrqa_squad-validation-7495", "mrqa_hotpotqa-validation-1475", "mrqa_triviaqa-validation-2999", "mrqa_squad-validation-3408", "mrqa_hotpotqa-validation-3853", "mrqa_hotpotqa-validation-430", "mrqa_triviaqa-validation-11", "mrqa_hotpotqa-validation-1720", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-1812"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 36, "before_eval": {"predictions": ["National Broadcasting Company", "Kevin Costner, Amber Heard, Hailee Steinfeld, Connie Nielsen, Richard Sammel, and Eriq Ebouaney", "king George III of England", "family history", "Cobham\u2013Edmonds thesis", "often human, or humanoid aliens", "prefix", "March 2012", "jazz club", "Muhammad Ali", "Coldplay", "Menorca", "to civil disobedients", "Julius Caesar", "2", "28, 1979", "James Halliday who, when he died, had announced in his will to the public that he had left an Easter egg inside OASIS, and the first person to find it would inherit his entire fortune and the corporation", "decision problem", "king", "lungs", "Miasma theory", "pint of 20 imperial fluid ounces ( 568 ml ) or an American pint of 16 US fluid ounces", "mountain ranges", "white cross", "The U.S. state of Georgia is known as the `` Peach State '' due to its significant production of peaches as early as 1571, with exports to other states occurring around 1858.", "nettle", "$12", "flat rate", "best", "to build a nationwide network in the UK", "roughly west", "Sudan"], "metric_results": {"EM": 0.25, "QA-F1": 0.3531347901002506}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.6666666666666666, 0.041666666666666664, 0.4444444444444445, 0.0, 0.0, 0.0, 0.3157894736842105, 1.0, 0.6666666666666666, 0.13333333333333333, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-652", "mrqa_triviaqa-validation-4824", "mrqa_triviaqa-validation-37", "mrqa_squad-validation-7720", "mrqa_hotpotqa-validation-4427", "mrqa_hotpotqa-validation-3911", "mrqa_triviaqa-validation-3803", "mrqa_hotpotqa-validation-2936", "mrqa_squad-validation-6759", "mrqa_naturalquestions-validation-4115", "mrqa_hotpotqa-validation-1884", "mrqa_naturalquestions-validation-3993", "mrqa_squad-validation-1634", "mrqa_triviaqa-validation-5936", "mrqa_naturalquestions-validation-5552", "mrqa_squad-validation-4877", "mrqa_naturalquestions-validation-8525", "mrqa_triviaqa-validation-199", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-2323", "mrqa_triviaqa-validation-6290", "mrqa_squad-validation-4626", "mrqa_squad-validation-9146", "mrqa_hotpotqa-validation-452"], "retrieved_ids": ["mrqa_naturalquestions-train-61269", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-3286", "mrqa_naturalquestions-train-9021", "mrqa_naturalquestions-train-52895", "mrqa_naturalquestions-train-1172", "mrqa_naturalquestions-train-46161", "mrqa_squad-validation-6733", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-train-24947", "mrqa_naturalquestions-train-16149", "mrqa_naturalquestions-train-86685", "mrqa_triviaqa-validation-5516", "mrqa_naturalquestions-train-38724", "mrqa_naturalquestions-train-79942", "mrqa_squad-validation-6915", "mrqa_triviaqa-validation-639", "mrqa_naturalquestions-train-53606", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-train-62285", "mrqa_triviaqa-validation-2715", "mrqa_naturalquestions-train-65265", "mrqa_triviaqa-validation-4268", "mrqa_naturalquestions-train-34003", "mrqa_naturalquestions-train-33929", "mrqa_naturalquestions-train-22546", "mrqa_naturalquestions-train-5558", "mrqa_naturalquestions-train-58840", "mrqa_naturalquestions-train-83458", "mrqa_naturalquestions-train-9056", "mrqa_naturalquestions-train-49469"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 37, "before_eval": {"predictions": ["Union Pacific Railroad", "broken", "7 December 2000", "Post Alley under Pike Place Market", "mother-of-pearl made between 500 AD and 2000", "February 20, 1978", "stomach", "Walter Mondale", "96", "first recorded use of the term `` liberal arts '' ( artes liberales ) occurs in De Inventione by Marcus Tullius Cicero, but it is unclear if he created the term", "japan", "a bubble on a black background representing the circle with glossy gold letters", "on the alluvial plain", "37 \u00b0 9' 58.23\" latitude, around 11 miles (18 km) south of San Jose", "Spotty Dog", "Henry", "Carlos Tevez", "desert's animals", "many events and festivals", "riper grapes", "1991", "indon", "7 January 1936", "lifetime protection", "twenty-33", "Carl Sagan", "Much of the city's tax base dissipated", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Bj\u00f8rnsh\u00f8j Poulsen", "defiant speech", "whey", "Boston, Massachusetts"], "metric_results": {"EM": 0.3125, "QA-F1": 0.37796306022408965}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, true, true, false, false, false, true, true], "QA-F1": [0.28571428571428575, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.23529411764705882, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.13333333333333333, 0.0, 0.0, 1.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-4768", "mrqa_squad-validation-4108", "mrqa_hotpotqa-validation-104", "mrqa_squad-validation-5451", "mrqa_triviaqa-validation-1015", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-3479", "mrqa_squad-validation-5702", "mrqa_naturalquestions-validation-9058", "mrqa_squad-validation-2660", "mrqa_triviaqa-validation-5258", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-1698", "mrqa_squad-validation-677", "mrqa_triviaqa-validation-7134", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-3876", "mrqa_hotpotqa-validation-2377", "mrqa_naturalquestions-validation-1375", "mrqa_hotpotqa-validation-633", "mrqa_squad-validation-6737"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 38, "before_eval": {"predictions": ["the NP-complete knapsack problem", "Dan Stevens", "New England", "Etienne de Mestre", "dragon", "slavery, especially Southern political leaders'resistance to attempts by Northern antislavery political forces to block the expansion of slavery into the western territories", "American Indian allies", "a children's story published by John Newbery in London in 1765", "the longest rotation period ( 243 days ) of any planet in the Solar System and rotates in the opposite direction to most other planets ( meaning the Sun would rise in the west and set in the east )", "The platform is open to backers from anywhere in the world and to creators from the US, UK, Canada, Australia, New Zealand, The Netherlands, Denmark, Ireland, Norway, Sweden, Spain, France, Germany, Austria", "Eden", "commissioned officers are given a one - time stipend when commissioned to purchase their required uniform items. Officers then maintain proper fit and appearance of their uniform items throughout their career.", "Jeff Meldrum", "741", "Patricia Greene", "Shoshone, his mother tongue, and other western American Indian languages", "The Paris Sisters", "suez Canal", "60", "journalist", "the fact that there is no revising chamber", "florida", "the points of algebro-geometric objects, via the notion of the spectrum of a ring", "most of the items in the collection, unless those were newly accessioned into the collection, probably don't show up in the computer system", "not satisfy the criteria for a medium of exchange cited above", "strychnine", "Texas", "the early 16th century", "Lord's", "emelia sehan Shah", "Mosher", "in sequence with each heartbeat"], "metric_results": {"EM": 0.125, "QA-F1": 0.23156622057758164}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.5714285714285715, 0.11764705882352941, 0.0, 0.0, 1.0, 0.09523809523809523, 0.0, 0.0, 0.0606060606060606, 0.04878048780487805, 0.0, 0.12903225806451613, 0.25, 0.0, 0.0, 0.15384615384615383, 0.0, 1.0, 0.2857142857142857, 1.0, 0.6, 0.0, 0.0, 0.34782608695652173, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-1862", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-8418", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-10255", "mrqa_hotpotqa-validation-727", "mrqa_naturalquestions-validation-4067", "mrqa_hotpotqa-validation-1116", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-4762", "mrqa_hotpotqa-validation-2682", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-6210", "mrqa_squad-validation-9478", "mrqa_triviaqa-validation-1473", "mrqa_squad-validation-9032", "mrqa_squad-validation-5505", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-7630", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-2100", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2555"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 39, "before_eval": {"predictions": ["Republic of Taiwan", "Dan Conner", "Berlin", "Lee Harvey Oswald", "Katharine Hepburn, Joan Bennett, Frances Dee, and Jean Parker", "violence", "Joaquin Phoenix as Cash, Reese Witherspoon as Carter, Ginnifer Goodwin as Vivian Liberto, and Robert Patrick as Cash's father", "keskes", "1977", "Edwin Hubble, known for \"Hubble's Law\" NASA astronaut John M. Grunsfeld, geneticist James Watson, best known as one of the co-discoverers of the structure of DNA, experimental physicist Luis Alvarez", "New York City", "elton john", "2003", "elk", "Fabbrica Italiana Automobili Torino", "the second Sunday of March", "relative units of force and mass", "woman", "Porto", "August 10, 1933", "Golden Gate Bridge", "Sochi, Russia", "those who already hold wealth", "B. Traven", "Disney-Pixar film \" Finding Nemo\"", "the subject of unidentified flying objects (UFOs) the extraterrestrial hypothesis (ETH) as well as paranormal and Fortean subjects in general", "oil prices", "krause", "the city's population was 264,152", "Princeton, New Jersey", "the German Empire", "high pressure or an electric current"], "metric_results": {"EM": 0.28125, "QA-F1": 0.4224643176574416}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.4, 0.0, 0.8, 1.0, 0.9230769230769231, 0.0, 0.0, 0.19354838709677416, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.6666666666666666, 0.10526315789473684, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-1438", "mrqa_hotpotqa-validation-944", "mrqa_naturalquestions-validation-9227", "mrqa_triviaqa-validation-2522", "mrqa_naturalquestions-validation-6554", "mrqa_squad-validation-8070", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-7195", "mrqa_triviaqa-validation-3017", "mrqa_squad-validation-10428", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-4024", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-489", "mrqa_hotpotqa-validation-1607", "mrqa_triviaqa-validation-2808", "mrqa_triviaqa-validation-1276", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-2203", "mrqa_squad-validation-3650"], "retrieved_ids": ["mrqa_triviaqa-validation-6385", "mrqa_naturalquestions-train-20436", "mrqa_naturalquestions-train-80958", "mrqa_naturalquestions-train-45745", "mrqa_naturalquestions-train-47177", "mrqa_squad-validation-7352", "mrqa_naturalquestions-train-84830", "mrqa_naturalquestions-train-45778", "mrqa_naturalquestions-train-52202", "mrqa_hotpotqa-validation-1758", "mrqa_hotpotqa-validation-5", "mrqa_naturalquestions-train-29696", "mrqa_naturalquestions-train-24615", "mrqa_naturalquestions-train-2006", "mrqa_naturalquestions-train-65495", "mrqa_triviaqa-validation-1792", "mrqa_naturalquestions-train-2187", "mrqa_squad-validation-3442", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-5091", "mrqa_triviaqa-validation-2722", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-2802", "mrqa_naturalquestions-train-3454", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-365", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-5997", "mrqa_naturalquestions-train-54644", "mrqa_naturalquestions-train-33009"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 40, "before_eval": {"predictions": ["50 fund", "Bartle Frere", "Samarkand", "ice dancing", "Isabella (Belle) Baumfree", "Vulcan (dorgie) and Holly (corgi)", "every year between 1346 and 1671", "five starting pitchers, seven relief pitchers, two catchers, six infielders, and five outfielders", "The antichrist of 2 Thessalonians 2", "Bacon", "Charlton Heston", "anti-inflammatory molecules, such as cortisol and catecholamines", "vUHMOaD/JVI aiAQBAJA jaMOJOD", "Jeff Garcia", "money", "the force of law, if based on the authority derived from statute or the Constitution itself", "red fort", "the Karluk Kara-Khanid ruler", "Sochi, Russia", "left", "Hudson Bay", "After the determination of responsibility for the accident", "vida Goldstein", "shorthand typist", "160", "Secret Intelligence Service", "100 billion", "et tu, brute", "photolysis of ozone by light of short wavelength", "4.7 / 5.5 - inch", "Queen City", "whistleblowing"], "metric_results": {"EM": 0.25, "QA-F1": 0.3028359661172161}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.4, 0.0, 1.0, 0.33333333333333337, 0.0, 0.375, 0.0, 1.0, 0.0, 0.15384615384615383, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6212", "mrqa_squad-validation-6248", "mrqa_triviaqa-validation-1571", "mrqa_triviaqa-validation-2475", "mrqa_squad-validation-4953", "mrqa_naturalquestions-validation-8961", "mrqa_squad-validation-2249", "mrqa_naturalquestions-validation-7457", "mrqa_squad-validation-6588", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3213", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-1063", "mrqa_triviaqa-validation-6127", "mrqa_squad-validation-8247", "mrqa_triviaqa-validation-1859", "mrqa_squad-validation-3932", "mrqa_triviaqa-validation-3280", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1791", "mrqa_triviaqa-validation-1877", "mrqa_triviaqa-validation-579", "mrqa_naturalquestions-validation-6848", "mrqa_naturalquestions-validation-993"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 41, "before_eval": {"predictions": ["all war", "Keshto Mukherjee", "Gaels", "Three-card brag", "d\u00edsir", "European or Eurasian cave lion", "Russian film industry", "sediment load", "Washington metropolitan area", "GTPase responsible for endocytosis in the eukaryotic cell", "User State Migration Tool ( USMT )", "Ordos City China Science Flying Universe Science and Technology Co.", "where the frisbee first Flew", "PPG Paints Arena, Pittsburgh, Pennsylvania ( Host : Duquesne University )", "jerry wall museum", "Section 30 of the Teaching Council Act 2001", "Wilbur, a pig who was almost killed due to being a runt", "1984", "huge-LQG", "Retreating Monsoon", "Romansh", "Tudor king", "MIX 94.5", "Q Branch (or later Q Division) the fictional research and development division of the British Secret Service", "Philippi in Greece during Paul's second missionary journey, which occurred between approximately 49 and 51 AD", "what builds nations'wealth, and is today a fundamental work in classical economics", "Gerard Marenghi (born January 24, 1920) known as Jerry Maren", "bobby brown", "Nebula Award", "Conservative Party Conference", "David", "written by Hugo Peretti, Luigi Creatore, and George David Weiss"], "metric_results": {"EM": 0.09375, "QA-F1": 0.2525374798812299}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.1818181818181818, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.05, 0.0, 0.33333333333333337, 0.4, 0.4, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.4, 0.0, 0.125, 0.0, 0.26666666666666666, 0.7499999999999999, 1.0, 0.3636363636363636, 0.0, 0.0, 0.4615384615384615]}}, "error_ids": ["mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-2375", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-9712", "mrqa_hotpotqa-validation-201", "mrqa_triviaqa-validation-4955", "mrqa_naturalquestions-validation-3058", "mrqa_triviaqa-validation-684", "mrqa_squad-validation-2142", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-5007", "mrqa_triviaqa-validation-2384", "mrqa_naturalquestions-validation-774", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-2181", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-4823", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8338", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-434", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-4320", "mrqa_naturalquestions-validation-9763"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 42, "before_eval": {"predictions": ["david carradine", "cavatelli, acini di pepe, pastina, orzo, etc. ), lentils, or grated parmesan cheese", "boston", "independence from the Duke of Savoy through an alliance between the city-state of Geneva and the Swiss Confederation", "the Roentgen rays, but by the ozone generated in contact with the skin, and to a lesser extent, by nitrous acid", "questions about the name of the war, the tariff, states'rights and the nature of Abraham Lincoln's war goals", "physicians, lawyers, engineers, and accountants", "lack of remorse, an attempt to avoid responsibility for her actions, and even a likelihood of repeating her illegal actions", "Princess Maria", "second vice-captain", "jonathan", "the side - chain of the amino acid N - terminal to the scissile amide bond ( the P position )", "disaccharide sucrose", "their unusual behavior, such as the number of men killed and the manner of the attacks", "Gainsborough Trinity Football Club", "Paul Laxalt, Eleanor Roosevelt, William Randolph Hearst, \u201cWendy\ufffds\u201d founder Dave Thomas", "by functions ; Introverted Sensing ( Si ), Extroverted Thinking ( Te ), Introverted Feeling ( Fi ) and Extrovert Intuition ( Ne )", "Thursday", "yellow", "drug choice, dose, route, frequency, and duration of therapy", "jupiter", "navigator and expedition leader", "a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "julio estevez", "The Private Education Student Financial Assistance", "bow hair", "The benefits of good works could be obtained by donating money to the church", "Britain", "two forces, one pointing north, and one pointing east", "Bills", "Jack Murphy Stadium", "The answer to such questions is given by the time and space hierarchy theorems respectively"], "metric_results": {"EM": 0.09375, "QA-F1": 0.24689323646125116}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.7000000000000001, 0.0, 0.0, 0.8666666666666666, 0.11764705882352941, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.7222222222222222, 0.0, 0.375, 0.5, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.0, 0.13333333333333333, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2952", "mrqa_naturalquestions-validation-8284", "mrqa_triviaqa-validation-3261", "mrqa_squad-validation-3044", "mrqa_squad-validation-1429", "mrqa_naturalquestions-validation-9093", "mrqa_squad-validation-1841", "mrqa_squad-validation-6735", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-5731", "mrqa_triviaqa-validation-4137", "mrqa_naturalquestions-validation-7233", "mrqa_naturalquestions-validation-4185", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-680", "mrqa_naturalquestions-validation-6706", "mrqa_squad-validation-9569", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7133", "mrqa_hotpotqa-validation-4130", "mrqa_squad-validation-3385", "mrqa_triviaqa-validation-1586", "mrqa_triviaqa-validation-289", "mrqa_squad-validation-2150", "mrqa_squad-validation-9792", "mrqa_squad-validation-10395", "mrqa_hotpotqa-validation-5522", "mrqa_squad-validation-1808"], "retrieved_ids": ["mrqa_naturalquestions-train-66546", "mrqa_squad-validation-4369", "mrqa_naturalquestions-train-5004", "mrqa_naturalquestions-train-51238", "mrqa_naturalquestions-train-70899", "mrqa_naturalquestions-train-79425", "mrqa_naturalquestions-train-24119", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-3387", "mrqa_naturalquestions-train-51088", "mrqa_naturalquestions-train-19223", "mrqa_naturalquestions-train-61701", "mrqa_naturalquestions-train-293", "mrqa_naturalquestions-train-87196", "mrqa_naturalquestions-train-84147", "mrqa_naturalquestions-train-34711", "mrqa_naturalquestions-train-58577", "mrqa_naturalquestions-train-78237", "mrqa_triviaqa-validation-2959", "mrqa_naturalquestions-train-76529", "mrqa_hotpotqa-validation-1509", "mrqa_naturalquestions-train-75592", "mrqa_naturalquestions-train-34788", "mrqa_naturalquestions-train-68895", "mrqa_hotpotqa-validation-3080", "mrqa_triviaqa-validation-2959", "mrqa_naturalquestions-train-62864", "mrqa_naturalquestions-train-68447", "mrqa_naturalquestions-train-51426", "mrqa_squad-validation-6734", "mrqa_naturalquestions-train-49810", "mrqa_naturalquestions-train-63918"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 43, "before_eval": {"predictions": ["American Falls", "letters", "Indiana", "Salt Lake", "French", "a \"homeward bounder\") a sailor coming home from a round trip", "domain name www.example.com", "a genetic disease such as severe combined immunodeficiency, acquired conditions such as HIV/AIDS, or the use of immunosuppressive medication", "Les Huguenots", "natural-ing recipes-only personal care products", "spider", "Rigoletto", "country on Earth by land area", "third-most", "furniture", "216", "south koreans", "Nicholas Stone", "Akon, T.I. Rick Ross, Fat Joe, Birdman and Lil Wayne", "the Outfield", "porec", "Michael Edwards ( briefly as the older Connor ) and then by teenage actor Edward Furlong throughout the remainder of the film", "railway locomotives", "eddie Fisher", "third quarter ( also known as last quarter )", "bresslaw", "chemists Glenn T. Seaborg", "Virginia", "many areas of technology incidental to rocketry and manned spaceflight", "africa", "about 615 square kilometers", "egypt"], "metric_results": {"EM": 0.0625, "QA-F1": 0.22723344820384295}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.5, 0.0, 1.0, 0.8, 0.0, 0.2222222222222222, 0.3333333333333333, 0.21052631578947367, 0.0, 0.6666666666666665, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.21052631578947367, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.4, 0.4210526315789474, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7744", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-6916", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-981", "mrqa_naturalquestions-validation-2663", "mrqa_squad-validation-6442", "mrqa_triviaqa-validation-5580", "mrqa_hotpotqa-validation-518", "mrqa_triviaqa-validation-5087", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-850", "mrqa_triviaqa-validation-4090", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-5160", "mrqa_squad-validation-5586", "mrqa_hotpotqa-validation-5370", "mrqa_naturalquestions-validation-5822", "mrqa_triviaqa-validation-7482", "mrqa_naturalquestions-validation-7641", "mrqa_squad-validation-3330", "mrqa_triviaqa-validation-2803", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-7215", "mrqa_squad-validation-8054", "mrqa_hotpotqa-validation-4624", "mrqa_squad-validation-3812", "mrqa_triviaqa-validation-5899", "mrqa_hotpotqa-validation-5541", "mrqa_triviaqa-validation-305"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 44, "before_eval": {"predictions": ["Wakanda", "income", "England and Wales Cricket Board ( ECB )", "London", "football", "campaign setting", "the `` Fourth Revised Edition '' ISBN 0 - 06 - 015547 - 7", "867 feet", "the Hebrew name Immanu'el ( \u05e2\u05b4\u05de\u05b8\u05bc\u05e0\u05d5\u05bc\u05d0\u05b5\u05dc \u202c, which means `` God with us. '' )", "Shape of You", "Christopher Lee as Count Dooku / Darth Tyranus", "8th", "Tikki Tikki Tembo", "physicians and other healthcare professionals", "increased patient health outcomes and decreased costs to the health care system", "Every Good Boy Deserves Favour", "Gabriel Alberto Azucena", "150 km / h", "between the Piazza di Spagna at the base and Piazzo Trinit\u00e0 dei Monti", "December 1, 2009", "an activist in the cause of anti-fascism", "eu", "editor of the works of Francis Bacon", "margaret", "Ministry of Corporate Affairs", "Irish", "ancient cult activity", "Penguin Classics", "oxygen", "Hubble Space Telescope", "love of our neighbors as ourselves", "chorale cantatas"], "metric_results": {"EM": 0.125, "QA-F1": 0.23686417748917749}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true], "QA-F1": [0.2857142857142857, 0.0, 0.4, 0.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 0.7272727272727273, 0.0, 0.0, 0.0, 0.39999999999999997, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6015", "mrqa_squad-validation-2236", "mrqa_naturalquestions-validation-2119", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-6579", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-910", "mrqa_naturalquestions-validation-10612", "mrqa_hotpotqa-validation-4649", "mrqa_naturalquestions-validation-5550", "mrqa_squad-validation-7067", "mrqa_triviaqa-validation-3664", "mrqa_squad-validation-6319", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-2406", "mrqa_hotpotqa-validation-4278", "mrqa_naturalquestions-validation-3459", "mrqa_naturalquestions-validation-8491", "mrqa_naturalquestions-validation-2169", "mrqa_hotpotqa-validation-4794", "mrqa_triviaqa-validation-5022", "mrqa_hotpotqa-validation-389", "mrqa_triviaqa-validation-3617", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-1725", "mrqa_triviaqa-validation-4027", "mrqa_squad-validation-8625", "mrqa_squad-validation-9951"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 45, "before_eval": {"predictions": ["Mickey Spillane", "Detroit Lions", "perique", "his left leg was cut off close by the hip, and under the left shoulder, he carried a crutch, which he managed with wonderful dexterity, hopping about upon it like a bird", "death penalty", "stout man with a \" Double chin, strong mouth, piercing deep-set eyes, fleshy face, and squat neck\"", "afulhas", "belfast king", "Mangal Pandey of the 34th BNI", "Ubiorum", "Hoss", "four of the 50 states of the United States in their full official state names", "curling", "the eighth series", "Carmel and Pacific Grove", "Los Angeles", "French", "Gareth", "LOVE Radio", "Miami Marlins", "the court from its members for a three - year term", "john travolta", "David Dobkin", "brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil to put an end to Saddam Hussein's occupation of Kuwait", "The Zombies", "Fox News Specialists", "usually scheduled for the Thursday following Labor Day and since 2004, it was hosted by the most recent Super Bowl champions", "Lunar Excursion Module (LEM, later shortened to Lunar Module, LM)", "San Francisco Bay Area at Santa Clara, California", "official residence of the President of the Russian Federation", "Operation Neptune", "Mediterranean Sea"], "metric_results": {"EM": 0.21875, "QA-F1": 0.2903921627207917}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.06451612903225806, 1.0, 0.0, 0.0, 0.0, 0.7692307692307693, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 0.07142857142857142, 0.0, 0.0, 0.4166666666666667, 0.16666666666666669, 0.4, 0.18181818181818182, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-2201", "mrqa_naturalquestions-validation-4123", "mrqa_squad-validation-2598", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-5587", "mrqa_naturalquestions-validation-4097", "mrqa_triviaqa-validation-6956", "mrqa_naturalquestions-validation-8728", "mrqa_triviaqa-validation-3335", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3363", "mrqa_hotpotqa-validation-5068", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-5149", "mrqa_naturalquestions-validation-9931", "mrqa_triviaqa-validation-455", "mrqa_squad-validation-9513", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-3509", "mrqa_naturalquestions-validation-3217", "mrqa_squad-validation-3845", "mrqa_squad-validation-13", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-2067"], "retrieved_ids": ["mrqa_naturalquestions-train-8443", "mrqa_naturalquestions-train-53290", "mrqa_naturalquestions-train-6506", "mrqa_naturalquestions-train-25583", "mrqa_naturalquestions-train-7251", "mrqa_naturalquestions-train-34624", "mrqa_naturalquestions-train-16813", "mrqa_naturalquestions-train-47056", "mrqa_naturalquestions-train-52667", "mrqa_naturalquestions-train-10617", "mrqa_naturalquestions-train-21283", "mrqa_naturalquestions-train-7055", "mrqa_naturalquestions-train-51350", "mrqa_hotpotqa-validation-5526", "mrqa_naturalquestions-validation-3209", "mrqa_naturalquestions-train-76561", "mrqa_naturalquestions-train-8824", "mrqa_naturalquestions-train-41055", "mrqa_naturalquestions-train-24615", "mrqa_triviaqa-validation-1616", "mrqa_naturalquestions-train-61034", "mrqa_squad-validation-7836", "mrqa_naturalquestions-train-60865", "mrqa_naturalquestions-train-69635", "mrqa_naturalquestions-train-64528", "mrqa_naturalquestions-train-83407", "mrqa_naturalquestions-train-58088", "mrqa_naturalquestions-train-37249", "mrqa_naturalquestions-train-34", "mrqa_hotpotqa-validation-2403", "mrqa_naturalquestions-train-36180", "mrqa_naturalquestions-train-12228"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 46, "before_eval": {"predictions": ["bat-and-ball", "convection currents in the asthenosphere, which is ductile, or plastic, and the brittle lithosphere ( crust and upper mantle )", "Take That, East 17 and Boyzone", "youngest", "electric lighting", "their knowledge of Native American languages as a basis to transmit coded messages", "Einstein", "the absenceistence of the ultraviolet catastrophe", "Premier League club Swansea City", "Pre-Raphaelites", "Elizabeth Weber", "It was released for PlayStation 4 and Xbox One on May 3, 2016.", "hundreds", "The Nightmare Before Christmas", "1998", "a new facility on The Watermark business park next to the MetroCentre in Gateshead", "pearmain", "partial funding", "5% abv draught beer", "inefficient", "Chu'Tsai", "Liz", "least onerous", "belfast", "Grissom, White, and Chaffee", "an American multinational retail corporation that operates a chain of hypermarkets, discount department stores, and grocery stores", "purple passion fruit", "Bharata Muni", "high iron concentration which oxidizes upon exposure to the air", "golf", "parts of the air in the vessel were converted into the classical element fire and thus were able to escape through pores in the glass", "the Emperor of Austria"], "metric_results": {"EM": 0.25, "QA-F1": 0.3656968390804598}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.5, 0.25, 1.0, 0.06896551724137931, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.42857142857142855, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.8, 0.0, 0.7499999999999999, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_naturalquestions-validation-8204", "mrqa_triviaqa-validation-6905", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-5352", "mrqa_squad-validation-10489", "mrqa_hotpotqa-validation-1831", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-3284", "mrqa_hotpotqa-validation-1074", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-73", "mrqa_squad-validation-5464", "mrqa_triviaqa-validation-7350", "mrqa_hotpotqa-validation-5239", "mrqa_naturalquestions-validation-2890", "mrqa_triviaqa-validation-2914", "mrqa_naturalquestions-validation-3616", "mrqa_triviaqa-validation-1128", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-2808", "mrqa_triviaqa-validation-2873", "mrqa_squad-validation-3523", "mrqa_triviaqa-validation-4430"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 47, "before_eval": {"predictions": ["florida", "horse racing", "Burnley and the New Zealand national team", "at the'Lord's Enclosure' (Mongolian: Edsen Khoroo) in Mongolia", "Styal Mill", "William Jennings Bryan", "Milk Barn Animation", "when they enter the army during initial entry training", "moral tale", "quit", "leeds", "star ( representing either the Star of Bethlehem or the star of David ), finials, or fairies", "260", "heathrow", "often social communities with considerable face-to-face interaction among members", "William Strauss and Neil Howe", "monophyletic", "insecticide toxicology", "candidates on specific catechism questions", "a pH indicator, a color marker, and a dye", "2.5 times the normal sea-level O2 partial pressure of about 21 kPa", "63,182,000", "George Whitefield", "Science and Discovery", "Euclid's fundamental theorem of arithmetic", "Western Kentucky University", "rolf w\u00fctherich", "appearing as Jude in the musical romance drama film \" Across the Universe\" (2007)", "maryland", "her arranged marriage to Chino, a friend of Bernardo's", "XXXTentacion", "stagnant wages for the working class amidst rising levels of property income for the capitalist class"], "metric_results": {"EM": 0.28125, "QA-F1": 0.44398032617463196}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.5, 0.8, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.19999999999999998, 1.0, 0.896551724137931, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7499999999999999, 0.5, 0.2222222222222222, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.16666666666666669, 0.0, 0.0, 1.0, 0.10526315789473682]}}, "error_ids": ["mrqa_triviaqa-validation-945", "mrqa_hotpotqa-validation-1924", "mrqa_squad-validation-6287", "mrqa_hotpotqa-validation-5877", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2864", "mrqa_naturalquestions-validation-5305", "mrqa_triviaqa-validation-1516", "mrqa_hotpotqa-validation-5086", "mrqa_naturalquestions-validation-8582", "mrqa_squad-validation-4479", "mrqa_triviaqa-validation-3868", "mrqa_squad-validation-2346", "mrqa_naturalquestions-validation-7849", "mrqa_squad-validation-3685", "mrqa_triviaqa-validation-4298", "mrqa_squad-validation-9061", "mrqa_hotpotqa-validation-662", "mrqa_triviaqa-validation-1799", "mrqa_hotpotqa-validation-4173", "mrqa_triviaqa-validation-7477", "mrqa_naturalquestions-validation-5241", "mrqa_squad-validation-7182"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 48, "before_eval": {"predictions": ["James Stenbeck", "Good Kid, M.A.D City", "Yosemite", "Interventive treatment", "3", "Lloyd Christ Wicke", "ry Charles", "During his epic battle with Frieza", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Social Democratic Party", "vienna", "Thon Maker", "a loop ( also called a self - loop or a `` buckle '' )", "calligraphy", "not given at the 1964 Republican National Convention in San Francisco, California as a nomination speech for presidential candidate Senator Barry Goldwater", "every good work", "annuity", "twin sister", "Buffalo Bill", "no place where justice resides", "the Viet Minh", "It insisted on its neutral rights, which included allowing private corporations and banks to sell or loan money to either side", "dry", "halal meat", "Hecuba", "pastors and teachers", "Wylie Draper", "separate religion from politics", "students occupied President George Beadle's office in a protest over the university's off-campus rental policies", "Bobby Hull and Dennis Hull", "New England Patriots", "famine"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3195841637937226}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 1.0, 0.22222222222222218, 0.0, 0.0, 1.0, 0.25, 0.2857142857142857, 0.0, 0.0, 0.16666666666666669, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2666666666666667, 0.0, 0.47058823529411764, 0.3076923076923077, 0.0, 0.4]}}, "error_ids": ["mrqa_hotpotqa-validation-5165", "mrqa_squad-validation-5665", "mrqa_squad-validation-9860", "mrqa_triviaqa-validation-3906", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-2607", "mrqa_triviaqa-validation-2849", "mrqa_naturalquestions-validation-2445", "mrqa_squad-validation-8248", "mrqa_naturalquestions-validation-2582", "mrqa_squad-validation-2259", "mrqa_naturalquestions-validation-3789", "mrqa_hotpotqa-validation-4075", "mrqa_squad-validation-2263", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-1649", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-4384", "mrqa_hotpotqa-validation-962", "mrqa_squad-validation-2337", "mrqa_naturalquestions-validation-1531", "mrqa_squad-validation-9518", "mrqa_squad-validation-7947", "mrqa_hotpotqa-validation-599", "mrqa_squad-validation-264", "mrqa_squad-validation-4774"], "retrieved_ids": ["mrqa_naturalquestions-train-28007", "mrqa_naturalquestions-train-12896", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-8896", "mrqa_naturalquestions-train-73356", "mrqa_naturalquestions-train-16730", "mrqa_naturalquestions-train-3911", "mrqa_naturalquestions-train-36855", "mrqa_hotpotqa-validation-5637", "mrqa_naturalquestions-train-58577", "mrqa_naturalquestions-train-1948", "mrqa_triviaqa-validation-3420", "mrqa_naturalquestions-train-22759", "mrqa_triviaqa-validation-2181", "mrqa_naturalquestions-train-87585", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-train-29696", "mrqa_triviaqa-validation-5160", "mrqa_naturalquestions-train-81426", "mrqa_naturalquestions-train-21199", "mrqa_triviaqa-validation-1276", "mrqa_naturalquestions-train-45584", "mrqa_naturalquestions-train-4972", "mrqa_hotpotqa-validation-1758", "mrqa_naturalquestions-train-2559", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-80729", "mrqa_naturalquestions-train-811", "mrqa_triviaqa-validation-3515", "mrqa_naturalquestions-train-18261", "mrqa_naturalquestions-train-70489", "mrqa_naturalquestions-train-83407"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 49, "before_eval": {"predictions": ["vehicles inspired by the Jeep that are suitable for use on rough terrain", "Aol", "Attar of Nishapur", "\"Losing My Religion\"", "between the Eastern Ghats and the Bay of Bengal", "Rav\u00e8na", "12", "georgia state", "1937", "improved", "a biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight until it was flying like a conventional plane", "he had assigned them to the company in lieu of stock", "Marxist and a Leninist", "variation in plants", "abortion, broadcasting policy, civil service, common markets for UK goods and services, constitution, electricity, coal, oil, gas, nuclear energy, defence and national security, drug policy, employment, foreign policy and relations with Europe,", "Philadelphia Eagles, Denver Broncos, Berlin Thunder, Las Vegas Outlaws and Ottawa Renegades", "3,600", "State Street", "al - Mamlakah al - \u02bbArab\u012byah as - Su\u02bb\u016bd\u012byah", "110", "george cukor", "horakhty", "Lawton Mainor Chiles Jr.", "Colonel Tigh, who has taken command of Galactica during Adama's incapacity", "Wisconsin v. Yoder", "uneven trade agreements", "chakula cha jioni", "ATP energy", "georgia state", "Ruth Elizabeth \"Bette\" Davis", "georgium", "7 December 2004"], "metric_results": {"EM": 0.125, "QA-F1": 0.2303848928848929}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.13333333333333333, 0.0, 0.0, 1.0, 0.18181818181818182, 0.15384615384615385, 0.2222222222222222, 0.0, 0.0, 0.6666666666666666, 0.3636363636363636, 0.888888888888889, 0.0, 0.0, 0.0, 0.09523809523809525, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2272", "mrqa_hotpotqa-validation-5452", "mrqa_squad-validation-6257", "mrqa_naturalquestions-validation-2275", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2941", "mrqa_triviaqa-validation-6073", "mrqa_hotpotqa-validation-987", "mrqa_squad-validation-716", "mrqa_squad-validation-1459", "mrqa_squad-validation-1306", "mrqa_hotpotqa-validation-2810", "mrqa_naturalquestions-validation-3663", "mrqa_squad-validation-9489", "mrqa_hotpotqa-validation-3964", "mrqa_squad-validation-10291", "mrqa_naturalquestions-validation-9013", "mrqa_triviaqa-validation-3250", "mrqa_triviaqa-validation-6475", "mrqa_hotpotqa-validation-1315", "mrqa_naturalquestions-validation-5283", "mrqa_squad-validation-7049", "mrqa_squad-validation-9807", "mrqa_squad-validation-8518", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-5876", "mrqa_triviaqa-validation-6380", "mrqa_hotpotqa-validation-2750"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 50, "before_eval": {"predictions": ["basketball", "Igor Stravinsky, Carl Orff, Paul Hindemith, Richard Strauss, Luigi Nono, Krzysztof Penderecki and Joaqu\u00edn Rodrigo", "minced meat ( commonly beef when named cottage pie or lamb when named shepherd's pie ), typically cooked in a gravy with onions and sometimes other vegetables, such as peas, celery or carrots, and topped with mashed potato", "Gatiman express", "The Rock", "Taylor Swift", "The issues of conflicting territorial claims between British and French colonies in North America were turned over to a commission to resolve", "Valmiki", "Epistle to Ramsay", "every two to six years ( depending on the positions being filled with most positions good for four years )", "Chinese", "Arsenal supporters", "2016", "Dan Castellaneta", "2007", "Wicked Twister", "multiplication, subtraction, and division", "R-7 Booster Rocket, Sputnik, and launching Laika and the first human being into space", "shoe", "unclear as to how or whether this connection is relevant on microscales", "supernatural psychological horror", "House of Representatives", "Leo\u2019s Butt", "evening", "Blue (Da Ba Dee\") is a song by the Italian music group Eiffel 65.", "unesco", "Faurot Field", "Dennis C. Stewart as Leo `` Craterface '' Balmudo, head of the Scorpions, a rival greaser gang", "the `` sandbar '' between river of life, with its outgoing `` flood '', and the ocean that lies beyond ( death ), the `` boundless deep '', to which we return.", "5", "kahramanmara\u015f", "If the car is slowed initially by manual use of the automatic gear box"], "metric_results": {"EM": 0.125, "QA-F1": 0.26657758596903336}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.4210526315789474, 0.1081081081081081, 0.6666666666666666, 0.0, 0.0, 0.3076923076923077, 0.2857142857142857, 0.0, 0.45454545454545453, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.14285714285714288, 0.0, 0.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.15384615384615385, 0.0, 1.0, 0.0, 0.8400000000000001, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4225", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-3395", "mrqa_triviaqa-validation-813", "mrqa_hotpotqa-validation-5623", "mrqa_squad-validation-10171", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-1607", "mrqa_naturalquestions-validation-7707", "mrqa_squad-validation-8134", "mrqa_hotpotqa-validation-1528", "mrqa_naturalquestions-validation-7812", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-118", "mrqa_triviaqa-validation-290", "mrqa_hotpotqa-validation-1350", "mrqa_triviaqa-validation-3428", "mrqa_squad-validation-10427", "mrqa_hotpotqa-validation-304", "mrqa_naturalquestions-validation-6782", "mrqa_triviaqa-validation-3572", "mrqa_hotpotqa-validation-2635", "mrqa_triviaqa-validation-7770", "mrqa_naturalquestions-validation-1008", "mrqa_naturalquestions-validation-71", "mrqa_squad-validation-290", "mrqa_triviaqa-validation-239", "mrqa_naturalquestions-validation-3022"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 51, "before_eval": {"predictions": ["third season", "the base of the right ventricle", "a card from a pack of playing cards by Alice", "Riverside", "Alex Breckenridge as Monique Valentine, Sebastian's superficial girlfriend", "seven", "Schr\u00f6dinger equation", "meat", "Ashland is home to Scribner-Fellows State Forest", "public schools", "ghostface mask", "Roger Thomas Staubach", "AC induction motor and transformer", "Eric Morecambe", "rock band Queens of the Stone Age", "after the conquest of Mexico, cocoa as a commodity travelled by boat from the port of Nueva Espa\u00f1a to the Spanish coast", "Robert John Day", "1775\u20131795", "Empiricism", "w Somerset maugham", "Miller Brewing", "redistributive while the latter is expected to foster technological progress and thus have a more positive impact on economic growth", "Peter Dinklage", "ozzie owl", "Content is `` something that is to be expressed through some medium, as speech, writing or any of various arts ''", "Saint Peter ( the keeper of the `` keys to the kingdom '' )", "Fourth Home Rule Bill", "nguy\u00ean \u00d0\u00e1n", "Gebhard v Consiglio dell\u2019 Ordine degli Avvocati e Procuratori di Milano", "Ukraine ( ; Ukrainian: \u0423\u043a\u0440\u0430\u0457\u043d\u0430, \" Ukrajina\") sometimes called the Ukraine, is a sovereign state in Eastern Europe, bordered by Russia to the east and northeast, Belarus", "1835", "Queen Queen, Gloriana or Good Queen Bess"], "metric_results": {"EM": 0.15625, "QA-F1": 0.3144740113490114}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.6666666666666666, 0.4324324324324324, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6, 0.33333333333333337, 0.2666666666666667, 0.5, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 0.3636363636363636, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.14285714285714288, 0.0909090909090909, 0.4, 0.22222222222222224]}}, "error_ids": ["mrqa_hotpotqa-validation-4919", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4006", "mrqa_naturalquestions-validation-6634", "mrqa_squad-validation-10386", "mrqa_triviaqa-validation-7398", "mrqa_squad-validation-1932", "mrqa_triviaqa-validation-787", "mrqa_hotpotqa-validation-4795", "mrqa_squad-validation-1185", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-4922", "mrqa_naturalquestions-validation-7484", "mrqa_hotpotqa-validation-4528", "mrqa_naturalquestions-validation-7312", "mrqa_triviaqa-validation-4890", "mrqa_squad-validation-7324", "mrqa_naturalquestions-validation-5370", "mrqa_triviaqa-validation-1046", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-91", "mrqa_hotpotqa-validation-2549", "mrqa_triviaqa-validation-3980", "mrqa_squad-validation-4430", "mrqa_hotpotqa-validation-3033", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3357"], "retrieved_ids": ["mrqa_naturalquestions-train-60918", "mrqa_naturalquestions-train-70320", "mrqa_naturalquestions-train-88101", "mrqa_naturalquestions-train-27576", "mrqa_naturalquestions-train-37625", "mrqa_naturalquestions-train-65378", "mrqa_naturalquestions-train-44171", "mrqa_naturalquestions-train-65216", "mrqa_naturalquestions-train-41621", "mrqa_naturalquestions-train-81061", "mrqa_triviaqa-validation-7578", "mrqa_naturalquestions-train-18324", "mrqa_naturalquestions-train-22930", "mrqa_naturalquestions-train-53977", "mrqa_naturalquestions-train-12721", "mrqa_naturalquestions-train-25190", "mrqa_triviaqa-validation-3280", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-train-79942", "mrqa_naturalquestions-train-58745", "mrqa_naturalquestions-train-2860", "mrqa_naturalquestions-train-11738", "mrqa_naturalquestions-train-74768", "mrqa_naturalquestions-train-27538", "mrqa_naturalquestions-train-37392", "mrqa_triviaqa-validation-313", "mrqa_naturalquestions-train-47066", "mrqa_naturalquestions-train-77664", "mrqa_naturalquestions-train-74764", "mrqa_naturalquestions-train-82478", "mrqa_squad-validation-7836", "mrqa_hotpotqa-validation-1048"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 52, "before_eval": {"predictions": ["Myllokunmingia", "Edd Kimber, Joanne Wheatley, John Whaite, Frances Quinn, Nancy Birtwhistle, Nadiya Hussain, Candice Brown and Sophie Faldo", "wednesday", "arbitrary integers", "`` asphyxia '' ( cutting off the oxygen supply ) and cooling", "Les Paul", "Carey Mulligan, Matthias Schoenaerts, Michael Sheen, Tom Sturridge and Juno Temple", "The Worm", "16 December 1908", "five community cards", "German hymns", "Bury Football Club is a professional association football club based in Bury, Greater Manchester, England.", "Fan S. Noli", "The Thing of It Is... is a 1967 novel written by William Goldman about Amos McCracken, a 31-year-old man who has written a popular show tune and who is having marriage troubles.", "NFC Championship Game", "Tom Robinson", "Macaulay Culkin", "2001", "the port city of Aden, on the southern coast", "quickly to meet the needs of major national and international patient information projects and health system interoperability goals", "about two-thirds the size of cytoplasmic ribosomes (around 17 nm vs 25 nm)", "leopard", "service sector", "Florida", "\"\"la f\u00e9e verte\" ( the green fairy) beverage", "wednesdays", "Bolton", "British", "the state recognizes no limits to its authority and strives to regulate every aspect of public and private life wherever feasible", "anarchists", "Just under 540,800 students", "British Sky Broadcasting Group plc"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3424674801319538}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false], "QA-F1": [0.4, 0.21052631578947367, 0.0, 1.0, 0.25, 1.0, 0.3076923076923077, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.25, 0.1111111111111111, 0.4, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.5185185185185185, 1.0, 0.4, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6188", "mrqa_naturalquestions-validation-8228", "mrqa_triviaqa-validation-4073", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-7688", "mrqa_naturalquestions-validation-1186", "mrqa_hotpotqa-validation-5318", "mrqa_squad-validation-2401", "mrqa_hotpotqa-validation-5482", "mrqa_squad-validation-7240", "mrqa_hotpotqa-validation-182", "mrqa_squad-validation-308", "mrqa_triviaqa-validation-498", "mrqa_naturalquestions-validation-8759", "mrqa_hotpotqa-validation-1871", "mrqa_squad-validation-6389", "mrqa_squad-validation-8849", "mrqa_hotpotqa-validation-1504", "mrqa_squad-validation-7377", "mrqa_hotpotqa-validation-1402", "mrqa_triviaqa-validation-5068", "mrqa_hotpotqa-validation-3044", "mrqa_hotpotqa-validation-1901", "mrqa_hotpotqa-validation-904", "mrqa_squad-validation-2918", "mrqa_squad-validation-2772"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 53, "before_eval": {"predictions": ["Following the election of the UK Labour Party to government in 1997", "two", "c 1600", "Professor Moriarty", "Atticus Finch", "1763", "lambic ales", "~74,000", "catherine o'Leary", "Bronwyn Kathleen Bishop (n\u00e9e Setright", "guidance and intervention", "the queen of spades", "jayhawker", "domination or control by a group of people over another", "a violation of nature and the resulting psychological effects on the mariner and on all those who hear him", "jomo kyatta", "John Gielgud", "Adam Smith", "25 June 1932", "gaspare Ghiretti", "James Abram Garfield", "stop motion animation", "chromosome", "Evey's mother", "10", "things that are indisputably bad", "Joudeh Al - Goudia family", "Wes Unseld", "1936", "nitrogen dioxide", "increased flooding and sedimentation", "Daniel Handler"], "metric_results": {"EM": 0.28125, "QA-F1": 0.393148682457893}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false, true, false], "QA-F1": [0.18181818181818182, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3157894736842105, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.7692307692307693, 0.28571428571428575, 0.4, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-4100", "mrqa_squad-validation-5390", "mrqa_squad-validation-7700", "mrqa_triviaqa-validation-776", "mrqa_squad-validation-9144", "mrqa_triviaqa-validation-7626", "mrqa_hotpotqa-validation-123", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-3014", "mrqa_squad-validation-9808", "mrqa_naturalquestions-validation-1161", "mrqa_triviaqa-validation-595", "mrqa_triviaqa-validation-6410", "mrqa_naturalquestions-validation-6485", "mrqa_triviaqa-validation-2821", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-117", "mrqa_squad-validation-6877", "mrqa_naturalquestions-validation-678", "mrqa_hotpotqa-validation-5614", "mrqa_naturalquestions-validation-6970", "mrqa_triviaqa-validation-177", "mrqa_triviaqa-validation-3268"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 54, "before_eval": {"predictions": ["The Christmas Invasion", "warkeeping force", "an expression of Priestley's socialist political principles", "March 9 to 18", "October 2016", "Romancing the Stone", "the seventh cranial nerve", "The 8th Habit", "Anishinaabeg", "general medical advice and a range of services that are now performed solely by other specialist practitioners, such as surgery and midwifery", "warm", "pierowall", "foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world", "22 September 2015", "\"Murder Request\") is a 2015 South Korean crime thriller film directed by Son Yong-ho", "Akhilleus", "TNT", "annually in late January or early February", "Corinna and seven-time Formula One World Champion Michael Schumacher", "the duodenum", "pastry", "Cuyler Reynolds", "Lacoste, France", "american beer", "brenda county cricket club", "Black Ravens", "saved something from the disaster when he sent John Bradstreet on an expedition that successfully destroyed Fort Frontenac", "various games typically found within a casino, ranging from card to slot machines", "Americana Manhasset", "\"Shoot Straight from Your Heart\"", "Jane Austen", "Kony Ealy"], "metric_results": {"EM": 0.1875, "QA-F1": 0.27056559315039447}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.13333333333333333, 0.7272727272727273, 0.0, 0.2666666666666667, 0.0, 0.0, 0.0, 0.08695652173913042, 0.0, 0.0, 0.4827586206896552, 1.0, 0.2666666666666667, 0.0, 0.0, 0.4444444444444444, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6980", "mrqa_naturalquestions-validation-5212", "mrqa_naturalquestions-validation-1383", "mrqa_hotpotqa-validation-4277", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-2684", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-525", "mrqa_squad-validation-6211", "mrqa_naturalquestions-validation-4960", "mrqa_triviaqa-validation-5166", "mrqa_naturalquestions-validation-4021", "mrqa_hotpotqa-validation-5109", "mrqa_triviaqa-validation-1396", "mrqa_triviaqa-validation-2356", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-5497", "mrqa_triviaqa-validation-1707", "mrqa_triviaqa-validation-6373", "mrqa_triviaqa-validation-5057", "mrqa_squad-validation-10293", "mrqa_hotpotqa-validation-1606", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-3710", "mrqa_triviaqa-validation-6164", "mrqa_squad-validation-979"], "retrieved_ids": ["mrqa_naturalquestions-train-24436", "mrqa_naturalquestions-train-22243", "mrqa_squad-validation-7836", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-validation-10676", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-6956", "mrqa_naturalquestions-train-45121", "mrqa_naturalquestions-train-82315", "mrqa_naturalquestions-train-17167", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-train-2630", "mrqa_naturalquestions-validation-4590", "mrqa_naturalquestions-train-22390", "mrqa_naturalquestions-train-7402", "mrqa_triviaqa-validation-1764", "mrqa_naturalquestions-train-25168", "mrqa_naturalquestions-train-63395", "mrqa_naturalquestions-train-59801", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-train-25023", "mrqa_hotpotqa-validation-2937", "mrqa_triviaqa-validation-5746", "mrqa_hotpotqa-validation-3774", "mrqa_naturalquestions-train-82478", "mrqa_naturalquestions-train-23656", "mrqa_naturalquestions-train-17119", "mrqa_naturalquestions-train-51334", "mrqa_naturalquestions-train-6703", "mrqa_naturalquestions-train-82748", "mrqa_triviaqa-validation-6410", "mrqa_naturalquestions-train-25258"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 55, "before_eval": {"predictions": ["over 10,000", "Denver's Executive Vice President of Football Operations and General Manager", "France", "Gracie the new `` face '' of the FBI. Hurt after being dumped by her boyfriend, fellow Agent Eric Matthews ( who gets relocated to Miami ), she agrees to the reassignment", "Brittany, Cornwall, Ireland, Isle of Man, Scotland and Wales", "stromal connective tissue", "paris", "around the world", "Gulf of Mexico", "a job where there are many workers willing to work a large amount of time (high supply) competing for a job that few require (low demand) will result in a low wage for that job", "octagon", "\"Rock With You\"", "Scott Mosier", "kujan", "weaving", "the art of the Persian Safavid dynasty from 1501 to 1722, in present - day Iran and Caucasia", "sepoys of the Company's army in the garrison town of Meerut, 40 miles northeast of Delhi ( now Old Delhi )", "DeMarcus Ware as time expired in the half", "Jack Nicholson", "daniel langdon", "boxing", "the port city of Kaffa in the Crimea in 1347", "the Persian style of architecture", "Iranian", "parphemus", "Landwehr", "often given priority because his work was published first", "as a preparation for the Feast of the Divine Mercy, celebrated each year on first Sunday after Easter", "accommodationism", "Parliamentarians ( `` Roundheads '' ) and Royalists ( `` Cavaliers '' )", "Niger\u2013 Congo language", "a lower index of refraction, typically a cladding of a different glass, or plastic"], "metric_results": {"EM": 0.03125, "QA-F1": 0.1535798971143799}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.2758620689655173, 1.0, 0.4, 0.0, 0.0, 0.0, 0.12121212121212122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9142857142857143, 0.0, 0.0, 0.0, 0.06666666666666667, 0.15384615384615385, 0.3333333333333333, 0.0, 0.0, 0.0, 0.7142857142857143, 0.0, 0.0, 0.5714285714285715, 0.0, 0.36363636363636365]}}, "error_ids": ["mrqa_squad-validation-5518", "mrqa_squad-validation-378", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-6918", "mrqa_triviaqa-validation-3568", "mrqa_squad-validation-4700", "mrqa_triviaqa-validation-140", "mrqa_squad-validation-7407", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-5479", "mrqa_hotpotqa-validation-3264", "mrqa_triviaqa-validation-6478", "mrqa_triviaqa-validation-938", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-4098", "mrqa_squad-validation-818", "mrqa_naturalquestions-validation-9536", "mrqa_triviaqa-validation-7421", "mrqa_naturalquestions-validation-9459", "mrqa_squad-validation-4773", "mrqa_naturalquestions-validation-819", "mrqa_hotpotqa-validation-5543", "mrqa_triviaqa-validation-7689", "mrqa_hotpotqa-validation-4215", "mrqa_squad-validation-3450", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-validation-8986", "mrqa_naturalquestions-validation-570", "mrqa_hotpotqa-validation-2699", "mrqa_naturalquestions-validation-7078"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 56, "before_eval": {"predictions": ["Randy", "1922 to 1991", "conclusive testing of hypotheses", "business sponsors and individual donors", "moral conservatism, literalism, and the attempt \"to implement Islamic values in all spheres of life\"", "Tesla Polyphase System", "65,535 bytes ( 8 byte header + 65,527 bytes of data )", "39, Newton was 26", "a smartphone - like, hands - free format", "increased their reserves (by expanding their money supplies) in amounts far greater than before. The result was a depreciation of the dollar and other industrialized nations' currencies. Because oil was priced in dollars, oil producers' real income decreased.", "parable on philosophy and morality", "the model for one of the characters in Jordan Mechner's game \" Prince of Persia\"", "john mailer London's Burning", "Robert \"Bumps\" Blackwell, Enotris Johnson, and Little Richard", "five", "negatively impact teachers' mental and physical health, productivity, and students' performance", "Subaru", "d'Artagnan", "if there are no repeated data values", "michael river", "1932", "March 31, 1944", "methanol, ethanol, isopropanol, furan, THF, diethyl ether, dioxane, ethyl acetate, DMF, DMSO, acetic acid, and formic acid", "former Manchester United and Danish international goalkeeper Peter Schmeichel", "the highest commissioned SS rank", "elinys", "the lowest known subaerial volcanic vents in the world, at 45 m ( 150 ft ) or more below sea level", "glastonbury Festival", "Sinai, the only part of the country located in Asia", "Alison Steadman", "most concentrated towards the macula", "benjamin williams"], "metric_results": {"EM": 0.09375, "QA-F1": 0.1730753395155569}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3636363636363636, 0.0, 0.0, 0.4782608695652174, 0.0, 0.0, 0.0, 0.5454545454545454, 0.6666666666666666, 0.23076923076923075, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.08695652173913045, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-3372", "mrqa_squad-validation-416", "mrqa_squad-validation-9608", "mrqa_naturalquestions-validation-1787", "mrqa_squad-validation-371", "mrqa_naturalquestions-validation-752", "mrqa_squad-validation-3720", "mrqa_triviaqa-validation-4390", "mrqa_hotpotqa-validation-627", "mrqa_triviaqa-validation-6223", "mrqa_hotpotqa-validation-4620", "mrqa_hotpotqa-validation-3651", "mrqa_squad-validation-2004", "mrqa_triviaqa-validation-2280", "mrqa_triviaqa-validation-326", "mrqa_naturalquestions-validation-486", "mrqa_triviaqa-validation-4640", "mrqa_squad-validation-3653", "mrqa_hotpotqa-validation-15", "mrqa_hotpotqa-validation-686", "mrqa_triviaqa-validation-892", "mrqa_naturalquestions-validation-1349", "mrqa_triviaqa-validation-5813", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-3069", "mrqa_naturalquestions-validation-7358", "mrqa_triviaqa-validation-2701"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 57, "before_eval": {"predictions": ["Saturn", "arthur goldsmith", "1.7 billion views", "Bathurst", "in the 1980s", "In inequality of opportunity was higher in the transition economies of Central and Eastern Europe and Central Asia", "the Psalms, the books of Hebrews, Romans, and Galatians", "Fe1 \u2212 xO", "Groucho", "materials melted near an impact crater", "Better Jacksonville Plan", "david paradine", "Charles Haley", "charles and duchy of Cornwall", "carbon dioxide", "Australian Greens", "His / Her Majesty's Ship", "all of the chicken sold is reared in South Africa", "October 15, 1997", "in the Hebrew Bible in the Book of Job, Psalms, and Isaiah", "modernized the outside of the building and renovated the inside as part of his first construction project in Manhattan", "the North Atlantic Ocean and Arctic Ocean", "wales", "lighthouse design", "tommy Sly", "five", "tescoense", "elis Taylor", "American conservative author and commentator", "chicken schnitzel", "Sam the Sham", "three terms"], "metric_results": {"EM": 0.25, "QA-F1": 0.3886205808080808}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.8, 0.33333333333333337, 1.0, 0.3, 0.2, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.4, 0.25, 0.3636363636363636, 0.3333333333333333, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.3333333333333333]}}, "error_ids": ["mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-4974", "mrqa_hotpotqa-validation-1567", "mrqa_hotpotqa-validation-5436", "mrqa_naturalquestions-validation-3969", "mrqa_squad-validation-2255", "mrqa_squad-validation-3598", "mrqa_triviaqa-validation-3648", "mrqa_triviaqa-validation-6053", "mrqa_triviaqa-validation-5785", "mrqa_squad-validation-2886", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-6365", "mrqa_hotpotqa-validation-3802", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-6425", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-3663", "mrqa_triviaqa-validation-7187", "mrqa_hotpotqa-validation-1975", "mrqa_triviaqa-validation-1742", "mrqa_hotpotqa-validation-1016", "mrqa_naturalquestions-validation-787"], "retrieved_ids": ["mrqa_naturalquestions-train-44661", "mrqa_naturalquestions-train-24377", "mrqa_triviaqa-validation-6872", "mrqa_squad-validation-2885", "mrqa_hotpotqa-validation-1099", "mrqa_naturalquestions-train-45753", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-6956", "mrqa_naturalquestions-train-49393", "mrqa_triviaqa-validation-7153", "mrqa_naturalquestions-train-49757", "mrqa_naturalquestions-train-67402", "mrqa_triviaqa-validation-5057", "mrqa_triviaqa-validation-2803", "mrqa_triviaqa-validation-1494", "mrqa_naturalquestions-train-21057", "mrqa_triviaqa-validation-2802", "mrqa_naturalquestions-train-84647", "mrqa_naturalquestions-train-73275", "mrqa_triviaqa-validation-3568", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-3515", "mrqa_naturalquestions-train-81209", "mrqa_naturalquestions-train-60220", "mrqa_naturalquestions-train-82915", "mrqa_triviaqa-validation-6956", "mrqa_naturalquestions-train-48429", "mrqa_squad-validation-7741", "mrqa_triviaqa-validation-7578", "mrqa_naturalquestions-train-17119", "mrqa_naturalquestions-train-82140", "mrqa_naturalquestions-train-68147"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 58, "before_eval": {"predictions": ["Dutch", "begins in the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "a house edge of between 0.5 % and 1 %", "phagosomal membrane", "in areas such as South Heaton in Newcastle", "1955", "henry v", "200 horsepower", "Nikola Tesla", "Lutheranism", "Night Ranger", "Buzz, the Honey Nut Cheerios Bee", "Sarah Winnemucca Hopkins", "Organizational interventions", "levels of economic inequality", "james Jones", "Edward Teller", "Yulia Tymoshenko", "henry", "1979", "mainly civil servants recruited in special university classes", "black earth", "Q \u00d7 1", "the medial epicondyle of the humerus", "weighing", "no man is an island", "Liao, Jin, and Song", "`` central '' or `` middle ''", "pigeons", "electric currents and magnetic fields", "Attack the Block", "Juliet"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3196766774891775}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.7272727272727273, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.4, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-9173", "mrqa_naturalquestions-validation-5554", "mrqa_squad-validation-8687", "mrqa_squad-validation-5202", "mrqa_triviaqa-validation-6185", "mrqa_squad-validation-1443", "mrqa_squad-validation-1535", "mrqa_hotpotqa-validation-593", "mrqa_naturalquestions-validation-10626", "mrqa_hotpotqa-validation-5654", "mrqa_hotpotqa-validation-5735", "mrqa_triviaqa-validation-5104", "mrqa_squad-validation-7880", "mrqa_hotpotqa-validation-4178", "mrqa_triviaqa-validation-7264", "mrqa_hotpotqa-validation-5127", "mrqa_squad-validation-2042", "mrqa_squad-validation-4233", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-9814", "mrqa_squad-validation-10351", "mrqa_triviaqa-validation-4817", "mrqa_naturalquestions-validation-5624", "mrqa_triviaqa-validation-6520", "mrqa_hotpotqa-validation-820", "mrqa_hotpotqa-validation-1706"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 59, "before_eval": {"predictions": ["profitable", "the exoskeleton of insects, the shells and membranes of externally deposited eggs, and skin", "Naturalization Act of 1790", "Guy Ritchie", "2 %", "monarch of the United Kingdom", "9 at the Century 16 multiplex ( operated by Cinemark ), located at the Town Center at Aurora shopping mall at 14300 E. Alameda Avenue", "James Edward Kelly", "metropolis", "50th Super Bowl", "sanxian", "Iran was the world's second- largest oil exporter and a close US ally", "in the U.S. state of Kansas", "yolk sac ( protruding from its lower part ) but no embryo, even after scanning across all planes of the gestational sac, thus being diagnostic of an anembryonic gestation", "Kelly Bundy", "Italy", "export to London and elsewhere", "Sir William Douglas, 1st Earl of Douglas", "Chromosomal crossover", "Mario Addison", "321,520", "June 12, 2017", "2005", "red deer", "tajikistan", "Helmuth von Moltke", "Baloo ( \u092d\u093e\u0932\u0942 Bh\u0101l\u016b, `` bear '' ; Himalayan brown bear )", "`` Turkey in the Straw ''", "as soon as 2019", "Selznick library", "dickson", "samoan t\u0101l\u0101"], "metric_results": {"EM": 0.125, "QA-F1": 0.18467837014032665}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.08695652173913042, 0.4, 0.0, 0.0, 0.0, 0.09999999999999999, 0.888888888888889, 0.14814814814814814, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-4838", "mrqa_squad-validation-6436", "mrqa_naturalquestions-validation-10009", "mrqa_hotpotqa-validation-1035", "mrqa_naturalquestions-validation-875", "mrqa_triviaqa-validation-2965", "mrqa_naturalquestions-validation-7974", "mrqa_hotpotqa-validation-1393", "mrqa_triviaqa-validation-3719", "mrqa_squad-validation-19", "mrqa_triviaqa-validation-2223", "mrqa_squad-validation-3730", "mrqa_hotpotqa-validation-740", "mrqa_naturalquestions-validation-8116", "mrqa_triviaqa-validation-5864", "mrqa_squad-validation-5124", "mrqa_hotpotqa-validation-5253", "mrqa_naturalquestions-validation-7035", "mrqa_squad-validation-825", "mrqa_squad-validation-7612", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-3685", "mrqa_hotpotqa-validation-4456", "mrqa_naturalquestions-validation-3571", "mrqa_naturalquestions-validation-1676", "mrqa_naturalquestions-validation-5649", "mrqa_triviaqa-validation-1933", "mrqa_triviaqa-validation-2525"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 60, "before_eval": {"predictions": ["Currer Bell", "the illegitimate son of Ned Stark, the honorable lord of Winterfell, an ancient fortress in the North of the fictional continent of Westeros", "in the pancreas", "The Catcher in the Rye", "the Anglo-Saxons", "Margiana", "Bendigo and its environs", "Thomas Jefferson", "She became a naturalized American citizen in 1994 and also received Hungarian citizenship in June 2007.", "July 1872", "Boston and Maine Railroad's Southern Division", "Kevin Harlan as play-by-play announcer, Boomer Esiason and Dan Fouts as color analysts, and James Lofton and Mark Malone as sideline reporters. Jim Gray will anchor the pre-game and halftime coverage", "the left of the dinner plate", "summer months", "Phillip Schofield and Christine Bleakley returned to co-present", "third \u00e9tude", "Korea, the Himalayan kingdoms and South East Asia", "segues", "daniel tomas", "tin and copper", "During the reign of King Beorhtric of Wessex ( 786 -- 802 ) three ships of `` Northmen '' landed at Portland Bay in Dorset", "a South Australian town located 21 kilometres south-east of Adelaide, in the Adelaide Hills", "Flag Day in 1954", "around 300,000", "Sir William James Herschel initiated fingerprinting in India", "Sexred", "Veronica Lodge instead of Betty Cooper", "neo-Nazi ideology with ethnic European paganism and opposition to \"foreign\" religions such as Christianity, Islam and Judaism", "Alan B'Stard", "its yearly budget also began to shrink in light of the successful landing, and NASA also had to make funds available for the development of the upcoming Space Shuttle. By 1971, the decision was made to also cancel missions 18 and 19.", "4 \u00d7 10 Hz ( 1 GeV gamma rays ) down to the local plasma frequency of the ionized interstellar medium ( ~ 1 kHz )", "the \"celebrity criminal\""], "metric_results": {"EM": 0.25, "QA-F1": 0.41502937233265386}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.4347826086956522, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.23529411764705882, 0.6666666666666666, 1.0, 0.125, 0.888888888888889, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.1904761904761905, 0.15384615384615385, 1.0, 1.0, 0.2, 0.0, 0.2857142857142857, 0.1111111111111111, 0.0, 0.3111111111111111, 0.8780487804878049, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-7225", "mrqa_hotpotqa-validation-2715", "mrqa_squad-validation-2842", "mrqa_naturalquestions-validation-9273", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5451", "mrqa_squad-validation-559", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-1786", "mrqa_squad-validation-5449", "mrqa_hotpotqa-validation-1033", "mrqa_triviaqa-validation-3404", "mrqa_triviaqa-validation-3684", "mrqa_naturalquestions-validation-4863", "mrqa_hotpotqa-validation-5311", "mrqa_naturalquestions-validation-6337", "mrqa_hotpotqa-validation-4293", "mrqa_naturalquestions-validation-6759", "mrqa_hotpotqa-validation-1971", "mrqa_triviaqa-validation-1284", "mrqa_squad-validation-4011", "mrqa_naturalquestions-validation-5798"], "retrieved_ids": ["mrqa_naturalquestions-train-64931", "mrqa_naturalquestions-train-7528", "mrqa_naturalquestions-train-34331", "mrqa_naturalquestions-train-74592", "mrqa_naturalquestions-train-71415", "mrqa_naturalquestions-train-8315", "mrqa_naturalquestions-train-71381", "mrqa_naturalquestions-train-41774", "mrqa_hotpotqa-validation-5899", "mrqa_naturalquestions-train-81286", "mrqa_naturalquestions-train-24957", "mrqa_triviaqa-validation-4298", "mrqa_naturalquestions-train-69376", "mrqa_squad-validation-2401", "mrqa_naturalquestions-train-22559", "mrqa_naturalquestions-train-67684", "mrqa_naturalquestions-train-79711", "mrqa_hotpotqa-validation-3241", "mrqa_naturalquestions-train-9516", "mrqa_triviaqa-validation-3568", "mrqa_naturalquestions-train-49719", "mrqa_naturalquestions-train-9735", "mrqa_naturalquestions-train-65805", "mrqa_naturalquestions-train-15739", "mrqa_naturalquestions-train-4310", "mrqa_naturalquestions-train-81286", "mrqa_hotpotqa-validation-987", "mrqa_naturalquestions-train-1225", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-65448", "mrqa_triviaqa-validation-5068", "mrqa_naturalquestions-train-72905"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 61, "before_eval": {"predictions": ["The Raigne of King Edward the Third", "john Hughes", "Riverside", "Judy Collins", "Out of Control", "the 2013 non-fiction book of the same name by David Finkel", "henry", "The Frost Report", "fell from his horse while hunting", "incitement to terrorism", "Liza", "the first flume ride in Ireland", "Mexico City", "iron", "Elk and Kanawha Rivers", "They circulate and are moved around within plant cells, and occasionally pinch in two to reproduce. Their behavior is strongly influenced by environmental factors like light color and intensity. Chloroplasts, like mitochondria, contain their own DNA", "'Bucks Point'", "to `` help bring creative projects to life ''", "mormon", "Old World fossil representatives", "the group 1 elements, excluding hydrogen ( H ), which is nominally a group 1 element but not normally considered to be an alkali metal", "Matt Damon", "Sufjan Stevens", "thicker consistency and a deeper flavour than sauce", "the internal reproductive anatomy ( such as the uterus in females )", "new zealand", "L'\u00c9glise fran\u00e7aise \u00e0 la Nouvelle-Amsterdam", "VH1's \"100 Greatest Artists of Hard Rock\"", "after AD 70", "1985", "electronic music", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II."], "metric_results": {"EM": 0.25, "QA-F1": 0.360878323065823}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.28571428571428575, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.4, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.25, 0.2222222222222222, 0.4, 0.0, 0.923076923076923, 0.0, 0.0, 0.0, 0.15999999999999998]}}, "error_ids": ["mrqa_hotpotqa-validation-1850", "mrqa_triviaqa-validation-3861", "mrqa_naturalquestions-validation-6118", "mrqa_hotpotqa-validation-5822", "mrqa_naturalquestions-validation-7407", "mrqa_triviaqa-validation-6418", "mrqa_squad-validation-6218", "mrqa_naturalquestions-validation-8368", "mrqa_hotpotqa-validation-4743", "mrqa_triviaqa-validation-2130", "mrqa_naturalquestions-validation-7483", "mrqa_squad-validation-8560", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-1699", "mrqa_hotpotqa-validation-4931", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6376", "mrqa_squad-validation-3067", "mrqa_hotpotqa-validation-1114", "mrqa_squad-validation-9370", "mrqa_hotpotqa-validation-327", "mrqa_triviaqa-validation-3985", "mrqa_hotpotqa-validation-3481"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 62, "before_eval": {"predictions": ["sleeps", "whiteness", "from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "the Monarch", "Canada", "Ballarat Bitter", "therefore", "Ricketts Glen State Park", "an openly wounded and unabashedly portentous rock balladeer", "new magma", "Hattie Jacques", "fee per unit of connection time", "coordinator", "1932", "February 7, 2018", "cattle", "Egypt", "Neil Armstrong, Michael Collins and Buzz Aldrin", "lorraine", "4145 ft above mean sea level", "advocacy of young earth creationism and intelligent design", "during the production company vanity cards shown following the closing credits of most programs", "Kansas\u2013Nebraska Act", "2 Constant ( C\u03bc and C\u03b4 ) gene segments", "1910\u20131940", "11:28 left in the second quarter", "Start Here", "thomas thomas", "Autobahn", "endocrine", "Baron of Holberg", "poodle"], "metric_results": {"EM": 0.375, "QA-F1": 0.46552322796934864}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.25, 0.4444444444444445, 0.13333333333333333, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.4, 0.1, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0689655172413793, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5396", "mrqa_squad-validation-7766", "mrqa_hotpotqa-validation-507", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-7270", "mrqa_squad-validation-4750", "mrqa_hotpotqa-validation-1510", "mrqa_squad-validation-9303", "mrqa_naturalquestions-validation-8696", "mrqa_squad-validation-2703", "mrqa_naturalquestions-validation-1555", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-3469", "mrqa_squad-validation-5816", "mrqa_naturalquestions-validation-538", "mrqa_squad-validation-824", "mrqa_triviaqa-validation-4095", "mrqa_hotpotqa-validation-5607", "mrqa_naturalquestions-validation-9064", "mrqa_triviaqa-validation-6254"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 63, "before_eval": {"predictions": ["Bank of China Tower", "isobaric (constant pressure) processes in the Rankine cycle", "dennis Leo Peter \"Ed\" McMahon, Jr.", "ARPANET", "Shakespeare was born on this day in 1564, the same day he died in 1616", "Kohlberg K Travis Roberts", "fleetwood coast", "probabilistic (or \"Monte Carlo\") and deterministic algorithms", "jamesi Hendrix", "eight", "Hong Kong\u2013based, Cayman Islands registered Mandarin and Cantonese-language television broadcaster that serves the Chinese mainland and Hong Kong along with other markets with substantial Chinese viewers", "a narcissistic ex-lover who did the protagonist wrong", "karlotte", "petrographic microscope", "michael thomas", "Haitian Revolution", "Sondheim", "773,347", "1.41665", "The colonists used wampum as money. But then, they used everything as money, including coins from many different European nations, all at the same time.", "1993", "The Daily Times", "ACL tears", "Mayo county council", "ear ossicles", "German", "2001", "their belief in the validity of the social contract, which is held to bind all to obey the laws that a government meeting certain standards of legitimacy has established, or else suffer the penalties set out in the law", "Professor Kantorek", "31 - member Senate", "henry and the woman whom Edward Hopper refers to as \"Yankee\"", "orkneys"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2649140211640212}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 0.5, 0.13333333333333333, 0.0, 0.0, 0.4444444444444445, 0.6666666666666666, 1.0, 0.07407407407407407, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.38095238095238093, 0.0, 0.5, 0.33333333333333337, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-189", "mrqa_squad-validation-3445", "mrqa_triviaqa-validation-7574", "mrqa_squad-validation-4629", "mrqa_triviaqa-validation-3153", "mrqa_hotpotqa-validation-97", "mrqa_triviaqa-validation-1518", "mrqa_squad-validation-9057", "mrqa_triviaqa-validation-54", "mrqa_hotpotqa-validation-366", "mrqa_naturalquestions-validation-6326", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-27", "mrqa_naturalquestions-validation-9755", "mrqa_hotpotqa-validation-55", "mrqa_triviaqa-validation-2129", "mrqa_triviaqa-validation-7011", "mrqa_hotpotqa-validation-3929", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-6651", "mrqa_triviaqa-validation-2850", "mrqa_squad-validation-7861", "mrqa_squad-validation-6707", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1533", "mrqa_triviaqa-validation-4037", "mrqa_triviaqa-validation-2254"], "retrieved_ids": ["mrqa_triviaqa-validation-2130", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-2442", "mrqa_triviaqa-validation-743", "mrqa_naturalquestions-train-69842", "mrqa_naturalquestions-train-48976", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-6956", "mrqa_triviaqa-validation-3814", "mrqa_hotpotqa-validation-5822", "mrqa_naturalquestions-train-64931", "mrqa_naturalquestions-train-42060", "mrqa_naturalquestions-train-84715", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-6872", "mrqa_hotpotqa-validation-4097", "mrqa_naturalquestions-train-46748", "mrqa_naturalquestions-train-41269", "mrqa_triviaqa-validation-7578", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-train-63552", "mrqa_naturalquestions-train-46712", "mrqa_naturalquestions-train-65924", "mrqa_triviaqa-validation-6053", "mrqa_naturalquestions-train-6794", "mrqa_naturalquestions-train-53835", "mrqa_naturalquestions-train-53798", "mrqa_hotpotqa-validation-1035", "mrqa_naturalquestions-train-61986", "mrqa_naturalquestions-validation-5651", "mrqa_squad-validation-3467", "mrqa_naturalquestions-train-52189"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 64, "before_eval": {"predictions": ["failure rate", "Jim Justice", "larvae", "Trey Parker and Matt Stone", "kanto", "one of popular music's most poignant anthems of sorrow regarding the environment", "trial division", "consultant", "Preston, Lancashire, UK", "Phineas and Ferb", "tuberculosis", "Saturn IB", "H. R. Haldeman", "brian Baldwin", "Wilfried Zaha", "duke of Urbino", "jaw", "Swiss Alps", "charles heen", "reform the lunisolar calendar to provide an accuracy of 365.2425 days of the year", "deadliest", "glenister", "immediate judgement discrepancy, or cognitive bias, where a person making an initial assessment of another person, place, or thing will assume ambiguous information based upon concrete information", "New Orleans going north through Chicago and to New York", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "macOS High Sierra", "Frazier", "parallelogram", "Bishop Martin Sasse", "loire river", "17th Century", "appropriates ( gives to, sets aside for ) money to specific federal government departments, agencies, and programs"], "metric_results": {"EM": 0.09375, "QA-F1": 0.1859924461962099}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.42857142857142855, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.16949152542372883, 0.0, 0.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.058823529411764705]}}, "error_ids": ["mrqa_triviaqa-validation-2902", "mrqa_naturalquestions-validation-4207", "mrqa_squad-validation-4567", "mrqa_triviaqa-validation-748", "mrqa_naturalquestions-validation-7857", "mrqa_squad-validation-8909", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-4702", "mrqa_squad-validation-3956", "mrqa_hotpotqa-validation-3489", "mrqa_triviaqa-validation-2032", "mrqa_hotpotqa-validation-811", "mrqa_triviaqa-validation-6398", "mrqa_triviaqa-validation-864", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-3208", "mrqa_squad-validation-8201", "mrqa_hotpotqa-validation-2257", "mrqa_triviaqa-validation-5521", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-2748", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-6706", "mrqa_squad-validation-2560", "mrqa_triviaqa-validation-4709", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-10533"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 65, "before_eval": {"predictions": ["Territory of Colorado", "Arabic numerals", "Diary of a Wimpy Kid : The Long Haul", "the recommended. html filename extension", "prayer", "serous pericardium", "Early Gothic", "keneland race track", "verreaux sifaka", "Boz", "1991", "ABC- DuMont Television Network", "Birmingham, Alabama", "the following identity ( Basel problem) due to Euler,", "2018", "d Dwina", "apple", "gravitation", "nonconservative forces act to change the internal energies of the system, and are often associated with the transfer of heat", "bullfights", "the applied force is opposed by static friction, generated between the object and the table surface", "ormond sacker", "whiskey", "Shut Up", "tzetzal", "Karina Smirnoff became the runners - up, and Jack Osbourne and Cheryl Burke received third place", "The Panthers beat the Seattle Seahawks in the divisional round, running up a 31\u20130 halftime lead and then holding off a furious second half comeback attempt to win 31\u201324, avenging their elimination from a year earlier.", "St. Augustine", "four", "On the Abrogation of the Private Mass", "kedah", "mycelium"], "metric_results": {"EM": 0.21875, "QA-F1": 0.2650297619047619}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.38095238095238093, 1.0, 0.2666666666666667, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-2737", "mrqa_squad-validation-482", "mrqa_naturalquestions-validation-8427", "mrqa_triviaqa-validation-1499", "mrqa_triviaqa-validation-2752", "mrqa_squad-validation-1100", "mrqa_triviaqa-validation-3048", "mrqa_triviaqa-validation-1923", "mrqa_squad-validation-5836", "mrqa_squad-validation-9021", "mrqa_naturalquestions-validation-1784", "mrqa_triviaqa-validation-5573", "mrqa_triviaqa-validation-3518", "mrqa_naturalquestions-validation-6075", "mrqa_squad-validation-10420", "mrqa_squad-validation-10313", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-7605", "mrqa_triviaqa-validation-2383", "mrqa_naturalquestions-validation-1783", "mrqa_squad-validation-239", "mrqa_hotpotqa-validation-4174", "mrqa_squad-validation-2296", "mrqa_triviaqa-validation-262", "mrqa_triviaqa-validation-3691"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 66, "before_eval": {"predictions": ["mayonnaise", "further develop and produce a new lighting system", "Firth of Clyde", "On the Computational Complexity of Algorithms", "from sea level", "Raja Dhilu", "Magnetically soft ( low coercivity ) iron is used for the cores in electromagnets", "The show has been in permanent residence at the Bellagio in Las Vegas, Nevada, United States, since October 1998.", "white rabbit", "Duke Kent-Brown", "oxygen", "bratwurst", "platypus", "tend to set much higher school fees than other public schools", "Vanessa Block", "verruckt", "Fermat", "white", "raven", "http://www.example.com/index.html", "spanish", "`` Goodbye, Abigail. ''", "two catechisms", "edward", "Steel pipes and storage vessels used to store and transmit both gaseous and liquid oxygen will act as a fuel", "Saint-Domingue", "translated the New Testament from Greek into German and poured out doctrinal and polemical writings", "1998", "Organisms in the domains of Archaea and Bacteria", "Commissioners", "King George's War", "sattu"], "metric_results": {"EM": 0.25, "QA-F1": 0.29568868631368633}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.07142857142857142, 1.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4099", "mrqa_squad-validation-1416", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-5927", "mrqa_hotpotqa-validation-5712", "mrqa_triviaqa-validation-7577", "mrqa_squad-validation-3477", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-3970", "mrqa_squad-validation-7130", "mrqa_hotpotqa-validation-4951", "mrqa_triviaqa-validation-515", "mrqa_squad-validation-8980", "mrqa_triviaqa-validation-5337", "mrqa_triviaqa-validation-6566", "mrqa_naturalquestions-validation-8229", "mrqa_triviaqa-validation-6654", "mrqa_naturalquestions-validation-5093", "mrqa_triviaqa-validation-5686", "mrqa_squad-validation-3483", "mrqa_squad-validation-2269", "mrqa_naturalquestions-validation-10357", "mrqa_squad-validation-10167", "mrqa_triviaqa-validation-2087"], "retrieved_ids": ["mrqa_naturalquestions-train-60641", "mrqa_triviaqa-validation-6373", "mrqa_triviaqa-validation-6373", "mrqa_triviaqa-validation-639", "mrqa_naturalquestions-train-43791", "mrqa_naturalquestions-train-70332", "mrqa_naturalquestions-train-8018", "mrqa_squad-validation-8979", "mrqa_naturalquestions-train-9627", "mrqa_naturalquestions-train-48852", "mrqa_naturalquestions-train-28761", "mrqa_triviaqa-validation-6259", "mrqa_naturalquestions-train-14848", "mrqa_squad-validation-3478", "mrqa_naturalquestions-train-57237", "mrqa_naturalquestions-train-79259", "mrqa_naturalquestions-train-68803", "mrqa_triviaqa-validation-1015", "mrqa_naturalquestions-train-43446", "mrqa_triviaqa-validation-4298", "mrqa_hotpotqa-validation-118", "mrqa_triviaqa-validation-3814", "mrqa_naturalquestions-train-79409", "mrqa_triviaqa-validation-781", "mrqa_naturalquestions-train-57", "mrqa_naturalquestions-train-10631", "mrqa_naturalquestions-train-44747", "mrqa_naturalquestions-train-50337", "mrqa_naturalquestions-train-79266", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-2959", "mrqa_naturalquestions-train-63860"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 67, "before_eval": {"predictions": ["high inequality goes hand-in-hand with weak labor movements and vice-versa", "king Richard'the Lionheart' of England", "funky Butt", "Sweden, Norway and Denmark", "the island of Spitsbergen in Svalbard, Norway", "sublimation and hence sometimes deposition is called desublimation", "Norman given name Robert, meaning `` bright renown '' -- from the Germanic elements `` hrod '' meaning renown and `` beraht '' meaning bright", "Crossed: Family Values", "wonga", "Doctor of Philosophy", "1,388 (0.9%) same-sex married couples or partnerships", "Otis Timson", "toothless, bearded hag", "Non Governmental and Intergovernmental Organizations", "$150,000 and $250,000 for primes with at least 100 million digits and 1 billion digits, respectively", "1963", "Ohio", "runner-up", "The Crowned Prince of the Philadelphia Mob", "\u201c Resign.\u201d", "romantic attraction, sexual attraction, or sexual behavior toward both males and females", "the Taih\u014d Code", "Big Eight Conference", "1920s", "American", "Cleopatra VII Philopator", "April Fool's Day", "willy lott", "diary of Thomas Vernon", "chloroplasts", "Salta, Chaco, Santa Fe, C\u00f3rdoba, Catamarca and Tucum\u00e1n", "more than 265 million business records worldwide"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3003651410723779}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [0.33333333333333337, 0.0, 0.0, 0.0, 0.6666666666666666, 0.2222222222222222, 0.10526315789473684, 0.0, 1.0, 0.5, 0.25, 0.0, 0.0, 0.1818181818181818, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.4444444444444445]}}, "error_ids": ["mrqa_squad-validation-7417", "mrqa_triviaqa-validation-6619", "mrqa_triviaqa-validation-7748", "mrqa_hotpotqa-validation-5415", "mrqa_hotpotqa-validation-2813", "mrqa_naturalquestions-validation-8643", "mrqa_naturalquestions-validation-6514", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-5297", "mrqa_squad-validation-4721", "mrqa_naturalquestions-validation-8911", "mrqa_triviaqa-validation-1583", "mrqa_squad-validation-8526", "mrqa_squad-validation-8837", "mrqa_naturalquestions-validation-1976", "mrqa_hotpotqa-validation-294", "mrqa_squad-validation-6974", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-3345", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-567", "mrqa_triviaqa-validation-5013", "mrqa_triviaqa-validation-3395", "mrqa_squad-validation-8929", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-2171"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 68, "before_eval": {"predictions": ["Viceroyalty of New Spain", "Delaware", "the law of Italy governing the acquisition, transmission and loss of Italian citizenship", "his last starring role was as Boston police detective Barry Frost", "The King of Chutzpah", "P $ C", "energy", "river Thames", "English", "john caird", "Monk's", "the minimum amount of time required by the most efficient algorithm solving a given problem", "chip", "the dot", "St. Nazaire", "1961", "Tyler Posey", "New South Wales", "hard-to-fill positions", "kung-fu", "brontosaurus", "qualifications", "Kelli Goss", "the final episode of the series", "Jennifer Jones", "1939", "tolled ( quota ) highways", "redox", "eastern and interior Venezuela and the llanos of Colombia", "tsar", "Lagos", "politically conservative"], "metric_results": {"EM": 0.28125, "QA-F1": 0.31547619047619047}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.14285714285714288, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "error_ids": ["mrqa_naturalquestions-validation-6352", "mrqa_triviaqa-validation-4485", "mrqa_hotpotqa-validation-4978", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-8791", "mrqa_triviaqa-validation-2723", "mrqa_triviaqa-validation-6896", "mrqa_naturalquestions-validation-339", "mrqa_squad-validation-1784", "mrqa_triviaqa-validation-6302", "mrqa_triviaqa-validation-5185", "mrqa_naturalquestions-validation-10496", "mrqa_hotpotqa-validation-1145", "mrqa_triviaqa-validation-3720", "mrqa_triviaqa-validation-5893", "mrqa_naturalquestions-validation-921", "mrqa_hotpotqa-validation-2867", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-10354", "mrqa_triviaqa-validation-3089", "mrqa_naturalquestions-validation-2632", "mrqa_hotpotqa-validation-4275"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 69, "before_eval": {"predictions": ["State Route 41", "Max Planck", "music became more expressive and emotional, expanding to encompass literary, artistic, and philosophical themes", "Thermochemical techniques", "North Kest even, Lincolnshire", "the Ming", "near major hotels and in the parking areas of major Chinese supermarkets", "al-Maridini", "Hellenismos", "the lower back", "The FCI accepted the long - haired type in 2010, listing it as the variety b", "unknown", "tea, horticultural produce, and coffee", "Barclays", "Christina Applegate as Sue Ellen `` Swell '' Crandell", "the blood spilled by the heroes who died in the name of their countrymen's Fatherland and Freedom", "island of island", "break off the cathode, pass out of the tube, and physically strike him", "Silver Gallery", "21.8 %", "placebo effect", "Yuan dynasty is usually considered to be the legitimate dynasty between the Song dynasty and the Ming dynasty", "Tony Orlando and Dawn", "Paris", "we Shall Overcome", "Martha Wainwright", "1,462", "island of man", "West Norse sailors", "symphonic", "Zephyrus", "1698"], "metric_results": {"EM": 0.15625, "QA-F1": 0.25747056118331274}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.25, 0.125, 1.0, 0.5, 0.0, 0.15384615384615383, 0.0, 0.0, 0.0, 0.11764705882352941, 0.09523809523809523, 1.0, 0.0, 0.4444444444444445, 0.10526315789473682, 0.0, 0.42857142857142855, 0.4, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8645", "mrqa_naturalquestions-validation-8059", "mrqa_hotpotqa-validation-4307", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-6949", "mrqa_squad-validation-6464", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-4844", "mrqa_triviaqa-validation-2464", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-8394", "mrqa_triviaqa-validation-1449", "mrqa_squad-validation-1388", "mrqa_squad-validation-5571", "mrqa_naturalquestions-validation-3035", "mrqa_squad-validation-3610", "mrqa_squad-validation-8185", "mrqa_hotpotqa-validation-3435", "mrqa_triviaqa-validation-5494", "mrqa_hotpotqa-validation-665", "mrqa_triviaqa-validation-6541", "mrqa_naturalquestions-validation-7217", "mrqa_triviaqa-validation-6782", "mrqa_triviaqa-validation-5088", "mrqa_hotpotqa-validation-1381"], "retrieved_ids": ["mrqa_naturalquestions-train-84927", "mrqa_squad-validation-9984", "mrqa_naturalquestions-train-47066", "mrqa_naturalquestions-validation-2808", "mrqa_naturalquestions-train-57237", "mrqa_naturalquestions-train-7341", "mrqa_naturalquestions-train-15426", "mrqa_naturalquestions-train-61196", "mrqa_naturalquestions-train-85155", "mrqa_naturalquestions-train-73609", "mrqa_naturalquestions-train-25813", "mrqa_naturalquestions-train-54223", "mrqa_naturalquestions-train-31407", "mrqa_naturalquestions-train-24281", "mrqa_naturalquestions-train-84845", "mrqa_naturalquestions-train-21090", "mrqa_naturalquestions-train-73434", "mrqa_naturalquestions-train-75322", "mrqa_naturalquestions-train-10375", "mrqa_naturalquestions-train-30455", "mrqa_naturalquestions-train-77573", "mrqa_naturalquestions-train-3601", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-2701", "mrqa_naturalquestions-train-12128", "mrqa_naturalquestions-train-52771", "mrqa_naturalquestions-train-16884", "mrqa_naturalquestions-train-30306", "mrqa_naturalquestions-train-6049", "mrqa_naturalquestions-train-31891", "mrqa_hotpotqa-validation-1376", "mrqa_squad-validation-5360"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 70, "before_eval": {"predictions": ["london", "Battle of White Plains", "36th season, Survivor : Ghost Island, will premiere on February 28, 2018", "hotel room", "closure announcements by all three companies in the 21st century will mean that Australia will no longer be a base for the global car industry", "cordwainer", "Eadred Lulisc", "1973", "2015", "a loanword of the Visigothic word guma `` man ''", "Major General Miles Francis Stapleton Fitzalan- Howard, 17th Duke of Norfolk,", "london", "from their June 2011 Milton Keynes performances, Australian and New Zealand tour", "chromium added to steel", "blue", "a domestic passenger flight that was hijacked by five al- Qaeda members on September 11, 2001, as part of the September 11 attacks", "from the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s ( brother, brotherly )", "82.30'E longitude, in Mirzapur, Uttar Pradesh, which is nearly on the corresponding longitude reference line", "unity of God", "Salman Khan", "Skip Tyler", "the attempt to discover first principles --'those universal principles which are the condition of the possibility of the existence of anything and everything '", "WBMA-LD", "two Nobel Peace Prizes", "Jack Daniels whiskey", "physically strike him", "king james i of Great Britain", "an individual's rights which, although not legally binding in themselves, have been elaborated in subsequent international treaties, economic transfers, regional human rights instruments, national constitutions, and other laws", "a straight line (see world line) traveling through time, which normally increases up or to the right in the diagram", "Newton", "International Imitation Hemingway Competition", "Kingsford, Michigan"], "metric_results": {"EM": 0.15625, "QA-F1": 0.33261828101005647}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.3076923076923077, 1.0, 0.08695652173913045, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.5882352941176471, 0.0, 0.9565217391304348, 0.4, 0.0, 0.0, 0.2, 0.4761904761904762, 1.0, 0.0, 1.0, 0.08, 1.0, 1.0, 0.8, 0.0, 0.9090909090909091, 0.05714285714285714, 0.21052631578947367, 0.0, 0.28571428571428575, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-831", "mrqa_hotpotqa-validation-4939", "mrqa_naturalquestions-validation-2083", "mrqa_squad-validation-2980", "mrqa_triviaqa-validation-5981", "mrqa_hotpotqa-validation-471", "mrqa_naturalquestions-validation-710", "mrqa_hotpotqa-validation-259", "mrqa_naturalquestions-validation-3019", "mrqa_hotpotqa-validation-5334", "mrqa_triviaqa-validation-2331", "mrqa_naturalquestions-validation-4567", "mrqa_triviaqa-validation-6740", "mrqa_triviaqa-validation-672", "mrqa_hotpotqa-validation-4323", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-4211", "mrqa_triviaqa-validation-1408", "mrqa_squad-validation-1389", "mrqa_triviaqa-validation-2546", "mrqa_naturalquestions-validation-7938", "mrqa_squad-validation-10477", "mrqa_squad-validation-361", "mrqa_hotpotqa-validation-4543", "mrqa_naturalquestions-validation-2229"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 71, "before_eval": {"predictions": ["oil magnate and wealthiest man in history John D. Rockefeller", "29", "electrical, water, sewage, phone, and cable facilities", "Saxe-Coburg and Gotha", "1,100", "There is no known case of any U.S. citizens buying Canadian drugs", "Dwight David \"Ike\" Eisenhower", "multi-purpose", "Tang, Song, as well as Khitan Liao and Jurchen Jin dynasties", "twelve", "manley royale", "Randal Keith Orton", "RuPaul", "oxygen is often transported in bulk as a liquid in specially insulated tankers", "Sunday", "October 16, 2012", "Austria", "2012", "Somatic motor neurons originate in the central nervous system, project their axons to skeletal muscles ( such as the muscles of the limbs, abdominal, and intercostal muscles ), which are involved in locomotion", "velvet", "goebbels: Mastermind of the Third Reich", "cauliflower", "be-Bop-A-Lula", "the Hongwu Emperor of the Ming Dynasty", "heptathlon", "1565", "rapid expansion in telecommunication and financial activity", "fair Maid of Perth", "Caroline Sterling, n\u00e9e Bone, formerly Pemberton ( born 3 April 1955 ; died 2017 ) ( Sara Coward )", "Libertarianism", "london", "American R&B"], "metric_results": {"EM": 0.25, "QA-F1": 0.3444093725165831}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 0.5882352941176471, 0.0, 0.0, 1.0, 0.1111111111111111, 0.0, 0.4, 0.0, 0.4, 1.0, 1.0, 0.0, 0.19999999999999998, 0.19354838709677416, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "error_ids": ["mrqa_squad-validation-7849", "mrqa_hotpotqa-validation-862", "mrqa_squad-validation-6383", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-5619", "mrqa_naturalquestions-validation-2635", "mrqa_triviaqa-validation-6938", "mrqa_hotpotqa-validation-583", "mrqa_naturalquestions-validation-8136", "mrqa_squad-validation-3689", "mrqa_hotpotqa-validation-5203", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2548", "mrqa_hotpotqa-validation-5162", "mrqa_triviaqa-validation-4291", "mrqa_triviaqa-validation-1185", "mrqa_naturalquestions-validation-8907", "mrqa_triviaqa-validation-237", "mrqa_naturalquestions-validation-2477", "mrqa_triviaqa-validation-1640", "mrqa_naturalquestions-validation-2847", "mrqa_hotpotqa-validation-1694", "mrqa_triviaqa-validation-4464", "mrqa_hotpotqa-validation-2866"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 72, "before_eval": {"predictions": ["Monk's Caf\u00e9", "a pagan custom, namely, the winter solstice which in Europe occurs in December", "fourth-level Republican leader in the House", "seven times", "vertex cover problem", "Operation Neptune", "john f kentucky", "Patrick Moore", "alpaca fiber and mohair from Angora goats", "Muskogean language", "Symbolic interactionism", "the fictional town of West Egg on prosperous Long Island in the summer of 1922", "gesture in which the head is tilted in alternating up and down arcs along the sagittal plane", "The West Wing", "snow and rain", "Parliament", "# 4 School of Public Health in the country", "shakespeare", "dolphin", "1883\u201384", "tree", "Les Surfs as `` A Pr\u00e9sent Tu Peux t'en Aller''' French also recorded", "fox", "Macau Peninsula, Macau", "divide to form new pyrenoids, or be produced \"de novo\"", "morrissey", "747", "satirical erotic romantic comedy film", "Greg (Shearsmith) Fran ( Sarah Hadland) Connie (Tamzin Outhwaite)", "1750", "Puente Hills Mall, located in the City of Industry, California, United States", "l'Arc de Triomphe"], "metric_results": {"EM": 0.125, "QA-F1": 0.2731376262626263}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false], "QA-F1": [0.4, 0.09999999999999999, 0.8000000000000002, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8181818181818181, 0.125, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 0.625, 0.0, 0.4, 1.0, 0.0, 0.0, 0.888888888888889, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-4815", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-1110", "mrqa_squad-validation-1760", "mrqa_hotpotqa-validation-4061", "mrqa_triviaqa-validation-6606", "mrqa_triviaqa-validation-5439", "mrqa_naturalquestions-validation-5531", "mrqa_hotpotqa-validation-946", "mrqa_naturalquestions-validation-1507", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-1359", "mrqa_hotpotqa-validation-5478", "mrqa_squad-validation-3592", "mrqa_naturalquestions-validation-9306", "mrqa_triviaqa-validation-417", "mrqa_triviaqa-validation-3799", "mrqa_squad-validation-9855", "mrqa_naturalquestions-validation-3638", "mrqa_triviaqa-validation-5393", "mrqa_hotpotqa-validation-1394", "mrqa_triviaqa-validation-564", "mrqa_triviaqa-validation-1162", "mrqa_hotpotqa-validation-3252", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1858", "mrqa_triviaqa-validation-7377"], "retrieved_ids": ["mrqa_naturalquestions-train-39211", "mrqa_naturalquestions-train-66866", "mrqa_naturalquestions-train-41128", "mrqa_naturalquestions-train-60023", "mrqa_hotpotqa-validation-1968", "mrqa_triviaqa-validation-4298", "mrqa_naturalquestions-train-83773", "mrqa_triviaqa-validation-7215", "mrqa_squad-validation-1862", "mrqa_naturalquestions-train-3097", "mrqa_hotpotqa-validation-304", "mrqa_naturalquestions-train-44487", "mrqa_triviaqa-validation-3420", "mrqa_naturalquestions-train-56676", "mrqa_naturalquestions-train-78621", "mrqa_naturalquestions-train-79410", "mrqa_naturalquestions-train-86362", "mrqa_naturalquestions-train-45208", "mrqa_naturalquestions-train-18868", "mrqa_naturalquestions-train-69860", "mrqa_naturalquestions-train-38985", "mrqa_naturalquestions-train-83106", "mrqa_triviaqa-validation-7371", "mrqa_naturalquestions-train-87260", "mrqa_triviaqa-validation-791", "mrqa_naturalquestions-train-23475", "mrqa_naturalquestions-train-6991", "mrqa_squad-validation-1879", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-train-53908", "mrqa_hotpotqa-validation-118", "mrqa_naturalquestions-train-59681"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 73, "before_eval": {"predictions": ["a course to the northwest, through the present North Sea", "ishmael", "light energy", "San Francisco Bay Area", "the \"Negro Cavalry\" by the Native American tribes they fought in the Indian Wars", "4 km", "The Future", "1969", "epic historical drama", "Wahhabist", "buzzards", "medication management system development, deployment and optimization", "that first set of endosymbiotic events", "Danish - Norwegian patronymic surname meaning `` son of Anders ''", "Edward Trowbridge Collins Sr.", "November 2014", "when a country's influence is felt in social and cultural circles, i.e. its soft power, such that it changes the moral, cultural and societal worldview of another", "increased productivity, trade, and secular economic trends", "purple rain", "After World War I", "1989", "political geographer", "around 200\u2013300", "his mind", "extended fluids, differences in pressure result in forces being directed along the pressure gradients", "an alternate, but rarely used unit of mass", "Polovtsian Dances", "tain", "the government - owned Panama Canal Authority", "Vernier, Switzerland", "aeoline", "PM Magazine"], "metric_results": {"EM": 0.28125, "QA-F1": 0.4284160862285862}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true], "QA-F1": [0.25, 1.0, 0.6666666666666666, 0.0, 0.42857142857142855, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.9090909090909091, 0.2222222222222222, 1.0, 0.4, 0.14814814814814814, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.3846153846153846, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-9043", "mrqa_squad-validation-8727", "mrqa_squad-validation-139", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-186", "mrqa_triviaqa-validation-56", "mrqa_hotpotqa-validation-3483", "mrqa_squad-validation-9592", "mrqa_triviaqa-validation-7082", "mrqa_squad-validation-8801", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-2069", "mrqa_squad-validation-9871", "mrqa_naturalquestions-validation-9530", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-2567", "mrqa_squad-validation-6550", "mrqa_squad-validation-1510", "mrqa_squad-validation-10430", "mrqa_squad-validation-10455", "mrqa_triviaqa-validation-4408", "mrqa_naturalquestions-validation-9753", "mrqa_triviaqa-validation-7160"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 74, "before_eval": {"predictions": ["an warm and swift Atlantic ocean current that originates in the Gulf of Mexico and stretches to the tip of Florida, and follows the eastern coastlines of the United States and Newfoundland before crossing the Atlantic Ocean", "Friedrich Nietzsche", "nellie", "Germanic languages", "waltham forest borough", "Zeppelin", "paul white", "DeWayne Warren", "width", "hard Candy", "George Merrill and Shannon Rubicam of the band Boy Meets Girl", "near the end of the El Camino Real de Tierra Adentro", "In 1943, at the peak of World War II", "translation", "wai Momi", "a \"consulting fee\"", "religious organizations or private individuals", "tolerance of civil disobedience", "ed smiths", "the British East India company to sell tea from China in American colonies without paying any taxes", "film scripts written by ghost writers, nonfiction books on military subjects, and video games", "British and French Canadian fur traders from before 1810, and American settlers from the mid-1830s", "Instagram", "Chiptune/breakcore artist DJ Scotch Egg", "1990, the threshold was lowered yet again to 1 year old", "her beautiful voice", "1922 July 1930", "gluons, which form part of the virtual pi and rho mesons", "the character of the Father, the Son, or the Holy Spirit", "on March 23, 2018, by Fox Searchlight Pictures, with a wide release on for April 13, 2018", "\"In the Bedroom\"", "business magnate, investor, and philanthropist"], "metric_results": {"EM": 0.09375, "QA-F1": 0.2182059077618288}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.12121212121212122, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.9090909090909091, 0.0, 0.2222222222222222, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.3157894736842105, 0.0, 0.25, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.16666666666666669, 0.0, 0.3157894736842105, 0.0, 0.33333333333333337]}}, "error_ids": ["mrqa_naturalquestions-validation-8072", "mrqa_hotpotqa-validation-5520", "mrqa_triviaqa-validation-6002", "mrqa_hotpotqa-validation-2098", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-6573", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2644", "mrqa_triviaqa-validation-4275", "mrqa_naturalquestions-validation-8008", "mrqa_naturalquestions-validation-7462", "mrqa_naturalquestions-validation-3188", "mrqa_triviaqa-validation-2938", "mrqa_squad-validation-1462", "mrqa_squad-validation-7086", "mrqa_triviaqa-validation-4614", "mrqa_naturalquestions-validation-642", "mrqa_hotpotqa-validation-2220", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-1027", "mrqa_hotpotqa-validation-726", "mrqa_naturalquestions-validation-9487", "mrqa_squad-validation-801", "mrqa_hotpotqa-validation-4964", "mrqa_squad-validation-10447", "mrqa_squad-validation-2391", "mrqa_naturalquestions-validation-177", "mrqa_triviaqa-validation-5108", "mrqa_hotpotqa-validation-83"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 75, "before_eval": {"predictions": ["around the time when ARPANET was interlinked with NSFNET in the late 1980s", "Andrea Palladio", "guitarist", "Gaahl", "darts", "Armored Vehicles", "bannockburn", "author of the New York Times bestseller Before I Fall Lauren Oliver", "kingdom of Bahrain", "1902", "Statue of Freedom", "18 November 1963", "17:16:20 GMT", "30 percent of the city's total population", "2D side-scrollers", "the state legislators of Assam", "350 government officials and climate change experts", "Gothic", "nazi", "scrolls", "Environmental Protection Agency", "2523 km", "tube", "Cher", "one person", "Luger P08", "Al-Masjid an-Nabawi", "the Turco - Mongol Timurid dynasty of Central Asia", "laundry", "from Manitoba, Ontario and Nova Scotia in southern Canada to northern Florida and from the Atlantic coast to the Missouri River and the eastern Great Plains", "manned lunar landing", "SAS"], "metric_results": {"EM": 0.1875, "QA-F1": 0.29203869047619047}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.3333333333333333, 0.25, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.14285714285714288, 1.0, 0.6666666666666666]}}, "error_ids": ["mrqa_squad-validation-5628", "mrqa_hotpotqa-validation-551", "mrqa_hotpotqa-validation-3306", "mrqa_triviaqa-validation-1217", "mrqa_naturalquestions-validation-9426", "mrqa_triviaqa-validation-6051", "mrqa_squad-validation-8035", "mrqa_triviaqa-validation-5878", "mrqa_hotpotqa-validation-5643", "mrqa_squad-validation-7732", "mrqa_squad-validation-683", "mrqa_hotpotqa-validation-1689", "mrqa_naturalquestions-validation-9546", "mrqa_squad-validation-8525", "mrqa_naturalquestions-validation-9576", "mrqa_triviaqa-validation-2879", "mrqa_naturalquestions-validation-3347", "mrqa_hotpotqa-validation-1298", "mrqa_squad-validation-903", "mrqa_triviaqa-validation-1205", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-4413", "mrqa_naturalquestions-validation-6482", "mrqa_triviaqa-validation-1199", "mrqa_naturalquestions-validation-2006", "mrqa_hotpotqa-validation-2646"], "retrieved_ids": ["mrqa_naturalquestions-train-72472", "mrqa_naturalquestions-train-19685", "mrqa_triviaqa-validation-140", "mrqa_naturalquestions-train-28545", "mrqa_naturalquestions-train-13255", "mrqa_triviaqa-validation-4298", "mrqa_naturalquestions-train-19752", "mrqa_squad-validation-3018", "mrqa_naturalquestions-train-38557", "mrqa_naturalquestions-train-68187", "mrqa_naturalquestions-train-17936", "mrqa_naturalquestions-train-38504", "mrqa_naturalquestions-train-48185", "mrqa_triviaqa-validation-6689", "mrqa_naturalquestions-train-55114", "mrqa_naturalquestions-train-14418", "mrqa_naturalquestions-train-25327", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-7506", "mrqa_naturalquestions-train-9756", "mrqa_naturalquestions-train-75388", "mrqa_triviaqa-validation-667", "mrqa_naturalquestions-train-1147", "mrqa_naturalquestions-train-25982", "mrqa_naturalquestions-train-1172", "mrqa_naturalquestions-train-44690", "mrqa_naturalquestions-train-42716", "mrqa_naturalquestions-train-15909", "mrqa_naturalquestions-train-30968", "mrqa_hotpotqa-validation-327", "mrqa_triviaqa-validation-6053", "mrqa_naturalquestions-train-30811"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 76, "before_eval": {"predictions": ["Raiders quarterback Rich Gannon", "2010", "Saint Etienne", "novelization of the 1977 film Star Wars", "May 27, 2016", "Colonization movement or Black Zionism", "elected", "mulberry leaves", "The Boy in the Striped Pyjamas", "Conrad Lewis", "2017 - 12 - 10", "Hertz Corporation", "2013", "Medicare", "Leonhard Euler", "Grease", "savannah", "Arizona", "reborn", "Bhavageete", "Tachycardia, also called tachyarrhythmia", "Honest", "Richard Burbage", "a type of \"blood poisoning\") and pneumonic (an airborne plague that attacks the lungs before the rest of the body) forms of the plague", "Westminster", "achievement-oriented motivations (\" pull\") such as vocation and more likely to involve the pursue of new products, services, or underserved market needs", "discarded", "A Song of Ice and Fire", "Morrissey", "Julia McKenzie and Anton Rodgers", "1998", "Amazon.com"], "metric_results": {"EM": 0.21875, "QA-F1": 0.32643545313382266}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.28571428571428575, 0.0, 0.25, 0.3636363636363636, 0.0, 0.2608695652173913, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-8747", "mrqa_naturalquestions-validation-7535", "mrqa_hotpotqa-validation-3337", "mrqa_naturalquestions-validation-2102", "mrqa_triviaqa-validation-4440", "mrqa_hotpotqa-validation-2786", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-53", "mrqa_hotpotqa-validation-3678", "mrqa_naturalquestions-validation-6258", "mrqa_hotpotqa-validation-3232", "mrqa_hotpotqa-validation-2550", "mrqa_naturalquestions-validation-6027", "mrqa_squad-validation-1927", "mrqa_hotpotqa-validation-4510", "mrqa_naturalquestions-validation-10131", "mrqa_hotpotqa-validation-956", "mrqa_naturalquestions-validation-2270", "mrqa_squad-validation-5031", "mrqa_squad-validation-9547", "mrqa_squad-validation-7323", "mrqa_triviaqa-validation-5849", "mrqa_naturalquestions-validation-9591"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 77, "before_eval": {"predictions": ["Fort Wayne", "400-meter freestyle", "two previously unknown but related clades (genetic branches) of the Y. pestis genome associated with medieval mass graves", "east-west", "four", "new zealand", "at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "lofsongur", "islay", "the troposphere", "Greater Antilles", "The Fixx", "67,038", "Forrest Gump", "Islam", "handwriting", "the highest quotient", "William the Conqueror", "Pristin", "Jack", "at any time after the auction", "the northernmost point at which the noon sun is just visible on the December solstice", "the original title for the show, Forever Sam Crow", "liverpool", "Great Yuan", "Song Il-gon", "1974", "Acura", "superoxide ion (O \u223c2) and hydrogen peroxide (H2O2)", "food and clothing", "The Emperor of Japan", "from the amino acids glycine and arginine"], "metric_results": {"EM": 0.34375, "QA-F1": 0.42830247846695213}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, false, false, true, false, true, true, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.21052631578947367, 1.0, 1.0, 0.0, 0.07407407407407407, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.2857142857142857, 0.07692307692307691, 0.27272727272727276, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857]}}, "error_ids": ["mrqa_naturalquestions-validation-70", "mrqa_triviaqa-validation-6483", "mrqa_squad-validation-4961", "mrqa_triviaqa-validation-3900", "mrqa_naturalquestions-validation-6452", "mrqa_triviaqa-validation-593", "mrqa_naturalquestions-validation-8584", "mrqa_hotpotqa-validation-5052", "mrqa_squad-validation-9520", "mrqa_squad-validation-9530", "mrqa_hotpotqa-validation-1558", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4766", "mrqa_naturalquestions-validation-4475", "mrqa_naturalquestions-validation-4117", "mrqa_naturalquestions-validation-7845", "mrqa_squad-validation-8046", "mrqa_triviaqa-validation-7610", "mrqa_squad-validation-3543", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-686"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 78, "before_eval": {"predictions": ["2007", "healthcare professionals with specialised education and training who perform various roles to ensure optimal health outcomes for their patients through the quality use of medicines", "fox", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "neuro-orthopaedic Irish veterinary surgeon", "law", "Anne of Green Gables", "hip joint", "The Vortex Jazz Club", "heineken", "east end of London", "digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids to completely neutralize any gastric acid that passes further down into the digestive tract", "light", "1484", "T cells ( for cell - mediated, cytotoxic adaptive immunity )", "electronic sound sources, live instrumental playing and digital signal processing", "1814", "the change in its enthalpy resulting from providing energy, typically heat, to a specific quantity of the substance to change its state from a solid to a liquid", "Mongolian", "Gap", "The Washington Post", "the late sixteenth century, Nurhaci, originally a Ming vassal, began organizing `` Banners '', military - social units that included Jurchen, Han Chinese, and Mongol elements", "mortadella", "Fort Frontenac on the north shore of Lake Ontario and an expedition through the wilderness of the Maine district and down the Chaudi\u00e8re River to attack the city of Quebec", "friendship to Jamukha, asking him to return to his side", "13.34% (116.7 sq mi or 302 km2)", "1983 -- 84 season, Tim Duncan leads the National Basketball Association ( NBA ) in the points - rebounds combination with 840, and John Stockton leads the points -- assists combination with 714", "DTIME(n2)", "the allegedly corrupt machinations of Fran\u00e7ois Bigot", "loud and dirty as possible", "don't tell my heart", "chocolate confectionery"], "metric_results": {"EM": 0.0625, "QA-F1": 0.22276161829330532}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [0.0, 0.3448275862068966, 0.0, 0.35294117647058826, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7301587301587301, 0.0, 1.0, 0.2, 0.26666666666666666, 0.0, 0.22857142857142854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42857142857142855, 0.33333333333333337, 0.25, 0.6842105263157895, 0.0, 1.0, 0.9090909090909091, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5348", "mrqa_squad-validation-6280", "mrqa_triviaqa-validation-1117", "mrqa_naturalquestions-validation-5838", "mrqa_hotpotqa-validation-1219", "mrqa_triviaqa-validation-4676", "mrqa_triviaqa-validation-1658", "mrqa_triviaqa-validation-4063", "mrqa_triviaqa-validation-1168", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-5011", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6497", "mrqa_naturalquestions-validation-9342", "mrqa_hotpotqa-validation-1296", "mrqa_squad-validation-9445", "mrqa_naturalquestions-validation-1119", "mrqa_squad-validation-8083", "mrqa_squad-validation-421", "mrqa_hotpotqa-validation-4864", "mrqa_naturalquestions-validation-7311", "mrqa_triviaqa-validation-1916", "mrqa_squad-validation-10239", "mrqa_squad-validation-6115", "mrqa_squad-validation-7476", "mrqa_naturalquestions-validation-276", "mrqa_squad-validation-1807", "mrqa_hotpotqa-validation-2437", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-1465"], "retrieved_ids": ["mrqa_squad-validation-1808", "mrqa_triviaqa-validation-11", "mrqa_naturalquestions-train-69046", "mrqa_triviaqa-validation-6566", "mrqa_naturalquestions-train-811", "mrqa_naturalquestions-train-76391", "mrqa_naturalquestions-train-33514", "mrqa_naturalquestions-train-23326", "mrqa_triviaqa-validation-11", "mrqa_naturalquestions-train-30076", "mrqa_naturalquestions-train-39520", "mrqa_triviaqa-validation-2007", "mrqa_naturalquestions-train-56363", "mrqa_naturalquestions-train-991", "mrqa_triviaqa-validation-3664", "mrqa_triviaqa-validation-4402", "mrqa_squad-validation-7042", "mrqa_naturalquestions-train-855", "mrqa_triviaqa-validation-7189", "mrqa_naturalquestions-train-47999", "mrqa_naturalquestions-train-50820", "mrqa_naturalquestions-train-44763", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-train-85803", "mrqa_naturalquestions-train-33306", "mrqa_naturalquestions-train-9712", "mrqa_naturalquestions-train-74865", "mrqa_naturalquestions-train-24615", "mrqa_naturalquestions-train-81657", "mrqa_naturalquestions-train-47238", "mrqa_triviaqa-validation-388", "mrqa_naturalquestions-train-10672"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 79, "before_eval": {"predictions": ["Sir Hiram Stevens Maxim", "1987", "The Forever People", "make direct representations to the Presiding Officer to nominate speakers", "Sully", "October 12, 2017", "an allegiance oath that must be taken by all immigrants who wish to become United States citizens", "Kevin Whately", "shirley", "clay animation or \"clay-mation\"", "Iowa", "1977", "precedes the value", "The stationary steam engine was a key component of the Industrial Revolution", "a spherical boundary of zero thickness in which photons that move on tangents to that sphere would be trapped in a circular orbit about the black hole", "Boletus edulis", "red Sox", "rugged terrain such as the Arctic", "Mexico\u2013united States border", "The American Football Conference (AFC) champion Denver Broncos", "a ribosome in the cytosol", "Massapequa", "Ringo Starr", "leicestershire", "gas turbines", "provided majority of members present at that time approved the bill either by voting or voice vote", "Shoushi Li (\u6388\u6642\u66a6) or Calendar for Fixing the Seasons", "16 %", "july 1", "Secretary of Defense", "pancake-shaped circular disks about 300\u2013600 nanometers in diameter", "thirteen American colonies, then at war with the Kingdom of Great Britain"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3384526554363511}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.35714285714285715, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.33333333333333337, 0.3636363636363636, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.5, 0.6086956521739131, 0.4, 0.2222222222222222, 0.0, 1.0, 0.18181818181818182, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4516", "mrqa_naturalquestions-validation-9184", "mrqa_naturalquestions-validation-7733", "mrqa_squad-validation-9407", "mrqa_naturalquestions-validation-1672", "mrqa_triviaqa-validation-7178", "mrqa_triviaqa-validation-5170", "mrqa_hotpotqa-validation-2846", "mrqa_triviaqa-validation-3844", "mrqa_naturalquestions-validation-4529", "mrqa_squad-validation-3256", "mrqa_naturalquestions-validation-5435", "mrqa_hotpotqa-validation-5589", "mrqa_triviaqa-validation-5983", "mrqa_squad-validation-3738", "mrqa_squad-validation-1", "mrqa_squad-validation-8960", "mrqa_hotpotqa-validation-3538", "mrqa_triviaqa-validation-5529", "mrqa_squad-validation-3369", "mrqa_naturalquestions-validation-3591", "mrqa_squad-validation-8233", "mrqa_naturalquestions-validation-8509", "mrqa_triviaqa-validation-1912", "mrqa_squad-validation-8811", "mrqa_naturalquestions-validation-360"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 80, "before_eval": {"predictions": ["Tom Courtenay", "new jersey", "flowers", "Sachin Tendulkar", "fREEWAY", "Charles Silverstein", "Tangmere, West Sussex", "oil shock", "london", "West Virginia American Water", "medical abnormalities, activation level, or recruitment order, or to analyze the biomechanics of human or animal movement", "beer", "HgO", "red", "aquarium fish", "1967", "Mondays at 21:30 (KST)", "22 miles", "1598", "the coastline peninsula of Davenports Neck called \"Bauffet's Point\"", "geoffrey", "margaret margaret", "Topeka, Kansas", "500", "red", "veil", "1894", "they were inserted before \"May God help me\" only in later versions of the speech and not recorded in witness accounts of the proceedings", "`` Nelson's Sparrow ''", "coldplay", "phowa", "primary law, secondary law and supplementary law"], "metric_results": {"EM": 0.1875, "QA-F1": 0.31325676638176636}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.4, 0.37037037037037035, 0.4, 0.0, 0.5, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-3100", "mrqa_triviaqa-validation-6493", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-2148", "mrqa_triviaqa-validation-6063", "mrqa_hotpotqa-validation-3056", "mrqa_hotpotqa-validation-3282", "mrqa_triviaqa-validation-7358", "mrqa_naturalquestions-validation-7848", "mrqa_hotpotqa-validation-3779", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-1261", "mrqa_hotpotqa-validation-2640", "mrqa_squad-validation-3038", "mrqa_squad-validation-3318", "mrqa_triviaqa-validation-1100", "mrqa_triviaqa-validation-2009", "mrqa_hotpotqa-validation-2840", "mrqa_squad-validation-10182", "mrqa_triviaqa-validation-5904", "mrqa_naturalquestions-validation-870", "mrqa_squad-validation-4975", "mrqa_squad-validation-2223", "mrqa_naturalquestions-validation-3822", "mrqa_triviaqa-validation-6129", "mrqa_squad-validation-1930"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 81, "before_eval": {"predictions": ["one rotation of the crank and two piston strokes", "Super Bowl 50", "2009\u201310 Specials", "an adviser", "USD 5.2 billion", "A rear - view mirror ( or rearview mirror )", "The Birds", "identify rock samples in the laboratory", "French & Saunders", "European Parliament and the Council of the European Union", "car crash", "on either February 28 or March 1, while others only observe birthdays on the authentic intercalary date, February 29", "Thomas Edison", "duo", "M\u00fcntzer's execution", "red hot poker", "a new entrance building", "national unity", "paris", "cuba", "construction service firms (e.g. engineering, architecture) and construction managers (firms engaged in managing construction projects without assuming direct financial responsibility for completion of the construction project)", "paraiso", "`` Feed Jake '' is a song written by Danny `` Bear '' Mayo, and recorded by the American country music band Pirates of the Mississippi", "several hundred thousand", "physics", "Noel Gallagher", "Max West", "Central Trains East Coast", "aluminium", "newspapers", "Lakshmibai", "moxibustion"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2756453462795374}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.2222222222222222, 0.0, 0.0, 1.0, 0.375, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5294117647058824, 0.0, 0.5384615384615384, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.33333333333333337, 0.4, 0.2222222222222222]}}, "error_ids": ["mrqa_squad-validation-3267", "mrqa_squad-validation-7", "mrqa_squad-validation-7728", "mrqa_naturalquestions-validation-2776", "mrqa_naturalquestions-validation-8591", "mrqa_hotpotqa-validation-1949", "mrqa_squad-validation-5055", "mrqa_triviaqa-validation-1724", "mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-5053", "mrqa_hotpotqa-validation-1701", "mrqa_triviaqa-validation-4596", "mrqa_squad-validation-5447", "mrqa_triviaqa-validation-1305", "mrqa_triviaqa-validation-5598", "mrqa_triviaqa-validation-2632", "mrqa_squad-validation-6764", "mrqa_triviaqa-validation-1476", "mrqa_naturalquestions-validation-7838", "mrqa_squad-validation-914", "mrqa_triviaqa-validation-296", "mrqa_hotpotqa-validation-3695", "mrqa_squad-validation-5288", "mrqa_triviaqa-validation-5741", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-1664", "mrqa_squad-validation-8265"], "retrieved_ids": ["mrqa_naturalquestions-train-38786", "mrqa_naturalquestions-train-85027", "mrqa_naturalquestions-train-39276", "mrqa_naturalquestions-train-70099", "mrqa_triviaqa-validation-1028", "mrqa_squad-validation-1", "mrqa_naturalquestions-train-16692", "mrqa_naturalquestions-train-5570", "mrqa_naturalquestions-train-79698", "mrqa_hotpotqa-validation-3779", "mrqa_naturalquestions-train-1807", "mrqa_triviaqa-validation-5746", "mrqa_squad-validation-8811", "mrqa_naturalquestions-train-33455", "mrqa_naturalquestions-train-25158", "mrqa_naturalquestions-train-2314", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-train-18920", "mrqa_naturalquestions-train-41954", "mrqa_naturalquestions-train-23010", "mrqa_naturalquestions-train-690", "mrqa_hotpotqa-validation-5899", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-train-51582", "mrqa_hotpotqa-validation-3870", "mrqa_naturalquestions-train-61742", "mrqa_triviaqa-validation-4681", "mrqa_naturalquestions-train-49194", "mrqa_triviaqa-validation-5068", "mrqa_naturalquestions-train-86260", "mrqa_naturalquestions-train-66021", "mrqa_naturalquestions-validation-5649"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 82, "before_eval": {"predictions": ["north west coast of Scotland", "Pauli repulsion (due to fermionic nature of electrons) follows resulting in the force that acts in a direction normal to the surface interface between two objects", "the opposite ( contralateral ) side of the body", "24 hours", "April 20, 1945", "dachshund", "helping farmers grow new pigeon pea varieties, instead of maize", "geologic, topographic, and natural ecosystem landscapes", "129", "Bunkhouse", "Adidas", "a growing sport in southern California, particularly at the high school level, with increasing numbers of schools adding rugby as an official school sport", "geese", "his friend and future rival, Jamukha, and his protector, Toghrul Khan of the Keraite tribe", "tentacles and named", "konakry", "Tagore", "Jason Ravnsborg", "water on the ground surface enters the soil", "9.9ft", "film and short novels", "American-Canadian mystery-drama television series", "Archer", "Gardnerville", "Nucleotides", "25-minute episodes", "Julian Paul Julian", "zapatista army of national Liberation", "Tinu Suresh Desai", "green eyed monster", "2003", "dead sea"], "metric_results": {"EM": 0.1875, "QA-F1": 0.33492527173913045}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, true], "QA-F1": [0.33333333333333337, 0.16, 0.0, 1.0, 0.5, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.782608695652174, 0.5, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-2133", "mrqa_squad-validation-10390", "mrqa_naturalquestions-validation-579", "mrqa_hotpotqa-validation-4153", "mrqa_hotpotqa-validation-4420", "mrqa_squad-validation-8322", "mrqa_squad-validation-2791", "mrqa_squad-validation-9603", "mrqa_naturalquestions-validation-2347", "mrqa_triviaqa-validation-5592", "mrqa_squad-validation-2748", "mrqa_triviaqa-validation-1053", "mrqa_squad-validation-6113", "mrqa_squad-validation-4616", "mrqa_triviaqa-validation-1685", "mrqa_naturalquestions-validation-8544", "mrqa_naturalquestions-validation-746", "mrqa_triviaqa-validation-863", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2361", "mrqa_hotpotqa-validation-4493", "mrqa_squad-validation-7708", "mrqa_hotpotqa-validation-1639", "mrqa_triviaqa-validation-2236", "mrqa_triviaqa-validation-2592", "mrqa_naturalquestions-validation-9117"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 83, "before_eval": {"predictions": ["alberto Aleandro Uderzo", "Southeast Europe, Western Asia, the Caucasus, North Africa, and the Horn of Africa", "young girl", "architecture from the gothic, renaissance, baroque and neoclassical periods", "back down to the ground", "the aboral organ", "piotr naskrecki", "scrooge", "northridge", "Cushman", "Maidstone", "kingdoms of Francia on the Lower Rhine, Burgundy on the Upper Rhine and Alemannia", "Selim II", "3", "shirley cameron", "difficult and intricate topics", "email fax", "ABC Radio, with Clear Channel Communications and Westwood One ( which had earlier purchased NBC's radio division, as well as the distribution rights to CBS's, and the Mutual Broadcasting System during the 1990s)", "Tahiti", "detritus from the settlement of the sedimentation", "Rated R", "matthew 14:22-36", "Steveston Outdoor pool", "electricity supply system", "Terry the Tomboy", "redbird", "Australia", "dimensionless", "pit road speed", "North Atlantic Conference", "at the group's final British performance on 14 July 2012 at the National Bowl in Milton Keynes", "british"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3059251568324149}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, true, false, false], "QA-F1": [0.0, 0.16666666666666669, 0.0, 0.0, 0.6153846153846153, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.15384615384615385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12903225806451613, 0.14285714285714288, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8484848484848485, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3493", "mrqa_squad-validation-9912", "mrqa_naturalquestions-validation-1805", "mrqa_squad-validation-920", "mrqa_naturalquestions-validation-6856", "mrqa_triviaqa-validation-3113", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-7486", "mrqa_squad-validation-3107", "mrqa_squad-validation-9349", "mrqa_hotpotqa-validation-1312", "mrqa_naturalquestions-validation-5214", "mrqa_triviaqa-validation-5960", "mrqa_hotpotqa-validation-2964", "mrqa_triviaqa-validation-90", "mrqa_squad-validation-5739", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-1798", "mrqa_triviaqa-validation-3987", "mrqa_naturalquestions-validation-7172", "mrqa_squad-validation-1159", "mrqa_triviaqa-validation-570", "mrqa_hotpotqa-validation-4102", "mrqa_naturalquestions-validation-8518", "mrqa_triviaqa-validation-5793"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 84, "before_eval": {"predictions": ["music", "26", "Nathan Bedford Forrest", "Theodosius I", "town of the Ubii", "in front of only 700 fans", "brianxton", "60", "Assistant Secretary for Administration and Management", "better fuel economy", "september", "during the engine cycle", "tree", "glial cells", "Carson City", "biologist", "Egyptians", "christ church", "Poems : Series 1", "United States ambassador to Ghana and to Czechoslovakia", "Cleveland Cavaliers", "Emma Thompson", "edible - nest swiftlets", "oratorio", "travel literature, cartography, geography, and scientific education", "prince at Wi\u015bniowiec, magnate, grandfather of future Polish\u2013Lithuanian Commonwealth monarch", "Archdeacon", "chiang Kai-Shek", "125 lb (57 kg)", "thoracic", "car", "as a prison from 1100 ( Ranulf Flambard ) until 1952 ( Kray twins ), although that was not its primary purpose"], "metric_results": {"EM": 0.25, "QA-F1": 0.3378975538350538}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.24000000000000002, 0.0, 0.0, 0.0, 0.4615384615384615, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 0.0, 0.1]}}, "error_ids": ["mrqa_hotpotqa-validation-3830", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-6794", "mrqa_triviaqa-validation-2014", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-1628", "mrqa_hotpotqa-validation-4414", "mrqa_triviaqa-validation-5687", "mrqa_squad-validation-3364", "mrqa_triviaqa-validation-1529", "mrqa_naturalquestions-validation-3368", "mrqa_triviaqa-validation-3538", "mrqa_naturalquestions-validation-10461", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-661", "mrqa_naturalquestions-validation-732", "mrqa_naturalquestions-validation-8660", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-1645", "mrqa_hotpotqa-validation-989", "mrqa_triviaqa-validation-159", "mrqa_naturalquestions-validation-3173", "mrqa_triviaqa-validation-4696", "mrqa_naturalquestions-validation-6660"], "retrieved_ids": ["mrqa_naturalquestions-train-14373", "mrqa_naturalquestions-train-46311", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-42135", "mrqa_triviaqa-validation-7506", "mrqa_naturalquestions-train-17468", "mrqa_hotpotqa-validation-1975", "mrqa_triviaqa-validation-3876", "mrqa_naturalquestions-train-85623", "mrqa_naturalquestions-train-39439", "mrqa_naturalquestions-train-55413", "mrqa_hotpotqa-validation-1660", "mrqa_naturalquestions-validation-10676", "mrqa_hotpotqa-validation-1850", "mrqa_squad-validation-3181", "mrqa_triviaqa-validation-2938", "mrqa_naturalquestions-train-32795", "mrqa_hotpotqa-validation-389", "mrqa_triviaqa-validation-5746", "mrqa_naturalquestions-train-34944", "mrqa_naturalquestions-train-80555", "mrqa_naturalquestions-train-2851", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-train-63308", "mrqa_naturalquestions-train-49469", "mrqa_triviaqa-validation-7398", "mrqa_hotpotqa-validation-811", "mrqa_triviaqa-validation-5813", "mrqa_naturalquestions-train-52296", "mrqa_naturalquestions-train-47045", "mrqa_triviaqa-validation-5936", "mrqa_hotpotqa-validation-4097"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 85, "before_eval": {"predictions": ["in Europe, North America, East Asia and South Asia", "10,000", "Stephen Crawford Young", "Autumn marks the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere ), when the duration of daylight becomes noticeably shorter and the temperature cools down considerably", "15:01.83", "October 13, 1980", "between 1765 and 1783", "discovery", "cardiothoracic", "generally on a bread plate, sometimes in the napkin ), napkin, and flatware ( knives and spoons to the right of the central plate, and forks to the left )", "meats", "bilaterally symmetrical", "he did not consider the papacy part of the biblical Church", "1837", "Yen Press", "Bambi, a Life in the Woods", "inefficiency", "from ages 12\u201318, while authors and readers of \"Young teen novels\" often define it as written for those aged 15 to the early 20s", "Retina Display", "on the equator and overlies the East African Rift covering a diverse and expansive terrain that extends roughly from Lake Victoria to Lake Turkana ( formerly called Lake Rudolf) and further south-east to the Indian Ocean", "in school at a given time in the school day ( such as lunch, recess or after school); or even to attend school on a non-school day", "guinea", "Jane Austen", "lactobacilli", "at a school or other place of formal education", "due to a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "davison", "magazines and journals", "EE Kestrel", "1970s", "ro Roland Ratzenberger", "a dose of 200 to 500 mg up to 7 ml"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2723611111111111}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 1.0, 0.0, 0.2, 0.25, 0.16, 1.0, 0.0, 0.08333333333333333, 0.0, 0.0, 1.0, 0.2222222222222222, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.16666666666666669]}}, "error_ids": ["mrqa_naturalquestions-validation-10601", "mrqa_squad-validation-2529", "mrqa_naturalquestions-validation-7818", "mrqa_triviaqa-validation-6151", "mrqa_naturalquestions-validation-3515", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-7425", "mrqa_naturalquestions-validation-2023", "mrqa_triviaqa-validation-710", "mrqa_naturalquestions-validation-7767", "mrqa_squad-validation-2267", "mrqa_naturalquestions-validation-7513", "mrqa_hotpotqa-validation-1506", "mrqa_naturalquestions-validation-2883", "mrqa_hotpotqa-validation-244", "mrqa_squad-validation-8266", "mrqa_squad-validation-1941", "mrqa_triviaqa-validation-5150", "mrqa_triviaqa-validation-4988", "mrqa_squad-validation-1891", "mrqa_naturalquestions-validation-9772", "mrqa_triviaqa-validation-4854", "mrqa_triviaqa-validation-2508", "mrqa_naturalquestions-validation-2907", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-8555"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 86, "before_eval": {"predictions": ["Native Americans", "thammasat university", "South Sentinel Island", "FCA US LLC", "Aaron Taylor- Johnson", "blue", "japan", "it is a gift", "Conservative", "Peter Davison, Colin Baker and Sylvester McCoy", "conservative Saudi-based Wahhabism", "Super Bowl XXVIII", "maquiladora", "Rich Girl", "in human and animals", "fifth", "a problem", "13 May 1787", "WTVG", "Great Lakes", "observations", "Judaism", "film playback singer", "southern whites", "Butler", "L", "University Grants Commission", "cooling all the atmosphere by spraying the whole atmosphere as if drawing letters in the air ( `` penciling '' )", "quantum mechanics", "the Allstars", "he stood by their contents", "August 1, 2016"], "metric_results": {"EM": 0.15625, "QA-F1": 0.24921875}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.5, 0.0, 0.8750000000000001, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.13333333333333333, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-3348", "mrqa_triviaqa-validation-4707", "mrqa_hotpotqa-validation-4230", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5195", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-734", "mrqa_squad-validation-2295", "mrqa_squad-validation-7281", "mrqa_squad-validation-7679", "mrqa_squad-validation-9589", "mrqa_squad-validation-782", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3704", "mrqa_hotpotqa-validation-4401", "mrqa_naturalquestions-validation-9878", "mrqa_triviaqa-validation-5001", "mrqa_squad-validation-8531", "mrqa_squad-validation-2331", "mrqa_hotpotqa-validation-367", "mrqa_naturalquestions-validation-9516", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-5241", "mrqa_naturalquestions-validation-3323", "mrqa_squad-validation-10506", "mrqa_squad-validation-2113", "mrqa_naturalquestions-validation-6912"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 87, "before_eval": {"predictions": ["First Street in downtown Dayton, Ohio, United States", "electronic gaming machines, table games, i Gaming and iLottery products, instant lottery games, lottery gaming systems, terminals and services, internet applications, server-based interactive gambling terminals, and gambling control systems", "2009", "Mohammad Reza Pahlavi", "digital streams", "American Broadcasting Companies, Inc.", "on a sound stage in front of a live audience in Burbank, California", "Mongolian, Tibetan, and Chinese", "the band has frequently dealt with their videos being taken down off YouTube", "Paul Walker", "carbon cycle", "tenant management", "their heart was weighed against the feather of truth", "Donna Noble (Catherine Tate) with Mickey Smith (noel Clarke) and Jack Harkness (Henry Barrowman) recurring", "salvaging a country usually seen as one of the most stable and prosperous in Africa", "Kree, briefly known as the Ruul", "titanium metal", "Talk That Talk", "the Kikuyu, Embu and Kamba words Kirinyaga, Kirenyaa and Kiinyaa", "henry kenzie", "domestic legislation", "esoteric", "henry chaplin", "the Senate chamber is located in the north wing of the Capitol, in Washington, D.C.", "apple", "Thutmose III", "ten to fifteen", "Buck Owens and the Buckaroos", "Kansas", "political support", "clownfish", "Wimbledon"], "metric_results": {"EM": 0.1875, "QA-F1": 0.35340735653235655}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false], "QA-F1": [0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 0.0, 1.0, 0.5, 0.6666666666666666, 0.6666666666666667, 0.0, 0.33333333333333337, 0.6666666666666666, 0.0, 0.6153846153846153, 0.0, 0.5714285714285715, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-3420", "mrqa_hotpotqa-validation-3885", "mrqa_naturalquestions-validation-2196", "mrqa_squad-validation-5648", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-3713", "mrqa_squad-validation-7703", "mrqa_squad-validation-8386", "mrqa_hotpotqa-validation-3692", "mrqa_triviaqa-validation-7649", "mrqa_hotpotqa-validation-3391", "mrqa_squad-validation-8276", "mrqa_triviaqa-validation-6049", "mrqa_squad-validation-9504", "mrqa_squad-validation-2122", "mrqa_triviaqa-validation-3943", "mrqa_naturalquestions-validation-3848", "mrqa_triviaqa-validation-2276", "mrqa_hotpotqa-validation-1282", "mrqa_squad-validation-8552", "mrqa_hotpotqa-validation-5743", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-2814"], "retrieved_ids": ["mrqa_triviaqa-validation-6689", "mrqa_naturalquestions-train-21281", "mrqa_naturalquestions-train-24946", "mrqa_triviaqa-validation-1453", "mrqa_naturalquestions-train-43281", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-51238", "mrqa_naturalquestions-train-59801", "mrqa_triviaqa-validation-3876", "mrqa_naturalquestions-train-5636", "mrqa_naturalquestions-train-54157", "mrqa_naturalquestions-train-67764", "mrqa_naturalquestions-train-79462", "mrqa_naturalquestions-train-46650", "mrqa_hotpotqa-validation-3232", "mrqa_hotpotqa-validation-1606", "mrqa_hotpotqa-validation-5899", "mrqa_triviaqa-validation-2635", "mrqa_naturalquestions-train-54644", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-6326", "mrqa_triviaqa-validation-2802", "mrqa_squad-validation-1903", "mrqa_naturalquestions-train-29696", "mrqa_naturalquestions-train-55090", "mrqa_naturalquestions-train-30927", "mrqa_hotpotqa-validation-1142", "mrqa_naturalquestions-train-53870", "mrqa_naturalquestions-train-78201", "mrqa_naturalquestions-train-27893", "mrqa_naturalquestions-train-74582"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 88, "before_eval": {"predictions": ["a defender", "smith family", "Queensland", "by denying having committed the crime", "Selena Gomez", "8th", "Forbes", "159 beats per minute ( bpm )", "small intestine", "guitarists", "\"public\" (state-controlled) and \"independent\"", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "Dan Dare, Pilot of the Future", "fictional", "in arid lowland or mountainous shrubland, widely dispersed in dry open country with scattered brush", "in exaggerating their seriousness", "sacerdotalism by considering all baptized Christians to be a holy priesthood", "discontinued", "an estimated half a million acres", "44", "from the Gaulish name R\u0113nos, which was adapted in Roman-era geography (1st century BC) as Greek \u1fec\u1fc6\u03bd\u03bf\u03c2 ( Rh\u0113nos) Latin Rhenus", "cash from investors such as banks and shareholders, as well as the outflow of cash to shareholders as dividends as the company generates income", "duke of england", "ceremonial counties", "Solange Knowles & Destiny's Child", "Ward", "Fryda Wolff", "Eddie Gottlieb Trophy", "Elvis Presley", "Afghans", "reversed", "phycobilisomes"], "metric_results": {"EM": 0.125, "QA-F1": 0.26141408878477845}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 0.0, 0.4, 0.15384615384615385, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.6666666666666666, 0.9655172413793104, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.2727272727272727, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2848", "mrqa_triviaqa-validation-6166", "mrqa_hotpotqa-validation-3930", "mrqa_squad-validation-6775", "mrqa_naturalquestions-validation-5785", "mrqa_hotpotqa-validation-3343", "mrqa_naturalquestions-validation-10162", "mrqa_triviaqa-validation-2784", "mrqa_triviaqa-validation-6636", "mrqa_squad-validation-6823", "mrqa_naturalquestions-validation-4351", "mrqa_triviaqa-validation-4242", "mrqa_hotpotqa-validation-799", "mrqa_naturalquestions-validation-8319", "mrqa_squad-validation-6396", "mrqa_squad-validation-2101", "mrqa_naturalquestions-validation-8965", "mrqa_hotpotqa-validation-2668", "mrqa_squad-validation-9246", "mrqa_naturalquestions-validation-5291", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4364", "mrqa_naturalquestions-validation-2264", "mrqa_squad-validation-843", "mrqa_naturalquestions-validation-8622", "mrqa_hotpotqa-validation-5498", "mrqa_naturalquestions-validation-8095", "mrqa_hotpotqa-validation-828"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 89, "before_eval": {"predictions": ["217", "if a person violates a law in order to create a test case as to the constitutionality of a law", "House of Fraser", "Bonkyll Castle", "veal stock", "time complexity", "Deadman's Gun", "m\u00e1laga airport", "the Philippines", "a Z-ring within the chloroplast's stroma", "margaret margaret", "1843", "France and the referendum in the Netherlands, the 2004 Treaty establishing a Constitution for Europe never came into force", "director", "William Allen White Book Award and the California Young Reader Medal", "Louis XVIII", "January 30, 1930", "the Hel Peninsula", "swissair", "all states, Washington, D.C. as well as all U.S. territories except American Samoa, but not on all Native American tribal lands", "the Gaussian integers Z[i] that is, the set of complex numbers of the form a + bi where i denotes the imaginary unit and a and b are arbitrary integers", "Janis Joplin with the poets Michael McClure and Bob Neuwirth", "Odinga", "homicides", "shropshire waterways & Leisure Group", "the boroughs of Teterboro, Moonachie, and Hasbrouck Heights in Bergen County", "November 6, 2018", "the Khitan Empire", "the American philosophy of pragmatism and particularly from the work of George Herbert Mead", "USC Trojans", "Richard Bremmer", "sequential proteolytic activation of complement molecules"], "metric_results": {"EM": 0.25, "QA-F1": 0.33229923818707313}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false], "QA-F1": [0.0, 0.1739130434782609, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4545454545454545, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6451612903225806, 0.23076923076923078, 0.9473684210526316, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-803", "mrqa_squad-validation-6773", "mrqa_hotpotqa-validation-1756", "mrqa_triviaqa-validation-3639", "mrqa_naturalquestions-validation-2981", "mrqa_triviaqa-validation-6387", "mrqa_naturalquestions-validation-7598", "mrqa_squad-validation-8898", "mrqa_triviaqa-validation-5020", "mrqa_squad-validation-3985", "mrqa_triviaqa-validation-603", "mrqa_hotpotqa-validation-289", "mrqa_naturalquestions-validation-3269", "mrqa_triviaqa-validation-615", "mrqa_triviaqa-validation-7020", "mrqa_hotpotqa-validation-2831", "mrqa_squad-validation-9079", "mrqa_naturalquestions-validation-9419", "mrqa_triviaqa-validation-5430", "mrqa_hotpotqa-validation-597", "mrqa_triviaqa-validation-3643", "mrqa_naturalquestions-validation-1450", "mrqa_squad-validation-2793", "mrqa_squad-validation-6645"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 90, "before_eval": {"predictions": ["isot oxygen made by the splitting of O2 by ultraviolet (UV) radiation", "reactive allotrope", "Black Guardian Trilogy", "tragedy", "animal phylum", "older", "magnetism", "bounding the time or space used by the algorithm", "lawful protest demonstration, nonviolent civil disobedience, and violent civil disobedience", "clefs", "four zonal offices at Chennai, Delhi, Kolkata and Mumbai", "black Bottom", "Douglas Richard Hofstadter", "Kitab Rudjdjar", "time", "photolysis", "1998", "captain Horatio Hornblower", "Grisha Alekandrovich Nikolaev", "cytotoxic", "rootlets", "violet", "Drew Barrymore", "red", "Pittsburgh Steelers", "anti epitopes", "south-central motorway", "baku", "september", "Morty", "Charlotte Louise Riley", "Costiff collection"], "metric_results": {"EM": 0.125, "QA-F1": 0.1767361111111111}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.26666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "error_ids": ["mrqa_squad-validation-3496", "mrqa_squad-validation-3497", "mrqa_hotpotqa-validation-2585", "mrqa_squad-validation-4421", "mrqa_squad-validation-4971", "mrqa_triviaqa-validation-6726", "mrqa_squad-validation-1689", "mrqa_triviaqa-validation-2940", "mrqa_naturalquestions-validation-7034", "mrqa_triviaqa-validation-5", "mrqa_hotpotqa-validation-4809", "mrqa_squad-validation-10358", "mrqa_naturalquestions-validation-2838", "mrqa_squad-validation-5814", "mrqa_triviaqa-validation-1246", "mrqa_naturalquestions-validation-3233", "mrqa_squad-validation-6884", "mrqa_naturalquestions-validation-1704", "mrqa_triviaqa-validation-3988", "mrqa_hotpotqa-validation-3250", "mrqa_triviaqa-validation-7407", "mrqa_squad-validation-259", "mrqa_squad-validation-6627", "mrqa_hotpotqa-validation-2128", "mrqa_triviaqa-validation-810", "mrqa_naturalquestions-validation-6248", "mrqa_hotpotqa-validation-5274", "mrqa_squad-validation-5441"], "retrieved_ids": ["mrqa_naturalquestions-train-82495", "mrqa_naturalquestions-train-58080", "mrqa_naturalquestions-train-27763", "mrqa_naturalquestions-train-54223", "mrqa_naturalquestions-train-51833", "mrqa_naturalquestions-train-21076", "mrqa_naturalquestions-train-15754", "mrqa_hotpotqa-validation-820", "mrqa_naturalquestions-train-34003", "mrqa_naturalquestions-train-35002", "mrqa_hotpotqa-validation-983", "mrqa_triviaqa-validation-1551", "mrqa_naturalquestions-train-62065", "mrqa_squad-validation-8924", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-4829", "mrqa_naturalquestions-train-414", "mrqa_naturalquestions-train-1669", "mrqa_naturalquestions-train-70875", "mrqa_naturalquestions-train-39918", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-80228", "mrqa_naturalquestions-validation-142", "mrqa_triviaqa-validation-6478", "mrqa_naturalquestions-train-36117", "mrqa_hotpotqa-validation-1558", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-train-43519", "mrqa_squad-validation-3478", "mrqa_naturalquestions-train-47971", "mrqa_naturalquestions-train-68815", "mrqa_triviaqa-validation-1451"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 91, "before_eval": {"predictions": ["40%", "catherine linton", "Mongolian patrimonial feudalism and the traditional Chinese autocratic-b Bureaucratic system", "nursery rhyme", "Afghanistan", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Australian", "around 100,000 writes", "paris", "tahiti", "tuscaloosa", "North Greenwich Arena", "ovules", "Masters Tournament", "the short film \"Endless Flight\" in omnibus \"horror Stories\"", "at least 90% certain that temperatures will continue to rise", "henry Bessemer", "improved firearms", "1856", "Hepatocytes", "deities and spirits", "gregory creek", "Belfast and elsewhere in the United Kingdom, Canada, Croatia, Iceland, Malta, Morocco, Spain, and the United States", "dave rowntree", "corvids", "South Africa", "Love Actually", "low-skilled workers", "at the center of the Northern Hemisphere", "Pakistan", "psilocin", "robby de niro"], "metric_results": {"EM": 0.15625, "QA-F1": 0.30411295825195256}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.46153846153846156, 0.0, 0.0, 0.47058823529411764, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.967741935483871, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.8]}}, "error_ids": ["mrqa_triviaqa-validation-4749", "mrqa_squad-validation-8411", "mrqa_triviaqa-validation-7698", "mrqa_squad-validation-9738", "mrqa_naturalquestions-validation-4471", "mrqa_hotpotqa-validation-2205", "mrqa_naturalquestions-validation-2146", "mrqa_triviaqa-validation-3055", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-888", "mrqa_naturalquestions-validation-2439", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1140", "mrqa_squad-validation-8577", "mrqa_triviaqa-validation-6102", "mrqa_squad-validation-9812", "mrqa_triviaqa-validation-1336", "mrqa_hotpotqa-validation-1068", "mrqa_triviaqa-validation-5143", "mrqa_naturalquestions-validation-7799", "mrqa_triviaqa-validation-6126", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-4229", "mrqa_squad-validation-7538", "mrqa_naturalquestions-validation-2721", "mrqa_hotpotqa-validation-277", "mrqa_triviaqa-validation-6594"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 92, "before_eval": {"predictions": ["a field in Somerset County, Pennsylvania", "communism", "The Gentle Don\" or \"the Docile Don\"", "the United States Congress declared war ( Public Law 77 - 328, 55 STAT 795 ) on the Empire of Japan in response to that country's surprise attack on Pearl Harbor the prior day", "pacific island", "July 25, 1898", "BitInstant", "\"Fame\", Debbie Berwick on \"Phil of the Future\"", "toledo", "railway locomotives, ships, steamboats and road vehicles", "Social Chapter", "Australia", "retina", "physics", "The Adventures of Ozzie and Harriet", "Eunice Kennedy Shriver", "Landry's, Inc.", "George Mifflin Dallas", "David Naughton, Jenny Agutter and Griffin Dunne", "national defence Volunteers", "In 1893, George Westinghouse won the bid to light the 1893 World's Columbian Exposition in Chicago with alternating current", "john alcock and brown", "December 1978", "inversely to member state size", "propeller", "Irvine business centers of The Irvine Spectrum, West Irvine, and international corporations headquartered at the University of California, Irvine", "Marine Corps Air Station Kaneohe Bay", "Teen Titans Go!", "rugby union", "November 20, 1942", "Hugues Capet, king of France", "Morning Edition"], "metric_results": {"EM": 0.3125, "QA-F1": 0.5012603715728716}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true], "QA-F1": [0.33333333333333337, 0.0, 1.0, 0.25, 0.0, 0.5, 1.0, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.4, 0.3333333333333333, 1.0, 0.6666666666666666, 0.1111111111111111, 0.8571428571428571, 0.6666666666666666, 0.9090909090909091, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4163", "mrqa_triviaqa-validation-3064", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-3366", "mrqa_naturalquestions-validation-3820", "mrqa_hotpotqa-validation-2474", "mrqa_squad-validation-3276", "mrqa_naturalquestions-validation-3316", "mrqa_squad-validation-10388", "mrqa_squad-validation-5811", "mrqa_hotpotqa-validation-5895", "mrqa_naturalquestions-validation-10080", "mrqa_hotpotqa-validation-2914", "mrqa_triviaqa-validation-6960", "mrqa_squad-validation-1400", "mrqa_triviaqa-validation-7185", "mrqa_hotpotqa-validation-4387", "mrqa_squad-validation-4210", "mrqa_triviaqa-validation-980", "mrqa_squad-validation-2696", "mrqa_triviaqa-validation-1267", "mrqa_squad-validation-3189"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 93, "before_eval": {"predictions": ["pacific region", "another principal networking paradigm, circuit switching", "white", "Utah", "paris", "Great Britain", "mid-size four - wheel drive luxury SUV", "black", "texas", "we want to practice Christian love toward them and pray that they convert", "two populations of rodents: one resistant to the disease, which act as hosts, keeping the disease endemic, and a second that lack resistance", "sometime between 124 and 800 CE, with some theories dating the earliest Polynesian settlements to the 10th or even 13th century", "T'Pau", "geheime Staatspolizei", "1815", "National Football Conference", "kimono", "North America where it has a core population in Michigan and surrounding states and provinces", "either because the faults are not planar or because rock layers are dragged along, forming drag folds as slip occurs along the fault", "2015", "man's disobedience toward God", "a representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "Winnie the Pooh", "Transpacific Yacht Race", "German", "registered with the General Teaching Council for Scotland (GTCS)", "heavy provider of commercial explosives and blasting systems to the mining, quarrying, oil and gas and construction markets, a supplier of sodium cyanide for gold extraction, and a specialist provider of ground support services in mining and tunnelling", "either yes or no, or alternately either 1 or 0", "Gibraltar, a British Overseas Territory, located at the southern tip of the Iberian Peninsula", "Season 5 premiere, `` Weight Loss ''", "lie detector", "mass murder of the future king"], "metric_results": {"EM": 0.125, "QA-F1": 0.2545023297422929}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [0.0, 0.5, 0.0, 0.06060606060606061, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.33333333333333337, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.0, 0.1935483870967742, 0.0, 0.0, 0.0, 0.34782608695652173, 0.0, 0.0, 0.33333333333333337, 0.0, 0.05555555555555556, 0.4615384615384615, 0.16666666666666669, 1.0, 1.0, 0.25]}}, "error_ids": ["mrqa_triviaqa-validation-7513", "mrqa_squad-validation-4747", "mrqa_triviaqa-validation-7064", "mrqa_naturalquestions-validation-126", "mrqa_triviaqa-validation-2960", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-1586", "mrqa_triviaqa-validation-6860", "mrqa_triviaqa-validation-6177", "mrqa_squad-validation-2368", "mrqa_squad-validation-4978", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-4204", "mrqa_squad-validation-9", "mrqa_triviaqa-validation-1548", "mrqa_naturalquestions-validation-2870", "mrqa_squad-validation-5111", "mrqa_hotpotqa-validation-421", "mrqa_triviaqa-validation-4668", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-2600", "mrqa_squad-validation-2781", "mrqa_hotpotqa-validation-3298", "mrqa_squad-validation-2040", "mrqa_hotpotqa-validation-1246", "mrqa_squad-validation-1652", "mrqa_naturalquestions-validation-3959", "mrqa_triviaqa-validation-1746"], "retrieved_ids": ["mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5997", "mrqa_naturalquestions-train-6794", "mrqa_naturalquestions-train-83658", "mrqa_naturalquestions-train-52318", "mrqa_hotpotqa-validation-118", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-train-23581", "mrqa_naturalquestions-train-19270", "mrqa_naturalquestions-train-79014", "mrqa_naturalquestions-train-21771", "mrqa_naturalquestions-train-63092", "mrqa_triviaqa-validation-3767", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-639", "mrqa_naturalquestions-train-23252", "mrqa_naturalquestions-train-25158", "mrqa_hotpotqa-validation-2340", "mrqa_naturalquestions-validation-9227", "mrqa_naturalquestions-train-24615", "mrqa_squad-validation-9984", "mrqa_naturalquestions-train-84328", "mrqa_triviaqa-validation-3664", "mrqa_triviaqa-validation-1162", "mrqa_squad-validation-8980", "mrqa_naturalquestions-train-167", "mrqa_naturalquestions-train-23581", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-52872", "mrqa_naturalquestions-train-30259", "mrqa_naturalquestions-train-12547"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 94, "before_eval": {"predictions": ["nucleus", "evil", "tin", "sino-Tibetan", "southeast of the city", "in the southwestern part of the island", "on the French island of Guadeloupe in the Lesser Antilles, mainly in the commune of Deshaies ( which doubles for the town of Honor\u00e9 on the fictional island of Saint Marie )", "Statute of Rageman", "1938", "63%", "outside the United States and Canada", "2015", "90% to 93% O2", "approximately 11 %", "special sovereignty", "Chen's theorem", "kiel canal", "short-tempered", "1987", "dastardly & muttley", "governing body for pharmacy health care professionals", "qc", "1968", "chief petty officer", "led about 1,500 army troops and provincial militia", "The Taliban", "June 11, 1986", "Cork, Portarlington, Lisburn, Waterford and Youghal", "Sukhvinder Singh, Mahalaxmi Iyer and Vijay Prakash", "Tom Coburn", "The River Wild", "the ground water level fell significantly. Dead branches dried up and the amount of forests on the flood plains decreased sharply"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3013118090393906}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43750000000000006, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.923076923076923, 0.4444444444444445, 0.0, 0.0, 0.08695652173913045]}}, "error_ids": ["mrqa_naturalquestions-validation-366", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-2404", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-7787", "mrqa_triviaqa-validation-2249", "mrqa_naturalquestions-validation-2624", "mrqa_hotpotqa-validation-4506", "mrqa_squad-validation-3677", "mrqa_naturalquestions-validation-5420", "mrqa_hotpotqa-validation-562", "mrqa_squad-validation-2486", "mrqa_naturalquestions-validation-1382", "mrqa_triviaqa-validation-3230", "mrqa_triviaqa-validation-5584", "mrqa_hotpotqa-validation-2328", "mrqa_naturalquestions-validation-4365", "mrqa_squad-validation-10196", "mrqa_squad-validation-3306", "mrqa_naturalquestions-validation-7496", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-659", "mrqa_squad-validation-9252"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 95, "before_eval": {"predictions": ["the \"richest 1 percent in the United States", "universal ruler", "red", "alain giresse", "the Italian Alps", "ideal", "The X-Files", "elephant slamander", "the American newspaper magnate William Randolph Hearst, Chicago tycoons Samuel Insull and Harold McCormick", "parashah ( or parshah / p\u0251\u02d0r\u0283\u0259 / or parsha )", "the s - block", "World Music Awards", "six", "8\u20134\u20134 system", "exposed to scrutiny", "73", "banned the growing of coffee", "no French regular army troops were stationed in North America", "no contest", "stimulated his brain cells", "the bean-nighe", "Daniel Ken", "the 2015 site, the Straits Course at Whistling Straits", "The first player to win four Super Bowl rings was tight - end Marv Fleming", "corrugated", "11", "Dean Stanton", "the Catholic Monarchs of Castile and Aragon", "a situation of relatively stagnant wages for the working class", "first adopted by the university's science club in 1886", "the Jurchen Aisin Gioro clan", "danish"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2686762479114453}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4210526315789474, 0.0, 0.28571428571428575, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.16666666666666666, 0.5714285714285715, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-7459", "mrqa_triviaqa-validation-5295", "mrqa_hotpotqa-validation-717", "mrqa_squad-validation-9135", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-879", "mrqa_naturalquestions-validation-6129", "mrqa_naturalquestions-validation-3546", "mrqa_naturalquestions-validation-1688", "mrqa_hotpotqa-validation-4523", "mrqa_triviaqa-validation-1838", "mrqa_naturalquestions-validation-2159", "mrqa_squad-validation-8288", "mrqa_squad-validation-10138", "mrqa_squad-validation-6914", "mrqa_squad-validation-1445", "mrqa_triviaqa-validation-2900", "mrqa_hotpotqa-validation-1234", "mrqa_naturalquestions-validation-5267", "mrqa_naturalquestions-validation-3093", "mrqa_triviaqa-validation-1682", "mrqa_naturalquestions-validation-2806", "mrqa_naturalquestions-validation-9985", "mrqa_squad-validation-7184", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-9639", "mrqa_triviaqa-validation-4796"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 96, "before_eval": {"predictions": ["iron (iron(III) oxide Fe2O3, in hematite and rust) and calcium carbonate (in limestone)", "Marcus Atilius Regulus", "an Islamic shrine located on the Temple Mount in the Old City of Jerusalem", "Public-Private Partnering", "the Greek word doxa ( \u03b4\u03cc\u03be\u03b1 )", "microcomputer", "Bulgaria", "4,577 nautical miles (8,477 km)", "Air Force, Army, Navy and other services", "Mr. Destiny", "Renhe Sports Management Ltd, 100 % owned by Xiu Li Dai and Yongge Dai. 25 % Owned by Narin Niruttinanon", "philadelphia", "salvation and subsequently eternal life is not earned by good deeds but is received only as a free gift of God's grace through faith in Jesus Christ as redeemer from sin", "hydrogen and helium", "they were accepted and allowed to worship freely", "the cannonball ( assumed constant ) v", "2006", "blind registration certificate", "the Ministry of War", "probably born in 1162 in Del\u00fc\u00fcn Boldog, near Burkhan Khaldun mountain and the Onon and Kherlen rivers in modern-day northern Mongolia", "el Che or simply Che", "924", "acts as a primer, by polymerizing the first few glucose molecules", "1971", "the oldest street in the United States of America", "American Christian rock band Needtobreathe", "the Iranian Islamic Revolution and apolitical Islam was a historical fluke of the \"short-lived era of the heyday of secular Arab nationalism between 1945 and 1970\"", "April 20, 1983", "cake or biscuit", "inverted", "electromagnetic", "kolinio Epeli Vanuacicila Rabuka and Salote Lomaloma Rabuka"], "metric_results": {"EM": 0.3125, "QA-F1": 0.4282358776844071}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, false, false, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.14285714285714288, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.4444444444444445, 0.0, 0.2857142857142857, 0.0, 0.23529411764705882, 0.42857142857142855, 1.0, 0.0, 1.0, 0.0, 1.0, 0.09523809523809523, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.16666666666666666, 1.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_squad-validation-3504", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-9323", "mrqa_triviaqa-validation-3165", "mrqa_squad-validation-3876", "mrqa_hotpotqa-validation-5639", "mrqa_triviaqa-validation-66", "mrqa_naturalquestions-validation-2208", "mrqa_triviaqa-validation-113", "mrqa_squad-validation-2100", "mrqa_squad-validation-3667", "mrqa_naturalquestions-validation-7297", "mrqa_triviaqa-validation-5093", "mrqa_squad-validation-6158", "mrqa_triviaqa-validation-7768", "mrqa_triviaqa-validation-3771", "mrqa_squad-validation-9574", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-4414", "mrqa_squad-validation-10311", "mrqa_triviaqa-validation-3409"], "retrieved_ids": ["mrqa_naturalquestions-train-60208", "mrqa_naturalquestions-train-58973", "mrqa_naturalquestions-train-49469", "mrqa_triviaqa-validation-2894", "mrqa_naturalquestions-train-73950", "mrqa_naturalquestions-train-44589", "mrqa_naturalquestions-train-27938", "mrqa_naturalquestions-train-69167", "mrqa_naturalquestions-train-41656", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-train-85910", "mrqa_naturalquestions-train-7332", "mrqa_squad-validation-10447", "mrqa_naturalquestions-train-37905", "mrqa_naturalquestions-train-79932", "mrqa_triviaqa-validation-667", "mrqa_naturalquestions-train-64545", "mrqa_naturalquestions-train-19223", "mrqa_triviaqa-validation-7215", "mrqa_triviaqa-validation-1306", "mrqa_triviaqa-validation-4668", "mrqa_naturalquestions-train-33360", "mrqa_naturalquestions-train-13167", "mrqa_naturalquestions-train-37120", "mrqa_naturalquestions-train-70361", "mrqa_triviaqa-validation-2130", "mrqa_naturalquestions-train-34076", "mrqa_triviaqa-validation-2938", "mrqa_triviaqa-validation-5997", "mrqa_naturalquestions-train-80097", "mrqa_squad-validation-2448", "mrqa_triviaqa-validation-4681"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 97, "before_eval": {"predictions": ["6th", "good work designed to attract God's favor", "suburb", "The National Era", "American Civil War", "blackberries", "July 1, 2005", "a few drops", "a little girl ( Addy Miller ), but she turns out to be a zombie", "new zealand", "Vice president", "postero - medially towards the optic chiasm", "the Continental Edison Company in France", "nonviolence", "The 1911 Encyclop\u00e6dia Britannica describes the Gararish as a semi-nomadic, semi-agricultural tribe", "Jai Courtney", "difficulties of the pulmonary circulation", "Galicia", "snob", "science fiction", "13 years and 48 days", "induction motor", "b\u00f4j B\u00f4j", "Nickelback", "The son of Montague", "the \"father of the Mongols\"", "indeed caused by chlorine and bromine from manmade organohalogens", "a combined concert/lecture by British progressive folk-rock band Gryphon", "australia", "british", "Psych", "multiple alternative realities"], "metric_results": {"EM": 0.125, "QA-F1": 0.2620135073260073}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 0.0, 0.0, 0.3076923076923077, 0.0, 1.0, 0.3333333333333333, 1.0, 0.3076923076923077, 0.0, 0.0, 0.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.19999999999999998, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4113", "mrqa_squad-validation-2262", "mrqa_hotpotqa-validation-3785", "mrqa_naturalquestions-validation-2536", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-7679", "mrqa_triviaqa-validation-5865", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-3358", "mrqa_squad-validation-1240", "mrqa_squad-validation-6859", "mrqa_hotpotqa-validation-4389", "mrqa_naturalquestions-validation-1680", "mrqa_hotpotqa-validation-1905", "mrqa_triviaqa-validation-636", "mrqa_hotpotqa-validation-2802", "mrqa_squad-validation-359", "mrqa_squad-validation-1423", "mrqa_triviaqa-validation-2161", "mrqa_naturalquestions-validation-7095", "mrqa_hotpotqa-validation-212", "mrqa_naturalquestions-validation-654", "mrqa_squad-validation-5359", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-6540", "mrqa_hotpotqa-validation-2271", "mrqa_naturalquestions-validation-2729"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 98, "before_eval": {"predictions": ["the most devastating stock market crash in the history of the United States ( acting as the most significant predicting indicator of the Great Depression ), when taking into consideration the full extent and duration of its after effects", "SKUM", "S6 Edge+", "frassati", "the eyespot allows the organism to move in response to light, often toward the light to assist in photosynthesis, and to predict day and night, the primary function of circadian rhythms", "france", "fossil sequences in which there was datable material, converting the old relative ages into new absolute ages", "heaviest album of all", "classical music", "in San Francisco, California with offices in New York City and Atlanta", "high unemployment, poverty, low profits, deflation, plunging farm incomes, and lost opportunities for economic growth and personal advancement", "ex as a noun is assumed to refer to a former sexual or romantic partner, especially a former spouse", "henry ivy", "phoenician", "people had rights to empower them to become economically and socially active", "foot-oriented steps combined with fluid movements in the torso", "music licensing contracts", "the Baltimore teenagers Ivan Ashford, Markel Steele, Cameron Brown, Tariq Al - Sabir and Avery Bargasse", "kent", "Sultans", "the end of the post-war communist control of the country and the reintroduction of a free-market economy", "wild palms", "less than the three million base pair Synechococcus genome, but much larger than the approximately 150,000 base pair genome of the more assimilated chloroplast", "shaft", "New York", "indian hall", "Germ\u00e1n Efromovich", "a limited period of time in exchange for detailed public disclosure of an invention", "kabuki", "Stritch", "It is between the three towns of Doncaster, Scunthorpe and Gainsborough, in the traditional West Riding of Lindsey.", "1908"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2534680016828605}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true], "QA-F1": [0.4878048780487805, 1.0, 0.0, 0.0, 0.10256410256410256, 0.0, 0.2222222222222222, 1.0, 0.0, 0.4, 0.09999999999999999, 0.47619047619047616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.07407407407407407, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.10526315789473684, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8837", "mrqa_hotpotqa-validation-2978", "mrqa_triviaqa-validation-6634", "mrqa_naturalquestions-validation-2213", "mrqa_triviaqa-validation-1952", "mrqa_squad-validation-5008", "mrqa_triviaqa-validation-2353", "mrqa_hotpotqa-validation-2856", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-10707", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2725", "mrqa_squad-validation-4433", "mrqa_hotpotqa-validation-5136", "mrqa_triviaqa-validation-1816", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-6382", "mrqa_squad-validation-992", "mrqa_triviaqa-validation-5737", "mrqa_squad-validation-8802", "mrqa_triviaqa-validation-2414", "mrqa_hotpotqa-validation-5724", "mrqa_triviaqa-validation-4769", "mrqa_naturalquestions-validation-688", "mrqa_triviaqa-validation-4550", "mrqa_naturalquestions-validation-886", "mrqa_hotpotqa-validation-1533"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 99, "before_eval": {"predictions": ["Ron Grainer", "india", "elstow", "he sent missionaries, backed by a fund to financially reward converts to Catholicism", "Gender pay gap", "Woody's horse", "May 5, 1939", "economic growth by collecting resources from colonies", "in 1997 by Bloomsbury", "October 1, 2017", "Washington, D.C.", "wick", "Toronto, Ontario, Canada", "a close and special if unknown connection with the royal nunnery of Shaftesbury (Dorset) founded by King Alfred", "France's claim to the region was superior to that of the British", "a balance sensor consisting of a statolith", "india", "field trips, supervise study halls, help with the organization of school functions, and serve as supervisors for extracurricular activities", "Prince Nikolai Sergeyevich Trubetzkoy", "an eccentric U.S. saloon-keeper and Justice of the Peace", "by the studies and developments department of the French firm R2E Micral in 1980 at the request of the company CCMC specializing in payroll and accounting", "the end of the 18th century, and in most areas was at its peak in the approximate period from 1800 to 1850", "based on the words spoken to Adam and Eve after their sin, reminds worshippers of their sinfulness and mortality and thus, implicitly, of their need to repent in time", "the author recounts how his own opinions changed about that line when he talks to the different people about their beliefs", "Chlo\u00eb Alexandra Adele Emily Agnew", "india", "india", "at most one prime number", "porto", "Bhaktivedanta Manor", "largest known species of jellyfish", "endothermic amniotes distinguished from reptiles ( including birds) by the possession of a neocortex (a region of the brain)"], "metric_results": {"EM": 0.1875, "QA-F1": 0.38098721377253986}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.923076923076923, 0.5, 0.6666666666666666, 1.0, 0.4444444444444445, 0.4, 1.0, 1.0, 0.0, 0.5, 0.0, 0.14814814814814817, 1.0, 0.0, 0.19999999999999998, 0.6666666666666666, 0.25, 0.24000000000000002, 0.9047619047619047, 0.0, 0.34782608695652173, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-7714", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-4903", "mrqa_squad-validation-3130", "mrqa_squad-validation-7449", "mrqa_triviaqa-validation-7414", "mrqa_squad-validation-9926", "mrqa_naturalquestions-validation-3651", "mrqa_triviaqa-validation-7769", "mrqa_hotpotqa-validation-4287", "mrqa_hotpotqa-validation-1086", "mrqa_squad-validation-10231", "mrqa_triviaqa-validation-4000", "mrqa_squad-validation-1870", "mrqa_hotpotqa-validation-5590", "mrqa_hotpotqa-validation-4961", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-251", "mrqa_hotpotqa-validation-4254", "mrqa_triviaqa-validation-6825", "mrqa_triviaqa-validation-6226", "mrqa_triviaqa-validation-4125", "mrqa_hotpotqa-validation-5833", "mrqa_naturalquestions-validation-2794", "mrqa_hotpotqa-validation-2779"], "retrieved_ids": ["mrqa_naturalquestions-train-44763", "mrqa_naturalquestions-train-49380", "mrqa_hotpotqa-validation-2682", "mrqa_squad-validation-9984", "mrqa_triviaqa-validation-2959", "mrqa_naturalquestions-train-72954", "mrqa_naturalquestions-train-50481", "mrqa_triviaqa-validation-6860", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-train-45978", "mrqa_triviaqa-validation-2007", "mrqa_naturalquestions-train-6991", "mrqa_naturalquestions-train-33739", "mrqa_naturalquestions-train-81209", "mrqa_triviaqa-validation-7768", "mrqa_naturalquestions-train-68269", "mrqa_naturalquestions-train-48741", "mrqa_naturalquestions-train-61820", "mrqa_naturalquestions-train-36750", "mrqa_naturalquestions-train-544", "mrqa_naturalquestions-train-31209", "mrqa_triviaqa-validation-1053", "mrqa_naturalquestions-train-811", "mrqa_naturalquestions-train-25813", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-3055", "mrqa_squad-validation-7445", "mrqa_triviaqa-validation-6901", "mrqa_naturalquestions-train-71503", "mrqa_naturalquestions-train-33455", "mrqa_naturalquestions-validation-6912"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}], "final_eval_results": {"overall_oncoming_test": {"EM": 0.1721875, "QA-F1": 0.2857546847983741}, "overall_error_number": 2649, "overall_instant_fixing_rate": 0.0, "final_instream_test": {"EM": 0.6828125, "QA-F1": 0.748353340381408}, "final_upstream_test": {"EM": 0.718, "QA-F1": 0.8237365911815505}}}