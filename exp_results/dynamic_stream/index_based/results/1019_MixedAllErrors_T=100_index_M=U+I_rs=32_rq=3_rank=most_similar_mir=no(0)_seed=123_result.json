{"model_update_steps": 4640, "method_class": "index_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, index_rank_method='most_similar', indexing_args_path='exp_results/supervision_data/1012_dm_simple.train_args.json', indexing_method='bart_index', inference_query_size=1, init_memory_cache_path='exp_results/data_streams/bart_index.init_memory.pkl', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/index_based/ckpt_dir/1019_MixedAllErrors_T=100_index_M=U+I_rs=32_rq=3_rank=most_similar_mir=no(0)_seed=123_ckpts/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/index_based/ckpt_dir/1019_MixedAllErrors_T=100_index_M=U+I_rs=32_rq=3_rank=most_similar_mir=no(0)_seed=123_ckpts/', replay_candidate_size=0, replay_frequency=3, replay_size=32, save_all_ckpts=0, skip_instant_eval=True, total_steps=10000, use_mir=False, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, max_input_length=888, max_output_length=50, max_timecode=100, num_beams=4, pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.upstream_eval.jsonl', predict_batch_size=16, replay_stream_json_path='', sampled_upstream_json_path='exp_results/data_streams/mrqa.nq_train.memory.jsonl', task_name='mrqa_naturalquestions', train_batch_size=8, use_sampled_upstream=True)", "online_eval_results": [{"timecode": 0, "before_eval": {"predictions": ["Raymond Briggs' 1978 children's book 'The Snowman", "the acceleration due to gravity decreased as an inverse square law", "a marquetry commode by the \u00e9b\u00e9niste Jean Henri Riesener dated c1780", "a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work", "lymphocyte", "Chinghiz", "Doctor Who and the Daleks in the Seven Keys to Doomsday", "Super Bowl 50 Host Committee", "a satirical television comedy programme on BBC Television in 1962 and 1963", "dynasty", "Br'er Rabbit", "Bodhi Natural Health Products", "Amphitrite Goddess of the Sea", "the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ), the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho", "March 14 ( 3 / 14 in the month / day format ) since 3, 1, and 4 are the first three significant digits of \u03c0", "the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing", "Horace Rumpole", "combustion", "A Sunday Afternoon on the Island of La Grande Jatte", "nobody knows for sure how a do-over in golf came to be called a mulligan", "Captain Meriwether Lewis's 30th birthday", "An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses", "2011", "electric eels", "before the first year begins", "New Jerusalem", "Samantha Spiro", "2013", "Ronnie Biggs", "Steve Carell as Felonious Gru, the former villain turned Anti-Villain League agent, Margo, Edith, and Agnes'adoptive father, and Lucy's husband", "an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "the assassination of US President John F. Kennedy"], "metric_results": {"EM": 0.0, "QA-F1": 0.0812722081839729}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.4799999999999999, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.11764705882352942, 0.15384615384615383]}}, "error_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-10322", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-2376", "mrqa_squad-validation-6677", "mrqa_squad-validation-6303", "mrqa_squad-validation-7821", "mrqa_squad-validation-392", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9688", "mrqa_triviaqa-validation-1551", "mrqa_squad-validation-3478", "mrqa_triviaqa-validation-5937", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-2730", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-4185", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3241", "mrqa_triviaqa-validation-7369", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-validation-7017", "mrqa_squad-validation-7746"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 1, "before_eval": {"predictions": ["the town of Acolman, just north of Mexico City", "Iroquois", "a natural extension of capitalism that arose from need for capitalist economies to constantly expand investment, material resources and manpower in such a way that necessitated colonial expansion", "The Darling Buds of May", "Virginia Wade", "Gary Morris", "the anterolateral system", "1966", "for scientific observation", "The Ronseal Phrase - Does Exactly What it Says on the Tin", "The Stock Market crash in New York", "New York Stadium", "norman Tebbit", "continental units", "john Forster", "Comptroller General of the Receipt and Issue of Her Majesty's Exchequer", "Sergio P\u00e9rez", "River Welland", "The Concubine", "1543", "the final revelation of God the Final Testament", "Vigor", "a policeman who investigates a series of mysterious killings and illnesses", "glowed even when turned off", "Florence Nightingale", "Budweiser", "numb3rs", "George Cross", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object", "The check written by DC Comics to Jerry Siegel and Joe Shuster for the exclusive rights to their then-new character, Superman", "May and June 2010"], "metric_results": {"EM": 0.15625, "QA-F1": 0.22138917914779982}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.06896551724137931, 0.0, 0.0, 1.0, 0.0, 0.0, 0.14285714285714288, 0.19999999999999998, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.07407407407407407, 0.1904761904761905, 0.4]}}, "error_ids": ["mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-5144", "mrqa_squad-validation-10015", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-5406", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-validation-1364", "mrqa_hotpotqa-validation-14", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-3971", "mrqa_squad-validation-2629", "mrqa_triviaqa-validation-2210", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-5802", "mrqa_triviaqa-validation-2096", "mrqa_naturalquestions-validation-1864", "mrqa_squad-validation-10410", "mrqa_triviaqa-validation-2367", "mrqa_hotpotqa-validation-3774"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 2, "before_eval": {"predictions": ["MS Kronprins Harald", "sports, among them cricket, rallying, football, rugby union and boxing", "Puritanism", "+, -, *, and / keys", "2009", "in different parts of the globe", "marioneth and Llantisilly Rail Traction Company Limited", "acetic acid", "John II Casimir Vasa", "marries Lord Darnley", "A55 North Wales Expressway", "phylum with relatively few species", "`` Everywhere '' is a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "technological advances to work from their homes", "is not yet a senior", "Bothtec", "Terry Reid", "available information about climate change based on published sources", "sept Princesses", "North America", "Andr\u00e9 3000", "a single veteran as Commander, with two rookies", "the Aten, a representation of the Egyptian god, Ra", "President Theodore Roosevelt", "the fourth season", "four", "the Western Bloc ( the United States, its NATO allies and others )", "in the 1970s", "CarmenCarmen (", "Matt Winer", "1688 and 1689", "the Pacific across the Amazonas Basin"], "metric_results": {"EM": 0.09375, "QA-F1": 0.25825312351047647}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.8235294117647058, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.0, 0.0, 0.4, 0.28571428571428575, 0.1, 0.13333333333333333, 0.0, 1.0, 0.0, 0.1818181818181818, 0.0, 0.4444444444444445, 0.4, 0.4444444444444445, 1.0, 0.4, 0.0, 1.0, 0.3636363636363636, 0.3333333333333333, 0.0, 0.4, 0.0, 0.4]}}, "error_ids": ["mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-8542", "mrqa_squad-validation-7149", "mrqa_naturalquestions-validation-10364", "mrqa_squad-validation-4253", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-validation-1202", "mrqa_hotpotqa-validation-3632", "mrqa_triviaqa-validation-1935", "mrqa_hotpotqa-validation-1888", "mrqa_squad-validation-4456", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-8448", "mrqa_triviaqa-validation-2136", "mrqa_naturalquestions-validation-539", "mrqa_squad-validation-8513", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-validation-5502", "mrqa_hotpotqa-validation-2679", "mrqa_squad-validation-4019", "mrqa_naturalquestions-validation-2501", "mrqa_hotpotqa-validation-1376", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-683", "mrqa_triviaqa-validation-2722", "mrqa_hotpotqa-validation-4367", "mrqa_squad-validation-3113", "mrqa_squad-validation-4283"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 3, "before_eval": {"predictions": ["id", "baijan", "c + angle d = 180 degrees", "between 27 July and 7 August 2022", "New York", "b b", "2006 British Academy Television Award for Best Drama Series", "Least of the Great Powers", "usually restricted to the lower motor neurons, the efferent nerves that directly innervate muscles", "babbage", "bums", "Fred Archer", "coronary thrombosis", "bollywood", "Overtime", "Sir Henry Cole", "has trouble distinguishing between carbon dioxide and oxygen", "bambi", "cement City, Texas", "the Democratic Unionist Party (DUP )", "23 July 1989", "many educational institutions especially within the US", "gurus often exercising a great deal of control over the lives of their disciples", "for control purposes", "bridges", "Callability", "2.26 GHz quad - core Snapdragon 800 processor", "over 10,000 British and 2,000 old master works", "al - khimar", "proteins", "bile duct", "berenice Abbott"], "metric_results": {"EM": 0.0625, "QA-F1": 0.11316964285714284}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.1, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-5362", "mrqa_naturalquestions-validation-5647", "mrqa_squad-validation-3058", "mrqa_triviaqa-validation-7382", "mrqa_squad-validation-7816", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-2571", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2327", "mrqa_triviaqa-validation-365", "mrqa_squad-validation-1539", "mrqa_triviaqa-validation-3901", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-3189", "mrqa_squad-validation-8832", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-2150", "mrqa_squad-validation-2191", "mrqa_squad-validation-2069", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-6402", "mrqa_naturalquestions-validation-2385", "mrqa_squad-validation-5517", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-5818", "mrqa_triviaqa-validation-6800", "mrqa_triviaqa-validation-2530"], "retrieved_ids": ["mrqa_naturalquestions-train-51238", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-train-66866", "mrqa_naturalquestions-train-43105", "mrqa_naturalquestions-train-49463", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-train-71828", "mrqa_triviaqa-validation-1616", "mrqa_naturalquestions-train-43105", "mrqa_naturalquestions-train-44065", "mrqa_naturalquestions-train-9737", "mrqa_naturalquestions-train-80958", "mrqa_naturalquestions-train-21309", "mrqa_naturalquestions-train-49719", "mrqa_naturalquestions-train-45105", "mrqa_naturalquestions-train-59537", "mrqa_naturalquestions-train-46247", "mrqa_naturalquestions-train-13583", "mrqa_naturalquestions-train-31611", "mrqa_naturalquestions-train-24436", "mrqa_naturalquestions-train-46904", "mrqa_naturalquestions-train-54288", "mrqa_naturalquestions-train-45745", "mrqa_naturalquestions-train-58486", "mrqa_naturalquestions-train-51238", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-train-66866", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-114", "mrqa_hotpotqa-validation-1376", "mrqa_naturalquestions-train-61037", "mrqa_naturalquestions-train-78881", "mrqa_naturalquestions-train-31705", "mrqa_naturalquestions-train-69270", "mrqa_naturalquestions-train-60822", "mrqa_naturalquestions-train-14012", "mrqa_naturalquestions-train-74858", "mrqa_naturalquestions-train-43791", "mrqa_naturalquestions-train-4155", "mrqa_naturalquestions-train-47999", "mrqa_naturalquestions-train-75921", "mrqa_naturalquestions-train-74228", "mrqa_naturalquestions-train-66570", "mrqa_naturalquestions-train-641", "mrqa_naturalquestions-train-64511", "mrqa_naturalquestions-train-2630", "mrqa_naturalquestions-train-19182", "mrqa_naturalquestions-train-73434", "mrqa_naturalquestions-train-30523", "mrqa_triviaqa-validation-1616", "mrqa_naturalquestions-train-86408", "mrqa_naturalquestions-validation-8948", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-train-51350", "mrqa_naturalquestions-train-25755", "mrqa_naturalquestions-train-74958", "mrqa_naturalquestions-train-61719", "mrqa_naturalquestions-train-3517", "mrqa_naturalquestions-train-85086", "mrqa_naturalquestions-train-57484", "mrqa_naturalquestions-train-9896", "mrqa_naturalquestions-train-78461", "mrqa_naturalquestions-train-87281", "mrqa_triviaqa-validation-2722", "mrqa_naturalquestions-train-35149", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-train-66866", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-train-35943", "mrqa_naturalquestions-train-43791", "mrqa_naturalquestions-train-47736", "mrqa_naturalquestions-train-57692", "mrqa_naturalquestions-train-9090", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-train-47660", "mrqa_naturalquestions-train-52882", "mrqa_naturalquestions-train-74521", "mrqa_naturalquestions-train-66951", "mrqa_naturalquestions-train-44892", "mrqa_naturalquestions-train-65943", "mrqa_naturalquestions-train-9107", "mrqa_naturalquestions-train-31830", "mrqa_naturalquestions-train-37120", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-train-54776", "mrqa_naturalquestions-train-36180", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-train-24615", "mrqa_naturalquestions-train-43105", "mrqa_naturalquestions-train-68895", "mrqa_naturalquestions-train-75876", "mrqa_naturalquestions-train-63807", "mrqa_naturalquestions-train-39182", "mrqa_naturalquestions-train-21281", "mrqa_naturalquestions-train-31587", "mrqa_naturalquestions-train-87077", "mrqa_naturalquestions-train-54999", "mrqa_naturalquestions-train-10736", "mrqa_naturalquestions-train-44067", "mrqa_naturalquestions-train-824", "mrqa_naturalquestions-train-25863", "mrqa_naturalquestions-train-24946", "mrqa_naturalquestions-train-49986", "mrqa_naturalquestions-train-43791", "mrqa_naturalquestions-train-67169", "mrqa_naturalquestions-train-73275", "mrqa_naturalquestions-train-14373", "mrqa_naturalquestions-train-34447", "mrqa_naturalquestions-train-72133", "mrqa_naturalquestions-train-83474", "mrqa_naturalquestions-train-67071", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-36503", "mrqa_naturalquestions-train-37993", "mrqa_naturalquestions-train-36180", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-train-4082", "mrqa_naturalquestions-train-62864", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-58830", "mrqa_naturalquestions-train-12547", "mrqa_naturalquestions-train-3814", "mrqa_naturalquestions-train-38744", "mrqa_naturalquestions-train-72496", "mrqa_naturalquestions-train-81469", "mrqa_naturalquestions-train-26710", "mrqa_naturalquestions-train-34415", "mrqa_naturalquestions-train-59943", "mrqa_naturalquestions-train-76391", "mrqa_naturalquestions-train-5448", "mrqa_naturalquestions-train-21309", "mrqa_naturalquestions-train-10736", "mrqa_naturalquestions-train-2187", "mrqa_naturalquestions-train-68703", "mrqa_naturalquestions-train-57407", "mrqa_naturalquestions-train-19954", "mrqa_naturalquestions-train-34340", "mrqa_naturalquestions-train-86071", "mrqa_naturalquestions-train-76861", "mrqa_naturalquestions-train-30550", "mrqa_naturalquestions-train-39157", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-train-51238", "mrqa_naturalquestions-train-68895", "mrqa_naturalquestions-train-49463", "mrqa_naturalquestions-train-717"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 4, "before_eval": {"predictions": ["United Kingdom", "at Nijmegen, over the Waal distributary of the Rhine", "December 9, 2016", "NASA discontinued the manned Block I program, using the Block I spacecraft only for unmanned Saturn V flights", "British progressive folk-rock band Gryphon", "1898", "numa Pompilius", "Moses", "museum", "Tetanus disease", "bounding the time or space", "gem\u2019s", "Alex O'Loughlin", "Eddie Leonski", "Jack", "a mixture of phencyclidine and cocaine", "bunker", "john Depp played him in Tim Burton's film Sweeney Todd", "Detective Eddie Thawne", "All Souls'Day", "1968", "baku", "Catholics", "Mona Vanderwaal", "Geoffrey Boycott", "Pyotr Ilyich Tchaikovsky", "2001 or 2010", "the Dutch Cape Colony in South Africa, the Dutch East Indies, the Caribbean, and several of the English colonies of North America, and Quebec", "comprehension and formulate language", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "steam turbine", "Splodgenessabounds"], "metric_results": {"EM": 0.125, "QA-F1": 0.2499255952380952}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.4, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 0.0, 0.4, 0.0, 0.33333333333333337, 1.0, 0.0, 0.3333333333333333, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09523809523809523, 0.75, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-6399", "mrqa_squad-validation-9358", "mrqa_naturalquestions-validation-5146", "mrqa_squad-validation-3971", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-3647", "mrqa_triviaqa-validation-7253", "mrqa_squad-validation-1688", "mrqa_triviaqa-validation-3808", "mrqa_hotpotqa-validation-1168", "mrqa_hotpotqa-validation-1289", "mrqa_hotpotqa-validation-2944", "mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-5654", "mrqa_squad-validation-3126", "mrqa_naturalquestions-validation-2900", "mrqa_triviaqa-validation-5168", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-3978", "mrqa_squad-validation-3021", "mrqa_naturalquestions-validation-3840", "mrqa_squad-validation-3467"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 5, "before_eval": {"predictions": ["june", "F\u00e9d\u00e9ration Sportive F\u00e9minine Internationale (FSFI )", "a soft wool fabric with a colorful swirled pattern of curved shapes", "Paspahegh Indians", "a delay or obstruction along the pathway that electrical impulses travel in your heart to make it beat", "South Dakota", "2 : 44 p.m. EDT", "swanee or swannee whistle", "sunflowers", "to start fires, hunt, and bury their dead", "India", "Parietal cells ( also known as oxyntic or delomorphous cells )", "placental", "Ready to Die", "june", "imperial rule", "1840", "make a defiant speech, or a speech explaining their actions", "Francis Marion Crawford, Robert Underwood Johnson, Stanford White, Fritz Lowenstein, George Scherff, and Kenneth Swezey", "kinks", "by using net wealth (adding up assets and subtracting debts )", "entropy", "my mind is averse to wedlock because I daily expect the death of a heretic", "8.7 -- 9.2", "China", "2 November 1902", "Inez", "May 7, 2018", "9 October 1940", "Selden", "structural collapses", "a man named Daedalus"], "metric_results": {"EM": 0.0625, "QA-F1": 0.17188614081996434}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5454545454545454, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.13333333333333333, 0.0, 0.47058823529411764, 0.0, 1.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.5]}}, "error_ids": ["mrqa_triviaqa-validation-4725", "mrqa_hotpotqa-validation-2197", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-2020", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3625", "mrqa_naturalquestions-validation-4513", "mrqa_triviaqa-validation-1550", "mrqa_hotpotqa-validation-484", "mrqa_triviaqa-validation-5704", "mrqa_hotpotqa-validation-4621", "mrqa_squad-validation-6733", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-254", "mrqa_squad-validation-7554", "mrqa_squad-validation-10423", "mrqa_squad-validation-2757", "mrqa_naturalquestions-validation-2884", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-1489", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-5848", "mrqa_squad-validation-2531", "mrqa_triviaqa-validation-4560"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 6, "before_eval": {"predictions": ["zinnemann", "to prevent the flame from being blown out", "Illinois", "1998", "city", "the east coast of the island of Menorca", "90-60's", "unaided independent school", "dolph Camilli", "times sign", "BAFTA Television Award", "Juice Newton", "1960", "HTTP Secure ( HTTPS )", "late - September through early January", "kansas", "monatomic", "Palm Springs is popular for its resort feel and nearby open spaces", "june", "toad", "butterflies", "blood", "left coronary artery", "1.1 \u00d7 1011 metric tonnes", "dale", "leaf tissue", "Indian club ATK", "land", "near Grande Comore, Comoros Islands", "rupees", "Norwegian", "burning of fossil fuels"], "metric_results": {"EM": 0.09375, "QA-F1": 0.16335565476190475}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-4165", "mrqa_hotpotqa-validation-5810", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-6887", "mrqa_triviaqa-validation-5261", "mrqa_squad-validation-2659", "mrqa_squad-validation-6947", "mrqa_triviaqa-validation-105", "mrqa_naturalquestions-validation-10356", "mrqa_squad-validation-7819", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-484", "mrqa_triviaqa-validation-5795", "mrqa_squad-validation-3463", "mrqa_squad-validation-2584", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-227", "mrqa_squad-validation-4181", "mrqa_triviaqa-validation-4966", "mrqa_squad-validation-8821", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-10042", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6207", "mrqa_hotpotqa-validation-3919", "mrqa_naturalquestions-validation-6644"], "retrieved_ids": ["mrqa_naturalquestions-train-68895", "mrqa_naturalquestions-train-56348", "mrqa_naturalquestions-train-6991", "mrqa_naturalquestions-train-43511", "mrqa_naturalquestions-train-47902", "mrqa_naturalquestions-train-51417", "mrqa_naturalquestions-train-62864", "mrqa_naturalquestions-train-30649", "mrqa_naturalquestions-train-14250", "mrqa_naturalquestions-train-9516", "mrqa_naturalquestions-train-13165", "mrqa_naturalquestions-train-48406", "mrqa_naturalquestions-train-5353", "mrqa_naturalquestions-train-27214", "mrqa_naturalquestions-train-36601", "mrqa_naturalquestions-train-41714", "mrqa_naturalquestions-train-76729", "mrqa_naturalquestions-train-7638", "mrqa_naturalquestions-train-49545", "mrqa_naturalquestions-train-38547", "mrqa_naturalquestions-train-19234", "mrqa_naturalquestions-train-37002", "mrqa_naturalquestions-train-21934", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-train-17119", "mrqa_naturalquestions-train-11503", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-8009", "mrqa_naturalquestions-train-68269", "mrqa_naturalquestions-train-34003", "mrqa_naturalquestions-train-70639", "mrqa_naturalquestions-train-5473", "mrqa_naturalquestions-train-80300", "mrqa_naturalquestions-train-19002", "mrqa_naturalquestions-train-54596", "mrqa_naturalquestions-train-40837", "mrqa_naturalquestions-train-85620", "mrqa_naturalquestions-train-80384", "mrqa_naturalquestions-train-27270", "mrqa_naturalquestions-train-70948", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-train-24615", "mrqa_naturalquestions-train-47895", "mrqa_naturalquestions-train-10232", "mrqa_naturalquestions-train-36180", "mrqa_naturalquestions-train-18812", "mrqa_naturalquestions-train-60689", "mrqa_naturalquestions-train-34795", "mrqa_naturalquestions-train-82635", "mrqa_naturalquestions-train-28761", "mrqa_naturalquestions-train-43807", "mrqa_naturalquestions-train-7289", "mrqa_naturalquestions-train-37244", "mrqa_naturalquestions-train-26110", "mrqa_naturalquestions-train-42981", "mrqa_naturalquestions-train-33527", "mrqa_naturalquestions-train-22793", "mrqa_naturalquestions-train-8824", "mrqa_naturalquestions-train-74958", "mrqa_naturalquestions-train-26695", "mrqa_naturalquestions-train-1337", "mrqa_naturalquestions-train-80699", "mrqa_naturalquestions-train-60865", "mrqa_naturalquestions-train-25085", "mrqa_naturalquestions-train-56959", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-train-36503", "mrqa_naturalquestions-train-5662", "mrqa_naturalquestions-train-1290", "mrqa_naturalquestions-train-67169", "mrqa_naturalquestions-train-58293", "mrqa_naturalquestions-train-36228", "mrqa_naturalquestions-train-9637", "mrqa_naturalquestions-train-48852", "mrqa_naturalquestions-train-811", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-59986", "mrqa_triviaqa-validation-3901", "mrqa_naturalquestions-train-73802", "mrqa_naturalquestions-train-78116", "mrqa_naturalquestions-train-45778", "mrqa_triviaqa-validation-671", "mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-6683", "mrqa_naturalquestions-train-49463", "mrqa_naturalquestions-train-15604", "mrqa_naturalquestions-train-28545", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-train-9081", "mrqa_naturalquestions-train-5169", "mrqa_naturalquestions-train-19752", "mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-train-27683", "mrqa_triviaqa-validation-5168", "mrqa_naturalquestions-train-9056", "mrqa_naturalquestions-train-50358", "mrqa_naturalquestions-train-46650", "mrqa_naturalquestions-train-35611", "mrqa_naturalquestions-train-23035", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-671", "mrqa_triviaqa-validation-4054", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-train-66570", "mrqa_naturalquestions-train-49927", "mrqa_naturalquestions-train-43139", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-16027", "mrqa_naturalquestions-train-14632", "mrqa_naturalquestions-train-71615", "mrqa_naturalquestions-train-47601", "mrqa_naturalquestions-train-10513", "mrqa_naturalquestions-train-72123", "mrqa_naturalquestions-train-57619", "mrqa_naturalquestions-train-73356", "mrqa_naturalquestions-train-83658", "mrqa_naturalquestions-train-81226", "mrqa_naturalquestions-train-64799", "mrqa_naturalquestions-train-25341", "mrqa_naturalquestions-train-56702", "mrqa_naturalquestions-train-53108", "mrqa_naturalquestions-train-56", "mrqa_naturalquestions-train-21912", "mrqa_naturalquestions-train-63031", "mrqa_naturalquestions-train-21309", "mrqa_naturalquestions-train-21529", "mrqa_naturalquestions-train-73356", "mrqa_naturalquestions-train-54343", "mrqa_naturalquestions-train-77279", "mrqa_naturalquestions-train-50473", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-82843", "mrqa_naturalquestions-train-69155", "mrqa_naturalquestions-train-47928", "mrqa_naturalquestions-train-31449", "mrqa_naturalquestions-train-66825", "mrqa_naturalquestions-train-53017", "mrqa_naturalquestions-train-16437"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 7, "before_eval": {"predictions": ["the wisdom and prudence of certain decisions of procurement", "Maya group", "a few", "Chaplain insignia", "Kairi", "Jacksonville Consolidation", "Ray Milland", "Armenia", "Revelation", "Beyonc\u00e9", "%", "gallantry", "16 million", "1950s", "haulage", "1998", "a priest", "49.2 percent of the Earth's crust", "2001", "family member", "long-term environmental changes", "Learjet", "the unbalanced centripetal force", "Terrell Suggs", "decide on all the motions and amendments", "a voyage of adventure", "Abraham Gottlob Werner", "George", "present-day Charleston, South Carolina", "\"quiescent\" stance", "Adam Karpel", "XIX Corps"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2709077380952381}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.28571428571428575, 0.6666666666666666, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5522", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-983", "mrqa_squad-validation-7296", "mrqa_triviaqa-validation-7415", "mrqa_squad-validation-6297", "mrqa_naturalquestions-validation-859", "mrqa_squad-validation-108", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-1293", "mrqa_squad-validation-7799", "mrqa_hotpotqa-validation-3898", "mrqa_naturalquestions-validation-98", "mrqa_squad-validation-358", "mrqa_squad-validation-3558", "mrqa_hotpotqa-validation-1142", "mrqa_squad-validation-10403", "mrqa_naturalquestions-validation-3474", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-291", "mrqa_triviaqa-validation-781", "mrqa_squad-validation-3181", "mrqa_squad-validation-9598", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-3146"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 8, "before_eval": {"predictions": ["computability theory", "george Ferrari Alfa Romeo", "45.3", "about 5 nanometers across, arranged in rows 6.4 nanometers apart, and shrinks to squeeze the chloroplast", "the eighth and eleventh episodes of the season", "Kyle Busch", "400", "cortisol", "( Latin liberalia studia )", "the Bowland Fells", "Richard, Duke of Gloucester", "Eureka", "1918", "2018", "rother united", "law firm", "Pottawatomie County", "owaita, an Aldabra giant tortoise", "Newton's Law of Gravitation", "The Parish Church of St Andrew", "EastEnders star Danny Dyer becomes London Underground station announcer", "Toronto, Ontario, Canada", "wales", "110 miles (177 km ) from the East River in New York City, along the North Shore of Long Island, to the south", "Kona coast of the island of Hawai\u02bb i about 12 mi south of Kailua-Kona", "Liberal conservatism", "one of the largest gold rushes the world has ever seen", "six degrees of freedom", "not guilty", "psychotherapeutic theories and associated techniques", "Quentin Coldwater", "acidic bogs throughout the cooler regions of the northern hemisphere"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2613753076988371}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.23529411764705882, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.25, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.25, 0.28571428571428575, 0.0, 0.0, 0.0, 0.1904761904761905, 0.2666666666666667, 0.5, 0.3636363636363636, 0.4, 0.0, 0.0, 0.0, 0.2222222222222222]}}, "error_ids": ["mrqa_triviaqa-validation-1521", "mrqa_naturalquestions-validation-6089", "mrqa_squad-validation-8869", "mrqa_naturalquestions-validation-856", "mrqa_triviaqa-validation-7506", "mrqa_naturalquestions-validation-1360", "mrqa_hotpotqa-validation-3161", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-5401", "mrqa_triviaqa-validation-7157", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-validation-3309", "mrqa_squad-validation-10369", "mrqa_squad-validation-5313", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-582", "mrqa_triviaqa-validation-4268", "mrqa_hotpotqa-validation-1021", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-4812", "mrqa_squad-validation-2987", "mrqa_naturalquestions-validation-7906", "mrqa_squad-validation-6915", "mrqa_triviaqa-validation-7767", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-187"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 9, "before_eval": {"predictions": ["I Seek You", "Argentinian", "Ear Institute at the University College London, and Widex, a Danish hearing aid manufacturer", "clangers", "photosynthesis", "a wide range of society figures of the period", "1600 Pennsylvania Avenue", "The Rebel Media", "triplet", "flour", "president", "citizens", "George, Margrave of Brandenburg-Ansbach", "a corruption of the Kamba version", "3D computer-animated comedy film", "a meat cold storage facility", "acting", "C. W. Grafton", "LED illuminated display", "Americans acting under orders", "iPod Classic", "The Girl in the romantic comedy \" My Sassy Girl\" ( 2001 ) (2001 ) one of the highest-grossing Korean comedies of all time", "removal of the waste products of metabolism and to drain the body of used up and broken down components in a liquid and gaseous state", "The Edge of Night", "wood or coal", "pedagogy", "\u039c\u03ad\u03b3\u03b1\u03bd \u0399\u03b5\u03c1\u03cc\u03bd \u03a3\u03c5\u03bd\u03ad\u03ba\u03b4\u03b7\u03bc\u03bf\u03bd", "root respiration", "soils", "drug dealer", "medium and heavy- Duty diesel trucks", "testes"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3365505210515291}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.125, 0.8, 0.0, 0.0, 0.13333333333333333, 0.4, 1.0, 1.0, 0.18181818181818182, 0.8571428571428571, 0.36363636363636365, 0.6666666666666666, 1.0, 0.2857142857142857, 1.0, 0.0, 0.1, 0.1290322580645161, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7272727272727272, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6844", "mrqa_squad-validation-5210", "mrqa_triviaqa-validation-3387", "mrqa_squad-validation-8924", "mrqa_squad-validation-5618", "mrqa_triviaqa-validation-2703", "mrqa_hotpotqa-validation-3428", "mrqa_squad-validation-3442", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-321", "mrqa_squad-validation-8259", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-5128", "mrqa_naturalquestions-validation-754", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1436", "mrqa_naturalquestions-validation-1294", "mrqa_squad-validation-3490", "mrqa_squad-validation-1879", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-5823", "mrqa_naturalquestions-validation-3677"], "retrieved_ids": ["mrqa_naturalquestions-train-7405", "mrqa_naturalquestions-train-53991", "mrqa_naturalquestions-train-79409", "mrqa_naturalquestions-train-9081", "mrqa_naturalquestions-train-30974", "mrqa_naturalquestions-train-55709", "mrqa_naturalquestions-train-38504", "mrqa_naturalquestions-train-12357", "mrqa_naturalquestions-train-54176", "mrqa_naturalquestions-train-33306", "mrqa_triviaqa-validation-3808", "mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4268", "mrqa_naturalquestions-train-51238", "mrqa_triviaqa-validation-1540", "mrqa_squad-validation-8832", "mrqa_naturalquestions-train-69824", "mrqa_naturalquestions-train-9107", "mrqa_naturalquestions-train-44892", "mrqa_naturalquestions-train-74251", "mrqa_naturalquestions-train-20491", "mrqa_hotpotqa-validation-1897", "mrqa_naturalquestions-train-60554", "mrqa_naturalquestions-train-30204", "mrqa_naturalquestions-train-64929", "mrqa_triviaqa-validation-1935", "mrqa_triviaqa-validation-4856", "mrqa_squad-validation-3181", "mrqa_triviaqa-validation-2368", "mrqa_triviaqa-validation-1575", "mrqa_hotpotqa-validation-1968", "mrqa_naturalquestions-train-30259", "mrqa_naturalquestions-train-82478", "mrqa_naturalquestions-train-19660", "mrqa_naturalquestions-train-61414", "mrqa_naturalquestions-train-63815", "mrqa_naturalquestions-train-59761", "mrqa_naturalquestions-train-40979", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-43791", "mrqa_naturalquestions-train-65943", "mrqa_naturalquestions-train-215", "mrqa_naturalquestions-train-34683", "mrqa_naturalquestions-train-47066", "mrqa_naturalquestions-train-13352", "mrqa_naturalquestions-train-20198", "mrqa_naturalquestions-train-63521", "mrqa_naturalquestions-train-13385", "mrqa_naturalquestions-train-40945", "mrqa_naturalquestions-train-20644", "mrqa_naturalquestions-train-63031", "mrqa_naturalquestions-train-70948", "mrqa_naturalquestions-train-59591", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-73356", "mrqa_naturalquestions-train-27887", "mrqa_naturalquestions-train-52050", "mrqa_naturalquestions-train-23512", "mrqa_naturalquestions-train-83148", "mrqa_naturalquestions-train-41112", "mrqa_naturalquestions-train-52189", "mrqa_naturalquestions-train-69829", "mrqa_naturalquestions-train-68967", "mrqa_naturalquestions-train-75255", "mrqa_naturalquestions-train-855", "mrqa_squad-validation-3181", "mrqa_naturalquestions-train-52286", "mrqa_naturalquestions-train-59801", "mrqa_naturalquestions-train-60814", "mrqa_naturalquestions-train-63611", "mrqa_naturalquestions-train-16149", "mrqa_naturalquestions-train-80201", "mrqa_naturalquestions-train-24991", "mrqa_naturalquestions-train-38161", "mrqa_naturalquestions-train-23475", "mrqa_naturalquestions-train-7311", "mrqa_naturalquestions-train-9712", "mrqa_naturalquestions-train-83473", "mrqa_naturalquestions-train-77471", "mrqa_naturalquestions-train-64931", "mrqa_hotpotqa-validation-983", "mrqa_naturalquestions-train-82748", "mrqa_naturalquestions-train-58365", "mrqa_naturalquestions-train-45457", "mrqa_naturalquestions-train-71668", "mrqa_naturalquestions-train-37019", "mrqa_naturalquestions-train-71403", "mrqa_naturalquestions-train-47479", "mrqa_naturalquestions-train-3077", "mrqa_naturalquestions-train-11458", "mrqa_naturalquestions-train-11158", "mrqa_squad-validation-4185", "mrqa_naturalquestions-train-16637", "mrqa_naturalquestions-train-81896", "mrqa_naturalquestions-train-74228", "mrqa_naturalquestions-train-57692", "mrqa_naturalquestions-train-43791", "mrqa_naturalquestions-train-82315", "mrqa_naturalquestions-train-61560", "mrqa_naturalquestions-train-76391", "mrqa_naturalquestions-train-61043", "mrqa_naturalquestions-train-60677", "mrqa_naturalquestions-train-77220", "mrqa_naturalquestions-train-51271", "mrqa_naturalquestions-train-68587", "mrqa_naturalquestions-train-64590", "mrqa_naturalquestions-train-67169", "mrqa_naturalquestions-train-87685", "mrqa_naturalquestions-train-7959", "mrqa_naturalquestions-train-44200", "mrqa_naturalquestions-train-47914", "mrqa_naturalquestions-train-8972", "mrqa_naturalquestions-train-64590", "mrqa_naturalquestions-train-67169", "mrqa_naturalquestions-train-58357", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-5325", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-train-47739", "mrqa_naturalquestions-train-51238", "mrqa_naturalquestions-train-67755", "mrqa_naturalquestions-train-28437", "mrqa_naturalquestions-train-83148", "mrqa_naturalquestions-train-62491", "mrqa_naturalquestions-train-45549", "mrqa_naturalquestions-train-9081", "mrqa_naturalquestions-train-66044", "mrqa_naturalquestions-train-14250", "mrqa_naturalquestions-train-8357", "mrqa_naturalquestions-train-28325"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 10, "before_eval": {"predictions": ["carruthers, Pope, 1857, p. 463", "Walter Reed Army Hospital in Washington, D.C. from 1942 to 1945", "three legal systems", "Las Vegas, Nevada", "status line", "globetrotters", "cruiserweight", "The Merderet was assigned to the U.S. 82nd Airborne Division as Mission Boston, scheduled for 5 hours before the amphibious landings on D - Day", "a Dubliner tried to kill Benito Mussolini", "casket letters", "menhirs", "Victoria, Duchess of Kent", "the base 10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "1947", "the MGM Grand Garden Arena", "Chicago History Museum", "Ronnie Hillman", "formulation of a single all-encompassing definition of the term is extremely difficult, if not impossible", "casket letters", "more than 60 percent of the state's total land surface", "Eagle Ridge Mall", "Pel\u00e9", "reduce pressure on the public food supply", "Monastir", "fire", "Gomer Pyle", "at least 18 or 21 years old ( or have a legal guardian present )", "Ward", "novelist and poet", "Jamestown", "Monet began the paintings in January or early February 1892. He visited Rouen on three occasions and completed 20 views of Rouen Cathedral between March 1892 and Feb-April 1893.", "tree growth stages"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2519071565407772}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.8, 1.0, 0.5, 1.0, 0.07142857142857142, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.14285714285714288, 0.0, 0.19999999999999998, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0689655172413793, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1860", "mrqa_hotpotqa-validation-3149", "mrqa_naturalquestions-validation-1085", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-4162", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-6389", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-10205", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-5682", "mrqa_triviaqa-validation-681", "mrqa_squad-validation-273", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-6119", "mrqa_squad-validation-3018", "mrqa_hotpotqa-validation-897", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-1451", "mrqa_hotpotqa-validation-3976", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-4803", "mrqa_triviaqa-validation-6639", "mrqa_squad-validation-4506"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 11, "before_eval": {"predictions": ["Vince Lombardi", "Arthur Schnitzler's 1926 novella \"Traumnovelle\"", "a Gender pay gap in favor of males in the labor market", "The TEU", "ice melting", "the narrator driving a truck owned by his brother, who died in action in the United States Army", "joseph", "length of their main span ( i.e. the length of suspended roadway between the bridge's towers )", "trams", "cricket", "King Crimson", "the Bulgars", "killed in battle and according to others, he was one of a handful of men captured and later executed", "Volkswagen Beetle", "joseph", "North American Technate", "Queen Elizabeth II", "infection, irritation, or allergies", "The tower is the most - visited paid monument in the world. An average of 25,000 people ascend the tower every day which can result in long queues. Tickets can be purchased online to avoid the long queues", "Town House Galleria", "catfish aquaculture", "atomic number 53", "James and D.J. Looney as Young Sparrow and DJ Dragon Nutz", "Kuwait", "a co-op of grape growers", "victor willsmeron", "viuseppe Verdi", "1952", "Los Angeles Lakers", "advisory speed limits", "Jean F kernel ( 1497 -- 1558 ), a French physician", "the chest, back, shoulders, torso and / or legs"], "metric_results": {"EM": 0.09375, "QA-F1": 0.21929125816993464}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 0.0, 0.23529411764705882, 0.0, 0.14285714285714288, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.17647058823529413, 0.28571428571428575, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.28571428571428575, 0.22222222222222224, 0.25]}}, "error_ids": ["mrqa_triviaqa-validation-5078", "mrqa_hotpotqa-validation-2852", "mrqa_squad-validation-7447", "mrqa_squad-validation-4169", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-6259", "mrqa_squad-validation-1003", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-validation-441", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-5184", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-3208", "mrqa_hotpotqa-validation-3921", "mrqa_triviaqa-validation-792", "mrqa_triviaqa-validation-5526", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6442"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 12, "before_eval": {"predictions": ["Coldplay", "Joe Turano", "fencers", "Margaret Thatcher", "Kevin Harlan as play-by-play announcer, Boomer Esiason and Dan Fouts as color analysts, and James Lofton and Mark Malone as sideline reporters", "August 31, 2014", "The stability, security, and predictability of British law and government enabled Hong Kong to flourish as a centre for international trade", "Athenian", "29 June 1941", "cienfuegos in the Las Villas province of Cuba", "adele Rose", "Forbes", "Johnson", "John Carroll Lynch", "ferdinand", "Orwell", "Czech Kingdom", "Bob Hill", "not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence", "for creative reasons and `` not a reflection '' of the actress'performance", "immunological memory", "under the tutelage of his uncle Juan Nepomuceno Guerra", "a musician", "tunisia", "December 1, 1969", "american", "jK Rowling", "California State Automobile Association", "\" faith alone justifies\"", "Cinderella", "the crew noticed a strange odor in their spacesuits, which delayed the sealing of the hatch", "due to a fear of seeming rude"], "metric_results": {"EM": 0.1875, "QA-F1": 0.31814230552031736}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.37037037037037035, 0.0, 0.34782608695652173, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.11764705882352941, 1.0, 1.0, 0.2424242424242424, 0.4, 1.0, 0.2222222222222222, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.5, 0.631578947368421]}}, "error_ids": ["mrqa_naturalquestions-validation-6167", "mrqa_triviaqa-validation-1671", "mrqa_squad-validation-568", "mrqa_hotpotqa-validation-2680", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-1758", "mrqa_hotpotqa-validation-4263", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-6902", "mrqa_hotpotqa-validation-1099", "mrqa_squad-validation-10202", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-2737", "mrqa_squad-validation-9793", "mrqa_naturalquestions-validation-5175", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-7371", "mrqa_squad-validation-2812", "mrqa_squad-validation-2372", "mrqa_squad-validation-3935", "mrqa_squad-validation-6924"], "retrieved_ids": ["mrqa_naturalquestions-train-39827", "mrqa_naturalquestions-train-85608", "mrqa_naturalquestions-train-60641", "mrqa_naturalquestions-train-26147", "mrqa_naturalquestions-train-12075", "mrqa_hotpotqa-validation-1201", "mrqa_naturalquestions-train-49463", "mrqa_naturalquestions-train-57692", "mrqa_naturalquestions-train-9756", "mrqa_naturalquestions-train-40321", "mrqa_naturalquestions-train-32005", "mrqa_naturalquestions-train-76265", "mrqa_squad-validation-4019", "mrqa_naturalquestions-train-78304", "mrqa_naturalquestions-train-855", "mrqa_naturalquestions-train-28130", "mrqa_naturalquestions-train-40755", "mrqa_naturalquestions-train-12142", "mrqa_naturalquestions-train-10655", "mrqa_naturalquestions-train-41935", "mrqa_naturalquestions-train-78849", "mrqa_naturalquestions-train-8707", "mrqa_naturalquestions-train-30469", "mrqa_naturalquestions-train-84075", "mrqa_naturalquestions-train-80228", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-13963", "mrqa_naturalquestions-train-49757", "mrqa_squad-validation-3181", "mrqa_naturalquestions-train-66044", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-train-20829", "mrqa_naturalquestions-train-54414", "mrqa_naturalquestions-train-81122", "mrqa_naturalquestions-train-43631", "mrqa_triviaqa-validation-1616", "mrqa_naturalquestions-train-36180", "mrqa_naturalquestions-train-24696", "mrqa_naturalquestions-train-68895", "mrqa_triviaqa-validation-2722", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-1494", "mrqa_naturalquestions-train-68895", "mrqa_triviaqa-validation-1935", "mrqa_naturalquestions-train-68147", "mrqa_naturalquestions-train-43677", "mrqa_naturalquestions-train-87031", "mrqa_naturalquestions-train-62765", "mrqa_naturalquestions-train-10243", "mrqa_naturalquestions-train-35255", "mrqa_naturalquestions-train-39276", "mrqa_naturalquestions-train-31580", "mrqa_naturalquestions-train-32358", "mrqa_naturalquestions-train-69270", "mrqa_naturalquestions-train-2860", "mrqa_naturalquestions-train-57162", "mrqa_naturalquestions-train-61318", "mrqa_naturalquestions-train-59266", "mrqa_naturalquestions-train-18324", "mrqa_naturalquestions-train-51238", "mrqa_triviaqa-validation-3808", "mrqa_naturalquestions-train-21309", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-1494", "mrqa_naturalquestions-train-83023", "mrqa_naturalquestions-train-34683", "mrqa_naturalquestions-train-33189", "mrqa_naturalquestions-train-9888", "mrqa_naturalquestions-train-16002", "mrqa_squad-validation-10015", "mrqa_naturalquestions-train-8707", "mrqa_naturalquestions-train-80333", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-train-9637", "mrqa_naturalquestions-train-70950", "mrqa_naturalquestions-train-19223", "mrqa_naturalquestions-train-34992", "mrqa_naturalquestions-train-81286", "mrqa_naturalquestions-train-7332", "mrqa_naturalquestions-train-45471", "mrqa_naturalquestions-train-84243", "mrqa_naturalquestions-train-7909", "mrqa_naturalquestions-train-50525", "mrqa_naturalquestions-train-64708", "mrqa_naturalquestions-train-15892", "mrqa_naturalquestions-train-13520", "mrqa_naturalquestions-train-51350", "mrqa_naturalquestions-train-22388", "mrqa_naturalquestions-train-64931", "mrqa_naturalquestions-train-19752", "mrqa_triviaqa-validation-1935", "mrqa_naturalquestions-train-38514", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-17468", "mrqa_naturalquestions-train-59259", "mrqa_naturalquestions-train-55159", "mrqa_naturalquestions-train-28557", "mrqa_naturalquestions-train-35789", "mrqa_naturalquestions-train-39182", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-6639", "mrqa_triviaqa-validation-1935", "mrqa_naturalquestions-train-80827", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-train-67105", "mrqa_naturalquestions-train-51350", "mrqa_naturalquestions-train-84103", "mrqa_triviaqa-validation-1551", "mrqa_naturalquestions-train-60328", "mrqa_naturalquestions-train-74228", "mrqa_naturalquestions-train-21826", "mrqa_naturalquestions-train-6133", "mrqa_naturalquestions-train-29886", "mrqa_naturalquestions-train-26091", "mrqa_naturalquestions-train-56348", "mrqa_naturalquestions-train-14699", "mrqa_naturalquestions-train-30308", "mrqa_naturalquestions-train-40409", "mrqa_naturalquestions-train-59051", "mrqa_naturalquestions-train-82315", "mrqa_naturalquestions-train-42632", "mrqa_squad-validation-3181", "mrqa_naturalquestions-train-2426", "mrqa_naturalquestions-train-78885", "mrqa_naturalquestions-train-39325", "mrqa_naturalquestions-train-17000", "mrqa_naturalquestions-train-74228", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-train-47619"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 13, "before_eval": {"predictions": ["Sister, Sister", "president of Guggenheim Partners", "Jason Lee", "Napoleon's army", "mourilandland", "3.7 percent of the entire student population", "a negative effect", "Matt Willis and Charlie Quirke", "Tenacious D", "Jerry Ekandjo ( until February 2018 ), Erastus Utoni   Deputy : Agnes Tjongarero", "discipline problems with the Flight Director's orders during their flight", "Heogony", "April 6, 2010", "amyotrophic lateral sclerosis (ALS)", "a gimmick called \"Odorama\". whereby viewers could smell what they saw on screen through scratch and sniff cards", "a pioneer in watch design, manufacturing and distribution", "October 17, 1938", "Torah or Bible", "Sicily", "He was a contemporary of Phil Hill, who went on to become the first and only U.S. born world grand prix champion.", "a jazz funeral without a body", "mid November", "Facebook", "beigel", "Tim \"Ripper\" Owens, singer in a Judas Priest tribute band who was chosen to replace singer Rob Halford when he left the band", "Issaquah, Washington (a suburb of Seattle)", "King George's War", "cheated on Miley", "Punk", "Fort Snelling, Minnesota", "a pinhole or lens to project an image of the scene outside upside-down onto a viewing surface", "infrequent rain"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2995332792207792}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true], "QA-F1": [0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.33333333333333337, 0.3636363636363636, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.25, 0.33333333333333337, 1.0, 0.2222222222222222, 0.0, 0.3333333333333333, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-1509", "mrqa_naturalquestions-validation-1135", "mrqa_squad-validation-830", "mrqa_triviaqa-validation-7638", "mrqa_squad-validation-7042", "mrqa_triviaqa-validation-6944", "mrqa_hotpotqa-validation-4058", "mrqa_naturalquestions-validation-6445", "mrqa_squad-validation-4060", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-4486", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1178", "mrqa_squad-validation-1850", "mrqa_hotpotqa-validation-800", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-2896", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-1932", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7310", "mrqa_hotpotqa-validation-3669", "mrqa_triviaqa-validation-6913"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 14, "before_eval": {"predictions": ["2002 Hong Kong comedy film directed by Raymond Yip, and starring Francis Ng, Michelle Reis and Daniel Wu", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "maryland", "rother's", "FX option", "electromagnetic waves", "a Wahhabi/ Salafi", "to be good", "a 3D system that made use of the Pulfrich effect requiring glasses with one darkened lens", "Surveyor 3 unmanned lunar probe", "January 1981", "lutein", "baptism", "a Lutheran pastor in Hochfelden used a sermon to urge his parishioners to murder Jews", "\u00a31 million", "slowing the vehicle", "Cheyenne rivers", "fossils in sedimentary rocks", "Hanna- Barbera, The Jetsons", "Cortina d'Ampezzo", "efficient and effective management of money ( funds )", "Alba Longa", "maryland", "geoscience Australia", "Timo Hildebrand", "both public services and public enterprises", "2 February 1940", "poverty", "a god of the Ammonites", "eye (orbital) sockets in the skull", "Fester Addams", "Bobby Thompson"], "metric_results": {"EM": 0.0625, "QA-F1": 0.14633387445887444}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.19999999999999998, 0.16666666666666669, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.5, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-970", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3253", "mrqa_triviaqa-validation-46", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10046", "mrqa_squad-validation-9751", "mrqa_triviaqa-validation-2442", "mrqa_squad-validation-7836", "mrqa_squad-validation-3999", "mrqa_naturalquestions-validation-5944", "mrqa_naturalquestions-validation-8180", "mrqa_squad-validation-2509", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-116", "mrqa_squad-validation-5178", "mrqa_squad-validation-6023", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-validation-3302", "mrqa_hotpotqa-validation-1244", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-5191", "mrqa_naturalquestions-validation-6019", "mrqa_hotpotqa-validation-241", "mrqa_squad-validation-8444", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-5256"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 15, "before_eval": {"predictions": ["superhuman abilities", "Deathly Hallows", "the Flatbush section of Brooklyn, New York City", "San Antonio", "Akon, Christina Aguilera and Taio Cruz", "geocentric", "a friend and publicist", "taryl\u00f3 de Alm\u00e1sy", "masons'marks", "Theodore Haynes (1989)", "the Old Town Hall, Gateshead", "Motown, Philly soul, and Earth, Wind & Fire ( particularly `` That's the Way of the World '' )", "The neck", "1898", "professional wrestler", "Payaya Indians", "to steal the plans for the Death Star, the Galactic Empire's super weapon", "jito corleone", "burtiss jN-4", "baryp", "chimpanzees", "March 15, 1945", "absolute temperature", "hacking the private intelligence firm Stratfor and releasing the leaks through the whistle-blowing website", "Sam Waterston", "bicuspid", "his brother, Menelaus", "3 December", "tallahassee", "prefabricated housing projects", "brian pynson", "WWSB and WOTV"], "metric_results": {"EM": 0.15625, "QA-F1": 0.18933150183150182}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.5714285714285715, 1.0, 0.0, 0.15384615384615383, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-2612", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1479", "mrqa_hotpotqa-validation-335", "mrqa_triviaqa-validation-7512", "mrqa_squad-validation-1374", "mrqa_triviaqa-validation-5979", "mrqa_naturalquestions-validation-10439", "mrqa_hotpotqa-validation-1048", "mrqa_squad-validation-5249", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-56", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-validation-368", "mrqa_hotpotqa-validation-573", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-5877", "mrqa_triviaqa-validation-7578", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-365", "mrqa_hotpotqa-validation-1714", "mrqa_triviaqa-validation-1736", "mrqa_naturalquestions-validation-9451", "mrqa_triviaqa-validation-1166", "mrqa_squad-validation-874", "mrqa_triviaqa-validation-1588", "mrqa_squad-validation-6091"], "retrieved_ids": ["mrqa_hotpotqa-validation-5325", "mrqa_naturalquestions-train-36855", "mrqa_naturalquestions-train-82122", "mrqa_naturalquestions-train-25841", "mrqa_naturalquestions-train-14926", "mrqa_naturalquestions-train-54715", "mrqa_naturalquestions-train-61869", "mrqa_naturalquestions-train-45764", "mrqa_naturalquestions-train-6235", "mrqa_naturalquestions-train-37249", "mrqa_naturalquestions-train-82484", "mrqa_naturalquestions-train-47470", "mrqa_naturalquestions-train-48787", "mrqa_naturalquestions-train-3222", "mrqa_naturalquestions-train-16956", "mrqa_naturalquestions-train-39439", "mrqa_naturalquestions-train-67764", "mrqa_naturalquestions-train-43494", "mrqa_naturalquestions-train-77664", "mrqa_naturalquestions-train-5636", "mrqa_triviaqa-validation-6639", "mrqa_triviaqa-validation-365", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-5526", "mrqa_naturalquestions-train-54577", "mrqa_naturalquestions-train-43248", "mrqa_naturalquestions-train-72123", "mrqa_naturalquestions-train-85346", "mrqa_naturalquestions-train-29696", "mrqa_naturalquestions-train-83407", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-6683", "mrqa_naturalquestions-train-21309", "mrqa_naturalquestions-train-59591", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-train-84845", "mrqa_naturalquestions-train-53606", "mrqa_naturalquestions-train-44254", "mrqa_naturalquestions-train-84083", "mrqa_naturalquestions-train-11357", "mrqa_naturalquestions-train-58901", "mrqa_naturalquestions-train-10949", "mrqa_naturalquestions-train-24912", "mrqa_naturalquestions-train-5558", "mrqa_naturalquestions-train-16432", "mrqa_naturalquestions-train-60387", "mrqa_naturalquestions-train-17748", "mrqa_naturalquestions-train-37698", "mrqa_naturalquestions-train-28428", "mrqa_naturalquestions-train-27365", "mrqa_naturalquestions-train-23912", "mrqa_naturalquestions-train-49810", "mrqa_naturalquestions-train-5734", "mrqa_naturalquestions-train-5205", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-train-29989", "mrqa_naturalquestions-train-78598", "mrqa_naturalquestions-train-61414", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-86071", "mrqa_hotpotqa-validation-5128", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-train-44274", "mrqa_naturalquestions-train-6534", "mrqa_naturalquestions-train-78579", "mrqa_naturalquestions-train-58694", "mrqa_naturalquestions-train-58577", "mrqa_naturalquestions-train-60946", "mrqa_naturalquestions-train-48977", "mrqa_naturalquestions-train-19270", "mrqa_naturalquestions-train-73507", "mrqa_naturalquestions-train-38975", "mrqa_naturalquestions-train-13512", "mrqa_naturalquestions-train-84830", "mrqa_naturalquestions-train-14926", "mrqa_naturalquestions-train-24121", "mrqa_naturalquestions-train-50526", "mrqa_naturalquestions-train-62065", "mrqa_naturalquestions-train-52239", "mrqa_naturalquestions-train-36010", "mrqa_naturalquestions-train-76583", "mrqa_naturalquestions-train-6578", "mrqa_naturalquestions-train-30547", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-3945", "mrqa_naturalquestions-train-24615", "mrqa_naturalquestions-train-62048", "mrqa_naturalquestions-train-80827", "mrqa_triviaqa-validation-7153", "mrqa_naturalquestions-train-31762", "mrqa_naturalquestions-train-76688", "mrqa_hotpotqa-validation-3898", "mrqa_naturalquestions-train-72924", "mrqa_naturalquestions-train-85700", "mrqa_naturalquestions-train-66160", "mrqa_naturalquestions-train-56048", "mrqa_naturalquestions-train-49719", "mrqa_naturalquestions-train-81971", "mrqa_naturalquestions-train-42705", "mrqa_naturalquestions-train-84265", "mrqa_naturalquestions-train-78040", "mrqa_naturalquestions-train-1985", "mrqa_naturalquestions-train-4999", "mrqa_naturalquestions-train-42542", "mrqa_naturalquestions-train-40409", "mrqa_naturalquestions-train-13963", "mrqa_naturalquestions-train-24696", "mrqa_naturalquestions-train-86071", "mrqa_naturalquestions-train-18884", "mrqa_naturalquestions-train-14250", "mrqa_naturalquestions-train-73434", "mrqa_naturalquestions-train-85021", "mrqa_naturalquestions-train-39157", "mrqa_naturalquestions-train-84103", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-6639", "mrqa_triviaqa-validation-3515", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-train-29696", "mrqa_naturalquestions-train-41621", "mrqa_naturalquestions-train-16490", "mrqa_naturalquestions-train-6133", "mrqa_naturalquestions-train-27805", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-2703", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-6887", "mrqa_triviaqa-validation-6385", "mrqa_naturalquestions-train-60865", "mrqa_naturalquestions-train-53909", "mrqa_naturalquestions-train-60641", "mrqa_naturalquestions-train-5205", "mrqa_naturalquestions-train-63152"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 16, "before_eval": {"predictions": ["galileo", "cheese", "galileo", "on the lateral side of the tibia, with which it is connected above and below", "cen\u00e9l n Gabr\u00e1in", "the North Sea, through the former Meuse estuary, near Rotterdam", "the Kalahari Desert", "Colin Montgomerie", "October 29, 1985", "Amway", "Mauritius", "Milton Friedman", "the Speaker or, in his absence, by the Deputy Speaker of the Lok Sabha or in the absence, the Deputy - Chairman of the Rajya Sabha", "the Golden Globe Award for Best Supporting Actor", "tANU", "Chad", "GMAT Sentence Correction (SC)", "statues of many of the British artists whose work is displayed in the museum", "Crowley brought back Samuel so that they could help him find Purgatory, the afterlife of monsters, and that Samuel has been working for him", "Fulham", "French", "michael Buerk", "U.S. Marshals", "her roles in television series \"The Heirs\" (2013) \"Descendants of the Sun\" (2016) and \" Fight for My Way\" (2017)", "supply chain management", "galileo", "Poland", "polynomial algebra", "Michael J. Fox", "Japanese", "sheepskin and Merino Wool products", "Honolulu"], "metric_results": {"EM": 0.0625, "QA-F1": 0.1128890435222672}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.06666666666666667, 0.0, 0.4, 0.0, 0.0, 0.21052631578947367, 0.0, 0.0, 1.0, 0.0, 0.0, 0.07692307692307693, 0.33333333333333337, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2324", "mrqa_naturalquestions-validation-9218", "mrqa_triviaqa-validation-313", "mrqa_squad-validation-9319", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4681", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-4094", "mrqa_naturalquestions-validation-3483", "mrqa_hotpotqa-validation-1566", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-4055", "mrqa_squad-validation-5407", "mrqa_naturalquestions-validation-7144", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-2445", "mrqa_triviaqa-validation-639", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-1001", "mrqa_triviaqa-validation-1770", "mrqa_squad-validation-8223", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-validation-9087", "mrqa_hotpotqa-validation-1250", "mrqa_hotpotqa-validation-2287"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 17, "before_eval": {"predictions": ["dymock, England", "Captain America: Civil War", "August 6, 1845", "isotopes", "Lula", "sovereign states", "Vice President of the United States (VPOTUS)", "the Discovery Institute's \"Teach the Controversy\" campaign", "Sam", "Australian", "military service for men was 30 months and for women 18 months ( although in accordance with a temporary order from January 10, 1968, six additional months were added to the mandatory service, 36 months for men and 24 months for women respectively", "geographic area and subject taught", "health and social problems (ob obesity, mental illness, homicides, teenage births, incarceration, child conflict, drug use)", "a private liberal arts college", "Roy Spencer", "\"antiforms\"", "second half of the third season", "\"Veyyil\" (2006)", "Grace Nail Johnson", "Mick Jagger", "n < p < 2n \u2212 2", "Bangor International Airport", "students learn from teachers who specialize in one subject and who tend to be more knowledgeable in that one area than a teacher who teaches many subjects", "the 180th meridian in a 360 \u00b0 - system", "Cartoon Network", "the Presiding Officer", "Miami Heat", "33", "dactylosphaera vitifoliae", "Annual Conference Cabinet", "brian", "first prompted by original star William Hartnell's poor health"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3065771447467876}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.08333333333333334, 0.0, 0.7499999999999999, 0.0, 0.0, 0.36734693877551017, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.28571428571428575, 0.25, 0.07407407407407407, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1298", "mrqa_squad-validation-608", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-3559", "mrqa_triviaqa-validation-314", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-3440", "mrqa_naturalquestions-validation-1090", "mrqa_squad-validation-2053", "mrqa_squad-validation-7572", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-4868", "mrqa_naturalquestions-validation-9171", "mrqa_squad-validation-8966", "mrqa_hotpotqa-validation-2782", "mrqa_squad-validation-1904", "mrqa_naturalquestions-validation-10347", "mrqa_hotpotqa-validation-613", "mrqa_hotpotqa-validation-4463", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-1130", "mrqa_squad-validation-7664"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 18, "before_eval": {"predictions": ["genocide against the Tutsi", "Co-teachers work in sync with one another to create a climate of learning", "500 metres", "Vili Fualaau and Mary Kay Letourneau", "entertainment division", "distance covered by a vehicle ( for example as recorded by an odometer ), person, animal, or object along a curved path from a point A to a point B", "the most times", "South Kensington Museum", "Edward I of England", "daveneyin", "dundee", "those at the bottom of the economic government", "lotheringhay Castle in England", "\"Grindhouse\" fake trailer", "lyle davenport", "digital transmission modes", "German states of Bavaria and Baden-W\u00fcrttemberg, the Austrian state of Vorarlberg, and the Swiss cantons of Thurgau and St. Gallen", "Tesla Gigafactory 1", "821", "Sky channels", "pressure", "Kim Hyun-ah", "colonizers", "transposition", "King of Cool", "President Wilson and the American delegation from the Paris Peace Conference", "araspas", "the fifth season", "dave dors", "Hockey Club Davos", "Michael Crawford", "lightning strike"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2695097117794486}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.4, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10526315789473684, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.4, 0.4, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-1441", "mrqa_squad-validation-1914", "mrqa_hotpotqa-validation-2145", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-3300", "mrqa_squad-validation-5257", "mrqa_hotpotqa-validation-2993", "mrqa_triviaqa-validation-5996", "mrqa_naturalquestions-validation-5215", "mrqa_triviaqa-validation-2683", "mrqa_hotpotqa-validation-2201", "mrqa_triviaqa-validation-2715", "mrqa_naturalquestions-validation-2222", "mrqa_squad-validation-9074", "mrqa_hotpotqa-validation-4415", "mrqa_squad-validation-2862", "mrqa_hotpotqa-validation-1855", "mrqa_squad-validation-9841", "mrqa_naturalquestions-validation-4497", "mrqa_squad-validation-9827", "mrqa_triviaqa-validation-1764", "mrqa_hotpotqa-validation-3798", "mrqa_triviaqa-validation-7100", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-511", "mrqa_naturalquestions-validation-10490"], "retrieved_ids": ["mrqa_naturalquestions-train-28081", "mrqa_hotpotqa-validation-5256", "mrqa_naturalquestions-train-42632", "mrqa_naturalquestions-train-87371", "mrqa_naturalquestions-train-58830", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-62864", "mrqa_naturalquestions-train-57692", "mrqa_naturalquestions-train-70771", "mrqa_naturalquestions-train-45622", "mrqa_naturalquestions-train-46161", "mrqa_naturalquestions-train-43105", "mrqa_naturalquestions-train-32795", "mrqa_naturalquestions-train-36503", "mrqa_naturalquestions-train-75876", "mrqa_naturalquestions-train-73143", "mrqa_naturalquestions-train-59549", "mrqa_naturalquestions-train-50473", "mrqa_naturalquestions-train-53137", "mrqa_naturalquestions-train-17067", "mrqa_naturalquestions-train-81971", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-train-42705", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-train-49719", "mrqa_naturalquestions-train-62445", "mrqa_naturalquestions-train-7327", "mrqa_naturalquestions-train-20665", "mrqa_naturalquestions-train-85069", "mrqa_naturalquestions-train-57079", "mrqa_naturalquestions-train-74228", "mrqa_naturalquestions-train-43248", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-29696", "mrqa_naturalquestions-train-51644", "mrqa_naturalquestions-train-27665", "mrqa_naturalquestions-train-49810", "mrqa_naturalquestions-train-78345", "mrqa_naturalquestions-train-45549", "mrqa_naturalquestions-train-53409", "mrqa_triviaqa-validation-5997", "mrqa_naturalquestions-train-31350", "mrqa_naturalquestions-train-24436", "mrqa_naturalquestions-train-68895", "mrqa_naturalquestions-train-60208", "mrqa_naturalquestions-train-29021", "mrqa_naturalquestions-train-23881", "mrqa_naturalquestions-train-11921", "mrqa_naturalquestions-train-25636", "mrqa_naturalquestions-train-40184", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-train-61719", "mrqa_naturalquestions-train-101", "mrqa_naturalquestions-train-53073", "mrqa_naturalquestions-train-29984", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1032", "mrqa_naturalquestions-train-35306", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6639", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-9171", "mrqa_naturalquestions-train-82478", "mrqa_naturalquestions-train-396", "mrqa_naturalquestions-train-69270", "mrqa_naturalquestions-train-69712", "mrqa_naturalquestions-train-19223", "mrqa_naturalquestions-train-53080", "mrqa_naturalquestions-train-1396", "mrqa_naturalquestions-train-68266", "mrqa_naturalquestions-train-62665", "mrqa_naturalquestions-train-24249", "mrqa_naturalquestions-train-76315", "mrqa_naturalquestions-train-18868", "mrqa_naturalquestions-train-4019", "mrqa_naturalquestions-train-68803", "mrqa_naturalquestions-train-41621", "mrqa_naturalquestions-train-50473", "mrqa_naturalquestions-train-74592", "mrqa_naturalquestions-train-59549", "mrqa_naturalquestions-train-69860", "mrqa_naturalquestions-train-87973", "mrqa_naturalquestions-train-17900", "mrqa_naturalquestions-train-6688", "mrqa_naturalquestions-train-46861", "mrqa_naturalquestions-train-13512", "mrqa_naturalquestions-train-57524", "mrqa_naturalquestions-train-27938", "mrqa_naturalquestions-train-40184", "mrqa_naturalquestions-train-86241", "mrqa_naturalquestions-train-50358", "mrqa_naturalquestions-train-33207", "mrqa_naturalquestions-train-843", "mrqa_naturalquestions-train-62864", "mrqa_naturalquestions-train-8972", "mrqa_naturalquestions-train-77024", "mrqa_naturalquestions-train-16811", "mrqa_naturalquestions-train-13346", "mrqa_naturalquestions-train-28491", "mrqa_naturalquestions-train-55239", "mrqa_naturalquestions-train-26463", "mrqa_naturalquestions-train-12852", "mrqa_triviaqa-validation-3515", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-5168", "mrqa_triviaqa-validation-6385", "mrqa_naturalquestions-train-24742", "mrqa_naturalquestions-train-65495", "mrqa_naturalquestions-train-71231", "mrqa_naturalquestions-train-36373", "mrqa_naturalquestions-train-79850", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2449", "mrqa_naturalquestions-train-68895", "mrqa_triviaqa-validation-5979", "mrqa_naturalquestions-train-85615", "mrqa_naturalquestions-train-32633", "mrqa_naturalquestions-train-15636", "mrqa_naturalquestions-train-75126", "mrqa_naturalquestions-train-51350", "mrqa_naturalquestions-train-36561", "mrqa_naturalquestions-train-81896", "mrqa_naturalquestions-train-22759", "mrqa_naturalquestions-train-61084", "mrqa_naturalquestions-train-2563", "mrqa_naturalquestions-train-75322", "mrqa_naturalquestions-train-61743", "mrqa_naturalquestions-train-2947", "mrqa_naturalquestions-train-5689", "mrqa_naturalquestions-train-42768"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 19, "before_eval": {"predictions": ["Mauritian", "Norman Macdonnell", "zaragoza", "11.1", "trans-Pacific flight", "Sharman Joshi", "usually have to sit in a classroom and do work, write lines or a punishment essay, or sit quietly", "Forster I, Forster II, and Forster III", "a recurring decimal", "Ana", "New Brunswick", "The Duel", "joseph smalley", "belisabeth thible", "a 1993 American comedy - drama film directed by Fred Schepisi, adapted from the Pulitzer Prize - nominated John Guare play of the same name", "blackstar", "Indian", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "1974", "Nicki Minaj", "fredic", "Huguenot ancestry", "f1", "friedrich Engels", "Rugrats", "William the Conqueror", "Ben Gurion International Airport", "two", "the Corinthian and Saronic Gulfs", "collecting samples of blood and other fluids from patients", "the youngest TV director ever", "Sunset Publishing Corporation"], "metric_results": {"EM": 0.21875, "QA-F1": 0.2466066919191919}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333334, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.1818181818181818, 1.0, 0.3333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2389", "mrqa_squad-validation-6963", "mrqa_hotpotqa-validation-303", "mrqa_squad-validation-1942", "mrqa_squad-validation-9214", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-9284", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-6935", "mrqa_naturalquestions-validation-4354", "mrqa_triviaqa-validation-1995", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-8025", "mrqa_triviaqa-validation-2015", "mrqa_squad-validation-3151", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4068", "mrqa_hotpotqa-validation-57", "mrqa_naturalquestions-validation-1565", "mrqa_hotpotqa-validation-5061", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-3552", "mrqa_hotpotqa-validation-2627"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 20, "before_eval": {"predictions": ["reciprocating", "Michael Koman", "the Sackler Centre for arts education", "masiin bey", "kaleidoscopes", "British Columbia", "the Gemini and Apollo programs", "ribosomal RNA", "kingfisher", "six", "CCH Pounder as Loretta Wade, medical examiner", "I Swear", "Cozonac", "Belfast West", "heliocentric", "Sulla", "Super Bowl LII, following the 2017 season", "Golden Globe", "Swahili", "the primacy of core Christian values such as love, patience, charity, and freedom", "nvelist", "a CMYKOG process", "Firoz Shah Tughlaq", "My Love from the Star", "San Jose", "sea wasp", "the Hawai\u02bbi House of Representatives", "a \"teleforce\" weapon", "the Thunderbird of Native American tradition", "the most giving Super Bowl ever", "29.7", "b.J. Hunnicutt"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2712029256146903}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.14814814814814814, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.11764705882352941, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-3368", "mrqa_hotpotqa-validation-779", "mrqa_squad-validation-5273", "mrqa_hotpotqa-validation-3547", "mrqa_triviaqa-validation-5091", "mrqa_naturalquestions-validation-4590", "mrqa_squad-validation-3964", "mrqa_hotpotqa-validation-2434", "mrqa_triviaqa-validation-3486", "mrqa_hotpotqa-validation-1210", "mrqa_naturalquestions-validation-1279", "mrqa_hotpotqa-validation-4627", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-4852", "mrqa_hotpotqa-validation-3623", "mrqa_naturalquestions-validation-2448", "mrqa_squad-validation-8464", "mrqa_squad-validation-2280", "mrqa_triviaqa-validation-6721", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-4015", "mrqa_squad-validation-315", "mrqa_triviaqa-validation-1453", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2064", "mrqa_triviaqa-validation-935"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 21, "before_eval": {"predictions": ["a Czech word, robota, meaning `` forced labor ''", "daphne du maurier", "various registries", "sheep", "Yazoo", "23 May 1926", "stars exceeding about eight times the mass of the sun", "the soul does not sleep (anima non sic dormit) but wakes (sed vigilat) and experiences visions", "cede the former, but was able to negotiate the retention of Saint Pierre and Miquelon, two small islands in the Gulf of St. Lawrence, along with fishing rights in the area", "Willie Nelson and Kris Kristofferson", "ill. (some col.)", "12 California State University campuses (Bakersfield, Channel Islands, Dominguez Hills, Northridge, Pomona, San Bernardino, San Diego, San Marcos, and San Luis Obispo) and private institutions", "French pirate", "Lewis", "Charles Dickens", "a forum in which the Nobel Peace Laureates and the Peace Laureate Organizations could come together to address global issues with a view to encourage and support peace and human well being in the world", "proteins", "February 2001", "(i.e. exceeds any given number)", "nina hossain", "padlocking the gates and using sickles to deflate one of the large domes covering two satellite dishes", "1969", "huldra", "western portions of the Great Lakes region (an area not directly subject to the conflict between the French and British) including the Huron, Mississauga, Ojibwa, Winnebago, and Potawatomi", "Orthodox Christians", "007", "4 in ( 10 cm )", "colt Teofilo", "eating both fish larvae and small crustaceans that would otherwise feed the adult fish", "\"Menace II Society\"", "backup", "Larry Gatlin & the Gatlin Brothers Band"], "metric_results": {"EM": 0.09375, "QA-F1": 0.23586895719815812}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.14285714285714288, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.1818181818181818, 0.0, 0.0, 0.07692307692307693, 0.0, 0.0, 0.0, 0.4210526315789474, 1.0, 0.6666666666666666, 0.888888888888889, 0.0, 0.23529411764705882, 1.0, 0.0, 0.41379310344827586, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.19999999999999998, 0.15384615384615385]}}, "error_ids": ["mrqa_naturalquestions-validation-3609", "mrqa_triviaqa-validation-5239", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-751", "mrqa_triviaqa-validation-4383", "mrqa_hotpotqa-validation-4430", "mrqa_triviaqa-validation-192", "mrqa_squad-validation-2412", "mrqa_squad-validation-10502", "mrqa_hotpotqa-validation-5480", "mrqa_triviaqa-validation-2961", "mrqa_squad-validation-2709", "mrqa_hotpotqa-validation-4950", "mrqa_naturalquestions-validation-5439", "mrqa_squad-validation-5345", "mrqa_hotpotqa-validation-35", "mrqa_naturalquestions-validation-5897", "mrqa_squad-validation-9020", "mrqa_triviaqa-validation-1954", "mrqa_squad-validation-6844", "mrqa_hotpotqa-validation-2399", "mrqa_squad-validation-10177", "mrqa_squad-validation-8456", "mrqa_triviaqa-validation-6950", "mrqa_naturalquestions-validation-6832", "mrqa_triviaqa-validation-1555", "mrqa_squad-validation-4648", "mrqa_naturalquestions-validation-2758", "mrqa_hotpotqa-validation-4676"], "retrieved_ids": ["mrqa_naturalquestions-train-52076", "mrqa_naturalquestions-train-86945", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-train-20428", "mrqa_naturalquestions-train-39439", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-3515", "mrqa_naturalquestions-train-61664", "mrqa_naturalquestions-train-26007", "mrqa_naturalquestions-train-64615", "mrqa_naturalquestions-train-42395", "mrqa_naturalquestions-train-20194", "mrqa_naturalquestions-train-43497", "mrqa_triviaqa-validation-2722", "mrqa_naturalquestions-train-43139", "mrqa_hotpotqa-validation-1001", "mrqa_naturalquestions-train-65896", "mrqa_naturalquestions-train-24615", "mrqa_naturalquestions-train-630", "mrqa_triviaqa-validation-4486", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-6639", "mrqa_naturalquestions-train-32171", "mrqa_naturalquestions-train-1038", "mrqa_naturalquestions-train-52296", "mrqa_naturalquestions-train-46161", "mrqa_naturalquestions-train-26324", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-1453", "mrqa_naturalquestions-train-69411", "mrqa_triviaqa-validation-2376", "mrqa_naturalquestions-train-50097", "mrqa_naturalquestions-train-80729", "mrqa_squad-validation-2372", "mrqa_naturalquestions-train-73356", "mrqa_naturalquestions-train-34634", "mrqa_naturalquestions-train-56205", "mrqa_naturalquestions-train-80186", "mrqa_naturalquestions-train-45978", "mrqa_naturalquestions-train-27314", "mrqa_naturalquestions-train-27538", "mrqa_naturalquestions-train-13334", "mrqa_hotpotqa-validation-779", "mrqa_naturalquestions-train-83407", "mrqa_naturalquestions-train-7375", "mrqa_naturalquestions-train-3478", "mrqa_triviaqa-validation-5091", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-7415", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-train-85620", "mrqa_naturalquestions-train-1467", "mrqa_naturalquestions-train-79359", "mrqa_naturalquestions-train-71368", "mrqa_naturalquestions-train-3688", "mrqa_naturalquestions-train-64528", "mrqa_naturalquestions-train-76561", "mrqa_naturalquestions-train-26295", "mrqa_naturalquestions-train-75672", "mrqa_naturalquestions-train-45309", "mrqa_naturalquestions-train-79099", "mrqa_naturalquestions-train-5205", "mrqa_naturalquestions-train-8986", "mrqa_naturalquestions-train-22", "mrqa_naturalquestions-train-17358", "mrqa_naturalquestions-train-46054", "mrqa_naturalquestions-train-45449", "mrqa_naturalquestions-train-12429", "mrqa_naturalquestions-train-20355", "mrqa_naturalquestions-train-84927", "mrqa_naturalquestions-train-85256", "mrqa_naturalquestions-train-60554", "mrqa_naturalquestions-train-69179", "mrqa_naturalquestions-train-25986", "mrqa_naturalquestions-train-36216", "mrqa_naturalquestions-train-72173", "mrqa_naturalquestions-train-84346", "mrqa_naturalquestions-train-11290", "mrqa_naturalquestions-train-16498", "mrqa_naturalquestions-train-41269", "mrqa_squad-validation-8966", "mrqa_naturalquestions-train-8315", "mrqa_naturalquestions-train-52318", "mrqa_naturalquestions-train-39508", "mrqa_naturalquestions-train-68904", "mrqa_naturalquestions-train-28545", "mrqa_naturalquestions-train-38333", "mrqa_naturalquestions-train-8770", "mrqa_naturalquestions-train-24615", "mrqa_naturalquestions-train-35494", "mrqa_naturalquestions-train-10617", "mrqa_naturalquestions-train-6494", "mrqa_naturalquestions-train-81327", "mrqa_naturalquestions-train-26143", "mrqa_naturalquestions-train-78463", "mrqa_naturalquestions-train-34340", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-11158", "mrqa_naturalquestions-train-17900", "mrqa_naturalquestions-train-76861", "mrqa_naturalquestions-train-75359", "mrqa_naturalquestions-train-68448", "mrqa_naturalquestions-train-63713", "mrqa_naturalquestions-train-81145", "mrqa_naturalquestions-train-73025", "mrqa_naturalquestions-train-63620", "mrqa_naturalquestions-train-34415", "mrqa_naturalquestions-train-76561", "mrqa_naturalquestions-train-64528", "mrqa_naturalquestions-train-40770", "mrqa_triviaqa-validation-2959", "mrqa_naturalquestions-train-24615", "mrqa_hotpotqa-validation-3976", "mrqa_triviaqa-validation-5997", "mrqa_hotpotqa-validation-2937", "mrqa_naturalquestions-train-54343", "mrqa_naturalquestions-train-69947", "mrqa_squad-validation-7836", "mrqa_naturalquestions-train-1740", "mrqa_naturalquestions-train-65495", "mrqa_naturalquestions-train-22090", "mrqa_naturalquestions-train-9712", "mrqa_naturalquestions-train-83473", "mrqa_naturalquestions-train-51582", "mrqa_naturalquestions-train-86442", "mrqa_naturalquestions-train-23475", "mrqa_naturalquestions-train-66951", "mrqa_naturalquestions-train-12527", "mrqa_naturalquestions-train-39439", "mrqa_naturalquestions-train-47660", "mrqa_naturalquestions-train-52224", "mrqa_naturalquestions-train-2833", "mrqa_naturalquestions-train-22705", "mrqa_naturalquestions-train-8124", "mrqa_naturalquestions-train-84896", "mrqa_naturalquestions-train-69289", "mrqa_hotpotqa-validation-779", "mrqa_naturalquestions-train-60641", "mrqa_naturalquestions-train-76529", "mrqa_naturalquestions-train-77664"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 22, "before_eval": {"predictions": ["Alex Skuby", "England", "the first Thursday in May", "MSC Crociere S. p.A.", "1934", "his friends, Humpty Dumpty and Kitty Softpaws", "The centre- left Australian Labor Party (ALP) the centre-right Liberal Party of Australia", "supporters of King Charles I against the supporters of the Long Parliament", "any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "either by being sprinkled over their heads or, in English - speaking countries, more often by being marked on their foreheads as a visible cross", "fandango Groovers", "jules verne", "Augustus Waters, an ex- Basketball player and amputee", "1619", "Tony Blair", "that a subscription to B SkyB was \u2018 Often damaging\u2019, along with alcohol, tobacco and gambling", "June 11, 1973", "Kenya", "a chronological collection of critical quotations about William Shakespeare and his works", "ethelbald", "an active supporter of the League of Nations", "Cargill", "AMC Theatres", "Paddy's Pub in South Philadelphia", "3 October 1990", "March 1, 2018", "beta decay ( of neutrons in atomic nuclei) and the associated radioactivity", "daedalus", "son", "Manhattan Project", "Chronicles of Barsetshire", "vast areas"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3017193998811646}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 0.0, 0.4, 0.0, 0.0, 0.4444444444444445, 0.39999999999999997, 0.0, 0.1111111111111111, 0.15, 0.0, 1.0, 0.6250000000000001, 1.0, 1.0, 0.11764705882352941, 0.0, 0.0, 0.625, 0.0, 0.1818181818181818, 0.0, 1.0, 0.4000000000000001, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1883", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-9789", "mrqa_triviaqa-validation-5698", "mrqa_naturalquestions-validation-7346", "mrqa_squad-validation-2884", "mrqa_hotpotqa-validation-2959", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-1578", "mrqa_naturalquestions-validation-3859", "mrqa_squad-validation-2769", "mrqa_hotpotqa-validation-482", "mrqa_squad-validation-8280", "mrqa_hotpotqa-validation-5655", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-190", "mrqa_squad-validation-10459", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-1929", "mrqa_naturalquestions-validation-1328"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 23, "before_eval": {"predictions": ["$105 billion", "mono", "about two-thirds the size", "1937 Austin Seven Ruby Open Top Tourer", "dreigo Luna", "red", "Dreamland", "animated film", "European Union institutions", "381.6 days", "nine other contenders from across the United States", "NASA's CAL IPSO satellite", "celandine flowers", "U.S. ambassador's residence with New York Times columnist James `` Scotty '' Reston", "Ronnie Schell", "p. falciparum", "Mumbai, Maharashtra", "east", "1939", "2017 / 18", "1707", "on Fresno's far southeast side, bounded by Chestnut Avenue to the West", "south western escarpment of the Jos Plateau", "dambala", "Incudomalleolar joint", "courts of Long Beach, California", "Democritus", "Santa Clara Marriott", "alante con moto", "political power generated by wealth", "log-space reductions", "Corey Brown"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3435763888888889}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.25, 0.25, 1.0, 0.25, 0.4, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3716", "mrqa_squad-validation-8850", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-3933", "mrqa_hotpotqa-validation-4348", "mrqa_hotpotqa-validation-789", "mrqa_squad-validation-4118", "mrqa_squad-validation-542", "mrqa_squad-validation-4228", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-validation-3714", "mrqa_hotpotqa-validation-1782", "mrqa_triviaqa-validation-3595", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-220", "mrqa_naturalquestions-validation-4048", "mrqa_squad-validation-4662", "mrqa_naturalquestions-validation-2212", "mrqa_triviaqa-validation-571", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-3420", "mrqa_hotpotqa-validation-2340", "mrqa_triviaqa-validation-6781", "mrqa_squad-validation-1750", "mrqa_squad-validation-769"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 24, "before_eval": {"predictions": ["2003", "often they can flow back down more H+-pumping electron transport chains to transport more hydrogen ions into the thylakoid space to generate more ATP", "Pitt", "alchemy", "WBC and lineal titles", "moluccas", "Saturday", "Cordelia", "arranged for Israel to pull back from the Sinai Peninsula and the Golan Heights", "1990", "J.R. R. Tolkien", "John Elway", "Selena Gomez", "learn a trade such as tailoring, carpentry, motor vehicle repair, brick-laying and masonry", "bingo", "Eugene", "is the world's first multi-purpose, domed sports stadium", "primes of the form 2p + 1 with p prime", "letter", "Fa Ze YouTubers", "the nine circles", "two Mongols", "along the eastern coast of the continent, from Nova Scotia and Newfoundland in the north, to Georgia in the south", "Friars Minor Conventual", "CD Castell\u00f3n", "between 1770 and 1848", "12\u20134", "by having colloblasts, which are sticky and adhere to prey", "Patrick Wachsberger", "STS-51-C", "due to clear weather", "mitterr"], "metric_results": {"EM": 0.15625, "QA-F1": 0.26876988251988254}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.9600000000000001, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7692307692307693, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.4, 0.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.21621621621621626, 0.0]}}, "error_ids": ["mrqa_squad-validation-8876", "mrqa_triviaqa-validation-3678", "mrqa_hotpotqa-validation-4444", "mrqa_triviaqa-validation-3340", "mrqa_naturalquestions-validation-9011", "mrqa_squad-validation-3741", "mrqa_squad-validation-5399", "mrqa_hotpotqa-validation-3247", "mrqa_naturalquestions-validation-10381", "mrqa_squad-validation-8473", "mrqa_triviaqa-validation-667", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-309", "mrqa_squad-validation-8979", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-3297", "mrqa_triviaqa-validation-3847", "mrqa_squad-validation-6204", "mrqa_squad-validation-10148", "mrqa_triviaqa-validation-5304", "mrqa_hotpotqa-validation-5588", "mrqa_naturalquestions-validation-3504", "mrqa_squad-validation-4417", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-1814", "mrqa_naturalquestions-validation-6508", "mrqa_triviaqa-validation-1306"], "retrieved_ids": ["mrqa_naturalquestions-train-53017", "mrqa_naturalquestions-train-19806", "mrqa_naturalquestions-train-19208", "mrqa_naturalquestions-train-11403", "mrqa_naturalquestions-train-8754", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-5091", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-7578", "mrqa_hotpotqa-validation-2262", "mrqa_naturalquestions-train-19223", "mrqa_naturalquestions-train-73434", "mrqa_naturalquestions-train-20363", "mrqa_naturalquestions-train-52286", "mrqa_triviaqa-validation-5997", "mrqa_naturalquestions-train-86944", "mrqa_naturalquestions-train-69860", "mrqa_naturalquestions-train-13963", "mrqa_naturalquestions-train-49463", "mrqa_naturalquestions-train-717", "mrqa_naturalquestions-train-30308", "mrqa_naturalquestions-train-24788", "mrqa_naturalquestions-train-46883", "mrqa_naturalquestions-train-27603", "mrqa_naturalquestions-train-28557", "mrqa_naturalquestions-train-10053", "mrqa_naturalquestions-train-39393", "mrqa_naturalquestions-train-79995", "mrqa_naturalquestions-train-28391", "mrqa_naturalquestions-train-66866", "mrqa_naturalquestions-train-25865", "mrqa_naturalquestions-train-28094", "mrqa_naturalquestions-train-5473", "mrqa_naturalquestions-train-116", "mrqa_naturalquestions-train-29589", "mrqa_naturalquestions-train-86945", "mrqa_naturalquestions-train-26322", "mrqa_naturalquestions-train-49999", "mrqa_naturalquestions-train-9400", "mrqa_naturalquestions-train-12719", "mrqa_naturalquestions-train-58053", "mrqa_naturalquestions-train-46200", "mrqa_naturalquestions-train-25975", "mrqa_naturalquestions-train-46861", "mrqa_naturalquestions-train-12896", "mrqa_naturalquestions-train-35181", "mrqa_naturalquestions-train-4972", "mrqa_naturalquestions-train-31830", "mrqa_naturalquestions-train-20033", "mrqa_naturalquestions-train-51238", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-5168", "mrqa_hotpotqa-validation-2449", "mrqa_naturalquestions-train-25908", "mrqa_naturalquestions-train-1148", "mrqa_naturalquestions-train-47660", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-train-2860", "mrqa_naturalquestions-train-18324", "mrqa_naturalquestions-train-49152", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-31102", "mrqa_naturalquestions-train-47470", "mrqa_naturalquestions-train-85803", "mrqa_squad-validation-8966", "mrqa_naturalquestions-train-51073", "mrqa_naturalquestions-train-23369", "mrqa_naturalquestions-train-4508", "mrqa_naturalquestions-train-29696", "mrqa_naturalquestions-train-51094", "mrqa_naturalquestions-train-41621", "mrqa_naturalquestions-train-36957", "mrqa_naturalquestions-train-55572", "mrqa_naturalquestions-train-23244", "mrqa_naturalquestions-train-71254", "mrqa_naturalquestions-train-67513", "mrqa_naturalquestions-train-69260", "mrqa_naturalquestions-train-33535", "mrqa_naturalquestions-train-32835", "mrqa_triviaqa-validation-7578", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-3515", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-4486", "mrqa_squad-validation-1003", "mrqa_naturalquestions-train-73534", "mrqa_naturalquestions-train-45778", "mrqa_naturalquestions-train-22090", "mrqa_naturalquestions-train-47028", "mrqa_naturalquestions-train-2612", "mrqa_naturalquestions-train-47470", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-80350", "mrqa_naturalquestions-train-71357", "mrqa_triviaqa-validation-313", "mrqa_triviaqa-validation-2015", "mrqa_hotpotqa-validation-2937", "mrqa_naturalquestions-train-83443", "mrqa_naturalquestions-train-37249", "mrqa_naturalquestions-train-18064", "mrqa_naturalquestions-train-31587", "mrqa_naturalquestions-train-37272", "mrqa_naturalquestions-train-43790", "mrqa_naturalquestions-train-13556", "mrqa_naturalquestions-train-55922", "mrqa_naturalquestions-train-45410", "mrqa_naturalquestions-train-56859", "mrqa_naturalquestions-train-29696", "mrqa_naturalquestions-train-66162", "mrqa_naturalquestions-train-60350", "mrqa_naturalquestions-train-8357", "mrqa_squad-validation-874", "mrqa_naturalquestions-train-42645", "mrqa_naturalquestions-train-84139", "mrqa_naturalquestions-train-21835", "mrqa_naturalquestions-train-13512", "mrqa_naturalquestions-train-35306", "mrqa_naturalquestions-train-76701", "mrqa_naturalquestions-train-73507", "mrqa_naturalquestions-train-62917", "mrqa_naturalquestions-train-49722", "mrqa_naturalquestions-train-9400", "mrqa_naturalquestions-train-85035", "mrqa_naturalquestions-train-86968", "mrqa_naturalquestions-train-19954", "mrqa_naturalquestions-train-64654", "mrqa_naturalquestions-train-27314", "mrqa_naturalquestions-train-7001", "mrqa_naturalquestions-train-35836", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-2722", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-5997"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 25, "before_eval": {"predictions": ["alpine skiing", "Sarah's slave", "over 50 million singles", "states'rights to expand slavery", "1923 and 1925", "the Orlando\u2013Kissimmee\u2013 Sanford, Florida Metropolitan Statistical Area", "January 19, 1962", "Frigate", "He was born in San Bernardino, California, the son of Eugene Ezra Hackman and Anna Lyda Elizabeth (n\u00e9e Gray)", "the d'Hondt method", "geese", "the move from the manufacturing sector to the service sector", "Moreton Bay", "Colin Baker", "August 14, 1848", "lower rates of social goods", "In at least some species, juveniles are capable of reproduction before reaching the adult size and shape. The combination of hermaphroditism and early reproduction enables small populations to grow at an explosive rate", "a toasted wheat bun", "a movement within the Church of England in the 18th century", "based on seventeen Broadway dancers auditioning for spots on a chorus line", "2,664 rooms", "g Flex", "a chute", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "Facility Services", "Symphony No. 7", "gironde", "1603", "ranked above the two personal physicians of the Emperor", "The Nutcracker", "We admitted we were powerless over alcohol -- that our lives had become unmanageable.   Came to believe that a Power greater than ourselves could restore us to sanity.", "Wrigley Field"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3789569805194805}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, true, false, false], "QA-F1": [0.0, 0.0, 0.3333333333333333, 0.25, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.3333333333333333, 0.33333333333333337, 0.6363636363636364, 1.0, 0.09090909090909091, 0.0, 0.6666666666666666, 0.0, 0.2857142857142857, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-5788", "mrqa_hotpotqa-validation-1818", "mrqa_naturalquestions-validation-10169", "mrqa_naturalquestions-validation-4072", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-3977", "mrqa_triviaqa-validation-6091", "mrqa_squad-validation-9532", "mrqa_squad-validation-7382", "mrqa_triviaqa-validation-7336", "mrqa_squad-validation-7677", "mrqa_hotpotqa-validation-4720", "mrqa_squad-validation-7301", "mrqa_squad-validation-4637", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-859", "mrqa_hotpotqa-validation-4282", "mrqa_triviaqa-validation-2454", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-7309", "mrqa_hotpotqa-validation-1414", "mrqa_squad-validation-6328", "mrqa_naturalquestions-validation-2481", "mrqa_triviaqa-validation-1259"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 26, "before_eval": {"predictions": ["Jewish audiences, with the Romans serving as external arbiters on disputes concerning Jewish customs and law", "north of the Lakes Region", "Magic formula investing", "true history of the Kelly Gang", "Honolulu", "1910\u20131940", "non-teaching posts", "Catch Me Who Can", "jazz saxophonist", "tennis", "4,000", "the founder of the Yuan dynasty", "Heathcliff", "canal aquaduct", "cinnamon", "Phil Hartman", "Raymond Unwin", "San Bernardino", "one of the finest streets in England", "Albany High School for Educating People of Color", "Agra garden", "a non-commissioned officer in the United States Army's premier special operations unit, the 1st Special Forces Operational Detachment- Delta (1SFOD-D) or \" Delta Force\"", "Shaw", "may enhance the chances for acquittal but make for more boring proceedings and reduced press coverage", "Cee - Lo", "The Church of England", "mammy two Shoes", "maryland", "magnetism", "sitting directly on the Gulf of Guinea on the Atlantic Ocean in Nigeria", "opportunity-based entrepreneurship", "January 11, 1755 or 1757"], "metric_results": {"EM": 0.15625, "QA-F1": 0.27642045454545455}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false], "QA-F1": [0.2222222222222222, 0.0, 0.0, 0.33333333333333337, 0.33333333333333337, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.5454545454545454, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8000000000000002]}}, "error_ids": ["mrqa_naturalquestions-validation-3027", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-2184", "mrqa_triviaqa-validation-6772", "mrqa_hotpotqa-validation-4553", "mrqa_squad-validation-1994", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-6689", "mrqa_hotpotqa-validation-5", "mrqa_squad-validation-6148", "mrqa_triviaqa-validation-3472", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6224", "mrqa_hotpotqa-validation-2436", "mrqa_naturalquestions-validation-2214", "mrqa_squad-validation-2428", "mrqa_squad-validation-5184", "mrqa_hotpotqa-validation-1446", "mrqa_naturalquestions-validation-800", "mrqa_hotpotqa-validation-4585", "mrqa_naturalquestions-validation-3658", "mrqa_squad-validation-6837", "mrqa_naturalquestions-validation-5476", "mrqa_triviaqa-validation-5839", "mrqa_triviaqa-validation-5050", "mrqa_triviaqa-validation-3393", "mrqa_hotpotqa-validation-945"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 27, "before_eval": {"predictions": ["ludwig adler", "michael hutchence", "blackberry", "o Orion", "Big Mamie", "nigeria", "film directed by Danny DeVito and written by David Mamet", "a light sky-blue color caused by absorption in the red", "they captured the Tower of London", "2009", "February 7", "the internal thylakoid system", "clangers", "events are centred on Melbourne, but others occur in regional cities, such as the V8 Supercars and Australian Motorcycle Grand Prix at Phillip Island, the Grand Annual Steeplechase at Warrnambool", "only in this case, B SkyB does not carry any control over the channel's content or carriage issues such as picture quality", "the fourth season", "a more fundamental electro weak interaction", "the availability of skilled tradespeople", "the ability of a material to resist wear and tear", "A simple iron boar crest adorns the top of this helmet associating it with the Benty Grange helmet and the Guilden Morden boar", "became new universities", "japan", "James David", "on kickoffs at the 25 - yard line", "the Latin centum, which means 100", "7,000", "lion, leopard, buffalo, rhinoceros, and elephant", "by faith", "ludwig smith", "can be produced with constant technology and resources per unit of time, such that more of one good could be produced only by diverting resources from the other good, resulting in less production of it", "antwerp", "company"], "metric_results": {"EM": 0.15625, "QA-F1": 0.31955215360535727}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.42857142857142855, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.08695652173913042, 0.0, 0.6666666666666665, 0.28571428571428575, 0.0, 0.34782608695652173, 0.8571428571428571, 0.0, 0.6666666666666666, 0.631578947368421, 0.0, 1.0, 1.0, 0.8, 0.0, 0.45454545454545453, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-2899", "mrqa_triviaqa-validation-6331", "mrqa_hotpotqa-validation-2428", "mrqa_squad-validation-3539", "mrqa_triviaqa-validation-2980", "mrqa_hotpotqa-validation-4022", "mrqa_squad-validation-8739", "mrqa_triviaqa-validation-1423", "mrqa_squad-validation-2966", "mrqa_squad-validation-2733", "mrqa_hotpotqa-validation-158", "mrqa_squad-validation-10312", "mrqa_squad-validation-6855", "mrqa_triviaqa-validation-5962", "mrqa_hotpotqa-validation-1226", "mrqa_squad-validation-5337", "mrqa_triviaqa-validation-1034", "mrqa_hotpotqa-validation-3949", "mrqa_naturalquestions-validation-9105", "mrqa_naturalquestions-validation-3771", "mrqa_squad-validation-2313", "mrqa_triviaqa-validation-388", "mrqa_naturalquestions-validation-2893", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-1507"], "retrieved_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-2449", "mrqa_triviaqa-validation-2959", "mrqa_naturalquestions-train-43105", "mrqa_triviaqa-validation-6683", "mrqa_naturalquestions-train-43105", "mrqa_naturalquestions-train-75876", "mrqa_hotpotqa-validation-2449", "mrqa_naturalquestions-train-68895", "mrqa_triviaqa-validation-192", "mrqa_naturalquestions-train-68441", "mrqa_triviaqa-validation-1453", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-train-49463", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-3847", "mrqa_triviaqa-validation-5788", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-6692", "mrqa_naturalquestions-train-16061", "mrqa_naturalquestions-train-7107", "mrqa_naturalquestions-train-50473", "mrqa_naturalquestions-train-80184", "mrqa_naturalquestions-train-74228", "mrqa_naturalquestions-train-4322", "mrqa_naturalquestions-train-74251", "mrqa_naturalquestions-train-1342", "mrqa_naturalquestions-train-16149", "mrqa_naturalquestions-train-19556", "mrqa_naturalquestions-train-67256", "mrqa_naturalquestions-train-24615", "mrqa_naturalquestions-train-87336", "mrqa_naturalquestions-train-84830", "mrqa_triviaqa-validation-314", "mrqa_naturalquestions-train-45182", "mrqa_naturalquestions-train-30083", "mrqa_naturalquestions-train-13675", "mrqa_naturalquestions-train-35494", "mrqa_naturalquestions-train-85591", "mrqa_naturalquestions-train-49927", "mrqa_naturalquestions-train-4691", "mrqa_naturalquestions-train-37432", "mrqa_naturalquestions-train-68628", "mrqa_naturalquestions-train-25540", "mrqa_triviaqa-validation-6639", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-1306", "mrqa_triviaqa-validation-3847", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-train-53720", "mrqa_naturalquestions-train-35943", "mrqa_naturalquestions-train-45309", "mrqa_naturalquestions-train-57692", "mrqa_naturalquestions-train-50473", "mrqa_naturalquestions-train-18524", "mrqa_naturalquestions-train-69270", "mrqa_naturalquestions-train-40128", "mrqa_naturalquestions-train-45720", "mrqa_naturalquestions-train-8357", "mrqa_naturalquestions-train-72300", "mrqa_naturalquestions-train-20416", "mrqa_naturalquestions-train-12721", "mrqa_naturalquestions-train-47354", "mrqa_naturalquestions-train-81715", "mrqa_naturalquestions-train-62627", "mrqa_squad-validation-10459", "mrqa_naturalquestions-train-59761", "mrqa_naturalquestions-train-71615", "mrqa_naturalquestions-train-49121", "mrqa_naturalquestions-train-31955", "mrqa_naturalquestions-train-74843", "mrqa_naturalquestions-train-66924", "mrqa_naturalquestions-train-42632", "mrqa_naturalquestions-train-66044", "mrqa_triviaqa-validation-5050", "mrqa_triviaqa-validation-3595", "mrqa_naturalquestions-train-68269", "mrqa_naturalquestions-train-28545", "mrqa_naturalquestions-train-24615", "mrqa_naturalquestions-train-6647", "mrqa_naturalquestions-train-66864", "mrqa_naturalquestions-train-41188", "mrqa_naturalquestions-train-58439", "mrqa_naturalquestions-train-57162", "mrqa_naturalquestions-train-78849", "mrqa_naturalquestions-train-76238", "mrqa_naturalquestions-train-2833", "mrqa_naturalquestions-train-21281", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-7578", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-1306", "mrqa_triviaqa-validation-6692", "mrqa_naturalquestions-train-61332", "mrqa_naturalquestions-train-58901", "mrqa_naturalquestions-train-59051", "mrqa_naturalquestions-train-53137", "mrqa_naturalquestions-train-50915", "mrqa_naturalquestions-train-38161", "mrqa_naturalquestions-train-60612", "mrqa_naturalquestions-train-50214", "mrqa_naturalquestions-train-68815", "mrqa_naturalquestions-train-14788", "mrqa_naturalquestions-train-57469", "mrqa_naturalquestions-train-21199", "mrqa_naturalquestions-train-79266", "mrqa_naturalquestions-train-25772", "mrqa_naturalquestions-train-39862", "mrqa_squad-validation-2372", "mrqa_naturalquestions-train-80729", "mrqa_naturalquestions-train-75505", "mrqa_naturalquestions-train-66511", "mrqa_squad-validation-2412", "mrqa_naturalquestions-train-73434", "mrqa_naturalquestions-train-61465", "mrqa_naturalquestions-train-11869", "mrqa_naturalquestions-train-68895", "mrqa_naturalquestions-train-40724", "mrqa_naturalquestions-train-23390", "mrqa_naturalquestions-train-44892", "mrqa_naturalquestions-train-79078", "mrqa_naturalquestions-train-78201", "mrqa_naturalquestions-train-21420", "mrqa_naturalquestions-train-68952", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-365", "mrqa_naturalquestions-train-24615", "mrqa_triviaqa-validation-3595", "mrqa_naturalquestions-train-48166", "mrqa_naturalquestions-train-62864", "mrqa_naturalquestions-train-24300", "mrqa_naturalquestions-train-75749", "mrqa_naturalquestions-train-35002"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 28, "before_eval": {"predictions": ["critical pamphlets on Islam", "Chris Weidman", "jury nullification", "Harishchandra", "poet", "Professor Eobard Thawne", "Damson plums", "a US$10 a week raise over Tesla's US$18 per week salary", "1875", "member states", "clarinets", "McKinsey's offices in Silicon Valley and India", "acrophobia", "Serious Charge", "ulcerative colitis", "Ondemar Dias", "Raya Yarbrough", "Arizona", "cruiserweight", "John D. Rockefeller", "Old Testament", "UPS", "local talent", "Football League", "Fifth Doctor", "falinth river", "contemporary accounts were exaggerations", "Mary Eugenia Surratt", "1340", "dodo bird", "through the process of `` like energy attracting like energy '' a person can improve their own health, wealth and personal relationships", "Stan Butler"], "metric_results": {"EM": 0.25, "QA-F1": 0.34573948948948946}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, false, true, false, false, true, false, true], "QA-F1": [0.4, 0.0, 0.3333333333333333, 0.0, 0.5, 1.0, 0.0, 0.5, 0.0, 0.4444444444444445, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.10810810810810811, 1.0]}}, "error_ids": ["mrqa_squad-validation-2291", "mrqa_hotpotqa-validation-1390", "mrqa_squad-validation-6835", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-5637", "mrqa_triviaqa-validation-4827", "mrqa_squad-validation-1276", "mrqa_hotpotqa-validation-4352", "mrqa_naturalquestions-validation-10495", "mrqa_triviaqa-validation-5237", "mrqa_hotpotqa-validation-5110", "mrqa_triviaqa-validation-3760", "mrqa_triviaqa-validation-6250", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-7110", "mrqa_triviaqa-validation-1347", "mrqa_squad-validation-8031", "mrqa_naturalquestions-validation-10687", "mrqa_hotpotqa-validation-1251", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-5810", "mrqa_squad-validation-8190", "mrqa_naturalquestions-validation-7821"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 29, "before_eval": {"predictions": ["red", "886 AD", "to finance his own projects", "24 Hours of Le Mans", "Xbox 360", "Tokyo", "Darian Stewart", "parallelogram rule of vector addition", "j John Thomas Scopes", "blackbirds", "Neutron sources", "Van Gogh", "a larger cylinder volume", "performs six major functions ; support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "Imperial Secretariat", "Doctorin' the Tardis", "National Basketball Development League (NBDL)", "gillingham", "St. Mary's County", "Graham Gano", "2,615", "Pyeongchang", "athlete", "a password recovery tool for Microsoft Windows", "Captain John Guidry", "Charles and Ray Eames", "Brazil", "alja Huber", "the smallest subfield", "stomach", "53", "photosynthesis"], "metric_results": {"EM": 0.125, "QA-F1": 0.25833625116713355}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false], "QA-F1": [0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 0.0, 0.0, 0.4, 1.0, 0.8, 0.38095238095238093, 0.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.058823529411764705, 0.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4708", "mrqa_naturalquestions-validation-866", "mrqa_squad-validation-1249", "mrqa_hotpotqa-validation-3497", "mrqa_triviaqa-validation-6632", "mrqa_naturalquestions-validation-6610", "mrqa_squad-validation-814", "mrqa_squad-validation-10408", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-3103", "mrqa_naturalquestions-validation-8653", "mrqa_squad-validation-3344", "mrqa_naturalquestions-validation-5826", "mrqa_squad-validation-8075", "mrqa_hotpotqa-validation-2928", "mrqa_triviaqa-validation-1280", "mrqa_naturalquestions-validation-5961", "mrqa_squad-validation-772", "mrqa_hotpotqa-validation-3031", "mrqa_naturalquestions-validation-9765", "mrqa_hotpotqa-validation-5383", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-7567", "mrqa_hotpotqa-validation-5709", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-2709", "mrqa_triviaqa-validation-814", "mrqa_squad-validation-8873"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 30, "before_eval": {"predictions": ["the judiciary", "named for Frederick Louis, Prince of Wales, son of King George II", "a wife named Sally", "abraham Lincoln", "prunella", "Erick Avari, Michael McKean, Amy D. Jacobson, Marty Ingels, Earl Boen, Jordana Capra, Dirk Benedict, and Rip Taylor", "BBC UKTV", "lesterbeth", "demographics and economic ties", "three or more", "The Kickoff Game", "narcolepsy", "arctic monkeys", "monza", "arthur", "usernames, passwords, commands and data can be read by anyone able to perform packet capture ( sniffing ) on the network", "A computer program", "National Party", "marduk", "Hekla", "a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda", "karl d. Daniels", "off the northeast coast of Australia", "Paragraph 4", "Delaware to the southeast, Maryland to the south, West Virginia to the southwest, Ohio to the west, Lake Erie and the Canadian province of Ontario to the northwest, New York to the north, and New Jersey to the east", "Evan Jonigkeit", "Sebastian Lund ( Rob Kerkovich ), a criminalist turned forensics agent and the team's newest member", "k. Kamaraj", "National Lottery", "Apollo", "catherine of aragon", "in order to comply with the Do - Not - Call Implementation Act of 2003"], "metric_results": {"EM": 0.0625, "QA-F1": 0.11082126480975164}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.10526315789473684, 0.0, 0.0, 0.0, 0.12121212121212122, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.9090909090909091]}}, "error_ids": ["mrqa_naturalquestions-validation-4645", "mrqa_hotpotqa-validation-2559", "mrqa_naturalquestions-validation-9608", "mrqa_triviaqa-validation-3920", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-4079", "mrqa_squad-validation-7793", "mrqa_triviaqa-validation-1921", "mrqa_squad-validation-2577", "mrqa_hotpotqa-validation-4578", "mrqa_naturalquestions-validation-3209", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-1722", "mrqa_naturalquestions-validation-3533", "mrqa_squad-validation-2885", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7090", "mrqa_squad-validation-8451", "mrqa_triviaqa-validation-7684", "mrqa_naturalquestions-validation-4710", "mrqa_squad-validation-6961", "mrqa_hotpotqa-validation-4604", "mrqa_hotpotqa-validation-3333", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-3134", "mrqa_hotpotqa-validation-5604", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-2091", "mrqa_naturalquestions-validation-10328"], "retrieved_ids": ["mrqa_naturalquestions-train-71368", "mrqa_naturalquestions-train-28586", "mrqa_naturalquestions-train-7204", "mrqa_naturalquestions-train-20327", "mrqa_naturalquestions-train-37291", "mrqa_naturalquestions-train-22851", "mrqa_naturalquestions-train-47115", "mrqa_naturalquestions-train-18614", "mrqa_naturalquestions-train-33515", "mrqa_naturalquestions-train-25755", "mrqa_naturalquestions-train-50358", "mrqa_naturalquestions-train-20509", "mrqa_naturalquestions-train-2860", "mrqa_naturalquestions-train-21199", "mrqa_naturalquestions-train-46650", "mrqa_naturalquestions-train-68087", "mrqa_naturalquestions-train-80555", "mrqa_naturalquestions-train-76338", "mrqa_naturalquestions-train-80543", "mrqa_naturalquestions-train-17468", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-3515", "mrqa_triviaqa-validation-1280", "mrqa_triviaqa-validation-2959", "mrqa_naturalquestions-train-18524", "mrqa_naturalquestions-train-42206", "mrqa_naturalquestions-train-2630", "mrqa_naturalquestions-train-69270", "mrqa_naturalquestions-train-51350", "mrqa_naturalquestions-train-396", "mrqa_naturalquestions-train-24227", "mrqa_naturalquestions-train-69712", "mrqa_naturalquestions-train-31951", "mrqa_naturalquestions-train-65854", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-5839", "mrqa_naturalquestions-train-36180", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-train-32796", "mrqa_naturalquestions-train-68904", "mrqa_naturalquestions-train-86442", "mrqa_naturalquestions-train-51582", "mrqa_naturalquestions-train-48627", "mrqa_naturalquestions-train-62271", "mrqa_naturalquestions-train-31705", "mrqa_naturalquestions-train-58475", "mrqa_naturalquestions-train-16239", "mrqa_naturalquestions-train-47764", "mrqa_naturalquestions-train-85399", "mrqa_naturalquestions-train-1327", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-train-22793", "mrqa_naturalquestions-train-84896", "mrqa_naturalquestions-train-85608", "mrqa_triviaqa-validation-1995", "mrqa_triviaqa-validation-5406", "mrqa_naturalquestions-train-60360", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-2959", "mrqa_naturalquestions-train-68895", "mrqa_naturalquestions-train-58312", "mrqa_squad-validation-4253", "mrqa_naturalquestions-train-46670", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-train-68895", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-5168", "mrqa_naturalquestions-train-87942", "mrqa_naturalquestions-train-85426", "mrqa_naturalquestions-train-76561", "mrqa_naturalquestions-train-69860", "mrqa_naturalquestions-train-80228", "mrqa_squad-validation-2884", "mrqa_naturalquestions-train-82140", "mrqa_naturalquestions-train-15888", "mrqa_naturalquestions-train-20219", "mrqa_naturalquestions-train-55313", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-7578", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-3847", "mrqa_triviaqa-validation-1995", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-5997", "mrqa_naturalquestions-train-24615", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-80228", "mrqa_naturalquestions-train-58738", "mrqa_naturalquestions-train-45027", "mrqa_naturalquestions-train-26677", "mrqa_naturalquestions-train-82997", "mrqa_triviaqa-validation-5997", "mrqa_naturalquestions-train-68895", "mrqa_hotpotqa-validation-2145", "mrqa_triviaqa-validation-6902", "mrqa_naturalquestions-train-59549", "mrqa_naturalquestions-train-37240", "mrqa_naturalquestions-train-31350", "mrqa_naturalquestions-train-58634", "mrqa_naturalquestions-train-73184", "mrqa_naturalquestions-train-15537", "mrqa_hotpotqa-validation-4164", "mrqa_naturalquestions-train-80799", "mrqa_naturalquestions-train-13914", "mrqa_hotpotqa-validation-26", "mrqa_naturalquestions-train-79014", "mrqa_naturalquestions-train-3307", "mrqa_naturalquestions-train-26521", "mrqa_naturalquestions-train-45745", "mrqa_hotpotqa-validation-1032", "mrqa_naturalquestions-train-21934", "mrqa_naturalquestions-train-8492", "mrqa_naturalquestions-train-58053", "mrqa_naturalquestions-train-75776", "mrqa_naturalquestions-train-12719", "mrqa_naturalquestions-train-8244", "mrqa_naturalquestions-train-84103", "mrqa_naturalquestions-train-3865", "mrqa_naturalquestions-train-51350", "mrqa_naturalquestions-train-38998", "mrqa_naturalquestions-train-3268", "mrqa_hotpotqa-validation-3637", "mrqa_naturalquestions-train-15099", "mrqa_naturalquestions-train-23969", "mrqa_triviaqa-validation-3420", "mrqa_naturalquestions-train-17830", "mrqa_naturalquestions-train-20085", "mrqa_naturalquestions-train-26873", "mrqa_naturalquestions-train-29696", "mrqa_naturalquestions-train-45527", "mrqa_naturalquestions-train-63695", "mrqa_naturalquestions-train-28545", "mrqa_hotpotqa-validation-3623", "mrqa_triviaqa-validation-3420", "mrqa_squad-validation-3999", "mrqa_triviaqa-validation-7578", "mrqa_triviaqa-validation-1935", "mrqa_triviaqa-validation-1306", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-1280", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-train-4931", "mrqa_naturalquestions-train-69717", "mrqa_naturalquestions-train-39439", "mrqa_naturalquestions-train-62579", "mrqa_naturalquestions-train-39682"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 31, "before_eval": {"predictions": ["Yolanda Sald\u00edvar", "Peter Andreas", "sarajevo", "skylab", "the crossroads of the Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide", "androids", "14 and one-half hands", "shopping mall", "explaining their actions", "DreamWorks Animation", "liszt Strauss Wagner Dvorak", "his own men", "emissions resulting from human activities are substantially increasing the atmospheric concentrations of the greenhouse gases, resulting on average in an additional warming of the Earth's surface", "poseidon", "the RAF", "reduce growth in relatively poor countries but encourage growth", "Ibrium", "Hayley Sanderson", "Polish-Jewish", "General Francisco Franco", "Cole Porter", "popham Down", "16,000", "Washington Street", "May 10, 1976", "five", "lexy gold", "his frustration with the atmosphere in the group at that time -- namely, Paul McCartney's over-assertiveness and criticism of his guitar playing", "1977", "Paul the Apostle", "1915", "the variety of occupations necessary to sustain the community as distinct from the indigenous population"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3149632706618001}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 0.23529411764705882, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.37037037037037035, 0.0, 0.0, 0.3636363636363636, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6428571428571429, 0.0, 0.0, 0.0, 0.13333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-1385", "mrqa_hotpotqa-validation-5585", "mrqa_triviaqa-validation-1050", "mrqa_hotpotqa-validation-2762", "mrqa_triviaqa-validation-457", "mrqa_triviaqa-validation-5071", "mrqa_naturalquestions-validation-5604", "mrqa_hotpotqa-validation-2564", "mrqa_triviaqa-validation-3300", "mrqa_squad-validation-6128", "mrqa_squad-validation-8589", "mrqa_triviaqa-validation-5513", "mrqa_naturalquestions-validation-954", "mrqa_squad-validation-7469", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-2493", "mrqa_naturalquestions-validation-9825", "mrqa_triviaqa-validation-2609", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-5307", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-743", "mrqa_naturalquestions-validation-4500", "mrqa_triviaqa-validation-4524", "mrqa_squad-validation-3106"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 32, "before_eval": {"predictions": ["an orbital scientific instrument package", "Orthodox Christians", "Fomento Econ\u00f3mico Mexicano", "beer", "gender test", "Matt Jones", "Health and Environmental effects", "in order to halt it following brake failure", "T cell", "relatively low salaries", "non-GMO", "Heading Out to the Highway", "Moonraker", "$12.99", "Michael Oppenheimer", "England national team", "rich and well socially standing Chinese", "Space is the Place", "Convention", "5,922", "December 5, 1991", "the title character in Luc Besson's \"Valerian and the City of a Thousand Planets\"", "76ers", "the historical Saint Nicholas ( a fourth - century Greek bishop and gift - giver of Myra ), the British figure of Father Christmas and the Dutch figure of Sinterklaas ( himself also based on Saint Nicholas )", "Stern-Plaza", "WBC/WBA heavyweight champion Joe Frazier", "23 March 1991", "lily-of-the-valley", "Dealey Plaza", "Nairobi", "During the last Ice Age", "Neon City"], "metric_results": {"EM": 0.28125, "QA-F1": 0.34318181818181814}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.1818181818181818, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.13333333333333333, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3066", "mrqa_naturalquestions-validation-4761", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-5510", "mrqa_triviaqa-validation-7156", "mrqa_hotpotqa-validation-1119", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2245", "mrqa_squad-validation-8617", "mrqa_hotpotqa-validation-305", "mrqa_squad-validation-8095", "mrqa_naturalquestions-validation-10311", "mrqa_hotpotqa-validation-2910", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1660", "mrqa_naturalquestions-validation-7049", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-2985", "mrqa_triviaqa-validation-1079", "mrqa_hotpotqa-validation-4221", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-5557"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 33, "before_eval": {"predictions": ["Habsburg control of the First Empire, the Spanish throne, and other royal houses", "\"Boston Herald\"", "1967", "\"Footprints in the Sand\"", "the twelfth most populous city in the United States", "115", "aribald haddock", "non- regulated growth stimulus", "lower", "J\u0101nis \u0160ipk\u0113vics", "Tevye", "New Orleans, Biloxi, Mississippi, Mobile, Alabama", "korea", "bridge", "Yunnan- Fu", "Mumbai", "Sydney", "2005", "all punishments", "\"The Doctor's Daughter\"", "bridge", "guerre", "The project must adhere to zoning and building code requirements", "1879", "daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia", "by staying with the same group of peers for all classes", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "enthusiasm and energy", "pasternak", "Datsun 810", "1983 to 1992", "T\u00f8nsberg"], "metric_results": {"EM": 0.15625, "QA-F1": 0.19946020074696547}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.14285714285714288, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11764705882352941, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-9984", "mrqa_hotpotqa-validation-2092", "mrqa_squad-validation-7278", "mrqa_naturalquestions-validation-8203", "mrqa_triviaqa-validation-1410", "mrqa_naturalquestions-validation-9271", "mrqa_squad-validation-7571", "mrqa_hotpotqa-validation-650", "mrqa_naturalquestions-validation-3523", "mrqa_squad-validation-10180", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-6499", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-5727", "mrqa_naturalquestions-validation-6358", "mrqa_squad-validation-7741", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-6186", "mrqa_squad-validation-6878", "mrqa_naturalquestions-validation-7387", "mrqa_hotpotqa-validation-2588", "mrqa_naturalquestions-validation-5297", "mrqa_squad-validation-2147", "mrqa_triviaqa-validation-2812", "mrqa_squad-validation-3733", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-1211"], "retrieved_ids": ["mrqa_naturalquestions-train-74228", "mrqa_naturalquestions-train-34003", "mrqa_naturalquestions-train-11403", "mrqa_naturalquestions-train-47600", "mrqa_naturalquestions-train-71615", "mrqa_naturalquestions-train-53606", "mrqa_naturalquestions-train-67181", "mrqa_naturalquestions-train-11357", "mrqa_naturalquestions-train-32620", "mrqa_naturalquestions-train-84583", "mrqa_naturalquestions-train-24162", "mrqa_naturalquestions-train-26876", "mrqa_naturalquestions-train-52895", "mrqa_naturalquestions-train-12311", "mrqa_naturalquestions-train-5928", "mrqa_naturalquestions-train-86445", "mrqa_naturalquestions-train-30523", "mrqa_naturalquestions-train-51582", "mrqa_naturalquestions-train-20981", "mrqa_naturalquestions-train-27894", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-388", "mrqa_triviaqa-validation-1616", "mrqa_naturalquestions-train-81657", "mrqa_naturalquestions-train-52050", "mrqa_naturalquestions-train-26623", "mrqa_naturalquestions-train-27538", "mrqa_naturalquestions-train-26674", "mrqa_squad-validation-7572", "mrqa_squad-validation-7301", "mrqa_naturalquestions-train-62064", "mrqa_naturalquestions-train-37611", "mrqa_naturalquestions-train-85867", "mrqa_naturalquestions-train-11406", "mrqa_naturalquestions-train-63956", "mrqa_naturalquestions-train-26211", "mrqa_naturalquestions-train-49261", "mrqa_naturalquestions-validation-9171", "mrqa_naturalquestions-train-19752", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-39157", "mrqa_naturalquestions-train-71415", "mrqa_naturalquestions-train-38514", "mrqa_naturalquestions-validation-4590", "mrqa_naturalquestions-train-70375", "mrqa_naturalquestions-train-12196", "mrqa_naturalquestions-train-64524", "mrqa_naturalquestions-train-52076", "mrqa_triviaqa-validation-2007", "mrqa_naturalquestions-train-6506", "mrqa_naturalquestions-train-16088", "mrqa_naturalquestions-train-52771", "mrqa_triviaqa-validation-7506", "mrqa_naturalquestions-train-28545", "mrqa_triviaqa-validation-5962", "mrqa_naturalquestions-train-80827", "mrqa_triviaqa-validation-5091", "mrqa_naturalquestions-train-43213", "mrqa_naturalquestions-train-6559", "mrqa_naturalquestions-train-43791", "mrqa_naturalquestions-train-31705", "mrqa_hotpotqa-validation-1855", "mrqa_hotpotqa-validation-1758", "mrqa_naturalquestions-train-80799", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-582", "mrqa_naturalquestions-train-48469", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-train-64329", "mrqa_naturalquestions-train-33644", "mrqa_naturalquestions-train-49634", "mrqa_naturalquestions-train-75032", "mrqa_squad-validation-7836", "mrqa_naturalquestions-train-1104", "mrqa_naturalquestions-train-214", "mrqa_naturalquestions-train-78190", "mrqa_naturalquestions-train-69270", "mrqa_triviaqa-validation-444", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-2007", "mrqa_naturalquestions-train-87815", "mrqa_naturalquestions-train-43511", "mrqa_naturalquestions-train-36180", "mrqa_naturalquestions-train-35577", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-3595", "mrqa_naturalquestions-train-8659", "mrqa_naturalquestions-train-47866", "mrqa_naturalquestions-train-62848", "mrqa_naturalquestions-train-38390", "mrqa_naturalquestions-train-11921", "mrqa_naturalquestions-train-20817", "mrqa_naturalquestions-train-41269", "mrqa_naturalquestions-train-24980", "mrqa_naturalquestions-train-31848", "mrqa_naturalquestions-train-7909", "mrqa_naturalquestions-train-44547", "mrqa_naturalquestions-train-58577", "mrqa_naturalquestions-train-23444", "mrqa_naturalquestions-train-34489", "mrqa_naturalquestions-train-75876", "mrqa_naturalquestions-train-62864", "mrqa_naturalquestions-train-69270", "mrqa_naturalquestions-train-34003", "mrqa_naturalquestions-train-18868", "mrqa_naturalquestions-train-35943", "mrqa_naturalquestions-train-34003", "mrqa_naturalquestions-train-9081", "mrqa_naturalquestions-train-76315", "mrqa_naturalquestions-train-69270", "mrqa_naturalquestions-train-78881", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-1280", "mrqa_triviaqa-validation-1306", "mrqa_naturalquestions-train-396", "mrqa_naturalquestions-train-19223", "mrqa_naturalquestions-train-58603", "mrqa_naturalquestions-train-34795", "mrqa_naturalquestions-train-87102", "mrqa_naturalquestions-train-7712", "mrqa_naturalquestions-train-42134", "mrqa_naturalquestions-train-42769", "mrqa_naturalquestions-train-51833", "mrqa_naturalquestions-train-84933", "mrqa_naturalquestions-train-69270", "mrqa_naturalquestions-train-73434", "mrqa_naturalquestions-train-66791", "mrqa_naturalquestions-train-42530", "mrqa_naturalquestions-train-14250"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 34, "before_eval": {"predictions": ["Chairman", "king George VI", "engaging in the forbidden speech", "Veronica", "Victorian College of the Arts and Melbourne Conservatorium of Music", "Britain", "boulangere", "0.52/ sq mi", "phileas Fogg", "France", "Ian Paisley", "bataan", "euro", "september 2009", "Imperial Japan", "1973", "1890", "Sam Bradford", "Stanwyck's bedroom window overlooks the night skyline of Manhattan", "Russell Crowe", "pole", "Johnny Darrell", "stroke", "margarine", "Euler's totient function", "ear canal", "binary strings", "Busiest airports in the United States by international passenger traffic", "orange", "Toyota Corolla", "Kurt Vonnegut", "princess"], "metric_results": {"EM": 0.125, "QA-F1": 0.2122481684981685}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.14285714285714288, 1.0, 1.0, 0.0, 0.5, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42857142857142855, 0.5, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7248", "mrqa_squad-validation-6673", "mrqa_naturalquestions-validation-6787", "mrqa_triviaqa-validation-5516", "mrqa_squad-validation-4369", "mrqa_triviaqa-validation-5496", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-3072", "mrqa_triviaqa-validation-1220", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-5740", "mrqa_naturalquestions-validation-4366", "mrqa_hotpotqa-validation-3002", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4288", "mrqa_triviaqa-validation-7592", "mrqa_squad-validation-4255", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-3408", "mrqa_squad-validation-1635", "mrqa_squad-validation-2751", "mrqa_triviaqa-validation-7184", "mrqa_squad-validation-3708", "mrqa_squad-validation-8034", "mrqa_triviaqa-validation-1198"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 35, "before_eval": {"predictions": ["up to 2% higher", "the interplay of supply and demand, which determines the prices of goods and services", "Emma Watson, Dan Stevens, Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Nathan Mack, Ian McKellen, and Emma Thompson", "the brain, muscles, and liver", "rings", "the Dallas Cowboys", "the General Assembly Hall of the Church of Scotland on the Royal Mile in Edinburgh", "William Howard Ashton", "wyoming", "promoting social dislocation, unrest and conflict", "Broward County", "Song Kang-ho, Lee Byung-hun, and Jung Woo-sung", "changing display or audio settings quickly", "cropredy Bridge", "derived from the spectroscopic notation for the associated atomic orbitals : sharp, principal, diffuse and fundamental, and then g which follows f in the alphabet", "declines over the medium term", "Beauty and the Beast", "South Africa", "Scotty Grainger Jr.", "alamo", "a seal illegally is broken", "United Methodist Church", "Brian Liesegang", "Don Hahn", "Port Moresby, Papua New Guinea", "Alvin and the Chipmunks", "National Association for the Advancement of Colored People", "1963\u20131989", "hland and Wolff", "2000", "Darrin Stephens", "6500 - 1500 BC"], "metric_results": {"EM": 0.21875, "QA-F1": 0.2988219246031746}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.125, 0.14285714285714288, 0.4, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.4444444444444445, 0.4, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-7704", "mrqa_triviaqa-validation-4829", "mrqa_naturalquestions-validation-4544", "mrqa_squad-validation-9400", "mrqa_triviaqa-validation-6423", "mrqa_squad-validation-7352", "mrqa_hotpotqa-validation-4447", "mrqa_hotpotqa-validation-2947", "mrqa_naturalquestions-validation-1587", "mrqa_triviaqa-validation-3767", "mrqa_naturalquestions-validation-585", "mrqa_squad-validation-7495", "mrqa_hotpotqa-validation-1475", "mrqa_triviaqa-validation-2999", "mrqa_squad-validation-3408", "mrqa_squad-validation-10036", "mrqa_hotpotqa-validation-3853", "mrqa_triviaqa-validation-11", "mrqa_hotpotqa-validation-1720", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-1812"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 36, "before_eval": {"predictions": ["National Broadcasting Company", "McG", "Uranus", "rudolf", "Cobham\u2013Edmonds thesis", "human, or humanoid aliens", "Best Male Pop Vocal Performance", "March 2012", "texas area", "Muhammad Ali", "Beyonc\u00e9 and Bruno Mars", "Spain", "to civil disobedients", "Julius Caesar", "2", "March 28, 1979", "a virtual reality simulator", "decision problem", "india", "heart", "Miasma theory", "serve beer, and also often for cider", "mountain ranges", "imperial war flag", "The U.S. state of Georgia", "perennial", "US $3 per barrel", "20 %", "david hartart", "to build a nationwide network in the UK", "roughly west through the Netherlands and extended to the southwest, through the English Channel and finally, to the Atlantic Ocean", "Sudan"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2681285014005602}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 1.0, 0.5, 0.35294117647058826, 0.4444444444444445, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.2222222222222222, 0.11764705882352941, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4824", "mrqa_triviaqa-validation-37", "mrqa_squad-validation-7720", "mrqa_hotpotqa-validation-4427", "mrqa_hotpotqa-validation-3911", "mrqa_triviaqa-validation-3803", "mrqa_hotpotqa-validation-2936", "mrqa_squad-validation-110", "mrqa_naturalquestions-validation-6011", "mrqa_squad-validation-6759", "mrqa_naturalquestions-validation-4115", "mrqa_hotpotqa-validation-1884", "mrqa_naturalquestions-validation-3993", "mrqa_squad-validation-1634", "mrqa_triviaqa-validation-5936", "mrqa_naturalquestions-validation-5552", "mrqa_squad-validation-4877", "mrqa_naturalquestions-validation-8525", "mrqa_triviaqa-validation-199", "mrqa_naturalquestions-validation-3344", "mrqa_triviaqa-validation-4069", "mrqa_squad-validation-3635", "mrqa_triviaqa-validation-6290", "mrqa_squad-validation-4626", "mrqa_squad-validation-9146", "mrqa_hotpotqa-validation-452"], "retrieved_ids": ["mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-6639", "mrqa_triviaqa-validation-7248", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-7512", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-7090", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-5487", "mrqa_naturalquestions-train-79942", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-train-13352", "mrqa_naturalquestions-train-50299", "mrqa_naturalquestions-train-40839", "mrqa_naturalquestions-train-46161", "mrqa_naturalquestions-train-20807", "mrqa_naturalquestions-train-49463", "mrqa_naturalquestions-train-9732", "mrqa_naturalquestions-train-22546", "mrqa_naturalquestions-train-23410", "mrqa_naturalquestions-train-85292", "mrqa_naturalquestions-train-49664", "mrqa_naturalquestions-train-40976", "mrqa_naturalquestions-train-80721", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-1410", "mrqa_naturalquestions-train-50792", "mrqa_naturalquestions-train-5558", "mrqa_naturalquestions-train-17330", "mrqa_naturalquestions-train-65378", "mrqa_naturalquestions-train-24788", "mrqa_squad-validation-108", "mrqa_naturalquestions-train-3943", "mrqa_naturalquestions-train-37704", "mrqa_naturalquestions-train-40054", "mrqa_naturalquestions-train-7289", "mrqa_naturalquestions-train-34003", "mrqa_naturalquestions-train-9804", "mrqa_naturalquestions-train-11503", "mrqa_naturalquestions-train-69270", "mrqa_naturalquestions-train-30958", "mrqa_naturalquestions-train-17355", "mrqa_naturalquestions-train-77024", "mrqa_squad-validation-6915", "mrqa_naturalquestions-train-68904", "mrqa_squad-validation-6733", "mrqa_naturalquestions-train-16951", "mrqa_naturalquestions-train-25975", "mrqa_naturalquestions-train-61880", "mrqa_naturalquestions-train-83831", "mrqa_naturalquestions-train-21935", "mrqa_naturalquestions-train-33296", "mrqa_naturalquestions-train-5350", "mrqa_naturalquestions-train-30547", "mrqa_naturalquestions-train-65265", "mrqa_naturalquestions-train-61269", "mrqa_naturalquestions-train-81657", "mrqa_naturalquestions-train-16149", "mrqa_naturalquestions-train-12896", "mrqa_naturalquestions-train-83458", "mrqa_naturalquestions-train-6703", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-train-38724", "mrqa_naturalquestions-train-9081", "mrqa_naturalquestions-train-22692", "mrqa_squad-validation-1635", "mrqa_hotpotqa-validation-2449", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-train-86685", "mrqa_naturalquestions-train-24957", "mrqa_naturalquestions-train-4531", "mrqa_naturalquestions-train-15972", "mrqa_naturalquestions-train-23433", "mrqa_naturalquestions-train-46650", "mrqa_naturalquestions-train-40233", "mrqa_naturalquestions-train-52318", "mrqa_naturalquestions-train-1172", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-train-20443", "mrqa_naturalquestions-train-33235", "mrqa_naturalquestions-train-16782", "mrqa_naturalquestions-train-64697", "mrqa_naturalquestions-train-20514", "mrqa_naturalquestions-train-24947", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-train-70965", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-5516", "mrqa_naturalquestions-train-52895", "mrqa_naturalquestions-train-79169", "mrqa_naturalquestions-train-74764", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-train-34447", "mrqa_naturalquestions-validation-10676", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-train-87942", "mrqa_triviaqa-validation-388", "mrqa_naturalquestions-train-69493", "mrqa_naturalquestions-train-26677", "mrqa_squad-validation-9984", "mrqa_naturalquestions-train-41668", "mrqa_naturalquestions-train-4244", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-1616", "mrqa_naturalquestions-train-53606", "mrqa_naturalquestions-train-79942", "mrqa_naturalquestions-train-9056", "mrqa_naturalquestions-train-2833", "mrqa_naturalquestions-train-62285", "mrqa_squad-validation-9319", "mrqa_naturalquestions-train-69860", "mrqa_naturalquestions-train-26741", "mrqa_naturalquestions-train-9929", "mrqa_naturalquestions-train-37664", "mrqa_naturalquestions-train-66044", "mrqa_squad-validation-9984", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-40945", "mrqa_naturalquestions-train-34003"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 37, "before_eval": {"predictions": ["Amtrak San Joaquins", "broken", "2007", "Post Alley under Pike Place Market", "mother-of-pearl", "February 20, 1978", "haggis", "Walter Mondale", "96", "De Inventione by Marcus Tullius Cicero", "japan", "black", "the alluvial plain", "37 \u00b0 9' 58.23\" latitude, around 11 miles (18 km) south of San Jose", "Spotty Dog", "Rumplestiltskin", "Carlos Tevez", "large birds or mammals", "many events and festivals", "tintrocken", "1991", "india", "7 January 1936", "lifetime protection", "twenty-four", "Carl Sagan", "lack of the city's tax base", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Christian Bj\u00f8rnsh\u00f8j Poulsen", "defiant speech", "sour cream", "Boston, Massachusetts"], "metric_results": {"EM": 0.28125, "QA-F1": 0.35121116628469573}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true], "QA-F1": [0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.23529411764705882, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727272, 0.13333333333333333, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-4768", "mrqa_hotpotqa-validation-104", "mrqa_triviaqa-validation-1015", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-3479", "mrqa_squad-validation-5702", "mrqa_naturalquestions-validation-9058", "mrqa_squad-validation-2660", "mrqa_triviaqa-validation-5258", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-1698", "mrqa_squad-validation-677", "mrqa_triviaqa-validation-7134", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-3876", "mrqa_hotpotqa-validation-2377", "mrqa_squad-validation-7246", "mrqa_naturalquestions-validation-1375", "mrqa_hotpotqa-validation-633", "mrqa_squad-validation-6737", "mrqa_triviaqa-validation-2524"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 38, "before_eval": {"predictions": ["the NP-complete Boolean satisfiability problem", "Dan Stevens", "New England", "Etienne de Mestre", "dragon", "slavery", "American Indian allies", "a children's story published by John Newbery in London in 1765", "The longest rotation period ( 243 days ) of any planet in the Solar System and rotates in the opposite direction to most other planets ( meaning the Sun would rise in the west and set in the east )", "gathering money from the public", "Thorgan Hazard", "commissioned to purchase their required uniform items", "Jeff Meldrum", "741 weeks", "Norman Painting", "French and English", "The Paris Sisters", "suez Canal", "60", "laboratory", "the fact that there is no revising chamber", "norm", "when lifted to an extension field", "most of the items in the collection, unless those were newly accessioned into the collection, probably don't show up in the computer system", "does not satisfy the criteria for a medium of exchange", "strychnine", "Texas", "The early modern period began approximately in the early 16th century ; notable historical milestones included the European Renaissance, the Age of Discovery, and the Protestant Reformation", "Lord's", "eddy Shah", "Louis Prima", "in sequence with each heartbeat"], "metric_results": {"EM": 0.1875, "QA-F1": 0.29393393752808716}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.11764705882352941, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0606060606060606, 0.47058823529411764, 0.6666666666666666, 0.22222222222222224, 0.25, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.6, 0.0, 0.0, 0.34782608695652173, 0.0, 1.0, 0.0, 0.3846153846153846, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-8418", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-10255", "mrqa_hotpotqa-validation-727", "mrqa_naturalquestions-validation-4067", "mrqa_hotpotqa-validation-1116", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-4762", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-6210", "mrqa_squad-validation-1660", "mrqa_squad-validation-9478", "mrqa_triviaqa-validation-1473", "mrqa_squad-validation-9032", "mrqa_squad-validation-5505", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-7630", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-2100", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2555"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 39, "before_eval": {"predictions": ["Republic of Taiwan", "Dan Conner", "karl karl", "president k Kennedy", "Katharine Hepburn, Joan Bennett, Frances Dee, and Jean Parker", "violence", "Joaquin Phoenix as Cash, Reese Witherspoon as Carter, Ginnifer Goodwin as Vivian Liberto, and Robert Patrick as Cash's father", "kuskusi", "1977", "John M. Grunsfeld", "New York City", "elton john", "2000", "antlers are dropped or shed and grown anew each and every year", "Fabbrica Italiana Automobili Torino", "the second Sunday of March", "relative units of force and mass then are fixed", "lady", "two", "August 10, 1933", "Golden Gate Bridge", "Sochi, Russia", "those who already hold wealth", "B. Traven", "Finding Nemo", "the subject of unidentified flying objects (UFOs) the extraterrestrial hypothesis (ETH) as well as paranormal and Fortean subjects in general", "oil prices", "the black bear, white-tailed deer, raccoon, coyote, grey squirrel, chipmunk, and other small rodents", "264,152", "Princeton", "the United States", "high pressure or an electric current"], "metric_results": {"EM": 0.28125, "QA-F1": 0.3902254543701912}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.0, 0.8, 1.0, 0.9230769230769231, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.10526315789473684, 0.0, 0.14285714285714288, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-1438", "mrqa_hotpotqa-validation-944", "mrqa_naturalquestions-validation-9227", "mrqa_triviaqa-validation-2522", "mrqa_naturalquestions-validation-6554", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-7195", "mrqa_naturalquestions-validation-3698", "mrqa_triviaqa-validation-3017", "mrqa_squad-validation-10428", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-4024", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-1607", "mrqa_triviaqa-validation-2808", "mrqa_triviaqa-validation-1276", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-2203", "mrqa_squad-validation-3650"], "retrieved_ids": ["mrqa_triviaqa-validation-3238", "mrqa_hotpotqa-validation-220", "mrqa_triviaqa-validation-2368", "mrqa_triviaqa-validation-3300", "mrqa_naturalquestions-train-66044", "mrqa_triviaqa-validation-1995", "mrqa_naturalquestions-train-24615", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-7578", "mrqa_triviaqa-validation-2703", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-3420", "mrqa_naturalquestions-train-83407", "mrqa_naturalquestions-train-75754", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-train-2187", "mrqa_naturalquestions-train-10375", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-train-77573", "mrqa_naturalquestions-train-31867", "mrqa_naturalquestions-train-54644", "mrqa_naturalquestions-train-28545", "mrqa_triviaqa-validation-2722", "mrqa_naturalquestions-train-56337", "mrqa_naturalquestions-validation-10676", "mrqa_triviaqa-validation-5091", "mrqa_naturalquestions-train-6525", "mrqa_naturalquestions-train-14373", "mrqa_naturalquestions-train-57237", "mrqa_naturalquestions-train-22760", "mrqa_naturalquestions-train-6937", "mrqa_naturalquestions-train-45745", "mrqa_triviaqa-validation-2959", "mrqa_naturalquestions-train-66819", "mrqa_naturalquestions-train-4551", "mrqa_triviaqa-validation-1280", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-train-1898", "mrqa_naturalquestions-train-8681", "mrqa_naturalquestions-train-20155", "mrqa_naturalquestions-train-84346", "mrqa_naturalquestions-train-16498", "mrqa_triviaqa-validation-7592", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-1616", "mrqa_naturalquestions-train-65495", "mrqa_naturalquestions-train-80958", "mrqa_naturalquestions-train-49719", "mrqa_naturalquestions-train-56048", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-train-28586", "mrqa_naturalquestions-train-62448", "mrqa_naturalquestions-train-67689", "mrqa_naturalquestions-train-34229", "mrqa_naturalquestions-train-42964", "mrqa_naturalquestions-train-33009", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-5997", "mrqa_naturalquestions-train-7732", "mrqa_naturalquestions-train-52170", "mrqa_hotpotqa-validation-1390", "mrqa_naturalquestions-train-2864", "mrqa_naturalquestions-train-47177", "mrqa_naturalquestions-train-2006", "mrqa_naturalquestions-train-11262", "mrqa_naturalquestions-train-23297", "mrqa_naturalquestions-train-18324", "mrqa_naturalquestions-train-2860", "mrqa_naturalquestions-train-60641", "mrqa_naturalquestions-train-49567", "mrqa_naturalquestions-train-65943", "mrqa_naturalquestions-train-43791", "mrqa_naturalquestions-train-21641", "mrqa_naturalquestions-train-52202", "mrqa_naturalquestions-train-33619", "mrqa_naturalquestions-train-84830", "mrqa_triviaqa-validation-1540", "mrqa_squad-validation-7352", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-5997", "mrqa_naturalquestions-train-64931", "mrqa_naturalquestions-train-3454", "mrqa_hotpotqa-validation-5", "mrqa_naturalquestions-train-79039", "mrqa_naturalquestions-train-56271", "mrqa_naturalquestions-train-45892", "mrqa_naturalquestions-train-62065", "mrqa_naturalquestions-validation-1328", "mrqa_squad-validation-7836", "mrqa_naturalquestions-train-41621", "mrqa_naturalquestions-train-6172", "mrqa_naturalquestions-train-45778", "mrqa_naturalquestions-train-20678", "mrqa_naturalquestions-train-20436", "mrqa_hotpotqa-validation-1758", "mrqa_naturalquestions-train-557", "mrqa_naturalquestions-train-8807", "mrqa_naturalquestions-train-75292", "mrqa_naturalquestions-train-59761", "mrqa_squad-validation-3442"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 40, "before_eval": {"predictions": ["50 fund", "Bartle Frere", "Samarkand", "bowie", "Isabella (Belle) Baumfree", "Willow and Holly", "14th to 17th centuries", "five starting pitchers, seven relief pitchers, two catchers, six infielders, and five outfielders", "his main interest was centered on the prophecy of the Little Horn in Daniel 8:9\u201312, 23\u201325", "Aristotle", "Charlton Heston", "anti-inflammatory molecules", "Highland garb", "berlin trilogy", "a tradeable entity used to avoid the inconveniences of a pure barter system", "United States Presidents", "son et lumi\u00e8re", "when the Mongols placed the Uighurs of the Kingdom of Qocho over the Koreans at the court the Korean King objected", "Sochi, Russia", "left", "Hudson Bay", "NASA immediately convened an accident review board", "australia", "doreen", "30 Major League Baseball teams and their 160 minor league baseball affiliates", "Secret Intelligence Service", "100 billion", "kai su, teknon", "photosynthesis", "4.7 / 5.5 - inch", "Queen City", "an American federal law that imposes liability on persons and companies ( typically federal contractors ) who defraud governmental programs"], "metric_results": {"EM": 0.21875, "QA-F1": 0.28547390995872035}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.375, 0.10526315789473682, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.09523809523809522, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.5882352941176471, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6212", "mrqa_squad-validation-6248", "mrqa_triviaqa-validation-1571", "mrqa_triviaqa-validation-2475", "mrqa_naturalquestions-validation-8961", "mrqa_squad-validation-2249", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-validation-7457", "mrqa_squad-validation-6588", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3213", "mrqa_naturalquestions-validation-1063", "mrqa_triviaqa-validation-6127", "mrqa_squad-validation-8247", "mrqa_triviaqa-validation-1859", "mrqa_squad-validation-3932", "mrqa_triviaqa-validation-3280", "mrqa_triviaqa-validation-4123", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1791", "mrqa_triviaqa-validation-1877", "mrqa_triviaqa-validation-579", "mrqa_squad-validation-3617", "mrqa_naturalquestions-validation-6848", "mrqa_naturalquestions-validation-993"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 41, "before_eval": {"predictions": ["all war", "Jyoti Swaroop", "Gaels", "Three card brag", "d\u00edsir", "lion", "Russian film industry", "sediment load", "Washington metropolitan area", "the scission of newly formed vesicles from the membrane of one cellular compartment and their targeting to, and fusion with, another compartment", "User State Migration Tool ( USMT )", "Ordos City China Science Flying Universe Science and Technology Co.", "plastics", "PPG Paints Arena, Pittsburgh, Pennsylvania", "edward ii", "Section 30 of the Teaching Council Act 2001", "agnes Moorehead as the Goose, an unnamed goose who is the one that encourages Wilbur to speak for the first time", "mid-1988", "quasars", "Monsoon", "Romansh", "ltd", "Fresh 92.7", "Ian Fleming's James Bond quartermaster Q", "Paul and Timothy first visited Philippi in Greece during Paul's second missionary journey, which occurred between approximately 49 and 51 AD", "the division of labour, productivity, and free markets", "Margaret Pellegrini", "Whitney Houston", "Nebula Award", "united Kingdom", "Saul", "Elvis Presley"], "metric_results": {"EM": 0.09375, "QA-F1": 0.17755766900814068}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7547169811320755, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-2375", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-9712", "mrqa_hotpotqa-validation-201", "mrqa_triviaqa-validation-4955", "mrqa_naturalquestions-validation-3058", "mrqa_triviaqa-validation-684", "mrqa_squad-validation-2142", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-5007", "mrqa_triviaqa-validation-2384", "mrqa_naturalquestions-validation-774", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-2181", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-4823", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8338", "mrqa_hotpotqa-validation-3872", "mrqa_triviaqa-validation-1585", "mrqa_hotpotqa-validation-434", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-4320", "mrqa_naturalquestions-validation-9763"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 42, "before_eval": {"predictions": ["Daryl Hannah", "sometimes contains pasta ( usually cavatelli, acini di pepe, pastina, orzo, etc. ), lentils, or grated parmesan cheese", "brian", "independence from the Duke of Savoy through an alliance between the city-state of Geneva and the Swiss Confederation", "the ozone generated in contact with the skin, and to a lesser extent, by nitrous acid", "conflicts involving slavery and states'rights", "physicians, lawyers, engineers, and accountants", "lack of remorse, an attempt to avoid responsibility for her actions, and even a likelihood of repeating her illegal actions", "a large Danish shipping company that operates passenger and freight services across northern Europe", "Giorgio Chiellini", "rommel", "the duodenum", "disaccharide sucrose", "their unusual behavior, such as the number of men killed and the manner of the attacks", "Gainsborough Trinity Football Club", "public and private collections", "Extroverted Feeling ( Fi )", "Thursday", "yellow", "the appropriateness of the drug therapy (e.g. drug choice, dose, route, frequency, and duration of therapy) and its efficacy", "jupiter", "in feats of exploration", "a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "bobby Kennedy", "The Education Service Contracting scheme of the government", "bow hair", "to raise money to rebuild St. Peter's Basilica in Rome", "Britain", "two forces, one pointing north, and one pointing east", "Bills", "Jack Murphy Stadium", "3 much more additional time or space is needed in order to increase the number of problems that can be solved"], "metric_results": {"EM": 0.0625, "QA-F1": 0.24811256451881453}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.14285714285714288, 0.0, 0.0, 0.0, 0.0, 0.375, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6923076923076924, 0.0, 0.8571428571428571, 0.13333333333333333, 0.0, 0.1818181818181818, 0.6666666666666666, 0.5714285714285715, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2952", "mrqa_naturalquestions-validation-8284", "mrqa_triviaqa-validation-3261", "mrqa_squad-validation-3044", "mrqa_naturalquestions-validation-9093", "mrqa_squad-validation-1841", "mrqa_squad-validation-6735", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-5731", "mrqa_triviaqa-validation-4137", "mrqa_naturalquestions-validation-7233", "mrqa_naturalquestions-validation-4185", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-680", "mrqa_naturalquestions-validation-6706", "mrqa_squad-validation-9569", "mrqa_triviaqa-validation-7426", "mrqa_squad-validation-6369", "mrqa_triviaqa-validation-7133", "mrqa_hotpotqa-validation-4130", "mrqa_squad-validation-3385", "mrqa_triviaqa-validation-1586", "mrqa_squad-validation-7034", "mrqa_triviaqa-validation-289", "mrqa_squad-validation-2150", "mrqa_squad-validation-9792", "mrqa_squad-validation-10395", "mrqa_hotpotqa-validation-5522", "mrqa_squad-validation-1808"], "retrieved_ids": ["mrqa_naturalquestions-train-43105", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-5997", "mrqa_naturalquestions-train-40409", "mrqa_naturalquestions-train-70899", "mrqa_naturalquestions-train-30531", "mrqa_naturalquestions-train-3175", "mrqa_naturalquestions-train-19954", "mrqa_naturalquestions-train-27314", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-6683", "mrqa_naturalquestions-train-68895", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-1494", "mrqa_naturalquestions-train-50097", "mrqa_naturalquestions-train-62864", "mrqa_hotpotqa-validation-5526", "mrqa_naturalquestions-train-7910", "mrqa_naturalquestions-train-59591", "mrqa_naturalquestions-train-55611", "mrqa_naturalquestions-train-79237", "mrqa_naturalquestions-train-62840", "mrqa_naturalquestions-train-80566", "mrqa_naturalquestions-train-77303", "mrqa_naturalquestions-train-42131", "mrqa_naturalquestions-train-24119", "mrqa_naturalquestions-train-72269", "mrqa_naturalquestions-train-34788", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-train-80212", "mrqa_naturalquestions-train-17257", "mrqa_naturalquestions-train-30204", "mrqa_naturalquestions-train-78237", "mrqa_naturalquestions-train-84583", "mrqa_naturalquestions-train-51088", "mrqa_naturalquestions-train-10001", "mrqa_hotpotqa-validation-5526", "mrqa_naturalquestions-train-68447", "mrqa_naturalquestions-train-824", "mrqa_naturalquestions-train-12584", "mrqa_squad-validation-9984", "mrqa_naturalquestions-train-19223", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-47600", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-2999", "mrqa_triviaqa-validation-2959", "mrqa_naturalquestions-train-63918", "mrqa_naturalquestions-train-87196", "mrqa_naturalquestions-train-40541", "mrqa_naturalquestions-train-49810", "mrqa_naturalquestions-train-9107", "mrqa_naturalquestions-train-15440", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-63821", "mrqa_naturalquestions-train-25251", "mrqa_naturalquestions-train-53810", "mrqa_naturalquestions-train-7004", "mrqa_naturalquestions-train-811", "mrqa_naturalquestions-train-56859", "mrqa_naturalquestions-train-4338", "mrqa_naturalquestions-train-6133", "mrqa_naturalquestions-train-61701", "mrqa_naturalquestions-train-67597", "mrqa_naturalquestions-train-21281", "mrqa_naturalquestions-train-24847", "mrqa_hotpotqa-validation-3080", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-train-46652", "mrqa_hotpotqa-validation-4537", "mrqa_triviaqa-validation-2007", "mrqa_hotpotqa-validation-1509", "mrqa_naturalquestions-train-76529", "mrqa_naturalquestions-train-68784", "mrqa_naturalquestions-train-32005", "mrqa_naturalquestions-train-78752", "mrqa_naturalquestions-train-78786", "mrqa_squad-validation-9568", "mrqa_hotpotqa-validation-3774", "mrqa_naturalquestions-train-7959", "mrqa_naturalquestions-train-37664", "mrqa_naturalquestions-train-70553", "mrqa_triviaqa-validation-199", "mrqa_naturalquestions-train-24947", "mrqa_naturalquestions-train-36138", "mrqa_triviaqa-validation-5168", "mrqa_naturalquestions-train-79425", "mrqa_naturalquestions-train-80987", "mrqa_naturalquestions-train-37019", "mrqa_naturalquestions-train-42291", "mrqa_naturalquestions-train-73711", "mrqa_naturalquestions-train-56352", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-5168", "mrqa_naturalquestions-train-49463", "mrqa_naturalquestions-train-34003", "mrqa_squad-validation-9984", "mrqa_naturalquestions-train-71615", "mrqa_naturalquestions-train-74228", "mrqa_naturalquestions-train-42632", "mrqa_naturalquestions-validation-4165", "mrqa_squad-validation-9032", "mrqa_naturalquestions-train-5004", "mrqa_naturalquestions-train-15567", "mrqa_naturalquestions-train-31868", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-314", "mrqa_triviaqa-validation-7578", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-6259", "mrqa_naturalquestions-train-11208", "mrqa_naturalquestions-train-46254", "mrqa_naturalquestions-train-24709", "mrqa_naturalquestions-train-12079", "mrqa_naturalquestions-train-83106", "mrqa_triviaqa-validation-2896", "mrqa_triviaqa-validation-3808", "mrqa_triviaqa-validation-3387", "mrqa_triviaqa-validation-2376", "mrqa_triviaqa-validation-1494", "mrqa_naturalquestions-train-17252", "mrqa_naturalquestions-train-69829", "mrqa_naturalquestions-train-66546", "mrqa_naturalquestions-train-84345", "mrqa_naturalquestions-train-55149", "mrqa_naturalquestions-train-58577", "mrqa_naturalquestions-train-76437", "mrqa_naturalquestions-train-6506", "mrqa_naturalquestions-train-19270", "mrqa_naturalquestions-train-58318", "mrqa_naturalquestions-train-80958", "mrqa_naturalquestions-train-49185", "mrqa_naturalquestions-train-75592", "mrqa_naturalquestions-train-60484", "mrqa_naturalquestions-train-49719", "mrqa_naturalquestions-train-72567", "mrqa_naturalquestions-train-34711", "mrqa_squad-validation-2428", "mrqa_naturalquestions-train-51426", "mrqa_naturalquestions-train-57621", "mrqa_squad-validation-4369", "mrqa_naturalquestions-train-52028", "mrqa_naturalquestions-train-41648", "mrqa_naturalquestions-train-84147", "mrqa_naturalquestions-train-58365"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 43, "before_eval": {"predictions": ["the Niagara River", "square", "Indiana", "temple square", "French", "a \"homeward bounder\" a sailor coming home from a round trip", "hostnames", "disorders of the immune system can result in autoimmune diseases, inflammatory diseases and cancer", "Raoul de Nangis", "vegan, some contain propolis and/or beeswax sourced from bees", "fly", "Rigoletto", "Russia is the largest country on Earth by land area, distances within Russia can be very long, and air travel is frequently needed for the President to travel across the country as well as internationally.", "third-most abundant element in the universe", "furniture", "216", "eribe peralta", "Nicholas Stone, Caius Gabriel Cibber, Grinling Gibbons, John Michael Rysbrack, Louis-Fran\u00e7ois Roubiliac, Peter Scheemakers", "Algernod Lanier Washington", "the Outfield", "bNP Paribas", "Edward Furlong", "railway locomotives", "Michael Wilding", "third quarter ( also known as last quarter )", "Bill Fraser", "chemists Glenn T. Seaborg, the developer of the actinide concept and Nobel Prize winning novelist Saul Bellow, political philosopher and author Allan Bloom", "Tennessee", "technology incidental to rocketry and manned spaceflight", "eve", "237 square miles", "magi"], "metric_results": {"EM": 0.0625, "QA-F1": 0.18773488562091503}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.5, 0.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.11764705882352941, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.4, 0.5, 0.6666666666666666, 0.5, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7744", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-6916", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-981", "mrqa_naturalquestions-validation-2663", "mrqa_squad-validation-6442", "mrqa_triviaqa-validation-5580", "mrqa_hotpotqa-validation-518", "mrqa_triviaqa-validation-5087", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-850", "mrqa_squad-validation-3671", "mrqa_triviaqa-validation-4090", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-5160", "mrqa_squad-validation-5586", "mrqa_hotpotqa-validation-5370", "mrqa_naturalquestions-validation-5822", "mrqa_triviaqa-validation-7482", "mrqa_squad-validation-3330", "mrqa_triviaqa-validation-2803", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-7215", "mrqa_squad-validation-8054", "mrqa_hotpotqa-validation-4624", "mrqa_squad-validation-3812", "mrqa_triviaqa-validation-5899", "mrqa_hotpotqa-validation-5541", "mrqa_triviaqa-validation-305"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 44, "before_eval": {"predictions": ["King T'Chaka of the African nation Wakanda", "income", "2003", "golf", "football", "campaign setting", "2003", "867 feet", "the Byzantine Empire ( as \u039c\u03b1\u03bd\u03bf\u03c5\u03ae\u03bb ) to Spain and Portugal", "Shape of You", "Christopher Lee as Count Dooku / Darth Tyranus", "14th", "chari bari ruchi-pip peri pembo", "hospitals", "increased patient health outcomes and decreased costs to the health care system", "treble clef", "Gawvi", "150 km / h", "between the Piazza di Spagna at the base and Piazz Trinit\u00e0 dei Monti", "December 1, 2009", "an English campaigner for the suffragette movement", "egypt", "philosophical advocate and practitioner of the scientific method", "birmingham", "Ministry of Corporate Affairs", "Irish", "ancient cult activity", "bront\u00eb sisters", "energy-storage molecules ATP and NADPH while freeing oxygen from water", "Hubble Space Telescope", "Sanctifying Grace", "Christ lag"], "metric_results": {"EM": 0.09375, "QA-F1": 0.20710880355276906}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.19999999999999998, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.5517241379310345, 0.0, 0.7272727272727273, 0.0, 0.0, 0.0, 0.39999999999999997, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-2236", "mrqa_naturalquestions-validation-2119", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-6579", "mrqa_hotpotqa-validation-5305", "mrqa_naturalquestions-validation-10619", "mrqa_hotpotqa-validation-910", "mrqa_naturalquestions-validation-10612", "mrqa_hotpotqa-validation-4649", "mrqa_naturalquestions-validation-5550", "mrqa_squad-validation-7067", "mrqa_triviaqa-validation-3664", "mrqa_squad-validation-6319", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-2406", "mrqa_hotpotqa-validation-4278", "mrqa_naturalquestions-validation-3459", "mrqa_naturalquestions-validation-8491", "mrqa_naturalquestions-validation-2169", "mrqa_hotpotqa-validation-4794", "mrqa_triviaqa-validation-5022", "mrqa_hotpotqa-validation-389", "mrqa_triviaqa-validation-3617", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-1725", "mrqa_triviaqa-validation-4027", "mrqa_squad-validation-8625", "mrqa_squad-validation-9951", "mrqa_squad-validation-2419"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 45, "before_eval": {"predictions": ["richard spillane", "Cleveland Browns", "perique", "cut off close by the hip", "death penalty", "a stout man with a \"practice chin, strong mouth, piercing deep-set eyes, fleshy face, and squat neck\"", "mexico", "rich Fisher king", "Mangal Pandey of the 34th BNI", "V Alaudae, a Celtic legion recruited from Gallia Narbonensis and XXI, possibly a Galatian legion from the other side of the empire", "Lorne Greene", "four of the 50 states of the United States in their full official state names", "sweden", "first group", "Pebble Beach", "Los Angeles", "French", "Gareth", "\"LOVE Radio\"", "Miami Marlins", "the court from its members for a three - year term", "richard burton", "David Dobkin", "brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil to put an end to Saddam Hussein's occupation of Kuwait", "The Zombies", "Fox News Specialists", "usually scheduled for the Thursday following Labor Day and since 2004, it was hosted by the most recent Super Bowl champions", "Lunar Excursion Module (LEM, later shortened to Lunar Module, LM)", "San Francisco Bay Area at Santa Clara, California", "a fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Operation Neptune", "peninsular mainland"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2883985805860806}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.7692307692307693, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 0.07142857142857142, 0.0, 0.0, 0.4166666666666667, 0.16666666666666669, 0.4, 0.07142857142857142, 1.0, 0.6666666666666666]}}, "error_ids": ["mrqa_triviaqa-validation-7473", "mrqa_hotpotqa-validation-1907", "mrqa_triviaqa-validation-2201", "mrqa_naturalquestions-validation-4123", "mrqa_squad-validation-2598", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-5587", "mrqa_naturalquestions-validation-4097", "mrqa_squad-validation-9296", "mrqa_naturalquestions-validation-8728", "mrqa_triviaqa-validation-3335", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3363", "mrqa_hotpotqa-validation-5068", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-5149", "mrqa_naturalquestions-validation-9931", "mrqa_triviaqa-validation-455", "mrqa_squad-validation-9513", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-3509", "mrqa_naturalquestions-validation-3217", "mrqa_squad-validation-3845", "mrqa_squad-validation-13", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-2067"], "retrieved_ids": ["mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-7215", "mrqa_triviaqa-validation-4054", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-3420", "mrqa_naturalquestions-train-64601", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-3002", "mrqa_naturalquestions-train-45569", "mrqa_naturalquestions-train-85981", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-7592", "mrqa_naturalquestions-train-69860", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-5261", "mrqa_naturalquestions-train-37249", "mrqa_naturalquestions-train-53290", "mrqa_naturalquestions-train-70332", "mrqa_naturalquestions-train-10617", "mrqa_naturalquestions-train-32874", "mrqa_naturalquestions-train-12228", "mrqa_hotpotqa-validation-482", "mrqa_naturalquestions-train-76701", "mrqa_naturalquestions-train-52667", "mrqa_squad-validation-7836", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-1588", "mrqa_naturalquestions-train-41055", "mrqa_triviaqa-validation-1770", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-2959", "mrqa_naturalquestions-train-33619", "mrqa_hotpotqa-validation-5526", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-train-21281", "mrqa_naturalquestions-train-43519", "mrqa_naturalquestions-train-39439", "mrqa_naturalquestions-train-18324", "mrqa_naturalquestions-train-2860", "mrqa_naturalquestions-train-62048", "mrqa_naturalquestions-train-38469", "mrqa_naturalquestions-train-76300", "mrqa_naturalquestions-train-48976", "mrqa_naturalquestions-train-5381", "mrqa_naturalquestions-train-27198", "mrqa_naturalquestions-train-34964", "mrqa_naturalquestions-train-41952", "mrqa_naturalquestions-train-6506", "mrqa_naturalquestions-train-33774", "mrqa_naturalquestions-train-34624", "mrqa_naturalquestions-train-13488", "mrqa_naturalquestions-train-4618", "mrqa_naturalquestions-train-48903", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-train-72653", "mrqa_naturalquestions-train-15915", "mrqa_naturalquestions-train-4516", "mrqa_naturalquestions-train-43056", "mrqa_naturalquestions-train-64128", "mrqa_naturalquestions-train-22654", "mrqa_naturalquestions-train-58088", "mrqa_naturalquestions-train-21738", "mrqa_naturalquestions-train-2833", "mrqa_naturalquestions-train-81334", "mrqa_naturalquestions-train-44124", "mrqa_naturalquestions-train-34", "mrqa_naturalquestions-train-60220", "mrqa_naturalquestions-train-50459", "mrqa_naturalquestions-train-22438", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-2403", "mrqa_naturalquestions-train-64528", "mrqa_naturalquestions-train-76561", "mrqa_naturalquestions-train-32795", "mrqa_naturalquestions-train-8443", "mrqa_naturalquestions-train-25583", "mrqa_naturalquestions-train-7251", "mrqa_naturalquestions-train-21885", "mrqa_naturalquestions-train-69635", "mrqa_naturalquestions-train-74515", "mrqa_naturalquestions-train-21130", "mrqa_naturalquestions-train-66564", "mrqa_naturalquestions-train-21283", "mrqa_naturalquestions-train-14342", "mrqa_naturalquestions-train-36180", "mrqa_naturalquestions-train-24615", "mrqa_naturalquestions-train-27643", "mrqa_hotpotqa-validation-1758", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-49100", "mrqa_naturalquestions-train-44117", "mrqa_naturalquestions-train-61034", "mrqa_naturalquestions-train-79711", "mrqa_naturalquestions-train-47056", "mrqa_naturalquestions-train-85298", "mrqa_naturalquestions-train-83407", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-train-6494", "mrqa_naturalquestions-train-40122", "mrqa_naturalquestions-train-71230", "mrqa_naturalquestions-train-77865", "mrqa_squad-validation-7836", "mrqa_naturalquestions-train-23098", "mrqa_naturalquestions-train-44632", "mrqa_naturalquestions-validation-3209", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-train-71254", "mrqa_naturalquestions-train-28414", "mrqa_naturalquestions-train-8824", "mrqa_naturalquestions-train-10237", "mrqa_naturalquestions-train-60865", "mrqa_naturalquestions-train-21514", "mrqa_naturalquestions-train-58698", "mrqa_naturalquestions-train-58865", "mrqa_naturalquestions-train-6133", "mrqa_squad-validation-315", "mrqa_naturalquestions-train-52322", "mrqa_naturalquestions-train-16813", "mrqa_naturalquestions-train-7055", "mrqa_naturalquestions-train-811", "mrqa_naturalquestions-train-15372", "mrqa_naturalquestions-train-22308", "mrqa_naturalquestions-train-86520", "mrqa_naturalquestions-train-50358", "mrqa_naturalquestions-train-32716", "mrqa_naturalquestions-train-12258", "mrqa_naturalquestions-train-8706", "mrqa_naturalquestions-train-43791", "mrqa_naturalquestions-train-17684"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 46, "before_eval": {"predictions": ["bat-and-ball", "convection currents in the asthenosphere, which is ductile, or plastic, and the brittle lithosphere ( crust and upper mantle )", "take That, East 17 and Boyzone", "being one of the youngest publicly documented people to be identified as transgender, and for being the youngest person to become a national transgender figure", "electric lighting", "used their knowledge of Native American languages as a basis to transmit coded messages", "Einstein", "electromagnetic theory", "Premier League club Swansea City", "jonathan holman hunt", "Elizabeth Weber", "video game", "hundreds", "The Nightmare Before Christmas", "June 22, 1978", "a new facility on The Watermark business park next to the MetroCentre in Gateshead", "apple", "partial funding", "5% abv pale lager", "production of more of at least one good without sacrificing the production of any other good", "Chu' Tsai", "Liz", "least onerous", "lake como", "Grissom, White, and Chaffee", "multinational retail corporation", "yellow", "Bharata Muni", "sand dunes cause a scrubbing noise as they rub against each other when walked on", "pGA", "parts of the air in the vessel were converted into the classical element fire", "imperial palace"], "metric_results": {"EM": 0.3125, "QA-F1": 0.4151580459770115}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, false, false, false, false, true, false, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.5, 0.4827586206896552, 1.0, 0.06666666666666667, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.42857142857142855, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0689655172413793, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_naturalquestions-validation-8204", "mrqa_triviaqa-validation-6905", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-5352", "mrqa_hotpotqa-validation-1831", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-3284", "mrqa_hotpotqa-validation-1074", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-73", "mrqa_squad-validation-5464", "mrqa_hotpotqa-validation-5239", "mrqa_naturalquestions-validation-2890", "mrqa_squad-validation-6271", "mrqa_triviaqa-validation-2914", "mrqa_triviaqa-validation-1128", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-2808", "mrqa_triviaqa-validation-2873", "mrqa_squad-validation-3523", "mrqa_triviaqa-validation-4430"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 47, "before_eval": {"predictions": ["margaret", "taghrooda", "New Zealand national team", "at the'Lord's Enclosure' (Mongolian: Edsen Khoroo) in Mongolia", "Styal Mill", "William Jennings Bryan", "Milk Barn Animation", "when they enter the army during initial entry training", "moral tale", "They announced a hiatus and re-united two years later for the release of their fourth and final studio album, Destiny Fulfilled ( 2004 )", "leeds", "star", "260", "\u00a35 one-way", "often social communities with considerable face-to-face interaction among members", "William Strauss and Neil Howe", "monophyletic", "insects", "candidates on specific catechism questions", "a pH indicator, a color marker, and a dye", "about 50% oxygen composition at standard pressure or 2.5 times the normal sea-level O2 partial pressure of about 21 kPa", "63,182,000", "Charles Wesley", "Science and Discovery", "Euclid's fundamental theorem of arithmetic", "Campbellsville", "car crash", "James Anthony Sturgess (born 16 May 1981) is an English actor and singer-songwriter. His breakthrough role was appearing as Jude in the musical romance drama film \"Cloud Atlas\" (2007)", "25.9 megajoules", "works in a bridal shop with Anita, the girlfriend of her brother, Bernardo", "Jocelyn Flores", "downward pressure on wages"], "metric_results": {"EM": 0.25, "QA-F1": 0.3742980365223012}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.11764705882352941, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7499999999999999, 0.5, 0.5384615384615384, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.07142857142857142, 0.0, 0.6666666666666665, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2770", "mrqa_triviaqa-validation-945", "mrqa_squad-validation-6287", "mrqa_hotpotqa-validation-5877", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2864", "mrqa_naturalquestions-validation-5305", "mrqa_triviaqa-validation-1516", "mrqa_hotpotqa-validation-5086", "mrqa_naturalquestions-validation-8582", "mrqa_squad-validation-4479", "mrqa_squad-validation-2346", "mrqa_naturalquestions-validation-7849", "mrqa_squad-validation-3685", "mrqa_triviaqa-validation-4298", "mrqa_hotpotqa-validation-2801", "mrqa_squad-validation-9061", "mrqa_hotpotqa-validation-662", "mrqa_triviaqa-validation-1799", "mrqa_hotpotqa-validation-4173", "mrqa_triviaqa-validation-7477", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-2092", "mrqa_squad-validation-7182"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 48, "before_eval": {"predictions": ["John Herrera", "Good Kid, M. a.A.D City", "el Capitan", "Interventive treatment", "3", "Lloyd Christ Wicke", "jonathan", "Goku becomes the first Saiyan in a thousand years to transform into a fabled Super Saiyan", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Social Democratic Party", "rue Miollis", "Thon Maker", "a loop ( also called a self - loop or a `` buckle '' )", "painting, mathematics, calligraphy, poetry, and theater", "the 1964 Republican National Convention in San Francisco, California", "good work", "annuity ticket", "Anakin Skywalker", "Buffalo Bill", "We will commit sins while we are here, for this life is not a place where justice resides", "France", "the United States declared neutrality and worked to broker a peace", "dry", "halal", "Hecuba", "to be memorised by the people themselves", "Wylie Draper", "political", "over the university's off-campus rental policies", "Manley MacDonald", "New England Patriots", "several existing conditions such as war, famine, and weather"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3411763583638584}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 0.0, 0.22222222222222218, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.15384615384615385, 0.0, 1.0, 0.1111111111111111, 1.0, 0.19999999999999998, 0.0, 0.0, 0.0, 0.2857142857142857, 0.2666666666666667, 1.0, 0.888888888888889, 0.4, 0.0, 0.6153846153846153]}}, "error_ids": ["mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-5165", "mrqa_triviaqa-validation-3967", "mrqa_squad-validation-5665", "mrqa_squad-validation-9860", "mrqa_triviaqa-validation-3906", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-2607", "mrqa_triviaqa-validation-2849", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2582", "mrqa_squad-validation-2259", "mrqa_naturalquestions-validation-3789", "mrqa_hotpotqa-validation-4075", "mrqa_squad-validation-2263", "mrqa_naturalquestions-validation-1649", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-4384", "mrqa_hotpotqa-validation-962", "mrqa_squad-validation-2337", "mrqa_naturalquestions-validation-1531", "mrqa_squad-validation-7947", "mrqa_hotpotqa-validation-599", "mrqa_squad-validation-264", "mrqa_squad-validation-4774"], "retrieved_ids": ["mrqa_naturalquestions-train-12079", "mrqa_naturalquestions-train-23538", "mrqa_naturalquestions-train-40062", "mrqa_naturalquestions-train-2259", "mrqa_naturalquestions-train-46451", "mrqa_hotpotqa-validation-3637", "mrqa_naturalquestions-train-49722", "mrqa_naturalquestions-train-26322", "mrqa_naturalquestions-train-29454", "mrqa_naturalquestions-train-80542", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-1451", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4137", "mrqa_naturalquestions-train-18107", "mrqa_naturalquestions-train-76315", "mrqa_naturalquestions-train-31987", "mrqa_naturalquestions-train-50948", "mrqa_naturalquestions-train-78752", "mrqa_naturalquestions-train-28007", "mrqa_naturalquestions-train-81426", "mrqa_naturalquestions-train-8619", "mrqa_naturalquestions-train-1912", "mrqa_naturalquestions-train-45584", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-6639", "mrqa_triviaqa-validation-1995", "mrqa_triviaqa-validation-2181", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-train-49731", "mrqa_naturalquestions-train-26147", "mrqa_naturalquestions-train-1201", "mrqa_naturalquestions-train-19", "mrqa_naturalquestions-train-58887", "mrqa_naturalquestions-train-811", "mrqa_naturalquestions-train-8896", "mrqa_naturalquestions-train-23387", "mrqa_naturalquestions-train-1948", "mrqa_naturalquestions-train-3911", "mrqa_triviaqa-validation-2715", "mrqa_naturalquestions-train-5946", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-3595", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-5160", "mrqa_triviaqa-validation-3515", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-7578", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-train-18261", "mrqa_naturalquestions-train-24879", "mrqa_naturalquestions-train-5734", "mrqa_naturalquestions-train-9302", "mrqa_naturalquestions-train-71635", "mrqa_naturalquestions-train-66951", "mrqa_naturalquestions-train-63692", "mrqa_squad-validation-2372", "mrqa_naturalquestions-train-62864", "mrqa_naturalquestions-train-74592", "mrqa_naturalquestions-train-36855", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-23881", "mrqa_naturalquestions-train-87585", "mrqa_naturalquestions-train-12896", "mrqa_naturalquestions-train-47000", "mrqa_naturalquestions-train-31830", "mrqa_naturalquestions-train-15542", "mrqa_hotpotqa-validation-1758", "mrqa_naturalquestions-validation-3658", "mrqa_hotpotqa-validation-5637", "mrqa_naturalquestions-validation-9499", "mrqa_squad-validation-2372", "mrqa_squad-validation-2412", "mrqa_naturalquestions-train-73356", "mrqa_naturalquestions-train-80729", "mrqa_naturalquestions-train-84390", "mrqa_naturalquestions-train-34071", "mrqa_squad-validation-9792", "mrqa_naturalquestions-train-70489", "mrqa_naturalquestions-train-58577", "mrqa_naturalquestions-train-64708", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-3387", "mrqa_triviaqa-validation-4827", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-train-51238", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-1015", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-5087", "mrqa_naturalquestions-train-9356", "mrqa_naturalquestions-train-78579", "mrqa_naturalquestions-train-22759", "mrqa_naturalquestions-train-80555", "mrqa_naturalquestions-train-86350", "mrqa_naturalquestions-train-67469", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-7728", "mrqa_squad-validation-1942", "mrqa_naturalquestions-train-80729", "mrqa_naturalquestions-train-2860", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-train-71795", "mrqa_naturalquestions-train-18324", "mrqa_naturalquestions-train-5078", "mrqa_naturalquestions-train-40184", "mrqa_naturalquestions-train-73466", "mrqa_naturalquestions-train-4972", "mrqa_naturalquestions-train-50358", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-9608", "mrqa_hotpotqa-validation-779", "mrqa_naturalquestions-train-87077", "mrqa_naturalquestions-train-21199", "mrqa_squad-validation-4019", "mrqa_naturalquestions-train-52902", "mrqa_naturalquestions-train-75039", "mrqa_naturalquestions-train-70658", "mrqa_naturalquestions-train-8912", "mrqa_naturalquestions-train-12011", "mrqa_naturalquestions-train-83407", "mrqa_hotpotqa-validation-4624", "mrqa_naturalquestions-train-18874", "mrqa_naturalquestions-train-29696", "mrqa_naturalquestions-train-55359"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 49, "before_eval": {"predictions": ["vehicles inspired by the Jeep that are suitable for use on rough terrain", "AOL", "Genghis Khan", "R.E.M.", "between the Eastern Ghats and the Bay of Bengal", "Ravenna", "12", "antlophobia", "1937", "improved", "a biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight until it was flying like a conventional plane", "He even lost control of the patents he had generated since he had assigned them to the company in lieu of stock.", "Georgian-born Soviet revolutionary and political leader", "Gregor Mendel", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "Las Vegas Outlaws", "3,600", "Route 5", "al - Mamlakah al - \u02bbArab\u012byah as - Su\u02bb\u016bd\u012byah", "44 hectares", "george cukor", "Horus", "Lawton Mainor Chiles Jr.", "`` Kobol's Last Gleaming ''", "Wisconsin", "uneven trade agreements", "Ugali with vegetables, sour milk, meat, fish or any other stew", "ATP energy", "Nebraska", "Ruth Elizabeth \"Bette\" Davis", "rhenium", "September 2014"], "metric_results": {"EM": 0.09375, "QA-F1": 0.23256743256743256}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [0.13333333333333333, 0.0, 0.0, 0.0, 0.18181818181818182, 0.15384615384615385, 0.2222222222222222, 0.0, 0.0, 0.6666666666666666, 0.3636363636363636, 0.5714285714285715, 0.0, 1.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8]}}, "error_ids": ["mrqa_naturalquestions-validation-2272", "mrqa_hotpotqa-validation-5452", "mrqa_squad-validation-6257", "mrqa_hotpotqa-validation-2032", "mrqa_naturalquestions-validation-2275", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2941", "mrqa_triviaqa-validation-6073", "mrqa_hotpotqa-validation-987", "mrqa_squad-validation-716", "mrqa_squad-validation-1459", "mrqa_squad-validation-1306", "mrqa_hotpotqa-validation-2810", "mrqa_squad-validation-9489", "mrqa_hotpotqa-validation-3964", "mrqa_squad-validation-10291", "mrqa_hotpotqa-validation-1023", "mrqa_naturalquestions-validation-9013", "mrqa_hotpotqa-validation-4530", "mrqa_triviaqa-validation-3250", "mrqa_triviaqa-validation-6475", "mrqa_hotpotqa-validation-1315", "mrqa_naturalquestions-validation-5283", "mrqa_squad-validation-7049", "mrqa_squad-validation-9807", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-5876", "mrqa_triviaqa-validation-6380", "mrqa_hotpotqa-validation-2750"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 50, "before_eval": {"predictions": ["basketball", "Igor Stravinsky, Carl Orff, Paul Hindemith, Richard Strauss, Luigi Nono, Krzysztof Penderecki and Joaqu\u00edn Rodrigo", "minced meat ( commonly beef when named cottage pie or lamb when named shepherd's pie ), typically cooked in a gravy with onions and sometimes other vegetables, such as peas, celery or carrots, and topped with mashed potato", "Gatiman express", "t.S. Eliot", "Taylor Swift", "the signing of the Treaty of Aix-la-Chapelle", "Valmiki", "Epistle to Ramsay by the Scottish poet William Hamilton", "every two to six years", "Chinese", "Arsenal FanTV", "2016", "Dan Castellaneta", "2007", "Wicked Twister", "multiplication", "Vernher von Braun", "khrushchev", "unclear as to how or whether this connection is relevant on microscales", "supernatural psychological horror film", "originate in the House of Representatives", "revenant", "the evening of the same day", "Blue (Da Ba Dee\") is a song by the Italian music group Eiffel 65.", "Constitution", "Faurot Field", "Annette Charles as Charlene `` Cha - Cha '' DiGregorio", "crossing the `` sandbar '' between river of life, with its outgoing `` flood '', and the ocean that lies beyond ( death ), the `` boundless deep '', to which we return ''", "5", "konya", "If the car is slowed initially by manual use of the automatic gear box and final stoppage"], "metric_results": {"EM": 0.15625, "QA-F1": 0.32042776560965414}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.4210526315789474, 0.1081081081081081, 0.6666666666666666, 0.4, 0.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.888888888888889, 0.0, 0.0, 0.4, 0.15384615384615385, 0.6666666666666666, 1.0, 0.0, 0.8627450980392156, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4225", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-3395", "mrqa_triviaqa-validation-813", "mrqa_hotpotqa-validation-5623", "mrqa_squad-validation-10171", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-1607", "mrqa_squad-validation-8134", "mrqa_hotpotqa-validation-1528", "mrqa_naturalquestions-validation-7812", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-118", "mrqa_triviaqa-validation-290", "mrqa_hotpotqa-validation-1350", "mrqa_squad-validation-10427", "mrqa_hotpotqa-validation-304", "mrqa_naturalquestions-validation-6782", "mrqa_triviaqa-validation-3572", "mrqa_squad-validation-2275", "mrqa_hotpotqa-validation-2635", "mrqa_triviaqa-validation-7770", "mrqa_naturalquestions-validation-1008", "mrqa_naturalquestions-validation-71", "mrqa_squad-validation-290", "mrqa_triviaqa-validation-239", "mrqa_naturalquestions-validation-3022"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 51, "before_eval": {"predictions": ["third", "the pulmonary arteries start as the pulmonary trunk or main pulmonary artery", "a card from a pack of playing cards by Alice", "Los Angeles, Orange, San Diego, San Bernardino, and Riverside", "Alex Breckenridge as Monique Valentine", "seven", "the Schr\u00f6dinger equation instead of Newtonian equations", "meat", "Ashland is home to Scribner-Fellows State Forest", "public schools in Alabama, Arkansas, Georgia, Louisiana, Mississippi, Oklahoma, Tennessee and Texas", "Ghostface mask", "Roger Thomas Staubach", "AC induction motor and transformer", "originally a three-part retrospective in tribute to Eric Morecambe", "Queens of the Stone Age", "the 16th century, when the colonization of the Americas began", "Lucy Muringo Gichuhi (n\u00e9e Munyiri)", "1775", "Empiricism", "w Somerset maugham", "Miller Brewing", "redistributive", "Peter Dinklage", "from 1990 to 2006", "content", "Saint Peter ( the keeper of the `` keys to the kingdom '' )", "An Act to provide for the better government of Ireland", "t\u1ebdt nguy\u00ean \u00d0\u00e1n", "Commission", "Belarus", "1835", "The Virgin Queen, Gloriana or Good Queen Bess"], "metric_results": {"EM": 0.15625, "QA-F1": 0.31969361813111813}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.4324324324324324, 0.19999999999999998, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.6, 0.33333333333333337, 0.761904761904762, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.5, 1.0, 0.4, 0.4444444444444445]}}, "error_ids": ["mrqa_hotpotqa-validation-4919", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4006", "mrqa_squad-validation-2432", "mrqa_naturalquestions-validation-6634", "mrqa_squad-validation-10386", "mrqa_triviaqa-validation-7398", "mrqa_squad-validation-1932", "mrqa_triviaqa-validation-787", "mrqa_hotpotqa-validation-4795", "mrqa_squad-validation-1185", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-4922", "mrqa_naturalquestions-validation-7484", "mrqa_hotpotqa-validation-4528", "mrqa_squad-validation-971", "mrqa_naturalquestions-validation-7312", "mrqa_triviaqa-validation-4890", "mrqa_squad-validation-7324", "mrqa_naturalquestions-validation-5370", "mrqa_triviaqa-validation-1046", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-91", "mrqa_triviaqa-validation-3980", "mrqa_squad-validation-4430", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3357"], "retrieved_ids": ["mrqa_naturalquestions-train-16239", "mrqa_naturalquestions-train-18884", "mrqa_naturalquestions-train-12721", "mrqa_naturalquestions-train-19249", "mrqa_naturalquestions-train-88216", "mrqa_naturalquestions-train-61800", "mrqa_naturalquestions-train-30649", "mrqa_naturalquestions-train-83939", "mrqa_naturalquestions-train-44470", "mrqa_naturalquestions-train-66570", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-train-5078", "mrqa_naturalquestions-train-2860", "mrqa_naturalquestions-train-75460", "mrqa_naturalquestions-train-47066", "mrqa_squad-validation-2428", "mrqa_naturalquestions-train-51088", "mrqa_naturalquestions-train-1225", "mrqa_naturalquestions-train-24946", "mrqa_naturalquestions-train-3593", "mrqa_naturalquestions-train-50746", "mrqa_naturalquestions-train-55983", "mrqa_naturalquestions-train-49997", "mrqa_naturalquestions-train-87141", "mrqa_naturalquestions-train-82548", "mrqa_naturalquestions-train-27576", "mrqa_naturalquestions-train-28849", "mrqa_naturalquestions-train-8421", "mrqa_squad-validation-10369", "mrqa_naturalquestions-train-5926", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-7592", "mrqa_naturalquestions-train-49463", "mrqa_triviaqa-validation-11", "mrqa_naturalquestions-train-74764", "mrqa_naturalquestions-train-81061", "mrqa_naturalquestions-train-32041", "mrqa_naturalquestions-train-85056", "mrqa_naturalquestions-train-64931", "mrqa_hotpotqa-validation-2389", "mrqa_naturalquestions-train-42253", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1551", "mrqa_hotpotqa-validation-3872", "mrqa_naturalquestions-train-2860", "mrqa_naturalquestions-train-46993", "mrqa_naturalquestions-train-18324", "mrqa_naturalquestions-train-44171", "mrqa_naturalquestions-train-51426", "mrqa_naturalquestions-train-12839", "mrqa_naturalquestions-train-59051", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-train-84250", "mrqa_naturalquestions-train-71635", "mrqa_naturalquestions-train-21881", "mrqa_naturalquestions-train-20687", "mrqa_naturalquestions-train-56792", "mrqa_hotpotqa-validation-4649", "mrqa_naturalquestions-train-60918", "mrqa_hotpotqa-validation-1032", "mrqa_naturalquestions-train-41085", "mrqa_naturalquestions-train-46861", "mrqa_naturalquestions-train-1505", "mrqa_naturalquestions-train-71932", "mrqa_naturalquestions-train-58694", "mrqa_naturalquestions-train-48977", "mrqa_naturalquestions-train-70320", "mrqa_naturalquestions-train-30890", "mrqa_naturalquestions-train-65378", "mrqa_naturalquestions-train-38228", "mrqa_hotpotqa-validation-1048", "mrqa_naturalquestions-train-5558", "mrqa_naturalquestions-train-47736", "mrqa_naturalquestions-train-5511", "mrqa_naturalquestions-train-29447", "mrqa_hotpotqa-validation-4352", "mrqa_naturalquestions-train-56363", "mrqa_naturalquestions-train-19075", "mrqa_naturalquestions-train-32157", "mrqa_naturalquestions-train-41621", "mrqa_naturalquestions-train-74768", "mrqa_naturalquestions-train-22930", "mrqa_naturalquestions-train-11350", "mrqa_triviaqa-validation-7578", "mrqa_triviaqa-validation-11", "mrqa_naturalquestions-train-59833", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-train-23098", "mrqa_squad-validation-7382", "mrqa_squad-validation-7352", "mrqa_naturalquestions-train-52894", "mrqa_naturalquestions-train-2717", "mrqa_naturalquestions-train-40837", "mrqa_naturalquestions-train-61318", "mrqa_naturalquestions-train-77664", "mrqa_naturalquestions-train-18324", "mrqa_naturalquestions-train-66951", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-388", "mrqa_triviaqa-validation-893", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-7415", "mrqa_naturalquestions-train-53977", "mrqa_naturalquestions-train-86874", "mrqa_naturalquestions-train-80384", "mrqa_naturalquestions-train-58745", "mrqa_naturalquestions-train-51229", "mrqa_naturalquestions-train-25190", "mrqa_naturalquestions-train-88101", "mrqa_naturalquestions-train-36855", "mrqa_naturalquestions-train-37625", "mrqa_naturalquestions-train-86963", "mrqa_triviaqa-validation-3280", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-455", "mrqa_triviaqa-validation-3847", "mrqa_triviaqa-validation-681", "mrqa_hotpotqa-validation-220", "mrqa_naturalquestions-train-82478", "mrqa_squad-validation-7836", "mrqa_naturalquestions-train-33306", "mrqa_naturalquestions-train-34003", "mrqa_naturalquestions-train-6579", "mrqa_naturalquestions-train-4469", "mrqa_naturalquestions-train-11738", "mrqa_naturalquestions-train-57704", "mrqa_naturalquestions-train-5636", "mrqa_triviaqa-validation-313", "mrqa_naturalquestions-train-45115", "mrqa_naturalquestions-train-70738", "mrqa_naturalquestions-train-2506", "mrqa_naturalquestions-train-37392"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 52, "before_eval": {"predictions": ["Myllokunmingia", "Edd Kimber", "Brenda", "the set of complex numbers of the form a + bi where i denotes the imaginary unit", "asphyxia", "SG", "Carey Mulligan, Matthias Schoenaerts, Michael Sheen, Tom Sturridge and Juno Temple", "The Worm", "16 December 1908", "five", "German hymns", "Bury Football Club is a professional association football club based in Bury, Greater Manchester, England.", "John Hancock, John Adams, John Quincy Adams, Rutherford B. Hayes, Theodore Roosevelt, Franklin D. Roosevelt, John F. Kennedy, Al Gore, George W. Bush and Barack Obama", "The Thing of It Is... is a 1967 novel written by William Goldman about Amos McCracken, a 31-year-old man who has written a popular show tune and who is having marriage troubles.", "the NFC Championship Game", "Tom Robinson", "Kevin McCallister", "2001", "Yemen's capital has been temporarily relocated to the port city of Aden, on the southern coast.", "quickly", "about two-thirds the size of cytoplasmic ribosomes (around 17 nm vs 25 nm)", "leopard", "service sector", "Florida", "\"\"la f\u00e9e verte\" ( the green fairy)", "david Mitchell", "Bolton", "British", "every aspect of public and private life wherever feasible", "civil disobedients", "Just under 540,800", "British Sky Broadcasting Group plc"], "metric_results": {"EM": 0.1875, "QA-F1": 0.33745516636141637}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false], "QA-F1": [0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3076923076923077, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.13333333333333333, 1.0, 0.4, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.8750000000000001, 0.0, 0.5, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6188", "mrqa_naturalquestions-validation-8228", "mrqa_squad-validation-9081", "mrqa_naturalquestions-validation-3351", "mrqa_triviaqa-validation-2598", "mrqa_naturalquestions-validation-7688", "mrqa_naturalquestions-validation-1186", "mrqa_hotpotqa-validation-5318", "mrqa_squad-validation-2401", "mrqa_hotpotqa-validation-5482", "mrqa_squad-validation-7240", "mrqa_hotpotqa-validation-182", "mrqa_squad-validation-308", "mrqa_triviaqa-validation-498", "mrqa_naturalquestions-validation-8759", "mrqa_hotpotqa-validation-1871", "mrqa_squad-validation-8849", "mrqa_hotpotqa-validation-1504", "mrqa_squad-validation-7377", "mrqa_hotpotqa-validation-1402", "mrqa_hotpotqa-validation-3044", "mrqa_hotpotqa-validation-1901", "mrqa_hotpotqa-validation-904", "mrqa_squad-validation-6709", "mrqa_squad-validation-2918", "mrqa_squad-validation-2772"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 53, "before_eval": {"predictions": ["Following the election of the UK Labour Party to government in 1997", "Levi's Stadium", "c 1600 from Bishopsgate with elaborately carved wood work and leaded windows", "Professor Moriarty to the Doctor's Sherlock Holmes", "Atticus Finch", "Treaty of Paris", "tart lambics", "~74,000", "no cow", "Bronwyn Kathleen Bishop (n\u00e9e Setright", "guidance and intervention from the European empire to aid in the governing of a more evolved social structure", "king of spades", "texas state", "domination or control by a group of people over another", "explores a violation of nature and the resulting psychological effects on the mariner and on all those who hear him", "Kikuyu tribe", "Sir Derek George Jacobi", "\u00a320 note", "25 June 1932", "Violin Caprice", "James A. Garfield", "seasonal television specials", "genome", "Evey", "10", "may seek changes or exemptions in the law that governs the land where the building will be built", "Joudeh Al - Goudia family", "Wes Unseld", "1912", "nitrogen dioxide", "increased flooding and sedimentation", "Daniel Handler"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2619591911711477}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false], "QA-F1": [0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.4615384615384615, 0.0, 0.0, 1.0, 0.08695652173913045, 0.28571428571428575, 0.4, 1.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-4100", "mrqa_squad-validation-169", "mrqa_squad-validation-5390", "mrqa_hotpotqa-validation-4571", "mrqa_triviaqa-validation-776", "mrqa_squad-validation-9144", "mrqa_triviaqa-validation-7626", "mrqa_hotpotqa-validation-123", "mrqa_squad-validation-9867", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-3014", "mrqa_squad-validation-9808", "mrqa_naturalquestions-validation-1161", "mrqa_triviaqa-validation-595", "mrqa_hotpotqa-validation-622", "mrqa_triviaqa-validation-6410", "mrqa_naturalquestions-validation-6485", "mrqa_triviaqa-validation-2821", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-117", "mrqa_naturalquestions-validation-9368", "mrqa_hotpotqa-validation-4294", "mrqa_squad-validation-6877", "mrqa_naturalquestions-validation-678", "mrqa_hotpotqa-validation-5614", "mrqa_triviaqa-validation-177", "mrqa_triviaqa-validation-3268"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 54, "before_eval": {"predictions": ["The Christmas Invasion", "an act of any such enemy of the United States", "the suicide of a young working - class woman, Eva Smith ( also known as Daisy Renton )", "March 9 to 18", "10 November 2017", "Romancing the Stone", "the oral mucosa ( a mucous membrane ) lining the mouth and also on the tongue and palates and mouth floor", "The 8th Habit", "Anishinaabeg", "medicine", "temperatures between December and March remains warm and is considered to be the most comfortable climatic conditions of the year", "pierowall", "to foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world", "22 September 2015", "\"Murder Request\") is a 2015 South Korean crime thriller film directed by Son Yong-ho.", "alicarnassus", "usagreb", "annually in late January or early February", "Michael Schumacher", "the duodenum", "pastry", "Cuyler Reynolds", "Lacoste, France", "cocktails", "cricket club", "two pieces of land near Oak Harbor, on Whidbey Island, in Island County, Washington", "saved something from the disaster when he sent John Bradstreet on an expedition that successfully destroyed Fort Frontenac", "typically found within a casino, ranging from card to slot machines", "Americana Manhasset", "\"Shoot Straight from Your Heart\"", "parsonage museum", "Kony Ealy"], "metric_results": {"EM": 0.125, "QA-F1": 0.23441445707070707}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.09090909090909093, 0.7272727272727273, 0.5, 0.2666666666666667, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4666666666666667, 1.0, 0.2666666666666667, 0.0, 0.0, 0.4444444444444444, 0.3636363636363636, 0.25, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6980", "mrqa_naturalquestions-validation-5212", "mrqa_naturalquestions-validation-1383", "mrqa_hotpotqa-validation-4277", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-2684", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-525", "mrqa_squad-validation-6211", "mrqa_naturalquestions-validation-4960", "mrqa_triviaqa-validation-5166", "mrqa_naturalquestions-validation-4021", "mrqa_hotpotqa-validation-5109", "mrqa_triviaqa-validation-1396", "mrqa_triviaqa-validation-2356", "mrqa_naturalquestions-validation-8441", "mrqa_hotpotqa-validation-4181", "mrqa_naturalquestions-validation-5497", "mrqa_triviaqa-validation-1707", "mrqa_triviaqa-validation-6373", "mrqa_triviaqa-validation-5057", "mrqa_hotpotqa-validation-2058", "mrqa_squad-validation-10293", "mrqa_hotpotqa-validation-1606", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-3710", "mrqa_triviaqa-validation-6164", "mrqa_squad-validation-979"], "retrieved_ids": ["mrqa_triviaqa-validation-7592", "mrqa_triviaqa-validation-5261", "mrqa_naturalquestions-train-28320", "mrqa_hotpotqa-validation-5526", "mrqa_triviaqa-validation-4890", "mrqa_naturalquestions-train-75572", "mrqa_naturalquestions-train-45121", "mrqa_naturalquestions-train-6631", "mrqa_naturalquestions-train-6703", "mrqa_naturalquestions-train-11021", "mrqa_naturalquestions-train-63395", "mrqa_naturalquestions-train-72549", "mrqa_naturalquestions-train-15748", "mrqa_naturalquestions-train-25258", "mrqa_naturalquestions-train-33710", "mrqa_hotpotqa-validation-118", "mrqa_hotpotqa-validation-3774", "mrqa_naturalquestions-train-26324", "mrqa_naturalquestions-train-71706", "mrqa_hotpotqa-validation-3241", "mrqa_naturalquestions-train-25168", "mrqa_naturalquestions-train-66951", "mrqa_naturalquestions-train-13554", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-validation-4590", "mrqa_naturalquestions-train-79675", "mrqa_naturalquestions-train-51334", "mrqa_naturalquestions-train-35118", "mrqa_naturalquestions-train-706", "mrqa_naturalquestions-train-84836", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-train-82748", "mrqa_naturalquestions-validation-8418", "mrqa_naturalquestions-train-5978", "mrqa_naturalquestions-train-10799", "mrqa_naturalquestions-train-32795", "mrqa_naturalquestions-train-13963", "mrqa_naturalquestions-train-52915", "mrqa_naturalquestions-train-75749", "mrqa_naturalquestions-train-22390", "mrqa_naturalquestions-train-80228", "mrqa_naturalquestions-validation-25", "mrqa_squad-validation-7836", "mrqa_squad-validation-3181", "mrqa_squad-validation-9984", "mrqa_naturalquestions-train-44156", "mrqa_naturalquestions-train-46162", "mrqa_naturalquestions-train-82764", "mrqa_naturalquestions-train-62147", "mrqa_naturalquestions-train-82315", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-5587", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-validation-7224", "mrqa_naturalquestions-train-48741", "mrqa_naturalquestions-train-49100", "mrqa_naturalquestions-train-25085", "mrqa_naturalquestions-train-54595", "mrqa_hotpotqa-validation-2937", "mrqa_naturalquestions-train-9736", "mrqa_hotpotqa-validation-1475", "mrqa_naturalquestions-train-20900", "mrqa_hotpotqa-validation-1436", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-671", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-1764", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-1877", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-3808", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-train-25023", "mrqa_naturalquestions-train-54810", "mrqa_naturalquestions-train-46340", "mrqa_naturalquestions-train-73609", "mrqa_naturalquestions-train-74285", "mrqa_naturalquestions-train-68703", "mrqa_naturalquestions-train-50602", "mrqa_naturalquestions-train-35355", "mrqa_naturalquestions-train-18324", "mrqa_naturalquestions-train-87077", "mrqa_naturalquestions-validation-4513", "mrqa_naturalquestions-train-13941", "mrqa_naturalquestions-train-40609", "mrqa_naturalquestions-train-23656", "mrqa_naturalquestions-train-77715", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-5087", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-7578", "mrqa_naturalquestions-train-75347", "mrqa_triviaqa-validation-1954", "mrqa_hotpotqa-validation-3669", "mrqa_naturalquestions-train-48528", "mrqa_naturalquestions-train-74228", "mrqa_naturalquestions-train-65248", "mrqa_naturalquestions-train-33306", "mrqa_squad-validation-4019", "mrqa_naturalquestions-train-27665", "mrqa_naturalquestions-train-21281", "mrqa_naturalquestions-train-2187", "mrqa_naturalquestions-train-83023", "mrqa_squad-validation-7836", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-17167", "mrqa_naturalquestions-train-34003", "mrqa_naturalquestions-train-80228", "mrqa_naturalquestions-train-82478", "mrqa_naturalquestions-train-59801", "mrqa_naturalquestions-train-37244", "mrqa_naturalquestions-train-17119", "mrqa_naturalquestions-train-24436", "mrqa_naturalquestions-train-32241", "mrqa_naturalquestions-train-22243", "mrqa_naturalquestions-train-2630", "mrqa_naturalquestions-train-34412", "mrqa_naturalquestions-train-43446", "mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-train-24615", "mrqa_hotpotqa-validation-2634", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-train-86643", "mrqa_naturalquestions-train-73434", "mrqa_squad-validation-7836", "mrqa_naturalquestions-train-35968", "mrqa_naturalquestions-train-7402"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 55, "before_eval": {"predictions": ["2,000", "Denver's Executive Vice President of Football Operations and General Manager", "Dutch navy captain Jurriaen Aernoutsz", "After being dumped by her boyfriend, fellow Agent Eric Matthews ( who gets relocated to Miami ), she agrees to the reassignment", "Brittany, Cornwall, Ireland, Isle of Man, Scotland and Wales", "stromal connective tissue", "paris", "Sierra Sky Park Airport is a residential airport community born of a unique agreement in transportation law to allow personal aircraft and automobiles to share certain roads.", "pangea", "job where there are many workers willing to work a large amount of time (high supply) competing for a job that few require (low demand) will result in a low wage for that job", "180 degrees", "\"Rock With You\"", "Scott Mosier", "verbal kint", "weaving", "art of the Persian Safavid dynasty from 1501 to 1722, in present - day Iran and Caucasia", "sepoys of the Company's army in the garrison town of Meerut, 40 miles northeast of Delhi ( now Old Delhi )", "Newton was sacked by DeMarcus Ware as time expired in the half", "Jack Nicholson -- Chinatown as J.J. `` Jake '' Gittes", "dan brown", "boxing, where a boxer who is still on their feet but close to being knocked down can be saved from losing by the bell ringing to indicate the end of the round", "at the port city of Kaffa in the Crimea in 1347", "Charbagh structure", "Iranian", "paris", "Landwehr", "his work was published first", "as a preparation for the Feast of the Divine Mercy, celebrated each year on first Sunday after Easter", "accommodationism", "Parliamentarians ( `` Roundheads '' ) and Royalists ( `` Cavaliers '' )", "the Niger\u2013 Congo language family spoken by most populations in Africa", "a lower index of refraction, typically a cladding of a different glass, or plastic"], "metric_results": {"EM": 0.09375, "QA-F1": 0.22725854904058562}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.34782608695652173, 1.0, 0.4, 0.0, 0.0, 0.0, 0.12121212121212122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9142857142857143, 0.42857142857142855, 0.0, 0.0, 0.9824561403508771, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.36363636363636365]}}, "error_ids": ["mrqa_squad-validation-378", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-6918", "mrqa_triviaqa-validation-3568", "mrqa_squad-validation-4700", "mrqa_triviaqa-validation-140", "mrqa_squad-validation-7407", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-5479", "mrqa_hotpotqa-validation-3264", "mrqa_triviaqa-validation-6478", "mrqa_triviaqa-validation-938", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-4098", "mrqa_squad-validation-818", "mrqa_naturalquestions-validation-9536", "mrqa_triviaqa-validation-7421", "mrqa_naturalquestions-validation-9459", "mrqa_squad-validation-4773", "mrqa_naturalquestions-validation-819", "mrqa_hotpotqa-validation-5543", "mrqa_triviaqa-validation-7689", "mrqa_hotpotqa-validation-4215", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-validation-8986", "mrqa_naturalquestions-validation-570", "mrqa_hotpotqa-validation-2699", "mrqa_naturalquestions-validation-7078"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 56, "before_eval": {"predictions": ["Donna", "1922", "predictions that can be tested in various ways", "the city council", "moral conservatism", "Tesla Polyphase System", "65,535 bytes", "39", "smart glasses", "the value of the dollar had been pegged to the price of gold", "friedrich nigeria", "the model for one of the characters in Jordan Mechner's game \" Prince of Persia\"", "monocled Mutineer", "Robert \"Bumps\" Blackwell", "five", "negatively impact teachers' mental and physical health, productivity, and students' performance", "impreza", "d'Artagnan", "If there are no repeated data values", "Yarra River", "1932", "March 31, 1944", "acetic anhydride", "Peter Schmeichel", "inferior only to \"Reichsf\u00fchrer-SS\"", "Erinys", "Dallol is a cinder cone volcano in the Danakil Depression, northeast of the Erta Ale Range in Ethiopia. It has been formed by the intrusion of basaltic magma into Miocene salt deposits", "2015", "in Egypt, the only part of the country located in Asia", "Melanie Walters", "the retina of mammalian eyes", "financial services"], "metric_results": {"EM": 0.25, "QA-F1": 0.3328242835595777}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 0.23076923076923075, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.35294117647058826, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-7080", "mrqa_squad-validation-9608", "mrqa_naturalquestions-validation-1787", "mrqa_squad-validation-371", "mrqa_naturalquestions-validation-752", "mrqa_squad-validation-3720", "mrqa_triviaqa-validation-4390", "mrqa_hotpotqa-validation-627", "mrqa_triviaqa-validation-6223", "mrqa_hotpotqa-validation-3651", "mrqa_squad-validation-2004", "mrqa_triviaqa-validation-2280", "mrqa_triviaqa-validation-326", "mrqa_naturalquestions-validation-486", "mrqa_triviaqa-validation-4640", "mrqa_squad-validation-3653", "mrqa_hotpotqa-validation-15", "mrqa_hotpotqa-validation-686", "mrqa_triviaqa-validation-892", "mrqa_naturalquestions-validation-1349", "mrqa_triviaqa-validation-5813", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-7358", "mrqa_triviaqa-validation-2701"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 57, "before_eval": {"predictions": ["Saturn", "Samuel Johnson", "1.7 billion", "Bathurst", "April 1948", "higher in the transition economies of Central and Eastern Europe and Central Asia than in some other developed economies in Western Europe ( except France, where inequality of opportunity was relatively high )", "the use of terms such as penance and righteousness by the Catholic Church in new ways", "FeO", "Groucho, and Zeppo", "materials melted near an impact crater", "a half-penny sales tax", "david pediaview", "Tom Brady", "paris", "carbon dioxide", "National Party of Australia", "His / Her Majesty's Ship", "South Africa", "October 15, 1997", "in the Hebrew Bible in the Book of Job, Psalms, and Isaiah", "renovated the inside as part of his first construction project in Manhattan", "Greenland shark", "pembroke castle", "Robert Louis Balfour Stevenson", "jorrocks", "five", "paris", "elizabeth Taylor played the part opposite Richard Burton as Mark Antony and Rex Harrison as Julius Caesar in the movie Cleopatra", "He is the son of writer William F. Buckley Jr. and socialite Patricia Buckley.", "chicken kiev and scotch eggs", "Sam the Sham", "as many as two consecutive terms"], "metric_results": {"EM": 0.25, "QA-F1": 0.34937492042271456}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.18181818181818182, 0.35294117647058826, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.28571428571428575, 0.25, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.4, 0.4]}}, "error_ids": ["mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-4974", "mrqa_hotpotqa-validation-5436", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-3969", "mrqa_squad-validation-2255", "mrqa_triviaqa-validation-3648", "mrqa_squad-validation-7226", "mrqa_triviaqa-validation-6053", "mrqa_triviaqa-validation-5785", "mrqa_squad-validation-2886", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-6365", "mrqa_hotpotqa-validation-3802", "mrqa_hotpotqa-validation-3544", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-6425", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-3663", "mrqa_triviaqa-validation-7187", "mrqa_hotpotqa-validation-1975", "mrqa_triviaqa-validation-1742", "mrqa_hotpotqa-validation-1016", "mrqa_naturalquestions-validation-787"], "retrieved_ids": ["mrqa_triviaqa-validation-6639", "mrqa_triviaqa-validation-7248", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-5839", "mrqa_naturalquestions-train-43677", "mrqa_naturalquestions-train-68147", "mrqa_hotpotqa-validation-1099", "mrqa_naturalquestions-train-31407", "mrqa_naturalquestions-train-74971", "mrqa_naturalquestions-train-52849", "mrqa_naturalquestions-train-64950", "mrqa_naturalquestions-train-42067", "mrqa_naturalquestions-train-14373", "mrqa_naturalquestions-train-22760", "mrqa_naturalquestions-train-54652", "mrqa_naturalquestions-train-39508", "mrqa_squad-validation-7836", "mrqa_naturalquestions-train-64419", "mrqa_naturalquestions-train-56782", "mrqa_squad-validation-7741", "mrqa_squad-validation-10015", "mrqa_naturalquestions-train-81209", "mrqa_naturalquestions-train-9637", "mrqa_naturalquestions-train-77024", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-2959", "mrqa_naturalquestions-train-25771", "mrqa_naturalquestions-train-41621", "mrqa_naturalquestions-train-64931", "mrqa_naturalquestions-train-9081", "mrqa_naturalquestions-train-34340", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-3568", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-5057", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-7578", "mrqa_triviaqa-validation-3945", "mrqa_naturalquestions-train-36010", "mrqa_squad-validation-2885", "mrqa_squad-validation-2884", "mrqa_naturalquestions-train-82140", "mrqa_naturalquestions-train-20219", "mrqa_naturalquestions-train-55313", "mrqa_squad-validation-4169", "mrqa_naturalquestions-train-85795", "mrqa_naturalquestions-train-12174", "mrqa_naturalquestions-train-63521", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-train-48429", "mrqa_naturalquestions-train-59740", "mrqa_naturalquestions-train-21061", "mrqa_naturalquestions-train-8742", "mrqa_naturalquestions-train-54227", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-47470", "mrqa_naturalquestions-validation-570", "mrqa_naturalquestions-train-85803", "mrqa_naturalquestions-train-67402", "mrqa_naturalquestions-train-58634", "mrqa_naturalquestions-train-87162", "mrqa_naturalquestions-train-14399", "mrqa_naturalquestions-train-67889", "mrqa_naturalquestions-train-28727", "mrqa_triviaqa-validation-3767", "mrqa_triviaqa-validation-2181", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-2803", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-7215", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-5087", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-6373", "mrqa_triviaqa-validation-7578", "mrqa_triviaqa-validation-5160", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-6639", "mrqa_triviaqa-validation-1451", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-3515", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-4137", "mrqa_triviaqa-validation-3901", "mrqa_naturalquestions-train-24377", "mrqa_naturalquestions-train-44661", "mrqa_naturalquestions-train-57406", "mrqa_naturalquestions-train-45753", "mrqa_hotpotqa-validation-1509", "mrqa_triviaqa-validation-6373", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-3814", "mrqa_naturalquestions-train-63956", "mrqa_naturalquestions-train-11406", "mrqa_naturalquestions-train-84647", "mrqa_naturalquestions-train-21057", "mrqa_naturalquestions-train-59390", "mrqa_naturalquestions-train-87143", "mrqa_naturalquestions-train-49393", "mrqa_naturalquestions-train-82915", "mrqa_naturalquestions-train-80378", "mrqa_naturalquestions-train-57408"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 58, "before_eval": {"predictions": ["Dutch", "begins in the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "a house edge of between 0.5 % and 1 %", "phagosomal membrane", "the Ouseburn valley", "25 July 1955", "henry vii", "200 horsepower", "Thomas Edison and Nikola Tesla", "theology and faith", "Night Ranger", "Buzz, the Honey Nut Cheerios Bee", "Thocmentony", "mentoring", "levels of economic inequality", "lutalo Muhammad", "Arthur H. Compton", "Angelina Jolie, Brad Pitt and Amal Clooney", "Timothy Brown", "1979", "mainly civil servants recruited in special university classes", "large areas", "O \u00d7 8", "humerus", "weighing", "no man is an island", "Liao, Jin, and Song", "`` central '' or `` middle ''", "pigeons", "ionized material", "Attack the Block", "William Shakespeare"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3383732586857587}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.7272727272727273, 0.6666666666666666, 1.0, 0.5, 0.5, 0.0, 0.15384615384615385, 0.0, 0.4, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-9173", "mrqa_naturalquestions-validation-5554", "mrqa_squad-validation-8687", "mrqa_hotpotqa-validation-4035", "mrqa_triviaqa-validation-6185", "mrqa_squad-validation-1443", "mrqa_squad-validation-1535", "mrqa_hotpotqa-validation-593", "mrqa_naturalquestions-validation-10626", "mrqa_hotpotqa-validation-5654", "mrqa_squad-validation-2057", "mrqa_triviaqa-validation-5104", "mrqa_squad-validation-7880", "mrqa_hotpotqa-validation-4178", "mrqa_triviaqa-validation-7264", "mrqa_hotpotqa-validation-5127", "mrqa_squad-validation-2042", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-9814", "mrqa_squad-validation-10351", "mrqa_triviaqa-validation-4817", "mrqa_naturalquestions-validation-5624", "mrqa_triviaqa-validation-6520", "mrqa_hotpotqa-validation-820", "mrqa_hotpotqa-validation-1706", "mrqa_naturalquestions-validation-3470"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 59, "before_eval": {"predictions": ["GE's four computer sales and service centers (Schenectady, Phoenix, Chicago, and Phoenix)", "The waxy cuticle of many leaves, the exoskeleton of insects, the shells and membranes of externally deposited eggs, and skin", "the Naturalization Act of 1790", "Matthew Vaughn", "around 2 %", "King George VI", "9", "John Christopher Lujack Jr. (pronounced Lu' jack", "metropolis", "24\u201310", "bachi", "It's only fair that, from now on, you should pay more for oil. Let's say ten times more", "5 mi east of Everest on highway K-20", "yolk sac ( protruding from its lower part )", "Kelly Bundy", "Italy", "for export to London and elsewhere", "the Queen's gaoler", "in the pachytene stage of prophase I of meiosis during a process called synapsis", "Jordan Norwood", "321,520", "June 12, 2017", "2005", "red deer", "kalapatthar", "General Helmuth von Moltke, Chief of the German General Staff and Secretary of State for Home Affairs", "Baloo ( \u092d\u093e\u0932\u0942 Bh\u0101l\u016b, `` bear '' ; Himalayan brown bear )", "`` Turkey in the Straw ''", "the 2020 National Football League ( NFL ) season ( although a move to Las Vegas could happen as soon as 2019 with Sam Boyd Stadium )", "Selznick library", "peter dickson", "Samoan t\u0101l\u0101"], "metric_results": {"EM": 0.25, "QA-F1": 0.34711418074379685}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.06896551724137931, 0.21052631578947367, 0.28571428571428575, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.14814814814814814, 0.16666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.9565217391304348, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.16, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-4838", "mrqa_squad-validation-6436", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-875", "mrqa_triviaqa-validation-2965", "mrqa_naturalquestions-validation-7974", "mrqa_hotpotqa-validation-1393", "mrqa_triviaqa-validation-3719", "mrqa_triviaqa-validation-2223", "mrqa_squad-validation-3730", "mrqa_hotpotqa-validation-740", "mrqa_naturalquestions-validation-8116", "mrqa_triviaqa-validation-5864", "mrqa_squad-validation-5124", "mrqa_naturalquestions-validation-7035", "mrqa_squad-validation-7612", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-3685", "mrqa_hotpotqa-validation-4456", "mrqa_naturalquestions-validation-3571", "mrqa_naturalquestions-validation-1676", "mrqa_naturalquestions-validation-5649", "mrqa_triviaqa-validation-1933", "mrqa_triviaqa-validation-2525"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 60, "before_eval": {"predictions": ["Currer Bell", "the illegitimate son of Ned Stark, the honorable lord of Winterfell, an ancient fortress in the North of the fictional continent of Westeros", "in positions Arg15 - Ile16", "The Catcher in the Rye", "Anglo-Saxons", "Alexandria ( Margiana) (\u1f08\u03bb\u03b5\u03be\u03ac\u03bd\u03b4\u03c1\u03b5\u03b9\u03b1) and Antiochia in Margiana", "Bendigo and its environs", "Thomas Jefferson", "Helena Sukov\u00e1", "July 1872", "Boston and Maine Railroad's Southern Division", "Kevin Harlan as play-by-play announcer, Boomer Esiason and Dan Fouts as color analysts", "the West, forks, plate, butter knife, and napkin generally are placed to the left of the dinner plate, and knives, spoons, stemware and tumblers, cups, and saucers to the right", "summer months", "Robin Cousins, Jason Gardiner, Barber and Ashley Roberts returned for their respective ninth, eighth, seventh and second series on The Ice Panel", "third \u00e9tude", "Korea", "segues", "brian lareggub", "iron", "During the reign of King Beorhtric of Wessex ( 786 -- 802 ) three ships of `` Northmen '' landed at Portland Bay in Dorset", "South Australian", "Flag Day in 1954", "around 300,000", "1858", "Sexred", "Riverdale teen Archie Andrews in the years that follow his college graduation when Archie makes his ultimate decision to marry Veronica Lodge instead of Betty Cooper", "neo-Nazi", "elton", "It was decided to use an existing Saturn V to launch the Skylab orbital laboratory pre-built on the ground, replacing the original plan to construct it in orbit from several Saturn IB launches", "frequency f", "The story is a satire on corruption in the administration of criminal justice and the concept of the \"celebrity criminal\""], "metric_results": {"EM": 0.25, "QA-F1": 0.3795771775744228}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.4347826086956522, 0.16666666666666666, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 0.2666666666666667, 0.32258064516129037, 0.0, 0.17391304347826084, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.1904761904761905, 0.0, 1.0, 1.0, 0.0, 0.0, 0.07692307692307693, 1.0, 0.0, 0.0625, 0.08333333333333334, 0.23529411764705882]}}, "error_ids": ["mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-7225", "mrqa_hotpotqa-validation-2715", "mrqa_squad-validation-2842", "mrqa_naturalquestions-validation-9273", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5451", "mrqa_squad-validation-559", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-1786", "mrqa_squad-validation-5449", "mrqa_hotpotqa-validation-1033", "mrqa_triviaqa-validation-3404", "mrqa_triviaqa-validation-3684", "mrqa_naturalquestions-validation-4863", "mrqa_hotpotqa-validation-5311", "mrqa_naturalquestions-validation-6337", "mrqa_hotpotqa-validation-4293", "mrqa_naturalquestions-validation-6759", "mrqa_triviaqa-validation-1284", "mrqa_squad-validation-4011", "mrqa_naturalquestions-validation-5798", "mrqa_hotpotqa-validation-3681"], "retrieved_ids": ["mrqa_naturalquestions-train-1096", "mrqa_naturalquestions-train-87227", "mrqa_naturalquestions-train-14989", "mrqa_naturalquestions-train-43248", "mrqa_naturalquestions-train-52921", "mrqa_naturalquestions-train-47115", "mrqa_naturalquestions-train-21248", "mrqa_naturalquestions-train-22559", "mrqa_naturalquestions-train-71381", "mrqa_naturalquestions-train-53815", "mrqa_naturalquestions-train-69376", "mrqa_naturalquestions-train-41774", "mrqa_naturalquestions-train-24946", "mrqa_naturalquestions-train-41714", "mrqa_naturalquestions-train-44294", "mrqa_naturalquestions-train-42530", "mrqa_naturalquestions-train-19223", "mrqa_naturalquestions-train-34964", "mrqa_naturalquestions-train-81286", "mrqa_naturalquestions-train-87102", "mrqa_naturalquestions-train-15739", "mrqa_naturalquestions-train-4310", "mrqa_naturalquestions-train-40318", "mrqa_naturalquestions-train-63325", "mrqa_naturalquestions-train-22233", "mrqa_naturalquestions-train-65448", "mrqa_hotpotqa-validation-3241", "mrqa_naturalquestions-train-61820", "mrqa_hotpotqa-validation-118", "mrqa_naturalquestions-train-6504", "mrqa_naturalquestions-train-29447", "mrqa_hotpotqa-validation-987", "mrqa_naturalquestions-train-57203", "mrqa_naturalquestions-train-83391", "mrqa_naturalquestions-train-74354", "mrqa_naturalquestions-train-32796", "mrqa_squad-validation-568", "mrqa_naturalquestions-train-78304", "mrqa_naturalquestions-train-50031", "mrqa_naturalquestions-train-25838", "mrqa_naturalquestions-train-83939", "mrqa_naturalquestions-train-34331", "mrqa_naturalquestions-train-24957", "mrqa_naturalquestions-train-29628", "mrqa_naturalquestions-train-15972", "mrqa_naturalquestions-train-87685", "mrqa_naturalquestions-train-72905", "mrqa_naturalquestions-train-25052", "mrqa_naturalquestions-train-67684", "mrqa_naturalquestions-train-62147", "mrqa_naturalquestions-train-17424", "mrqa_naturalquestions-train-81286", "mrqa_naturalquestions-train-71415", "mrqa_naturalquestions-train-37171", "mrqa_naturalquestions-train-65378", "mrqa_naturalquestions-train-73275", "mrqa_naturalquestions-train-74592", "mrqa_naturalquestions-train-17119", "mrqa_naturalquestions-train-21934", "mrqa_naturalquestions-train-39394", "mrqa_naturalquestions-train-48852", "mrqa_naturalquestions-train-70807", "mrqa_naturalquestions-train-8315", "mrqa_naturalquestions-train-51396", "mrqa_squad-validation-2401", "mrqa_triviaqa-validation-639", "mrqa_naturalquestions-train-600", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-7578", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-3568", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-140", "mrqa_naturalquestions-train-11365", "mrqa_naturalquestions-train-55922", "mrqa_naturalquestions-train-31694", "mrqa_naturalquestions-train-7528", "mrqa_naturalquestions-train-76149", "mrqa_hotpotqa-validation-1426", "mrqa_naturalquestions-train-71382", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-65805", "mrqa_naturalquestions-train-1225", "mrqa_naturalquestions-train-81626", "mrqa_naturalquestions-train-54155", "mrqa_naturalquestions-train-4107", "mrqa_naturalquestions-train-7536", "mrqa_naturalquestions-train-73577", "mrqa_naturalquestions-train-66044", "mrqa_naturalquestions-train-14250", "mrqa_hotpotqa-validation-389", "mrqa_squad-validation-9984", "mrqa_naturalquestions-train-60387", "mrqa_naturalquestions-train-36638", "mrqa_naturalquestions-train-9735", "mrqa_naturalquestions-train-75876", "mrqa_naturalquestions-train-4863", "mrqa_naturalquestions-train-11757", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-455", "mrqa_triviaqa-validation-6478", "mrqa_triviaqa-validation-1995", "mrqa_triviaqa-validation-2803", "mrqa_naturalquestions-validation-5215", "mrqa_squad-validation-1306", "mrqa_naturalquestions-train-17467", "mrqa_naturalquestions-train-45488", "mrqa_naturalquestions-train-79711", "mrqa_naturalquestions-train-35311", "mrqa_naturalquestions-train-17039", "mrqa_naturalquestions-train-51271", "mrqa_naturalquestions-train-34656", "mrqa_naturalquestions-train-39439", "mrqa_naturalquestions-train-58365", "mrqa_naturalquestions-train-39792", "mrqa_naturalquestions-train-42379", "mrqa_naturalquestions-train-21438", "mrqa_naturalquestions-train-22618"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 61, "before_eval": {"predictions": ["The Two Noble Kinsmen", "shermer high school", "Riverside", "Judy Collins", "Out of Control", "the 2013 non-fiction book of the same name by David Finkel", "Leapy Lee", "The Frost Report", "fell from his horse while hunting", "incitement to terrorism", "Henry and Liza", "the first flume ride in Ireland", "Mexico City", "oxygen", "Elk and Kanawha Rivers", "They circulate and are moved around within plant cells, and occasionally pinch in two to reproduce. Their behavior is strongly influenced by environmental factors like light color and intensity.", "'Bucks Point'", "to `` help bring creative projects to life ''", "The Church of the Latter Day Saints", "Old World fossil representatives", "in salts and never as the free elements", "Matt Damon", "daniel smith", "thicker consistency and a deeper flavour than sauce", "the internal reproductive anatomy ( such as the uterus in females )", "new zealand", "L'\u00c9glise fran\u00e7aise \u00e0 la Nouvelle-Amsterdam", "Nine Inch Nails", "after AD 70", "1985", "dance", "served in this position until 1941, when he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II."], "metric_results": {"EM": 0.21875, "QA-F1": 0.2999875992063492}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.4, 0.1875, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.25, 0.2222222222222222, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1875]}}, "error_ids": ["mrqa_hotpotqa-validation-1850", "mrqa_naturalquestions-validation-6118", "mrqa_hotpotqa-validation-5822", "mrqa_naturalquestions-validation-7407", "mrqa_triviaqa-validation-6418", "mrqa_squad-validation-6218", "mrqa_naturalquestions-validation-8368", "mrqa_hotpotqa-validation-4743", "mrqa_triviaqa-validation-2130", "mrqa_naturalquestions-validation-7483", "mrqa_squad-validation-8560", "mrqa_naturalquestions-validation-10292", "mrqa_triviaqa-validation-3355", "mrqa_naturalquestions-validation-1699", "mrqa_hotpotqa-validation-4931", "mrqa_triviaqa-validation-3960", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6376", "mrqa_squad-validation-3067", "mrqa_hotpotqa-validation-1114", "mrqa_squad-validation-9370", "mrqa_hotpotqa-validation-327", "mrqa_triviaqa-validation-3985", "mrqa_hotpotqa-validation-3481"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 62, "before_eval": {"predictions": ["penitential suffering", "whiteness", "decomposers", "the Monarch", "Canada", "Ballarat Bitter", "therefore", "Ricketts Glen State Park", "Can't Change Me", "magma", "harridan Grizelda Pugh", "a fee per unit of connection time", "coordinator", "2010", "winter of the 2017 -- 18 network television season", "cattle", "Egypt", "Neil Armstrong", "moselle", "over 2500 ft", "his advocacy of young earth creationism and intelligent design", "on Demand to the beginning of the ABC show", "Kansas\u2013Nebraska Act", "2 Constant ( C\u03bc and C\u03b4 )", "1910\u20131940", "11:28 left in the second quarter", "Start Here", "geoffrey beevers", "Autobahn", "endocrine", "writer", "poodle"], "metric_results": {"EM": 0.25, "QA-F1": 0.4018620627456834}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.25, 0.4444444444444445, 0.13333333333333333, 1.0, 0.5714285714285715, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.25, 0.0, 0.0, 0.4444444444444445, 0.0, 0.4, 0.3636363636363636, 0.9333333333333333, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0689655172413793, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-2339", "mrqa_naturalquestions-validation-5396", "mrqa_squad-validation-7766", "mrqa_hotpotqa-validation-507", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-7270", "mrqa_triviaqa-validation-1698", "mrqa_squad-validation-4750", "mrqa_hotpotqa-validation-1510", "mrqa_naturalquestions-validation-8696", "mrqa_squad-validation-2703", "mrqa_naturalquestions-validation-1555", "mrqa_squad-validation-3961", "mrqa_triviaqa-validation-4878", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-3469", "mrqa_squad-validation-5816", "mrqa_naturalquestions-validation-538", "mrqa_squad-validation-824", "mrqa_triviaqa-validation-4095", "mrqa_hotpotqa-validation-5607", "mrqa_naturalquestions-validation-9064", "mrqa_hotpotqa-validation-1692", "mrqa_triviaqa-validation-6254"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 63, "before_eval": {"predictions": ["Bank of China Tower", "heat addition (in the boiler) and rejection ( in the condenser)", "moffit", "Operating System Principles", "William Shakespeare celebrated - Apr 23, 1564", "Kohlberg K Travis Roberts", "kent", "probabilistic (or \"Monte Carlo\") and deterministic algorithms", "jamesi hrix", "eight days after their initial broadcast", "Hong Kong\u2013based, Cayman Islands registered Mandarin and Cantonese-language television broadcaster that serves the Chinese mainland and Hong Kong along with other markets with substantial Chinese viewers", "a narcissistic ex-lover who did the protagonist wrong", "catawba river", "petrographic microscope", "\"2 Legit 2 Quit\"", "Haitian Revolution", "Sondheim", "276,170", "3.762", "money", "1993", "Daily Express", "ACL tears", "marx", "ear ossicles of the middle ears", "Germany", "1999", "their belief in the validity of the social contract, which is held to bind all to obey the laws that a government meeting certain standards of legitimacy has established, or else suffer the penalties set out in the law", "Professor Kantorek", "31 - member Senate", "whippoorwill", "orkneys"], "metric_results": {"EM": 0.125, "QA-F1": 0.21412037037037038}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.4444444444444445, 0.0, 0.2857142857142857, 0.07407407407407407, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.38095238095238093, 0.0, 0.5, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-189", "mrqa_squad-validation-3445", "mrqa_triviaqa-validation-7574", "mrqa_squad-validation-4629", "mrqa_triviaqa-validation-3153", "mrqa_hotpotqa-validation-97", "mrqa_triviaqa-validation-1518", "mrqa_squad-validation-9057", "mrqa_triviaqa-validation-54", "mrqa_squad-validation-5843", "mrqa_hotpotqa-validation-366", "mrqa_naturalquestions-validation-6326", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-27", "mrqa_naturalquestions-validation-9755", "mrqa_hotpotqa-validation-55", "mrqa_triviaqa-validation-2129", "mrqa_triviaqa-validation-7011", "mrqa_hotpotqa-validation-3929", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-6651", "mrqa_triviaqa-validation-2850", "mrqa_hotpotqa-validation-4301", "mrqa_squad-validation-6707", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1533", "mrqa_triviaqa-validation-4037", "mrqa_triviaqa-validation-2254"], "retrieved_ids": ["mrqa_naturalquestions-train-17119", "mrqa_naturalquestions-train-21934", "mrqa_naturalquestions-train-74592", "mrqa_naturalquestions-train-24436", "mrqa_naturalquestions-train-39394", "mrqa_squad-validation-10459", "mrqa_naturalquestions-train-8754", "mrqa_naturalquestions-train-64419", "mrqa_squad-validation-3467", "mrqa_squad-validation-10403", "mrqa_triviaqa-validation-5078", "mrqa_triviaqa-validation-3014", "mrqa_triviaqa-validation-7184", "mrqa_hotpotqa-validation-5822", "mrqa_triviaqa-validation-935", "mrqa_squad-validation-4626", "mrqa_naturalquestions-train-79942", "mrqa_naturalquestions-train-53606", "mrqa_naturalquestions-validation-5651", "mrqa_naturalquestions-train-2833", "mrqa_naturalquestions-train-24377", "mrqa_naturalquestions-train-9240", "mrqa_naturalquestions-train-12405", "mrqa_naturalquestions-train-6991", "mrqa_naturalquestions-train-63860", "mrqa_naturalquestions-train-52189", "mrqa_naturalquestions-train-79627", "mrqa_naturalquestions-train-74715", "mrqa_naturalquestions-train-63552", "mrqa_naturalquestions-train-27091", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-1410", "mrqa_triviaqa-validation-3515", "mrqa_triviaqa-validation-6053", "mrqa_squad-validation-10386", "mrqa_naturalquestions-train-73434", "mrqa_naturalquestions-train-18513", "mrqa_naturalquestions-train-65495", "mrqa_naturalquestions-train-22679", "mrqa_triviaqa-validation-671", "mrqa_triviaqa-validation-2722", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-6250", "mrqa_triviaqa-validation-743", "mrqa_naturalquestions-train-21301", "mrqa_naturalquestions-train-87923", "mrqa_naturalquestions-train-18263", "mrqa_naturalquestions-train-88064", "mrqa_naturalquestions-train-36978", "mrqa_naturalquestions-train-10586", "mrqa_naturalquestions-train-38329", "mrqa_naturalquestions-train-49897", "mrqa_naturalquestions-train-41621", "mrqa_naturalquestions-train-53835", "mrqa_naturalquestions-train-53798", "mrqa_naturalquestions-train-29648", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-73322", "mrqa_naturalquestions-train-65495", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-140", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-7215", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6478", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-5839", "mrqa_naturalquestions-train-46748", "mrqa_naturalquestions-train-69270", "mrqa_naturalquestions-train-6794", "mrqa_naturalquestions-train-42060", "mrqa_naturalquestions-train-61986", "mrqa_hotpotqa-validation-4097", "mrqa_squad-validation-3181", "mrqa_naturalquestions-train-43526", "mrqa_squad-validation-9984", "mrqa_naturalquestions-train-84103", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-5071", "mrqa_triviaqa-validation-667", "mrqa_naturalquestions-train-40409", "mrqa_triviaqa-validation-1792", "mrqa_naturalquestions-train-65317", "mrqa_hotpotqa-validation-1758", "mrqa_naturalquestions-train-47895", "mrqa_naturalquestions-train-36523", "mrqa_naturalquestions-train-33455", "mrqa_naturalquestions-train-68058", "mrqa_naturalquestions-train-64931", "mrqa_naturalquestions-train-50733", "mrqa_naturalquestions-train-30259", "mrqa_naturalquestions-train-70645", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-train-51238", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-140", "mrqa_triviaqa-validation-2368", "mrqa_triviaqa-validation-3684", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-2130", "mrqa_triviaqa-validation-6351", "mrqa_naturalquestions-train-16159", "mrqa_naturalquestions-train-36730", "mrqa_naturalquestions-train-45712", "mrqa_naturalquestions-train-38789", "mrqa_naturalquestions-train-45778", "mrqa_naturalquestions-train-84715", "mrqa_naturalquestions-train-29021", "mrqa_naturalquestions-train-45547", "mrqa_naturalquestions-train-42615", "mrqa_naturalquestions-train-23475", "mrqa_naturalquestions-train-69842", "mrqa_naturalquestions-train-48215", "mrqa_naturalquestions-train-48976", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-28221", "mrqa_naturalquestions-train-65924", "mrqa_naturalquestions-train-77410", "mrqa_naturalquestions-train-84539", "mrqa_naturalquestions-train-46712", "mrqa_naturalquestions-train-55413", "mrqa_triviaqa-validation-7578", "mrqa_triviaqa-validation-938", "mrqa_naturalquestions-train-21514", "mrqa_naturalquestions-train-24615", "mrqa_naturalquestions-train-50526", "mrqa_triviaqa-validation-2442", "mrqa_triviaqa-validation-7689", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-7578", "mrqa_triviaqa-validation-5746"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 64, "before_eval": {"predictions": ["failure rate", "the governor of West Virginia", "larvae", "Trey Parker and Matt Stone", "kanto", "one of popular music's most poignant anthems of sorrow regarding the environment", "trial division", "consultant", "Preston", "The Suite Life of Zack & Cody", "tuberculosis", "Saturn IB", "H. R. Haldeman", "brian wart", "Player of the Year", "saint george and the dragon raphael", "skull", "badrutts Palace Hotel", "charles heen", "granaries were ordered built throughout the empire", "The Charkhi Dadri mid-air collision", "John Simm", "immediate judgement discrepancy, or cognitive bias", "New Orleans", "the final episode of the series", "macOS High Sierra", "George Foreman", "parallelograms", "Bishop Martin Sasse", "Loire river", "17th Century sources referring to Cardinal Richelieu after he was named to head the royal council in 1624", "appropriates ( gives to, sets aside for ) money to specific federal government departments, agencies, and programs"], "metric_results": {"EM": 0.3125, "QA-F1": 0.37687639158227393}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.058823529411764705]}}, "error_ids": ["mrqa_triviaqa-validation-2902", "mrqa_squad-validation-4567", "mrqa_triviaqa-validation-748", "mrqa_naturalquestions-validation-7857", "mrqa_squad-validation-8909", "mrqa_squad-validation-3956", "mrqa_hotpotqa-validation-3489", "mrqa_triviaqa-validation-2032", "mrqa_triviaqa-validation-6398", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-3208", "mrqa_hotpotqa-validation-2257", "mrqa_triviaqa-validation-5521", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-2748", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-6706", "mrqa_squad-validation-2560", "mrqa_triviaqa-validation-4709", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-10533"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 65, "before_eval": {"predictions": ["the lightly populated \"Cow Counties\" of southern California", "Arabic numerals", "Diary of a Wimpy Kid : The Long Haul", "the four - letter suffix", "prayer", "heart", "Early Gothic", "Kentucky Derby", "squirrels", "Brian Keith Bosworth", "1991", "DuMont Television Network", "Birmingham, Alabama", "the following identity", "2018", "bee gee", "apple", "gravitation", "nonconservative forces act to change the internal energies of the system, and are often associated with the transfer of heat", "bulls", "the applied force is opposed by static friction, generated between the object and the table surface", "ormond sacker", "sazerac", "the BPI gold-selling \" Shut Up\"", "nahuatl", "Derek Hough", "Seattle Seahawks", "Florida", "four", "Private Mass", "kevin national flag", "mycelium"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2424603174603175}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.38095238095238093, 0.0, 0.2666666666666667, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-2737", "mrqa_squad-validation-482", "mrqa_naturalquestions-validation-8427", "mrqa_triviaqa-validation-1499", "mrqa_squad-validation-1100", "mrqa_triviaqa-validation-1923", "mrqa_hotpotqa-validation-2442", "mrqa_squad-validation-5836", "mrqa_squad-validation-9021", "mrqa_naturalquestions-validation-1784", "mrqa_triviaqa-validation-5573", "mrqa_triviaqa-validation-3518", "mrqa_naturalquestions-validation-6075", "mrqa_squad-validation-10420", "mrqa_triviaqa-validation-4080", "mrqa_squad-validation-10313", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-7605", "mrqa_hotpotqa-validation-875", "mrqa_triviaqa-validation-2383", "mrqa_naturalquestions-validation-1783", "mrqa_squad-validation-239", "mrqa_naturalquestions-validation-5468", "mrqa_hotpotqa-validation-4174", "mrqa_squad-validation-2296", "mrqa_triviaqa-validation-262", "mrqa_triviaqa-validation-3691"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 66, "before_eval": {"predictions": ["cucumber", "to fund his Colorado Springs experiments", "the eastern shore of the Firth of Clyde, Scotland, at the north-western corner of the county of Ayrshire", "On the Computational Complexity of Algorithms", "from sea level", "Raja Dhilu", "Magnetically soft ( low coercivity ) iron", "\"O\" whose name is pronounced the same way as \"eau\" the French word for \"water\"", "white rabbit", "Duke Kent- Brown", "O2", "oysters", "platypus", "better academic results than government schools formerly reserved for other race groups", "Vanessa Block", "insane", "2p + 1", "white", "lion", "http://www.example.com/index.html", "george i", "`` Abigail ''", "two catechisms", "cheddar", "Steel pipes and storage vessels used to store and transmit both gaseous and liquid oxygen will act as a fuel", "Saint-Domingue", "disappearance during his return trip back to Wittenberg", "1963", "living prokaryotic cell ( or organelle )", "Commissioners", "the traditional name or sometimes the Seven Years' War", "sattu paratha"], "metric_results": {"EM": 0.15625, "QA-F1": 0.27318218954248363}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.125, 0.35294117647058826, 1.0, 1.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.4, 0.0, 0.0, 0.0, 0.375, 0.28571428571428575, 0.0, 0.8, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.07142857142857142, 1.0, 0.2222222222222222, 0.0, 0.33333333333333337, 1.0, 0.13333333333333333, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4099", "mrqa_squad-validation-1416", "mrqa_hotpotqa-validation-1052", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-5927", "mrqa_hotpotqa-validation-5712", "mrqa_triviaqa-validation-7577", "mrqa_squad-validation-6945", "mrqa_squad-validation-3477", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-3970", "mrqa_squad-validation-7130", "mrqa_hotpotqa-validation-4951", "mrqa_triviaqa-validation-515", "mrqa_squad-validation-8980", "mrqa_triviaqa-validation-5337", "mrqa_triviaqa-validation-6566", "mrqa_naturalquestions-validation-8229", "mrqa_triviaqa-validation-6654", "mrqa_naturalquestions-validation-5093", "mrqa_triviaqa-validation-5686", "mrqa_squad-validation-3483", "mrqa_squad-validation-2269", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-10357", "mrqa_squad-validation-10167", "mrqa_triviaqa-validation-2087"], "retrieved_ids": ["mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-6689", "mrqa_naturalquestions-train-13563", "mrqa_naturalquestions-train-72722", "mrqa_naturalquestions-train-82315", "mrqa_naturalquestions-train-53606", "mrqa_naturalquestions-train-11357", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-train-86241", "mrqa_naturalquestions-train-48852", "mrqa_naturalquestions-train-24237", "mrqa_naturalquestions-train-72123", "mrqa_naturalquestions-train-47600", "mrqa_naturalquestions-train-6133", "mrqa_naturalquestions-train-43248", "mrqa_naturalquestions-train-78860", "mrqa_naturalquestions-train-44747", "mrqa_naturalquestions-train-68803", "mrqa_naturalquestions-train-72853", "mrqa_naturalquestions-train-43015", "mrqa_naturalquestions-train-28761", "mrqa_naturalquestions-train-60689", "mrqa_hotpotqa-validation-820", "mrqa_naturalquestions-train-43791", "mrqa_naturalquestions-train-57147", "mrqa_hotpotqa-validation-2635", "mrqa_hotpotqa-validation-859", "mrqa_hotpotqa-validation-118", "mrqa_naturalquestions-validation-114", "mrqa_hotpotqa-validation-5325", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-893", "mrqa_triviaqa-validation-7592", "mrqa_naturalquestions-train-18324", "mrqa_naturalquestions-train-17358", "mrqa_naturalquestions-train-3503", "mrqa_naturalquestions-train-19241", "mrqa_naturalquestions-train-52350", "mrqa_naturalquestions-train-872", "mrqa_naturalquestions-train-33482", "mrqa_naturalquestions-train-80441", "mrqa_naturalquestions-train-8018", "mrqa_naturalquestions-train-79259", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-455", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-6398", "mrqa_triviaqa-validation-2803", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-6326", "mrqa_triviaqa-validation-6478", "mrqa_naturalquestions-train-80708", "mrqa_naturalquestions-train-58800", "mrqa_naturalquestions-train-24834", "mrqa_naturalquestions-train-68904", "mrqa_naturalquestions-train-4468", "mrqa_hotpotqa-validation-779", "mrqa_naturalquestions-train-60641", "mrqa_naturalquestions-train-27782", "mrqa_naturalquestions-train-32005", "mrqa_naturalquestions-train-15636", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-3664", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-2959", "mrqa_squad-validation-8979", "mrqa_squad-validation-8966", "mrqa_naturalquestions-train-4508", "mrqa_naturalquestions-train-86422", "mrqa_naturalquestions-train-29696", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-262", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-6373", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-5087", "mrqa_triviaqa-validation-1015", "mrqa_triviaqa-validation-6373", "mrqa_naturalquestions-train-10631", "mrqa_naturalquestions-validation-2663", "mrqa_naturalquestions-train-33678", "mrqa_naturalquestions-train-79409", "mrqa_naturalquestions-train-2856", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-1293", "mrqa_triviaqa-validation-2181", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-train-9627", "mrqa_naturalquestions-train-70332", "mrqa_naturalquestions-train-54715", "mrqa_naturalquestions-train-48803", "mrqa_naturalquestions-train-43446", "mrqa_triviaqa-validation-2959", "mrqa_naturalquestions-train-35002", "mrqa_triviaqa-validation-7578", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-5997", "mrqa_squad-validation-3478", "mrqa_hotpotqa-validation-5822", "mrqa_naturalquestions-train-62864", "mrqa_naturalquestions-train-57237", "mrqa_naturalquestions-train-60814", "mrqa_naturalquestions-train-22090", "mrqa_naturalquestions-train-63860", "mrqa_naturalquestions-train-15372", "mrqa_naturalquestions-train-87808", "mrqa_naturalquestions-train-50337", "mrqa_naturalquestions-train-44839", "mrqa_naturalquestions-train-946", "mrqa_naturalquestions-train-13920", "mrqa_naturalquestions-train-66434", "mrqa_naturalquestions-train-7961", "mrqa_naturalquestions-train-55873", "mrqa_naturalquestions-train-57", "mrqa_naturalquestions-train-4691", "mrqa_naturalquestions-train-27805", "mrqa_naturalquestions-train-14848", "mrqa_naturalquestions-train-44626", "mrqa_naturalquestions-train-79266", "mrqa_naturalquestions-train-24557", "mrqa_naturalquestions-train-18672", "mrqa_naturalquestions-train-57469", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-6373", "mrqa_triviaqa-validation-3568", "mrqa_triviaqa-validation-7506"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 67, "before_eval": {"predictions": ["high inequality", "king Richard'the Lionheart' of England", "funk", "Sweden, Norway and Denmark", "Spitsbergen in Svalbard, Norway", "Deposition", "Norman origin", "Preacher", "payday loans", "Doctor of Philosophy", "1,389", "Otis Timson", "cross-fire hurricane", "government representatives", "$150,000", "1963", "Ohio", "runner-up", "The Crowned Prince of the Philadelphia Mob", "\u201c Resign.\u201d", "romantic attraction, sexual attraction, or sexual behavior toward both males and females, or romantic or sexual attraction to people of any sex or gender identity", "Ranked positions in the pre-Heian Imperial court were established", "Big 12 Conference", "1920s", "American", "Cleopatra VII Philopator", "Mean Girls", "mill stream of Flatford mill", "american Revolution", "chloroplasts", "Salta, Chaco, Santa Fe, C\u00f3rdoba, Catamarca and Tucum\u00e1n", "more than 265 million business records worldwide"], "metric_results": {"EM": 0.25, "QA-F1": 0.3572984307359307}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.25, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.5454545454545454, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.4444444444444445]}}, "error_ids": ["mrqa_triviaqa-validation-6619", "mrqa_triviaqa-validation-7748", "mrqa_hotpotqa-validation-5415", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-4648", "mrqa_triviaqa-validation-1385", "mrqa_hotpotqa-validation-5297", "mrqa_squad-validation-4721", "mrqa_naturalquestions-validation-8911", "mrqa_triviaqa-validation-1583", "mrqa_squad-validation-8526", "mrqa_squad-validation-8837", "mrqa_naturalquestions-validation-1976", "mrqa_hotpotqa-validation-294", "mrqa_squad-validation-6974", "mrqa_hotpotqa-validation-1283", "mrqa_squad-validation-6327", "mrqa_hotpotqa-validation-3345", "mrqa_hotpotqa-validation-5872", "mrqa_triviaqa-validation-5013", "mrqa_triviaqa-validation-3395", "mrqa_squad-validation-8929", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-2171"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 68, "before_eval": {"predictions": ["Nuevo Reino de Le\u00f3n", "Delaware", "Diego Rub\u00e9n Tonetto", "Lee Thompson Young", "The King of Chutzpah", "P $ C", "energy", "bakerloo", "English", "joseph smith", "Monk's", "upper and lower bounds", "bridge", "the dot", "dieppe", "13 August 1961", "Tyler Posey", "Buckland Valley near Bright", "hard-to-fill positions", "martial arts", "brontosaurus", "a citizen of the United Kingdom", "Megyn Price", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "cathy Overton-Clapham, Jill Officer and Dawn Askin", "1939", "tolled ( quota ) highways", "redox", "eastern and interior Venezuela", "tsar", "Sokovia", "politically conservative"], "metric_results": {"EM": 0.3125, "QA-F1": 0.35063244047619047}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, true, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.6666666666666666]}}, "error_ids": ["mrqa_naturalquestions-validation-6352", "mrqa_triviaqa-validation-4485", "mrqa_hotpotqa-validation-4978", "mrqa_naturalquestions-validation-8791", "mrqa_triviaqa-validation-2723", "mrqa_triviaqa-validation-6896", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-6302", "mrqa_naturalquestions-validation-10496", "mrqa_hotpotqa-validation-1145", "mrqa_squad-validation-2843", "mrqa_triviaqa-validation-3720", "mrqa_triviaqa-validation-5893", "mrqa_squad-validation-9493", "mrqa_naturalquestions-validation-7286", "mrqa_hotpotqa-validation-2867", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-10354", "mrqa_triviaqa-validation-3089", "mrqa_naturalquestions-validation-2632", "mrqa_hotpotqa-validation-4275"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 69, "before_eval": {"predictions": ["State Route 41", "Max Planck", "music became more expressive and emotional, expanding to encompass literary, artistic, and philosophical themes", "Thermochemical techniques", "North Kest even, Lincolnshire, England", "the Ming", "on a stretch of Forsyth Street at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "Abu al-Qasim al-Zahrawi (Abulcasis)", "Hellenismos", "the lower back", "The FCI accepted the long - haired type in 2010, listing it as the variety b", "unknown", "tea, horticultural produce, and coffee", "Barclays", "Christina Applegate as Sue Ellen `` Swell '' Crandell", "Red", "john Cassavetes", "break off the cathode, pass out of the tube, and physically strike him", "The Painting Galleries", "21.8 %", "a performance boost from enriched O2 mixtures only if they are breathed during aerobic exercise", "Yuan dynasty was the first time that non-native Chinese people ruled all of China", "Tony Orlando and Dawn", "Paris", "weavers", "Martha Wainwright", "1,462", "island of man", "West Norse sailors", "dukas", "wind", "1698"], "metric_results": {"EM": 0.15625, "QA-F1": 0.32571231617647056}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.25, 0.125, 1.0, 0.6666666666666665, 0.0, 0.8125, 0.8571428571428571, 0.0, 0.0, 0.11764705882352941, 0.09523809523809523, 1.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.42857142857142855, 0.22222222222222224, 0.2857142857142857, 0.0, 0.11764705882352941, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8645", "mrqa_naturalquestions-validation-8059", "mrqa_hotpotqa-validation-4307", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-6949", "mrqa_squad-validation-6464", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-4844", "mrqa_triviaqa-validation-2464", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-8394", "mrqa_triviaqa-validation-1449", "mrqa_squad-validation-1388", "mrqa_squad-validation-5571", "mrqa_naturalquestions-validation-3035", "mrqa_squad-validation-3610", "mrqa_squad-validation-8185", "mrqa_hotpotqa-validation-3435", "mrqa_triviaqa-validation-5494", "mrqa_hotpotqa-validation-665", "mrqa_triviaqa-validation-6541", "mrqa_naturalquestions-validation-7217", "mrqa_triviaqa-validation-6782", "mrqa_triviaqa-validation-5088", "mrqa_hotpotqa-validation-1381"], "retrieved_ids": ["mrqa_naturalquestions-train-10375", "mrqa_naturalquestions-train-42433", "mrqa_naturalquestions-train-16884", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-train-30306", "mrqa_naturalquestions-train-6631", "mrqa_naturalquestions-train-30389", "mrqa_naturalquestions-train-64654", "mrqa_naturalquestions-train-52487", "mrqa_naturalquestions-train-78716", "mrqa_naturalquestions-train-21281", "mrqa_naturalquestions-train-8124", "mrqa_naturalquestions-train-31407", "mrqa_naturalquestions-train-30178", "mrqa_naturalquestions-train-61196", "mrqa_naturalquestions-train-40770", "mrqa_naturalquestions-train-52771", "mrqa_naturalquestions-train-9023", "mrqa_naturalquestions-train-54157", "mrqa_naturalquestions-train-47418", "mrqa_naturalquestions-train-3601", "mrqa_naturalquestions-train-44626", "mrqa_naturalquestions-train-30545", "mrqa_naturalquestions-train-54999", "mrqa_naturalquestions-train-55816", "mrqa_naturalquestions-train-61743", "mrqa_naturalquestions-train-85411", "mrqa_naturalquestions-train-43731", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-train-75322", "mrqa_naturalquestions-train-25813", "mrqa_naturalquestions-train-51524", "mrqa_naturalquestions-train-30332", "mrqa_naturalquestions-train-7341", "mrqa_naturalquestions-train-30253", "mrqa_naturalquestions-train-10617", "mrqa_naturalquestions-train-23035", "mrqa_naturalquestions-train-6049", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-16027", "mrqa_naturalquestions-train-52318", "mrqa_naturalquestions-train-31891", "mrqa_naturalquestions-validation-2808", "mrqa_naturalquestions-train-43519", "mrqa_naturalquestions-train-8315", "mrqa_naturalquestions-train-27314", "mrqa_naturalquestions-train-47066", "mrqa_naturalquestions-train-8810", "mrqa_naturalquestions-train-77573", "mrqa_naturalquestions-train-73609", "mrqa_naturalquestions-train-49259", "mrqa_naturalquestions-train-25549", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-6888", "mrqa_naturalquestions-train-82315", "mrqa_naturalquestions-train-66748", "mrqa_naturalquestions-train-80490", "mrqa_naturalquestions-train-55983", "mrqa_naturalquestions-train-70253", "mrqa_naturalquestions-train-77702", "mrqa_naturalquestions-train-46650", "mrqa_naturalquestions-train-2187", "mrqa_naturalquestions-train-78345", "mrqa_naturalquestions-train-62491", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-5822", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-4130", "mrqa_naturalquestions-train-6991", "mrqa_naturalquestions-train-21987", "mrqa_squad-validation-1612", "mrqa_naturalquestions-train-45268", "mrqa_naturalquestions-train-84845", "mrqa_naturalquestions-train-80228", "mrqa_naturalquestions-train-64329", "mrqa_naturalquestions-train-17917", "mrqa_squad-validation-5449", "mrqa_naturalquestions-train-21090", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-train-41844", "mrqa_naturalquestions-train-66021", "mrqa_naturalquestions-train-50358", "mrqa_naturalquestions-train-46650", "mrqa_naturalquestions-train-66846", "mrqa_naturalquestions-train-32645", "mrqa_squad-validation-5360", "mrqa_naturalquestions-train-73538", "mrqa_naturalquestions-train-70715", "mrqa_naturalquestions-train-63092", "mrqa_naturalquestions-train-59761", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-6172", "mrqa_naturalquestions-train-71615", "mrqa_naturalquestions-train-53922", "mrqa_squad-validation-9984", "mrqa_naturalquestions-train-84927", "mrqa_naturalquestions-train-12128", "mrqa_naturalquestions-train-73275", "mrqa_naturalquestions-train-57237", "mrqa_hotpotqa-validation-189", "mrqa_naturalquestions-train-31251", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-579", "mrqa_triviaqa-validation-1995", "mrqa_triviaqa-validation-6186", "mrqa_naturalquestions-train-73434", "mrqa_naturalquestions-train-24281", "mrqa_naturalquestions-train-47504", "mrqa_naturalquestions-train-8132", "mrqa_naturalquestions-train-69270", "mrqa_naturalquestions-train-61641", "mrqa_naturalquestions-train-24615", "mrqa_hotpotqa-validation-452", "mrqa_naturalquestions-train-62065", "mrqa_naturalquestions-train-42632", "mrqa_squad-validation-9984", "mrqa_naturalquestions-train-15426", "mrqa_naturalquestions-validation-1565", "mrqa_naturalquestions-train-58545", "mrqa_naturalquestions-train-80350", "mrqa_naturalquestions-train-28545", "mrqa_triviaqa-validation-6398", "mrqa_naturalquestions-train-51350", "mrqa_naturalquestions-train-77486", "mrqa_naturalquestions-train-35494", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-train-54223", "mrqa_naturalquestions-train-61932", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-train-34220", "mrqa_naturalquestions-train-7666", "mrqa_naturalquestions-train-30455", "mrqa_naturalquestions-train-85155", "mrqa_naturalquestions-train-30515"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 70, "before_eval": {"predictions": ["kurt steiner", "Battle of White Plains", "February 28, 2018", "his hotel room", "Ford's Victorian plants\u2013in Broadmeadows and Geelong\u2013will close in October 2016", "cobbler", "Eadred Lulisc", "1972", "film", "a loanword of the Visigothic word guma `` man ''", "Major General Miles Francis Stapleton Fitzalan-Howard, 17th Duke of Norfolk", "kaph", "New Zealand", "iron and chromium", "white", "home from their vacation in Cape Cod", "derived from the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s ( brother, brotherly )", "82.30'E longitude, in Mirzapur, Uttar Pradesh, which is nearly on the corresponding longitude reference line", "a \"theo-democracy\" based on the principles of: tawhid ( unity of God) risala (prophethood) and khilafa (caliphate)", "Salman Khan", "Skip Tyler", "the attempt to discover first principles --'those universal principles which are the condition of the possibility of the existence of anything and everything '", "WBMA-LD", "two Nobel Peace Prizes, once in 1954 and again in 1981", "Jack Daniel's Old No. 7 Brand front chest logo", "physically strike him", "king james i of Great Britain", "an individual's rights which, although not legally binding in themselves, have been elaborated in subsequent international treaties, economic transfers, regional human rights instruments, national constitutions, and other laws", "a straight line (see world line) traveling through time, which normally increases up or to the right in the diagram", "Newton", "International Imitation Hemingway Competition", "Kingsford, Michigan"], "metric_results": {"EM": 0.125, "QA-F1": 0.29070182816409595}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.7499999999999999, 0.0, 0.3076923076923077, 0.5, 0.0, 0.0, 0.19047619047619047, 0.4761904761904762, 0.35294117647058826, 0.0, 1.0, 0.08, 1.0, 0.5333333333333333, 0.3636363636363636, 0.0, 0.9090909090909091, 0.05714285714285714, 0.21052631578947367, 0.0, 0.28571428571428575, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-831", "mrqa_hotpotqa-validation-4939", "mrqa_naturalquestions-validation-2083", "mrqa_squad-validation-2980", "mrqa_triviaqa-validation-5981", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-259", "mrqa_naturalquestions-validation-3019", "mrqa_hotpotqa-validation-5334", "mrqa_triviaqa-validation-2331", "mrqa_naturalquestions-validation-4567", "mrqa_triviaqa-validation-6740", "mrqa_triviaqa-validation-672", "mrqa_hotpotqa-validation-4323", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-4891", "mrqa_squad-validation-9661", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-4211", "mrqa_hotpotqa-validation-4356", "mrqa_triviaqa-validation-1408", "mrqa_squad-validation-1389", "mrqa_triviaqa-validation-2546", "mrqa_naturalquestions-validation-7938", "mrqa_squad-validation-10477", "mrqa_squad-validation-361", "mrqa_hotpotqa-validation-4543", "mrqa_naturalquestions-validation-2229"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 71, "before_eval": {"predictions": ["oil magnate and wealthiest man in history John D. Rockefeller", "42", "electrical, water, sewage, phone, and cable facilities", "Saxe-Coburg and Gotha", "16,000", "no known case of any U.S. citizens buying Canadian drugs for personal use with a prescription", "Dwight D. Eisenhower", "multi-purpose", "Khitan Liao and Jurchen Jin", "twelve", "Henley royale", "Randal Keith Orton", "Lily Tomlin", "liquid oxygen storage containers", "Sunday", "October 16, 2012", "Austria", "1996", "alpha efferent neurons", "antlers", "Deborah Lipstadt", "cabbage", "rock and roll and rockabilly", "the Hongwu Emperor of the Ming Dynasty", "heptathlon", "1565", "rapid expansion in telecommunication and financial activity", "fair Maid of Perth", "Caroline Sterling, n\u00e9e Bone, formerly Pemberton", "freedom of choice, voluntary association, individual judgment, and self-ownership", "london", "R&B"], "metric_results": {"EM": 0.125, "QA-F1": 0.22075988247863249}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.4615384615384615, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.625, 0.1111111111111111, 0.4, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4]}}, "error_ids": ["mrqa_squad-validation-7849", "mrqa_hotpotqa-validation-253", "mrqa_hotpotqa-validation-862", "mrqa_squad-validation-4344", "mrqa_squad-validation-6383", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-5619", "mrqa_squad-validation-8291", "mrqa_naturalquestions-validation-2635", "mrqa_triviaqa-validation-6938", "mrqa_hotpotqa-validation-583", "mrqa_naturalquestions-validation-8136", "mrqa_squad-validation-3689", "mrqa_hotpotqa-validation-5203", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2548", "mrqa_triviaqa-validation-2157", "mrqa_hotpotqa-validation-5162", "mrqa_triviaqa-validation-4291", "mrqa_triviaqa-validation-1185", "mrqa_naturalquestions-validation-8907", "mrqa_triviaqa-validation-237", "mrqa_naturalquestions-validation-2477", "mrqa_triviaqa-validation-1640", "mrqa_naturalquestions-validation-2847", "mrqa_hotpotqa-validation-1694", "mrqa_triviaqa-validation-4464", "mrqa_hotpotqa-validation-2866"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 72, "before_eval": {"predictions": ["Monk's Caf\u00e9", "ancient Rome with gift - giving during the Saturnalia holiday, which took place that month", "fourth-most Republican leader in the House", "seven", "vertex cover problem", "Operation Neptune", "john f kenna", "Patrick Moore", "alpaca fiber and mohair from Angora goats", "Muskogean", "Symbolic interactionism", "the fictional town of West Egg on prosperous Long Island", "gesture in which the head is tilted in alternating up and down arcs along the sagittal plane", "Little Big League", "snow and rain", "Conservative Member of Parliament", "# 4 School of Public Health", "henry", "dolphin", "1883\u201384", "thornless honey locust", "Les Surfs as `` A Pr\u00e9sent Tu Peux t'en Aller '", "fox river", "Macau Peninsula, Macau", "produce \"de novo\"", "jack smiths", "747-8", "erotic romantic comedy", "Greg", "eighteenth century", "Puente Hills Mall, located in the City of Industry, California, United States", "longchamp"], "metric_results": {"EM": 0.09375, "QA-F1": 0.2939183212620713}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true], "QA-F1": [0.4, 0.7272727272727274, 0.8000000000000002, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.125, 0.0, 0.0, 0.4, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.7692307692307693, 0.0, 0.4, 0.30769230769230765, 0.6666666666666666, 0.0, 0.8571428571428571, 0.0, 0.2857142857142857, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-4815", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-1110", "mrqa_squad-validation-1760", "mrqa_hotpotqa-validation-4061", "mrqa_triviaqa-validation-6606", "mrqa_triviaqa-validation-5439", "mrqa_naturalquestions-validation-5531", "mrqa_hotpotqa-validation-946", "mrqa_naturalquestions-validation-1507", "mrqa_naturalquestions-validation-1359", "mrqa_hotpotqa-validation-5478", "mrqa_squad-validation-3592", "mrqa_hotpotqa-validation-2192", "mrqa_naturalquestions-validation-9306", "mrqa_triviaqa-validation-417", "mrqa_triviaqa-validation-3799", "mrqa_squad-validation-9855", "mrqa_triviaqa-validation-4730", "mrqa_naturalquestions-validation-3638", "mrqa_triviaqa-validation-5393", "mrqa_hotpotqa-validation-1394", "mrqa_squad-validation-8829", "mrqa_triviaqa-validation-564", "mrqa_triviaqa-validation-1162", "mrqa_hotpotqa-validation-3252", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1858"], "retrieved_ids": ["mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-train-78402", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-train-15372", "mrqa_naturalquestions-train-56778", "mrqa_naturalquestions-train-23475", "mrqa_naturalquestions-train-8810", "mrqa_naturalquestions-train-69875", "mrqa_naturalquestions-train-3097", "mrqa_naturalquestions-train-13554", "mrqa_naturalquestions-train-40166", "mrqa_naturalquestions-train-81327", "mrqa_naturalquestions-train-16644", "mrqa_naturalquestions-train-59681", "mrqa_naturalquestions-train-80588", "mrqa_hotpotqa-validation-4356", "mrqa_naturalquestions-train-7947", "mrqa_naturalquestions-train-76606", "mrqa_naturalquestions-train-49827", "mrqa_naturalquestions-train-40793", "mrqa_squad-validation-9984", "mrqa_naturalquestions-train-18868", "mrqa_naturalquestions-train-74912", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-train-54176", "mrqa_naturalquestions-train-87260", "mrqa_naturalquestions-train-21199", "mrqa_naturalquestions-train-47470", "mrqa_naturalquestions-train-58887", "mrqa_naturalquestions-train-49469", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-train-40261", "mrqa_naturalquestions-train-60977", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-train-21014", "mrqa_naturalquestions-train-24615", "mrqa_triviaqa-validation-5997", "mrqa_naturalquestions-validation-6326", "mrqa_triviaqa-validation-455", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-train-44589", "mrqa_naturalquestions-train-69860", "mrqa_naturalquestions-train-53908", "mrqa_naturalquestions-train-59943", "mrqa_naturalquestions-train-16490", "mrqa_naturalquestions-train-35494", "mrqa_naturalquestions-train-755", "mrqa_naturalquestions-train-79410", "mrqa_naturalquestions-train-64565", "mrqa_naturalquestions-train-82315", "mrqa_squad-validation-1879", "mrqa_naturalquestions-train-61465", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-36135", "mrqa_naturalquestions-train-65923", "mrqa_naturalquestions-train-7959", "mrqa_naturalquestions-train-56461", "mrqa_naturalquestions-train-49719", "mrqa_naturalquestions-train-87351", "mrqa_naturalquestions-train-78621", "mrqa_hotpotqa-validation-2937", "mrqa_naturalquestions-train-30259", "mrqa_hotpotqa-validation-1968", "mrqa_naturalquestions-train-20900", "mrqa_naturalquestions-train-59204", "mrqa_naturalquestions-train-86362", "mrqa_naturalquestions-train-69875", "mrqa_naturalquestions-validation-570", "mrqa_naturalquestions-train-66951", "mrqa_hotpotqa-validation-118", "mrqa_naturalquestions-train-52895", "mrqa_naturalquestions-train-82140", "mrqa_naturalquestions-train-40770", "mrqa_hotpotqa-validation-3774", "mrqa_naturalquestions-train-18797", "mrqa_naturalquestions-train-60667", "mrqa_naturalquestions-train-83106", "mrqa_naturalquestions-train-67230", "mrqa_naturalquestions-train-49007", "mrqa_triviaqa-validation-5057", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-6639", "mrqa_triviaqa-validation-7371", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-7090", "mrqa_naturalquestions-validation-142", "mrqa_triviaqa-validation-5997", "mrqa_naturalquestions-train-87250", "mrqa_naturalquestions-train-53264", "mrqa_naturalquestions-train-13642", "mrqa_naturalquestions-train-45208", "mrqa_naturalquestions-train-72538", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-3664", "mrqa_triviaqa-validation-4708", "mrqa_triviaqa-validation-2530", "mrqa_naturalquestions-train-41128", "mrqa_naturalquestions-train-36076", "mrqa_naturalquestions-train-56676", "mrqa_naturalquestions-train-60023", "mrqa_naturalquestions-train-42768", "mrqa_triviaqa-validation-1859", "mrqa_triviaqa-validation-7598", "mrqa_triviaqa-validation-3515", "mrqa_triviaqa-validation-6896", "mrqa_triviaqa-validation-140", "mrqa_naturalquestions-train-62011", "mrqa_naturalquestions-train-64931", "mrqa_naturalquestions-train-35494", "mrqa_naturalquestions-train-33306", "mrqa_naturalquestions-train-67017", "mrqa_naturalquestions-train-57521", "mrqa_naturalquestions-train-79942", "mrqa_naturalquestions-train-52050", "mrqa_naturalquestions-train-36274", "mrqa_naturalquestions-train-74251", "mrqa_triviaqa-validation-7215", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-6896", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-4298", "mrqa_naturalquestions-train-38985", "mrqa_hotpotqa-validation-118", "mrqa_naturalquestions-train-66866", "mrqa_hotpotqa-validation-4277", "mrqa_naturalquestions-train-67401", "mrqa_hotpotqa-validation-304", "mrqa_naturalquestions-train-24447", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-983", "mrqa_naturalquestions-train-6991", "mrqa_naturalquestions-train-83773", "mrqa_naturalquestions-train-70465", "mrqa_naturalquestions-train-85623", "mrqa_naturalquestions-train-79093", "mrqa_naturalquestions-train-54999", "mrqa_naturalquestions-train-49194", "mrqa_naturalquestions-train-49380", "mrqa_naturalquestions-train-13655", "mrqa_naturalquestions-train-59203"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 73, "before_eval": {"predictions": ["northwest", "abraham", "light", "Boston", "the 10th Cavalry Regiment", "nearly 8 km from the Swedish coast to the artificial island Peberholm in the middle of the strait", "The Future", "1969", "epic historical drama", "Wahhabist", "buzzards", "drug management system development, deployment and optimization", "from that first set of endosymbiotic events", "Danish - Norwegian patronymic surname meaning `` son of Anders ''", "Edward Trowbridge Collins Sr.", "September 2014", "when a country's influence is felt in social and cultural circles, i.e. its soft power, such that it changes the moral, cultural and societal worldview of another", "increased productivity, trade, and secular economic trends", "wendy and Lisa", "After World War I", "1958 and 1979", "geographer", "around 200\u2013300", "all in his mind", "structure and forces that act on one part of an object might affect other parts of an objects", "gravity", "the \"Gliding Dance of the Maidens\" from the \"Polovtsian Dances\" in the opera \"Prince Igor\"", "tain", "government - owned Panama Canal Authority", "Vernier, Switzerland", "harmonica", "\"PM Magazine\""], "metric_results": {"EM": 0.28125, "QA-F1": 0.38898745929995937}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.8333333333333333, 0.2222222222222222, 1.0, 0.33333333333333337, 0.14814814814814814, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.1111111111111111, 0.0, 0.3076923076923077, 0.0, 0.3846153846153846, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-5024", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-186", "mrqa_triviaqa-validation-56", "mrqa_hotpotqa-validation-3483", "mrqa_squad-validation-9592", "mrqa_triviaqa-validation-7082", "mrqa_squad-validation-6388", "mrqa_squad-validation-8801", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-2069", "mrqa_squad-validation-9871", "mrqa_naturalquestions-validation-9530", "mrqa_triviaqa-validation-7642", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-2567", "mrqa_squad-validation-6550", "mrqa_squad-validation-10430", "mrqa_squad-validation-10455", "mrqa_hotpotqa-validation-4284", "mrqa_triviaqa-validation-4408", "mrqa_naturalquestions-validation-9753", "mrqa_triviaqa-validation-7160"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 74, "before_eval": {"predictions": ["the Gulf of Mexico", "Friedrich Nietzsche", "Fern", "Germanic languages", "Tower Hamlets", "Zeppelin", "nedale white", "DeWayne Warren", "5 ft", "hard Candy", "George Merrill and Shannon Rubicam of the band Boy Meets Girl", "Independence, Missouri", "1943", "translation", "wai Momi", "$125 per month", "student tuition, endowments, scholarship/voucher funds, and donations and grants from religious organizations or private individuals", "tolerance of civil disobedience", "ed mcdaniel", "China in American colonies", "film scripts written by ghost writers, nonfiction books on military subjects, and video games", "British and French Canadian fur traders from before 1810, and American settlers from the mid-1830s", "Instagram", "John Sebastian Bach, Karlheinz Stockhausen, Terry Riley, Philip Glass and Moondog", "1986", "listen to her beautiful voice", "22 July 1930", "gluons, which form part of the virtual pi and rho mesons", "written each article of the Creed to express the character of the Father, the Son, or the Holy Spirit", "March 23, 2018", "moulin Rouge", "business magnate, investor, and philanthropist"], "metric_results": {"EM": 0.15625, "QA-F1": 0.26096117424242427}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.9090909090909091, 0.4, 1.0, 1.0, 0.0, 0.0, 0.125, 1.0, 0.5, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.33333333333333337]}}, "error_ids": ["mrqa_naturalquestions-validation-8072", "mrqa_hotpotqa-validation-5520", "mrqa_triviaqa-validation-6002", "mrqa_hotpotqa-validation-2098", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-6573", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2644", "mrqa_triviaqa-validation-4275", "mrqa_naturalquestions-validation-8008", "mrqa_naturalquestions-validation-7462", "mrqa_triviaqa-validation-2938", "mrqa_squad-validation-1462", "mrqa_squad-validation-7086", "mrqa_triviaqa-validation-4614", "mrqa_naturalquestions-validation-642", "mrqa_hotpotqa-validation-2220", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-1027", "mrqa_hotpotqa-validation-726", "mrqa_naturalquestions-validation-9487", "mrqa_squad-validation-801", "mrqa_hotpotqa-validation-4964", "mrqa_squad-validation-10447", "mrqa_squad-validation-2391", "mrqa_triviaqa-validation-5108", "mrqa_hotpotqa-validation-83"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 75, "before_eval": {"predictions": ["December 1974", "Inigo Jones", "Bert Jansch", "Gaahl", "20,19,18,17,16,15", "light tank", "james iv", "American writer and satirist Kurt Vonnegut", "bhasa", "1902", "a bronze statue designed by Thomas Crawford ( 1814 -- 1857 ) that, since 1863, has crowned the dome of the U.S. Capitol building in Washington, D.C.", "1963", "17:16:20 GMT", "34 percent of the city's total population", "Limbo", "the state legislators of Assam", "350 government officials and climate change experts", "roofs of the choir side - aisles", "borodino", "comic", "Environmental Protection Agency", "523 km", "tfL", "Cher", "one person", "Luger P08", "Al-Masjid an-Nabawi", "the ascension of Akbar the Great to the throne", "asylums", "from Manitoba, Ontario and Nova Scotia in southern Canada to northern Florida and from the Atlantic coast to the Missouri River and the eastern Great Plains", "manned lunar landing", "sweden"], "metric_results": {"EM": 0.1875, "QA-F1": 0.24441964285714285}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.2857142857142857, 1.0, 0.3333333333333333, 0.25, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.14285714285714288, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8685", "mrqa_squad-validation-5628", "mrqa_hotpotqa-validation-551", "mrqa_hotpotqa-validation-3306", "mrqa_triviaqa-validation-1217", "mrqa_naturalquestions-validation-9426", "mrqa_triviaqa-validation-6051", "mrqa_squad-validation-8035", "mrqa_triviaqa-validation-5878", "mrqa_naturalquestions-validation-4619", "mrqa_squad-validation-7732", "mrqa_squad-validation-683", "mrqa_naturalquestions-validation-9546", "mrqa_squad-validation-8525", "mrqa_naturalquestions-validation-9576", "mrqa_triviaqa-validation-2879", "mrqa_naturalquestions-validation-3347", "mrqa_hotpotqa-validation-1298", "mrqa_squad-validation-903", "mrqa_triviaqa-validation-1205", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-4413", "mrqa_naturalquestions-validation-6482", "mrqa_triviaqa-validation-1199", "mrqa_naturalquestions-validation-2006", "mrqa_hotpotqa-validation-2646"], "retrieved_ids": ["mrqa_naturalquestions-train-27314", "mrqa_naturalquestions-train-19954", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-train-21115", "mrqa_naturalquestions-validation-10612", "mrqa_naturalquestions-train-77702", "mrqa_naturalquestions-train-44690", "mrqa_naturalquestions-train-38998", "mrqa_naturalquestions-train-32336", "mrqa_naturalquestions-train-58596", "mrqa_naturalquestions-train-62596", "mrqa_squad-validation-5360", "mrqa_naturalquestions-train-55845", "mrqa_naturalquestions-train-1984", "mrqa_naturalquestions-train-855", "mrqa_naturalquestions-train-25982", "mrqa_naturalquestions-train-70592", "mrqa_naturalquestions-train-57162", "mrqa_naturalquestions-train-14418", "mrqa_naturalquestions-train-48612", "mrqa_triviaqa-validation-1162", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-667", "mrqa_naturalquestions-train-42768", "mrqa_naturalquestions-train-43598", "mrqa_naturalquestions-train-30968", "mrqa_naturalquestions-train-38997", "mrqa_naturalquestions-train-36540", "mrqa_naturalquestions-train-58545", "mrqa_naturalquestions-train-38504", "mrqa_naturalquestions-train-30811", "mrqa_naturalquestions-train-14811", "mrqa_naturalquestions-train-33306", "mrqa_naturalquestions-train-16326", "mrqa_naturalquestions-train-42716", "mrqa_naturalquestions-train-80831", "mrqa_naturalquestions-train-41621", "mrqa_naturalquestions-train-29696", "mrqa_triviaqa-validation-140", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-3617", "mrqa_naturalquestions-train-74223", "mrqa_naturalquestions-train-74228", "mrqa_naturalquestions-train-42632", "mrqa_naturalquestions-train-66044", "mrqa_naturalquestions-train-36153", "mrqa_naturalquestions-train-43631", "mrqa_naturalquestions-train-85836", "mrqa_naturalquestions-train-18555", "mrqa_naturalquestions-train-37229", "mrqa_naturalquestions-train-25327", "mrqa_squad-validation-3018", "mrqa_naturalquestions-train-1807", "mrqa_naturalquestions-train-9021", "mrqa_naturalquestions-train-15909", "mrqa_naturalquestions-train-24688", "mrqa_naturalquestions-train-38557", "mrqa_naturalquestions-train-84539", "mrqa_naturalquestions-train-12315", "mrqa_naturalquestions-train-55413", "mrqa_naturalquestions-train-52732", "mrqa_naturalquestions-train-71306", "mrqa_squad-validation-6709", "mrqa_naturalquestions-train-68456", "mrqa_naturalquestions-train-3286", "mrqa_naturalquestions-train-75388", "mrqa_naturalquestions-train-69875", "mrqa_naturalquestions-train-84083", "mrqa_naturalquestions-train-10617", "mrqa_naturalquestions-train-72472", "mrqa_naturalquestions-train-68187", "mrqa_triviaqa-validation-2999", "mrqa_triviaqa-validation-2938", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-498", "mrqa_triviaqa-validation-6619", "mrqa_naturalquestions-train-48630", "mrqa_naturalquestions-train-36185", "mrqa_naturalquestions-train-19685", "mrqa_naturalquestions-train-55156", "mrqa_naturalquestions-train-46215", "mrqa_naturalquestions-train-51088", "mrqa_naturalquestions-train-8099", "mrqa_naturalquestions-train-24946", "mrqa_naturalquestions-train-9732", "mrqa_naturalquestions-train-71368", "mrqa_hotpotqa-validation-1201", "mrqa_naturalquestions-train-74279", "mrqa_naturalquestions-train-48185", "mrqa_naturalquestions-train-824", "mrqa_naturalquestions-train-20155", "mrqa_triviaqa-validation-2723", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-7592", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-2959", "mrqa_hotpotqa-validation-389", "mrqa_squad-validation-9984", "mrqa_naturalquestions-train-42632", "mrqa_naturalquestions-train-30259", "mrqa_naturalquestions-train-66044", "mrqa_squad-validation-6464", "mrqa_naturalquestions-train-61743", "mrqa_naturalquestions-train-13255", "mrqa_naturalquestions-train-1147", "mrqa_naturalquestions-train-12110", "mrqa_naturalquestions-train-21090", "mrqa_squad-validation-6257", "mrqa_naturalquestions-train-55114", "mrqa_naturalquestions-train-65989", "mrqa_naturalquestions-train-26395", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-6689", "mrqa_naturalquestions-train-59850", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-train-28545", "mrqa_naturalquestions-train-1172", "mrqa_naturalquestions-train-66000", "mrqa_naturalquestions-train-17936", "mrqa_naturalquestions-train-293", "mrqa_naturalquestions-train-50008", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-6053", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-2130", "mrqa_triviaqa-validation-4298"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 76, "before_eval": {"predictions": ["Raiders", "2018", "Saint Etienne", "novelization", "May 27, 2016", "the Colonization movement or Black Zionism", "elected", "mulberry tree", "Remus Lupin", "Conrad Lewis, a Canadian electrical engineer with two legally blind sisters", "As of December 10, 2017 ( 2017 - 12 - 10 )", "Hertz Corporation", "2010", "Medicaid", "Christian Goldbach", "Grease", "Netrobalane canopus", "federal government", "be reborn", "Shukratara", "Tachycardia", "Future", "Richard Burbage", "an airborne plague that attacks the lungs before the rest of the body", "Parliament of the United Kingdom", "achievement-oriented motivations", "discarded", "The Revolting World of Stanley Brown", "Morrissey", "Julia McKenzie and Anton Rodgers", "15 February 1998", "Amazon.com"], "metric_results": {"EM": 0.25, "QA-F1": 0.3412660256410256}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, true, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.25, 0.15384615384615383, 0.4, 1.0, 1.0, 0.20000000000000004, 1.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-8747", "mrqa_naturalquestions-validation-7535", "mrqa_hotpotqa-validation-3337", "mrqa_naturalquestions-validation-2102", "mrqa_triviaqa-validation-4440", "mrqa_hotpotqa-validation-2786", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-53", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-1314", "mrqa_naturalquestions-validation-6258", "mrqa_squad-validation-9014", "mrqa_hotpotqa-validation-3232", "mrqa_hotpotqa-validation-2550", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-10131", "mrqa_hotpotqa-validation-956", "mrqa_naturalquestions-validation-2270", "mrqa_squad-validation-5031", "mrqa_squad-validation-9547", "mrqa_hotpotqa-validation-3634", "mrqa_triviaqa-validation-5849", "mrqa_naturalquestions-validation-9591"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 77, "before_eval": {"predictions": ["New York Knickerbockers", "100-meter freestyle", "Y. pestis", "east-west", "four", "new zealand", "at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "lofsongur", "islay", "troposphere", "The Caribbean Islands of Greater and Lesser Antilles", "The Fixx", "67,038", "The Swiss Express", "progressive moderates who seek to separate religion from politics", "graphology", "constituency seats", "Henry I", "After School", "Timmy Sanders", "at any time after the auction", "the northernmost point at which the noon sun is just visible on the December solstice", "in the fictional town of Charming, California, adjacent to the Teller - Morrow auto mechanic shop", "liverpool", "the Great Yuan", "Deepak Tijori", "1974", "Acura", "hydrogen peroxide (H2O2)", "food and clothing", "Masahiko Takeshita", "from the amino acids glycine and arginine"], "metric_results": {"EM": 0.25, "QA-F1": 0.3013456450956451}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.07407407407407407, 0.0, 1.0, 1.0, 0.4444444444444445, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.07692307692307691, 0.14285714285714288, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857]}}, "error_ids": ["mrqa_triviaqa-validation-6483", "mrqa_squad-validation-4961", "mrqa_triviaqa-validation-3900", "mrqa_naturalquestions-validation-6452", "mrqa_triviaqa-validation-593", "mrqa_triviaqa-validation-2095", "mrqa_naturalquestions-validation-8584", "mrqa_hotpotqa-validation-5052", "mrqa_hotpotqa-validation-1921", "mrqa_squad-validation-9520", "mrqa_triviaqa-validation-6204", "mrqa_squad-validation-9530", "mrqa_hotpotqa-validation-1558", "mrqa_hotpotqa-validation-4167", "mrqa_naturalquestions-validation-4475", "mrqa_naturalquestions-validation-4117", "mrqa_naturalquestions-validation-7845", "mrqa_squad-validation-8046", "mrqa_hotpotqa-validation-2622", "mrqa_triviaqa-validation-7610", "mrqa_squad-validation-3543", "mrqa_naturalquestions-validation-8163", "mrqa_hotpotqa-validation-1237", "mrqa_naturalquestions-validation-686"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 78, "before_eval": {"predictions": ["2005", "healthcare professionals with specialised education and training who perform various roles to ensure optimal health outcomes for their patients through the quality use of medicines", "fox", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "neuro-orthopaedic Irish veterinary surgeon", "criminality", "Matthew Cuthbert", "synovial joint", "qc", "slogans", "East End of London", "digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "1484", "A lymphocyte is one of the subtypes of white blood cell in a vertebrate's immune system.", "by a combination of electronic sound sources, live instrumental playing and digital signal processing", "1814", "a specific quantity of the substance to change its state from a solid to a liquid, ( or resulting from the release of energy from a substance during transition from liquid to solid ), at constant pressure", "Mongolian", "Gap", "The Indianapolis Times and the Cleveland Press", "the Jurchen Aisin Gioro clan in Manchuria", "baloney", "an expedition through the wilderness of the Maine district and down the Chaudi\u00e8re River to attack the city of Quebec", "friendship", "86.66% (757.7 sq mi or 1,962 km2)", "Tim Duncan", "computation time", "the allegedly corrupt machinations of Fran\u00e7ois Bigot", "loud and dirty", "achy breaky heart", "chocolate confectionery"], "metric_results": {"EM": 0.125, "QA-F1": 0.2593131487500829}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false], "QA-F1": [0.0, 0.3448275862068966, 0.0, 0.35294117647058826, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.14634146341463414, 0.0, 0.0, 0.5714285714285715, 0.2857142857142857, 0.0, 0.1212121212121212, 1.0, 0.0, 0.2666666666666667, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5348", "mrqa_squad-validation-6280", "mrqa_triviaqa-validation-1117", "mrqa_naturalquestions-validation-5838", "mrqa_hotpotqa-validation-1219", "mrqa_triviaqa-validation-4676", "mrqa_triviaqa-validation-1658", "mrqa_triviaqa-validation-4063", "mrqa_triviaqa-validation-1168", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-5011", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6497", "mrqa_naturalquestions-validation-9342", "mrqa_hotpotqa-validation-1296", "mrqa_squad-validation-9445", "mrqa_naturalquestions-validation-1119", "mrqa_squad-validation-8083", "mrqa_squad-validation-421", "mrqa_hotpotqa-validation-4864", "mrqa_naturalquestions-validation-7311", "mrqa_triviaqa-validation-1916", "mrqa_squad-validation-10239", "mrqa_squad-validation-7476", "mrqa_naturalquestions-validation-276", "mrqa_hotpotqa-validation-2437", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-1465"], "retrieved_ids": ["mrqa_naturalquestions-train-9223", "mrqa_naturalquestions-train-23326", "mrqa_naturalquestions-train-55251", "mrqa_naturalquestions-train-33514", "mrqa_naturalquestions-train-69270", "mrqa_naturalquestions-train-50507", "mrqa_naturalquestions-train-69046", "mrqa_naturalquestions-train-63972", "mrqa_squad-validation-8444", "mrqa_naturalquestions-train-25636", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-5168", "mrqa_triviaqa-validation-6566", "mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-2327", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-train-45598", "mrqa_naturalquestions-train-76033", "mrqa_naturalquestions-train-811", "mrqa_naturalquestions-train-7892", "mrqa_naturalquestions-train-62037", "mrqa_naturalquestions-train-39520", "mrqa_squad-validation-5586", "mrqa_squad-validation-7880", "mrqa_naturalquestions-train-66904", "mrqa_triviaqa-validation-11", "mrqa_naturalquestions-train-76391", "mrqa_naturalquestions-train-23581", "mrqa_triviaqa-validation-7592", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-3404", "mrqa_naturalquestions-train-72364", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-5686", "mrqa_naturalquestions-train-86071", "mrqa_naturalquestions-train-8238", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-train-28545", "mrqa_triviaqa-validation-11", "mrqa_naturalquestions-train-36180", "mrqa_naturalquestions-train-10533", "mrqa_naturalquestions-validation-3571", "mrqa_triviaqa-validation-4291", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-388", "mrqa_triviaqa-validation-639", "mrqa_naturalquestions-train-24615", "mrqa_naturalquestions-train-28545", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-639", "mrqa_naturalquestions-train-64654", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-train-81657", "mrqa_naturalquestions-train-17622", "mrqa_naturalquestions-train-65956", "mrqa_naturalquestions-validation-1294", "mrqa_naturalquestions-train-47866", "mrqa_squad-validation-6878", "mrqa_naturalquestions-train-47238", "mrqa_naturalquestions-train-10672", "mrqa_naturalquestions-train-47971", "mrqa_naturalquestions-train-47999", "mrqa_naturalquestions-train-60509", "mrqa_naturalquestions-train-44763", "mrqa_naturalquestions-train-55873", "mrqa_hotpotqa-validation-5637", "mrqa_hotpotqa-validation-3870", "mrqa_naturalquestions-train-53311", "mrqa_naturalquestions-train-43276", "mrqa_naturalquestions-train-1567", "mrqa_naturalquestions-train-79064", "mrqa_naturalquestions-train-56363", "mrqa_naturalquestions-train-36590", "mrqa_naturalquestions-train-30414", "mrqa_naturalquestions-train-83391", "mrqa_naturalquestions-train-23475", "mrqa_naturalquestions-train-52050", "mrqa_naturalquestions-train-32091", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-train-74251", "mrqa_naturalquestions-train-52378", "mrqa_squad-validation-8185", "mrqa_naturalquestions-train-13346", "mrqa_naturalquestions-train-85803", "mrqa_naturalquestions-train-2187", "mrqa_naturalquestions-train-21243", "mrqa_naturalquestions-train-51698", "mrqa_naturalquestions-train-74865", "mrqa_naturalquestions-train-64678", "mrqa_naturalquestions-train-51088", "mrqa_squad-validation-3181", "mrqa_naturalquestions-train-17119", "mrqa_naturalquestions-train-60814", "mrqa_naturalquestions-train-33306", "mrqa_hotpotqa-validation-5203", "mrqa_naturalquestions-train-45778", "mrqa_hotpotqa-validation-1394", "mrqa_naturalquestions-train-57237", "mrqa_squad-validation-3181", "mrqa_naturalquestions-train-33306", "mrqa_triviaqa-validation-6373", "mrqa_triviaqa-validation-3664", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-5087", "mrqa_triviaqa-validation-5516", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-train-19954", "mrqa_naturalquestions-train-64654", "mrqa_naturalquestions-train-47660", "mrqa_naturalquestions-train-39789", "mrqa_squad-validation-7042", "mrqa_squad-validation-6963", "mrqa_naturalquestions-train-25133", "mrqa_naturalquestions-train-50820", "mrqa_naturalquestions-train-3673", "mrqa_naturalquestions-train-28312", "mrqa_naturalquestions-train-21281", "mrqa_naturalquestions-train-2187", "mrqa_naturalquestions-train-46650", "mrqa_naturalquestions-train-4931", "mrqa_naturalquestions-train-37249", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-train-855", "mrqa_naturalquestions-train-101", "mrqa_naturalquestions-train-9712", "mrqa_triviaqa-validation-6250", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-579", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-1162", "mrqa_triviaqa-validation-6373", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-6053", "mrqa_triviaqa-validation-2039"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 79, "before_eval": {"predictions": ["Sir Hiram Stevens Maxim", "1987", "February 2011", "make direct representations to the Presiding Officer to nominate speakers", "Chesley Burnett \"Sully\" Sullenberger III", "October 12, 2017", "The United States Oath of Allegiance", "Kevin Whately", "shirley", "clay animation or \"clay-mation\"", "caucuses", "1977", "precedes the value", "The stationary steam engine was a key component of the Industrial Revolution", "a spherical boundary of zero thickness in which photons that move on tangents to that sphere would be trapped in a circular orbit about the black hole", "Boletus edulis", "wGN-TV", "rugged terrain such as the Arctic", "the Mexico\u2013united States border", "Denver Broncos", "a ribosome in the cytosol", "the Town of Oyster Bay in southeastern Nassau County, New York", "Ringo Starr", "shoppe", "gas turbines", "by voting or voice vote", "Shoushi Li", "2 %", "japan", "Secretary of Defense", "pancake-shaped circular disks about 300\u2013600 nanometers in diameter", "thirteen American colonies, then at war with the Kingdom of Great Britain"], "metric_results": {"EM": 0.1875, "QA-F1": 0.34748588829471183}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.4, 0.3636363636363636, 0.33333333333333337, 1.0, 0.5882352941176471, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.33333333333333337, 0.3636363636363636, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.5, 0.33333333333333337, 1.0, 0.0, 0.5, 0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.18181818181818182, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4516", "mrqa_naturalquestions-validation-9184", "mrqa_naturalquestions-validation-7733", "mrqa_squad-validation-9407", "mrqa_hotpotqa-validation-4934", "mrqa_naturalquestions-validation-1672", "mrqa_triviaqa-validation-7178", "mrqa_triviaqa-validation-5170", "mrqa_hotpotqa-validation-2846", "mrqa_triviaqa-validation-3844", "mrqa_naturalquestions-validation-4529", "mrqa_squad-validation-3256", "mrqa_naturalquestions-validation-5435", "mrqa_hotpotqa-validation-5589", "mrqa_triviaqa-validation-5983", "mrqa_squad-validation-3738", "mrqa_squad-validation-1", "mrqa_squad-validation-8960", "mrqa_hotpotqa-validation-3538", "mrqa_triviaqa-validation-5529", "mrqa_squad-validation-3369", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-8509", "mrqa_triviaqa-validation-1912", "mrqa_squad-validation-8811", "mrqa_naturalquestions-validation-360"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 80, "before_eval": {"predictions": ["John Mills and Tom Courtenay", "Calliope Kinema Club", "Piet van der Walt", "Shivnarine Chanderpaul", "hart to hart", "Charles Silverstein", "Tangmere, West Sussex", "the dollar price of oil had risen by less than two percent per year", "the French Senate", "State Bar of Arizona", "medical abnormalities, activation level, or recruitment order", "beer and soft drinks", "sunlight", "red", "discus fish", "1967", "Mondays at 21:30 (KST)", "22 miles", "1598", "the coastline peninsula of Davenports Neck called \"Bauffet's Point\"", "geoffrey", "red", "Topeka, Kansas", "500", "rochdale", "hijab", "1894", "given his nature", "`` Nelson's Sparrow ''", "Parachutes", "through phowa and siddhi consciously determined to be reborn, often many times, in order to continue their Bodhisattva vow is called a Tulku", "primary law, secondary law and supplementary law"], "metric_results": {"EM": 0.15625, "QA-F1": 0.25261752136752136}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true], "QA-F1": [0.22222222222222224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13333333333333336, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 1.0, 0.4, 0.0, 0.4, 0.0, 0.3076923076923077, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-3100", "mrqa_triviaqa-validation-6493", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-2148", "mrqa_triviaqa-validation-6063", "mrqa_hotpotqa-validation-3056", "mrqa_hotpotqa-validation-3282", "mrqa_squad-validation-3699", "mrqa_triviaqa-validation-7358", "mrqa_naturalquestions-validation-3092", "mrqa_naturalquestions-validation-7848", "mrqa_squad-validation-3570", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-1261", "mrqa_hotpotqa-validation-2640", "mrqa_squad-validation-3038", "mrqa_squad-validation-3318", "mrqa_triviaqa-validation-1100", "mrqa_triviaqa-validation-2009", "mrqa_hotpotqa-validation-2840", "mrqa_squad-validation-10182", "mrqa_triviaqa-validation-5904", "mrqa_squad-validation-4975", "mrqa_squad-validation-2223", "mrqa_naturalquestions-validation-3822", "mrqa_triviaqa-validation-6129", "mrqa_squad-validation-1930"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 81, "before_eval": {"predictions": ["one", "National Football Conference", "2005", "as an adviser", "USD 5.2 billion", "A rear - view mirror ( or rearview mirror )", "The Birds", "identify rock samples", "French & Saunders", "the European Parliament", "car crash", "on either February 28 or March 1", "Thomas Edison", "trio", "the Battle of Frankenhausen", "red hot poker", "a new entrance building", "maryland", "karl Liebknecht", "about a quarter (fifth) of a full barrel", "construction service firms", "valparaiso", "`` Feed Jake '' is a song written by Danny `` Bear '' Mayo, and recorded by the American country music band Pirates of the Mississippi", "several hundred thousand", "photoelectric effect", "Noel Gallagher", "Max West", "Northern Rail", "alumina", "thirty-seventh", "Lakshmibai", "medical techniques such as acupuncture, moxibustion, pulse diagnosis, and various herbal drugs and elixirs"], "metric_results": {"EM": 0.125, "QA-F1": 0.31778178418803416}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.375, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.5384615384615384, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.4, 0.8333333333333333]}}, "error_ids": ["mrqa_squad-validation-7", "mrqa_squad-validation-7728", "mrqa_squad-validation-2481", "mrqa_naturalquestions-validation-2776", "mrqa_naturalquestions-validation-8591", "mrqa_hotpotqa-validation-1949", "mrqa_squad-validation-5055", "mrqa_squad-validation-4541", "mrqa_triviaqa-validation-1724", "mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-5053", "mrqa_hotpotqa-validation-1701", "mrqa_squad-validation-2287", "mrqa_triviaqa-validation-4596", "mrqa_squad-validation-5447", "mrqa_triviaqa-validation-1305", "mrqa_triviaqa-validation-5598", "mrqa_triviaqa-validation-2632", "mrqa_squad-validation-6764", "mrqa_triviaqa-validation-1476", "mrqa_naturalquestions-validation-7838", "mrqa_squad-validation-914", "mrqa_hotpotqa-validation-3695", "mrqa_squad-validation-5288", "mrqa_triviaqa-validation-5741", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-1664", "mrqa_squad-validation-8265"], "retrieved_ids": ["mrqa_squad-validation-1", "mrqa_naturalquestions-train-25158", "mrqa_naturalquestions-train-21771", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-train-41954", "mrqa_hotpotqa-validation-5822", "mrqa_naturalquestions-train-2314", "mrqa_naturalquestions-train-87577", "mrqa_naturalquestions-train-8742", "mrqa_naturalquestions-train-29696", "mrqa_naturalquestions-train-46446", "mrqa_naturalquestions-train-58318", "mrqa_naturalquestions-train-71368", "mrqa_naturalquestions-train-26295", "mrqa_naturalquestions-train-69860", "mrqa_naturalquestions-train-38219", "mrqa_naturalquestions-train-29671", "mrqa_naturalquestions-train-16692", "mrqa_naturalquestions-train-66021", "mrqa_naturalquestions-train-31160", "mrqa_naturalquestions-train-9081", "mrqa_naturalquestions-train-22308", "mrqa_naturalquestions-train-61742", "mrqa_naturalquestions-train-16195", "mrqa_naturalquestions-train-63070", "mrqa_naturalquestions-train-58779", "mrqa_naturalquestions-train-81499", "mrqa_naturalquestions-train-72872", "mrqa_naturalquestions-train-68737", "mrqa_naturalquestions-train-9941", "mrqa_squad-validation-8811", "mrqa_naturalquestions-train-84677", "mrqa_naturalquestions-train-44589", "mrqa_squad-validation-1904", "mrqa_naturalquestions-train-33455", "mrqa_squad-validation-4169", "mrqa_squad-validation-4118", "mrqa_naturalquestions-train-79701", "mrqa_naturalquestions-train-70633", "mrqa_naturalquestions-train-1481", "mrqa_triviaqa-validation-1954", "mrqa_naturalquestions-train-86260", "mrqa_triviaqa-validation-4681", "mrqa_naturalquestions-train-25983", "mrqa_triviaqa-validation-5057", "mrqa_naturalquestions-train-33535", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-train-67513", "mrqa_naturalquestions-train-80618", "mrqa_naturalquestions-train-60679", "mrqa_naturalquestions-train-85041", "mrqa_naturalquestions-train-690", "mrqa_naturalquestions-train-83020", "mrqa_naturalquestions-train-40318", "mrqa_naturalquestions-train-51582", "mrqa_naturalquestions-train-77583", "mrqa_hotpotqa-validation-3870", "mrqa_naturalquestions-train-86095", "mrqa_naturalquestions-train-39276", "mrqa_naturalquestions-train-45720", "mrqa_naturalquestions-train-6539", "mrqa_hotpotqa-validation-4951", "mrqa_naturalquestions-train-11365", "mrqa_naturalquestions-train-29696", "mrqa_squad-validation-4773", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-train-85317", "mrqa_naturalquestions-train-28545", "mrqa_triviaqa-validation-5091", "mrqa_triviaqa-validation-1028", "mrqa_squad-validation-7836", "mrqa_naturalquestions-train-79698", "mrqa_naturalquestions-train-34340", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-44151", "mrqa_naturalquestions-train-33243", "mrqa_triviaqa-validation-314", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-455", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-6063", "mrqa_triviaqa-validation-1995", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-6373", "mrqa_triviaqa-validation-7605", "mrqa_triviaqa-validation-3395", "mrqa_triviaqa-validation-1015", "mrqa_triviaqa-validation-4827", "mrqa_naturalquestions-train-85027", "mrqa_naturalquestions-train-80228", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-train-31955", "mrqa_naturalquestions-train-7332", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-2959", "mrqa_hotpotqa-validation-4001", "mrqa_triviaqa-validation-5686", "mrqa_naturalquestions-train-80827", "mrqa_naturalquestions-train-23010", "mrqa_naturalquestions-train-17363", "mrqa_naturalquestions-train-75644", "mrqa_naturalquestions-train-70099", "mrqa_naturalquestions-train-78254", "mrqa_squad-validation-3018", "mrqa_naturalquestions-train-1807", "mrqa_naturalquestions-train-35201", "mrqa_naturalquestions-train-44113", "mrqa_squad-validation-9984", "mrqa_naturalquestions-train-84933", "mrqa_naturalquestions-train-52189", "mrqa_naturalquestions-train-6576", "mrqa_naturalquestions-train-855", "mrqa_naturalquestions-train-60220", "mrqa_naturalquestions-train-38786", "mrqa_naturalquestions-train-18920", "mrqa_naturalquestions-train-38329", "mrqa_naturalquestions-train-60814", "mrqa_naturalquestions-train-60931", "mrqa_triviaqa-validation-6740", "mrqa_triviaqa-validation-1578", "mrqa_naturalquestions-train-57147", "mrqa_naturalquestions-train-68269", "mrqa_triviaqa-validation-4402", "mrqa_squad-validation-3812", "mrqa_hotpotqa-validation-4624", "mrqa_squad-validation-6369", "mrqa_naturalquestions-train-83407", "mrqa_naturalquestions-train-49194", "mrqa_naturalquestions-train-67977", "mrqa_naturalquestions-train-27665", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-train-15740", "mrqa_naturalquestions-train-84681", "mrqa_naturalquestions-train-36961", "mrqa_naturalquestions-train-77270", "mrqa_naturalquestions-validation-9064", "mrqa_squad-validation-6328", "mrqa_naturalquestions-train-22654"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 82, "before_eval": {"predictions": ["john Morrison", "normal force", "legs", "24 hours later", "April 20, 1945", "dachshund", "A consortium led by the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT)", "geologic", "129", "Boutique hotel", "Puma", "an official school sport", "geese", "Jamukha", "tentacles and prey", "tumbo", "Tagore", "Lawrence County State's Attorney John Fitzgerald, Chief Deputy Attorney General Charlie McGuigan, and attorney and 2014 U.S. Senate candidate Jason Ravnsborg", "water on the ground surface enters the soil", "9", "film and short novels", "American-Canadian mystery-drama television series", "Arrested Development", "Gardnerville", "three subunit molecules : a nitrogenous base, a five - carbon sugar ( ribose or deoxyribose ), and at least one phosphate group", "25-minute", "Julian Paul Julian", "Zapatista army of national Liberation", "Vikram Bhatt, Bhushan Patel and Tinu Suresh Desai", "The Merchant of Venice", "2003", "127"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3650904052843708}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.8, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.2857142857142857, 0.5, 0.0, 1.0, 0.5517241379310345, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5454545454545454, 0.0, 0.33333333333333337, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2133", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-215", "mrqa_hotpotqa-validation-4153", "mrqa_hotpotqa-validation-4420", "mrqa_squad-validation-8322", "mrqa_squad-validation-2791", "mrqa_squad-validation-9603", "mrqa_naturalquestions-validation-2347", "mrqa_triviaqa-validation-1053", "mrqa_squad-validation-6113", "mrqa_squad-validation-4616", "mrqa_triviaqa-validation-1685", "mrqa_naturalquestions-validation-8544", "mrqa_naturalquestions-validation-746", "mrqa_triviaqa-validation-863", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2361", "mrqa_naturalquestions-validation-9324", "mrqa_hotpotqa-validation-1639", "mrqa_triviaqa-validation-2236", "mrqa_hotpotqa-validation-164", "mrqa_triviaqa-validation-2592", "mrqa_naturalquestions-validation-9117", "mrqa_triviaqa-validation-7419"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 83, "before_eval": {"predictions": ["alberto Aleandro Uderzo", "North Africa", "a young girl", "architecture from the gothic, renaissance, baroque and neoclassical periods", "the ground", "the aboral organ (at the opposite end from the mouth)", "bird", "Bob Cratchit", "6", "Berlin", "Maidstone", "kingdoms of Francia on the Lower Rhine, Burgundy on the Upper Rhine and Alemannia", "Sultan Selim II", "Characters 1 -- 3 ( the category of disease )", "Titanic", "difficult and intricate topics", "email", "ABC Radio, with Clear Channel Communications and Westwood One ( which had earlier purchased NBC's radio division, as well as the distribution rights to CBS's, and the Mutual Broadcasting System during the 1990s)", "the coasts of Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "detritus from the settlement of the sedimentation", "Rated R", "Matthew 14:22-36", "Steveston Outdoor pool in Richmond, BC", "Nikola Tesla", "Terry the Tomboy", "red", "Australia", "Mach number", "pit road speed", "North Atlantic Conference", "at the group's final British performance on 14 July 2012 at the National Bowl in Milton Keynes", "bbc"], "metric_results": {"EM": 0.21875, "QA-F1": 0.31278565744291553}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, true, false, true, false, true, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.4, 0.15384615384615385, 0.0, 0.0, 1.0, 0.0, 0.0, 0.12903225806451613, 1.0, 0.33333333333333337, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8484848484848485, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3493", "mrqa_squad-validation-9912", "mrqa_naturalquestions-validation-1805", "mrqa_squad-validation-920", "mrqa_naturalquestions-validation-6856", "mrqa_squad-validation-4490", "mrqa_triviaqa-validation-3113", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-7486", "mrqa_hotpotqa-validation-2221", "mrqa_squad-validation-3107", "mrqa_squad-validation-9349", "mrqa_hotpotqa-validation-1312", "mrqa_naturalquestions-validation-5214", "mrqa_hotpotqa-validation-2964", "mrqa_triviaqa-validation-90", "mrqa_squad-validation-5739", "mrqa_naturalquestions-validation-1798", "mrqa_triviaqa-validation-3987", "mrqa_naturalquestions-validation-7172", "mrqa_squad-validation-1159", "mrqa_triviaqa-validation-570", "mrqa_hotpotqa-validation-4102", "mrqa_naturalquestions-validation-8518", "mrqa_triviaqa-validation-5793"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 84, "before_eval": {"predictions": ["music", "13 years and 48 days", "Major General Nathan Bedford Forrest", "Theodosius I", "oppidum Ubiorum", "Montreal", "brixton", "60", "Assistant Secretary for Administration and Management", "better fuel economy", "guitar", "evacuate the cylinder, choking it and giving excessive compression", "tree", "optic disc", "Carson City", "biologist", "Egyptians", "m Marilyn Monroe", "Poems : Series 1", "United States ambassador to Ghana and to Czechoslovakia and also served as Chief of Protocol of the United States", "Warriors", "Emma Thompson and Alice Eve", "Chinese", "christ", "travel literature, cartography, geography, and scientific education", "King of Poland and Grand Duke of Lithuania", "King Richard the Lionheart, Henry III, and Edward I", "chiang Kai-Shek", "57 kg", "trunk", "Lexus", "as part of the Norman Conquest of England"], "metric_results": {"EM": 0.25, "QA-F1": 0.41905753968253967}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.2222222222222222, 0.8571428571428571, 0.0, 0.4, 0.0, 0.5, 1.0, 1.0, 1.0, 0.8, 0.24000000000000002, 0.2857142857142857, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2]}}, "error_ids": ["mrqa_squad-validation-370", "mrqa_naturalquestions-validation-1147", "mrqa_squad-validation-9298", "mrqa_naturalquestions-validation-6794", "mrqa_triviaqa-validation-2014", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-1628", "mrqa_hotpotqa-validation-4414", "mrqa_triviaqa-validation-5687", "mrqa_squad-validation-3364", "mrqa_triviaqa-validation-1529", "mrqa_naturalquestions-validation-3368", "mrqa_triviaqa-validation-3538", "mrqa_naturalquestions-validation-10461", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-661", "mrqa_naturalquestions-validation-732", "mrqa_naturalquestions-validation-8660", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-1645", "mrqa_hotpotqa-validation-989", "mrqa_triviaqa-validation-159", "mrqa_hotpotqa-validation-1810", "mrqa_naturalquestions-validation-6660"], "retrieved_ids": ["mrqa_squad-validation-361", "mrqa_naturalquestions-train-43841", "mrqa_naturalquestions-train-25617", "mrqa_naturalquestions-train-44051", "mrqa_naturalquestions-train-64300", "mrqa_naturalquestions-train-57237", "mrqa_naturalquestions-train-14373", "mrqa_naturalquestions-train-22760", "mrqa_naturalquestions-train-11403", "mrqa_naturalquestions-train-2005", "mrqa_squad-validation-9296", "mrqa_naturalquestions-train-47470", "mrqa_naturalquestions-train-49152", "mrqa_naturalquestions-train-87260", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-32795", "mrqa_naturalquestions-train-28391", "mrqa_naturalquestions-train-36503", "mrqa_naturalquestions-train-40549", "mrqa_naturalquestions-train-26844", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-train-17468", "mrqa_naturalquestions-train-33306", "mrqa_squad-validation-3181", "mrqa_hotpotqa-validation-5822", "mrqa_naturalquestions-train-47045", "mrqa_naturalquestions-train-44716", "mrqa_naturalquestions-train-52296", "mrqa_naturalquestions-train-40409", "mrqa_naturalquestions-train-36951", "mrqa_hotpotqa-validation-389", "mrqa_naturalquestions-train-11208", "mrqa_naturalquestions-train-55413", "mrqa_naturalquestions-train-34003", "mrqa_squad-validation-7836", "mrqa_naturalquestions-train-25717", "mrqa_naturalquestions-train-17684", "mrqa_naturalquestions-train-59078", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-3232", "mrqa_triviaqa-validation-5813", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-3324", "mrqa_hotpotqa-validation-2449", "mrqa_naturalquestions-train-2612", "mrqa_squad-validation-10148", "mrqa_naturalquestions-train-26729", "mrqa_naturalquestions-train-46587", "mrqa_squad-validation-9984", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-5237", "mrqa_triviaqa-validation-3113", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-3876", "mrqa_naturalquestions-train-2176", "mrqa_naturalquestions-train-9637", "mrqa_naturalquestions-train-86071", "mrqa_naturalquestions-train-66570", "mrqa_naturalquestions-train-79403", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-6639", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-4383", "mrqa_naturalquestions-train-10038", "mrqa_naturalquestions-train-5078", "mrqa_naturalquestions-train-42135", "mrqa_naturalquestions-train-39439", "mrqa_naturalquestions-validation-4098", "mrqa_naturalquestions-train-28320", "mrqa_naturalquestions-train-59681", "mrqa_hotpotqa-validation-452", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-20198", "mrqa_naturalquestions-train-76196", "mrqa_hotpotqa-validation-118", "mrqa_hotpotqa-validation-4153", "mrqa_hotpotqa-validation-1660", "mrqa_naturalquestions-train-21987", "mrqa_naturalquestions-train-53006", "mrqa_naturalquestions-train-34944", "mrqa_naturalquestions-train-85623", "mrqa_naturalquestions-train-59856", "mrqa_naturalquestions-train-9741", "mrqa_naturalquestions-train-12896", "mrqa_naturalquestions-train-74251", "mrqa_naturalquestions-train-80201", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-train-5078", "mrqa_naturalquestions-train-2851", "mrqa_hotpotqa-validation-1850", "mrqa_naturalquestions-train-10694", "mrqa_naturalquestions-validation-10676", "mrqa_triviaqa-validation-4298", "mrqa_hotpotqa-validation-1975", "mrqa_hotpotqa-validation-2588", "mrqa_naturalquestions-train-16951", "mrqa_naturalquestions-train-45753", "mrqa_naturalquestions-train-69860", "mrqa_naturalquestions-train-7712", "mrqa_naturalquestions-train-80555", "mrqa_naturalquestions-train-77003", "mrqa_naturalquestions-train-21697", "mrqa_naturalquestions-train-41936", "mrqa_triviaqa-validation-4486", "mrqa_triviaqa-validation-2236", "mrqa_triviaqa-validation-2938", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-595", "mrqa_naturalquestions-train-41656", "mrqa_naturalquestions-train-66915", "mrqa_naturalquestions-train-68187", "mrqa_naturalquestions-train-30062", "mrqa_naturalquestions-train-19252", "mrqa_naturalquestions-train-46370", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-train-77481", "mrqa_naturalquestions-train-63308", "mrqa_naturalquestions-train-811"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 85, "before_eval": {"predictions": ["South Asia", "about 10,000", "Stephen Crawford Young", "the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere ), when the duration of daylight becomes noticeably shorter and the temperature cools down considerably", "3000 metres", "October 13, 1980", "1765", "kill", "heart transplant", "to the right of the central plate", "Yeats", "bilaterally symmetrical ( Bilateria )", "he did not consider the papacy part of the biblical Church because historistical interpretation of Bible prophecy concluded that the Papacy was the Antichrist", "1889", "Yen Press", "Bambi, a Life in the Woods", "point on the frontier indicates efficient use of the available inputs ( such as points B, D and C in the graph )", "from ages 12\u201318", "the Apple A6X chip and the Lightning connector", "on the equator and overlies the East African Rift covering a diverse and expansive terrain that extends roughly from Lake Victoria to Lake Turkana ( formerly called Lake Rudolf) and further south-east to the Indian Ocean", "in school", "guinea", "elizabeth bennet", "lactobacilli", "a school or other place of formal education", "a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "Anmer", "Ben Johnston", "EE", "1970s", "mexican driver Roland Ratzenberger", "200 to 500 mg up to 7 ml"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2294368961352657}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2608695652173913, 0.0, 0.0, 0.2, 0.32, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.25, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2]}}, "error_ids": ["mrqa_naturalquestions-validation-10601", "mrqa_squad-validation-2529", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-3515", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-7425", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-7767", "mrqa_squad-validation-2267", "mrqa_hotpotqa-validation-3220", "mrqa_naturalquestions-validation-7513", "mrqa_hotpotqa-validation-1506", "mrqa_naturalquestions-validation-2883", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2157", "mrqa_squad-validation-8266", "mrqa_squad-validation-1941", "mrqa_triviaqa-validation-5150", "mrqa_triviaqa-validation-4988", "mrqa_squad-validation-1891", "mrqa_naturalquestions-validation-9772", "mrqa_triviaqa-validation-4854", "mrqa_squad-validation-1566", "mrqa_triviaqa-validation-2508", "mrqa_naturalquestions-validation-2907", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-8555"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 86, "before_eval": {"predictions": ["Spanish", "thammasat", "South Sentinel Island", "FCA US LLC", "Aaron Taylor-Johnson", "Beyonce", "san Francisco", "a gift", "Conservative", "Peter Davison, Colin Baker and Sylvester McCoy", "preached that Muslims should not only \" always oppose\" infidels \"in every way\" but \" hate them for their religion... for Allah's sake\" that democracy \"is responsible for all the horrible wars of the 20th century", "Super Bowl XXVIII at the end of the 1993 season", "maquiladora", "Rich Girl", "in human and animals as a short - lived product in biochemical processes", "\"1989\"", "problem instance", "1787", "WJRT-TV", "Hudson River", "at or above the maximum rates proposed during the last IPCC report in 2001", "Judaism", "film playback singer, director, writer and producer", "southern whites", "Marie", "L", "Association of Commonwealth Universities (ACU)", "drawing letters in the air ( `` penciling '' )", "string theory", "All Stars", "he stood by their contents", "1980"], "metric_results": {"EM": 0.21875, "QA-F1": 0.34935616419991417}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.8750000000000001, 0.2702702702702703, 0.2222222222222222, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.09523809523809525, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-4707", "mrqa_hotpotqa-validation-4230", "mrqa_hotpotqa-validation-5610", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-734", "mrqa_squad-validation-7281", "mrqa_squad-validation-7679", "mrqa_squad-validation-9589", "mrqa_squad-validation-782", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3704", "mrqa_hotpotqa-validation-4401", "mrqa_squad-validation-1768", "mrqa_naturalquestions-validation-9878", "mrqa_squad-validation-5911", "mrqa_triviaqa-validation-5001", "mrqa_squad-validation-8531", "mrqa_squad-validation-2331", "mrqa_hotpotqa-validation-367", "mrqa_naturalquestions-validation-9516", "mrqa_hotpotqa-validation-5241", "mrqa_naturalquestions-validation-3323", "mrqa_squad-validation-10506", "mrqa_triviaqa-validation-1431", "mrqa_squad-validation-2113"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 87, "before_eval": {"predictions": ["First Street in downtown Dayton, Ohio, United States", "electronic gaming machines, table games, i Gaming and iLottery products, instant lottery games, lottery gaming systems, terminals and services, internet applications, server-based interactive gambling terminals, and gambling control systems", "August 21, 1995", "the last Shah of Iran", "digital streams", "Life Savers", "Burbank, California", "Mongolian, Tibetan, and Chinese", "Due to the controversial and explicit nature of many of their songs, the band has frequently dealt with their videos being taken down off YouTube", "Meghan Trainor", "carbon cycle", "tenant management", "the feather of truth", "Donna Noble (Catherine Tate) with Mickey Smith (Noel Clarke) and Jack Harkness ( John Barrowman) recurring as secondary companion figures", "salvaging a country usually seen as one of the most stable and prosperous in Africa", "Kree", "titanium metal", "Loud", "Kirenyaa", "margaret smith", "domestic legislation", "actions-oriented", "Kaiser Wilhelm II", "the Senate is composed of senators, each of whom represents a single state in its entirety, with each state being equally represented by two senators, regardless of its population, serving staggered terms of six years", "apple wine", "Thutmose III", "two", "Buck Owens", "Kansas", "political support", "Finding Nemo", "all England tennis club"], "metric_results": {"EM": 0.3125, "QA-F1": 0.4788493116079323}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, false], "QA-F1": [0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.06896551724137931, 0.0, 1.0, 0.5, 0.8571428571428571, 0.6428571428571429, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.0, 0.5714285714285715, 1.0, 0.0, 0.05128205128205128, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571]}}, "error_ids": ["mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-3420", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-6012", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-3713", "mrqa_squad-validation-7703", "mrqa_squad-validation-8386", "mrqa_hotpotqa-validation-3692", "mrqa_triviaqa-validation-7649", "mrqa_squad-validation-8276", "mrqa_triviaqa-validation-6049", "mrqa_squad-validation-9504", "mrqa_triviaqa-validation-3943", "mrqa_naturalquestions-validation-3848", "mrqa_triviaqa-validation-2276", "mrqa_hotpotqa-validation-1282", "mrqa_squad-validation-8552", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-2814"], "retrieved_ids": ["mrqa_naturalquestions-train-24246", "mrqa_naturalquestions-train-11217", "mrqa_squad-validation-7836", "mrqa_naturalquestions-train-30622", "mrqa_naturalquestions-train-47895", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-4415", "mrqa_naturalquestions-train-811", "mrqa_naturalquestions-train-23029", "mrqa_hotpotqa-validation-1606", "mrqa_naturalquestions-train-53870", "mrqa_naturalquestions-train-24946", "mrqa_naturalquestions-train-54157", "mrqa_naturalquestions-train-57406", "mrqa_naturalquestions-train-50773", "mrqa_naturalquestions-train-82501", "mrqa_naturalquestions-train-21281", "mrqa_naturalquestions-train-26912", "mrqa_naturalquestions-train-23912", "mrqa_naturalquestions-train-38860", "mrqa_naturalquestions-train-87571", "mrqa_naturalquestions-train-25986", "mrqa_naturalquestions-train-11357", "mrqa_naturalquestions-train-53606", "mrqa_naturalquestions-train-75556", "mrqa_hotpotqa-validation-5822", "mrqa_hotpotqa-validation-3976", "mrqa_naturalquestions-validation-6326", "mrqa_hotpotqa-validation-1390", "mrqa_naturalquestions-train-79091", "mrqa_naturalquestions-train-19223", "mrqa_naturalquestions-train-57692", "mrqa_naturalquestions-train-74582", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-85923", "mrqa_naturalquestions-train-49152", "mrqa_squad-validation-8185", "mrqa_naturalquestions-train-41564", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-47470", "mrqa_naturalquestions-train-5636", "mrqa_naturalquestions-validation-9227", "mrqa_naturalquestions-train-67764", "mrqa_naturalquestions-train-54644", "mrqa_naturalquestions-train-59266", "mrqa_naturalquestions-train-6660", "mrqa_naturalquestions-train-29021", "mrqa_naturalquestions-train-43281", "mrqa_naturalquestions-train-80987", "mrqa_naturalquestions-train-78201", "mrqa_naturalquestions-train-27042", "mrqa_naturalquestions-train-49469", "mrqa_naturalquestions-train-7402", "mrqa_naturalquestions-train-72723", "mrqa_naturalquestions-train-59801", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-6740", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-177", "mrqa_naturalquestions-train-21281", "mrqa_naturalquestions-train-47294", "mrqa_naturalquestions-train-5911", "mrqa_naturalquestions-train-11406", "mrqa_naturalquestions-train-63956", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-6896", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-564", "mrqa_triviaqa-validation-3515", "mrqa_naturalquestions-train-21283", "mrqa_naturalquestions-validation-570", "mrqa_naturalquestions-train-52318", "mrqa_naturalquestions-train-46650", "mrqa_naturalquestions-train-36626", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-3493", "mrqa_triviaqa-validation-3664", "mrqa_triviaqa-validation-671", "mrqa_triviaqa-validation-2635", "mrqa_naturalquestions-train-20644", "mrqa_naturalquestions-train-20198", "mrqa_naturalquestions-train-79462", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-train-71101", "mrqa_triviaqa-validation-6373", "mrqa_naturalquestions-train-51238", "mrqa_naturalquestions-train-63821", "mrqa_triviaqa-validation-2130", "mrqa_hotpotqa-validation-3232", "mrqa_squad-validation-5360", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-77486", "mrqa_naturalquestions-train-73434", "mrqa_naturalquestions-train-27893", "mrqa_naturalquestions-train-40474", "mrqa_naturalquestions-train-65943", "mrqa_naturalquestions-train-12896", "mrqa_naturalquestions-train-31830", "mrqa_naturalquestions-validation-5312", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-1453", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-5057", "mrqa_triviaqa-validation-6689", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5785", "mrqa_naturalquestions-train-8126"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 88, "before_eval": {"predictions": ["older brother, 2006 FIFA World Cup and Ballon d'Or winner Fabio Cannavaro, who was also a defender", "Janet Jackson", "The Gold Coast", "to create a test case as to the constitutionality of a law", "Selena Gomez", "8th", "Forbes", "3 -- 4 years : Tachycardia > 137 bpm", "small intestine", "guitarists", "The South African Schools Act of 1996", "Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "blue dare", "fictional character", "the southwestern United States, Mexico, and Central America", "a financial self- interest in \"diagnosing\" as many conditions as possible, and in exaggerating their seriousness", "sacerdotalism", "not being profitable", "an estimated half a million acres", "44 (or virtual channel 6 via PSIP)", "the Gaulish name R\u0113nos, which was adapted in Roman-era geography (1st century BC) as Greek \u1fec\u1fc6\u03bd\u03bf\u03c2 ( Rh\u0113nos) Latin Rhenus", "the financing activities section", "duke of galilee", "gloucestershire", "Solange Knowles & Destiny's Child", "Ward", "Kevin Michael Richardson", "Eddie Gottlieb Trophy", "Rihanna", "Marmakhel Tribe", "reversed", "phycobilisomes"], "metric_results": {"EM": 0.15625, "QA-F1": 0.3171437324929972}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true], "QA-F1": [0.11764705882352941, 0.6666666666666666, 0.6666666666666666, 0.13333333333333333, 0.0, 1.0, 0.4, 0.2857142857142857, 0.0, 0.0, 0.8, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.25, 0.2857142857142857, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2848", "mrqa_triviaqa-validation-6166", "mrqa_hotpotqa-validation-3930", "mrqa_squad-validation-6775", "mrqa_naturalquestions-validation-5785", "mrqa_hotpotqa-validation-3343", "mrqa_naturalquestions-validation-10162", "mrqa_triviaqa-validation-2784", "mrqa_triviaqa-validation-6636", "mrqa_squad-validation-6823", "mrqa_naturalquestions-validation-4351", "mrqa_triviaqa-validation-4242", "mrqa_naturalquestions-validation-8319", "mrqa_squad-validation-6396", "mrqa_squad-validation-2101", "mrqa_naturalquestions-validation-8965", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-4843", "mrqa_squad-validation-9246", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4364", "mrqa_naturalquestions-validation-2264", "mrqa_squad-validation-843", "mrqa_naturalquestions-validation-8622", "mrqa_hotpotqa-validation-5498", "mrqa_naturalquestions-validation-8095", "mrqa_hotpotqa-validation-828"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 89, "before_eval": {"predictions": ["13", "conscientious lawbreakers must be punished", "House of Fraser", "Bonkyll Castle", "veal stock", "time complexity", "Deadman's Gun", "Costa del Sol", "Philippines", "filaments", "david london", "1938", "the referendum in the Netherlands", "henry", "middle America", "Louis XVIII", "1930", "jastarnia", "swissair", "all U.S. territories except American Samoa", "the Gaussian integers Z[i]", "Janis Joplin", "Odinga", "homicides", "shropshire", "Bergen County", "August 9, 2017", "kenya", "microsociology and social psychology", "USC Trojans", "Ian Hart", "proteases"], "metric_results": {"EM": 0.28125, "QA-F1": 0.39207589285714284}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.6, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.37499999999999994, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-6773", "mrqa_hotpotqa-validation-1756", "mrqa_triviaqa-validation-3639", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-7598", "mrqa_squad-validation-8898", "mrqa_triviaqa-validation-5020", "mrqa_hotpotqa-validation-2774", "mrqa_squad-validation-3985", "mrqa_triviaqa-validation-603", "mrqa_naturalquestions-validation-3269", "mrqa_hotpotqa-validation-5484", "mrqa_triviaqa-validation-615", "mrqa_triviaqa-validation-7020", "mrqa_hotpotqa-validation-2831", "mrqa_naturalquestions-validation-9419", "mrqa_triviaqa-validation-5430", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-4515", "mrqa_triviaqa-validation-3643", "mrqa_naturalquestions-validation-1450", "mrqa_squad-validation-2793", "mrqa_squad-validation-6645"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 90, "before_eval": {"predictions": ["a very reactive allotrope of oxygen that is damaging to lung tissue", "the splitting of O2 by ultraviolet (UV) radiation", "the Doctor battling a rogue Time Lord called The Master", "opera", "phylum", "younger", "magnetic field", "complexity classes", "specific terminology has no more (or no less) meaning than the individual orator intends it to have", "clef", "four", "red hot peppers", "a self-referential time-related adage, coined by Douglas Hofstadter and named after him", "The Book of Roger", "Gravitational force", "photolysis", "fall of 2015", "raoul Walsh", "G. Callen", "antibody-dependent (or cytotoxic) hypersensitivity", "rootlets ( branch roots )", "v Violet Newstead", "Jeanne Tripplehorn as Jacqueline Kennedy (Little Edie's cousin) and Ken Howard as Phelan Beale", "red", "New England Patriots", "antigenic variation", "A74(M)", "Absheron peninsula", "september", "Morty", "Charlotte Louise Riley", "Vivienne Westwood costumes"], "metric_results": {"EM": 0.09375, "QA-F1": 0.1652529761904762}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.16666666666666669, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.28571428571428575, 0.28571428571428575, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8]}}, "error_ids": ["mrqa_squad-validation-3496", "mrqa_squad-validation-3497", "mrqa_squad-validation-7657", "mrqa_hotpotqa-validation-2585", "mrqa_squad-validation-4421", "mrqa_triviaqa-validation-6726", "mrqa_squad-validation-6812", "mrqa_triviaqa-validation-2940", "mrqa_naturalquestions-validation-7034", "mrqa_triviaqa-validation-5", "mrqa_hotpotqa-validation-4809", "mrqa_squad-validation-1070", "mrqa_naturalquestions-validation-2838", "mrqa_squad-validation-5814", "mrqa_triviaqa-validation-1246", "mrqa_naturalquestions-validation-3233", "mrqa_squad-validation-6884", "mrqa_naturalquestions-validation-1704", "mrqa_triviaqa-validation-3988", "mrqa_hotpotqa-validation-3250", "mrqa_triviaqa-validation-7407", "mrqa_squad-validation-259", "mrqa_squad-validation-6627", "mrqa_hotpotqa-validation-2128", "mrqa_triviaqa-validation-3575", "mrqa_triviaqa-validation-810", "mrqa_naturalquestions-validation-6248", "mrqa_hotpotqa-validation-5274", "mrqa_squad-validation-5441"], "retrieved_ids": ["mrqa_naturalquestions-train-56337", "mrqa_naturalquestions-train-42632", "mrqa_naturalquestions-train-23704", "mrqa_naturalquestions-train-14757", "mrqa_naturalquestions-train-39106", "mrqa_squad-validation-3478", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-58293", "mrqa_naturalquestions-train-76391", "mrqa_naturalquestions-train-30979", "mrqa_naturalquestions-train-4988", "mrqa_hotpotqa-validation-2750", "mrqa_naturalquestions-train-64511", "mrqa_naturalquestions-train-34340", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-85910", "mrqa_naturalquestions-train-1669", "mrqa_naturalquestions-train-43519", "mrqa_naturalquestions-train-26912", "mrqa_naturalquestions-train-17116", "mrqa_naturalquestions-train-44690", "mrqa_naturalquestions-train-16982", "mrqa_naturalquestions-train-74056", "mrqa_naturalquestions-train-68254", "mrqa_naturalquestions-train-86880", "mrqa_triviaqa-validation-6053", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-train-54223", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-train-42291", "mrqa_naturalquestions-train-78463", "mrqa_naturalquestions-train-68221", "mrqa_naturalquestions-train-63488", "mrqa_naturalquestions-train-17622", "mrqa_triviaqa-validation-4829", "mrqa_triviaqa-validation-5237", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-2331", "mrqa_naturalquestions-train-82495", "mrqa_squad-validation-4019", "mrqa_naturalquestions-train-85233", "mrqa_naturalquestions-train-27763", "mrqa_naturalquestions-train-8060", "mrqa_triviaqa-validation-1551", "mrqa_naturalquestions-train-18524", "mrqa_naturalquestions-train-51833", "mrqa_triviaqa-validation-11", "mrqa_naturalquestions-train-24206", "mrqa_hotpotqa-validation-4130", "mrqa_naturalquestions-train-34003", "mrqa_naturalquestions-train-71615", "mrqa_naturalquestions-train-42632", "mrqa_naturalquestions-train-80228", "mrqa_naturalquestions-train-19019", "mrqa_naturalquestions-train-46447", "mrqa_naturalquestions-train-79133", "mrqa_naturalquestions-train-60651", "mrqa_naturalquestions-train-45761", "mrqa_naturalquestions-train-9107", "mrqa_squad-validation-8924", "mrqa_naturalquestions-train-52050", "mrqa_naturalquestions-train-74251", "mrqa_naturalquestions-train-68815", "mrqa_naturalquestions-train-36707", "mrqa_naturalquestions-train-66434", "mrqa_naturalquestions-train-65346", "mrqa_naturalquestions-train-44839", "mrqa_naturalquestions-train-70582", "mrqa_triviaqa-validation-6063", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-6478", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3080", "mrqa_naturalquestions-train-86861", "mrqa_naturalquestions-train-1915", "mrqa_naturalquestions-train-24079", "mrqa_naturalquestions-train-57524", "mrqa_naturalquestions-train-58080", "mrqa_naturalquestions-train-58120", "mrqa_naturalquestions-train-2544", "mrqa_naturalquestions-train-65495", "mrqa_hotpotqa-validation-1558", "mrqa_naturalquestions-train-76688", "mrqa_naturalquestions-train-37544", "mrqa_naturalquestions-train-414", "mrqa_naturalquestions-train-36117", "mrqa_naturalquestions-train-68867", "mrqa_naturalquestions-train-6387", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-6639", "mrqa_triviaqa-validation-1995", "mrqa_naturalquestions-train-35306", "mrqa_naturalquestions-train-1931", "mrqa_hotpotqa-validation-1782", "mrqa_naturalquestions-train-51833", "mrqa_naturalquestions-train-23413", "mrqa_naturalquestions-validation-142", "mrqa_triviaqa-validation-2959", "mrqa_naturalquestions-train-24615", "mrqa_triviaqa-validation-11", "mrqa_naturalquestions-train-54058", "mrqa_naturalquestions-train-8443", "mrqa_naturalquestions-train-52902", "mrqa_naturalquestions-train-12011", "mrqa_naturalquestions-train-18317", "mrqa_naturalquestions-train-38668", "mrqa_naturalquestions-train-39918", "mrqa_naturalquestions-train-31726", "mrqa_naturalquestions-train-38589", "mrqa_naturalquestions-train-9637", "mrqa_naturalquestions-train-47971", "mrqa_squad-validation-9319", "mrqa_naturalquestions-train-64931", "mrqa_naturalquestions-train-47600", "mrqa_naturalquestions-train-49847", "mrqa_naturalquestions-train-37539", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-1162", "mrqa_naturalquestions-train-48091", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-6639", "mrqa_triviaqa-validation-4890", "mrqa_triviaqa-validation-3945", "mrqa_triviaqa-validation-2849", "mrqa_naturalquestions-train-35002", "mrqa_naturalquestions-train-67693", "mrqa_naturalquestions-train-33446", "mrqa_naturalquestions-train-38329", "mrqa_naturalquestions-train-51833", "mrqa_hotpotqa-validation-983", "mrqa_hotpotqa-validation-2382", "mrqa_naturalquestions-train-35355", "mrqa_naturalquestions-train-62065", "mrqa_hotpotqa-validation-3634", "mrqa_naturalquestions-train-34229", "mrqa_naturalquestions-train-69879", "mrqa_naturalquestions-train-78708", "mrqa_naturalquestions-train-70875", "mrqa_naturalquestions-train-21076"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 91, "before_eval": {"predictions": ["40%", "Catherine", "Mongolian patrimonial feudalism", "Aldwych", "Afghanistan", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Australian", "around 100,000 writes", "king philip II", "melville", "tuscaloosa", "North Greenwich Arena", "ovules", "Masters Tournament", "Chiba, Japan", "90%", "Bessemer process", "communication", "1856", "Hepatocytes", "many deities and spirits, including the belief that spirits are found in non-human beings and objects such as animals, the waves, and the sky", "stony creek granite", "Belfast and elsewhere in the United Kingdom, Canada, Croatia, Iceland, Malta, Morocco, Spain, and the United States", "gregillaz", "corvids", "South Africa", "Four Weddings and a Funeral", "low-skilled workers", "at the center of the Northern Hemisphere", "Pakistan", "psilocybin, psilocin and baeocystin", "robert de niro"], "metric_results": {"EM": 0.21875, "QA-F1": 0.33051329402728835}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.47058823529411764, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.967741935483871, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.4, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-4749", "mrqa_squad-validation-8411", "mrqa_triviaqa-validation-7698", "mrqa_squad-validation-9738", "mrqa_naturalquestions-validation-4471", "mrqa_hotpotqa-validation-2205", "mrqa_naturalquestions-validation-2146", "mrqa_triviaqa-validation-3055", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-888", "mrqa_naturalquestions-validation-2439", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1140", "mrqa_squad-validation-8577", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-1336", "mrqa_hotpotqa-validation-1068", "mrqa_triviaqa-validation-5143", "mrqa_naturalquestions-validation-7799", "mrqa_triviaqa-validation-6126", "mrqa_triviaqa-validation-4353", "mrqa_squad-validation-7538", "mrqa_naturalquestions-validation-2721", "mrqa_hotpotqa-validation-277", "mrqa_hotpotqa-validation-1473"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 92, "before_eval": {"predictions": ["a field in Somerset County, Pennsylvania", "communism", "\"the Gentle Don\" or \"the Docile Don\"", "the United States Congress declared war ( Public Law 77 - 328, 55 STAT 795 ) on the Empire of Japan in response to that country's surprise attack on Pearl Harbor the prior day", "pacific", "July 25, 1898", "Bit Instant", "\"Stuck in the Suburbs\"", "florida", "farm tractors, motorcycles ( without much success) and even automobiles as the Stanley Steamer", "Social Chapter", "Australia", "retina", "the physics", "The Adventures of Ozzie and Harriet", "Sargent Shriver Jr.", "Landry's, Inc.", "James Knox Polk", "David Naughton, Jenny Agutter and Griffin Dunne", "national defence Volunteers", "Chicago", "london", "1978", "inversely", "propeller", "Irvine", "Marine Corps Air Station Kaneohe Bay", "Teen Titans Go!", "rugby union", "November 20, 1942", "Hugues Capet, king of France", "Morning Edition"], "metric_results": {"EM": 0.34375, "QA-F1": 0.41484374999999996}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true, false, true], "QA-F1": [0.33333333333333337, 0.0, 1.0, 0.25, 0.0, 0.5, 0.0, 0.0, 0.0, 0.125, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-4163", "mrqa_triviaqa-validation-3064", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-3366", "mrqa_naturalquestions-validation-3820", "mrqa_hotpotqa-validation-5281", "mrqa_hotpotqa-validation-2474", "mrqa_triviaqa-validation-5888", "mrqa_squad-validation-3276", "mrqa_naturalquestions-validation-3316", "mrqa_squad-validation-10388", "mrqa_squad-validation-5811", "mrqa_hotpotqa-validation-5895", "mrqa_naturalquestions-validation-10080", "mrqa_hotpotqa-validation-2914", "mrqa_triviaqa-validation-6960", "mrqa_triviaqa-validation-7185", "mrqa_triviaqa-validation-980", "mrqa_squad-validation-2696", "mrqa_triviaqa-validation-1267", "mrqa_squad-validation-3189"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 93, "before_eval": {"predictions": ["pacific region", "a method which pre-allocates dedicated network bandwidth specifically for each communication session, each having a constant bit rate and latency between nodes", "district line", "nearly 100 locations across Utah, including : Mount Timpanogos, Ashley National Forest, Leeds, Snow Canyon State Park, St. George, Sundance Resort, Uinta National Park, and Zion National Park", "paris", "Great Britain", "mid-size four - wheel drive luxury SUV", "brathwaite", "Mr. Burns", "we want to practice Christian love toward them and pray that they convert", "two populations of rodents", "around 1200", "T'Pau", "gestapo", "1815", "American Football Conference", "kimono", "North America", "\"synforms\"", "2015", "man's disobedience", "a representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "\"Winnie the Pooh\" (2011)", "Transpacific", "German", "registered with the General Teaching Council for Scotland (GTCS)", "commercial explosives and blasting systems to the mining, quarrying, oil and gas and construction markets", "either yes or no, or alternately either 1 or 0", "Gibraltar", "In the Season 5 premiere, `` Weight Loss ''", "lie detector", "white Ship"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3108356629484589}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.8813559322033898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.10526315789473684, 1.0, 0.0, 0.0, 0.34782608695652173, 0.0, 0.0, 0.33333333333333337, 0.0, 0.13333333333333333, 0.4615384615384615, 1.0, 0.9090909090909091, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7513", "mrqa_squad-validation-4747", "mrqa_triviaqa-validation-7064", "mrqa_naturalquestions-validation-126", "mrqa_triviaqa-validation-2960", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-1586", "mrqa_triviaqa-validation-6860", "mrqa_triviaqa-validation-6177", "mrqa_squad-validation-2368", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-4204", "mrqa_triviaqa-validation-1548", "mrqa_naturalquestions-validation-2870", "mrqa_hotpotqa-validation-421", "mrqa_triviaqa-validation-4668", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-2600", "mrqa_squad-validation-2781", "mrqa_hotpotqa-validation-3298", "mrqa_squad-validation-2040", "mrqa_hotpotqa-validation-1246", "mrqa_squad-validation-1652", "mrqa_naturalquestions-validation-9903", "mrqa_triviaqa-validation-1746"], "retrieved_ids": ["mrqa_triviaqa-validation-140", "mrqa_triviaqa-validation-6639", "mrqa_triviaqa-validation-7184", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-5686", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-15327", "mrqa_naturalquestions-train-62286", "mrqa_naturalquestions-train-12547", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-2723", "mrqa_triviaqa-validation-1162", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-train-30306", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-9227", "mrqa_naturalquestions-train-52872", "mrqa_naturalquestions-train-35836", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-1284", "mrqa_naturalquestions-train-16389", "mrqa_naturalquestions-train-19270", "mrqa_naturalquestions-train-83658", "mrqa_naturalquestions-train-8880", "mrqa_naturalquestions-train-58577", "mrqa_naturalquestions-train-55282", "mrqa_naturalquestions-train-56778", "mrqa_naturalquestions-train-9516", "mrqa_naturalquestions-train-72954", "mrqa_naturalquestions-train-38028", "mrqa_triviaqa-validation-2803", "mrqa_triviaqa-validation-7578", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4364", "mrqa_hotpotqa-validation-1758", "mrqa_triviaqa-validation-7407", "mrqa_naturalquestions-validation-142", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-11", "mrqa_naturalquestions-validation-3571", "mrqa_squad-validation-2372", "mrqa_naturalquestions-train-76701", "mrqa_naturalquestions-train-63620", "mrqa_squad-validation-1612", "mrqa_naturalquestions-train-38437", "mrqa_naturalquestions-train-52076", "mrqa_naturalquestions-train-77227", "mrqa_naturalquestions-train-65943", "mrqa_naturalquestions-train-39439", "mrqa_naturalquestions-train-4931", "mrqa_naturalquestions-train-86880", "mrqa_naturalquestions-train-6494", "mrqa_naturalquestions-train-167", "mrqa_naturalquestions-train-28724", "mrqa_naturalquestions-train-41128", "mrqa_squad-validation-4019", "mrqa_triviaqa-validation-6398", "mrqa_naturalquestions-train-57523", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-train-24615", "mrqa_triviaqa-validation-5686", "mrqa_naturalquestions-train-23581", "mrqa_triviaqa-validation-7407", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-2340", "mrqa_naturalquestions-train-42305", "mrqa_naturalquestions-train-23252", "mrqa_naturalquestions-train-51630", "mrqa_naturalquestions-train-74602", "mrqa_naturalquestions-train-21115", "mrqa_hotpotqa-validation-118", "mrqa_naturalquestions-train-38985", "mrqa_hotpotqa-validation-3241", "mrqa_naturalquestions-train-26324", "mrqa_naturalquestions-train-65669", "mrqa_triviaqa-validation-3664", "mrqa_hotpotqa-validation-3232", "mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-6683", "mrqa_squad-validation-9063", "mrqa_squad-validation-6878", "mrqa_naturalquestions-train-84083", "mrqa_squad-validation-9984", "mrqa_naturalquestions-train-52318", "mrqa_naturalquestions-train-84328", "mrqa_naturalquestions-train-18907", "mrqa_naturalquestions-train-38612", "mrqa_hotpotqa-validation-983", "mrqa_hotpotqa-validation-2484", "mrqa_naturalquestions-train-23581", "mrqa_naturalquestions-train-43132", "mrqa_naturalquestions-train-43791", "mrqa_naturalquestions-train-6794", "mrqa_naturalquestions-train-39326", "mrqa_naturalquestions-train-49100", "mrqa_naturalquestions-train-17368", "mrqa_naturalquestions-train-26147", "mrqa_naturalquestions-train-84908", "mrqa_naturalquestions-train-14170", "mrqa_naturalquestions-train-53232", "mrqa_naturalquestions-train-44654", "mrqa_naturalquestions-train-84845", "mrqa_naturalquestions-train-54176", "mrqa_naturalquestions-train-56778", "mrqa_naturalquestions-train-44589", "mrqa_naturalquestions-train-51229", "mrqa_naturalquestions-train-53810", "mrqa_naturalquestions-train-76391", "mrqa_naturalquestions-train-80615", "mrqa_naturalquestions-train-63092", "mrqa_squad-validation-8980", "mrqa_naturalquestions-train-15754", "mrqa_naturalquestions-train-4123", "mrqa_naturalquestions-train-24834", "mrqa_naturalquestions-train-43185", "mrqa_naturalquestions-train-48803", "mrqa_naturalquestions-train-20042", "mrqa_naturalquestions-train-67255", "mrqa_naturalquestions-train-82810", "mrqa_triviaqa-validation-3767", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-6185", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-6425"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 94, "before_eval": {"predictions": ["nucleus", "horror fiction", "texas", "bantu", "southeast of the city", "Caparra", "on the French island of Guadeloupe in the Lesser Antilles, mainly in the commune of Deshaies ( which doubles for the town of Honor\u00e9 on the fictional island of Saint Marie )", "Statute of Rageman", "1938", "about 63%", "American pharmaceutical company", "2016", "90% to 93% O2", "approximately 11 %", "the flags of dependent territories and other areas of special sovereignty", "Goldbach's conjecture", "bremen", "short-tempered and even harsher", "1987", "dastardly & Muttley", "regulates the practice of pharmacists and pharmacy technicians", "henry wentworth", "1968", "all E-7s are called chief petty officer", "led about 1,500 army troops and provincial militia on an expedition in June 1755 to take Fort Duquesne", "The Taliban", "June 11, 1986", "Dublin, Cork, Portarlington, Lisburn, Waterford and Youghal", "Sukhvinder Singh, Mahalaxmi Iyer and Vijay Prakash", "Tom Coburn", "2014", "increased and the ground water level fell significantly"], "metric_results": {"EM": 0.125, "QA-F1": 0.24698456925019424}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43750000000000006, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.923076923076923, 0.1, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 0.25]}}, "error_ids": ["mrqa_naturalquestions-validation-366", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-2404", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-7787", "mrqa_triviaqa-validation-2249", "mrqa_naturalquestions-validation-2624", "mrqa_squad-validation-3315", "mrqa_hotpotqa-validation-4506", "mrqa_naturalquestions-validation-2949", "mrqa_squad-validation-3677", "mrqa_naturalquestions-validation-5420", "mrqa_hotpotqa-validation-562", "mrqa_squad-validation-9001", "mrqa_triviaqa-validation-6433", "mrqa_squad-validation-2486", "mrqa_naturalquestions-validation-1382", "mrqa_triviaqa-validation-3230", "mrqa_triviaqa-validation-5584", "mrqa_hotpotqa-validation-2328", "mrqa_naturalquestions-validation-4365", "mrqa_squad-validation-10196", "mrqa_naturalquestions-validation-7496", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-659", "mrqa_squad-validation-9252"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 95, "before_eval": {"predictions": ["the \"richest 1 percent in the United States now own more wealth than the bottom 90 percent\".", "universal ruler", "red", "alain Giresse", "the Italian Alps", "ideal", "david duchovny", "elephant slamander", "the American newspaper magnate William Randolph Hearst", "parashah", "the s - block", "World Music Awards", "six", "8\u20134\u20134 system", "exposed to scrutiny", "79", "banned the growing of coffee", "troupes de la marine", "a compromise between the two", "stimulated his brain cells", "the bean-nighe", "Incumbent Democrat U.S. Senator Daniel Inouye", "David Toms", "Tom Brady", "corrugated metal", "11 free suburban weekly newspapers together covering the Adelaide metropolitan area", "Burt Hammersmith", "The Portuguese", "raises the productivity of each worker", "first adopted by the university's science club in 1886", "the Jurchen Aisin Gioro clan", "kronborg castle"], "metric_results": {"EM": 0.15625, "QA-F1": 0.25744848976273127}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.35294117647058826, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4210526315789474, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.20000000000000004, 0.5714285714285715, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-7459", "mrqa_triviaqa-validation-5295", "mrqa_hotpotqa-validation-717", "mrqa_squad-validation-9135", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-879", "mrqa_naturalquestions-validation-6129", "mrqa_naturalquestions-validation-3546", "mrqa_naturalquestions-validation-1688", "mrqa_hotpotqa-validation-4523", "mrqa_triviaqa-validation-1838", "mrqa_squad-validation-8288", "mrqa_squad-validation-10138", "mrqa_squad-validation-6914", "mrqa_squad-validation-1445", "mrqa_triviaqa-validation-2900", "mrqa_hotpotqa-validation-1234", "mrqa_naturalquestions-validation-5267", "mrqa_naturalquestions-validation-3093", "mrqa_triviaqa-validation-1682", "mrqa_hotpotqa-validation-5033", "mrqa_naturalquestions-validation-2806", "mrqa_naturalquestions-validation-9985", "mrqa_squad-validation-7184", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-9639", "mrqa_triviaqa-validation-4796"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 96, "before_eval": {"predictions": ["silicon", "Marcus Atilius Regulus", "The Dome of the Rock", "private finance initiatives", "the word is used to translate several Hebrew words, including Hod ( \u05d4\u05d5\u05d3 ) and kabod", "bicapitalized microcomputer software", "Bulgarian National Movement (IMRO) and National Front for the Salvation of Bulgaria (NFSB)", "4,577 nautical miles (8,477 km)", "Army, Navy and other services", "He is the younger brother of comic actor John Belushi", "Renhe Sports Management Ltd", "kansas city", "salvation and subsequently eternal life is not earned by good deeds but is received only as a free gift of God's grace through faith in Jesus Christ as redeemer from sin", "Oxygen", "the Dutch Cape Colony in South Africa, the Dutch East Indies, the Caribbean, and several of the English colonies of North America, and Quebec, where they were accepted and allowed to worship freely", "the cannonball ( assumed constant ) v", "2006", "74", "the Ministry of War", "1162", "el Che", "Porsche 968", "a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "1971", "The \"Huguenot Street Historic District\" in New Paltz", "American Christian rock band Needtobreathe", "the Iranian Islamic Revolution and apolitical Islam was a historical fluke of the \"short-lived era of the heyday of secular Arab nationalism between 1945 and 1970\"", "1986", "cake", "concave mirrors and converging lenses", "electroweak interaction", "kolinio Epeli Vanuacicila Rabuka and Salote Lomaloma Rabuka"], "metric_results": {"EM": 0.1875, "QA-F1": 0.31416170111023056}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.15384615384615385, 0.5714285714285715, 0.5714285714285715, 0.0, 0.0, 0.0, 0.23529411764705882, 0.16666666666666669, 0.4324324324324324, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6363636363636364, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.19999999999999998, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5675", "mrqa_squad-validation-6778", "mrqa_naturalquestions-validation-9323", "mrqa_triviaqa-validation-3165", "mrqa_hotpotqa-validation-5490", "mrqa_squad-validation-3876", "mrqa_hotpotqa-validation-5639", "mrqa_triviaqa-validation-66", "mrqa_naturalquestions-validation-2208", "mrqa_triviaqa-validation-113", "mrqa_squad-validation-2100", "mrqa_squad-validation-3667", "mrqa_squad-validation-3023", "mrqa_naturalquestions-validation-7297", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-7768", "mrqa_hotpotqa-validation-1399", "mrqa_naturalquestions-validation-9409", "mrqa_triviaqa-validation-3771", "mrqa_squad-validation-3057", "mrqa_squad-validation-9574", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-3432", "mrqa_squad-validation-10311", "mrqa_triviaqa-validation-3409"], "retrieved_ids": ["mrqa_naturalquestions-train-77486", "mrqa_naturalquestions-train-64545", "mrqa_naturalquestions-train-7222", "mrqa_naturalquestions-train-19283", "mrqa_naturalquestions-train-34429", "mrqa_naturalquestions-train-7807", "mrqa_naturalquestions-train-38494", "mrqa_naturalquestions-train-68734", "mrqa_naturalquestions-train-27782", "mrqa_naturalquestions-train-21514", "mrqa_naturalquestions-train-85910", "mrqa_naturalquestions-train-49810", "mrqa_naturalquestions-train-5734", "mrqa_naturalquestions-train-23475", "mrqa_naturalquestions-train-69167", "mrqa_triviaqa-validation-888", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-train-47895", "mrqa_naturalquestions-train-60208", "mrqa_naturalquestions-train-68895", "mrqa_naturalquestions-train-49463", "mrqa_hotpotqa-validation-118", "mrqa_hotpotqa-validation-5203", "mrqa_naturalquestions-train-24946", "mrqa_naturalquestions-train-31170", "mrqa_naturalquestions-train-56674", "mrqa_naturalquestions-train-80908", "mrqa_naturalquestions-train-34076", "mrqa_naturalquestions-train-41656", "mrqa_naturalquestions-train-50820", "mrqa_naturalquestions-train-7332", "mrqa_naturalquestions-train-81209", "mrqa_naturalquestions-train-19223", "mrqa_naturalquestions-train-21826", "mrqa_naturalquestions-train-87102", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-7215", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-train-24227", "mrqa_naturalquestions-train-29696", "mrqa_naturalquestions-train-10255", "mrqa_naturalquestions-train-11365", "mrqa_naturalquestions-train-48627", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4681", "mrqa_squad-validation-2313", "mrqa_naturalquestions-train-80729", "mrqa_naturalquestions-train-12311", "mrqa_naturalquestions-train-61626", "mrqa_naturalquestions-train-5928", "mrqa_naturalquestions-train-74251", "mrqa_naturalquestions-train-27938", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-train-37120", "mrqa_naturalquestions-train-28894", "mrqa_squad-validation-8095", "mrqa_naturalquestions-train-55414", "mrqa_naturalquestions-train-60554", "mrqa_naturalquestions-train-42307", "mrqa_naturalquestions-train-14048", "mrqa_naturalquestions-train-82315", "mrqa_naturalquestions-train-70361", "mrqa_naturalquestions-train-81971", "mrqa_naturalquestions-train-58973", "mrqa_naturalquestions-train-843", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-7064", "mrqa_triviaqa-validation-1217", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-2938", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-2236", "mrqa_naturalquestions-train-72722", "mrqa_naturalquestions-train-82157", "mrqa_hotpotqa-validation-5401", "mrqa_naturalquestions-train-42542", "mrqa_naturalquestions-train-37747", "mrqa_naturalquestions-train-215", "mrqa_naturalquestions-train-1701", "mrqa_naturalquestions-train-6457", "mrqa_naturalquestions-train-68815", "mrqa_naturalquestions-train-86215", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-5087", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-2635", "mrqa_hotpotqa-validation-3232", "mrqa_naturalquestions-train-13059", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-train-82427", "mrqa_naturalquestions-train-20198", "mrqa_naturalquestions-train-81145", "mrqa_naturalquestions-train-34415", "mrqa_naturalquestions-train-7381", "mrqa_naturalquestions-train-20327", "mrqa_naturalquestions-train-79932", "mrqa_naturalquestions-train-13167", "mrqa_naturalquestions-train-44643", "mrqa_naturalquestions-train-62687", "mrqa_naturalquestions-train-55816", "mrqa_naturalquestions-train-85477", "mrqa_naturalquestions-train-37905", "mrqa_naturalquestions-train-31830", "mrqa_naturalquestions-train-11357", "mrqa_naturalquestions-train-25623", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-train-33360", "mrqa_naturalquestions-train-49719", "mrqa_naturalquestions-train-7959", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-train-80958", "mrqa_naturalquestions-train-4869", "mrqa_squad-validation-10312", "mrqa_naturalquestions-train-73950", "mrqa_squad-validation-10427", "mrqa_squad-validation-10403", "mrqa_squad-validation-10447", "mrqa_triviaqa-validation-1306", "mrqa_triviaqa-validation-5020", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-3089", "mrqa_triviaqa-validation-6896"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 97, "before_eval": {"predictions": ["the franchise's 6th season in the National Football League (NFL)", "the victor over sin, death, and the world", "suburb", "The National Era", "during the American Civil War", "Rubus", "July 1, 2005", "a few drops", "a little girl ( Addy Miller )", "casket letters", "President pro tempore of the Senate", "postero - medially towards the optic chiasm, where there is a partial decussation ( crossing ) of fibres from the temporal visual fields ( the nasal hemi - retina ) of both eyes", "Edison Machine Works on Manhattan's lower east side", "quote Shelley's Masque of Anarchy to vast audiences during the campaign for a free India", "the Gararish", "Jai Courtney as John `` Jack '' McClane, Jr.", "difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "Galicia", "brian family", "French-German science fiction drama", "26", "AC", "kolkata", "Nickelback", "Juliet", "the \"father of the Mongols\"", "indeed caused by chlorine and bromine from manmade organohalogens", "a combined concert/lecture by British progressive folk-rock band Gryphon", "island", "brian", "Psych", "a series of newsreel films depicting multiple alternative realities rather than a novel ( although this idea may actually be borrowed from Dick's later novel Valis which features a mysterious film depicting yet another dystopian alternative history of the USA )"], "metric_results": {"EM": 0.125, "QA-F1": 0.2835664335664336}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.4, 0.0, 0.0, 0.3076923076923077, 0.0, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 0.20000000000000004, 0.0, 0.18181818181818182, 0.5555555555555556, 0.0, 0.4444444444444445, 0.14285714285714288, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.19999999999999998, 0.0, 0.0, 0.0, 0.33333333333333337, 0.05128205128205128]}}, "error_ids": ["mrqa_hotpotqa-validation-4113", "mrqa_squad-validation-2262", "mrqa_hotpotqa-validation-3785", "mrqa_naturalquestions-validation-2536", "mrqa_naturalquestions-validation-7957", "mrqa_triviaqa-validation-5041", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-7679", "mrqa_triviaqa-validation-5865", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-3358", "mrqa_squad-validation-1240", "mrqa_squad-validation-6859", "mrqa_hotpotqa-validation-4389", "mrqa_naturalquestions-validation-2504", "mrqa_naturalquestions-validation-1680", "mrqa_hotpotqa-validation-1905", "mrqa_triviaqa-validation-636", "mrqa_hotpotqa-validation-2802", "mrqa_triviaqa-validation-2161", "mrqa_naturalquestions-validation-7095", "mrqa_hotpotqa-validation-212", "mrqa_naturalquestions-validation-654", "mrqa_squad-validation-5359", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-6540", "mrqa_hotpotqa-validation-2271", "mrqa_naturalquestions-validation-2729"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 98, "before_eval": {"predictions": ["the most devastating stock market crash in the history of the United States ( acting as the most significant predicting indicator of the Great Depression ), when taking into consideration the full extent and duration of its after effects", "SKUM", "S6 Edge+", "paemontese", "sense light", "london", "fossil sequences in which there was datable material, converting the old relative ages into new absolute ages", "heaviest", "choral work", "San Francisco, California", "economic recession", "ex", "plum", "cyprus", "\"citizenship\", so that people had rights to empower them to become economically and socially active, rather than economic activity being a precondition for rights", "Jacking", "all media rights", "The Blind Boys of Alabama", "kent", "Sultans", "the end of the post-war communist control of the country and the reintroduction of a free-market economy", "charles", "much larger than the approximately 150,000 base pair genome of the more assimilated chloroplast", "roger penrose", "north", "theater", "Germ\u00e1n Efromovich", "limited period of time", "edo", "American actor and drag queen Divine, who was best known for his frequent appearances in several films directed by filmmaker John Waters", "between the three towns of Doncaster, Scunthorpe and Gainsborough", "1908"], "metric_results": {"EM": 0.21875, "QA-F1": 0.30606193856956054}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, true], "QA-F1": [0.4878048780487805, 1.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.2222222222222222, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 1.0, 1.0, 0.30769230769230765, 0.0, 0.42857142857142855, 0.18181818181818182, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8837", "mrqa_hotpotqa-validation-2978", "mrqa_triviaqa-validation-6634", "mrqa_naturalquestions-validation-2213", "mrqa_triviaqa-validation-1952", "mrqa_squad-validation-5008", "mrqa_hotpotqa-validation-5011", "mrqa_triviaqa-validation-2353", "mrqa_naturalquestions-validation-10707", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2725", "mrqa_squad-validation-4433", "mrqa_hotpotqa-validation-5136", "mrqa_triviaqa-validation-1816", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-6382", "mrqa_squad-validation-992", "mrqa_triviaqa-validation-5737", "mrqa_squad-validation-8802", "mrqa_triviaqa-validation-2414", "mrqa_hotpotqa-validation-5724", "mrqa_naturalquestions-validation-688", "mrqa_triviaqa-validation-4550", "mrqa_naturalquestions-validation-886", "mrqa_hotpotqa-validation-1533"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 99, "before_eval": {"predictions": ["Ron Grainer", "1919", "elstow", "acted increasingly aggressively to force the Huguenots to convert", "a Gender pay gap in favor of males in the labor market", "wild horse", "December 19, 1967", "economic growth", "1997", "October 1, 2017", "Washington, D.C.", "wagner", "Toronto, Ontario, Canada", "St Elgiva", "told Washington that France's claim to the region was superior to that of the British", "a balance sensor", "yosemite national", "field trips, supervise study halls, help with the organization of school functions, and serve as supervisors for extracurricular activities", "Prince Nikolai Sergeyevich Trubetzkoy", "practiced law", "the studies and developments department of the French firm R2E Micral in 1980 at the request of the company CCMC specializing in payroll and accounting", "1800 to 1850", "in English - speaking countries, more often by being marked on their foreheads as a visible cross", "the author recounts how his own opinions changed about that line when he talks to the different people about their beliefs", "Chlo\u00eb Alexandra Adele Emily Agnew", "Trinidad and Tobago", "derry", "9", "porto", "Bhaktivedanta Manor", "the largest known species of jellyfish", "Mammals"], "metric_results": {"EM": 0.25, "QA-F1": 0.3558627025625023}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.18181818181818182, 0.2105263157894737, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.13333333333333333, 1.0, 1.0, 0.19999999999999998, 0.6666666666666666, 0.0, 0.25, 0.23076923076923078, 0.0, 0.34782608695652173, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-7714", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-4903", "mrqa_squad-validation-3130", "mrqa_squad-validation-7449", "mrqa_triviaqa-validation-7414", "mrqa_hotpotqa-validation-4384", "mrqa_triviaqa-validation-7769", "mrqa_hotpotqa-validation-4287", "mrqa_hotpotqa-validation-1086", "mrqa_squad-validation-10231", "mrqa_squad-validation-1870", "mrqa_hotpotqa-validation-5590", "mrqa_hotpotqa-validation-4961", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-251", "mrqa_hotpotqa-validation-4254", "mrqa_triviaqa-validation-6825", "mrqa_triviaqa-validation-6226", "mrqa_squad-validation-8972", "mrqa_triviaqa-validation-4125", "mrqa_hotpotqa-validation-5833", "mrqa_naturalquestions-validation-2794"], "retrieved_ids": ["mrqa_naturalquestions-train-79850", "mrqa_naturalquestions-train-36676", "mrqa_naturalquestions-train-41085", "mrqa_naturalquestions-train-30667", "mrqa_naturalquestions-train-60968", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-train-56257", "mrqa_naturalquestions-train-34071", "mrqa_naturalquestions-train-66330", "mrqa_naturalquestions-train-23352", "mrqa_naturalquestions-train-50948", "mrqa_naturalquestions-train-68895", "mrqa_naturalquestions-train-6991", "mrqa_hotpotqa-validation-5822", "mrqa_naturalquestions-train-55139", "mrqa_naturalquestions-train-23421", "mrqa_naturalquestions-train-49277", "mrqa_naturalquestions-train-48852", "mrqa_naturalquestions-train-25986", "mrqa_naturalquestions-train-45978", "mrqa_naturalquestions-train-71382", "mrqa_naturalquestions-train-49719", "mrqa_naturalquestions-train-31170", "mrqa_naturalquestions-train-824", "mrqa_naturalquestions-train-79169", "mrqa_hotpotqa-validation-3232", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-5087", "mrqa_hotpotqa-validation-482", "mrqa_naturalquestions-train-3106", "mrqa_naturalquestions-train-8742", "mrqa_naturalquestions-train-72688", "mrqa_naturalquestions-train-29696", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-7768", "mrqa_triviaqa-validation-2959", "mrqa_hotpotqa-validation-4221", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5727", "mrqa_squad-validation-2331", "mrqa_naturalquestions-train-7910", "mrqa_naturalquestions-train-38643", "mrqa_naturalquestions-train-7307", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-train-74023", "mrqa_naturalquestions-train-27314", "mrqa_naturalquestions-train-62579", "mrqa_naturalquestions-train-60125", "mrqa_naturalquestions-train-65943", "mrqa_squad-validation-1879", "mrqa_squad-validation-1942", "mrqa_squad-validation-2966", "mrqa_naturalquestions-train-53720", "mrqa_naturalquestions-train-69860", "mrqa_naturalquestions-train-72954", "mrqa_hotpotqa-validation-3632", "mrqa_naturalquestions-train-74438", "mrqa_naturalquestions-train-28249", "mrqa_naturalquestions-train-79133", "mrqa_naturalquestions-train-36561", "mrqa_naturalquestions-train-81209", "mrqa_hotpotqa-validation-5639", "mrqa_hotpotqa-validation-1758", "mrqa_naturalquestions-train-41621", "mrqa_naturalquestions-train-33739", "mrqa_naturalquestions-validation-6554", "mrqa_naturalquestions-train-42067", "mrqa_naturalquestions-train-52849", "mrqa_naturalquestions-train-65265", "mrqa_naturalquestions-train-77137", "mrqa_naturalquestions-train-49380", "mrqa_naturalquestions-train-21881", "mrqa_naturalquestions-train-544", "mrqa_naturalquestions-train-20847", "mrqa_naturalquestions-train-25813", "mrqa_naturalquestions-train-71503", "mrqa_naturalquestions-train-53922", "mrqa_naturalquestions-train-65989", "mrqa_hotpotqa-validation-2993", "mrqa_naturalquestions-train-72472", "mrqa_naturalquestions-train-3192", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-train-811", "mrqa_naturalquestions-train-39181", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-6860", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-2725", "mrqa_squad-validation-7732", "mrqa_triviaqa-validation-4054", "mrqa_triviaqa-validation-1053", "mrqa_naturalquestions-train-31209", "mrqa_naturalquestions-train-86218", "mrqa_naturalquestions-train-25908", "mrqa_squad-validation-9081", "mrqa_naturalquestions-train-22865", "mrqa_squad-validation-10428", "mrqa_naturalquestions-train-53337", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-7768", "mrqa_triviaqa-validation-2236", "mrqa_triviaqa-validation-3055", "mrqa_naturalquestions-train-8124", "mrqa_naturalquestions-validation-5651", "mrqa_naturalquestions-train-60089", "mrqa_naturalquestions-train-55816", "mrqa_naturalquestions-train-3178", "mrqa_naturalquestions-train-36750", "mrqa_squad-validation-4185", "mrqa_squad-validation-9984", "mrqa_naturalquestions-train-14250", "mrqa_naturalquestions-train-66044"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}], "final_eval_results": {"overall_oncoming_test": {"EM": 0.1771875, "QA-F1": 0.28688422082265064}, "overall_error_number": 2633, "overall_instant_fixing_rate": 0.0, "final_instream_test": {"EM": 0.6965625, "QA-F1": 0.761047467894434}, "final_upstream_test": {"EM": 0.735, "QA-F1": 0.8417271952034805}}}