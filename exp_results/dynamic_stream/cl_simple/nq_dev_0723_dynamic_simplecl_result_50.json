{"model_update_steps": 520, "method_class": "continual_finetuning", "base_model_args": "Namespace(base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, learning_rate=3e-05, max_grad_norm=0.1, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/cl_simple/nq_dev_0723_dynamic_simplecl_ckpts/', save_all_ckpts=1, total_steps=10000, use_sampled_upstream=False, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=10, append_another_bos=1, bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', data_stream_json_path='bug_data/mrqa_naturalquestions_dev.data_stream.test.json', do_lowercase=False, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, pass_pool_jsonl_path='bug_data/mrqa_naturalquestions_dev.sampled_pass.jsonl', predict_batch_size=16, sampled_upstream_json_path='bug_data/mrqa_naturalquestions.sampled_upstream.jsonl', task_name='mrqa_naturalquestions', train_batch_size=8)", "online_eval_results": [{"timecode": 0, "accumulative_EM": 0.75, "accumulative_forgotten_ids": [], "accumulative_fixed_ids": [], "accumulative_forgotten_rate": 0.0, "accumulative_fixed_rate": 0.0, "before_eval": {"predictions": ["Merry Clayton", "Quantitative psychological research", "October 22, 2017", "Descriptive research", "Mike Myers", "CBS", "Landry's, Inc.", "Juan Francisco Ochoa", "RMS Titanic", "Gary Player", "Ryan Seacrest", "Mitch Murray", "every two to six years ( depending on the positions being filled with most positions good for four years )", "an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "each team is given a position in the drafting order in reverse order relative to its record in the previous year, which means that the last place team is positioned first", "in all cases affecting ambassadors, other public ministers and consuls, and those in which a state shall be party", "James Halliday", "Western Australia", "Ali Daei", "204,408 in 2013", "symbolises the freedom of the recipient to enter and leave the city at will, as a trusted friend of city residents", "10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "Georgia Groome", "18", "The Mandate of Heaven", "Chicago", "Michael English", "Massachusetts", "Ali", "an Abstergo agent", "Ming dynasty"], "metric_results": {"EM": 0.75, "QA-F1": 0.784691360870367}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.45454545454545453, 0.11764705882352942, 0.0, 0.13793103448275862, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "after_eval": {"predictions": ["Merry Clayton", "statistical estimation or statistical inference", "October 22, 2017", "Descriptive research", "Mike Myers", "CBS", "Landry's, Inc.", "Juan Francisco Ochoa", "RMS Titanic", "Gary Player", "Giuliana Rancic", "Mitch Murray", "every two to six years", "a metaphor for a burden to be carried as penance", "backflow prevention", "each team", "in all cases affecting ambassadors, other public ministers and consuls, and those in which a state shall be party", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "South Korea", "Ali Daei", "204,408 in 2013", "symbolises the freedom of the recipient to enter and leave the city at will, as a trusted friend of city residents", "greater than 14", "Georgia Groome", "18", "The Mandate of Heaven", "Chicago", "Michael English", "Massachusetts", "Ali", "an Abstergo agent", "imperial rule"], "metric_results": {"EM": 0.90625, "QA-F1": 0.91875}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-8279"], "retained_ids": ["mrqa_naturalquestions-validation-9658", "mrqa_naturalquestions-validation-7591", "mrqa_naturalquestions-validation-6568", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-6268", "mrqa_naturalquestions-validation-7392", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-8546", "mrqa_naturalquestions-validation-2053", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-1819", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-7197", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-6109", "mrqa_naturalquestions-validation-1789", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-3922"], "fixed_ids": ["mrqa_naturalquestions-validation-7707", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-816"], "unfixed_ids": ["mrqa_naturalquestions-validation-10080"], "error_ids": ["mrqa_naturalquestions-validation-10080", "mrqa_naturalquestions-validation-7707", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-816"], "instant_fixing_rate": 0.875, "instant_retention_rate": 0.9166666666666666, "doing-nothing_instant_EM": 0.75, "doing-nothing_accmulative_EM": 0.75}, {"timecode": 1, "before_eval": {"predictions": ["Atlanta, Georgia", "Pat McCormick", "the Gentiles", "Felicity Huffman", "1976", "Piet van der Walt", "Ohio", "Thomas Jefferson", "any data that can be used to identify a living individual", "overharvesting, pollution, habitat destruction, introduction of invasive species ( such as new predators and food competitors ), overhunting, and other influences", "October 1941", "a loop", "mid November", "testes", "Pl\u016bt\u014d", "1974", "Domhnall Gleeson", "360", "Presley Smith", "Taron Egerton", "Woody Paige", "The Main Ingredient", "the Battle of Antietam", "Mad - Eye Moody", "Matthew Broderick", "Leo Howard", "Wales and Yorkshire", "seven", "needle - like", "2005", "enforcing racially separated educational facilities", "China"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7276785714285714}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "after_eval": {"predictions": ["Atlanta, Georgia", "Pat McCormick", "to Jewish audiences", "Felicity Huffman", "1976", "Erastus Utoni", "Toledo", "Thomas Jefferson", "to identify a living individual", "explosive, unsustainable human population growth", "October 1941", "a loop", "late November or early December", "Reproductive system", "Dis Pater", "1889", "Domhnall Gleeson", "360", "Presley Smith", "Taron Egerton", "Woody Paige", "Ronnie Dyson", "the Battle of Antietam", "Mad - Eye Moody", "Matthew Broderick", "Leo Howard", "Wales and Yorkshire", "seven", "greater effective for such prey because they can easily grip their slippery and narrow bodies", "2005", "enforcing racially separated educational facilities", "China"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9553571428571428}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-3927", "mrqa_naturalquestions-validation-5119"], "retained_ids": ["mrqa_naturalquestions-validation-7649", "mrqa_naturalquestions-validation-412", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-160", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-4459", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9588", "mrqa_naturalquestions-validation-5281", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-6641", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-9004"], "fixed_ids": ["mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-8529", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-10326", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-6166"], "unfixed_ids": [], "error_ids": ["mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-8529", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-10326", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-6166"], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9130434782608695, "doing-nothing_instant_EM": 0.6875, "doing-nothing_accmulative_EM": 0.71875}, {"timecode": 2, "before_eval": {"predictions": ["Liberia", "Cashiers", "Boutique hotel", "Woodrow Wilson", "Gary Grimes", "1773", "Stephen Lang", "2016", "1999", "Lauren Tom", "Robert Koch", "Mission : Impossible -- Fallout", "Ozzie Smith", "members of the gay ( LGBT ) community", "Amitabh Bachchan", "Miami Heat", "Mahatma Gandhi", "1995", "First Lieutenant Israel Greene", "judges", "Lord's", "1979", "New York Yankees'third baseman Alex Rodriguez", "presided over by the Speaker or, in his absence, by the Deputy Speaker of the Lok Sabha", "a combination of genetics and the male hormone dihydrotestosterone", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "September 1993", "a certain ethical set of ideals, principles, doctrines, myths, or symbols of a social movement, institution, class, or large group that explains how society should work, and offers some political and cultural blueprint for a certain social order", "Dilwale Dulhania Le Jayenge", "New England", "a numbers operator who was wanted for questioning in the bombing of rival numbers racketeer and future boxing promoter Don King's home three days earlier", "the President"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7579391891891892}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.19999999999999998, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.05405405405405405, 1.0, 0.0, 0.0, 1.0]}}, "after_eval": {"predictions": ["only Ethiopia and Liberia", "across western North Carolina including Asheville, Cashiers and Saluda", "Bed and breakfast", "Woodrow Wilson", "Gary Grimes", "1773", "Stephen Lang", "2016", "1999", "Lauren Tom", "Robert Koch", "Mission : Impossible -- Fallout", "Ozzie Smith", "members of the gay ( LGBT ) community", "Amitabh Bachchan", "Miami Heat", "Mahatma Gandhi", "1995", "First Lieutenant Israel Greene", "The courts", "Lord's", "1979", "Alex Rodriguez", "President", "a combination of genetics and the male hormone dihydrotestosterone", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "September 1993", "a political ideology", "Dilwale Dulhania Le Jayenge", "1958", "Dollree Mapp", "President"], "metric_results": {"EM": 0.9375, "QA-F1": 0.95}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-1950"], "retained_ids": ["mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-1605", "mrqa_naturalquestions-validation-5751", "mrqa_naturalquestions-validation-2025", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-8726", "mrqa_naturalquestions-validation-9872", "mrqa_naturalquestions-validation-5063", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-5883", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-8423", "mrqa_naturalquestions-validation-8908", "mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-4532", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-684", "mrqa_naturalquestions-validation-3000", "mrqa_naturalquestions-validation-9716"], "fixed_ids": ["mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-2482"], "unfixed_ids": ["mrqa_naturalquestions-validation-2100"], "error_ids": ["mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-2482"], "instant_fixing_rate": 0.8888888888888888, "instant_retention_rate": 0.9565217391304348, "doing-nothing_instant_EM": 0.8125, "doing-nothing_accmulative_EM": 0.75}, {"timecode": 3, "before_eval": {"predictions": ["Bill Russell", "Johnson", "1837", "Broken Hill and Sydney", "Justin Timberlake", "alternation of generations", "China in American colonies without paying any taxes", "June Gable", "seawater pearls", "Donald Trump", "February 25, 2004", "Havana Harbor", "about 3.5 mya", "The Internet protocol suite ( TCP / IP )", "Clement Attlee's Labour Party", "The Han", "extends 2,000 kilometres ( 1,200 mi ) down the Australian northeast coast", "Database - Protocol driver ( Pure Java driver )", "May 2010", "reserved for cars of the royal family", "Benzodiazepines", "1983", "The federal government", "Paul Hogan", "Tori Kelly", "Christianity", "886 AD", "the rear leg of the cow", "Cedric Alexander", "October 2012", "a children's song, based on a dialogue between two characters, called Henry and Liza, about a leaky bucket", "sepoys of the Company's army"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7659632034632035}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.27272727272727276]}}, "after_eval": {"predictions": ["Bill Russell", "Johnson", "Sir Rowland Hill", "Broken Hill and Sydney", "Justin Timberlake", "alternation of generations", "East India Company", "June Gable", "Indian Ocean", "Donald Trump", "February 25, 2004", "Havana Harbor", "about 3.5 mya", "The Internet protocol suite ( TCP / IP )", "Labour Party", "Qing", "South Pacific", "Database - Protocol driver ( Pure Java driver )", "May 2010", "reserved for cars of the royal family", "Benzodiazepines", "1983", "states", "Paul Hogan", "Tori Kelly", "Christianity", "the 1960s", "the rear leg of the cow", "Cedric Alexander", "October 2012", "two characters, called Henry and Liza", "sepoys of the Company's army in the garrison town of Meerut, 40 miles northeast of Delhi ( now Old Delhi )"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9973214285714286}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9142857142857143]}}, "forgotten_ids": [], "retained_ids": ["mrqa_naturalquestions-validation-10692", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-397", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-1535", "mrqa_naturalquestions-validation-10476", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-7922", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-7027", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-3732", "mrqa_naturalquestions-validation-9972", "mrqa_naturalquestions-validation-6110", "mrqa_naturalquestions-validation-4442", "mrqa_naturalquestions-validation-9685", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-9786", "mrqa_naturalquestions-validation-3222", "mrqa_naturalquestions-validation-6821", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-3431"], "fixed_ids": ["mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-8368"], "unfixed_ids": ["mrqa_naturalquestions-validation-4098"], "error_ids": ["mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-8368", "mrqa_naturalquestions-validation-4098"], "instant_fixing_rate": 0.8888888888888888, "instant_retention_rate": 1.0, "doing-nothing_instant_EM": 0.71875, "doing-nothing_accmulative_EM": 0.7421875}, {"timecode": 4, "before_eval": {"predictions": ["Henry Selick", "honey bees", "2010", "Michael Rosen", "interpreter", "June 1991", "a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "the subcutis, the layer of skin directly below the dermis and epidermis, collectively referred to as the cutis", "18 - season", "young girl", "A United States Navy enlisted rate indicates where an enlisted sailor stands within the chain of command, and also defines one's pay grade.", "July 1, 1890", "a lifestyle of poverty, traveling, and living in urban areas for purposes of preaching, evangelism, and ministry, especially to the poor", "Skat", "Karen Gillan", "Iowa", "Kim Basinger", "Roxette", "George II", "1939", "Kyla Pratt", "Keegan - Michael Key", "1 mile ( 1.6 km )", "Gatiman express its ranges 160km / hour between Delhi to Agra In 100 min its cross 180km", "Alton, Elora, King Township, Toronto, Uxbridge, and Whitevale, all located in the Canadian province of Ontario", "Ewan McGregor", "nomads from Inner Asia", "126 PYG to 1 USD", "Sanchez Navarro", "Frank Langella", "a desire to be reckoned with as an openly wounded and unabashedly portentous rock balladeer", "Imperial Japan"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7311577311577311}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6363636363636364, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5384615384615384, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "after_eval": {"predictions": ["Henry Selick", "honey bees", "2010", "Michael Rosen", "source code", "June 1991", "acts as a primer, by polymerizing the first few glucose molecules", "the subcutis, the layer of skin directly below the dermis and epidermis, collectively referred to as the cutis", "2001", "an illustration by Everest creative Maganlal Daiya back in the 1960s", "E-9s master chief petty officer", "July 1, 1890", "preaching, evangelism, and ministry, especially to the poor", "Skat", "Karen Gillan", "South Dakota", "Kim Basinger", "Roxette", "George II", "1939", "Kyla Pratt", "Keegan - Michael Key", "1 mile ( 1.6 km )", "160km / hour", "Alton, Elora, King Township, Toronto, Uxbridge, and Whitevale", "Ewan McGregor", "nomads from Inner Asia", "126 PYG to 1 USD", "Sanchez Navarro", "Frank Langella", "Can't Change Me", "Taft -- Katsura Agreement"], "metric_results": {"EM": 0.84375, "QA-F1": 0.8989712732919255}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6956521739130436, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.5]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-8488"], "retained_ids": ["mrqa_naturalquestions-validation-3612", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-9722", "mrqa_naturalquestions-validation-5744", "mrqa_naturalquestions-validation-2435", "mrqa_naturalquestions-validation-8795", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-4691", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-7108", "mrqa_naturalquestions-validation-10284", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-10492", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-576", "mrqa_naturalquestions-validation-7816", "mrqa_naturalquestions-validation-4450", "mrqa_naturalquestions-validation-8883", "mrqa_naturalquestions-validation-793"], "fixed_ids": ["mrqa_naturalquestions-validation-9885", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-1578", "mrqa_naturalquestions-validation-3395"], "unfixed_ids": ["mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-7270", "mrqa_naturalquestions-validation-8990"], "error_ids": ["mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-9885", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-1578", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-7270", "mrqa_naturalquestions-validation-8990"], "instant_fixing_rate": 0.7, "instant_retention_rate": 0.9090909090909091, "doing-nothing_instant_EM": 0.6875, "doing-nothing_accmulative_EM": 0.73125}, {"timecode": 5, "before_eval": {"predictions": ["Wisconsin", "local authorities, specifically London boroughs, Metropolitan Boroughs, unitary authorities, and district councils", "Aegisthus", "Vincenzo Peruggia", "the 9th century", "Kostas", "Wylie Draper", "2003", "Cristeta Comerford", "The Lykan Hypersport", "Meri", "Arthur Chung", "1984", "to form a higher alkane", "1940s", "self - closing flood barrier ( SCFB )", "February 27, 2007", "Billie `` The Blue Bear ''", "3 -- 4 years : Tachycardia > 137 bpm", "Viceroyalty of New Spain", "February 3, 2017", "October 6, 2017", "people in the 20th century who used obscure languages as a means of secret communication during wartime", "Pete Maravich", "Walter Brennan", "runoff will usually occur unless there is some physical barrier", "Anguillara Sabazia outside of Rome", "October 14, 2017", "the 2nd century", "ensure consistency within a document and across multiple documents and enforces best practice in usage and in language composition, visual composition, orthography and typography", "Coton in the Elms", "Baaghi"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7703482177009525}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.19354838709677422, 1.0, 1.0, 0.18181818181818182, 0.8333333333333333, 1.0, 0.8, 0.30434782608695654, 1.0, 0.5]}}, "after_eval": {"predictions": ["Wisconsin", "local authorities, specifically London Boroughs, Metropolitan boroughs, unitary authorities, and district councils", "Agamemnon", "Vincenzo Peruggia", "the 9th century", "Kostas", "Alex Burrall, Jason Weaver and Wylie Draper played Michael Jackson in different eras", "`` Fourth Revised Edition '' ISBN 0 - 06 - 015547 - 7", "Cristeta Comerford", "The Lykan Hypersport", "Meri", "Arthur Chung", "1984", "to form a higher alkane", "1940s", "self - closing flood barrier ( SCFB )", "February 27, 2007", "Billie `` The Blue Bear '', a German ex-prostitute who has a reputation as a dirty fighter", "Tachycardia", "1535", "February 3, 2017", "October 6, 2017", "Native Americans in the United States Marine Corps whose primary job was the transmission of secret tactical messages", "Pete Maravich", "Walter Brennan", "runoff", "in the town Anguillara Sabazia outside of Rome", "October 14, 2017", "in the 2nd century", "ensure consistency within a document and across multiple documents and enforces best practice in usage and in language composition, visual composition, orthography and typography", "Coton in the Elms", "Baaghi ( English : Rebel )"], "metric_results": {"EM": 0.875, "QA-F1": 0.906832298136646}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30434782608695654, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-10529"], "retained_ids": ["mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-3507", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-2637", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-5787", "mrqa_naturalquestions-validation-5230", "mrqa_naturalquestions-validation-6877", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-557", "mrqa_naturalquestions-validation-5199"], "fixed_ids": ["mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-10619", "mrqa_naturalquestions-validation-10162", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-4496", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-9912", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-3623"], "unfixed_ids": ["mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-4050"], "error_ids": ["mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-10619", "mrqa_naturalquestions-validation-10162", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-4496", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-9912", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-4050", "mrqa_naturalquestions-validation-3623"], "instant_fixing_rate": 0.8181818181818182, "instant_retention_rate": 0.9047619047619048, "doing-nothing_instant_EM": 0.6875, "doing-nothing_accmulative_EM": 0.7239583333333334}, {"timecode": 6, "before_eval": {"predictions": ["Content", "1932 and 1980", "Dido", "Jonathan Goldstein", "Americans who served in the armed forces and as civilians during World War II", "Rugrats in Paris : The Movie", "a layer of material ranging from fractions of a nanometer ( monolayer ) to several micrometers in thickness", "UTC \u2212 09 : 00", "1955", "1876", "just after the Super Bowl", "Crepuscular animals", "activates a relay which will handle the higher current load", "distance covered by a vehicle ( for example as recorded by an odometer ), person, animal, or object along a curved path from a point A to a point B", "Disha Vakani", "103", "balance sheet", "Vienna", "July 1, 2005", "Zoroastrian", "one body and one Spirit just as you were called to the one hope that belongs to your call one Lord, one faith, one baptism, one God and Father of all, who is over all and through all and in all", "Jack Nicklaus", "Steven `` Steve '' Hale", "December 20, 1951", "Crescent City Live - Stock Landing and Slaughter - House Company", "Dr. Jesse Bennett", "southwestern United States, Mexico, and Central America", "Bob Dylan", "Hold On", "1800 to 1850", "B.J. Thomas", "Cadillac"], "metric_results": {"EM": 0.65625, "QA-F1": 0.699256993006993}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.14545454545454545, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.23076923076923078, 1.0, 1.0]}}, "after_eval": {"predictions": ["information", "twice", "Dido", "Jonathan Goldstein", "Americans who served in the armed forces and as civilians during World War II", "Rugrats in Paris : The Movie", "a layer of material ranging from fractions of a nanometer ( monolayer ) to several micrometers in thickness", "UTC \u2212 09 : 00", "1955", "1876", "just after the Super Bowl", "Crepuscular", "via pulse - width modulation of the pump voltage", "displacement", "Disha Vakani", "103", "balance sheet", "Vienna", "April 26, 2005", "Zoroastrian", "the oneness of the body, the church, through what Christians have in common, what they have communion in", "Jack Nicklaus", "Steven `` Steve '' Hale ( originally introduced as Steve Peters ; portrayed by Scott Weinger )", "December 20, 1951", "New Orleans", "Dr. Jesse Bennett", "in arid lowland or mountainous shrubland, widely dispersed in dry open country with scattered brush", "Bob Dylan", "Hold On", "the end of the 18th century, and in most areas was at its peak in the approximate period from 1800 to 1850", "B.J. Thomas", "Cadillac"], "metric_results": {"EM": 0.875, "QA-F1": 0.9459462233169129}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.9655172413793104, 1.0, 1.0, 0.9047619047619047, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-5043"], "retained_ids": ["mrqa_naturalquestions-validation-8182", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-9646", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-2682", "mrqa_naturalquestions-validation-3985", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-1028", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-159", "mrqa_naturalquestions-validation-4151", "mrqa_naturalquestions-validation-7362", "mrqa_naturalquestions-validation-135", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-6125", "mrqa_naturalquestions-validation-8181"], "fixed_ids": ["mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-9848", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-7300"], "unfixed_ids": ["mrqa_naturalquestions-validation-5267", "mrqa_naturalquestions-validation-8319", "mrqa_naturalquestions-validation-3505"], "error_ids": ["mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-9848", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-5267", "mrqa_naturalquestions-validation-7300", "mrqa_naturalquestions-validation-8319", "mrqa_naturalquestions-validation-3505"], "instant_fixing_rate": 0.7272727272727273, "instant_retention_rate": 0.9523809523809523, "doing-nothing_instant_EM": 0.71875, "doing-nothing_accmulative_EM": 0.7232142857142857}, {"timecode": 7, "before_eval": {"predictions": ["Arkansas, turning south near Fulton, Arkansas, and flowing into Louisiana, where it flows into the Atchafalaya River", "Kate Warne", "May 2017", "to prevent organizations from abusing their tax - exempt status", "substantive fields of law", "Max Martin", "`` Robber baron ''", "Mike Jones featuring Nicole Wray, Trillville, Juvenile featuring Skip & Wacko, Nasty Nardo, 8Ball & MJG, Lil'Boosie & Webbie and other artists", "before the first year begins", "Andy", "Freddie Highmore", "2008", "brain, heart, and pancreas", "Keith Thibodeaux", "through a chute beneath his or her feet", "President pro tempore of the Senate", "c. 1000 AD", "Charlene Holt", "Pittsburgh", "electron pairs", "Authority", "gonadotropin - releasing hormone ( GnRH ) and thus secretion of luteinizing hormone ( LH ) and follicle - stimulating hormone ( FSH )", "Frank Zappa", "Gabourey Sidibe as older Regina, Nya Auzenne as young Regina )", "1989", "Daenerys Targaryen ( season 1 -- present ) portrayed by Emilia Clarke", "the public", "Reverend J. Long", "late summer", "1", "Shepperton Studios in Surrey, United Kingdom", "unknown origin"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7059835997335997}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.7999999999999999, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.923076923076923, 0.20000000000000004, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0]}}, "after_eval": {"predictions": ["the Texas Panhandle", "Kate Warne", "May 2017", "provides the public with financial information about a nonprofit organization", "English law", "Max Martin", "Robber baron", "Mike Jones featuring Nicole Wray, Trillville, Juvenile featuring Skip & Wacko, Nasty Nardo, 8Ball & MJG, Lil'Boosie & Webbie and other artists", "the student's transition from the study of preclinical to clinical health sciences", "Andy", "Freddie Highmore", "2008", "brain, heart, and pancreas", "Keith Thibodeaux", "a chute beneath his or her feet", "Speaker of the House of Representatives", "c. 1000 AD", "Charlene Holt", "the present ( 2016 -- 2018, contemporaneous with airing ) and a storyline taking place at a set time in the past ; but some episodes are set in one time period or use multiple flashback time periods", "shared pairs or bonding pairs", "Authority", "estrogen", "Frank Zappa", "Bonnie Lipton ( portrayed by Skyler Samuels )", "1989", "Emilia Clarke", "the public", "Reverend J. Long", "late - September through early January", "1", "Shepperton Studios in Surrey, United Kingdom", "the French expression `` et voil\u00e0! ''"], "metric_results": {"EM": 0.84375, "QA-F1": 0.9080357142857143}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7999999999999999, 0.8571428571428572, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-9071"], "retained_ids": ["mrqa_naturalquestions-validation-4273", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-6924", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-1886", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-2863", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-8054", "mrqa_naturalquestions-validation-8426", "mrqa_naturalquestions-validation-6950"], "fixed_ids": ["mrqa_naturalquestions-validation-8064", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-8180", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-7792", "mrqa_naturalquestions-validation-226"], "unfixed_ids": ["mrqa_naturalquestions-validation-8791", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-1179"], "error_ids": ["mrqa_naturalquestions-validation-8064", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-8791", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-8180", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-7792", "mrqa_naturalquestions-validation-226"], "instant_fixing_rate": 0.7692307692307693, "instant_retention_rate": 0.8947368421052632, "doing-nothing_instant_EM": 0.625, "doing-nothing_accmulative_EM": 0.7109375}, {"timecode": 8, "before_eval": {"predictions": ["A simple majority", "Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Ian McKellen, and Emma Thompson in supporting roles", "Acts 1 : 13", "Muhammad", "Total Drama World Tour", "personnel directors", "Jack Barry", "around 4 P.M. on Monday, July 10, 2017", "1979", "1939", "Rocky Dzidzornu", "Blake Lively", "English or Greek", "Dustin Johnson", "Michael Buffer", "1947", "Christopher Allen Lloyd", "Dorothy Kilgallen", "1959", "the Ramones", "the seven churches : to Ephesus, and to Smyrna", "architect Louis Le Vau, landscape architect Andr\u00e9 Le N\u00f4tre, and painter - decorator Charles Lebrun", "complex structured and unstructured information used by a computer system", "ideology", "72 individuals with the active help by A.O Hume, a retired British officer", "in southern Turkey, dividing the Mediterranean coastal region of southern Turkey from the central Anatolian Plateau", "President Lyndon Johnson", "Selena Gomez", "fertilization", "1996", "a god of the Ammonites, as well as Tyrian Melqart and others", "Cheryl Lynn"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7248086734693877}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.6938775510204082, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.125, 1.0, 1.0, 0.625, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "after_eval": {"predictions": ["A simple majority", "Emma Watson and Dan Stevens as the eponymous characters with Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Ian McKellen, and Emma", "Acts 1 : 13", "Muhammad", "Total Drama World Tour", "NFL coaches, general managers, and scouts", "Jack Barry", "around 4 P.M. on Monday, July 10, 2017", "1979", "1940", "Rocky Dzidzornu", "Blake Lively", "Greek", "Dustin Johnson", "Michael Buffer", "1947", "Christopher Allen Lloyd", "John Daly", "1959", "the Ramones", "Smyrna", "Louis XIV", "complex structured and unstructured information used by a computer system", "ideology", "A.O Hume, a retired British officer", "the Mediterranean coastal region of southern Turkey from the central Anatolian Plateau", "President Lyndon Johnson", "Instagram's own account", "fertilization", "2010", "king", "Cheryl Lynn"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9793154761904762}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-9672"], "retained_ids": ["mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-7264", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-1807", "mrqa_naturalquestions-validation-6763", "mrqa_naturalquestions-validation-9165", "mrqa_naturalquestions-validation-2920", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-33", "mrqa_naturalquestions-validation-115", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-1037", "mrqa_naturalquestions-validation-9581", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-10187"], "fixed_ids": ["mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-407", "mrqa_naturalquestions-validation-6517", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-3693", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-2085"], "unfixed_ids": ["mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-5785"], "error_ids": ["mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-407", "mrqa_naturalquestions-validation-6517", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-3693", "mrqa_naturalquestions-validation-5785", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-2085"], "instant_fixing_rate": 0.8181818181818182, "instant_retention_rate": 0.9523809523809523, "doing-nothing_instant_EM": 0.71875, "doing-nothing_accmulative_EM": 0.7118055555555556}, {"timecode": 9, "before_eval": {"predictions": ["constitutional right", "Aristotle", "March 23, 2013", "associations or hypothesized relationships", "2017", "Richard Crispin Armitage", "the final episode of the series", "December 24, 1836", "mobilization of the population on a scale not previously attempted in Britain, with a combined military force of over 615,000 in December 1803", "Thomas Hobbes in his Leviathan", "the status line", "1608", "the foot of biblical Mount Sinai", "a prison", "3D modeling ( or three - dimensional modeling )", "Donna Mills", "Magnetically soft ( low coercionivity ) iron", "to denote the groups of the periodic table", "A error does not count as a hit", "Amanda Leighton", "Manley", "free floating and depending upon its supply market finds or sets a value to it that continues to change as the supply of money is changed with respect to the economy's demand", "Ella Mitchell", "1938", "St. Albans, Vermont", "reinforce the front and sides of the trachea to protect and maintain the airway", "Elizabeth Lail", "displacement", "instructions", "photon energy E. Frequencies observed in astronomy range from 7023239999999999999 \u2660 2.4 \u00d7 10 Hz ( 1 GeV gamma rays ) down to the local plasma frequency of the ionized interstellar medium (", "South Dakota", "Zeebo"], "metric_results": {"EM": 0.59375, "QA-F1": 0.668717598858728}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.07407407407407408, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.0, 1.0, 0.05128205128205128, 1.0, 1.0, 0.06451612903225806, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.8, 0.0, 1.0]}}, "after_eval": {"predictions": ["constitutional right", "Aristotle", "March 23, 2013", "alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "2018", "Richard Crispin Armitage", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "December 24, 1836", "Napoleon's planned invasion of the United Kingdom", "Thomas Hobbes in his Leviathan", "the status line", "1608", "at the foot of biblical Mount Sinai", "as a royal residence", "3D modeling ( or three - dimensional modeling )", "Donna Mills, known for her portrayal of Abby Cunningham on the prime time soap, Knots Landing", "energy loss", "to denote the groups of the periodic table", "counts as an at bat for the batter unless, in the scorer's judgment, the batter would have reached first base safely but one or more of the additional base ( s ) reached was the result of the fielder's mistake", "Amanda Leighton as Poppy, the optimistic queen of the Trolls, and Branch's girlfriend", "Manley", "fiat money", "Ella Mitchell as Hattie Mae Pierce ( Big Momma )", "1933", "St. Albans, Vermont", "reinforce the front and sides of the trachea to protect and maintain the airway", "Elizabeth Mitchell", "displacement", "A computer program", "photon energy E. Frequencies observed in astronomy range from 7023239999999999999 \u2660 2.4 \u00d7 10 Hz ( 1 GeV gamma rays ) down to the local plasma frequency of the ionized interstellar medium (", "Wyoming", "Zeebo"], "metric_results": {"EM": 0.71875, "QA-F1": 0.8345056712588881}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 0.90625, 0.3076923076923077, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8062"], "retained_ids": ["mrqa_naturalquestions-validation-2242", "mrqa_naturalquestions-validation-7033", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-7996", "mrqa_naturalquestions-validation-1453", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-4323", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-971", "mrqa_naturalquestions-validation-4801", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-469"], "fixed_ids": ["mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-8796", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-2624", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-9852"], "unfixed_ids": ["mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-8700", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-4506"], "error_ids": ["mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-8700", "mrqa_naturalquestions-validation-8796", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-2624", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-9852", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-4506"], "instant_fixing_rate": 0.6153846153846154, "instant_retention_rate": 0.7894736842105263, "doing-nothing_instant_EM": 0.625, "doing-nothing_accmulative_EM": 0.703125}, {"timecode": 10, "accumulative_EM": 0.7585227272727273, "accumulative_forgotten_ids": ["mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-7197", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-5281", "mrqa_naturalquestions-validation-5119", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-1605", "mrqa_naturalquestions-validation-5063", "mrqa_naturalquestions-validation-8908", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-3732", "mrqa_naturalquestions-validation-6110", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-6821", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-4691", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-7108", "mrqa_naturalquestions-validation-10492", "mrqa_naturalquestions-validation-8488", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-10529", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-3985", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-5043", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-8734", "mrqa_naturalquestions-validation-9264", "mrqa_naturalquestions-validation-2556"], "accumulative_fixed_ids": ["mrqa_naturalquestions-validation-7707", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-816", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-8529", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-10326", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-8368", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-9885", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-10619", "mrqa_naturalquestions-validation-10162", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-4496", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-7300", "mrqa_naturalquestions-validation-8319", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-8064", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-8180", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-7792", "mrqa_naturalquestions-validation-226", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-407", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-2624", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-9852"], "accumulative_forgotten_rate": 0.13920454545454544, "accumulative_fixed_rate": 0.20454545454545456, "before_eval": {"predictions": ["the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere ), when the duration of daylight becomes noticeably shorter and the temperature cools down considerably", "the major contributor and the associated free software philosophy", "Professor Kantorek", "2002", "Barbara Windsor", "uterine tubes", "Alice", "Colman", "16", "ancient cult activity as far back as 7th century BCE", "Husrev Pasha", "Columbia University", "after the 2015 model year", "Ledger", "23", "responsible for the encouragement, regulation and enforcement of workplace health, safety and welfare, and for research into occupational risks in Great Britain", "Anthony Morris III ( 2005 )", "Napoleon Bonaparte", "Charles Sherrington", "arthropods, molluscs, roundworms, ringed worms, flatworms, and other phyla in Ecdysozoa and Spiralia", "Bill Pullman", "sex hormones", "Kaley Christine Cuoco ( / \u02c8ke\u026ali \u02c8kwo\u028ako\u028a / KAY - lee KWOH - koh ; born November 30, 1985 )", "William Jennings Bryan, Woodrow Wilson and Al Smith on the Democratic side", "to foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world", "Kyla Pratt as Young Monica", "Gina Tognoni / to\u028an\u02c8jo\u028ani / ( born November 28, 1973 )", "1998", "Eagles", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "inequality of opportunity related to gender was low in Europe and Central Asia but medium to high in respect of labour practices, employment and entrepreneurship and in access to finance", "Katherine Kiernan Maria `` Kate '' Mulgrew"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5943531388843889}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.06666666666666667, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 0.375, 0.5945945945945945, 0.4666666666666667, 0.5714285714285715, 0.4444444444444445, 0.0, 1.0, 0.4615384615384615, 0.18181818181818182, 1.0]}}, "after_eval": {"predictions": ["fall", "Richard Stallman", "Paul Baumer", "2002", "Barbara Windsor", "midpiece", "a card from a pack of playing cards by Alice, yet somehow she is able to talk and is the ruler of the lands in the story, alongside her tiny husband, the King of Hearts", "Colman", "16", "Vesta", "Husrev Pasha", "Columbia University", "after the 2015 model year", "Ledger", "23", "responsible for the encouragement, regulation and enforcement of workplace health, safety and welfare, and for research into occupational risks in Great Britain", "Anthony Morris III", "Napoleon Bonaparte", "Charles Sherrington", "a vertebral column ( spine )", "Bill Pullman", "reproductive", "Kaley Christine Cuoco", "Jane Addams, Grace Abbott, Edith Abbott and Sophonisba Breckinridge", "reduce poverty around the world", "Kyla Pratt", "Gina Tognoni", "1997", "Eagles", "toasted wheat bun", "Inequality of opportunity", "Katherine Kiernan Maria `` Kate '' Mulgrew"], "metric_results": {"EM": 1.0, "QA-F1": 1.0}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": [], "retained_ids": ["mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-7029", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-10361", "mrqa_naturalquestions-validation-6093", "mrqa_naturalquestions-validation-1", "mrqa_naturalquestions-validation-8841", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-5790", "mrqa_naturalquestions-validation-3163"], "fixed_ids": ["mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-4444", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4006", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-8734", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-4021", "mrqa_naturalquestions-validation-9264", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-8763", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3969"], "unfixed_ids": [], "error_ids": ["mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-4444", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4006", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-8734", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-4021", "mrqa_naturalquestions-validation-9264", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-8763", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3969"], "instant_fixing_rate": 1.0, "instant_retention_rate": 1.0, "doing-nothing_instant_EM": 0.59375, "doing-nothing_accmulative_EM": 0.6931818181818182}, {"timecode": 11, "before_eval": {"predictions": ["from London to Canterbury to visit the shrine of Saint Thomas Becket at Canterbury Cathedral", "The Man", "in Arrow's second season", "2017", "Thomas M\u00fcller", "September 27, 2017", "Lori Rom", "Buffalo Bill", "the art of the Persian Safavid dynasty from 1501 to 1722, in present - day Iran and Caucasia", "John Hurt", "2017", "1919", "The Maginot Line", "2015", "Allison Janney", "Rock Island, Illinois", "Wynonna Judd", "Carpenter", "Hermann Ebbinghaus", "Wales", "2013", "Subduction of the northern part of the Pacific Plate and the NW North American Plate that is forming the Aleutian Islands", "prescribe conditions for its debate and amendment", "Andy Warhol", "Camping World Stadium in Orlando, Florida", "Frankie Valli", "Texas A&M Aggies", "Jason Marsden", "43", "Ann E. Todd as Loretta Merchant", "Dr. Lexie Grey", "Bachendri Pal"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8360119047619048}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true], "QA-F1": [0.13333333333333333, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "after_eval": {"predictions": ["London", "The Man", "second season", "2017", "James Rodr\u00edguez", "September 27, 2017", "Lori Rom", "Buffalo Bill", "ceramics", "John Hurt", "2017", "1919", "Maginot Line", "2015", "Allison Janney", "Rock Island, Illinois", "Wynonna Judd", "Carpenter", "Hermann Ebbinghaus", "separately in England and Wales", "2013", "Subduction of the northern part of the Pacific Plate and the NW North American Plate that is forming the Aleutian Islands", "determining under what rule other bills will come to the floor", "Andy Warhol", "Camping World Stadium in Orlando, Florida", "Frankie Valli", "Texas A&M Aggies", "Jason Marsden", "43", "Anthony Caruso", "Dr. Lexie Grey", "Bachendri Pal"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9971590909090909}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": [], "retained_ids": ["mrqa_naturalquestions-validation-7527", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-9917", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-1853", "mrqa_naturalquestions-validation-10428", "mrqa_naturalquestions-validation-5864", "mrqa_naturalquestions-validation-5082", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-1910", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-1625", "mrqa_naturalquestions-validation-2646", "mrqa_naturalquestions-validation-651", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-10682", "mrqa_naturalquestions-validation-5346", "mrqa_naturalquestions-validation-1389", "mrqa_naturalquestions-validation-4443", "mrqa_naturalquestions-validation-13", "mrqa_naturalquestions-validation-10176", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-10268"], "fixed_ids": ["mrqa_naturalquestions-validation-9348", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-7339", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9681", "mrqa_naturalquestions-validation-3882"], "unfixed_ids": ["mrqa_naturalquestions-validation-4711"], "error_ids": ["mrqa_naturalquestions-validation-9348", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-7339", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-9681", "mrqa_naturalquestions-validation-3882"], "instant_fixing_rate": 0.8571428571428571, "instant_retention_rate": 1.0, "doing-nothing_instant_EM": 0.84375, "doing-nothing_accmulative_EM": 0.7057291666666666}, {"timecode": 12, "before_eval": {"predictions": ["Human anatomy", "a rotationally symmetric saltire", "Neil Young", "Matt Flinders", "sinoatrial node", "1997", "ITV", "along the length of peninsular Italy", "mocks the differences between what liberals say and what they do", "John B. Watson", "2015", "amphetamines", "1 US dollar", "25 December 2010", "Norman origin, deriving from the Norman given name Robert, meaning `` bright renown '' -- from the Germanic elements `` hrod '' meaning renown and `` beraht '' meaning bright", "Lionel Richie, Stevie Wonder, Paul Simon, Kenny Rogers, James Ingram, Tina Turner, and Billy Joel", "62", "Macon Blair", "Fa Ze Rug", "Solomon", "Kiss", "Simon Callow", "115", "Garwin Sanford", "Todd Bridges", "Sally Field", "Wilhelm Conrad R\u00f6ntgen", "Paul", "from the leaves of the plant species Stevia rebaudiana", "from the Old English pyrige ( pear tree ), referring to one who dwells by a pear tree", "on the southeastern coast of the Commonwealth of Virginia in the United States", "Fennel seeds"], "metric_results": {"EM": 0.625, "QA-F1": 0.7345017005978272}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5454545454545454, 1.0, 0.17391304347826084, 0.23529411764705882, 1.0, 1.0, 0.4, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.5263157894736842, 1.0, 1.0]}}, "after_eval": {"predictions": ["biochemistry", "the symbol \u00d7", "Neil Young", "Matt Flinders", "sinoatrial node", "1997", "the BBC", "along the length of peninsular Italy", "mocks the differences between what liberals say and what they do", "John B. Watson", "2017", "weight - loss amphetamines", "1 US dollar worth close to 5,770 guaranies", "25 December 2010", "Norman origin", "Kenny Rogers", "62", "Macon Blair", "Fa Ze Rug", "Solomon, king of the United Kingdom of Israel and Judah", "Kiss", "Simon Callow", "Mickey Mantle", "Garwin Sanford", "Todd Bridges", "Sally Field", "Wilhelm Conrad R\u00f6ntgen", "Philippians", "the leaves of the plant species Stevia rebaudiana", "Old English pyrige ( pear tree )", "on the southeastern coast of the Commonwealth of Virginia in the United States", "Fennel seeds"], "metric_results": {"EM": 0.90625, "QA-F1": 0.91875}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-2949"], "retained_ids": ["mrqa_naturalquestions-validation-8747", "mrqa_naturalquestions-validation-8466", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-10344", "mrqa_naturalquestions-validation-917", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-5679", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-9322", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-8128", "mrqa_naturalquestions-validation-10062", "mrqa_naturalquestions-validation-5200", "mrqa_naturalquestions-validation-5971", "mrqa_naturalquestions-validation-2162", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-4336"], "fixed_ids": ["mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-8381", "mrqa_naturalquestions-validation-4420", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-8553", "mrqa_naturalquestions-validation-4274", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-9172", "mrqa_naturalquestions-validation-6881"], "unfixed_ids": ["mrqa_naturalquestions-validation-3297"], "error_ids": ["mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-8381", "mrqa_naturalquestions-validation-4420", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-8553", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-4274", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-9172", "mrqa_naturalquestions-validation-6881"], "instant_fixing_rate": 0.9166666666666666, "instant_retention_rate": 0.9, "doing-nothing_instant_EM": 0.75, "doing-nothing_accmulative_EM": 0.7091346153846154}, {"timecode": 13, "before_eval": {"predictions": ["\u014czaru ( \u5927 \u733f, lit. `` Great Ape '' ) form", "authority", "Miami Heat", "the center core", "Warren Zevon", "a bridge over the Merderet in the fictional town of Ramelle", "James `` Jamie '' Dornan", "an unknown recipient", "an eleventh series to air in 2019", "Database - Protocol driver", "reproductive role", "$2.18 billion", "three", "Humpty Alexander Dumpty", "administrative supervision over all courts and the personnel thereof", "1665 to 1666", "1951 -- 52", "a war With the main goal of ending aggressive militarism and indeed ending all wars", "digestive systems", "Anglican", "Jonathan Breck", "17 December 1968", "August 5, 1937", "The British Indian Association", "a form of business network, for example, a local organization of businesses whose goal is to further the interests of businesses", "digestion of proteins", "1886", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "regulatory site", "Utah", "1939", "Yuzuru Hanyu"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7346861471861472}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 1.0, 0.2857142857142857, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.19999999999999998, 0.0, 1.0, 1.0, 0.06060606060606061, 1.0, 1.0]}}, "after_eval": {"predictions": ["Dragon Ball GT", "authority", "Miami Heat", "the center core", "Warren Zevon", "over the Merderet in the fictional town of Ramelle", "James `` Jamie '' Dornan", "an unknown recipient", "2019", "Database - Protocol driver ( Pure Java driver )", "endocrine ( hormonal ) systems", "over $1.84 billion", "three", "Humpty Alexander Dumpty", "administrative supervision over all courts and the personnel thereof", "1665 to 1666", "1951 -- 52", "neutrality", "digestive systems", "The Church of England", "Jonathan Breck", "17 December 1968", "August 5, 1937", "The British Indian Association", "a form of business network", "plays a key role in digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "first published in 1890", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "regulatory site", "in nearly 100 locations across Utah, including : Mount Timpanogos, Ashley National Forest, Leeds, Snow Canyon State Park, St. George, Sundance Resort, Uinta National Forest and Wasatch - Cache National Forest", "1939", "Yuzuru Hanyu"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9545115018508725}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19354838709677416, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9508196721311475, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-4519"], "retained_ids": ["mrqa_naturalquestions-validation-1290", "mrqa_naturalquestions-validation-4110", "mrqa_naturalquestions-validation-5218", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-5378", "mrqa_naturalquestions-validation-7056", "mrqa_naturalquestions-validation-8372", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-1915", "mrqa_naturalquestions-validation-6655", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-10014", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-3469", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2996", "mrqa_naturalquestions-validation-4074"], "fixed_ids": ["mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-5651", "mrqa_naturalquestions-validation-2575", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-4366"], "unfixed_ids": ["mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-126"], "error_ids": ["mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-5651", "mrqa_naturalquestions-validation-2575", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-126"], "instant_fixing_rate": 0.8181818181818182, "instant_retention_rate": 0.9523809523809523, "doing-nothing_instant_EM": 0.71875, "doing-nothing_accmulative_EM": 0.7098214285714286}, {"timecode": 14, "before_eval": {"predictions": ["Aman Gandotra and Natasha Bharadwaj", "1 -- 3 ( the category of disease )", "1960", "between $10,000 and $30,000", "1916", "Mohammad Reza Pahlavi", "season two", "Canada", "1940", "lower the normal boiling point of the liquid", "July 2012", "Tokyo", "Ramanaa", "five", "ummat al - Islamiyah", "Amanda Leighton", "allows Parliament or provincial legislatures to override certain portions of the Charter", "November 2, 2016", "a perceived harmful event, attack, or threat to survival", "in the pancreas", "the fertile crescent region", "London, United Kingdom", "Gibraltar", "Gabrielle - Suzanne Barbot de Villeneuve", "originated in the Roman Empire", "Latin liberalia studia", "Lee County", "Celtic", "Covington, Kentucky", "National Football League", "translation", "Supreme Court of Canada"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6201190476190477}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true], "QA-F1": [0.5714285714285715, 0.0, 1.0, 0.32, 0.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "after_eval": {"predictions": ["Natasha Bharadwaj", "5", "1960", "believed to cost between $10,000 and $30,000", "1945", "Cyrus", "season two", "Canada", "1940", "The higher the vapor pressure of a liquid at a given temperature", "July 2012", "Tokyo", "Tagore", "five", "ummat al - Islamiyah", "Amanda Leighton", "allows Parliament or provincial legislatures to override certain portions of the Charter", "November 2, 2016", "the adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "positions 14 - 15, 146 - 147 and 148 - 149", "Mesopotamia", "London, United Kingdom", "Gibraltar", "Gabrielle - Suzanne Barbot de Villeneuve", "the Roman Empire", "the Roman Empire", "Lee County", "Celtic", "Covington, Kentucky", "the most recent Super Bowl champions", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome ) as directed by a three - nucleotide sequence ( codon ) in a messenger RNA ( mRNA )", "the Supreme Court of Canada"], "metric_results": {"EM": 0.9375, "QA-F1": 0.953125}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": [], "retained_ids": ["mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-4855", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-8575", "mrqa_naturalquestions-validation-5213", "mrqa_naturalquestions-validation-4989", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-5458", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-6754", "mrqa_naturalquestions-validation-6076"], "fixed_ids": ["mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-1181", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-9058", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-7003"], "unfixed_ids": ["mrqa_naturalquestions-validation-5214", "mrqa_naturalquestions-validation-4768"], "error_ids": ["mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-5214", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-1181", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-9058", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-7003"], "instant_fixing_rate": 0.8571428571428571, "instant_retention_rate": 1.0, "doing-nothing_instant_EM": 0.71875, "doing-nothing_accmulative_EM": 0.7104166666666667}, {"timecode": 15, "before_eval": {"predictions": ["the Golden Gate, the one - mile - wide ( 1.6 km ) strait connecting San Francisco Bay and the Pacific Ocean", "Elena Anaya", "South Carolina", "Jacob Packer", "228", "65,535 bytes ( 8 byte header + 65,527 bytes of data )", "liquids", "Warren Hastings", "the mitochondrial inner membrane", "never made", "the pace car circles the track at pit road speed during the warm - up laps", "a 17th - century carving over a door of the famous T\u014dsh\u014d - g\u016b shrine in Nikk\u014d, Japan", "March 31, 2017", "present - day southeastern Texas", "the beginning of 2016", "Adam Caine", "2011", "Mainland Greece", "the Jurchen Aisin Gioro clan in Manchuria", "Weston - super-Mare", "Edd Kimber", "1926", "retinal ganglion cell axons and glial cells", "Judi Dench", "the immortal Hawke", "Martin Lawrence", "creating a so called minimum viable product that addresses and solves a problem or need that exists", "Sheev Palpatine", "at least a moderate amount of manual dexterity", "a number of small marsupials including the endangered sandhill dunnart ( Sminthopsis psammophila ) and the crest - tailed mulgara ( Dasycercus cristicauda", "Chilliwack", "Austin Winkler"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6284026645868751}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, false, true, false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, true, false], "QA-F1": [0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3636363636363636, 0.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.4444444444444445, 0.15384615384615383, 0.3157894736842105, 1.0, 0.0]}}, "after_eval": {"predictions": ["one - mile - wide ( 1.6 km ) strait connecting San Francisco Bay and the Pacific Ocean", "Elena Anaya", "South Carolina", "Jacob Packer", "228 minutes", "8 bytes", "solids", "Warren Hastings", "mitochondrial membrane", "never made", "pit road speed", "Nikk\u014d, Japan", "March 31, 2017", "near Arenosa Creek and Matagorda Bay", "near the end of 2015 and the beginning of 2016", "Adam Caine", "2011", "peninsula", "Manchuria", "Weston - super-Mare", "Edd Kimber", "1926", "optic chiasma", "Judi Dench", "under `` the immortal Hawke ''", "Martin Lawrence", "Mark Andreessen", "Sheev Palpatine, ( colloquial : Darth Sidious and The Emperor )", "an example of a useless, time - wasting activity", "sandhill dunnart", "Chilliwack", "Emmanuelle Chriqui"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9575520833333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [0.375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-9110"], "retained_ids": ["mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-6836", "mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-5236", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-95", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-4853", "mrqa_naturalquestions-validation-3763", "mrqa_naturalquestions-validation-8043", "mrqa_naturalquestions-validation-5259"], "fixed_ids": ["mrqa_naturalquestions-validation-7998", "mrqa_naturalquestions-validation-1787", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-8940", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-7311", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-3482"], "unfixed_ids": ["mrqa_naturalquestions-validation-3108"], "error_ids": ["mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-7998", "mrqa_naturalquestions-validation-1787", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-8940", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-7311", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-3482"], "instant_fixing_rate": 0.9411764705882353, "instant_retention_rate": 0.9333333333333333, "doing-nothing_instant_EM": 0.625, "doing-nothing_accmulative_EM": 0.705078125}, {"timecode": 16, "before_eval": {"predictions": ["under Solomon, during the united monarchy of Israel and Judah", "Michael Clarke Duncan", "Rachel Sarah Bilson", "Barry Bonds", "Charles Foster Kane, played by Welles, a character based in part upon the American newspaper magnate William Randolph Hearst, Chicago tycoons Samuel Insull and Harold McCormick, and aspects of Welles's own life", "Audrey II", "Jason Paige", "the British Isles of French and Latin origin", "The illusion experienced by visitors results from the oddly tilted environment as well as standing on a tilted floor", "Luther Ingram", "Bhupendranath Dutt", "The genome", "February 7, 2018", "backbones", "Norma's brother, Caleb", "John Young", "1992", "Mesopotamia", "G. Callen", "Tokyo", "Steve Hale", "Christina Perri", "John Cooper Clarke", "April 1917", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "Scheria", "rearview mirror", "red - billed hornbill and majordomo", "Derek Hough", "a hole, using her hind legs, and lays her eggs in it", "James Brown", "Robert Remak"], "metric_results": {"EM": 0.625, "QA-F1": 0.7108337842712843}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.22222222222222224, 0.09090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.4, 0.7499999999999999, 0.0, 0.16666666666666669, 1.0, 1.0]}}, "after_eval": {"predictions": ["the mid-10th century BCE", "Michael Clarke Duncan", "Rachel Sarah Bilson", "Barry Bonds", "William Randolph Hearst", "Audrey II", "Jason Paige", "Old French", "gravity hill, tilt - induced visual illusion", "Luther Ingram", "Bhupendranath Dutt", "The genome", "February 7, 2018", "backbones", "Norma's brother, Caleb", "John Young", "1992", "Iran", "Grisha", "Tokyo", "Steve Hale", "Christina Perri", "John Cooper Clarke", "April 1917", "Claims adjuster", "Scheria", "rear - view mirror", "red - billed hornbill", "Karina Smirnoff", "a hole", "James Brown", "Robert Remak"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9866071428571428}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": [], "retained_ids": ["mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-10019", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4155", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-9733", "mrqa_naturalquestions-validation-9332", "mrqa_naturalquestions-validation-9367", "mrqa_naturalquestions-validation-7766", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-3376", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-3502", "mrqa_naturalquestions-validation-9199", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-10180", "mrqa_naturalquestions-validation-3678"], "fixed_ids": ["mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6129", "mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-4946", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8708", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-6524"], "unfixed_ids": ["mrqa_naturalquestions-validation-1904"], "error_ids": ["mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6129", "mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-4946", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8708", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-6524"], "instant_fixing_rate": 0.9166666666666666, "instant_retention_rate": 1.0, "doing-nothing_instant_EM": 0.71875, "doing-nothing_accmulative_EM": 0.7058823529411765}, {"timecode": 17, "before_eval": {"predictions": ["Jerry Kramer", "16 or older", "Amartya Sen", "Golan Heights", "New Orleans", "Sylvia F. Porter", "AM 5778", "England", "citizens", "Jason Lee", "1987", "Confederate States Army", "Dadra and Nagar Haveli", "Detroit Tigers", "season four episode `` Run ''", "Sedimentary rock", "the 1962 novel of the same name by Madeleine L'Engle", "thylakoid membrane", "2002", "Lulu", "cheated", "Mendel", "Megan Park", "Felicity Huffman", "April 9, 2018", "at the center of the Northern Hemisphere", "2014", "February 6, 2005", "President pro tempore of the United States Senate", "Anglican", "May 3, 2005", "more than 1,000"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6541005291005291}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.07407407407407408, 1.0, 1.0, 1.0]}}, "after_eval": {"predictions": ["Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher", "16 or older", "Amartya Sen", "Golan Heights", "Speakeasies", "William Strauss and Neil Howe, well known for their generational theory, define the social generation of Boomers as that cohort born from 1943 to 1960, who were too young to have any personal memory of World War II", "AM 5778", "England", "unknown", "Dashiell Robert Parr / Dash, the Parrs'second child", "mid-March", "The Confederate States Army ( C.S.A. )", "Meghalaya", "White Sox", "the season four episode `` Run ''", "metamorphic rock", "the 1962 novel of the same name by Madeleine L'Engle", "thylakoid membrane", "2002 -- 03", "Lulu, composed by John Barry, and the lyrics to the song were written by Don Black", "Miley finally ends it with him", "Mendel", "Megan Park", "Julie Gonzalo", "April 9, 2018", "latitude 90 \u00b0 North", "2014 -- 15", "February 6, 2005", "The President pro tempore of the United States Senate", "Anglican services in Jamestown 1607, which became the established church in 1619, and culminates with the Virginia Statute for Religious Freedom in 1786", "May 3, 2005", "more than 1,000"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7743147994792732}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, false, false, true, true], "QA-F1": [0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.6666666666666666, 0.13333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407408, 0.0909090909090909, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-5926", "mrqa_naturalquestions-validation-4225", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-7579"], "retained_ids": ["mrqa_naturalquestions-validation-322", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-9940", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-1639", "mrqa_naturalquestions-validation-3666", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10342", "mrqa_naturalquestions-validation-9219", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6169"], "fixed_ids": ["mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2953"], "unfixed_ids": ["mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-10631"], "error_ids": ["mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2953", "mrqa_naturalquestions-validation-10631"], "instant_fixing_rate": 0.6923076923076923, "instant_retention_rate": 0.6842105263157895, "doing-nothing_instant_EM": 0.65625, "doing-nothing_accmulative_EM": 0.703125}, {"timecode": 18, "before_eval": {"predictions": ["rootlets ( branch roots )", "Foofa", "sovereign states", "16 seasons", "Moctezuma II", "Nala", "31 December 1960", "on the edge or underside of a fertile frond", "random - access memory ( RAM )", "the U.S. Electoral College, known as electors", "a Germanic name deriving from two stems : Rod or Hr\u014d\u00f0, meaning `` fame '', and olf meaning `` wolf ''", "Proposition 103", "September 2017", "about 1500 BC", "Michelle Stafford", "Melanie Walters", "March 2, 2016", "October 27, 2017", "unknown and it may originate in plainchant, but a 1619 attribution to John Bull is sometimes made", "Darren McGavin", "on what was then called Bedloe's Island", "a bank", "Hank Williams for MGM", "Coppolas and, technically, the Farrow / Previn / Allens", "Missouri, during the Kirtland period of Latter Day Saint history, circa 1834", "Road / Track", "Fifty Shades Darker", "the Indian Ocean near Grande Comore, Comoros Islands", "Mercedes - Benz Stadium in Atlanta, Georgia", "2009", "+, -, *, and / keys", "Muhammad Yunus"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7506200396825397}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, false, true, false, false, true], "QA-F1": [0.28571428571428575, 1.0, 0.08333333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.6666666666666666, 0.2857142857142857, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "after_eval": {"predictions": ["SURFACE AREA OF ROOTS", "Foofa", "Westphalia", "16 seasons", "Moctezuma II", "Nala", "The last National Servicemen left the armed forces in May 1963", "indusium", "random - access memory ( RAM )", "the U.S. Electoral College", "Germanic name deriving from two stems : Rod or Hr\u014d\u00f0, meaning `` fame '', and olf meaning `` wolf ''", "Proposition 103", "September 2017", "pre-Columbian times", "Michelle Stafford", "Alison Steadman", "March 2, 2016", "October 27, 2017", "unknown and it may originate in plainchant, but a 1619 attribution to John Bull is sometimes made", "Darren McGavin", "France", "a bank", "Hank Williams", "The Hustons", "Jackson County, Missouri", "Road / Track", "Fifty Shades Darker", "23 November 1996", "Mercedes - Benz Stadium in Atlanta, Georgia", "1994", "*", "Muhammad Yunus"], "metric_results": {"EM": 0.8125, "QA-F1": 0.8090277777777778}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-6507", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-3069"], "retained_ids": ["mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-777", "mrqa_naturalquestions-validation-5933", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-980", "mrqa_naturalquestions-validation-6984", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-8251", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-7780", "mrqa_naturalquestions-validation-8105", "mrqa_naturalquestions-validation-730"], "fixed_ids": ["mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-1133", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-10364"], "unfixed_ids": ["mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-4844"], "error_ids": ["mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-1133", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-10364"], "instant_fixing_rate": 0.8181818181818182, "instant_retention_rate": 0.8095238095238095, "doing-nothing_instant_EM": 0.71875, "doing-nothing_accmulative_EM": 0.7039473684210527}, {"timecode": 19, "before_eval": {"predictions": ["usually found on a page before the start of a written work", "sublimation and hence sometimes deposition is called desublimation", "The United States Secretary of State acts similarly to a foreign minister and under Executive leadership is the primary conductor of state - to - state diplomacy", "naos", "47 cents", "A tree - topper or treetopper", "white blood cell", "Steve Russell, in collaboration with Martin Graetz and Wayne Wiitanen, and programmed by Russell with assistance from others including Bob Saunders and Steve Piner", "Christopher Lloyd", "ESPN", "the last book accepted into the Christian biblical canon", "Reverse - Flash", "supply and demand", "Philip Seymour Hoffman", "The Catskill Aqueduct", "AMX - 30, main battle tank", "232", "Bengal tiger named Richard Parker", "1981", "rain or any other form of precipitation that is unusually acidic, meaning that it has elevated levels of hydrogen ions ( low pH )", "the longest rotation period ( 243 days ) of any planet in the Solar System", "Neal Dahlen", "British and French Canadian fur traders from before 1810, and American settlers from the mid-1830s, with its coastal areas north from the Columbia River frequented by ships from all nations engaged in the maritime fur trade", "acting as a carbon sink, aiding in regulating climate, purifying water, mitigating natural hazards such as floods, and serving as a genetic reserve", "tomato pur\u00e9e has a thicker consistency and a deeper flavour than sauce", "Neuropsychology", "September 13, 2012", "the illegitimate son of Ned Stark, the honorable lord of Winterfell, an ancient fortress in the North of the fictional continent of Westeros", "the south western escarpment of the Jos Plateau", "1858", "1904", "Scheidemann \u2032 s resignation"], "metric_results": {"EM": 0.375, "QA-F1": 0.4821617844715671}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.2222222222222222, 0.25000000000000006, 1.0, 1.0, 0.1, 0.0, 0.15384615384615385, 1.0, 1.0, 0.4444444444444445, 0.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.08333333333333334, 0.14285714285714288, 0.0, 0.1142857142857143, 0.4444444444444445, 0.18181818181818182, 1.0, 1.0, 0.4347826086956522, 0.0, 0.0, 1.0, 0.0]}}, "after_eval": {"predictions": ["usually found on a page before the start of a written work", "Deposition", "Commander in Chief of the United States Armed Forces", "naos", "47 cents", "a star ( representing either the Star of Bethlehem or the star of David ), finials, angels ( `` Christmas angel '' ), or fairies", "lymph", "Steve Russell", "Christopher Lloyd", "ESPN", "last book", "Professor Eobard Thawne", "capital and financial markets", "Philip Seymour Hoffman", "The Catskill Aqueduct", "Atelier de Construction d'Issy - Les - Moulineaux", "232", "Richard Parker", "1981", "Acid rain", "224.7 Earth days", "Tom Brady", "British", "purifying water", "consistency", "Neuropsychology", "September 13, 2012", "the illegitimate son of Ned Stark", "April", "In 1840", "1904", "President Friedrich Ebert"], "metric_results": {"EM": 0.96875, "QA-F1": 0.96875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": [], "retained_ids": ["mrqa_naturalquestions-validation-9209", "mrqa_naturalquestions-validation-4922", "mrqa_naturalquestions-validation-929", "mrqa_naturalquestions-validation-5345", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-4681", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-4278", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-9962", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-1504"], "fixed_ids": ["mrqa_naturalquestions-validation-8643", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-3148", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10201", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-1622"], "unfixed_ids": ["mrqa_naturalquestions-validation-2212"], "error_ids": ["mrqa_naturalquestions-validation-8643", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-3148", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10201", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-1622"], "instant_fixing_rate": 0.95, "instant_retention_rate": 1.0, "doing-nothing_instant_EM": 0.5, "doing-nothing_accmulative_EM": 0.69375}, {"timecode": 20, "accumulative_EM": 0.8571428571428571, "accumulative_forgotten_ids": ["mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-2053", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-1789", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-1950", "mrqa_naturalquestions-validation-6110", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-2435", "mrqa_naturalquestions-validation-1578", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-8488", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-6877", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-1037", "mrqa_naturalquestions-validation-9581", "mrqa_naturalquestions-validation-9672", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-8841", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-1910", "mrqa_naturalquestions-validation-2646", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-10176", "mrqa_naturalquestions-validation-4336", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-5458", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-5236", "mrqa_naturalquestions-validation-9110", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-5926", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-980", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-783"], "accumulative_fixed_ids": ["mrqa_naturalquestions-validation-7707", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-816", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-8529", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-10326", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-8368", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-4450", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-10619", "mrqa_naturalquestions-validation-10162", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-4496", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-7300", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-8180", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-7792", "mrqa_naturalquestions-validation-226", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-407", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-2624", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-9852", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-4444", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4006", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4021", "mrqa_naturalquestions-validation-8763", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-9681", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-5218", "mrqa_naturalquestions-validation-5651", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-5214", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-9058", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-1787", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-7311", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-6129", "mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-4946", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-8643", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10201", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-559"], "accumulative_forgotten_rate": 0.08035714285714286, "accumulative_fixed_rate": 0.24553571428571427, "before_eval": {"predictions": ["North Atlantic Ocean", "M\u00e1xima Zorreguieta Cerruti", "Elvis Presley", "Alex Skuby", "Battle of Antietam", "Matt Monro", "September 1959", "right side of the heart", "Sukhvinder Singh, Mahalaxmi Iyer and Vijay Prakash", "A footling breech", "July 21, 1861", "March 10, 2017", "New Mexico", "November 2, 2016", "Giorgio Vasari", "Nick Sager", "16 August 1975", "U.S.F.", "Julius Caesar", "Lead and lead dioxide", "Simon Callow", "Noel Harrison", "Aristotle", "The Sun", "Ashoka", "In 1943", "North America", "December 18, 2017", "2010", "one of the uses of money", "Edie McClurg", "thylakoid membranes"], "metric_results": {"EM": 0.625, "QA-F1": 0.7117063492063492}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4444444444444445, 1.0, 1.0, 0.13333333333333333, 0.0, 0.8]}}, "after_eval": {"predictions": ["the North Atlantic Ocean", "King Willem - Alexander", "Hugo Peretti", "Doug Pruzan", "Battle of Antietam", "Matt Monro", "September 1959", "lungs", "Vijay Prakash", "A footling breech", "July 21, 1861", "March 10, 2017", "New Mexico", "November 2, 2016", "Giorgio Vasari", "Nick Sager", "16 August 1975", "`` United States Ship '' ( USS )", "emperors", "Lead and lead dioxide", "Simon Callow", "Noel Harrison", "Bacon", "The Sun", "Ashoka", "1943", "physiographically a part of the continent of North America", "December 18, 2017", "2010", "a tradeable entity used to avoid the inconveniences of a pure barter system", "Stefanie Scott", "on the thylakoid membranes"], "metric_results": {"EM": 0.96875, "QA-F1": 0.96875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-5552"], "retained_ids": ["mrqa_naturalquestions-validation-670", "mrqa_naturalquestions-validation-5387", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-121", "mrqa_naturalquestions-validation-10350", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-9579", "mrqa_naturalquestions-validation-7564", "mrqa_naturalquestions-validation-4988", "mrqa_naturalquestions-validation-2800", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-8994", "mrqa_naturalquestions-validation-8120", "mrqa_naturalquestions-validation-4264", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-10704"], "fixed_ids": ["mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-9763", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-validation-3188", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-783"], "unfixed_ids": [], "error_ids": ["mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-9763", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-validation-3188", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-783"], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.95, "doing-nothing_instant_EM": 0.65625, "doing-nothing_accmulative_EM": 0.6919642857142857}, {"timecode": 21, "before_eval": {"predictions": ["Guwahati", "physician George Huntington", "Timmy Smith", "Lori McKenna", "the 1970s", "1623", "Kenneth O. Preston", "Raksha ( \u0930\u0915\u094d\u0937\u093e Rak\u1e63\u0101, `` protection '' ; Indian wolf )", "Isabela Moner", "The Yankees", "Lake Wales, Florida", "Marcus Atilius Regulus", "2018", "Parthenogenesis", "Friedman Billings Ramsey", "frustration with the atmosphere in the group at that time -- namely, Paul McCartney's over-assertiveness and criticism of his guitar playing, John Lennon's lack of engagement with the project and dismissal of Harrison as a songwriter", "gastrocnemius muscle", "Space is the Place", "Joanne Wheatley", "rapper Chandan Shetty", "the President", "a lower index of refraction", "A Turtle's Tale : Sammy's Adventures", "1985", "Leonardo da Vinci", "Billy Gibbons", "Gothic architecture", "glucose", "Emtienne - Jules Marey", "Dunedin, Port Chalmers and on the Otago Peninsula, Saint Bathans in Central Otago and at the Cape Campbell Lighthouse in Marlborough", "Cozonac", "United States Department of the Interior"], "metric_results": {"EM": 0.5, "QA-F1": 0.5396577380952381}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.45000000000000007, 1.0, 1.0, 1.0, 1.0, 0.0, 0.13333333333333333, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0]}}, "after_eval": {"predictions": ["1921", "physician George Huntington", "Timmy Smith", "Lori McKenna", "the 90s", "1603", "Daniel A. Dailey", "Shere Khan", "Isabela Moner", "The Yankees", "Lake Wales, Florida", "Xanthippus", "2018", "Parthenogenesis", "investment bank Friedman Billings Ramsey", "his frustration with the atmosphere in the group at that time", "gastrocnemius muscle", "cat in the hat", "Joanne Wheatley", "Chandan Shetty", "the Attorney General", "cylinder of glass or plastic that runs along the fiber's length", "Diary of a Wimpy Kid : The Long Haul", "1985", "Leonardo da Vinci", "Canadian rock band Nickelback", "piped masonry often carved in decorative patterns", "F fructose", "Ren\u00e9 Georges Hermann - Paul", "New Zealand", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak", "247.3 million"], "metric_results": {"EM": 0.78125, "QA-F1": 0.9046502976190476}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.8571428571428572, 1.0, 1.0, 1.0, 1.0, 0.8750000000000001, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-3688", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-8503"], "retained_ids": ["mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-9192", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-509", "mrqa_naturalquestions-validation-5605", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-8215", "mrqa_naturalquestions-validation-4068", "mrqa_naturalquestions-validation-1157"], "fixed_ids": ["mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-4259", "mrqa_naturalquestions-validation-3571", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-1304", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-6027"], "unfixed_ids": ["mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-4185"], "error_ids": ["mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-4259", "mrqa_naturalquestions-validation-3571", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-4185", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-1304", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-6027"], "instant_fixing_rate": 0.8125, "instant_retention_rate": 0.75, "doing-nothing_instant_EM": 0.53125, "doing-nothing_accmulative_EM": 0.6846590909090909}, {"timecode": 22, "before_eval": {"predictions": ["Moton Field", "the fifth studio album", "The Stanley Hotel", "last Ice Age", "northernmost point at which the noon sun is just visible on the December solstice", "located within nine coastal southern Nigerian states", "2 : 44 p.m. EDT", "Andy", "Thomas Middleditch", "at least 90 % of the FCS maximum of 63 scholarship equivalents over a two - year period", "Kenneth Kaunda", "on average USD 5.2 billion", "November 23, 2006", "Hanna Alstr\u00f6m and Bj\u00f8rn Floberg", "Hellenism", "Edward IV of England", "the English", "a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "James Rodr\u00edguez", "inferior thoracic border", "1960", "Mandy", "Russia", "Lou Rawls", "Gorakhpur Junction", "his friends, Humpty Dumpty and Kitty Softpaws", "Jyotirindra Basu", "Danny Veltri", "the Miracles", "16 December 1908", "George Merrill and Shannon Rubicam of the band Boy Meets Girl", "Joseph Heller"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6883030164280164}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true], "QA-F1": [0.5, 0.0, 1.0, 0.8571428571428571, 0.07692307692307691, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.5555555555555556, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.9090909090909091, 1.0]}}, "after_eval": {"predictions": ["Moton Field, the Tuskegee Army Air Field", "Help!", "The Stanley Hotel", "During the last Ice Age", "most northerly of the five major circles of latitude as shown on maps of Earth", "sitting directly on the Gulf of Guinea on the Atlantic Ocean in Nigeria", "about 6 : 44 p.m. UTC ( 2 : 44 P. m. EDT )", "Andy", "Thomas Middleditch", "5", "Edgar Lungu", "on average USD 5.2 billion and for the Winter Games USD 3.1 billion dollars", "Kansas City Chiefs", "Hanna Alstr\u00f6m", "Hellenism", "Richard, Duke of Gloucester", "the English", "technological advances in printing", "James Rodr\u00edguez", "inferior thoracic border", "1960", "Mandy", "Russia", "Lou Rawls", "Gorakhpur Junction", "Humpty Dumpty", "Jyotirindra Basu", "Danny Veltri", "the Miracles", "31 March 1909", "George Merrill and Shannon Rubicam of the band Boy Meets Girl", "Joseph Heller"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9667771464646464}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7777777777777777, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-8744"], "retained_ids": ["mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-4895", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-7321", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-5684", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-7594", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-5775", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-9505"], "fixed_ids": ["mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-4117", "mrqa_naturalquestions-validation-7801", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-2776", "mrqa_naturalquestions-validation-338", "mrqa_naturalquestions-validation-4382", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-1186"], "unfixed_ids": ["mrqa_naturalquestions-validation-2020", "mrqa_naturalquestions-validation-8008"], "error_ids": ["mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-4117", "mrqa_naturalquestions-validation-7801", "mrqa_naturalquestions-validation-2020", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-2776", "mrqa_naturalquestions-validation-338", "mrqa_naturalquestions-validation-4382", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-1186", "mrqa_naturalquestions-validation-8008"], "instant_fixing_rate": 0.8666666666666667, "instant_retention_rate": 0.9411764705882353, "doing-nothing_instant_EM": 0.65625, "doing-nothing_accmulative_EM": 0.6834239130434783}, {"timecode": 23, "before_eval": {"predictions": ["Icarus", "U.S. President John F. Kennedy", "85 %", "Renishaw Hall, Derbyshire, England, UK", "`` the quintessential New Orleans art form -- a jazz funeral without a body. ''", "Laura Jane Haddock", "Roger Federer", "The Enchantress", "Nicole Gale Anderson", "Chlorofluorocarbons ( CFCs )", "September 19, 2017", "Great Britain", "AD 95 -- 110", "on the intracellular side of plasma membrane", "1948", "parthenogenesis", "Peking", "twelve", "the pigeons", "near the city of Cairo, Illinois", "subduction zone", "to provide school districts with federal funds, in the form of competitive grants, to establish innovative educational programs for students with limited English speaking ability", "Rent's script", "1999", "the people of Britain", "any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "total cost ( TC )", "$2 million in 2011", "Canadian Rockies continental divide", "Symbolic interactionism", "books of Exodus and Deuteronomy", "reared"], "metric_results": {"EM": 0.5625, "QA-F1": 0.633234126984127}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.28571428571428575, 0.1111111111111111, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0]}}, "after_eval": {"predictions": ["Icarus", "U.S. President John F. Kennedy", "33 %", "Renishaw Hall, Derbyshire, England, UK", "Second line", "Laura Jane Haddock", "Roger Federer", "The Magician", "Nicole Gale Anderson", "Chlorofluorocarbons", "September 19, 2017", "Great Britain", "AD 95 -- 110", "the cytoskeletons of adjacent cells", "1960", "parthenogenesis", "Peking", "twelve", "kiss", "near the city of Cairo, Illinois", "subduction zone", "to provide school districts with federal funds, in the form of competitive grants, to establish innovative educational programs for students with limited English speaking ability", "script", "filming began in September 2000 at Leavesden Film Studios and in London, with production ending in July 2001", "the city of Oslo, Norway", "any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "total cost ( TC )", "$2 million in 2011", "Hudson Bay", "effective conceptualizations", "Deuteronomy", "South Africa"], "metric_results": {"EM": 0.8125, "QA-F1": 0.8345734126984127}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809525, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-10603"], "retained_ids": ["mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-1121", "mrqa_naturalquestions-validation-10408", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-7031", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-1222", "mrqa_naturalquestions-validation-2304", "mrqa_naturalquestions-validation-3982", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-10693", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-10537"], "fixed_ids": ["mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-5659", "mrqa_naturalquestions-validation-10352", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-9635", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-4744"], "unfixed_ids": ["mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8759", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-1507"], "error_ids": ["mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-5659", "mrqa_naturalquestions-validation-10352", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-8759", "mrqa_naturalquestions-validation-9635", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-1507", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-4744"], "instant_fixing_rate": 0.6428571428571429, "instant_retention_rate": 0.9444444444444444, "doing-nothing_instant_EM": 0.53125, "doing-nothing_accmulative_EM": 0.6770833333333334}, {"timecode": 24, "before_eval": {"predictions": ["physichanics of human or animal movement", "Japanese", "Owen Vaccaro", "John Donne", "Three Billboards Outside Ebbing, Missouri", "thirteen American colonies", "Randy", "the Washington metropolitan area", "one person", "January 2004", "Brazil", "1,350 at the 2010 census", "the President of the United States", "Elizabeth Dean Lail", "Kingdom of Strathclyde", "Andrew Lloyd Webber", "cells", "USS Chesapeake", "Bart Howard", "Kimberly Marie `` Kim '' Matula", "the Bee Gees", "Finance Minister of India in Parliament", "1978", "September 29, 2017", "31", "1920", "the head of the United States Department of Justice per 28 U.S.C. \u00a7 503, concerned with all legal affairs", "Dr. Hartwell Carver", "2009", "because they believed that it violated their rights as Englishmen to `` No taxation without representation ''", "a loanword of the Visigothic word guma `` man ''", "Butter Island"], "metric_results": {"EM": 0.5625, "QA-F1": 0.676118672993673}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.8095238095238095, 1.0, 1.0, 0.5769230769230769, 0.2857142857142857, 0.3636363636363636]}}, "after_eval": {"predictions": ["the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Japanese", "Owen Vaccaro", "John Donne", "Three Billboards Outside Ebbing, Missouri", "Second Continental Congress", "Donna", "FedExField in Landover, Maryland", "the hands of one person, whose decisions are subject to neither external legal restraints nor regularized mechanisms of popular control ( except perhaps for the implicit threat of a coup d'\u00e9tat or mass insurrection )", "January 2004", "Brazil", "1,350", "the President of the United States", "Elizabeth Dean Lail", "a Scottish surname", "Andrew Lloyd Webber", "nearly all living cells", "USS Chesapeake", "Bart Howard", "Kimberly Marie `` Kim '' Matula", "the Bee Gees", "the Finance Minister of India", "1977", "September 29, 2017", "a 31 - member Senate and a 150 - member House of Representatives", "1920", "the head of the United States Department of Justice per 28 U.S.C. \u00a7 503, concerned with all legal affairs, and is the chief lawyer of the U States government", "Dr. Hartwell Carver", "Christmas 2009", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "man", "Butter Island off North Haven, Maine in the Penobscot Bay"], "metric_results": {"EM": 0.84375, "QA-F1": 0.9157689950980392}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true], "QA-F1": [0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.96, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-4646", "mrqa_naturalquestions-validation-8737"], "retained_ids": ["mrqa_naturalquestions-validation-6351", "mrqa_naturalquestions-validation-6926", "mrqa_naturalquestions-validation-5406", "mrqa_naturalquestions-validation-5392", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-7696", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-926", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-4672", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9850"], "fixed_ids": ["mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-3704", "mrqa_naturalquestions-validation-1861", "mrqa_naturalquestions-validation-4042", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-10382"], "unfixed_ids": ["mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-1533", "mrqa_naturalquestions-validation-5903"], "error_ids": ["mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-3704", "mrqa_naturalquestions-validation-1861", "mrqa_naturalquestions-validation-4042", "mrqa_naturalquestions-validation-1533", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-10382"], "instant_fixing_rate": 0.7857142857142857, "instant_retention_rate": 0.8888888888888888, "doing-nothing_instant_EM": 0.6875, "doing-nothing_accmulative_EM": 0.6775}, {"timecode": 25, "before_eval": {"predictions": ["other locations in Cambridgeshire and across the United Kingdom", "1994", "primarily on Saturday evenings with a following Sunday night results show", "Spike, occasionally referred to as Butch or Killer", "eleven", "Norman Greenbaum", "Phosphorus pentoxide", "23 hours, 56 minutes, and 4 seconds", "orogenic belt", "annuity", "Mike Czerwien", "Matt Monro", "around 2 %", "crossbar", "New Croton Reservoir in Westchester and Putnam counties", "house edge of between 0.5 % and 1 %", "Brittany Paige Bouck", "4 September 1936", "Speaker of the House", "Annette Strean", "Timothy B. Schmit", "the imperial court", "Jason Weaver", "Hugo Peretti", "Night Ranger", "`` P.K. '' John Hamilton", "September 19, 2017", "Thomas Chisholm", "in the United States in 1974", "The management team", "Matt Monro", "John Rhoades"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6202967171717172}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false], "QA-F1": [0.8, 1.0, 0.33333333333333337, 0.2222222222222222, 1.0, 1.0, 1.0, 0.2, 1.0, 0.16666666666666669, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "after_eval": {"predictions": ["Cambridge University, and at other locations in Cambridgeshire and across the United Kingdom", "1994", "Sunday night", "Spike", "four", "Norman Greenbaum", "Phosphorus pentoxide", "about 24 hours", "orogenic belt", "the cash / annuity choice to be made after winning ( usually 60 days after claiming the ticket ), although in Florida the 60 - day `` clock '' starts with the drawing in which the jackpot prize was won", "Mike Czerwien", "Matt Monro", "4 percent cumulative effect", "arm", "New Croton Reservoir in Westchester and Putnam counties", "between 0.5 % and 1 %", "Brittany Paige Bouck", "4 September 1936", "Speaker of the House", "Annette Strean", "Timothy B. Schmit", "the Mandate of Heaven", "Wylie Draper", "Elvis Presley", "`` I Burn for You '' ( performed by Danny Peck and Nancy Shanks )", "Barry Watson", "September 19, 2017", "Thomas Chisholm", "1984", "The management team", "Matt Monro", "France"], "metric_results": {"EM": 0.90625, "QA-F1": 0.915625}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-6951"], "retained_ids": ["mrqa_naturalquestions-validation-1339", "mrqa_naturalquestions-validation-1736", "mrqa_naturalquestions-validation-3632", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-3466", "mrqa_naturalquestions-validation-9563", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6636", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-2097", "mrqa_naturalquestions-validation-747", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-2455", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-9666", "mrqa_naturalquestions-validation-8725"], "fixed_ids": ["mrqa_naturalquestions-validation-3524", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-6157", "mrqa_naturalquestions-validation-1514", "mrqa_naturalquestions-validation-42", "mrqa_naturalquestions-validation-10626", "mrqa_naturalquestions-validation-1258", "mrqa_naturalquestions-validation-9135"], "unfixed_ids": ["mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-504"], "error_ids": ["mrqa_naturalquestions-validation-3524", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-6157", "mrqa_naturalquestions-validation-1514", "mrqa_naturalquestions-validation-42", "mrqa_naturalquestions-validation-10626", "mrqa_naturalquestions-validation-1258", "mrqa_naturalquestions-validation-9135", "mrqa_naturalquestions-validation-504"], "instant_fixing_rate": 0.8666666666666667, "instant_retention_rate": 0.9411764705882353, "doing-nothing_instant_EM": 0.75, "doing-nothing_accmulative_EM": 0.6802884615384616}, {"timecode": 26, "before_eval": {"predictions": ["Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Isaiah Amir Mustafa", "1970", "The Buckwheat Boyz", "Audrey II", "Woodward, Oklahoma", "the attempted transformation of the Southern United States from 1863 to 1877, as directed by Congress, from states with economies dependent upon slavery, to states in which former slaves were citizens with civil rights", "either two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "Mummy Zebra", "South America", "a place of trade, entertainment, and education", "2017", "Brian Lara", "In `` A.A.R.M. ''", "1960", "a constitutional monarchy in which the power of the Emperor is limited and is relegated primarily to ceremonial duties", "two", "2026", "1773", "the United States in the late - 19th and early - 20th centuries", "the neuromuscular junction", "all productive processes", "1989", "1999", "Vasoepididymostomy", "Niveditha, Diwakar, Shruti", "201", "Kelly Reno", "1 August 1965", "Eydie Gorm\u00e9", "matiasma", "American production duo The Chainsmokers"], "metric_results": {"EM": 0.625, "QA-F1": 0.7404716849246773}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.5714285714285715, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.9473684210526316, 0.0, 0.19354838709677416, 0.0, 1.0, 1.0, 0.5, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "after_eval": {"predictions": ["Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Isaiah Amir Mustafa", "1970", "The Buckwheat Boyz", "Audrey II", "Woodward, Oklahoma", "the attempted transformation of the Southern United States from 1863 to 1877, as directed by Congress, from states with economies dependent upon slavery, to states in which former slaves were citizens with civil rights", "peptide bond", "Zoe Zebra", "South America", "a place of trade, entertainment, and education", "2017", "Alastair Cook", "`` Goodbye Toby ''", "1960", "constitutional monarchy", "two", "2026", "1773", "in the United States in the late - 19th and early - 20th centuries", "muscles", "in all productive processes, adding more of one factor of production, while holding all others constant ( `` ceteris paribus '' ), will at some point yield lower incremental per - unit returns", "1991", "1999", "Vasoepididymostomy", "Shruti", "As of January 17, 2018, 201 episodes", "Kelly Reno", "1 August 1965", "Eydie Gorm\u00e9", "matiasma", "American production duo The Chainsmokers"], "metric_results": {"EM": 0.96875, "QA-F1": 0.99375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": [], "retained_ids": ["mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-425", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-8236", "mrqa_naturalquestions-validation-10671", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-9890", "mrqa_naturalquestions-validation-8421", "mrqa_naturalquestions-validation-5578", "mrqa_naturalquestions-validation-124"], "fixed_ids": ["mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-6635", "mrqa_naturalquestions-validation-7013", "mrqa_naturalquestions-validation-10246", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-4280"], "unfixed_ids": ["mrqa_naturalquestions-validation-10354"], "error_ids": ["mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-6635", "mrqa_naturalquestions-validation-7013", "mrqa_naturalquestions-validation-10246", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-4280"], "instant_fixing_rate": 0.9166666666666666, "instant_retention_rate": 1.0, "doing-nothing_instant_EM": 0.75, "doing-nothing_accmulative_EM": 0.6828703703703703}, {"timecode": 27, "before_eval": {"predictions": ["1978", "15 December 2017", "the plane crash in 1959 which killed early rock and roll performers Buddy Holly, The Big Bopper, and Ritchie Valens", "The British", "Dr. Rajendra Prasad", "Lesley Gore", "Organisms in the domains of Archaea and Bacteria", "Cairo, Illinois", "Smith Jerrod", "Mirror Image", "Randy VanWarmer", "Emma Thompson", "efficient and effective management of money ( funds ) in such a manner as to accomplish the objectives of the organization", "water ice", "the most recent Super Bowl champion", "Terrell Suggs", "Michael Moriarty", "the Berlin School of experimental psychology", "Arizona ( except for the Navajo, who do observe daylight saving time on tribal lands ), Hawaii, and the overseas territories of American Samoa, Guam, the Northern Mariana Islands, Puerto Rico, and, the United States Virgin Islands", "The Osmonds", "American actor and drag queen Divine, who was best known for his frequent appearances in several films directed by filmmaker John Waters", "1952", "April 12, 2017", "September 28, 2017", "Jerry Ekandjo", "The onset of rigor mortis and its resolution partially determine the tenderness of meat", "Jean - Paul Valley ( a.k.a. Azrael )", "2010", "the Senate", "Francisco Pizarro", "Bob Dylan", "25 - yard line"], "metric_results": {"EM": 0.5625, "QA-F1": 0.662823547979798}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.5909090909090909, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.375]}}, "after_eval": {"predictions": ["1978", "2018", "the plane crash in 1959 which killed early rock and roll performers Buddy Holly, The Big Bopper, and Ritchie Valens", "the French", "Dr. Rajendra Prasad", "Lesley Gore", "prokaryotic", "Cairo, Illinois", "waiter turned emerging young actor Smith Jerrod", "`` Mirror Image ''", "American singer - songwriter Randy VanWarmer", "Alice Eve", "accomplish the objectives of the organization", "water ice", "the most recent Super Bowl champion", "Mike Czerwien, Waynesburg University, 2002 -- 04", "Michael Moriarty", "experimental psychology", "Hawaii", "The Osmonds", "American actor and drag queen Divine", "1952", "April 12, 2017", "September 28, 2017", "Erastus Utoni", "tenderness of meat", "Jean - Paul Valley ( a.k.a. Azrael )", "2010", "the President", "Francisco Pizarro", "Bob Dylan", "a touchback on kickoffs at the 25 - yard line instead of the previous 20 - Yard line"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9866071428571428}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-3308"], "retained_ids": ["mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-5936", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-255", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-8391", "mrqa_naturalquestions-validation-3209", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-2311", "mrqa_naturalquestions-validation-6252", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-1248"], "fixed_ids": ["mrqa_naturalquestions-validation-3341", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-5439", "mrqa_naturalquestions-validation-732", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-7335", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-9105"], "unfixed_ids": [], "error_ids": ["mrqa_naturalquestions-validation-3341", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-5439", "mrqa_naturalquestions-validation-732", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-7335", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-9105"], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9444444444444444, "doing-nothing_instant_EM": 0.625, "doing-nothing_accmulative_EM": 0.6808035714285714}, {"timecode": 28, "before_eval": {"predictions": ["1987", "Roman Reigns", "Middle Eastern alchemy", "pools campaign contributions from members and donates those funds to campaign for or against candidates, ballot initiatives, or legislation", "to alleviate musculoskeletal pain and spasms and to reduce spasticity in a variety of neurological conditions", "Sylvester Stallone", "1979", "from the northern California coast north to the southern Oregon Coast", "Andy Serkis", "biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "in the fictional town of West Egg on prosperous Long Island in the summer of 1922", "July 4, 1898", "Havana Harbor", "Chernobyl Nuclear Power Plant near the now - abandoned town of Pripyat, in northern Ukrainian Soviet Socialist Republic, Soviet Union, approximately 104 km ( 65 mi ) north of Kiev", "Meghalaya", "Patrick Swayze", "Catherine Tramell", "three", "to cross the world's oceans for centuries, and enabled European empire expansion into the Americas and trade routes to become established across the Atlantic and Pacific oceans", "The Intolerable Acts", "West Norse", "Kerris Lilla Dorsey", "part husky or other Nordic breed, and possibly part terrier", "Missouri River", "14", "Sean O'Neal", "San Antonio", "temporal lobes", "because chlorine and bromine from manmade organohalogens", "American rock band Panic! at the Disco", "23 % of GDP, and employed 59 % of the country's total workforce in 2016", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7168441698096156}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 0.782608695652174, 1.0, 1.0, 0.2666666666666667, 0.04878048780487806, 1.0, 1.0, 1.0, 0.8292682926829268, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.6666666666666666, 0.15384615384615385, 0.47058823529411764]}}, "after_eval": {"predictions": ["1996", "Roman Reigns", "Middle Eastern alchemy", "pools campaign contributions from members and donates those funds to campaign for or against candidates, ballot initiatives, or legislation", "to alleviate musculoskeletal pain and spasms and to reduce spasticity in a variety of neurological conditions", "Sylvester Stallone", "1979", "from the northern California coast north to the southern Oregon Coast", "Andy Serkis", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "the fictional town of West Egg on prosperous Long Island", "July 4, 1898", "Havana Harbor", "Chernobyl Nuclear Power Plant", "part of the present Indian constitutive state of Meghalaya ( formerly Assam )", "Patrick Swayze", "Catherine Tramell", "three", "enabled European empire expansion into the Americas and trade routes to become established across the Atlantic and Pacific oceans", "The Intolerable Acts", "West Norse sailors", "Kerris Lilla Dorsey", "mongrel female", "Missouri River", "14", "Sean O'Neal", "the Alamodome", "temporal lobes", "chlorine and bromine from manmade organohalogens", "Panic! at the Disco", "23 %", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation"], "metric_results": {"EM": 0.84375, "QA-F1": 0.8945903361344538}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.47058823529411764]}}, "forgotten_ids": [], "retained_ids": ["mrqa_naturalquestions-validation-851", "mrqa_naturalquestions-validation-2396", "mrqa_naturalquestions-validation-961", "mrqa_naturalquestions-validation-2563", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-6574", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-8340", "mrqa_naturalquestions-validation-4559", "mrqa_naturalquestions-validation-5440", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-8857", "mrqa_naturalquestions-validation-2273", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-2226"], "fixed_ids": ["mrqa_naturalquestions-validation-9184", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-2194", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-2739"], "unfixed_ids": ["mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-5826"], "error_ids": ["mrqa_naturalquestions-validation-9184", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-2194", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-5826"], "instant_fixing_rate": 0.6153846153846154, "instant_retention_rate": 1.0, "doing-nothing_instant_EM": 0.71875, "doing-nothing_accmulative_EM": 0.6821120689655172}, {"timecode": 29, "before_eval": {"predictions": ["the pyloric valve", "Disney must pay royalties for all future use of the characters", "William Wyler", "17 August 1945", "GTPase", "nine", "Super Bowl LII", "`` Feed Jake '' is a song written by Danny `` Bear '' Mayo, and recorded by the American country music band Pirates of the Mississippi", "Staci Keanan", "a seer", "Real Madrid", "September 6, 2019", "126 by Wilt Chamberlain from October 19, 1961 -- January 19, 1963", "American singer Selena Gomez", "Noel Kahn", "from 35 to 40 hours per week", "up to 100,000 present in a single human cell", "Himax HX7309 LCoS display", "March 11, 2018", "March 9, 2018", "December 9, 2016", "the NCAA tournament", "George Strait", "a mixture of phencyclidine and cocaine", "the primal rib", "two years and eight months for men ( with some roles requiring an additional four months of service ), and two years for women", "Rob Reiner", "an American multinational retail corporation", "G. Hannelius", "Himadri Station", "maquiladora", "Mariah Carey"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7253943972693973}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5384615384615384, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 0.6666666666666666, 1.0, 0.923076923076923, 1.0, 0.7272727272727273, 1.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0]}}, "after_eval": {"predictions": ["the pyloric valve", "the Slesinger family", "William Wyler", "17 August 1945", "scission", "nine", "Super Bowl LII", "written by Danny `` Bear '' Mayo, and recorded by the American country music band Pirates of the Mississippi", "Staci Keanan", "a haruspex named Spurinna", "Real Madrid", "September 6, 2019", "126", "Selena Gomez", "Noel Kahn", "35 to 40 hours per week", "up to 100,000 present in a single human cell", "640 \u00d7 360 Himax HX7309 LCoS display", "March 11, 2018", "March 9, 2018", "30 days after the original air date", "2003", "George Strait", "a mixture of phencyclidine and cocaine", "the primal rib", "two years and eight months for men ( with some roles requiring an additional four months of service ), and two years for women", "Rob Reiner", "multinational retail corporation", "G. Hannelius", "Himadri Station", "special economic zones", "Mariah Carey"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9383912655971479}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.05882352941176471, 1.0, 1.0, 0.6363636363636364, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": [], "retained_ids": ["mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-5219", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-2479", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5597", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-937", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-7936", "mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-1070", "mrqa_naturalquestions-validation-8080", "mrqa_naturalquestions-validation-400", "mrqa_naturalquestions-validation-1127", "mrqa_naturalquestions-validation-3682"], "fixed_ids": ["mrqa_naturalquestions-validation-2545", "mrqa_naturalquestions-validation-7234", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-2740", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-756", "mrqa_naturalquestions-validation-5146", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3004"], "unfixed_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-7838", "mrqa_naturalquestions-validation-5437"], "error_ids": ["mrqa_naturalquestions-validation-2545", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-7838", "mrqa_naturalquestions-validation-7234", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-2740", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-756", "mrqa_naturalquestions-validation-5146", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-5437", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3004"], "instant_fixing_rate": 0.7692307692307693, "instant_retention_rate": 1.0, "doing-nothing_instant_EM": 0.6875, "doing-nothing_accmulative_EM": 0.6822916666666666}, {"timecode": 30, "accumulative_EM": 0.8397177419354839, "accumulative_forgotten_ids": ["mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-1789", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9588", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-5119", "mrqa_naturalquestions-validation-684", "mrqa_naturalquestions-validation-3000", "mrqa_naturalquestions-validation-1535", "mrqa_naturalquestions-validation-6110", "mrqa_naturalquestions-validation-4442", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-1578", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-8488", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-5787", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-7362", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-9672", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-1", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-10428", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-2646", "mrqa_naturalquestions-validation-10176", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-4336", "mrqa_naturalquestions-validation-4110", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-1915", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-1181", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-5458", "mrqa_naturalquestions-validation-5236", "mrqa_naturalquestions-validation-95", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-3678", "mrqa_naturalquestions-validation-9940", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-5926", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-7579", "mrqa_naturalquestions-validation-6169", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-980", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-9209", "mrqa_naturalquestions-validation-121", "mrqa_naturalquestions-validation-8994", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-3982", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-6351", "mrqa_naturalquestions-validation-3466", "mrqa_naturalquestions-validation-747", "mrqa_naturalquestions-validation-8725", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-8625", "mrqa_naturalquestions-validation-7153", "mrqa_naturalquestions-validation-375"], "accumulative_fixed_ids": ["mrqa_naturalquestions-validation-7707", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-816", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-3927", "mrqa_naturalquestions-validation-8529", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-10326", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-8368", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-9885", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-10619", "mrqa_naturalquestions-validation-10162", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-4496", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-7300", "mrqa_naturalquestions-validation-8064", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-8180", "mrqa_naturalquestions-validation-7792", "mrqa_naturalquestions-validation-226", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-407", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-9852", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-4444", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4006", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-4021", "mrqa_naturalquestions-validation-8763", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-9681", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-5218", "mrqa_naturalquestions-validation-5651", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-5214", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-9058", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-1787", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-7311", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-6129", "mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-4946", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-8643", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10201", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-validation-3188", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-3571", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-4117", "mrqa_naturalquestions-validation-7801", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-2776", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-1186", "mrqa_naturalquestions-validation-8008", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-5659", "mrqa_naturalquestions-validation-10352", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-8759", "mrqa_naturalquestions-validation-9635", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-3704", "mrqa_naturalquestions-validation-1533", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-6157", "mrqa_naturalquestions-validation-10626", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-6635", "mrqa_naturalquestions-validation-7013", "mrqa_naturalquestions-validation-10246", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-5439", "mrqa_naturalquestions-validation-732", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3209", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-7335", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-9184", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-2545", "mrqa_naturalquestions-validation-7234", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-5146", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-6912", "mrqa_naturalquestions-validation-276"], "accumulative_forgotten_rate": 0.08770161290322581, "accumulative_fixed_rate": 0.25, "before_eval": {"predictions": ["a Gram - negative, facultatively anaerobic, rod - shaped, coliform bacterium of the genus Escherichia that is commonly found in the lower intestine of warm - blooded organisms ( endotherms )", "Ben Fransham", "in the retina of mammalian eyes", "American figure skater Gillis Grafstr\u00f6m", "Joseph Sherrard Kearns", "an individual's rights which, although not legally binding in themselves, have been elaborated in subsequent international treaties, economic transfers, regional human rights instruments, national constitutions, and other laws", "William Alan `` Will '' Friedle", "1", "absolute temperature", "economic", "elsewhere on the North Shore, at locations in Beverly, Essex, Gloucester, Swampscott, Lynn, Middleton, Tewksbury, and Salem", "Parashara", "cool all the atmosphere by spraying the whole atmosphere as if drawing letters in the air ( `` penciling '' )", "a security feature for `` card not present '' payment card transactions instituted to reduce the incidence of credit card fraud", "8.7 -- 9.2", "the president", "1979 -- 80 season", "October 17, 2017", "King James II & VII", "1980", "The Miracles", "Shirley Mae Jones", "Tim Duncan leads the National Basketball Association ( NBA ) in the points - rebounds combination with 840", "lacteal", "2018", "1960", "C\u03bc and C\u03b4 ) gene segments", "Roanoke", "tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "the University of Michigan campus at Ann Arbor, Michigan", "a male given name originating in the Hebrew name Immanu'el ( \u05e2\u05b4\u05de\u05b8\u05bc\u05e0\u05d5\u05bc\u05d0\u05b5\u05dc \u202c, which means `` God with us. ''", "October 15, 1997"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5140118643611291}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, false, false, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, false, false, false, false, true, false, false], "QA-F1": [0.08, 1.0, 0.25, 0.0, 1.0, 0.05714285714285714, 1.0, 1.0, 0.0, 0.0, 0.11764705882352941, 1.0, 0.13333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.07692307692307693, 1.0, 0.0, 0.3333333333333333]}}, "after_eval": {"predictions": ["Escherichia coli", "Ben Fransham", "in the fovea centralis, a 0.3 mm diameter rod - free area with very thin, densely packed cones which quickly reduce in number towards the periphery of the retina", "Canadian ice dancers Tessa Virtue and Scott Moir", "Joseph Sherrard Kearns", "consists of thirty articles affirming an individual's rights", "William Alan `` Will '' Friedle", "1", "volume", "Central Germany", "elsewhere on the North Shore, at locations in Beverly, Essex, Gloucester, Swampscott, Lynn, Middleton, Tewksbury, and Salem", "Parashara", "penciling", "a security feature for `` card not present '' payment card transactions instituted to reduce the incidence of credit card fraud", "The 1700 Cascadia earthquake", "Chief Election Commissioner", "1979 -- 80 season", "October 17, 2017", "English", "1980", "The Miracles", "Shirley Mae Jones", "Tim Duncan", "lacteal", "2018", "Super Bowl LII, their fourth NFL title, after winning in 1948, 1949, and 1960", "V", "American Horror Story : Roanoke", "Indo - Pacific", "the University of Michigan campus at Ann Arbor, Michigan", "possibly brought from the Byzantine Empire ( as \u039c\u03b1\u03bd\u03bf\u03c5\u03ae\u03bb ) to Spain and Portugal, where it has been used since at least the 13th century", "July 8, 1997"], "metric_results": {"EM": 0.84375, "QA-F1": 0.898733660130719}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.7999999999999999, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-276"], "retained_ids": ["mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-9208", "mrqa_naturalquestions-validation-2592", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-5102", "mrqa_naturalquestions-validation-6912", "mrqa_naturalquestions-validation-10459", "mrqa_naturalquestions-validation-5481", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-3122"], "fixed_ids": ["mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-8625", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-7153", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-10612", "mrqa_naturalquestions-validation-6365"], "unfixed_ids": ["mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7938", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-2190"], "error_ids": ["mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-7938", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-8625", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-7153", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-10612", "mrqa_naturalquestions-validation-6365"], "instant_fixing_rate": 0.7647058823529411, "instant_retention_rate": 0.9333333333333333, "doing-nothing_instant_EM": 0.53125, "doing-nothing_accmulative_EM": 0.6774193548387096}, {"timecode": 31, "before_eval": {"predictions": ["Deuteronomy 5 : 4 -- 25", "light novel series written by Patora Fuyuhara and illustrated by Eiji Usatsuka", "Buffalo Lookout", "203 members, elected for two - year terms from single member districts", "Kevin Kline", "September 19, 2017", "Joaquin Phoenix", "composed by Turner Layton with lyrics by Henry Creamer", "March 1, 2018", "Jewel Akens", "Las Vegas, Nevada", "the last million years", "on the Cap - Vert peninsula on the Atlantic coast and is the westernmost city in the Old World as well as on the African mainland", "Justice Harlan", "as of October 1, 2015, when the green class A was retired", "Spanish / Basque origin", "all Americans", "the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing", "Islamic shrine located on the Temple Mount in the Old City of Jerusalem", "March 6, 2018", "growth factors or growth hormones", "from January to May 2014", "in a thousand years", "4 January 2011", "the root respiration", "1988", "Sarah Silverman", "mashed potato", "between the front ranges of the Rocky Mountains on the east and the Cascade Range and Sierra Nevada on the west", "Justin Timberlake", "15 February 1998", "Samantha Jo `` Mandy '' Moore"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6168503487253487}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.18181818181818182, 0.3636363636363636, 0.0, 1.0, 1.0, 0.6, 0.5185185185185185, 1.0, 0.0, 1.0, 1.0, 0.4799999999999999, 0.14285714285714288, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "after_eval": {"predictions": ["Deuteronomy 5 : 4 -- 25", "Isekai wa Sum\u0101tofon to Tomo ni", "Buffalo Lookout", "203", "Kevin Kline", "September 19, 2017", "Joaquin Phoenix as Cash, Reese Witherspoon as Carter, Ginnifer Goodwin as Vivian Liberto, and Robert Patrick as Cash's father", "Turner Layton", "September 21, 2017", "Jewel Akens", "Las Vegas, Nevada", "In the last million years since the Late Miocene", "on the Cap - Vert peninsula on the Atlantic coast", "Justice Harlan", "during initial entry training", "Spanish / Basque origin", "all Americans", "the direction from which the wind is blowing", "Dome of the Rock", "March 6, 2018", "Plant hormones", "London, England", "During his epic battle with Frieza", "4 January 2011", "ATP", "1988", "Sarah Silverman", "mashed potato", "between the front ranges of the Rocky Mountains on the east and the Cascade Range and Sierra Nevada on the west", "Justin Timberlake", "1994", "Samantha Jo `` Mandy '' Moore"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9975961538461539}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9230769230769231, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": [], "retained_ids": ["mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-8485", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-4800", "mrqa_naturalquestions-validation-4597", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10389", "mrqa_naturalquestions-validation-8528", "mrqa_naturalquestions-validation-10204", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-7404", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-10039"], "fixed_ids": ["mrqa_naturalquestions-validation-3386", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-9094", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2076", "mrqa_naturalquestions-validation-4360", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-5542", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-9591"], "unfixed_ids": ["mrqa_naturalquestions-validation-9227"], "error_ids": ["mrqa_naturalquestions-validation-3386", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-9227", "mrqa_naturalquestions-validation-9094", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2076", "mrqa_naturalquestions-validation-4360", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-5542", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-9591"], "instant_fixing_rate": 0.9333333333333333, "instant_retention_rate": 1.0, "doing-nothing_instant_EM": 0.71875, "doing-nothing_accmulative_EM": 0.6787109375}, {"timecode": 32, "before_eval": {"predictions": ["16.5 quadrillion BTUs", "the contestant", "during the night Montgomery has also destroyed the only boats on the island", "at least 18 or 21 years old ( or have a legal guardian present ), and must sign a waiver prior to shooting", "mainly of Anglo - Saxon origin", "Michael Christopher McDowell", "Tandi, in Lahaul", "Andrea Brooks", "Washington Redskins", "SS - 4 construction site at San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "his fellow actor, Richard Burbage", "Gravity, or gravitation", "Alan Shearer", "Garbi\u00f1e Muguruza", "13 May 1787", "Elijah, Rebekah, Klaus and Davina surrounding Kol as he dies", "Bohrium", "either on the surface of scales or leaves, which are often modified to form cones, or solitary as in Yew, Torreya, Ginkgo", "the judiciary", "Sloane Stephens", "Ed Sheeran", "Canadian singer Justin Bieber", "Tyrann Devine Mathieu", "Jay Baruchel", "help bring creative projects to life", "up to three terms until 2015 where the three term limit and two year terms were replaced with a two four - year terms", "Joe Turano", "10 May 1940", "Wimbledon, London", "Washington metropolitan area", "Geoffrey Zakarian", "Chemistry professor E.H.S. Bailey"], "metric_results": {"EM": 0.4375, "QA-F1": 0.596813326387817}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true, false, true, true, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 0.06451612903225806, 0.08333333333333333, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.8387096774193548, 0.5263157894736842, 0.5, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.32, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "after_eval": {"predictions": ["30.4 % in 2016 Coal supplied 16.5 quadrillion BTUs of primary energy to electric power plants in 2013, which made up nearly 92 % of coal's contribution to energy supply", "the contestant makes a thirty - second call to one of a number of friends ( who provide their phone numbers in advance ) and reads them the question and answer choices, after which the friend provides input", "Montgomery, his servant M'ling, and the Sayer of the Law die after a scuffle with the Beast Folk", "Typically, no license or advanced training beyond just firearm familiarization ( for rentals ) and range rules familiarization", "Anglo - Saxon origin", "Michael Christopher McDowell", "Tandi, in Lahaul", "Andrea Brooks", "Washington Redskins", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "almost certainly wrote his version of the title role for his fellow actor, Richard Burbage", "Gravity, or gravitation", "Alan Shearer ( Blackburn Rovers, 1994 -- 95 )", "Garbi\u00f1e Muguruza", "18 January 1788", "Kol", "Bohrium", "either on the surface of scales or leaves", "judges", "Sloane Stephens", "Ed Sheeran", "Singh H Spot", "Tyrann Devine Mathieu", "Jay Baruchel", "global crowdfunding platform focused on creativity and merchandising", "two four - year terms", "Frederick in a duet with Teresa James", "10 May 1940", "London", "Washington metropolitan area", "Geoffrey Zakarian", "the university's science club in 1886"], "metric_results": {"EM": 0.8125, "QA-F1": 0.8794701042684914}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false], "QA-F1": [0.19354838709677416, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727272]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-10032"], "retained_ids": ["mrqa_naturalquestions-validation-3862", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-5268", "mrqa_naturalquestions-validation-7937", "mrqa_naturalquestions-validation-6947", "mrqa_naturalquestions-validation-4611", "mrqa_naturalquestions-validation-8263", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-7097"], "fixed_ids": ["mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-894", "mrqa_naturalquestions-validation-965", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-2270", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-2439", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-6814", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-787", "mrqa_naturalquestions-validation-6167", "mrqa_naturalquestions-validation-8147"], "unfixed_ids": ["mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-8707"], "error_ids": ["mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-894", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-965", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-2270", "mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-2439", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-6814", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-787", "mrqa_naturalquestions-validation-6167", "mrqa_naturalquestions-validation-8147", "mrqa_naturalquestions-validation-8707"], "instant_fixing_rate": 0.7777777777777778, "instant_retention_rate": 0.8571428571428571, "doing-nothing_instant_EM": 0.625, "doing-nothing_accmulative_EM": 0.6770833333333334}, {"timecode": 33, "before_eval": {"predictions": ["Cody Fern", "Tara / Ghost of Christmas Past", "PPG Paints Arena, Pittsburgh, Pennsylvania ( Host : Duquesne University )", "Humayun", "John Wesley ( 1703 -- 1791 ) and his younger brother Charles ( 1707 -- 1788 )", "Mercedes - Benz Stadium in Atlanta, Georgia", "pigs", "Harlem River", "2016", "seven", "Kansas City Chiefs", "Beijing, China", "Portuguese", "the praises of Whig candidates William Henry Harrison ( the `` hero of Tippecanoe '' ) and John Tyler, while denigrating incumbent Democrat Martin Van Buren", "Veronica Lodge instead of Betty Cooper", "Inti, the sun god of the Inca religion", "2014 Olympic Games in Sochi, Krasnodar Krai, Russia", "37.7", "Marty Robbins", "Deuteronomy ( Dvarim ) 5 : 1 -- 23", "London's West End in 1986", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "startup neutron source", "1956", "Ganesh Chaturthi", "between the left atrium and the left ventricle", "four", "Hon July Moyo", "Pakhangba", "Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "Sicily", "between the lungs, in the middle compartment of the chest"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7010404156775125}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.4, 0.0, 0.08, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.7096774193548387, 0.2857142857142857, 0.2, 0.2222222222222222, 0.0, 1.0, 1.0, 0.33333333333333337, 0.8387096774193548, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0]}}, "after_eval": {"predictions": ["Cody Fern", "Tara / Ghost of Christmas Past", "PPG Paints Arena, Pittsburgh, Pennsylvania ( Host : Duquesne University )", "Babur", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "Mercedes - Benz Stadium in Atlanta, Georgia", "pigs", "Harlem River", "2007", "seven", "Kansas City Chiefs", "Beijing, China", "Portuguese", "Whig candidates William Henry Harrison ( the `` hero of Tippecanoe '' ) and John Tyler", "Betty instead of to Veronica", "the May Revolution of 1810", "2014", "won gold in the half - pipe", "Marty Robbins", "Deuteronomy ( Dvarim ) 5 : 1 -- 23", "1986", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "startup neutron source", "1956", "Ganesh", "between the left atrium and the left ventricle", "four", "Hon July Moyo", "Pakhangba", "Dean Martin", "Sicily", "between the lungs, in the middle compartment of the chest"], "metric_results": {"EM": 0.875, "QA-F1": 0.9345430107526882}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8387096774193548, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": [], "retained_ids": ["mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-2338", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-9813", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-9001", "mrqa_naturalquestions-validation-3275", "mrqa_naturalquestions-validation-3273", "mrqa_naturalquestions-validation-8653", "mrqa_naturalquestions-validation-9277", "mrqa_naturalquestions-validation-9971", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-150", "mrqa_naturalquestions-validation-5081"], "fixed_ids": ["mrqa_naturalquestions-validation-6482", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6765", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-8343", "mrqa_naturalquestions-validation-6968"], "unfixed_ids": ["mrqa_naturalquestions-validation-3058", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-10353"], "error_ids": ["mrqa_naturalquestions-validation-3058", "mrqa_naturalquestions-validation-6482", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6765", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-8343", "mrqa_naturalquestions-validation-6968"], "instant_fixing_rate": 0.7142857142857143, "instant_retention_rate": 1.0, "doing-nothing_instant_EM": 0.65625, "doing-nothing_accmulative_EM": 0.6764705882352942}, {"timecode": 34, "before_eval": {"predictions": ["Madhya Pradesh", "an expression of Priestley's socialist political principles", "muscles", "EVR Race", "Jessica Simpson", "Jules Shear", "B. Traven", "in the north - east of South Africa, in the eastern parts of Limpopo and Mpumalanga provinces", "Bartle Frere", "2018", "Generally more expensive", "relief", "a temporary release of a prisoner who agrees to certain conditions before the completion of the maximum sentence period", "Geraldine Margaret Agnew - Somerville", "the primacy of a culturally distinct core area, centered on the Yellow River valley", "Erika Mitchell Leonard", "the adrenal medulla", "Teri Hatcher as Mel Jones", "Walter Egan", "September 2014", "1546", "push the food down the esophagus", "Drew tells Brianna that he and Rick want to adopt her, and she is thrilled", "R.H. Thomson", "Grand Inquisitor", "I Write Sins Not Tragedies", "3 - phosphoglycerate", "Michael Edwards ( briefly as the older Connor ) and then by teenage actor Edward Furlong throughout the remainder of the film", "Kimberlin Brown", "Latin liberalia studia", "2003", "the polar climate there is more ice or snow on the ground"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6590687216872395}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.35294117647058826, 1.0, 0.5714285714285715, 1.0, 0.33333333333333337, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 0.0, 0.0, 0.0]}}, "after_eval": {"predictions": ["Madhya Pradesh", "a scathing critique of the hypocrisies of Victorian / Edwardian English society and as an expression of Priestley's socialist political principles", "muscles", "EVR Race", "Jessica Simpson", "Jules Shear", "bilingual German author B. Traven", "in the north - east of South Africa, in the eastern parts of Limpopo and Mpumalanga provinces", "Cetshwayo", "2018", "non-ferrous", "both natural and man - made features", "a temporary release of a prisoner who agrees to certain conditions before the completion of the maximum sentence period", "Geraldine Margaret Agnew - Somerville", "the primacy of a culturally distinct core area, centered on the Yellow River valley, as distinguished from the tribal periphery", "Erika Mitchell Leonard ( born 7 March 1963 ), known by her pen name E.L. James", "the adrenal medulla", "Teri Hatcher", "Walter Egan", "Microsoft Windows, PlayStation 4, and Xbox One in September 2014", "1546", "push the food down the esophagus", "Rick", "R.H. Thomson", "Grand Inquisitor", "I Write Sins Not Tragedies", "3 - phosphoglycerate", "Michael Edwards ( briefly as the older Connor ) and then by teenage actor Edward Furlong throughout the remainder of the film", "Kimberlin Brown", "artes liberales", "2003", "a low concentration in pigmentation"], "metric_results": {"EM": 0.8125, "QA-F1": 0.863034991324465}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 0.6923076923076924, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 0.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-7200"], "retained_ids": ["mrqa_naturalquestions-validation-7317", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-2613", "mrqa_naturalquestions-validation-2602", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-2626", "mrqa_naturalquestions-validation-3765", "mrqa_naturalquestions-validation-6723", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-9328", "mrqa_naturalquestions-validation-1436", "mrqa_naturalquestions-validation-6810", "mrqa_naturalquestions-validation-2500", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-2857", "mrqa_naturalquestions-validation-6699", "mrqa_naturalquestions-validation-2745"], "fixed_ids": ["mrqa_naturalquestions-validation-6212", "mrqa_naturalquestions-validation-6388", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-2866", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-6214"], "unfixed_ids": ["mrqa_naturalquestions-validation-5212", "mrqa_naturalquestions-validation-5272", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-8175"], "error_ids": ["mrqa_naturalquestions-validation-5212", "mrqa_naturalquestions-validation-5272", "mrqa_naturalquestions-validation-6212", "mrqa_naturalquestions-validation-6388", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-2866", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-6214"], "instant_fixing_rate": 0.6153846153846154, "instant_retention_rate": 0.9473684210526315, "doing-nothing_instant_EM": 0.625, "doing-nothing_accmulative_EM": 0.675}, {"timecode": 35, "before_eval": {"predictions": ["J. Presper Eckert and John William Mauchly's ENIAC, but was initially omitted so that it could be finished sooner", "along the Californian coast at The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "Walter Damrosch", "Janie Crawford", "`` Everywhere '' is a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "The president", "around 1600 BC ( possibly a fragmentary copy of a text from 2500 BC )", "Matt Czuchry as Senior Resident Dr. Conrad Hawkins   Emily VanCamp as Nurse Nicolette `` Nic '' Nevin, a nurse practitioner", "Robert James Anderson", "12 times", "Texas A&M University", "1940", "node example.com as a sub domain of the com domain", "Cephalopoda with squids, cuttlefish and nautiloids", "Meg Tilly as Jeanie McMahon, McMahon's wife", "an integral membrane protein that builds up a proton gradient across a biological membrane", "Tristan Rogers", "Todd Griffin", "Eddie Perfect", "Ricky Nelson", "Montreal Canadiens", "Richard Masur", "international orange", "Cincinnati, filming at The Christ Hospital", "a spherical boundary of zero thickness in which photons that move on tangents to that sphere would be trapped in a circular orbit about the black hole", "1960s", "Jacques Cartier", "tabby", "H CO ( equivalently OC ( OH )", "Ministry of Corporate Affairs", "nitrogenous base, a five - carbon sugar ( ribose or deoxyribose ), and at least one phosphate group", "a list of actions or event steps typically defining the interactions between a role ( known in the Unified Modeling Language as an actor ) and a system to achieve a goal"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5896304444824182}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true], "QA-F1": [0.09090909090909091, 1.0, 0.33333333333333337, 1.0, 0.1, 0.4, 0.9523809523809523, 0.21052631578947367, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.25, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "after_eval": {"predictions": ["John von Neumann", "along the Californian coast at The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "maestro Walter Damrosch and great Russian composer Pyotr Ilyich Tchaikovsky", "Janie Crawford", "Everywhere", "the President of the United States", "1600 BC ( possibly a fragmentary copy of a text from 2500 BC )", "Matt Czuchry", "Robert James Anderson", "five", "Texas A&M University", "1908", "subdomain", "class Cephalopoda", "Meg Tilly", "an integral membrane protein that builds up a proton gradient across a biological membrane", "Tristan Rogers", "Todd Griffin", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "Ricky Nelson", "Montreal Canadiens", "Richard Masur", "an orange vermilion called international orange", "Cincinnati", "a radius 1.5 times the Schwarzschild radius", "1969", "Jacques Cartier", "tabby", "H CO ( equivalently OC ( OH ) )", "Indian government", "Nucleotides", "a list of actions or event steps typically defining the interactions between a role ( known in the Unified Modeling Language as an actor ) and a system to achieve a goal"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9831730769230769}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-7710"], "retained_ids": ["mrqa_naturalquestions-validation-2250", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-6919", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-5207", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-8654", "mrqa_naturalquestions-validation-6240", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-1299", "mrqa_naturalquestions-validation-9338", "mrqa_naturalquestions-validation-9594", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-1749"], "fixed_ids": ["mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3115", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5445", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2663", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-2992", "mrqa_naturalquestions-validation-1577", "mrqa_naturalquestions-validation-2895", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-9324"], "unfixed_ids": [], "error_ids": ["mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3115", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5445", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2663", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-2992", "mrqa_naturalquestions-validation-1577", "mrqa_naturalquestions-validation-2895", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-9324"], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9333333333333333, "doing-nothing_instant_EM": 0.625, "doing-nothing_accmulative_EM": 0.6736111111111112}, {"timecode": 36, "before_eval": {"predictions": ["Bonnie Hunt", "Welsh poet Dylan Thomas ( 1914 -- 1953 )", "Computer simulation", "the court from its members for a three - year term", "Philippe Petit", "a chimera ( a mixture of several animals ), who would probably be classified as a carnivore overall", "Bette Midler", "20", "display interactive web pages, online games, and to playback video and audio content", "April 1917", "India", "Atlanta Falcons, the San Francisco 49ers, the Dallas Cowboys, the Washington Redskins and the Baltimore Ravens, winning the Super Bowl with both the 49ers and the Cowboys", "Q'eqchi", "September 1972", "Spencer Davis Group", "336", "Automobile drivetrains", "the United Kingdom", "a warrior, mage, or rogue coming from an elven, human, or dwarven background", "a dramatic expansion in the size of the orchestra and in the dynamic range and diversity of instruments used in this ensemble", "Joseph M. Scriven", "Ireland", "Valmiki", "Lucknow", "September 8, 2017", "21.8 %", "the Jurchen Aisin Gioro clan in Manchuria", "a protocol ( http ), a hostname ( www.example.com ), and a file name ( index. html )", "October 2008", "twin", "sedimentary rocks that accumulate along the margins of continents", "subdural hematoma ( SDH )"], "metric_results": {"EM": 0.625, "QA-F1": 0.7287866181011342}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true], "QA-F1": [0.0, 0.8, 1.0, 0.2222222222222222, 1.0, 0.14285714285714288, 1.0, 1.0, 0.962962962962963, 1.0, 1.0, 0.25806451612903225, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.2857142857142857, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0]}}, "after_eval": {"predictions": ["John Goodman", "Welsh poet Dylan Thomas", "Computer simulation", "the court", "Philippe Petit", "a chimera ( a mixture of several animals ), who would probably be classified as a carnivore overall", "Bette Midler", "20", "to display interactive web pages, online games, and to playback video and audio content", "April 1917", "India", "New York Yankees, the Atlanta Braves, the Cincinnati Reds and the San Francisco Giants", "K'iche '", "September 1972", "Spencer Davis Group", "336", "Automobile drivetrains", "the United Kingdom", "a warrior, mage, or rogue coming from an elven, human, or dwarven background", "a dramatic expansion in the size of the orchestra and in the dynamic range and diversity of instruments used in this ensemble", "Joseph M. Scriven", "Ireland", "traditionally ascribed to the Hindu sage Valmiki", "Lucknow", "September 8, 2017", "approximately 21.8 % of the world's population", "the Qianlong Emperor", "a protocol ( http ), a hostname ( www.example.com ), and a file name ( index. html )", "October 2008", "twin", "a convergent plate boundary", "subdural hematoma"], "metric_results": {"EM": 0.875, "QA-F1": 0.9385551948051948}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727272, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.8]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-7369"], "retained_ids": ["mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-2542", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-5854", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6186", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-8059", "mrqa_naturalquestions-validation-2512", "mrqa_naturalquestions-validation-1962", "mrqa_naturalquestions-validation-2887", "mrqa_naturalquestions-validation-8754", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-5164"], "fixed_ids": ["mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-8825", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-8853", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-5522", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-8699"], "unfixed_ids": ["mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-3035", "mrqa_naturalquestions-validation-8229"], "error_ids": ["mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-8825", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-8853", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-5522", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-3035", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-8699"], "instant_fixing_rate": 0.75, "instant_retention_rate": 0.95, "doing-nothing_instant_EM": 0.6875, "doing-nothing_accmulative_EM": 0.6739864864864865}, {"timecode": 37, "before_eval": {"predictions": ["1980s", "Walter Pauk", "May 18, 2018", "a charbagh", "Texas", "Tachycardia", "habitat", "a tropical desert climate", "New England ( Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont )   Division 2 : Mid-Atlantic ( New Jersey, New York, and Pennsylvania )", "Tiffany Adams Coyne", "1961", "2 % naturally blond to 16 % in the US", "If These Dolls Could Talk", "two - third of the total members present", "minced meat ( commonly beef when named cottage pie or lamb when named shepherd's pie ), typically cooked in a gravy with onions and sometimes other vegetables, such as peas, celery or carrots, and topped with mashed potato", "the 1840s", "Medicaid", "ecological regions", "Triple threat", "the pilot", "James I", "Kevin Spacey", "George Strait", "substitute good", "$66.5 million in 2014", "Joseph Stalin", "Hasmukh Adhia", "Annette", "head - up display", "DeWayne Warren", "microscopic channels which traverse the cell walls of plant cells and some algal cells, enabling transport and communication between them", "Ewan McGregor"], "metric_results": {"EM": 0.625, "QA-F1": 0.7554029163404163}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.9333333333333333, 1.0, 0.4615384615384615, 0.1081081081081081, 1.0, 0.08333333333333333, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "after_eval": {"predictions": ["in the 1980s", "Walter Pauk, an education professor at Cornell University", "May 18, 2018", "Agra", "Texas", "Tachycardia", "species", "a tropical desert climate, K\u00f6ppen classification Bwh, because of its location within the Northern desert belt", "Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "Tiffany Adams Coyne", "1961", "from 2 % naturally blond to 16 % in the US", "If These Dolls Could Talk", "majority of members present at that time", "minced meat ( commonly beef when named cottage pie or lamb when named shepherd's pie ), typically cooked in a gravy with onions and sometimes other vegetables, such as peas, celery or carrots, and topped with mashed potato", "the 1840s", "Medicaid, provides for the states to finance health care for individuals who were at or close to the public assistance level with federal matching funds", "ecological regions", "Triple threat", "Waylon Jennings, Tommy Allsup, and Carl Bunch", "James I, King of England", "Kevin Spacey", "George Strait", "substitute good", "$66.5 million in 2014", "Joseph Stalin", "Hasmukh Adhia", "Annette", "a head - up display", "De Wayne Warren", "microscopic channels which traverse the cell walls of plant cells and some algal cells, enabling transport and communication between them", "Padm\u00e9 Amidala"], "metric_results": {"EM": 0.71875, "QA-F1": 0.8223939012541954}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.5, 1.0, 0.35294117647058826, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1081081081081081, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-2980", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-9508"], "retained_ids": ["mrqa_naturalquestions-validation-2165", "mrqa_naturalquestions-validation-9216", "mrqa_naturalquestions-validation-4501", "mrqa_naturalquestions-validation-3899", "mrqa_naturalquestions-validation-1049", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-5041", "mrqa_naturalquestions-validation-9991", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-3734", "mrqa_naturalquestions-validation-3192", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-750", "mrqa_naturalquestions-validation-4061"], "fixed_ids": ["mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-8509", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6168", "mrqa_naturalquestions-validation-5550"], "unfixed_ids": ["mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-5753"], "error_ids": ["mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-8509", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-6168", "mrqa_naturalquestions-validation-5753", "mrqa_naturalquestions-validation-5550"], "instant_fixing_rate": 0.5833333333333334, "instant_retention_rate": 0.8, "doing-nothing_instant_EM": 0.59375, "doing-nothing_accmulative_EM": 0.671875}, {"timecode": 38, "before_eval": {"predictions": ["Duisburg", "Ravi Shastri", "the American Revolutionary War because Great Britain ceded the land in question to the United States in the Treaty of Paris ( 1783 )", "at slightly different times when viewed from different points on Earth", "Lana Del Rey", "six", "Chicago metropolitan area", "Animal fibers", "Eric Clapton", "a means of restarting play after a minor infringement", "NFL owners", "Nathan Hale", "August 5, 1937", "John Fitzgerald, Chief Deputy Attorney General Charlie McGuigan, and attorney and 2014 U.S. Senate candidate Jason Ravnsborg", "Queenstown ( now Cobh ) in Ireland", "scythe", "Dan Stevens", "eight", "A 30 - something man ( XXXX ), is a London underworld criminal who has established himself as one of the biggest cocaine suppliers in the city, with effective legitimate cover", "9", "Doreen Mantle", "Kristy Swanson", "Lana Del Rey", "Mary Chapin Carpenter", "National Training Center at StubHub Center in Carson, California", "Jessica Simpson", "Germany", "1951 -- 52", "a subarachnoid cisterns via three small foramina : the central median aperture and the two lateral apertures", "terrestrial biosphere", "No Secrets", "a maritime signal, indicating that the vessel flying it is about to leave, and Reed chose the name to represent'a voyage of adventure'on which the programme would set out"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7290723019670388}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.10526315789473684, 0.060606060606060615, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8333333333333333, 1.0, 1.0, 1.0, 0.6399999999999999, 0.0, 1.0, 1.0, 1.0, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6111111111111112]}}, "after_eval": {"predictions": ["Duisburg", "Ravi Shastri", "1783", "when the Moon's ecliptic longitude and the Sun's Ecliptics longitude differ by 0 \u00b0, 90 \u00b0, 180 \u00b0, and 270 \u00b0, respectively", "Lana Del Rey", "six", "Chicago metropolitan area", "silk", "George Harrison", "restarting play after a minor infringement", "NFL owners", "Nathan Hale", "August 5, 1937", "Lawrence County State's Attorney John Fitzgerald, Chief Deputy Attorney General Charlie McGuigan, and attorney and 2014 U.S. Senate candidate Jason Ravnsborg", "about 375 miles ( 600 km ) south of Newfoundland", "scythe", "Dan Stevens", "eight", "XXXX", "9", "Doreen Mantle", "Kristy Swanson", "Lana Del Rey", "Mary Chapin Carpenter", "Los Angeles Galaxy", "Jessica Simpson", "Germany", "1951 -- 52", "the fourth ventricle", "terrestrial biosphere", "Lana Del Rey", "a maritime signal, indicating that the vessel flying it is about to leave"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9487889983579639}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8095238095238095, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5517241379310345, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-1829"], "retained_ids": ["mrqa_naturalquestions-validation-2703", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-2989", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-8727", "mrqa_naturalquestions-validation-3226", "mrqa_naturalquestions-validation-4245", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-7319", "mrqa_naturalquestions-validation-3423", "mrqa_naturalquestions-validation-1610", "mrqa_naturalquestions-validation-9113", "mrqa_naturalquestions-validation-7123", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-7669", "mrqa_naturalquestions-validation-9188", "mrqa_naturalquestions-validation-2599", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-2087", "mrqa_naturalquestions-validation-8502"], "fixed_ids": ["mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-3310", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-8061", "mrqa_naturalquestions-validation-7632", "mrqa_naturalquestions-validation-1301"], "unfixed_ids": ["mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-8544"], "error_ids": ["mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-3310", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-8544", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-8061", "mrqa_naturalquestions-validation-7632", "mrqa_naturalquestions-validation-1301"], "instant_fixing_rate": 0.8181818181818182, "instant_retention_rate": 0.9523809523809523, "doing-nothing_instant_EM": 0.78125, "doing-nothing_accmulative_EM": 0.6746794871794872}, {"timecode": 39, "before_eval": {"predictions": ["a destroyer's railgun called in by Simmons", "15 March every year", "vanishing point", "copper", "Ki Toy Johnson", "18", "novelization", "2002", "Jack Gleeson", "As of December 10, 2017 ( 2017 - 12 - 10 ), 205 original episodes of Keeping Up with the Kardashians have aired concluding the fourteen seasons", "utilizes highly reduced chemical compounds such as NADH and FADH2 ( for example produced during glycolysis and the citric acid cycle ) to establish an electrochemical gradient ( often a proton gradient ) across a membrane", "Eastern Division Champion, 2017 Georgia Bulldogs football team against the Western Division Co-Champion, the 2017 Auburn Tigers football team", "when a population temporarily exceeds the long term carrying capacity of its environment", "John Pulling", "Hathi Jr.", "in the early 1980s", "163 episodes", "a 2.26 GHz quad - core Snapdragon 800 processor", "various American Indian forces", "All Souls'Day", "Bactrian", "during the 1890s Klondike Gold Rush, when strong sled dogs were in high demand", "1 October 2006", "in the central plains", "proscribes conduct perceived as threatening, harmful, or otherwise endangering to the property, health, safety, and moral welfare of people", "Comptroller General of the Receipt and Issue of Her Majesty's Exchequer", "Akshay Kumar", "2018", "1945", "2014", "Kyla Coleman", "1995"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6849451991721729}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0909090909090909, 0.05714285714285715, 0.21052631578947367, 1.0, 0.4, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.0, 0.28571428571428575, 0.25, 0.5555555555555556, 1.0, 0.8, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "after_eval": {"predictions": ["Ravage and the Decepticon Rampage", "15 March every year", "vanishing point", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "Ki Toy Johnson", "18", "film", "2002", "Jack Gleeson ( born 20 May 1992 ) is an Irish former actor, best known for his portrayal of Joffrey Baratheon in the HBO television series Game of Thrones", "fourteen", "ATP synthesis from ADP and inorganic phosphate", "Georgia Bulldogs football team against the Western Division Co-Champion, the 2017 Auburn Tigers football team", "when a population temporarily exceeds the long term carrying capacity of its environment", "the church sexton Robert Newman and Captain John Pulling", "Hathi Jr.", "in the very late 1980s", "163 episodes", "2.26 GHz quad - core Snapdragon 800 processor", "Cherokee", "All Saints ( or All Hallows )", "Bactrian", "during the 1890s Klondike Gold Rush", "1 October 2006", "the central plains", "proscribes conduct perceived as threatening, harmful, or otherwise endangering to the property, health, safety, and moral welfare of people", "The comptroller ( who is also auditor general and head of the National Audit Office )", "Akshay Kumar", "2018", "1945", "2014", "Kyla Coleman from Lacey, Washington", "1995"], "metric_results": {"EM": 0.8125, "QA-F1": 0.885854828042328}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.14814814814814814, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-5571"], "retained_ids": ["mrqa_naturalquestions-validation-722", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-7138", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-7333", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-6341", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-7894", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-4033", "mrqa_naturalquestions-validation-2491"], "fixed_ids": ["mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-53", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-1364"], "unfixed_ids": ["mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-1069", "mrqa_naturalquestions-validation-8046"], "error_ids": ["mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-53", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-1069", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-1364"], "instant_fixing_rate": 0.7333333333333333, "instant_retention_rate": 0.8823529411764706, "doing-nothing_instant_EM": 0.625, "doing-nothing_accmulative_EM": 0.6734375}, {"timecode": 40, "accumulative_EM": 0.8262195121951219, "accumulative_forgotten_ids": ["mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7197", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-6109", "mrqa_naturalquestions-validation-1789", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-684", "mrqa_naturalquestions-validation-1535", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-6110", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-7108", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-8488", "mrqa_naturalquestions-validation-7816", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-10529", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-3985", "mrqa_naturalquestions-validation-5043", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-6763", "mrqa_naturalquestions-validation-9672", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-1", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-7527", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-8466", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-8553", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-4336", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-5458", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-9367", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-9940", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-5926", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-9219", "mrqa_naturalquestions-validation-980", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-7780", "mrqa_naturalquestions-validation-2800", "mrqa_naturalquestions-validation-8994", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-5775", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-3982", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-6351", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-3466", "mrqa_naturalquestions-validation-747", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-5578", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-8391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-4559", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1070", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-10032", "mrqa_naturalquestions-validation-7200", "mrqa_naturalquestions-validation-6810", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-2542", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-1069", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-5334", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-9677"], "accumulative_fixed_ids": ["mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-816", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-8529", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-10326", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-8368", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-9885", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4450", "mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-10619", "mrqa_naturalquestions-validation-10162", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-4496", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-7300", "mrqa_naturalquestions-validation-8319", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-8180", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-7792", "mrqa_naturalquestions-validation-226", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-407", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-2624", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-9852", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-4444", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4006", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-4021", "mrqa_naturalquestions-validation-8763", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-9681", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-8747", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-5651", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-9058", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-7311", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-6129", "mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-4946", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3069", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-8643", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-10201", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-9763", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-validation-3188", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-3571", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-4117", "mrqa_naturalquestions-validation-7801", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-2776", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-1186", "mrqa_naturalquestions-validation-8008", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-5659", "mrqa_naturalquestions-validation-10352", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-9635", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-3704", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-6157", "mrqa_naturalquestions-validation-10626", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-7013", "mrqa_naturalquestions-validation-10246", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-5439", "mrqa_naturalquestions-validation-732", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-9105", "mrqa_naturalquestions-validation-9184", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2545", "mrqa_naturalquestions-validation-7234", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-5146", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-5437", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-6912", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-10612", "mrqa_naturalquestions-validation-6365", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-2270", "mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-2439", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-787", "mrqa_naturalquestions-validation-6167", "mrqa_naturalquestions-validation-6482", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6765", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-8653", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-5212", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-6212", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-3115", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2663", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-5522", "mrqa_naturalquestions-validation-8059", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-8509", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6168", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-3310", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-8061", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-53", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6341", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-1364"], "accumulative_forgotten_rate": 0.08765243902439024, "accumulative_fixed_rate": 0.23551829268292682, "before_eval": {"predictions": ["Carol Worthington", "2008", "Kit Harington", "Narendra Modi", "about 175 Ma", "Kevin Spacey", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "31", "Nicole DuPort", "Spain", "presidential representative democratic republic", "Leeza Miller", "Alfred Thomas `` Freddie '' Highmore", "Bonnie Lipton", "Angel Benitez", "23 September 1889", "BC Jean and Toby Gad", "March 2016", "Tulsa, Oklahoma", "Brian Johnson", "in the 1820s", "two", "Baltimore, Maryland", "September 2017", "Ben Faulks", "supporting the natural rights of self - defense and resistance to oppression", "the President of the United States", "Ole Einar Bj\u00f8rndalen", "the virtual band the Archies", "Dmitri Ivanenko", "May 2002", "funicular railways with two connected railway cars on inclined tracks"], "metric_results": {"EM": 0.71875, "QA-F1": 0.8353136446886447}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.22222222222222218, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4444444444444445, 1.0, 0.6, 1.0, 1.0, 0.5, 1.0, 1.0, 0.15384615384615383]}}, "after_eval": {"predictions": ["Carol Worthington", "2008", "Kit Harington", "Narendra Modi", "Pangaea", "Kevin Spacey", "the director's own approved edit", "31", "Nicole DuPort", "Spain", "presidential representative democratic republic", "Joe Pizzulo and Leeza Miller", "Alfred Thomas `` Freddie '' Highmore", "Twisty the Clown ( portrayed by John Carroll Lynch )", "Angel Benitez", "23 September 1889", "BC Jean and Toby Gad", "March 2016", "Tulsa, Oklahoma", "Brian Johnson", "1820s", "two", "Baltimore, Maryland", "BETA game was released in September 2017", "Ben Faulks", "the natural rights of self - defense and resistance to oppression, and the civic duty to act in concert in defense of the state", "the President of the United States", "Ole Einar Bj\u00f8rndalen", "the Archies", "Werner Heisenberg", "May 2002", "An elevator with a counterbalance"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9449404761904762}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-4246"], "retained_ids": ["mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-6590", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-283", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-5635", "mrqa_naturalquestions-validation-4263", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-2241", "mrqa_naturalquestions-validation-8118", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-5548", "mrqa_naturalquestions-validation-7889", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-2112", "mrqa_naturalquestions-validation-6777"], "fixed_ids": ["mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-896", "mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-2730"], "unfixed_ids": ["mrqa_naturalquestions-validation-3342", "mrqa_naturalquestions-validation-5334"], "error_ids": ["mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-3342", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-5334", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-896", "mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-2730"], "instant_fixing_rate": 0.7777777777777778, "instant_retention_rate": 0.9565217391304348, "doing-nothing_instant_EM": 0.875, "doing-nothing_accmulative_EM": 0.6783536585365854}, {"timecode": 41, "before_eval": {"predictions": ["Zeus", "Florida, where new arrival Roy makes two oddball friends and a bad enemy, and joins an effort to stop construction of a pancake house which would destroy a colony of burrowing owls who live on the site", "Gibraltar", "16,801 students in 12 separate colleges / schools, including the Leonard M. Miller School of Medicine in Miami's Health District, a law school on the main campus, and the Rosenstiel School of Marine and Atmospheric Science", "United Nations", "~ 3.5 million years old from Idaho, USA", "The Bellamy Brothers", "Texhoma", "synchronizes the difference between the rotation of the front and rear wheels, and may contain one or more sets of low range gears for off - road use", "Forbes Burnham", "Abid Ali Neemuchwala", "James I, King of England", "New York Yankees", "2001", "Judy Collins'version charted in 1975 and 1977", "Tu'i Malila, a radiated tortoise", "the Feast of the Divine Mercy, celebrated each year on first Sunday after Easter", "Judi Dench", "Thomas Edison", "Epithelial tissues", "generally on a bread plate, sometimes in the napkin ), napkin, and flatware ( knives and spoons to the right of the central plate, and forks to the left )", "1999", "18, set by the Chicago White Stockings ( now the Cubs ) against the Detroit Wolverines on September 6, 1883", "the cast", "No. 1 seed Virginia and No. 4 seed Arizona", "1973", "Interphase", "R.E.M.", "1923", "at the hour of death or in the presence of the dying", "Glycogen", "Roman Reigns"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6325737995557075}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.06060606060606061, 1.0, 0.12121212121212122, 1.0, 0.7272727272727273, 0.5, 1.0, 0.7368421052631579, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.125, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "after_eval": {"predictions": ["Zeus", "Florida", "Gibraltar", "16,801", "United Nations", "~ 3.5 million years old", "American country music duo The Bellamy Brothers", "Texhoma", "transfers power from the transmission to the front and rear axles", "Forbes Burnham", "Abid Ali Neemuchwala", "James I", "New York Yankees", "2001", "Glynis Johns", "tortoise", "Good Friday", "Judi Dench", "Thomas Edison", "Epithelium", "at each place", "October 2000", "18", "the cast", "UMBC", "1973", "Interphase", "R.E.M.", "1923", "at the hour of death or in the presence of the dying", "glucose", "Roman Reigns"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9895833333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": [], "retained_ids": ["mrqa_naturalquestions-validation-10562", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-592", "mrqa_naturalquestions-validation-8931", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-10343", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-8328", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-143", "mrqa_naturalquestions-validation-4818", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-1990", "mrqa_naturalquestions-validation-8568"], "fixed_ids": ["mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-5511", "mrqa_naturalquestions-validation-2719", "mrqa_naturalquestions-validation-6189", "mrqa_naturalquestions-validation-6118", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-validation-7459", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-10469", "mrqa_naturalquestions-validation-1556", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-9726"], "unfixed_ids": ["mrqa_naturalquestions-validation-3427"], "error_ids": ["mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-5511", "mrqa_naturalquestions-validation-2719", "mrqa_naturalquestions-validation-6189", "mrqa_naturalquestions-validation-6118", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-validation-7459", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-10469", "mrqa_naturalquestions-validation-1556", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-9726"], "instant_fixing_rate": 0.9333333333333333, "instant_retention_rate": 1.0, "doing-nothing_instant_EM": 0.8125, "doing-nothing_accmulative_EM": 0.6815476190476191}, {"timecode": 42, "before_eval": {"predictions": ["Waylon Jennings", "Treme ( 2011 ) - Captain John Guidry ( 4 Episodes )", "the goal lines, in front of the end zones", "Wilt Chamberlain", "red lead primer and a lead - based topcoat", "Kenny Rogers and the First Edition", "T.J. Miller", "near the city of Alessandria, in Piedmont, Italy", "Primary storage ( also known as main memory or internal memory )", "Kathy Najimy", "Dolby Theatre in Hollywood, Los Angeles, California", "Crohn's disease or ulcerative colitis", "SUV", "Numbers 22 : 28", "The Fifth Amendment and Fourteenth Amendment to the United States Constitution", "the Noahic Covenant", "Sir Hugh Beaver", "Alan Shearer", "21 February", "Raymond Unwin", "Mary Elizabeth Patterson", "1858", "Parietal cells ( also known as oxyntic or delomorphous cells )", "Johannes Gutenberg", "Ann Gillespie", "Chesapeake Bay, south of Annapolis in Maryland", "the narrator reflects on his life as a troubadour", "South Africa", "Justice A.K Mathur", "Lizzy Greene", "Effy", "February 14, 2015"], "metric_results": {"EM": 0.625, "QA-F1": 0.7146386599511599}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, false, true], "QA-F1": [0.0, 0.0, 0.25, 1.0, 0.25, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5833333333333334, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "after_eval": {"predictions": ["Carl Belew", "The Man ( 1 Episode )", "17 yards ( 7 yards in Canadian football ) longer than the distance of the line of scrimmage to the goal line", "Wilt Chamberlain", "red lead primer and a lead - based topcoat", "Kenny Rogers and the First Edition", "T.J. Miller", "near the city of Alessandria, in Piedmont, Italy", "Primary storage ( also known as main memory or internal memory ), often referred to simply as memory", "Kathy Najimy", "Dolby Theatre in Hollywood, Los Angeles, California", "high - output fistula, very severe Crohn's disease or ulcerative colitis, and certain pediatric GI disorders including congenital GI anomalies and necrotizing enterocolitis", "sport utility vehicles and off - road vehicles", "Numbers 22 : 22", "The Fifth Amendment and Fourteenth Amendment to the United States Constitution", "the Noahic Covenant", "Sir Hugh Beaver", "Alan Shearer", "21 February", "The planner Raymond Unwin and the architect Barry Parker", "Mary Elizabeth Patterson", "1757", "the gastric glands found in the lining of the fundus and in the body of the stomach", "Johannes Gutenberg", "Ann Gillespie", "Chesapeake Bay, south of Annapolis in Maryland", "the narrator reflects on his life as a troubadour, feeling that he is content with what he has accomplished", "South Africa", "Justice A.K Mathur", "Lizzy Greene", "Michelle", "February 14, 2015"], "metric_results": {"EM": 0.84375, "QA-F1": 0.9276041666666666}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-230"], "retained_ids": ["mrqa_naturalquestions-validation-6668", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-6683", "mrqa_naturalquestions-validation-10520", "mrqa_naturalquestions-validation-3858", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7043", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-2524", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-7268", "mrqa_naturalquestions-validation-2005", "mrqa_naturalquestions-validation-1231", "mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-10372", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-3440"], "fixed_ids": ["mrqa_naturalquestions-validation-840", "mrqa_naturalquestions-validation-10270", "mrqa_naturalquestions-validation-8002", "mrqa_naturalquestions-validation-2214", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-4513", "mrqa_naturalquestions-validation-7778", "mrqa_naturalquestions-validation-9675"], "unfixed_ids": ["mrqa_naturalquestions-validation-7567", "mrqa_naturalquestions-validation-1617", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-2282"], "error_ids": ["mrqa_naturalquestions-validation-840", "mrqa_naturalquestions-validation-7567", "mrqa_naturalquestions-validation-10270", "mrqa_naturalquestions-validation-1617", "mrqa_naturalquestions-validation-8002", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-2214", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-4513", "mrqa_naturalquestions-validation-7778", "mrqa_naturalquestions-validation-9675"], "instant_fixing_rate": 0.6666666666666666, "instant_retention_rate": 0.95, "doing-nothing_instant_EM": 0.78125, "doing-nothing_accmulative_EM": 0.6838662790697675}, {"timecode": 43, "before_eval": {"predictions": ["$72", "0 \u00b0", "Spanish", "the left ring finger", "Aaron Rodgers", "January 1, 1976", "20 November 1989 ( the 30th anniversary of its Declaration of the Rights of the Child )", "Ford", "to start fires, hunt, and bury their dead", "later in the 1970s", "frontal lobe", "1957", "Brooke Wexler", "1834", "Ghost Island", "Kate Walsh", "Grey matter ( or gray matter )", "The cella of the Parthenon", "Patrick Walshe", "The Republic of Tecala", "Massillon Jackson High School", "the Spanish Empire", "October 27, 1904", "August 18, 1998", "1671", "The enthalpy of fusion of a substance", "Africa", "1872", "biscuit", "in the attempt to discover first principles --'those universal principles which are the condition of the possibility of the existence of anything and everything '", "1813", "Britain"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6878316536404772}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5454545454545454, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.4, 0.5882352941176471, 0.0, 0.6666666666666666, 0.0, 0.07692307692307691, 1.0, 0.0]}}, "after_eval": {"predictions": ["$72", "the 180th meridian in a 360 \u00b0 - system", "Spanish", "the left ring finger mostly for men", "Aaron Rodgers", "January 1, 1976", "20 November 1989", "Ford", "to start fires, hunt, and bury their dead", "the 1970s", "frontal lobe", "1957", "Brooke Wexler", "1834", "February 28, 2018", "Kate Walsh", "Grey matter ( or gray matter ) is a major component of the central nervous system, consisting of neuronal cell bodies, neuropil ( dendrites and myelinated as well as unmyelinated axons )", "cella", "Patrick Walshe", "The Republic of Tecala", "Massillon, Ohio", "Ta\u00edno", "October 27, 1904", "August 18, 1998", "As early as 1671", "The enthalpy of fusion of a substance, also known as ( latent ) heat of fusion", "an edible tuber", "around 1872", "The product's classification as a cake or biscuit was part of a VAT tribunal in 1991, with the court finding in McVitie's favour that the Jaffa cake should be considered a cake for tax purposes", "Latin : liberalis, `` worthy of a free person ''", "1813", "Germany"], "metric_results": {"EM": 0.8125, "QA-F1": 0.8752620646190256}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.32258064516129037, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4736842105263158, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-8476", "mrqa_naturalquestions-validation-2083", "mrqa_naturalquestions-validation-3060"], "retained_ids": ["mrqa_naturalquestions-validation-7509", "mrqa_naturalquestions-validation-8854", "mrqa_naturalquestions-validation-4789", "mrqa_naturalquestions-validation-4816", "mrqa_naturalquestions-validation-2741", "mrqa_naturalquestions-validation-6750", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-8308", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-2144", "mrqa_naturalquestions-validation-5905", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-1515", "mrqa_naturalquestions-validation-7516", "mrqa_naturalquestions-validation-1917"], "fixed_ids": ["mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-272", "mrqa_naturalquestions-validation-2968", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-4211", "mrqa_naturalquestions-validation-3214"], "unfixed_ids": ["mrqa_naturalquestions-validation-10347", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-4414"], "error_ids": ["mrqa_naturalquestions-validation-10347", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-272", "mrqa_naturalquestions-validation-2968", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-4211", "mrqa_naturalquestions-validation-3214"], "instant_fixing_rate": 0.7857142857142857, "instant_retention_rate": 0.8333333333333334, "doing-nothing_instant_EM": 0.71875, "doing-nothing_accmulative_EM": 0.6846590909090909}, {"timecode": 44, "before_eval": {"predictions": ["Michael Jackson ( vocals in the chorus ) and Jermaine Jackson ( additional backing vocals )", "a passing train", "Rugrats in Paris : The Movie", "The three wise monkeys ( Japanese : \u4e09\u733f, Hepburn : san'en or sanzaru", "The Parlement de Bretagne ( Administrative and judicial centre of Brittany, Breton : Breujo\u00f9 Breizh )", "Peter Greene", "President Theodore Roosevelt", "a Remington 870 Police Magnum 12 \u2010 gauge shotgun and a Sig Sauer M400 rifle in 5.56 mm", "2010", "The optic nerve", "1994", "James Intveld", "1992", "Erica Rivera", "Betty", "proteins carried genetic information", "Thomas Edison", "Morgan Woodward", "Abraham", "the government - owned Panama Canal Authority", "six", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "October 14, 2017", "Keeley Clare Julia Hawes", "Deadman's Gun", "The vascular cambium", "Terry Kath", "a specific temperature on the Celsius scale as well as a unit to indicate a temperature interval, a difference between two temperatures or an uncertainty", "John Smith", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "1990", "Ptolemy"], "metric_results": {"EM": 0.625, "QA-F1": 0.6938609674639087}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.5294117647058824, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 0.0, 1.0, 1.0, 0.3846153846153846, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.08, 1.0, 1.0, 1.0, 1.0]}}, "after_eval": {"predictions": ["Rockwell", "Manhattan", "Rugrats in Paris : The Movie", "Japanese : \u4e09\u733f, Hepburn : san'en or sanzaru, alternatively \u4e09 \u5339 \u306e \u733f sanbiki no saru, literally `` three monkeys ''", "The Parlement de Bretagne ( Administrative and judicial centre of Brittany, Breton : Breujo\u00f9 Breizh )", "Peter Greene", "Roosevelt Corollary", "Remington 870 Police Magnum 12 \u2010 gauge shotgun", "2010", "The optic nerve carries the ganglion cell axons to the brain, and the blood vessels that supply the retina", "UNESCO", "James Intveld", "1992", "Erica Rivera", "neither issue made it clear whether Archie was married to Betty or Veronica", "The results of the Avery -- MacLeod -- McCarty experiment, published in 1944, suggested that DNA was the genetic material, but there was still some hesitation within the general scientific community to accept this", "Fred Ott", "Morgan Woodward", "Abraham", "the government - owned Panama Canal Authority", "six", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "October 14, 2017", "Keeley Clare Julia Hawes", "Far Away", "The vascular cambium", "Terry Kath", "named after the Swedish astronomer Anders Celsius", "John Smith", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "1990", "Ptolemy"], "metric_results": {"EM": 0.875, "QA-F1": 0.9311587615815557}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8095238095238095, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 0.3846153846153846, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-6340"], "retained_ids": ["mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-312", "mrqa_naturalquestions-validation-8449", "mrqa_naturalquestions-validation-8710", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-6486", "mrqa_naturalquestions-validation-8035", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-7244", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-3756", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-6149"], "fixed_ids": ["mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-2501", "mrqa_naturalquestions-validation-2767", "mrqa_naturalquestions-validation-1641", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3771"], "unfixed_ids": ["mrqa_naturalquestions-validation-9087", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-9753"], "error_ids": ["mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-9087", "mrqa_naturalquestions-validation-2501", "mrqa_naturalquestions-validation-2767", "mrqa_naturalquestions-validation-1641", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3771"], "instant_fixing_rate": 0.75, "instant_retention_rate": 0.95, "doing-nothing_instant_EM": 0.6875, "doing-nothing_accmulative_EM": 0.6847222222222222}, {"timecode": 45, "before_eval": {"predictions": ["re-education, as `` we don't see men's violence against women as stemming from individual pathology, but rather from a socially reinforced sense of entitlement. ''", "October 1, 2017", "The ecliptic", "U.S. population exceeds 75 million", "Hero Beauregard Fiennes - Tiffin", "Pakistan", "Hugo von Mohl", "Currington", "Georges Auguste Escoffier", "The UN General Assembly", "Stephen A. Davis", "four", "George Strait", "1918", "Left Behind", "The Gupta Empire", "Buddhism", "Harry Dean Stanton", "butane", "Philipp Scheidemann", "James W. Marshall", "February 2017", "6 - 7 % average GDP growth annually", "Jason Flemyng", "Paspahegh Indians", "Langdon", "September 19, 1977", "no more than 4.25 inches ( 108 mm )", "Brooke Wexler", "small orange collection boxes distributed to millions of trick - or - treaters", "Ace", "summer months"], "metric_results": {"EM": 0.8125, "QA-F1": 0.8152173913043479}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, false], "QA-F1": [0.08695652173913045, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "after_eval": {"predictions": ["re-education", "May 20, 2018", "The ecliptic", "U.S. population exceeds 75 million", "Hero Beauregard Fiennes - Tiffin", "Pakistan", "Hugo von Mohl", "Currington", "Georges Auguste Escoffier", "The UN General Assembly", "Stephen A. Davis", "four", "George Strait", "1918", "Left Behind", "The Gupta Empire", "Buddhism", "Harry Dean Stanton", "butane", "Gustav Bauer", "James W. Marshall", "March 2018", "6 - 7 % average GDP growth annually", "Jason Flemyng", "uninhabited", "Langdon", "September 19, 1977", "no more than 4.25 inches ( 108 mm )", "Brooke Wexler", "UNICEF's global programing", "Ace", "summer months"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9241071428571428}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-5630"], "retained_ids": ["mrqa_naturalquestions-validation-8271", "mrqa_naturalquestions-validation-8608", "mrqa_naturalquestions-validation-1764", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-9397", "mrqa_naturalquestions-validation-858", "mrqa_naturalquestions-validation-2665", "mrqa_naturalquestions-validation-629", "mrqa_naturalquestions-validation-6616", "mrqa_naturalquestions-validation-6122", "mrqa_naturalquestions-validation-7911", "mrqa_naturalquestions-validation-6323", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-6431", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-6680", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-6804", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-4850"], "fixed_ids": ["mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3962"], "unfixed_ids": ["mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-8027"], "error_ids": ["mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-8027"], "instant_fixing_rate": 0.6666666666666666, "instant_retention_rate": 0.9615384615384616, "doing-nothing_instant_EM": 0.84375, "doing-nothing_accmulative_EM": 0.6881793478260869}, {"timecode": 46, "before_eval": {"predictions": ["White House Executive Chef", "Janis Joplin", "ancient Rome", "directly into the bloodstream", "The Elk River", "Bulgaria", "February 2001", "semi-independent State of Vietnam", "International Border ( IB )", "Wilbur", "Thomas Jefferson", "Shenzi", "Ali", "The citation style may request the full date and time of the article revision you are using", "Yahya Khan", "Thunder Road", "creative reasons and `` not a reflection '' of the actress'performance", "Scottish singer - songwriter Dougie MacLean", "Aaron Lewis", "an English occupational name for one who obtained his living by fishing or living by a fishing weir", "stromal connective tissue", "Albert Einstein", "The papillary dermis is the uppermost layer of the dermis. It intertwines with the rete ridges of the epidermis and is composed of fine and loosely arranged collagen fibers", "Ed Roland", "In the sixteenth series Dr Dalton is killed in an explosion", "the wearers preference", "Mesoamerica", "a number of Early Christian communities in Galatia", "nerves and ganglia outside the brain and spinal cord", "James Chadwick", "Mark Jackson", "TWitch"], "metric_results": {"EM": 0.5, "QA-F1": 0.641242784992785}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.33333333333333337, 0.3636363636363636, 0.5, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 0.4444444444444445, 0.5714285714285715, 1.0, 1.0, 0.4, 1.0, 0.2857142857142857, 1.0, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "after_eval": {"predictions": ["White House Executive Chef", "singer Janis Joplin with the poets Michael McClure and Bob Neuwirth", "in ancient Rome with gift - giving during the Saturnalia holiday", "into the gastrointestinal tract through a series of ducts", "Kanawha River", "Bulgaria", "2001", "France", "International Border ( IB )", "Paul Lynde", "Thomas Jefferson", "Shenzi", "Ali", "at the bottom of every page ( above the copyright notice )", "Yahya Khan", "Thunder Road", "creative reasons", "Dougie MacLean", "Aaron Lewis, lead guitarist Mike Mushok, bassist and backing vocalist Johnny April, and drummer Jon Wysocki", "an English occupational name for one who obtained his living by fishing or living by a fishing weir", "Stromal cells", "Albert Einstein", "the uppermost layer of the dermis", "Ed Roland", "in an explosion", "the right", "the Americas", "a number of Early Christian communities in Galatia", "the nerves and ganglia outside the brain and spinal cord", "James Chadwick", "Mark Jackson", "Nigel Lythgoe"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9444444444444444}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-1089"], "retained_ids": ["mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-7521", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-3018", "mrqa_naturalquestions-validation-9707", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-6604", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-3003", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-4335", "mrqa_naturalquestions-validation-1123"], "fixed_ids": ["mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-7480", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-811", "mrqa_naturalquestions-validation-5175", "mrqa_naturalquestions-validation-1592", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-3971", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-7852"], "unfixed_ids": ["mrqa_naturalquestions-validation-7212"], "error_ids": ["mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-7480", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-811", "mrqa_naturalquestions-validation-5175", "mrqa_naturalquestions-validation-1592", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-3971", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-7852"], "instant_fixing_rate": 0.9375, "instant_retention_rate": 0.9375, "doing-nothing_instant_EM": 0.65625, "doing-nothing_accmulative_EM": 0.6875}, {"timecode": 47, "before_eval": {"predictions": ["Hercules", "Pittsburgh", "Tommy James and the Shondells", "Rashida Jones", "Argentine composer Lalo Schifrin", "Louis Hynes", "1957", "Alta Wind Energy Center in California", "To capitalize on her publicity", "between 1923 and 1925", "Amanda Fuller ( season 1 ) as Kristin Beth Baxter", "1940s", "Cyndi Grecco", "14 and 15 August 1945", "Plank", "James Intveld", "Crist\u00f3bal Baca ( Vaca ) and his wife Ana Ortiz", "24 November 1949", "Master Christopher Jones", "after the Spanish -- American War in the 1898 Treaty of Paris", "3.2 million", "De Wayne Warren", "in the 18th century in the United Kingdom when members of parliament disparagingly used the title in reference to Sir Robert Walpole", "the states or the people", "Twenty - seven", "1912", "on the French Caribbean island of Guadeloupe", "a kiss - off to a narcissistic ex-lover who did the protagonist wrong", "Tokyo", "MacFarlane", "Games played", "maximum speed 160 km / h"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7041035353535354}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.22222222222222224, 0.4, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0]}}, "after_eval": {"predictions": ["Hercules", "Pittsburgh", "Tommy James and the Shondells", "Rashida Jones", "Lalo Schifrin", "Malina Weissman", "1957", "Iowa", "gets relocated to Miami", "1923", "Amanda Fuller", "1940s", "Cyndi Grecco", "14 and 15 August 1945", "Plank", "James Intveld", "New Mexico", "24 November 1949", "Master Christopher Jones", "1898", "3.2 million", "DeWayne Warren", "in the 18th century in the United Kingdom when members of parliament disparagingly used the title in reference to Sir Robert Walpole", "the states or the people", "Thirty - three", "1912", "the French Caribbean island of Guadeloupe", "Rihanna", "Tokyo for the 2020 Summer Olympics", "MacFarlane", "Games played", "160 km / h"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9921875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-3416"], "retained_ids": ["mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-7088", "mrqa_naturalquestions-validation-370", "mrqa_naturalquestions-validation-1634", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-8709", "mrqa_naturalquestions-validation-6489", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-7973", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-7114", "mrqa_naturalquestions-validation-2630", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-716"], "fixed_ids": ["mrqa_naturalquestions-validation-156", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-7630", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-8623", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-2156", "mrqa_naturalquestions-validation-7788", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6610"], "unfixed_ids": [], "error_ids": ["mrqa_naturalquestions-validation-156", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-7630", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-8623", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-2156", "mrqa_naturalquestions-validation-7788", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6610"], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9473684210526315, "doing-nothing_instant_EM": 0.75, "doing-nothing_accmulative_EM": 0.6888020833333334}, {"timecode": 48, "before_eval": {"predictions": ["Sam Waterston", "Herod's grandson Agrippa I", "Peter Dinklage", "Nepal", "Italy", "Iden Versio", "the heat", "April 7, 2016", "translocation Down syndrome", "Ethel `` Edy '' Proctor", "Daren Maxwell Kagasoff", "Rodney Crowell", "Giancarlo Stanton", "Kristy Swanson", "northern China", "Ernest Hemingway", "December 1, 2009", "one of popular music's most poignant anthems of sorrow regarding the environment", "September 19 - 22, 2017", "1936", "masons'marks", "Midwest Regional, CenturyLink Center Omaha, Omaha, Nebraska", "Abraham Gottlob Werner", "Villa de Bejar", "Buddhism", "Pac - 12 Conference Champions Stanford Cardinal", "four", "the company generates income", "Phil Simms", "Bart Cummings", "Clarence Darrow", "dispense summary justice or merely deal with local administrative applications in common law jurisdictions"], "metric_results": {"EM": 0.46875, "QA-F1": 0.568558253340862}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.42857142857142855, 1.0, 0.0, 0.0, 0.8333333333333333, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.782608695652174]}}, "after_eval": {"predictions": ["Sam Waterston", "Herod", "Tyrion Lannister ( season 1 -- present ) portrayed by Peter Dinklage", "Nepal", "Italy", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad", "an oxidant, usually atmospheric oxygen", "April 7, 2016", "a rearrangement of chromosomal material between chromosome 21 and another chromosome", "Ethel `` Edy '' Proctor", "Daren Maxwell Kagasoff", "Rodney Crowell", "Giancarlo Stanton", "Kristy Swanson", "northern China", "Ernest Hemingway", "May 18, 2010", "sorrow regarding the environment", "September 19 - 22, 2017", "1912", "the name of a work gang", "Staples Center, Los Angeles, California", "James Hutton", "Spanish explorers", "Buddhism", "Stanford Cardinal", "20 regional offices and 11 sub-offices", "the financing activities section", "Phil Simms", "Bart Cummings", "William Jennings Bryan", "deal with local administrative applications in common law jurisdictions"], "metric_results": {"EM": 1.0, "QA-F1": 1.0}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": [], "retained_ids": ["mrqa_naturalquestions-validation-8109", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-734", "mrqa_naturalquestions-validation-5363", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-2672", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5405", "mrqa_naturalquestions-validation-9876", "mrqa_naturalquestions-validation-5809", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-2466"], "fixed_ids": ["mrqa_naturalquestions-validation-4294", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-2938", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-7034", "mrqa_naturalquestions-validation-5291", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-2476"], "unfixed_ids": [], "error_ids": ["mrqa_naturalquestions-validation-4294", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-2938", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-7034", "mrqa_naturalquestions-validation-5291", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-2476"], "instant_fixing_rate": 1.0, "instant_retention_rate": 1.0, "doing-nothing_instant_EM": 0.6875, "doing-nothing_accmulative_EM": 0.6887755102040817}, {"timecode": 49, "accumulative_EM": 0.808125, "accumulative_forgotten_ids": ["mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-7197", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-6109", "mrqa_naturalquestions-validation-1789", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-5119", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-684", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-3000", "mrqa_naturalquestions-validation-1535", "mrqa_naturalquestions-validation-3732", "mrqa_naturalquestions-validation-6110", "mrqa_naturalquestions-validation-7001", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-7108", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-3985", "mrqa_naturalquestions-validation-1028", "mrqa_naturalquestions-validation-5043", "mrqa_naturalquestions-validation-7362", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-9672", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-2242", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-10361", "mrqa_naturalquestions-validation-1", "mrqa_naturalquestions-validation-8841", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-2646", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-10176", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-8466", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-917", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-8553", "mrqa_naturalquestions-validation-4336", "mrqa_naturalquestions-validation-1915", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-1181", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-5236", "mrqa_naturalquestions-validation-9110", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-5259", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-3678", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-9219", "mrqa_naturalquestions-validation-6169", "mrqa_naturalquestions-validation-6507", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-9209", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-121", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-8994", "mrqa_naturalquestions-validation-3688", "mrqa_naturalquestions-validation-3759", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-1121", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-6351", "mrqa_naturalquestions-validation-5392", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-3466", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-2563", "mrqa_naturalquestions-validation-6574", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-9094", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-8528", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-10032", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-7200", "mrqa_naturalquestions-validation-6810", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-6699", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-5854", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-2887", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-4061", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-7894", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-10562", "mrqa_naturalquestions-validation-2719", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-10520", "mrqa_naturalquestions-validation-7043", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-3440", "mrqa_naturalquestions-validation-8476", "mrqa_naturalquestions-validation-3060", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-8608", "mrqa_naturalquestions-validation-6122", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-3018", "mrqa_naturalquestions-validation-7973", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-7398", "mrqa_naturalquestions-validation-7588"], "accumulative_fixed_ids": ["mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-816", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-8529", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-10326", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-9885", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-4450", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-10619", "mrqa_naturalquestions-validation-10162", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-4496", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-7300", "mrqa_naturalquestions-validation-8319", "mrqa_naturalquestions-validation-8064", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-8180", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-7792", "mrqa_naturalquestions-validation-226", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-407", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-2624", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-9852", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-4444", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4006", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-4021", "mrqa_naturalquestions-validation-8763", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-9681", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-8747", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-6881", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-5218", "mrqa_naturalquestions-validation-5651", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-9058", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-7311", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-4946", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3069", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-8643", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-9763", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-validation-3188", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-4117", "mrqa_naturalquestions-validation-7801", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-1186", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-5659", "mrqa_naturalquestions-validation-10352", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-9635", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-3704", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-6157", "mrqa_naturalquestions-validation-10626", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-7013", "mrqa_naturalquestions-validation-10246", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-5439", "mrqa_naturalquestions-validation-732", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-9184", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2545", "mrqa_naturalquestions-validation-7234", "mrqa_naturalquestions-validation-5146", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-5437", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-10612", "mrqa_naturalquestions-validation-6365", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-2270", "mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-2439", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-787", "mrqa_naturalquestions-validation-6167", "mrqa_naturalquestions-validation-3058", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6765", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-5212", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-5272", "mrqa_naturalquestions-validation-6212", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-3115", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2663", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-5522", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-8509", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-3310", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-8061", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-53", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-1364", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-896", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-6118", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-7567", "mrqa_naturalquestions-validation-1617", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-2214", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-272", "mrqa_naturalquestions-validation-2968", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-4211", "mrqa_naturalquestions-validation-2501", "mrqa_naturalquestions-validation-1641", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3771", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-5175", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-7630", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-2156", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-7034", "mrqa_naturalquestions-validation-5291", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-2476"], "accumulative_forgotten_rate": 0.098125, "accumulative_fixed_rate": 0.218125, "before_eval": {"predictions": ["New living tissue", "1967", "Caparra", "L lubricates the heart", "1840s", "Chinese cooking for over 400 years", "James Madison", "euro", "4 January 2011", "Werner Ruchti", "the people of the United States", "medieval", "Costa Rica, Brazil, and the Philippines accounted for nearly one - third of the world's production of pineapples", "Carol Ann Susi", "Ray Charles", "1986", "P.V. Sindhu", "September 14, 2008", "2004", "Paul Gass", "Theater 9 at the Century 16 multiplex ( operated by Cinemark ), located at the Town Center at Aurora shopping mall at 14300 E. Alameda Avenue", "1913", "Missi Hale", "Rihanna", "Johnny Cash", "Kanab, Utah", "2001", "in northwestern Highlands County at 27 \u00b0 35 \u2032 40 '' N 81 \u00b0 30 \u2032 12 '' W \ufeff", "Upstate New York", "Mona Vanderwaal", "Holly Sorensen", "the close quarters and poor hygiene exhibited at that time Athens became a breeding ground for disease and many citizens died including Pericles, his wife, and his sons Paralus and Xanthippus"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6322303921568628}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, false], "QA-F1": [0.7499999999999999, 1.0, 0.0, 0.8, 1.0, 0.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.29411764705882354]}}, "after_eval": {"predictions": ["Transport of fluids between the roots and the shoots in the xylem and phloem", "1967", "Puerto Rico", "Sets heart in mediastinum and limits its motion", "1840s", "edible - nest swiftlets using solidified saliva", "John Adams, Benjamin Franklin, Alexander Hamilton, John Jay, Thomas Jefferson, James Madison, and George Washington", "the eurozone", "4 January 2011", "Werner Ruchti", "the people of the United States", "medieval", "Costa Rica, Brazil, and the Philippines accounted for nearly one - third of the world's production of pineapples", "Carol Ann Susi", "Ray Charles", "1990", "P.V. Sindhu", "September 14, 2008", "2004", "the Missouri River", "Theater 9 at the Century 16 multiplex ( operated by Cinemark ), located at the Town Center at Aurora shopping mall at 14300 E. Alameda Avenue", "1913", "Missi Hale", "The Beatles", "Johnny Cash", "Lake Powell", "2001", "in northwestern Highlands County at 27 \u00b0 35 \u2032 40 '' N 81 \u00b0 30 \u2032 12 '' W \ufeff / \ufefd 27.59444 \u00b0 N 81.50333 \u00b0 W", "Upstate New York", "CeCe Drake", "former elite, Olympian or NCAA champion gymnasts", "close quarters and poor hygiene"], "metric_results": {"EM": 0.84375, "QA-F1": 0.9070215017825312}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8727272727272728, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_ids": ["mrqa_naturalquestions-validation-1162"], "retained_ids": ["mrqa_naturalquestions-validation-3433", "mrqa_naturalquestions-validation-3625", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-3594", "mrqa_naturalquestions-validation-6586", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-7953", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-1901"], "fixed_ids": ["mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9477", "mrqa_naturalquestions-validation-9487", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-7588", "mrqa_naturalquestions-validation-10156"], "unfixed_ids": ["mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-7398", "mrqa_naturalquestions-validation-7974", "mrqa_naturalquestions-validation-10130"], "error_ids": ["mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-9477", "mrqa_naturalquestions-validation-7398", "mrqa_naturalquestions-validation-9487", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-7974", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-10130", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-7588", "mrqa_naturalquestions-validation-10156"], "instant_fixing_rate": 0.7333333333333333, "instant_retention_rate": 0.9411764705882353, "doing-nothing_instant_EM": 0.65625, "doing-nothing_accmulative_EM": 0.688125}]}