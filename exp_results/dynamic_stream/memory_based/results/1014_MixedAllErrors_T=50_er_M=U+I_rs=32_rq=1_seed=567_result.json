{"model_update_steps": 1955, "method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/1014_MixedAllErrors_T=50_er_M=U+I_rs=32_rq=1_seed=567_ckpts/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/1014_MixedAllErrors_T=50_er_M=U+I_rs=32_rq=1_seed=567_ckpts/', replay_candidate_size=8, replay_frequency=1, replay_size=32, save_all_ckpts=0, skip_instant_eval=True, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=4, pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.upstream_eval.jsonl', predict_batch_size=16, replay_stream_json_path='', sampled_upstream_json_path='exp_results/data_streams/mrqa.nq_train.memory.jsonl', task_name='mrqa_naturalquestions', train_batch_size=8, use_sampled_upstream=True)", "online_eval_results": [{"timecode": 0, "before_eval": {"predictions": ["Raymond Briggs' 1978 children's book 'The Snowman", "the acceleration due to gravity decreased as an inverse square law", "a marquetry commode by the \u00e9b\u00e9niste Jean Henri Riesener dated c1780", "a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work", "lymphocyte", "Chinghiz", "Doctor Who and the Daleks in the Seven Keys to Doomsday", "Super Bowl 50 Host Committee", "a satirical television comedy programme on BBC Television in 1962 and 1963", "dynasty", "Br'er Rabbit", "Bodhi Natural Health Products", "Amphitrite Goddess of the Sea", "the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ), the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho", "March 14 ( 3 / 14 in the month / day format ) since 3, 1, and 4 are the first three significant digits of \u03c0", "the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing", "Horace Rumpole", "combustion", "A Sunday Afternoon on the Island of La Grande Jatte", "nobody knows for sure how a do-over in golf came to be called a mulligan", "Captain Meriwether Lewis's 30th birthday", "An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses", "2011", "electric eels", "before the first year begins", "New Jerusalem", "Samantha Spiro", "2013", "Ronnie Biggs", "Steve Carell as Felonious Gru, the former villain turned Anti-Villain League agent, Margo, Edith, and Agnes'adoptive father, and Lucy's husband", "an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "the assassination of US President John F. Kennedy"], "metric_results": {"EM": 0.0, "QA-F1": 0.0812722081839729}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.4799999999999999, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.11764705882352942, 0.15384615384615383]}}, "error_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-10322", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-2376", "mrqa_squad-validation-6677", "mrqa_squad-validation-6303", "mrqa_squad-validation-7821", "mrqa_squad-validation-392", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9688", "mrqa_triviaqa-validation-1551", "mrqa_squad-validation-3478", "mrqa_triviaqa-validation-5937", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-2730", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-4185", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3241", "mrqa_triviaqa-validation-7369", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-validation-7017", "mrqa_squad-validation-7746"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 1, "before_eval": {"predictions": ["the town of Acolman, just north of Mexico City", "The Iroquois", "a natural extension of capitalism that arose from need for capitalist economies to constantly expand investment, material resources and manpower in such a way that necessitated colonial expansion", "john Leslie", "Queen Elizabeth II", "Dallas Lovers' Song", "to the anterolateral corner of the spinal cord", "1966", "for scientific observation", "product or policy that is open, honest and delivers against its promise", "Stock Market crash in New York", "New York Stadium", "posealike and former Commons Clerk Sir Robert Rogers", "continental units", "john Forster", "Comptroller General of the Receipt and Issue of Her Majesty's Exchequer", "Sergio P\u00e9rez", "River Welland", "The Concubine", "1543", "the final revelation of God the Final Testament", "Vigor", "a policeman who investigates a series of mysterious killings and illnesses", "glowed even when turned off", "Florence Nightingale", "Budweiser", "numb3rs - Season 1", "the great heroism or of the most conspicuous courage in circumstances of extreme danger", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object", "the check written by DC Comics to Jerry Siegel and Joe Shuster for the exclusive rights to their then-new character, Superman", "May and June 2010"], "metric_results": {"EM": 0.0625, "QA-F1": 0.14436712032427043}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.06896551724137931, 0.0, 0.0, 0.0, 0.23529411764705885, 0.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0, 0.5, 0.0, 0.4444444444444445, 0.07407407407407407, 0.1904761904761905, 0.4]}}, "error_ids": ["mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-5144", "mrqa_squad-validation-10015", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-5406", "mrqa_hotpotqa-validation-5899", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-1924", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-validation-1364", "mrqa_hotpotqa-validation-14", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-3971", "mrqa_squad-validation-2629", "mrqa_triviaqa-validation-2210", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-5802", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-2096", "mrqa_naturalquestions-validation-1864", "mrqa_squad-validation-10410", "mrqa_triviaqa-validation-2367", "mrqa_hotpotqa-validation-3774"], "retrieved_ids": ["mrqa_naturalquestions-train-78177", "mrqa_naturalquestions-train-62898", "mrqa_naturalquestions-train-36925", "mrqa_naturalquestions-train-60655", "mrqa_naturalquestions-train-41696", "mrqa_naturalquestions-train-58694", "mrqa_naturalquestions-train-88104", "mrqa_naturalquestions-train-74354", "mrqa_naturalquestions-train-73399", "mrqa_naturalquestions-train-2563", "mrqa_naturalquestions-train-52482", "mrqa_naturalquestions-train-26608", "mrqa_naturalquestions-train-61269", "mrqa_naturalquestions-train-79385", "mrqa_naturalquestions-train-84490", "mrqa_naturalquestions-train-82343", "mrqa_naturalquestions-train-22424", "mrqa_naturalquestions-train-51653", "mrqa_naturalquestions-train-21683", "mrqa_naturalquestions-train-45527", "mrqa_naturalquestions-train-50705", "mrqa_naturalquestions-train-23444", "mrqa_naturalquestions-train-17964", "mrqa_naturalquestions-train-39440", "mrqa_naturalquestions-train-88097", "mrqa_naturalquestions-train-24235", "mrqa_naturalquestions-train-57186", "mrqa_naturalquestions-train-24311", "mrqa_naturalquestions-train-7062", "mrqa_naturalquestions-train-82157", "mrqa_naturalquestions-train-62775", "mrqa_naturalquestions-train-64015"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 2, "before_eval": {"predictions": ["MS Kronprins Harald", "rugby union", "Puritanism", "+, -, *, and / keys", "2003", "in different parts of the globe", "rioneth and Llantisilly Rail Traction Company Limited", "acetate", "John II Casimir Vasa", "marries Lord Darnley", "A55 North Wales Expressway", "phylum with relatively few species", "`` Everywhere '' is a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "sprawl leads to urban decay and a concentration of lower income residents in the inner city", "for second year in United States high school or college", "Bothtec", "Terry Reid", "reports from government agencies and non-governmental organizations", "sept Princesses", "North America", "Andr\u00e9 3000", "Commander", "Akhenaten", "President Theodore Roosevelt", "the fourth season following on from the demise of the SRO Group's FIA GT1 World Championship", "Denver Broncos", "the Western Bloc ( the United States, its NATO allies and others )", "in the 1970s", "is an opera in four acts by French composer Georges Bizet", "Matt Winer", "1671", "Pacific across the Amazonas Basin"], "metric_results": {"EM": 0.0625, "QA-F1": 0.21291811604311603}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.15384615384615385, 0.0, 0.0, 0.0, 0.0, 0.4, 0.28571428571428575, 0.1, 0.3243243243243243, 0.18181818181818182, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.4, 0.0, 0.0, 0.4, 0.0, 0.0, 0.3636363636363636, 0.3333333333333333, 0.18181818181818182, 0.4, 0.0, 0.4]}}, "error_ids": ["mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-8542", "mrqa_squad-validation-7149", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-validation-1202", "mrqa_hotpotqa-validation-3632", "mrqa_triviaqa-validation-1935", "mrqa_hotpotqa-validation-1888", "mrqa_squad-validation-4456", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-8448", "mrqa_triviaqa-validation-2136", "mrqa_naturalquestions-validation-539", "mrqa_squad-validation-8513", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-validation-5502", "mrqa_hotpotqa-validation-2679", "mrqa_squad-validation-4019", "mrqa_naturalquestions-validation-6896", "mrqa_naturalquestions-validation-2501", "mrqa_hotpotqa-validation-1376", "mrqa_squad-validation-194", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-683", "mrqa_triviaqa-validation-2722", "mrqa_hotpotqa-validation-4367", "mrqa_squad-validation-3113", "mrqa_squad-validation-4283"], "retrieved_ids": ["mrqa_naturalquestions-train-2697", "mrqa_naturalquestions-train-16923", "mrqa_naturalquestions-train-88216", "mrqa_naturalquestions-train-69246", "mrqa_naturalquestions-train-57912", "mrqa_naturalquestions-train-15952", "mrqa_naturalquestions-train-28169", "mrqa_naturalquestions-train-34333", "mrqa_naturalquestions-train-82365", "mrqa_naturalquestions-train-37748", "mrqa_naturalquestions-train-28985", "mrqa_naturalquestions-train-48852", "mrqa_naturalquestions-train-69738", "mrqa_naturalquestions-train-49179", "mrqa_naturalquestions-train-21683", "mrqa_naturalquestions-train-33933", "mrqa_naturalquestions-train-62258", "mrqa_naturalquestions-train-15426", "mrqa_naturalquestions-train-12218", "mrqa_naturalquestions-train-33921", "mrqa_naturalquestions-train-68904", "mrqa_naturalquestions-train-43926", "mrqa_naturalquestions-train-47541", "mrqa_naturalquestions-train-23341", "mrqa_naturalquestions-train-2643", "mrqa_naturalquestions-train-72723", "mrqa_naturalquestions-train-66819", "mrqa_naturalquestions-train-47045", "mrqa_naturalquestions-train-37993", "mrqa_naturalquestions-train-36986", "mrqa_naturalquestions-train-71615", "mrqa_naturalquestions-train-55168"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 3, "before_eval": {"predictions": ["id", "is not involved", "id", "between 27 July and 7 August 2022", "New Paltz", "is getting a remake", "2006 British Academy Television Award for Best Drama Series", "Least of the Great Powers", "lower motor neurons, the efferent nerves that directly innervate muscles", "is a British television game show based on the American game show Family Feud", "is the leading, branded boiled mint in the UK", "Lester Piggott", "coronary thrombosis", "bollywood", "Overtime", "Sir Henry Cole", "has trouble distinguishing between carbon dioxide and oxygen", "is a British sitcom, broadcast in the United Kingdom from 1982 to 1984", "cement City, Texas", "the Democratic Unionist Party (DUP )", "23 July 1989", "many educational institutions especially within the US", "the spiritual teacher is known as a guru, and, in many traditions of Hinduism - especially those common in the West - the emphasis on spiritual mentorship is extremely high", "control purposes", "island in the Sun", "callable bonds", "2.26 GHz quad - core Snapdragon 800 processor", "over 10,000 British and 2,000 old master works", "al - khimar", "proteins carried genetic information", "bile duct", "berenice Abbott"], "metric_results": {"EM": 0.125, "QA-F1": 0.18060470779220777}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, 0.1, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.07999999999999999, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-5362", "mrqa_naturalquestions-validation-5647", "mrqa_squad-validation-3058", "mrqa_triviaqa-validation-7382", "mrqa_squad-validation-7816", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-2571", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2327", "mrqa_squad-validation-1539", "mrqa_triviaqa-validation-3901", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-3189", "mrqa_squad-validation-8832", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-2150", "mrqa_squad-validation-2191", "mrqa_squad-validation-2069", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-6402", "mrqa_squad-validation-5517", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-5818", "mrqa_triviaqa-validation-6800", "mrqa_triviaqa-validation-2530"], "retrieved_ids": ["mrqa_naturalquestions-train-87999", "mrqa_naturalquestions-train-74902", "mrqa_naturalquestions-train-67080", "mrqa_naturalquestions-train-77353", "mrqa_naturalquestions-train-26214", "mrqa_naturalquestions-train-52155", "mrqa_naturalquestions-train-47873", "mrqa_naturalquestions-train-33890", "mrqa_naturalquestions-train-59266", "mrqa_naturalquestions-train-65460", "mrqa_naturalquestions-train-3268", "mrqa_naturalquestions-train-33604", "mrqa_naturalquestions-train-82776", "mrqa_naturalquestions-train-48067", "mrqa_naturalquestions-train-61428", "mrqa_naturalquestions-train-72073", "mrqa_naturalquestions-train-57639", "mrqa_naturalquestions-train-46454", "mrqa_naturalquestions-train-7389", "mrqa_naturalquestions-train-22788", "mrqa_naturalquestions-train-34656", "mrqa_naturalquestions-train-29087", "mrqa_naturalquestions-train-76644", "mrqa_naturalquestions-train-56981", "mrqa_naturalquestions-train-48292", "mrqa_naturalquestions-train-29371", "mrqa_naturalquestions-train-42575", "mrqa_naturalquestions-train-17245", "mrqa_naturalquestions-train-2097", "mrqa_naturalquestions-train-14519", "mrqa_naturalquestions-train-19954", "mrqa_naturalquestions-train-10843"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 4, "before_eval": {"predictions": ["Austria", "at Nijmegen, over the Waal distributary of the Rhine", "December 9, 2016", "NASA discontinued the manned Block I program, using the Block I spacecraft only for unmanned Saturn V flights", "British progressive folk-rock band Gryphon", "1898", "Januarius", "The Man Moses", "in the southernmost part of Brooklyn, New York", "tetanus", "bounding the time or space used by the algorithm", "ring", "Alex O'Loughlin", "Eddie Leonski", "Jack", "a mixture of phencyclidine and cocaine", "bunker", "in an age when people were far more dependent on outside caterers than we are today", "Reverse - Flash", "All Hallows'Day", "1934", "Azerbaijan", "new converts", "Mona Vanderwaal", "cricket", "Pyotr Ilyich Tchaikovsky", "2001 or 2010", "Quebec", "comprehend and formulate language", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "steam turbine", "Splodgenessabounds"], "metric_results": {"EM": 0.34375, "QA-F1": 0.41882440476190474}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.4, 0.33333333333333337, 1.0, 0.0, 0.0, 0.4, 1.0, 0.25, 0.0, 0.0, 0.4, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-9358", "mrqa_naturalquestions-validation-5146", "mrqa_squad-validation-3971", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-3647", "mrqa_squad-validation-1688", "mrqa_triviaqa-validation-3808", "mrqa_hotpotqa-validation-1168", "mrqa_hotpotqa-validation-1289", "mrqa_hotpotqa-validation-2944", "mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-93", "mrqa_naturalquestions-validation-2900", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-3978", "mrqa_squad-validation-3467"], "retrieved_ids": ["mrqa_naturalquestions-train-80201", "mrqa_naturalquestions-train-43103", "mrqa_naturalquestions-train-36381", "mrqa_naturalquestions-train-75292", "mrqa_naturalquestions-train-86013", "mrqa_naturalquestions-train-74958", "mrqa_naturalquestions-train-9747", "mrqa_naturalquestions-train-26707", "mrqa_naturalquestions-train-75211", "mrqa_naturalquestions-train-2937", "mrqa_naturalquestions-train-32341", "mrqa_naturalquestions-train-76439", "mrqa_naturalquestions-train-22436", "mrqa_naturalquestions-train-16923", "mrqa_naturalquestions-train-58860", "mrqa_naturalquestions-train-1653", "mrqa_naturalquestions-train-87066", "mrqa_naturalquestions-train-6110", "mrqa_naturalquestions-train-55330", "mrqa_naturalquestions-train-23341", "mrqa_naturalquestions-train-2690", "mrqa_naturalquestions-train-25275", "mrqa_naturalquestions-train-81051", "mrqa_naturalquestions-train-30649", "mrqa_naturalquestions-train-56694", "mrqa_naturalquestions-train-75096", "mrqa_naturalquestions-train-27429", "mrqa_naturalquestions-train-18539", "mrqa_naturalquestions-train-80770", "mrqa_naturalquestions-train-10687", "mrqa_naturalquestions-train-15132", "mrqa_naturalquestions-train-86874"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 5, "before_eval": {"predictions": ["june", "F\u00e9d\u00e9ration Sportive F\u00e9minine Internationale (FSFI )", "Renfrewshire", "Paspahegh Indians", "a delay or obstruction along the pathway that electrical impulses travel in your heart to make it beat", "South Dakota", "7 : 25 a.m. HST", "swanee or swannee whistle", "rapeseed plant", "used stone tools", "1962", "Parietal cells ( also known as oxyntic or delomorphous cells )", "placental", "September 13, 1994", "june", "imperial rule", "February 1840", "make a defiant speech, or a speech explaining their actions", "George Sylvester Viereck", "kinks", "by using net wealth (adding up assets and subtracting debts )", "entropy increases", "my mind is averse to wedlock because I daily expect the death of a heretic", "8.7 -- 9.2", "China", "2 November 1902", "present - day southeastern Texas", "May 7, 2018", "9 October 1940", "Selden", "structural collapses", "a way of housing a fierce half-man, half-bull creature known as the Minotaur"], "metric_results": {"EM": 0.09375, "QA-F1": 0.1496323529411765}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.13333333333333333, 0.0, 0.47058823529411764, 0.0, 1.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4725", "mrqa_hotpotqa-validation-2197", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-2020", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3625", "mrqa_naturalquestions-validation-4513", "mrqa_triviaqa-validation-1550", "mrqa_triviaqa-validation-5704", "mrqa_hotpotqa-validation-4621", "mrqa_squad-validation-6733", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-254", "mrqa_squad-validation-7554", "mrqa_squad-validation-10423", "mrqa_squad-validation-2757", "mrqa_naturalquestions-validation-2884", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-1489", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-5848", "mrqa_squad-validation-2531", "mrqa_triviaqa-validation-4560"], "retrieved_ids": ["mrqa_naturalquestions-train-16387", "mrqa_naturalquestions-train-72567", "mrqa_naturalquestions-train-26673", "mrqa_naturalquestions-train-35663", "mrqa_naturalquestions-train-50136", "mrqa_naturalquestions-train-54961", "mrqa_naturalquestions-train-74757", "mrqa_naturalquestions-train-39733", "mrqa_naturalquestions-train-26521", "mrqa_naturalquestions-train-43169", "mrqa_naturalquestions-train-38071", "mrqa_naturalquestions-train-56902", "mrqa_naturalquestions-train-19252", "mrqa_naturalquestions-train-77013", "mrqa_naturalquestions-train-72797", "mrqa_naturalquestions-train-51852", "mrqa_naturalquestions-train-60125", "mrqa_naturalquestions-train-69277", "mrqa_naturalquestions-train-41954", "mrqa_naturalquestions-train-62903", "mrqa_naturalquestions-train-24880", "mrqa_naturalquestions-train-65396", "mrqa_naturalquestions-train-50611", "mrqa_naturalquestions-train-21196", "mrqa_naturalquestions-train-46634", "mrqa_naturalquestions-train-85131", "mrqa_naturalquestions-train-69502", "mrqa_naturalquestions-train-14129", "mrqa_naturalquestions-train-71497", "mrqa_naturalquestions-train-38005", "mrqa_naturalquestions-train-78141", "mrqa_naturalquestions-train-66404"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 6, "before_eval": {"predictions": ["zinnemann", "to prevent the flame from being blown out", "Illinois", "1998", "North Carolina", "island in the Mediterranean Sea situated", "90-60's", "unaided independent school", "dolph Camilli", "times sign", "BAFTA Television Award", "Juice Newton", "1960", "HTTP Secure", "late - September through early January", "Wichita", "monatomic", "Palm Springs is popular for its resort feel and nearby open spaces", "june", "clangers", "moths", "universal", "left coronary artery", "1.1 \u00d7 1011 metric tonnes", "dale", "leaf tissue", "Indian club ATK", "land that a nation has conquered and expanded", "Indian Ocean near Grande Comore, Comoros Islands", "`` 1 lakh people ''", "Norwegian", "burning of fossil fuels"], "metric_results": {"EM": 0.15625, "QA-F1": 0.2339409722222222}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-4165", "mrqa_hotpotqa-validation-5810", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-5261", "mrqa_squad-validation-2659", "mrqa_squad-validation-6947", "mrqa_triviaqa-validation-105", "mrqa_naturalquestions-validation-10356", "mrqa_squad-validation-7819", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-484", "mrqa_squad-validation-3463", "mrqa_squad-validation-2584", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-227", "mrqa_squad-validation-4181", "mrqa_triviaqa-validation-4966", "mrqa_squad-validation-8821", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-10042", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6207", "mrqa_hotpotqa-validation-3919", "mrqa_naturalquestions-validation-6644"], "retrieved_ids": ["mrqa_naturalquestions-train-44457", "mrqa_naturalquestions-train-32218", "mrqa_naturalquestions-train-61948", "mrqa_naturalquestions-train-53746", "mrqa_naturalquestions-train-24634", "mrqa_naturalquestions-train-13537", "mrqa_naturalquestions-train-77220", "mrqa_naturalquestions-train-6169", "mrqa_naturalquestions-train-63675", "mrqa_naturalquestions-train-7690", "mrqa_naturalquestions-train-82140", "mrqa_naturalquestions-train-40825", "mrqa_naturalquestions-train-20447", "mrqa_naturalquestions-train-12904", "mrqa_naturalquestions-train-44091", "mrqa_naturalquestions-train-57147", "mrqa_naturalquestions-train-35746", "mrqa_naturalquestions-train-22957", "mrqa_naturalquestions-train-15170", "mrqa_naturalquestions-train-81711", "mrqa_naturalquestions-train-21641", "mrqa_naturalquestions-train-76764", "mrqa_naturalquestions-train-8533", "mrqa_naturalquestions-train-65748", "mrqa_naturalquestions-train-33811", "mrqa_naturalquestions-train-29733", "mrqa_naturalquestions-train-68266", "mrqa_naturalquestions-train-11276", "mrqa_naturalquestions-train-25133", "mrqa_naturalquestions-train-31449", "mrqa_naturalquestions-train-36733", "mrqa_naturalquestions-train-28187"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 7, "before_eval": {"predictions": ["the wisdom and prudence of certain decisions of procurement", "The Q'eqchi '", "only a few", "The U.S. Army Chaplain insignia", "Kairi", "both inner city blacks, who wanted more involvement in government, and whites in the suburbs, who want more services and more control over the central city", "director", "the Mongolian steppes", "the last book accepted into the Christian biblical canon", "Bruno Mars", "% IACS", "for gallantry", "16 million", "1950s", "work oxen for haulage", "1998", "Jeff Brannigan", "23.1", "2001", "family member", "long-term environmental changes", "William Powell Lear", "the radial (centripetal ) force", "Terrell Suggs", "decide on all the motions and amendments that have been moved that day", "a voyage of adventure", "James Hutton", "marlborough", "present-day Charleston, South Carolina", "an uncompromising stand, calling for the destruction of Israel and the establishment of an Islamic state in Palestine", "Adam Karpel", "German general (colonel-general from 1940 )"], "metric_results": {"EM": 0.1875, "QA-F1": 0.26932043650793647}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5522", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-983", "mrqa_squad-validation-7296", "mrqa_triviaqa-validation-7415", "mrqa_squad-validation-6297", "mrqa_naturalquestions-validation-859", "mrqa_squad-validation-108", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-1293", "mrqa_squad-validation-7799", "mrqa_hotpotqa-validation-3898", "mrqa_naturalquestions-validation-98", "mrqa_squad-validation-358", "mrqa_hotpotqa-validation-3846", "mrqa_squad-validation-3558", "mrqa_hotpotqa-validation-1142", "mrqa_squad-validation-10403", "mrqa_naturalquestions-validation-3474", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1301", "mrqa_triviaqa-validation-781", "mrqa_squad-validation-3181", "mrqa_squad-validation-9598", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-3146"], "retrieved_ids": ["mrqa_naturalquestions-train-74926", "mrqa_naturalquestions-train-35943", "mrqa_naturalquestions-train-6919", "mrqa_naturalquestions-train-75270", "mrqa_naturalquestions-train-47418", "mrqa_naturalquestions-train-22922", "mrqa_naturalquestions-train-25965", "mrqa_naturalquestions-train-58782", "mrqa_naturalquestions-train-70893", "mrqa_naturalquestions-train-59105", "mrqa_naturalquestions-train-71033", "mrqa_naturalquestions-train-3485", "mrqa_naturalquestions-train-4510", "mrqa_naturalquestions-train-22930", "mrqa_naturalquestions-train-80873", "mrqa_naturalquestions-train-64762", "mrqa_naturalquestions-train-25132", "mrqa_naturalquestions-train-39799", "mrqa_naturalquestions-train-29172", "mrqa_naturalquestions-train-23377", "mrqa_naturalquestions-train-62293", "mrqa_naturalquestions-train-53250", "mrqa_naturalquestions-train-42437", "mrqa_naturalquestions-train-55439", "mrqa_naturalquestions-train-57237", "mrqa_naturalquestions-train-74804", "mrqa_naturalquestions-train-61630", "mrqa_naturalquestions-train-85363", "mrqa_naturalquestions-train-73836", "mrqa_naturalquestions-train-18726", "mrqa_naturalquestions-train-29462", "mrqa_naturalquestions-train-87357"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 8, "before_eval": {"predictions": ["computational complexity theory", "iuseppe Antonio 'Nino' Farina", "46", "6.4 nanometers apart", "Meghan Ory", "Kyle Busch", "over 400", "adrenal glands", "artes liberales", "Bowland Fells", "Richard, Duke of Gloucester", "St. Louis County", "1868", "2018", "club", "law firm", "Pottawatomie County", "The tuatara", "Albert Einstein", "The church tower", "EastEnders star Danny Dyer", "Toronto", "wales", "110 miles (177 km ) from the East River in New York City, along the North Shore of Long Island, to the south", "Kona coast of the island of Hawai\u02bb i about 12 mi south of Kailua-Kona", "Liberal conservatism", "largest gold rushes the world has ever seen", "six", "not guilty", "psychotherapeutic", "Quentin Coldwater", "acidic"], "metric_results": {"EM": 0.1875, "QA-F1": 0.29744543650793653}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true], "QA-F1": [0.4, 0.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.2666666666666667, 0.5, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-1705", "mrqa_triviaqa-validation-1521", "mrqa_naturalquestions-validation-6089", "mrqa_squad-validation-8869", "mrqa_naturalquestions-validation-856", "mrqa_hotpotqa-validation-3789", "mrqa_triviaqa-validation-7506", "mrqa_hotpotqa-validation-3161", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5401", "mrqa_triviaqa-validation-7157", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-validation-3309", "mrqa_squad-validation-10369", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-582", "mrqa_triviaqa-validation-4268", "mrqa_hotpotqa-validation-1021", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-4812", "mrqa_squad-validation-2987", "mrqa_squad-validation-6915", "mrqa_triviaqa-validation-7767", "mrqa_hotpotqa-validation-1032"], "retrieved_ids": ["mrqa_naturalquestions-train-33594", "mrqa_naturalquestions-train-77726", "mrqa_naturalquestions-train-59715", "mrqa_naturalquestions-train-24769", "mrqa_naturalquestions-train-35597", "mrqa_naturalquestions-train-68578", "mrqa_naturalquestions-train-20736", "mrqa_naturalquestions-train-17936", "mrqa_naturalquestions-train-50101", "mrqa_naturalquestions-train-44678", "mrqa_naturalquestions-train-34923", "mrqa_naturalquestions-train-20604", "mrqa_naturalquestions-train-36829", "mrqa_naturalquestions-train-14215", "mrqa_naturalquestions-train-66332", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-train-53195", "mrqa_naturalquestions-train-7107", "mrqa_naturalquestions-train-44654", "mrqa_naturalquestions-train-51219", "mrqa_squad-validation-1688", "mrqa_naturalquestions-train-25980", "mrqa_naturalquestions-train-2855", "mrqa_naturalquestions-train-86780", "mrqa_naturalquestions-train-8436", "mrqa_naturalquestions-train-68211", "mrqa_naturalquestions-train-59766", "mrqa_naturalquestions-train-58422", "mrqa_naturalquestions-train-36989", "mrqa_naturalquestions-train-54577", "mrqa_naturalquestions-train-43185", "mrqa_naturalquestions-train-45024"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 9, "before_eval": {"predictions": ["the English phrase `` I Seek You ''", "Argentinian", "The report claimed that these noise levels would have a negative long-term impact on the health of the city's residents", "slow ground almonds", "the efficiency of photosynthesis", "a wide range of society figures of the period", "w. Bush", "The Daily Stormer", "triplet", "water", "president", "almost all officeholders", "George, Margrave of Brandenburg-Ansbach", "a corruption of the Kamba version", "3D computer-animated comedy film", "Worcester Cold Storage and Warehouse Co.", "acting", "C. W. Grafton", "LED illuminated display", "Americans", "The Apple iPod+HP", "My Sassy Girl", "prevent damage to the body", "The Edge of Night", "non-combustible substances that corrode, such as iron", "pedagogy", "\u039c\u03ad\u03b3\u03b1\u03bd \u0399\u03b5\u03c1\u03cc\u03bd \u03a3\u03c5\u03bd\u03ad\u03ba\u03b4\u03b7\u03bc\u03bf\u03bd ) book of prayers", "ATP", "soils", "Melbourne drug- dealer, Dennis Allen", "medium and heavy-duty diesel trucks", "testes"], "metric_results": {"EM": 0.21875, "QA-F1": 0.38263267063818535}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, true, false], "QA-F1": [0.0, 1.0, 0.11764705882352941, 0.0, 0.0, 0.125, 0.0, 1.0, 0.0, 0.13333333333333333, 0.4, 0.0, 1.0, 0.18181818181818182, 0.8571428571428571, 0.923076923076923, 0.6666666666666666, 1.0, 0.2857142857142857, 0.4, 0.6666666666666666, 0.3333333333333333, 0.15384615384615383, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6844", "mrqa_squad-validation-5210", "mrqa_triviaqa-validation-3387", "mrqa_squad-validation-8924", "mrqa_squad-validation-5618", "mrqa_triviaqa-validation-2703", "mrqa_squad-validation-3442", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-4520", "mrqa_squad-validation-8259", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-5128", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-1327", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1436", "mrqa_naturalquestions-validation-1294", "mrqa_squad-validation-3490", "mrqa_squad-validation-1879", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-2449", "mrqa_naturalquestions-validation-3677"], "retrieved_ids": ["mrqa_naturalquestions-train-79153", "mrqa_naturalquestions-train-12559", "mrqa_naturalquestions-train-8807", "mrqa_naturalquestions-train-35634", "mrqa_naturalquestions-train-46406", "mrqa_naturalquestions-train-74317", "mrqa_naturalquestions-train-74655", "mrqa_naturalquestions-train-41565", "mrqa_naturalquestions-train-33514", "mrqa_naturalquestions-train-55238", "mrqa_naturalquestions-train-83622", "mrqa_naturalquestions-train-20238", "mrqa_naturalquestions-train-16183", "mrqa_naturalquestions-train-88097", "mrqa_naturalquestions-train-82195", "mrqa_naturalquestions-train-33908", "mrqa_naturalquestions-train-38393", "mrqa_naturalquestions-train-57630", "mrqa_naturalquestions-train-34615", "mrqa_naturalquestions-train-28282", "mrqa_naturalquestions-train-6917", "mrqa_naturalquestions-train-58318", "mrqa_naturalquestions-train-15054", "mrqa_naturalquestions-train-83218", "mrqa_naturalquestions-train-46567", "mrqa_naturalquestions-train-62390", "mrqa_naturalquestions-train-81329", "mrqa_naturalquestions-train-46166", "mrqa_naturalquestions-train-85298", "mrqa_naturalquestions-train-86204", "mrqa_naturalquestions-train-62493", "mrqa_naturalquestions-train-69565"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 10, "before_eval": {"predictions": ["Pope, Alexander (1688-1744)", "yellow fever", "three legal systems", "Las Vegas, Nevada", "status line", "globetrotters", "Anthony Bellew", "Manoir de la Fi\u00e8re and Chef - du - Pont", "1987", "great ideas of the human race", "cromlech", "the British Royal Family", "7", "Kon-Tiki", "Britney Spears Live from Las Vegas", "digital fashion gallery", "Ronnie Hillman", "all-encompassing", "\" When Body Snatchers Targeted Mormons and the Miraculous Dream That Saved a Slain\"", "60", "Eagle Ridge Mall", "Pel\u00e9", "to reduce pressure on the public food supply", "Monastir", "the classical element fire", "\"The Andy Griffith Show\"", "the shooter must be at least 18 or 21 years old ( or have a legal guardian present ), and must sign a waiver prior to shooting", "Ward", "novelist and poet", "Jamestown", "Monet", "tree growth stages"], "metric_results": {"EM": 0.1875, "QA-F1": 0.25416666666666665}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, true, false], "QA-F1": [0.5, 0.0, 0.0, 0.8, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1860", "mrqa_hotpotqa-validation-3149", "mrqa_naturalquestions-validation-1085", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-4162", "mrqa_hotpotqa-validation-2016", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-6389", "mrqa_triviaqa-validation-4791", "mrqa_naturalquestions-validation-10205", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-5682", "mrqa_triviaqa-validation-681", "mrqa_squad-validation-273", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-6119", "mrqa_hotpotqa-validation-897", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-1451", "mrqa_hotpotqa-validation-3976", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-4803", "mrqa_squad-validation-4506"], "retrieved_ids": ["mrqa_naturalquestions-train-75642", "mrqa_naturalquestions-train-85298", "mrqa_naturalquestions-train-60484", "mrqa_naturalquestions-train-73271", "mrqa_naturalquestions-train-19033", "mrqa_naturalquestions-train-73275", "mrqa_naturalquestions-train-36465", "mrqa_naturalquestions-train-70464", "mrqa_naturalquestions-train-87066", "mrqa_naturalquestions-train-72679", "mrqa_naturalquestions-train-43937", "mrqa_naturalquestions-train-26280", "mrqa_naturalquestions-train-86936", "mrqa_naturalquestions-train-62737", "mrqa_naturalquestions-train-76273", "mrqa_naturalquestions-train-57120", "mrqa_naturalquestions-train-36550", "mrqa_naturalquestions-train-36381", "mrqa_naturalquestions-train-46567", "mrqa_naturalquestions-train-36135", "mrqa_naturalquestions-train-39052", "mrqa_naturalquestions-train-62437", "mrqa_naturalquestions-train-33137", "mrqa_naturalquestions-train-67468", "mrqa_naturalquestions-train-50099", "mrqa_naturalquestions-train-36638", "mrqa_naturalquestions-train-29410", "mrqa_naturalquestions-train-39670", "mrqa_naturalquestions-train-41188", "mrqa_naturalquestions-train-25295", "mrqa_naturalquestions-train-70553", "mrqa_naturalquestions-train-7564"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 11, "before_eval": {"predictions": ["Vince Lombardi", "Traumnovelle", "a Gender pay gap in favor of males in the labor market", "Treaty on the Functioning of the European Union (TFEU )", "ice melting at 100 degrees", "the narrator driving a truck owned by his brother, who died in action in the United States Army", "alexander", "The world's longest suspension bridges are listed according to the length of their main span", "Luas", "tunisia", "tunisia", "the Bulgars", "died in battle and according to others, he was one of a handful of men captured and later executed", "Volkswagen Beetle", "alexander", "North American Free Trade Agreement", "Queen Elizabeth I", "infections, irritation, or allergies", "the most - visited paid monument in the world", "Town House Galleria", "catfish aquaculture", "atomic number 53", "Whiskey Shivers as Saddle Up, a country - bluegrass - based band competing against the Bellas", "Iraq", "a co-op of grape growers", "mann on a mission", "tunisia", "1952", "the Charlotte Hornets", "the words `` speed limit '' omitted and an additional panel stating the type of hazard ahead", "Jean F kernel ( 1497 -- 1558 ), a French physician", "the back of the head"], "metric_results": {"EM": 0.09375, "QA-F1": 0.23038283248081842}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.6666666666666666, 0.25, 0.23529411764705882, 0.0, 0.14285714285714288, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.6666666666666666, 0.28571428571428575, 0.4, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.08695652173913043, 0.22222222222222224, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-5078", "mrqa_squad-validation-7447", "mrqa_squad-validation-4169", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-5248", "mrqa_squad-validation-1003", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-validation-441", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-5184", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-3208", "mrqa_triviaqa-validation-7703", "mrqa_hotpotqa-validation-3921", "mrqa_triviaqa-validation-792", "mrqa_triviaqa-validation-5526", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5769"], "retrieved_ids": ["mrqa_naturalquestions-train-67301", "mrqa_naturalquestions-train-3822", "mrqa_naturalquestions-train-67853", "mrqa_naturalquestions-train-2319", "mrqa_naturalquestions-train-32228", "mrqa_naturalquestions-train-71376", "mrqa_naturalquestions-train-30469", "mrqa_naturalquestions-train-24474", "mrqa_naturalquestions-train-66484", "mrqa_naturalquestions-train-81499", "mrqa_naturalquestions-train-18873", "mrqa_naturalquestions-train-47031", "mrqa_naturalquestions-train-20647", "mrqa_naturalquestions-train-26280", "mrqa_naturalquestions-train-49135", "mrqa_naturalquestions-train-25980", "mrqa_naturalquestions-train-66453", "mrqa_naturalquestions-train-43153", "mrqa_naturalquestions-train-85599", "mrqa_naturalquestions-train-59943", "mrqa_naturalquestions-train-87478", "mrqa_naturalquestions-train-80387", "mrqa_naturalquestions-train-30029", "mrqa_naturalquestions-train-54446", "mrqa_naturalquestions-train-13115", "mrqa_naturalquestions-train-25857", "mrqa_naturalquestions-train-17007", "mrqa_naturalquestions-train-6572", "mrqa_naturalquestions-train-61273", "mrqa_naturalquestions-validation-5146", "mrqa_naturalquestions-train-34624", "mrqa_naturalquestions-train-23570"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 12, "before_eval": {"predictions": ["Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Joe Turano", "norway", "Margaret Thatcher", "Kevin Harlan as play-by-play announcer, Boomer Esiason and Dan Fouts as color analysts, and James Lofton and Mark Malone as sideline reporters", "2014", "The stability, security, and predictability of British law and government enabled Hong Kong to flourish as a centre for international trade", "Minoan", "29 June 1941", "cienfuegos", "norway", "New South Wales", "Fort Niagara", "dandy", "norway", "Orwell", "Bohemia", "Gregg Popovich", "not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence", "for creative reasons and `` not a reflection '' of the actress'performance", "innate response", "Mexican", "a musician", "norway", "December 1, 1969", "american", "norway", "California State Automobile Association", "\"alone\"", "Cinderella", "The astronauts were asphyxiated before the hatch could be opened", "civil disobedients have nonetheless found it hard to resist responding to investigators' questions"], "metric_results": {"EM": 0.09375, "QA-F1": 0.2144383888229796}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false], "QA-F1": [0.19999999999999998, 0.0, 0.0, 1.0, 0.37037037037037035, 0.0, 0.34782608695652173, 0.6666666666666666, 0.0, 0.0, 0.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.2424242424242424, 0.4, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.16666666666666666, 0.2222222222222222]}}, "error_ids": ["mrqa_squad-validation-93", "mrqa_naturalquestions-validation-6167", "mrqa_triviaqa-validation-1671", "mrqa_squad-validation-568", "mrqa_hotpotqa-validation-2680", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-1758", "mrqa_hotpotqa-validation-4263", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-6902", "mrqa_hotpotqa-validation-1099", "mrqa_squad-validation-10202", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-2737", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-2886", "mrqa_squad-validation-9793", "mrqa_naturalquestions-validation-5175", "mrqa_squad-validation-6678", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-7371", "mrqa_squad-validation-2812", "mrqa_squad-validation-3935", "mrqa_squad-validation-6924"], "retrieved_ids": ["mrqa_naturalquestions-train-56048", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-train-50435", "mrqa_naturalquestions-train-62437", "mrqa_naturalquestions-train-15740", "mrqa_naturalquestions-train-63770", "mrqa_naturalquestions-train-38094", "mrqa_naturalquestions-train-47814", "mrqa_naturalquestions-train-26015", "mrqa_naturalquestions-train-25133", "mrqa_naturalquestions-train-75572", "mrqa_naturalquestions-train-9943", "mrqa_naturalquestions-train-71212", "mrqa_naturalquestions-train-32085", "mrqa_naturalquestions-train-80422", "mrqa_naturalquestions-train-21794", "mrqa_triviaqa-validation-1924", "mrqa_naturalquestions-train-56911", "mrqa_naturalquestions-train-65437", "mrqa_naturalquestions-train-44775", "mrqa_naturalquestions-train-79688", "mrqa_naturalquestions-train-58505", "mrqa_naturalquestions-train-83218", "mrqa_naturalquestions-train-37705", "mrqa_naturalquestions-train-22308", "mrqa_naturalquestions-train-19407", "mrqa_naturalquestions-train-66157", "mrqa_naturalquestions-train-70911", "mrqa_naturalquestions-train-45881", "mrqa_naturalquestions-train-15639", "mrqa_naturalquestions-train-57784", "mrqa_naturalquestions-train-43601"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 13, "before_eval": {"predictions": ["Sister, Sister ( 1982 film)", "former president of Guggenheim Partners", "Jason Lee", "Napoleon's army", "bakers", "3.7 percent", "High and persistent unemployment, in which inequality increases, has a negative effect on subsequent long-run economic growth", "Garthy &\u00a0T Travis", "Tenacious D", "Jerry Ekandjo ( until February 2018 ), Erastus Utoni", "discipline problems with the Flight Director's orders during their flight", "9", "paddington", "amyotrophic lateral sclerosis ( ALS)", "\"Odorama\" whereby viewers could smell what they saw on screen through scratch and sniff cards", "Swiss made", "mid 1970s", "Torah or Bible", "on the western coast of Italy", "Phil Hill, who went on to become the first and only U.S. born world grand prix champion", "brass band parades", "mid November", "Facebook", "bajgiel", "Tim \"Ripper\" Owens", "Issaquah, Washington (a suburb of Seattle)", "territories of conflicting territorial claims between British and French colonies in North America were turned over to a commission to resolve, but it reached no decision", "cheated on Miley", "punk rock", "Fort Snelling, Minnesota", "pinhole camera", "infrequent rain and many sunny days"], "metric_results": {"EM": 0.09375, "QA-F1": 0.2555747204184704}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.33333333333333337, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.2222222222222222, 0.0, 0.0, 0.4444444444444445, 0.3636363636363636, 0.0, 0.0, 0.0, 0.125, 1.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.28571428571428575, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.2222222222222222, 0.5, 0.3333333333333333, 0.0, 0.5]}}, "error_ids": ["mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-1509", "mrqa_naturalquestions-validation-1135", "mrqa_squad-validation-830", "mrqa_triviaqa-validation-7638", "mrqa_squad-validation-7042", "mrqa_squad-validation-7351", "mrqa_triviaqa-validation-6944", "mrqa_hotpotqa-validation-4058", "mrqa_naturalquestions-validation-6445", "mrqa_squad-validation-4060", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-4486", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-2037", "mrqa_squad-validation-1850", "mrqa_naturalquestions-validation-150", "mrqa_hotpotqa-validation-800", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-2896", "mrqa_hotpotqa-validation-1932", "mrqa_squad-validation-10168", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7310", "mrqa_hotpotqa-validation-3669", "mrqa_triviaqa-validation-6913", "mrqa_squad-validation-2656"], "retrieved_ids": ["mrqa_naturalquestions-train-5343", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-train-10607", "mrqa_naturalquestions-train-56792", "mrqa_naturalquestions-train-28269", "mrqa_naturalquestions-train-88103", "mrqa_naturalquestions-train-7700", "mrqa_naturalquestions-train-31705", "mrqa_naturalquestions-train-6538", "mrqa_naturalquestions-train-83622", "mrqa_naturalquestions-train-13110", "mrqa_naturalquestions-train-25908", "mrqa_naturalquestions-train-15974", "mrqa_naturalquestions-train-27288", "mrqa_naturalquestions-train-1312", "mrqa_naturalquestions-train-36679", "mrqa_naturalquestions-train-15431", "mrqa_naturalquestions-train-30553", "mrqa_naturalquestions-train-57993", "mrqa_naturalquestions-train-87973", "mrqa_hotpotqa-validation-2673", "mrqa_naturalquestions-train-5926", "mrqa_naturalquestions-train-14248", "mrqa_naturalquestions-train-3865", "mrqa_naturalquestions-train-2187", "mrqa_naturalquestions-train-41819", "mrqa_naturalquestions-train-49822", "mrqa_naturalquestions-train-5497", "mrqa_naturalquestions-train-43791", "mrqa_naturalquestions-train-1342", "mrqa_naturalquestions-train-23010", "mrqa_naturalquestions-train-84237"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 14, "before_eval": {"predictions": ["Beauty and the Breast", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "Chicago, Illinois", "Dark Blood", "FX option or currency option", "electromagnetic waves", "a Wahhabi/ Salafi Jihad extremist militant group", "auspiciousness", "Dimensions in Time", "Surveyor 3", "January 1981", "gonadotropin - releasing hormone ( GnRH )", "the structure and substance of his questions and answers concerning baptism in the Small Catechism", "a Lutheran pastor in Hochfelden used a sermon to urge his parishioners to murder Jews", "\u00a31 million", "slowing the vehicle", "Cheyenne rivers", "( Sometimes) absence", "Hanna- Barbera, The Jetsons", "Cortina d'Ampezzo", "efficient and effective management of money ( funds ) in such a manner as to accomplish the objectives of the organization", "Alba Longa", "mort and an adopted daughter named Ysabell, who later got married and left him to become the Duke and Duchess of Sto Helit", "maryland", "Manchester United", "both public services and public enterprises", "1940", "weak government institutions", "a god of the Ammonites", "eye (orbital) sockets in the skull", "Uncle Fester", "Charles Whitman"], "metric_results": {"EM": 0.0625, "QA-F1": 0.181398503457327}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.16666666666666669, 0.0, 1.0, 0.5882352941176471, 0.0, 0.5454545454545454, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.14285714285714288, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-970", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3253", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10046", "mrqa_squad-validation-9751", "mrqa_triviaqa-validation-2442", "mrqa_squad-validation-7836", "mrqa_squad-validation-3999", "mrqa_naturalquestions-validation-5944", "mrqa_naturalquestions-validation-8180", "mrqa_squad-validation-2448", "mrqa_squad-validation-2509", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-116", "mrqa_squad-validation-5178", "mrqa_squad-validation-6023", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-validation-3302", "mrqa_hotpotqa-validation-1244", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-5191", "mrqa_naturalquestions-validation-6019", "mrqa_hotpotqa-validation-241", "mrqa_squad-validation-8444", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-5256"], "retrieved_ids": ["mrqa_naturalquestions-train-46380", "mrqa_naturalquestions-train-42253", "mrqa_naturalquestions-train-81285", "mrqa_naturalquestions-train-78256", "mrqa_naturalquestions-train-51906", "mrqa_naturalquestions-train-11351", "mrqa_naturalquestions-train-41128", "mrqa_hotpotqa-validation-3846", "mrqa_naturalquestions-train-27607", "mrqa_naturalquestions-train-62033", "mrqa_naturalquestions-train-80006", "mrqa_naturalquestions-train-55300", "mrqa_squad-validation-6947", "mrqa_naturalquestions-train-55166", "mrqa_naturalquestions-train-65340", "mrqa_naturalquestions-train-49364", "mrqa_triviaqa-validation-2096", "mrqa_naturalquestions-train-76644", "mrqa_naturalquestions-train-56108", "mrqa_naturalquestions-train-55150", "mrqa_naturalquestions-train-75138", "mrqa_naturalquestions-train-75876", "mrqa_naturalquestions-train-74843", "mrqa_naturalquestions-train-59367", "mrqa_naturalquestions-train-44782", "mrqa_naturalquestions-train-63001", "mrqa_naturalquestions-train-57735", "mrqa_naturalquestions-train-62975", "mrqa_naturalquestions-train-24834", "mrqa_naturalquestions-train-541", "mrqa_naturalquestions-train-34559", "mrqa_naturalquestions-train-22478"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 15, "before_eval": {"predictions": ["superhuman abilities", "Deathly Hallows", "the Flatbush section of Brooklyn, New York City", "San Antonio", "Christina Aguilera and Taio Cruz", "jupiter (now known as the Galilean moons)", "Galileo Ferraris", "hana", "'Lady Arbuthnot's Chamber ''", "Theodore Haynes (1989)", "the Old Town Hall, Gateshead", "Motown, Philly soul, and Earth, Wind & Fire ( particularly `` That's the Way of the World '' )", "The neck", "1898", "professional wrestler, mixed martial artist and a former amateur wrestler", "Payaya Indians", "steal the plans for the Death Star, the Galactic Empire's superweapon", "The Bells of St. Mary's", "a Curtiss JN-4 airplane", "tibility for impressions, and an inclination to be touched by emotions", "gorillas", "March 15, 1945", "absolute temperature", "the private intelligence firm Stratfor", "Sam Waterston", "bicuspid", "his brother, Menelaus", "25 November 2015", "tallahassee", "prefabricated housing projects", "boston b brides\u2019s", "Tampa, Florida (WFTS-TV and WWSB) and Grand Rapids, Michigan (WZZM and WOTV)"], "metric_results": {"EM": 0.09375, "QA-F1": 0.13234577922077922}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.5714285714285715, 1.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-2612", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1479", "mrqa_hotpotqa-validation-335", "mrqa_triviaqa-validation-7512", "mrqa_squad-validation-1374", "mrqa_triviaqa-validation-5979", "mrqa_naturalquestions-validation-10439", "mrqa_hotpotqa-validation-1048", "mrqa_squad-validation-5249", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-56", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-validation-368", "mrqa_hotpotqa-validation-573", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-5877", "mrqa_triviaqa-validation-7578", "mrqa_hotpotqa-validation-5188", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-365", "mrqa_hotpotqa-validation-1714", "mrqa_triviaqa-validation-1736", "mrqa_naturalquestions-validation-9451", "mrqa_hotpotqa-validation-413", "mrqa_triviaqa-validation-1166", "mrqa_squad-validation-874", "mrqa_triviaqa-validation-1588", "mrqa_squad-validation-6091"], "retrieved_ids": ["mrqa_naturalquestions-train-3264", "mrqa_naturalquestions-train-2998", "mrqa_naturalquestions-train-9689", "mrqa_naturalquestions-train-84048", "mrqa_naturalquestions-train-50285", "mrqa_naturalquestions-train-18549", "mrqa_naturalquestions-train-5978", "mrqa_naturalquestions-train-12519", "mrqa_naturalquestions-train-67383", "mrqa_naturalquestions-train-32880", "mrqa_naturalquestions-train-32838", "mrqa_naturalquestions-train-56694", "mrqa_naturalquestions-train-70771", "mrqa_naturalquestions-train-88225", "mrqa_naturalquestions-train-70855", "mrqa_naturalquestions-train-30649", "mrqa_naturalquestions-train-74979", "mrqa_hotpotqa-validation-3978", "mrqa_naturalquestions-train-29733", "mrqa_naturalquestions-train-14055", "mrqa_naturalquestions-train-28644", "mrqa_naturalquestions-train-45845", "mrqa_naturalquestions-train-36274", "mrqa_triviaqa-validation-681", "mrqa_naturalquestions-train-86481", "mrqa_naturalquestions-train-14989", "mrqa_naturalquestions-train-27706", "mrqa_naturalquestions-train-27365", "mrqa_naturalquestions-train-15426", "mrqa_naturalquestions-train-67181", "mrqa_naturalquestions-train-38997", "mrqa_naturalquestions-train-50182"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 16, "before_eval": {"predictions": ["florida", "cheeses made with milk collected from a group of farms that are located within close proximity to where the cheese is produced", "florida", "on the lateral side of the tibia, with which it is connected above and below", "ferguside royal clan", "the North Sea, through the former Meuse estuary, near Rotterdam", "the Kalahari Desert", "Colin Montgomerie", "October 29, 1985", "Amway", "the Mauritius Examinations Syndicate", "George Stigler", "Speaker of the Lok Sabha or in his absence, the Deputy - Chairman of the Rajya Sabha", "the Royal Festival Hall", "kPMG (T) Limited", "the Republic of Niger", "florida", "a statue of fame", "allowing a child to go through a torturous treatment to gain information", "Fulham, Greater London, England", "French, English and Spanish", "florida", "U.S. Marshals", "What's Up (TV series)", "supply chain", "Mars", "Poland's Last King and English Culture", "polynomial algebra", "florida", "three mystic apes", "sheepskin", "Honolulu"], "metric_results": {"EM": 0.0625, "QA-F1": 0.13437500000000002}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.28571428571428575, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21428571428571425, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2324", "mrqa_naturalquestions-validation-9218", "mrqa_triviaqa-validation-313", "mrqa_squad-validation-9319", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4681", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-4094", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-3483", "mrqa_hotpotqa-validation-1566", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-4055", "mrqa_squad-validation-5407", "mrqa_naturalquestions-validation-7144", "mrqa_hotpotqa-validation-4164", "mrqa_triviaqa-validation-639", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-1001", "mrqa_triviaqa-validation-1770", "mrqa_triviaqa-validation-866", "mrqa_squad-validation-8223", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-validation-9087", "mrqa_hotpotqa-validation-2287"], "retrieved_ids": ["mrqa_naturalquestions-train-77220", "mrqa_naturalquestions-train-32001", "mrqa_naturalquestions-train-58276", "mrqa_naturalquestions-train-18148", "mrqa_naturalquestions-train-62641", "mrqa_naturalquestions-train-36487", "mrqa_naturalquestions-train-57242", "mrqa_naturalquestions-train-37024", "mrqa_naturalquestions-train-24249", "mrqa_naturalquestions-train-61047", "mrqa_naturalquestions-train-24403", "mrqa_naturalquestions-train-56738", "mrqa_naturalquestions-train-6774", "mrqa_triviaqa-validation-6119", "mrqa_naturalquestions-train-61694", "mrqa_naturalquestions-validation-5651", "mrqa_naturalquestions-train-88003", "mrqa_naturalquestions-train-53688", "mrqa_naturalquestions-train-80350", "mrqa_naturalquestions-train-62956", "mrqa_naturalquestions-train-63739", "mrqa_naturalquestions-train-62378", "mrqa_naturalquestions-train-10931", "mrqa_naturalquestions-train-69906", "mrqa_naturalquestions-train-37579", "mrqa_naturalquestions-train-25331", "mrqa_naturalquestions-train-65138", "mrqa_naturalquestions-train-59801", "mrqa_naturalquestions-train-15366", "mrqa_naturalquestions-train-43213", "mrqa_naturalquestions-train-36118", "mrqa_naturalquestions-train-75749"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 17, "before_eval": {"predictions": ["pasteurised cows' milk soft cheese", "X-Men: Apocalypse", "August 6, 1845", "maximum energy of 687 keV and an average energy of 251 keV", "James Zeebo", "sovereign states", "president of the United States", "\"Teach the Controversy\" campaign", "Major William Lennox and Master Sergeant Robert Epps", "Australian", "military service for men was 30 months and for women 18 months ( although in accordance with a temporary order from January 10, 1968, six additional months were added to the mandatory service, 36 months for men and 24 months for women respectively", "opportunities will vary by geographic area and subject taught", "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued)", "a private liberal arts college", "Roy Warren Spencer", "\"antiforms\"", "fifth season", "\"Veyyil\" (2006)", "Grace Nail Johnson", "Mick Jagger", "prime number p with n < p < 2n \u2212 2, for any natural number n > 3", "Bangor International Airport", "knowledgeable in that one area", "0 \u00b0", "Cartoon Network", "Presiding Officer", "Miami Heat", "33", "dactylosphaera vitifoliae", "Annual Conference Cabinet", "bronze medal", "William Hartnell's poor health"], "metric_results": {"EM": 0.21875, "QA-F1": 0.36082482993197273}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.08333333333333334, 0.0, 1.0, 0.16666666666666666, 0.0, 0.36734693877551017, 0.19999999999999998, 0.16, 0.0, 0.8, 1.0, 0.28571428571428575, 0.6666666666666666, 0.3333333333333333, 0.0, 0.5, 0.25, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1298", "mrqa_squad-validation-608", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3559", "mrqa_triviaqa-validation-314", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-3440", "mrqa_naturalquestions-validation-1090", "mrqa_squad-validation-2053", "mrqa_squad-validation-7572", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-4868", "mrqa_naturalquestions-validation-9171", "mrqa_squad-validation-8966", "mrqa_hotpotqa-validation-2782", "mrqa_squad-validation-1904", "mrqa_naturalquestions-validation-10347", "mrqa_hotpotqa-validation-613", "mrqa_hotpotqa-validation-4463", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-1130", "mrqa_squad-validation-7664"], "retrieved_ids": ["mrqa_naturalquestions-train-42599", "mrqa_naturalquestions-train-21927", "mrqa_naturalquestions-train-70332", "mrqa_naturalquestions-train-50054", "mrqa_naturalquestions-train-84237", "mrqa_naturalquestions-train-43580", "mrqa_naturalquestions-train-88235", "mrqa_naturalquestions-train-85292", "mrqa_naturalquestions-train-76193", "mrqa_naturalquestions-train-59810", "mrqa_naturalquestions-train-82427", "mrqa_naturalquestions-train-66804", "mrqa_naturalquestions-train-78095", "mrqa_naturalquestions-train-25583", "mrqa_naturalquestions-train-27365", "mrqa_naturalquestions-train-34683", "mrqa_naturalquestions-train-9599", "mrqa_naturalquestions-train-54068", "mrqa_naturalquestions-train-53324", "mrqa_naturalquestions-train-35314", "mrqa_naturalquestions-train-15758", "mrqa_naturalquestions-train-57961", "mrqa_naturalquestions-train-49622", "mrqa_naturalquestions-train-54474", "mrqa_naturalquestions-train-75430", "mrqa_naturalquestions-train-40405", "mrqa_triviaqa-validation-781", "mrqa_naturalquestions-train-43677", "mrqa_naturalquestions-train-6283", "mrqa_naturalquestions-train-53196", "mrqa_naturalquestions-train-50636", "mrqa_naturalquestions-train-41033"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 18, "before_eval": {"predictions": ["the genocide against the Tutsi", "working harmoniously to fulfill the needs of every student in the classroom", "500 metres", "Vili Fualaau and Mary Kay Letourneau", "the entertainment division", "distance covered by a vehicle ( for example as recorded by an odometer ), person, animal, or object along a curved path from a point A to a point B", "12", "the Great Exhibition of 1851", "Edward Longshanks and the Hammer of the Scots", "the Chagos Archipelago", "jute factories traditionally associated with the city had by then closed down along with other related industries", "the person compelled to pay for reformist programs", "1587", "\"Grindhouse\" fake trailer", "Maureen Connolly", "digital transmission modes such as MFSK and Olivia", "the Swiss- Austrian border", "lithium-ion battery factory", "821", "HD channels and Video On Demand content which was not previously carried by cable", "pressure difference", "Kim Hyun-ah", "the races of highest'social efficiency'", "transposition changes the relationship of the overall pitch range compared to the range of the instruments or voices that perform the music", "the \" King of Cool\" for his seemingly effortless charisma and self-assurance", "President Wilson and the American delegation from the Paris Peace Conference", "pharaoh the Younger", "the fifth season", "the kray twins, 49, had been seen in public since being sentenced to life imprisonment for murder", "Hockey Club Davos", "Michael Crawford", "Aibak's successor and son - in - law Iltutmish"], "metric_results": {"EM": 0.125, "QA-F1": 0.30186332764457763}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.4, 0.18181818181818182, 1.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.8, 1.0, 0.125, 0.6666666666666666, 0.0, 0.6666666666666665, 0.0, 0.4615384615384615, 0.8, 0.0, 0.0, 0.0, 0.4, 0.4, 0.25]}}, "error_ids": ["mrqa_hotpotqa-validation-1441", "mrqa_squad-validation-1914", "mrqa_hotpotqa-validation-2145", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-3300", "mrqa_squad-validation-5257", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-5036", "mrqa_triviaqa-validation-2683", "mrqa_hotpotqa-validation-2201", "mrqa_triviaqa-validation-2715", "mrqa_naturalquestions-validation-2222", "mrqa_squad-validation-9074", "mrqa_hotpotqa-validation-4415", "mrqa_squad-validation-2862", "mrqa_triviaqa-validation-4279", "mrqa_hotpotqa-validation-1855", "mrqa_squad-validation-9841", "mrqa_naturalquestions-validation-4497", "mrqa_hotpotqa-validation-4068", "mrqa_squad-validation-9827", "mrqa_triviaqa-validation-1764", "mrqa_hotpotqa-validation-3798", "mrqa_triviaqa-validation-7100", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-511", "mrqa_naturalquestions-validation-10490"], "retrieved_ids": ["mrqa_naturalquestions-train-62445", "mrqa_naturalquestions-train-56738", "mrqa_naturalquestions-train-76576", "mrqa_naturalquestions-train-33783", "mrqa_naturalquestions-train-39788", "mrqa_naturalquestions-train-10251", "mrqa_naturalquestions-train-25982", "mrqa_naturalquestions-train-68062", "mrqa_naturalquestions-train-6651", "mrqa_naturalquestions-train-18489", "mrqa_naturalquestions-train-44455", "mrqa_naturalquestions-train-25102", "mrqa_naturalquestions-train-35572", "mrqa_naturalquestions-train-50885", "mrqa_naturalquestions-train-44166", "mrqa_naturalquestions-train-19008", "mrqa_naturalquestions-train-14129", "mrqa_naturalquestions-train-15969", "mrqa_naturalquestions-train-81896", "mrqa_naturalquestions-train-71286", "mrqa_naturalquestions-train-33677", "mrqa_naturalquestions-train-75817", "mrqa_naturalquestions-train-67715", "mrqa_naturalquestions-train-3814", "mrqa_naturalquestions-train-15222", "mrqa_naturalquestions-train-52867", "mrqa_naturalquestions-train-53077", "mrqa_naturalquestions-train-51271", "mrqa_naturalquestions-train-48604", "mrqa_naturalquestions-train-81410", "mrqa_naturalquestions-train-56326", "mrqa_naturalquestions-train-47693"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 19, "before_eval": {"predictions": ["Mauritian", "Norman Macdonnell", "aragon", "7.8 percent", "the first trans-Pacific flight", "Sharman Joshi", "quietly", "Forster I, Forster II, and Forster III", "prime", "Ana", "Cherry Hill", "In `` The Chump ''", "venus cheney", "venys o'Donnell", "a 1993 American comedy - drama film", "Blackstar", "Indian", "fear of the Lord", "1974", "Nicki Minaj", "slave of duty", "Huguenot ancestry", "venio fagioli", "friedrich Engels", "Drawn Together", "William the Conqueror", "Tel Aviv", "two", "Corinthian and Saronic Gulfs", "venus trix", "Guinness World Records", "Southern Progress Corporation"], "metric_results": {"EM": 0.25, "QA-F1": 0.3442708333333333}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.6666666666666666, 0.8, 0.0, 1.0, 1.0, 0.25, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2389", "mrqa_triviaqa-validation-6901", "mrqa_squad-validation-6963", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-303", "mrqa_squad-validation-9214", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-9284", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-6935", "mrqa_naturalquestions-validation-4354", "mrqa_triviaqa-validation-1995", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-8025", "mrqa_squad-validation-3151", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4068", "mrqa_hotpotqa-validation-57", "mrqa_naturalquestions-validation-1565", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-3552", "mrqa_hotpotqa-validation-4020"], "retrieved_ids": ["mrqa_naturalquestions-train-43875", "mrqa_naturalquestions-train-76537", "mrqa_naturalquestions-train-7001", "mrqa_naturalquestions-train-16977", "mrqa_naturalquestions-train-20273", "mrqa_naturalquestions-train-51653", "mrqa_naturalquestions-train-5161", "mrqa_naturalquestions-train-2884", "mrqa_naturalquestions-train-5996", "mrqa_naturalquestions-train-56167", "mrqa_naturalquestions-train-10931", "mrqa_naturalquestions-train-78881", "mrqa_naturalquestions-train-82715", "mrqa_naturalquestions-train-10813", "mrqa_naturalquestions-train-37455", "mrqa_naturalquestions-train-45764", "mrqa_squad-validation-3113", "mrqa_naturalquestions-train-18339", "mrqa_naturalquestions-train-48993", "mrqa_naturalquestions-train-66330", "mrqa_naturalquestions-train-9602", "mrqa_naturalquestions-train-71093", "mrqa_naturalquestions-train-53177", "mrqa_naturalquestions-train-1948", "mrqa_naturalquestions-train-2426", "mrqa_naturalquestions-train-54343", "mrqa_naturalquestions-train-57610", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-train-85136", "mrqa_naturalquestions-train-57484", "mrqa_naturalquestions-train-21691", "mrqa_naturalquestions-train-87680"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 20, "before_eval": {"predictions": ["gas turbines", "Michael Koman", "prints and architectural drawings", "kathy Najimy", "kalos", "British Columbia, in the Abbotsford, Vancouver and Langley areas", "the Gemini and Apollo programs", "ribosomal", "kookaburra", "six", "Scott Bakula as Dwayne `` King '' Cassius Pride, NCIS Supervisory Special Agent", "'Friends\"", "Cozonac", "brian Kingston (DUP)", "a lunar radiation environment experiment using a Geiger\u2013M\u00fcller tube detector and a lunar photography experiment", "Lucius Cornelius Sulla Felix", "following the 2017 season", "Golden Globe", "Swahili", "the primacy of core Christian values such as love, patience, charity, and freedom", "karl marx", "Pantone Matching System (PMS)", "Firoz Shah Tughlaq", "\"The Love from the Star\"", "San Jose Marriott", "sea wasp", "Suzanne N.J.'Susie' Chun Oakland", "a \"teleforce\" weapon", "Native American tradition", "giving Super Bowl ever", "65 years of age or older", "george marx"], "metric_results": {"EM": 0.15625, "QA-F1": 0.27278860845037317}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.48484848484848486, 0.0, 0.0, 1.0, 0.0, 0.3076923076923077, 0.0, 0.2857142857142857, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.888888888888889, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-779", "mrqa_hotpotqa-validation-3547", "mrqa_triviaqa-validation-5091", "mrqa_naturalquestions-validation-4590", "mrqa_squad-validation-3964", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-1210", "mrqa_naturalquestions-validation-1279", "mrqa_hotpotqa-validation-4627", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-4852", "mrqa_hotpotqa-validation-3623", "mrqa_hotpotqa-validation-2407", "mrqa_naturalquestions-validation-2448", "mrqa_squad-validation-8464", "mrqa_squad-validation-2280", "mrqa_triviaqa-validation-6721", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-4015", "mrqa_squad-validation-315", "mrqa_triviaqa-validation-1453", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2064", "mrqa_squad-validation-393", "mrqa_squad-validation-7272", "mrqa_triviaqa-validation-935"], "retrieved_ids": ["mrqa_naturalquestions-train-21734", "mrqa_squad-validation-392", "mrqa_naturalquestions-train-49362", "mrqa_naturalquestions-train-47505", "mrqa_naturalquestions-train-68697", "mrqa_naturalquestions-train-40831", "mrqa_naturalquestions-train-872", "mrqa_naturalquestions-train-7385", "mrqa_naturalquestions-train-21959", "mrqa_naturalquestions-train-71648", "mrqa_naturalquestions-train-17271", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-train-26894", "mrqa_naturalquestions-train-37123", "mrqa_naturalquestions-train-21807", "mrqa_squad-validation-8832", "mrqa_naturalquestions-train-82297", "mrqa_naturalquestions-train-47115", "mrqa_naturalquestions-train-79304", "mrqa_naturalquestions-train-27583", "mrqa_naturalquestions-train-38028", "mrqa_naturalquestions-train-32032", "mrqa_naturalquestions-train-8619", "mrqa_naturalquestions-train-31300", "mrqa_naturalquestions-train-7464", "mrqa_naturalquestions-train-13928", "mrqa_naturalquestions-train-82926", "mrqa_naturalquestions-train-76060", "mrqa_naturalquestions-train-9229", "mrqa_naturalquestions-train-74401", "mrqa_naturalquestions-train-37288", "mrqa_naturalquestions-validation-3208"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 21, "before_eval": {"predictions": ["a Czech word, robota, meaning `` forced labor ''", "daphne du maurier", "various registries", "d Ananya Mandal, MD", "Yazoo", "22 April 1894", "black hole", "the soul of a man \"in this life\" (homo enim in hac vita) tired from his daily labour", "The British offered France the choice of surrendering either its continental North American possessions east of the Mississippi or the Caribbean islands of Guadeloupe and Martinique", "Kris Kristofferson", "Manson & Woods", "The Tech Coast is a moniker that has gained use as a descriptor for the region's diversified technology and industrial base as well as its multitude of prestigious and world-renowned research universities and other public and private institutions", "a French pirate active in the Caribbean and off the coast of Africa", "Smith Jerrod", "Charles Dickens", "a forum in which the Nobel Peace Laureates and the Peace Laureate Organizations could come together to address global issues with a view to encourage and support peace and human well being in the world", "adenine", "2001", "that there are infinitely many primes", "morning news", "padlocking the gates and using sickles to deflate one of the large domes covering two satellite dishes", "the 1950s", "Tallemaja \"pine tree Mary\"", "in western portions of the Great Lakes region (an area not directly subject to the conflict between the French and British", "Orthodox Christians", "James Bond", "640 \u00d7 1136 at 326 ppi", "george", "the Western Atlantic ctenophore Mnemiopsis leidyi", "\"Menace II Society\"", "quarterback", "Larry Gatlin & the Gatlin Brothers Band"], "metric_results": {"EM": 0.09375, "QA-F1": 0.19550282309279987}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.14285714285714288, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.22222222222222218, 0.0, 0.0, 0.05714285714285715, 0.0, 0.4444444444444445, 0.0, 0.4210526315789474, 0.0, 1.0, 0.0, 0.0, 0.23529411764705882, 0.0, 0.0, 0.56, 0.0, 0.0, 0.0, 0.0, 0.7692307692307693, 1.0, 0.25, 0.15384615384615385]}}, "error_ids": ["mrqa_naturalquestions-validation-3609", "mrqa_triviaqa-validation-5239", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-751", "mrqa_triviaqa-validation-4383", "mrqa_hotpotqa-validation-4430", "mrqa_squad-validation-2412", "mrqa_squad-validation-10502", "mrqa_hotpotqa-validation-5480", "mrqa_triviaqa-validation-2961", "mrqa_squad-validation-2709", "mrqa_hotpotqa-validation-4950", "mrqa_naturalquestions-validation-5439", "mrqa_squad-validation-5345", "mrqa_hotpotqa-validation-35", "mrqa_squad-validation-3627", "mrqa_squad-validation-9020", "mrqa_triviaqa-validation-1954", "mrqa_squad-validation-6844", "mrqa_naturalquestions-validation-8689", "mrqa_hotpotqa-validation-2399", "mrqa_squad-validation-10177", "mrqa_squad-validation-8456", "mrqa_triviaqa-validation-6950", "mrqa_naturalquestions-validation-6832", "mrqa_triviaqa-validation-1555", "mrqa_squad-validation-4648", "mrqa_naturalquestions-validation-2758", "mrqa_hotpotqa-validation-4676"], "retrieved_ids": ["mrqa_naturalquestions-train-23992", "mrqa_naturalquestions-train-9623", "mrqa_naturalquestions-train-43371", "mrqa_naturalquestions-train-33348", "mrqa_naturalquestions-train-18206", "mrqa_naturalquestions-train-77220", "mrqa_naturalquestions-train-82140", "mrqa_naturalquestions-train-86662", "mrqa_naturalquestions-train-51482", "mrqa_naturalquestions-train-62106", "mrqa_naturalquestions-train-76498", "mrqa_naturalquestions-train-30799", "mrqa_naturalquestions-train-42119", "mrqa_naturalquestions-train-87613", "mrqa_naturalquestions-train-87578", "mrqa_naturalquestions-train-16364", "mrqa_naturalquestions-train-9210", "mrqa_naturalquestions-train-72438", "mrqa_naturalquestions-train-80194", "mrqa_naturalquestions-train-52562", "mrqa_naturalquestions-train-7510", "mrqa_naturalquestions-train-37894", "mrqa_squad-validation-2069", "mrqa_naturalquestions-train-43221", "mrqa_naturalquestions-train-50755", "mrqa_naturalquestions-train-51928", "mrqa_naturalquestions-train-4613", "mrqa_naturalquestions-train-54064", "mrqa_naturalquestions-train-28233", "mrqa_naturalquestions-train-83407", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-train-58404"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 22, "before_eval": {"predictions": ["Alex Skuby", "England", "the first Thursday in May", "MSC Crociere S. p.A.", "trait\u00e9 de la science des Finances", "Humpty Dumpty", "The centre-right Liberal Party of Australia", "Parliamentarians", "any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "placed on the heads of Christians on Ash Wednesday, either by being sprinkled over their heads or, in English - speaking countries, more often by being marked on their foreheads as a visible cross", "al\u00e9a seydoux", "alibris", "Augustus Waters", "1619", "gail Rebuck", "gambling", "June 11, 1973", "in Kenya and in the Masai Mara in particular", "a chronological collection of critical quotations about William Shakespeare and his works", "alison i", "neutrality", "Cargill", "AMC Entertainment Holdings, Inc.", "\"The Gang\"", "3 October 1990", "March 1, 2018", "The weak force", "daedalus", "younger", "Manhattan District ; `` Manhattan ''", "fictional county of Barsetshire", "vast"], "metric_results": {"EM": 0.09375, "QA-F1": 0.25534925144300147}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.6666666666666665, 0.33333333333333337, 0.1111111111111111, 0.16666666666666666, 0.0, 0.0, 0.3636363636363636, 1.0, 0.0, 0.0, 0.0, 0.4, 0.625, 0.0, 0.33333333333333337, 0.0, 0.3333333333333333, 0.0, 0.2, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.6666666666666666]}}, "error_ids": ["mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1883", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-9789", "mrqa_triviaqa-validation-5698", "mrqa_squad-validation-2884", "mrqa_hotpotqa-validation-2959", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-1578", "mrqa_triviaqa-validation-2797", "mrqa_naturalquestions-validation-3859", "mrqa_triviaqa-validation-4731", "mrqa_squad-validation-2769", "mrqa_hotpotqa-validation-482", "mrqa_squad-validation-8280", "mrqa_hotpotqa-validation-5655", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-190", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-1929", "mrqa_naturalquestions-validation-1328", "mrqa_hotpotqa-validation-2448", "mrqa_squad-validation-2828"], "retrieved_ids": ["mrqa_naturalquestions-train-2259", "mrqa_naturalquestions-train-55096", "mrqa_naturalquestions-train-87508", "mrqa_naturalquestions-train-39276", "mrqa_naturalquestions-train-61501", "mrqa_naturalquestions-train-83188", "mrqa_naturalquestions-train-88000", "mrqa_naturalquestions-train-46596", "mrqa_naturalquestions-train-41697", "mrqa_naturalquestions-train-37012", "mrqa_hotpotqa-validation-1436", "mrqa_naturalquestions-train-62103", "mrqa_naturalquestions-train-6406", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-train-56024", "mrqa_naturalquestions-train-9574", "mrqa_naturalquestions-train-43368", "mrqa_naturalquestions-train-78881", "mrqa_naturalquestions-train-83530", "mrqa_naturalquestions-train-10724", "mrqa_naturalquestions-train-2191", "mrqa_naturalquestions-train-82036", "mrqa_naturalquestions-train-48741", "mrqa_naturalquestions-train-68456", "mrqa_naturalquestions-train-19157", "mrqa_naturalquestions-train-81233", "mrqa_naturalquestions-train-64472", "mrqa_naturalquestions-train-66434", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-train-21934", "mrqa_naturalquestions-train-55308"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 23, "before_eval": {"predictions": ["$105 billion", "mono", "about two-thirds the size", "1937 Austin Seven Ruby Open Top Tourer", "j Javier (Luna)", "a lead - based topcoat", "Steeplechase Park", "Best Animated Feature", "European Union institutions", "26, 45, and 46", "nine other contenders from across the United States", "NASA's CAL IPSO satellite", "celandine", "Ulbricht", "Ronald Ralph \"Ronnie\" Schell", "sesquiterpene lactone", "Mumbai, Maharashtra", "east", "1939", "2017 / 18", "1707", "on Fresno's far southeast side, bounded by Chestnut Avenue to the West", "southern part of Nigeria", "benanga", "Incudomalleolar joint", "bobby riggs", "Democritus", "Santa Clara Marriott", "benjamin barenboim", "political power generated by wealth", "log-space reductions", "Ted Ginn Jr."], "metric_results": {"EM": 0.21875, "QA-F1": 0.3472222222222222}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.6666666666666666, 0.25, 0.0, 0.25, 0.4, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 0.2, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-3716", "mrqa_squad-validation-8850", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-3933", "mrqa_naturalquestions-validation-1617", "mrqa_hotpotqa-validation-789", "mrqa_squad-validation-4118", "mrqa_hotpotqa-validation-2741", "mrqa_squad-validation-542", "mrqa_squad-validation-4228", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-validation-3714", "mrqa_hotpotqa-validation-1782", "mrqa_triviaqa-validation-3595", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-220", "mrqa_naturalquestions-validation-4048", "mrqa_squad-validation-4662", "mrqa_naturalquestions-validation-2212", "mrqa_triviaqa-validation-571", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-3420", "mrqa_hotpotqa-validation-2340", "mrqa_triviaqa-validation-6781", "mrqa_squad-validation-1750"], "retrieved_ids": ["mrqa_naturalquestions-train-36117", "mrqa_naturalquestions-train-57610", "mrqa_naturalquestions-train-56996", "mrqa_naturalquestions-train-41596", "mrqa_naturalquestions-train-40117", "mrqa_naturalquestions-train-47535", "mrqa_naturalquestions-train-76754", "mrqa_naturalquestions-train-21691", "mrqa_naturalquestions-train-85233", "mrqa_naturalquestions-train-71230", "mrqa_naturalquestions-train-32105", "mrqa_naturalquestions-train-64039", "mrqa_naturalquestions-train-32584", "mrqa_naturalquestions-train-80089", "mrqa_naturalquestions-train-66404", "mrqa_naturalquestions-train-9576", "mrqa_naturalquestions-train-5926", "mrqa_naturalquestions-train-14726", "mrqa_naturalquestions-train-27450", "mrqa_triviaqa-validation-2683", "mrqa_naturalquestions-train-63521", "mrqa_naturalquestions-train-45410", "mrqa_naturalquestions-train-40683", "mrqa_naturalquestions-train-19615", "mrqa_naturalquestions-train-85636", "mrqa_naturalquestions-train-48221", "mrqa_naturalquestions-train-67080", "mrqa_naturalquestions-train-16031", "mrqa_naturalquestions-train-67438", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-train-8140", "mrqa_naturalquestions-train-36874"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 24, "before_eval": {"predictions": ["the 1965 -- 66 season", "NADP+ though sometimes they can flow back down more H+-pumping electron transport chains to transport more hydrogen ions into the thylakoid space to generate more ATP", "Pitt", "maddened him with hunger the next", "WBC and lineal titles", "moluccas", "Saturday in May", "Cordelia", "United States Secretary of State Henry Kissinger had negotiated an Israeli troop withdrawal from parts of the Sinai Peninsula", "1990", "J.R. R. Tolkien", "John Elway", "Selena Gomez", "learn a trade such as tailoring, carpentry, motor vehicle repair, brick-laying and masonry", "bingo", "Eugene", "new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "prime numbers that are of the form 2p \u2212 1, where p is an arbitrary prime", "letter series", "Fa Ze Banks", "red", "two Mongols and a Muslim", "along the eastern coast of the continent, from Nova Scotia and Newfoundland in the north, to Georgia in the south", "friars Minor Conventual (O.F.M. Conv)", "CD Castell\u00f3n", "1789, or 1798", "finished the regular season with a 12\u20134 record, and denied the New England Patriots a chance to defend their title from Super Bowl XLIX by defeating them 20\u201318 in the AFC Championship Game", "having colloblasts", "Jon M. Chu", "Mission Specialist for mission STS-51-L.", "due to clear weather", "prime minister of France"], "metric_results": {"EM": 0.09375, "QA-F1": 0.2284419240294921}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.07692307692307693, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.20000000000000004, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.4210526315789473, 0.0, 0.0, 0.0, 0.4, 0.2222222222222222, 0.0, 0.4, 0.0, 0.0689655172413793, 1.0, 0.0, 0.33333333333333337, 0.21621621621621626, 0.4]}}, "error_ids": ["mrqa_naturalquestions-validation-8444", "mrqa_squad-validation-8876", "mrqa_triviaqa-validation-3678", "mrqa_hotpotqa-validation-4444", "mrqa_triviaqa-validation-3340", "mrqa_hotpotqa-validation-3290", "mrqa_naturalquestions-validation-9011", "mrqa_squad-validation-3741", "mrqa_squad-validation-5399", "mrqa_hotpotqa-validation-3247", "mrqa_naturalquestions-validation-10381", "mrqa_squad-validation-8473", "mrqa_triviaqa-validation-667", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-309", "mrqa_squad-validation-8979", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-3297", "mrqa_triviaqa-validation-3847", "mrqa_squad-validation-6204", "mrqa_squad-validation-10148", "mrqa_triviaqa-validation-5304", "mrqa_hotpotqa-validation-5588", "mrqa_naturalquestions-validation-3504", "mrqa_squad-validation-67", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-1814", "mrqa_naturalquestions-validation-6508", "mrqa_triviaqa-validation-1306"], "retrieved_ids": ["mrqa_naturalquestions-train-7564", "mrqa_naturalquestions-train-36676", "mrqa_naturalquestions-train-68584", "mrqa_naturalquestions-train-22506", "mrqa_naturalquestions-train-10464", "mrqa_naturalquestions-train-55031", "mrqa_naturalquestions-train-2629", "mrqa_naturalquestions-train-29731", "mrqa_naturalquestions-train-79403", "mrqa_naturalquestions-train-44654", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-train-88035", "mrqa_naturalquestions-train-31758", "mrqa_naturalquestions-train-56987", "mrqa_naturalquestions-train-48296", "mrqa_naturalquestions-train-54961", "mrqa_naturalquestions-train-16721", "mrqa_naturalquestions-train-10122", "mrqa_naturalquestions-train-18922", "mrqa_naturalquestions-train-9565", "mrqa_naturalquestions-train-71277", "mrqa_squad-validation-5407", "mrqa_naturalquestions-train-4728", "mrqa_naturalquestions-train-21594", "mrqa_naturalquestions-train-64144", "mrqa_naturalquestions-train-22930", "mrqa_naturalquestions-train-41813", "mrqa_naturalquestions-train-75211", "mrqa_naturalquestions-train-27439", "mrqa_hotpotqa-validation-501", "mrqa_naturalquestions-train-16782", "mrqa_naturalquestions-train-47333"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 25, "before_eval": {"predictions": ["synchronized swimming", "france", "over 50 million singles", "secessionists of the Confederate States, who advocated for states'rights to expand slavery", "1923 and 1925", "Orlando\u2013Kissimmee\u2013 Sanford, Florida Metropolitan Statistical Area", "January 19, 1962", "Frigate", "france", "The party with the highest quotient is awarded the seat, which is then added to its constituency seats in allocating the second seat", "geese", "the move from the manufacturing sector to the service sector", "france", "Sylvester McCoy", "August 14, 1848", "lower rates of health and social problems", "juveniles", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "the way they used `` rule '' and `` method '' to go about their religious affairs", "Nicholas Dante", "2,664", "there is a specific weak point on the inside of the chassis right beneath the volume buttons that allows it to bend very easily with pressure added in the right place", "a chute", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "Facility Services", "Symphony No. 7", "dordogne", "1603", "ranked above the two personal physicians of the Emperor", "france", "to improve our conscious contact with God", "wrigley"], "metric_results": {"EM": 0.15625, "QA-F1": 0.30262393856143854}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.3333333333333333, 0.13333333333333333, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.3333333333333333, 0.25, 0.33333333333333337, 0.4615384615384615, 0.923076923076923, 0.0, 1.0, 0.0, 0.2857142857142857, 0.3636363636363636, 0.0, 1.0, 0.0, 1.0, 0.22222222222222224, 0.0, 0.1, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-5788", "mrqa_hotpotqa-validation-1818", "mrqa_naturalquestions-validation-10169", "mrqa_naturalquestions-validation-4072", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-3977", "mrqa_triviaqa-validation-6091", "mrqa_squad-validation-9532", "mrqa_squad-validation-7382", "mrqa_triviaqa-validation-7336", "mrqa_squad-validation-7677", "mrqa_hotpotqa-validation-4720", "mrqa_squad-validation-7301", "mrqa_squad-validation-4637", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-859", "mrqa_triviaqa-validation-2454", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-7309", "mrqa_hotpotqa-validation-1414", "mrqa_triviaqa-validation-2098", "mrqa_squad-validation-6328", "mrqa_triviaqa-validation-6221", "mrqa_naturalquestions-validation-2481", "mrqa_triviaqa-validation-1259"], "retrieved_ids": ["mrqa_naturalquestions-train-33235", "mrqa_naturalquestions-train-68685", "mrqa_naturalquestions-train-9200", "mrqa_naturalquestions-train-85923", "mrqa_naturalquestions-train-62390", "mrqa_naturalquestions-train-24394", "mrqa_naturalquestions-train-2319", "mrqa_naturalquestions-train-11684", "mrqa_naturalquestions-train-19615", "mrqa_naturalquestions-train-77130", "mrqa_naturalquestions-train-59714", "mrqa_naturalquestions-train-28849", "mrqa_naturalquestions-train-8492", "mrqa_naturalquestions-train-76798", "mrqa_naturalquestions-train-50650", "mrqa_naturalquestions-train-24256", "mrqa_naturalquestions-train-74005", "mrqa_naturalquestions-train-27665", "mrqa_naturalquestions-train-60677", "mrqa_naturalquestions-train-64103", "mrqa_naturalquestions-train-52458", "mrqa_naturalquestions-train-11342", "mrqa_naturalquestions-train-41769", "mrqa_naturalquestions-train-47183", "mrqa_naturalquestions-train-26521", "mrqa_naturalquestions-train-12963", "mrqa_naturalquestions-train-19393", "mrqa_naturalquestions-train-17118", "mrqa_naturalquestions-train-46661", "mrqa_naturalquestions-train-14248", "mrqa_naturalquestions-train-86785", "mrqa_naturalquestions-train-24253"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 26, "before_eval": {"predictions": ["Jewish audiences", "north of the Lakes Region and south of the Kancamagus Highway", "Magic formula investing", "true history of the Kelly Gang", "Waialua District of the island of O\u02bb ahu", "1910\u20131940", "non-teaching posts", "Salamanca", "jazz saxophonist", "tennis", "4,000", "the founder of the Yuan dynasty", "Catherine", "canal", "spice products", "The Simpsons Spin-Off Showcase", "Barry Parker", "Orange", "a portion of Grainger Town was demolished in the 1960s to make way for the Eldon Square Shopping Centre", "Albany High School for Educating People of Color", "mughal garden of rashtrapati bhavan", "Sergeant First Class", "Shaw", "seek jury nullification", "Cee - Lo", "Anglican", "mammy two Shoes", "king George V class battleship", "magnetism", "sitting directly on the Gulf of Guinea on the Atlantic Ocean in Nigeria", "redistributive", "January 11, 1755 or 1757"], "metric_results": {"EM": 0.09375, "QA-F1": 0.2705176767676768}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.8, 0.19999999999999998, 0.0, 0.33333333333333337, 0.16666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.4, 0.5454545454545454, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8000000000000002]}}, "error_ids": ["mrqa_naturalquestions-validation-3027", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-2184", "mrqa_triviaqa-validation-6772", "mrqa_hotpotqa-validation-4553", "mrqa_squad-validation-1994", "mrqa_squad-validation-3176", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-6689", "mrqa_hotpotqa-validation-5", "mrqa_squad-validation-6148", "mrqa_triviaqa-validation-3472", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6224", "mrqa_hotpotqa-validation-2436", "mrqa_naturalquestions-validation-2214", "mrqa_squad-validation-2428", "mrqa_squad-validation-5184", "mrqa_hotpotqa-validation-1446", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-3658", "mrqa_squad-validation-6837", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-7581", "mrqa_triviaqa-validation-5839", "mrqa_triviaqa-validation-5050", "mrqa_triviaqa-validation-3393", "mrqa_squad-validation-7319", "mrqa_hotpotqa-validation-945"], "retrieved_ids": ["mrqa_naturalquestions-train-18600", "mrqa_naturalquestions-train-13825", "mrqa_naturalquestions-train-48639", "mrqa_naturalquestions-train-23684", "mrqa_naturalquestions-train-48935", "mrqa_naturalquestions-train-72285", "mrqa_naturalquestions-train-47999", "mrqa_naturalquestions-train-80038", "mrqa_naturalquestions-train-20253", "mrqa_naturalquestions-train-24880", "mrqa_naturalquestions-train-87879", "mrqa_naturalquestions-train-86643", "mrqa_naturalquestions-train-77486", "mrqa_naturalquestions-train-83516", "mrqa_naturalquestions-train-80466", "mrqa_naturalquestions-train-43036", "mrqa_naturalquestions-train-10813", "mrqa_naturalquestions-train-7154", "mrqa_naturalquestions-train-51088", "mrqa_naturalquestions-train-65350", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-train-34415", "mrqa_naturalquestions-train-16539", "mrqa_naturalquestions-train-66748", "mrqa_naturalquestions-train-39520", "mrqa_naturalquestions-train-38113", "mrqa_naturalquestions-train-47477", "mrqa_naturalquestions-train-1931", "mrqa_naturalquestions-train-75030", "mrqa_naturalquestions-train-46250", "mrqa_naturalquestions-train-12103", "mrqa_naturalquestions-train-46406"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 27, "before_eval": {"predictions": ["l.A. producer Bones Howe", "jools Holland", "blackberry", "crimson College Observatory", "\" Big Mamie\"", "birmingham", "Hoffa", "a light sky-blue color caused by absorption in the red", "the peasants from Kent and Essex marched on London", "2009", "the Golden State Warriors", "inner mitochondria membrane", "cezanne", "the V8 Supercars and Australian Motorcycle Grand Prix at Phillip Island", "BSkyB has no veto", "the fourth season", "a more fundamental electro weak interaction", "the availability of skilled tradespeople", "hardness", "A simple iron boar crest adorns the top of this helmet", "the University of Northumbria at Newcastle", "japan", "Lofton", "on kickoffs at the 25 - yard line", "It is named after the Swedish astronomer Anders Celsius", "about 7,000", "lion, leopard, buffalo, rhinoceros, and elephant", "the righteousness of Christ", "margaret smith", "possible combinations of two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "Antwerp", "group"], "metric_results": {"EM": 0.125, "QA-F1": 0.30701414813256916}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.42857142857142855, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0, 0.6666666666666665, 0.28571428571428575, 0.0, 0.6153846153846153, 0.0, 0.0, 0.0, 0.631578947368421, 0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6060606060606061, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-2899", "mrqa_triviaqa-validation-6331", "mrqa_hotpotqa-validation-2428", "mrqa_squad-validation-3539", "mrqa_triviaqa-validation-2980", "mrqa_hotpotqa-validation-4022", "mrqa_squad-validation-8739", "mrqa_triviaqa-validation-1423", "mrqa_squad-validation-2966", "mrqa_squad-validation-2733", "mrqa_hotpotqa-validation-158", "mrqa_squad-validation-10312", "mrqa_squad-validation-6855", "mrqa_triviaqa-validation-5962", "mrqa_hotpotqa-validation-1226", "mrqa_squad-validation-5337", "mrqa_triviaqa-validation-1034", "mrqa_hotpotqa-validation-3949", "mrqa_naturalquestions-validation-9105", "mrqa_naturalquestions-validation-3771", "mrqa_squad-validation-5125", "mrqa_squad-validation-2313", "mrqa_triviaqa-validation-388", "mrqa_naturalquestions-validation-2893", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-1507"], "retrieved_ids": ["mrqa_naturalquestions-train-8398", "mrqa_naturalquestions-train-11208", "mrqa_naturalquestions-train-8296", "mrqa_naturalquestions-train-57056", "mrqa_naturalquestions-train-79895", "mrqa_naturalquestions-train-38714", "mrqa_naturalquestions-train-53795", "mrqa_naturalquestions-train-70533", "mrqa_naturalquestions-train-46200", "mrqa_naturalquestions-train-27583", "mrqa_naturalquestions-train-51904", "mrqa_naturalquestions-train-21794", "mrqa_naturalquestions-train-10276", "mrqa_naturalquestions-train-73993", "mrqa_naturalquestions-train-10574", "mrqa_hotpotqa-validation-859", "mrqa_naturalquestions-train-80987", "mrqa_naturalquestions-train-24708", "mrqa_naturalquestions-train-10154", "mrqa_naturalquestions-train-52269", "mrqa_naturalquestions-train-62044", "mrqa_naturalquestions-train-61509", "mrqa_naturalquestions-train-48429", "mrqa_naturalquestions-train-23625", "mrqa_naturalquestions-train-41656", "mrqa_naturalquestions-train-24912", "mrqa_naturalquestions-train-1096", "mrqa_naturalquestions-train-78116", "mrqa_naturalquestions-train-47904", "mrqa_naturalquestions-train-33771", "mrqa_naturalquestions-train-81660", "mrqa_naturalquestions-train-18292"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 28, "before_eval": {"predictions": ["the Turk", "Chris Weidman", "jury nullification", "Harishchandra", "poet", "Professor Eobard Thawne", "slivovitz", "a US$10 a week raise over Tesla's US&18 per week salary", "1875", "member states", "because of all the instruments", "McKinsey's offices in Silicon Valley and India", "acrophobia", "2000", "Crohn's disease or ulcerative colitis", "Ondemar Dias", "Raya Yarbrough", "No. 1 seed Virginia and No. 4 seed Arizona", "cruiserweight", "John D. Rockefeller", "Old Testament", "australian-based United Parcel Service", "local talent", "Football League", "tristan Farnon", "australian province of British Columbia", "the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague", "surrattsville", "1332", "dodo bird", "people and their thoughts are both made from `` pure energy '', and that through the process of `` like energy attracting like energy '' a person can improve their own health, wealth and personal relationships", "australian"], "metric_results": {"EM": 0.15625, "QA-F1": 0.26347080498866216}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false], "QA-F1": [0.0, 0.0, 0.3333333333333333, 0.0, 0.5, 1.0, 0.0, 0.5, 0.0, 0.4444444444444445, 0.0, 0.4444444444444445, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.1, 0.0, 1.0, 1.0, 0.2040816326530612, 0.0]}}, "error_ids": ["mrqa_squad-validation-2291", "mrqa_hotpotqa-validation-1390", "mrqa_squad-validation-6835", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-5637", "mrqa_triviaqa-validation-4827", "mrqa_squad-validation-1276", "mrqa_hotpotqa-validation-4352", "mrqa_naturalquestions-validation-10495", "mrqa_triviaqa-validation-5237", "mrqa_hotpotqa-validation-5110", "mrqa_triviaqa-validation-3760", "mrqa_triviaqa-validation-6250", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-7110", "mrqa_triviaqa-validation-1347", "mrqa_squad-validation-8031", "mrqa_naturalquestions-validation-10687", "mrqa_triviaqa-validation-7669", "mrqa_hotpotqa-validation-1251", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-6881", "mrqa_squad-validation-4921", "mrqa_triviaqa-validation-5810", "mrqa_naturalquestions-validation-7821", "mrqa_triviaqa-validation-4308"], "retrieved_ids": ["mrqa_naturalquestions-train-75559", "mrqa_naturalquestions-train-33604", "mrqa_naturalquestions-train-21092", "mrqa_naturalquestions-train-78155", "mrqa_naturalquestions-train-61465", "mrqa_naturalquestions-train-29418", "mrqa_naturalquestions-train-79788", "mrqa_naturalquestions-train-67839", "mrqa_naturalquestions-train-10968", "mrqa_naturalquestions-train-30645", "mrqa_naturalquestions-train-48104", "mrqa_naturalquestions-train-79006", "mrqa_naturalquestions-train-53898", "mrqa_naturalquestions-train-61763", "mrqa_naturalquestions-train-57819", "mrqa_naturalquestions-train-41055", "mrqa_naturalquestions-train-56184", "mrqa_naturalquestions-train-3737", "mrqa_triviaqa-validation-1616", "mrqa_hotpotqa-validation-1001", "mrqa_naturalquestions-train-75157", "mrqa_triviaqa-validation-7415", "mrqa_naturalquestions-train-48612", "mrqa_naturalquestions-train-39212", "mrqa_naturalquestions-train-69398", "mrqa_naturalquestions-train-87234", "mrqa_naturalquestions-train-77974", "mrqa_naturalquestions-train-51833", "mrqa_naturalquestions-train-38329", "mrqa_naturalquestions-train-53053", "mrqa_naturalquestions-train-21006", "mrqa_naturalquestions-train-63645"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 29, "before_eval": {"predictions": ["poisonous", "886 AD", "to finance his own projects with varying degrees of success", "Can-Am", "Kinect", "Tokyo", "defensive end Kony Ealy", "parallelogram rule of vector addition", "public speaking", "lion", "a neutron source used for stable and reliable initiation of nuclear chain reaction in nuclear reactors, when they are loaded with fresh nuclear fuel, whose neutron flux from spontaneous fission is insufficient for a reliable startup, or after prolonged shutdown periods", "starry starry night", "lower-pressure", "performs six major functions ; support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "Imperial Secretariat", "Doctor Who Theme", "National Basketball Development League (NBDL)", "gillingham", "St. Mary's County", "T. J. Ward", "2,615 at the 2010 census", "Pyeongchang", "Kaep", "a password recovery tool for Microsoft Windows", "Sordid Lives", "Charles and Ray Eames", "Brazil", "pawel Kopczynski", "the smallest subfield", "pamper an irritated digestive tract", "from 53% in Botswana to -40% in Bahrain", "photosynthesis"], "metric_results": {"EM": 0.03125, "QA-F1": 0.128078314659197}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 0.0, 0.0, 0.14285714285714288, 0.0, 0.0, 0.38095238095238093, 0.0, 0.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.058823529411764705, 0.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4708", "mrqa_naturalquestions-validation-866", "mrqa_squad-validation-1249", "mrqa_hotpotqa-validation-3497", "mrqa_triviaqa-validation-6632", "mrqa_naturalquestions-validation-6610", "mrqa_squad-validation-814", "mrqa_squad-validation-10408", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-3103", "mrqa_naturalquestions-validation-8653", "mrqa_triviaqa-validation-7032", "mrqa_squad-validation-3344", "mrqa_naturalquestions-validation-5826", "mrqa_squad-validation-8075", "mrqa_squad-validation-7914", "mrqa_hotpotqa-validation-2928", "mrqa_triviaqa-validation-1280", "mrqa_naturalquestions-validation-5961", "mrqa_squad-validation-772", "mrqa_hotpotqa-validation-3031", "mrqa_naturalquestions-validation-9765", "mrqa_hotpotqa-validation-5383", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-7567", "mrqa_hotpotqa-validation-5709", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-2709", "mrqa_triviaqa-validation-814", "mrqa_squad-validation-7445", "mrqa_squad-validation-8873"], "retrieved_ids": ["mrqa_naturalquestions-train-4806", "mrqa_naturalquestions-train-37629", "mrqa_naturalquestions-train-82585", "mrqa_naturalquestions-train-22679", "mrqa_naturalquestions-train-39687", "mrqa_naturalquestions-train-44765", "mrqa_naturalquestions-train-52008", "mrqa_naturalquestions-train-69020", "mrqa_naturalquestions-train-7119", "mrqa_naturalquestions-train-5381", "mrqa_naturalquestions-train-13957", "mrqa_naturalquestions-train-21248", "mrqa_naturalquestions-train-71164", "mrqa_naturalquestions-train-43621", "mrqa_naturalquestions-train-28391", "mrqa_naturalquestions-train-67624", "mrqa_naturalquestions-train-78148", "mrqa_naturalquestions-train-49463", "mrqa_naturalquestions-train-77303", "mrqa_naturalquestions-train-4062", "mrqa_naturalquestions-train-31729", "mrqa_naturalquestions-train-35181", "mrqa_naturalquestions-train-14835", "mrqa_naturalquestions-train-21947", "mrqa_naturalquestions-train-87102", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-train-1646", "mrqa_naturalquestions-train-52028", "mrqa_naturalquestions-train-3536", "mrqa_naturalquestions-train-3077", "mrqa_naturalquestions-train-29804", "mrqa_naturalquestions-train-54637"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 30, "before_eval": {"predictions": ["the judiciary", "named for Frederick Louis, Prince of Wales, son of King George II", "an outgoing, eccentic, big - hearted, loving, sweet, and thoughtful elephant and teacher", "arpers Ferry", "Basil Fawlty", "Erick Avari", "BBC", "arthur marlin", "demographics and economic ties", "three or more", "The Kickoff Game", "narcolepsy", "arctic monkeys", "Monza", "arthur", "usernames, passwords, commands and data", "A computer program", "The centre-right Australian Labor Party (ALP)", "marduk", "hekla", "a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda", "koreans", "South Pacific off the northeast coast of Australia", "Article 7, Paragraph 4", "New Jersey", "Easy", "Spencer Brody ( Zoe McLellan ), a transfer from the NCIS Great Lakes field office, who has worked as a Special Agent Afloat and is keen to leave her past behind as she moves to New Orleans", "k. kamaraj", "National Lottery", "arthur", "katherine of aragon", "to facilitate compliance with the Telephone Consumer Protection Act of 1991"], "metric_results": {"EM": 0.09375, "QA-F1": 0.15243965458990613}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.10526315789473684, 0.0, 0.4444444444444445, 0.4, 1.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.4000000000000001]}}, "error_ids": ["mrqa_naturalquestions-validation-4645", "mrqa_hotpotqa-validation-2559", "mrqa_naturalquestions-validation-9608", "mrqa_triviaqa-validation-3920", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-4079", "mrqa_squad-validation-7793", "mrqa_triviaqa-validation-1921", "mrqa_squad-validation-2577", "mrqa_hotpotqa-validation-4578", "mrqa_naturalquestions-validation-3209", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-1722", "mrqa_naturalquestions-validation-3533", "mrqa_squad-validation-2885", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7090", "mrqa_squad-validation-8451", "mrqa_triviaqa-validation-7684", "mrqa_naturalquestions-validation-4710", "mrqa_squad-validation-6961", "mrqa_hotpotqa-validation-3333", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-3134", "mrqa_hotpotqa-validation-5604", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-2091", "mrqa_naturalquestions-validation-10328"], "retrieved_ids": ["mrqa_naturalquestions-train-85848", "mrqa_naturalquestions-train-41263", "mrqa_naturalquestions-train-79285", "mrqa_naturalquestions-train-52458", "mrqa_naturalquestions-train-73197", "mrqa_naturalquestions-train-22692", "mrqa_naturalquestions-validation-8180", "mrqa_naturalquestions-train-2253", "mrqa_naturalquestions-train-1234", "mrqa_naturalquestions-train-24660", "mrqa_naturalquestions-train-77534", "mrqa_naturalquestions-train-55076", "mrqa_naturalquestions-train-26076", "mrqa_naturalquestions-train-15099", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-train-40668", "mrqa_naturalquestions-train-49555", "mrqa_naturalquestions-train-46559", "mrqa_triviaqa-validation-1935", "mrqa_naturalquestions-train-81626", "mrqa_naturalquestions-train-70465", "mrqa_naturalquestions-train-13825", "mrqa_hotpotqa-validation-5128", "mrqa_naturalquestions-train-69270", "mrqa_naturalquestions-train-7972", "mrqa_naturalquestions-train-49110", "mrqa_naturalquestions-train-15740", "mrqa_naturalquestions-train-65138", "mrqa_naturalquestions-train-88097", "mrqa_naturalquestions-train-65515", "mrqa_naturalquestions-train-61428", "mrqa_naturalquestions-train-55816"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 31, "before_eval": {"predictions": ["Yolanda Sald\u00edvar", "Andreas", "boston", "sports commentator", "Newell Highway", "mitcharf", "a horse is 15 hands, 3  inches tall", "a shopping mall located in Bloomington, Minnesota, United States ( a suburb of the Twin Cities )", "a defiant speech, or a speech explaining their actions", "DreamWorks Animation", "zigeunerbaron", "his own men", "emissions resulting from human activities are substantially increasing the atmospheric concentrations of the greenhouse gases, resulting on average in an additional warming of the Earth's surface", "stainon", "the RAF", "reduce growth in relatively poor countries but encourage growth", "Ibrium", "alex daniel", "Polish-Jewish", "a variety of political groups that supported the Spanish coup of July 1936 against the Second Spanish Republic, including the Falange, the CEDA, and two rival monarchist claimants : the Alfonsists and the Carlists", "casket letters", "ottoexeter", "an estimated 390 billion individual trees divided into 16,000 species", "Washington Street between Boylston Street and Kneeland Street", "May 10, 1976", "six", "London Tipton", "his frustration with the atmosphere in the group at that time", "surtsey", "John Smith", "surtania", "the variety of occupations necessary to sustain the community as distinct from the indigenous population"], "metric_results": {"EM": 0.15625, "QA-F1": 0.24350718725718729}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.15384615384615385, 0.9333333333333333, 0.0, 0.0, 0.0, 0.37037037037037035, 0.0, 0.0, 0.3636363636363636, 1.0, 0.0, 0.0, 0.07407407407407407, 0.0, 0.0, 0.3636363636363636, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.13333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-1385", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-1050", "mrqa_triviaqa-validation-457", "mrqa_triviaqa-validation-5071", "mrqa_naturalquestions-validation-5604", "mrqa_squad-validation-6734", "mrqa_hotpotqa-validation-2564", "mrqa_triviaqa-validation-3300", "mrqa_squad-validation-6128", "mrqa_squad-validation-8589", "mrqa_triviaqa-validation-5513", "mrqa_naturalquestions-validation-954", "mrqa_squad-validation-7469", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-2493", "mrqa_naturalquestions-validation-9825", "mrqa_triviaqa-validation-3286", "mrqa_triviaqa-validation-2609", "mrqa_squad-validation-4415", "mrqa_hotpotqa-validation-3233", "mrqa_hotpotqa-validation-3728", "mrqa_squad-validation-932", "mrqa_hotpotqa-validation-5307", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-4524", "mrqa_squad-validation-3106"], "retrieved_ids": ["mrqa_naturalquestions-train-17446", "mrqa_naturalquestions-train-10451", "mrqa_naturalquestions-train-58234", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-train-86039", "mrqa_naturalquestions-train-74317", "mrqa_naturalquestions-train-56569", "mrqa_naturalquestions-validation-10328", "mrqa_naturalquestions-train-33933", "mrqa_naturalquestions-train-33383", "mrqa_naturalquestions-train-17358", "mrqa_naturalquestions-train-24328", "mrqa_naturalquestions-train-82038", "mrqa_naturalquestions-train-5428", "mrqa_naturalquestions-train-26846", "mrqa_naturalquestions-train-71926", "mrqa_naturalquestions-train-53274", "mrqa_naturalquestions-train-29379", "mrqa_naturalquestions-train-11443", "mrqa_naturalquestions-train-86689", "mrqa_naturalquestions-train-50721", "mrqa_naturalquestions-train-56", "mrqa_naturalquestions-train-83318", "mrqa_naturalquestions-train-5666", "mrqa_naturalquestions-train-82397", "mrqa_naturalquestions-train-27547", "mrqa_naturalquestions-train-9875", "mrqa_naturalquestions-train-73662", "mrqa_naturalquestions-train-63614", "mrqa_naturalquestions-train-88039", "mrqa_naturalquestions-train-38415", "mrqa_naturalquestions-train-44134"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 32, "before_eval": {"predictions": ["A cylindrical Service Module (SM)", "the Orthodox Christians", "Coca-Cola", "beer", "gender testing", "Matt Jones", "a soft, silvery-white metal, member of the alkali group of the periodic chart", "a heavy vehicle can be fatal to the driver as the top of the cab can be crushed or sliced off as it swings round violently and tries to fold under the trailer", "T cell receptor", "high school teachers, median salaries in 2007 ranged from $35,000 in South Dakota to $71,000", "fruit, vegetables and tomatoes", "Heading Out to the Highway", "Moonraker", "$12.99", "Michael Oppenheimer", "England national team", "status of people within the four-class system was not an indication of their actual social power and wealth, but just entailed \"degrees of privilege\" to which they were entitled institutionally and legally,", "Jumping on the Moon", "Convention", "5,922", "December 5, 1991", "a 2016 science fiction psychological horror", "Philadelphia 76ers", "the historical Saint Nicholas", "Stern-Plaza", "WBC/WBA heavyweight champion Joe Frazier", "23 March 1991", "Monday", "Dealey Plaza", "Dar es Salaam", "last Ice Age", "Anno 2053"], "metric_results": {"EM": 0.125, "QA-F1": 0.27406204906204906}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12121212121212122, 0.8571428571428571, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.1142857142857143, 0.0, 0.0, 1.0, 0.0, 0.9090909090909091, 0.0, 0.8, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0]}}, "error_ids": ["mrqa_squad-validation-3887", "mrqa_hotpotqa-validation-572", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3066", "mrqa_naturalquestions-validation-4761", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-5510", "mrqa_squad-validation-6602", "mrqa_squad-validation-2234", "mrqa_triviaqa-validation-7156", "mrqa_hotpotqa-validation-1119", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2245", "mrqa_squad-validation-8617", "mrqa_hotpotqa-validation-305", "mrqa_squad-validation-8095", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-10311", "mrqa_hotpotqa-validation-2910", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1660", "mrqa_naturalquestions-validation-7049", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-2985", "mrqa_triviaqa-validation-1079", "mrqa_hotpotqa-validation-4221", "mrqa_hotpotqa-validation-3557", "mrqa_naturalquestions-validation-5960"], "retrieved_ids": ["mrqa_naturalquestions-train-85388", "mrqa_triviaqa-validation-1995", "mrqa_naturalquestions-train-15944", "mrqa_triviaqa-validation-4681", "mrqa_naturalquestions-train-81557", "mrqa_triviaqa-validation-1921", "mrqa_naturalquestions-train-11221", "mrqa_hotpotqa-validation-4734", "mrqa_naturalquestions-train-60350", "mrqa_naturalquestions-train-80300", "mrqa_naturalquestions-train-59640", "mrqa_naturalquestions-train-56898", "mrqa_naturalquestions-train-9155", "mrqa_naturalquestions-train-86617", "mrqa_naturalquestions-train-64772", "mrqa_naturalquestions-train-72505", "mrqa_naturalquestions-train-42736", "mrqa_naturalquestions-train-67824", "mrqa_naturalquestions-train-11121", "mrqa_naturalquestions-train-6529", "mrqa_naturalquestions-train-70025", "mrqa_naturalquestions-train-14956", "mrqa_naturalquestions-train-71112", "mrqa_naturalquestions-train-12527", "mrqa_naturalquestions-train-13190", "mrqa_naturalquestions-train-65641", "mrqa_naturalquestions-train-3581", "mrqa_triviaqa-validation-4055", "mrqa_naturalquestions-train-47600", "mrqa_naturalquestions-train-6707", "mrqa_naturalquestions-train-37024", "mrqa_triviaqa-validation-2327"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 33, "before_eval": {"predictions": ["Habsburg control of the First Empire, the Spanish throne, and other royal houses", "\"Boston Herald\" Rumor Clinic", "1967", "legprints in the Sand", "the twelfth most populous city in the United States", "115", "bridge", "changes in gene expression", "lower", "Bass", "Chava", "New Orleans, Biloxi, Mississippi, Mobile, Alabama and small settlements in the Illinois Country, hugging the east side of the Mississippi River and its tributaries", "Japan", "seaman", "Yunnan- Fu", "Mumbai", "Broken Hill and Sydney", "2005", "buyers from all punishments and granted them salvation were in error", "\"The Doctor's Daughter\"", "september", "bridge", "structure that fails to adhere to codes does not benefit the owner", "1879", "niece of Empress Taitu Bitul, consort of Emperor Menelik II of Ethiopia", "stay with the same group of peers for all classes", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "enthusiasm", "september", "Datsun 810", "Bill Clinton", "Oslo county"], "metric_results": {"EM": 0.09375, "QA-F1": 0.18082582622056304}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.14285714285714288, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12121212121212123, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 0.0, 0.10526315789473685, 0.0, 0.15384615384615385, 0.8888888888888888, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0]}}, "error_ids": ["mrqa_squad-validation-9984", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-2092", "mrqa_squad-validation-7278", "mrqa_naturalquestions-validation-8203", "mrqa_triviaqa-validation-1410", "mrqa_naturalquestions-validation-9271", "mrqa_squad-validation-7571", "mrqa_hotpotqa-validation-650", "mrqa_naturalquestions-validation-3523", "mrqa_squad-validation-10180", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-6499", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-5727", "mrqa_squad-validation-2010", "mrqa_squad-validation-7741", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-6186", "mrqa_squad-validation-6878", "mrqa_naturalquestions-validation-7387", "mrqa_hotpotqa-validation-2588", "mrqa_squad-validation-1903", "mrqa_naturalquestions-validation-5297", "mrqa_squad-validation-2147", "mrqa_triviaqa-validation-2812", "mrqa_squad-validation-3733", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-1211"], "retrieved_ids": ["mrqa_naturalquestions-train-12029", "mrqa_naturalquestions-train-62990", "mrqa_naturalquestions-train-20253", "mrqa_naturalquestions-train-60328", "mrqa_naturalquestions-train-14628", "mrqa_naturalquestions-train-52201", "mrqa_naturalquestions-train-13452", "mrqa_naturalquestions-train-16901", "mrqa_naturalquestions-train-18759", "mrqa_naturalquestions-train-63669", "mrqa_naturalquestions-train-37360", "mrqa_naturalquestions-train-80422", "mrqa_naturalquestions-train-52522", "mrqa_naturalquestions-train-63952", "mrqa_naturalquestions-train-72458", "mrqa_naturalquestions-train-12305", "mrqa_naturalquestions-train-37465", "mrqa_naturalquestions-train-50420", "mrqa_naturalquestions-train-83082", "mrqa_naturalquestions-train-28833", "mrqa_naturalquestions-train-70836", "mrqa_naturalquestions-train-19232", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-train-85807", "mrqa_naturalquestions-train-79498", "mrqa_naturalquestions-train-56941", "mrqa_naturalquestions-train-33712", "mrqa_naturalquestions-train-16854", "mrqa_naturalquestions-validation-2020", "mrqa_naturalquestions-train-38085", "mrqa_naturalquestions-train-69841", "mrqa_naturalquestions-train-75619"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 34, "before_eval": {"predictions": ["Speaker", "a Botticelli figure in clinging ivory silk, trailed with jasmine, smilax, syringa and small white rose-like blossoms", "Threatening government officials", "Veronica", "Victorian College of the Arts and Melbourne Conservatorium of Music", "Britain", "onion", "0.2 inhabitants per square kilometre", "j James David Graham Niven", "France", "Ian Paisley", "emperor japan", "euro", "suggs", "the United States", "Sherry Rowland", "London and New York, c. 1886", "second sophomore", "Stanwyck's bedroom window overlooks the night skyline of Manhattan", "Joaquin Phoenix as Commodus", "pole", "Johnny Darrell", "carotid artery disease", "a Belgian law requiring all margarine to be in cube shaped packages", "Euler's totient function", "ear canal", "the set of all connected graphs", "second-busiest", "red", "Toyota Corona", "Kurt Vonnegut", "Rapunzel"], "metric_results": {"EM": 0.09375, "QA-F1": 0.14419261294261293}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.14285714285714288, 1.0, 1.0, 0.0, 0.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42857142857142855, 0.5, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2962", "mrqa_triviaqa-validation-7248", "mrqa_squad-validation-6673", "mrqa_naturalquestions-validation-6787", "mrqa_triviaqa-validation-5516", "mrqa_squad-validation-4369", "mrqa_triviaqa-validation-5496", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-3072", "mrqa_triviaqa-validation-1220", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-5740", "mrqa_naturalquestions-validation-4366", "mrqa_hotpotqa-validation-3002", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4288", "mrqa_triviaqa-validation-7592", "mrqa_squad-validation-4255", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-3408", "mrqa_squad-validation-1635", "mrqa_squad-validation-2751", "mrqa_triviaqa-validation-7184", "mrqa_squad-validation-3708", "mrqa_squad-validation-8034", "mrqa_triviaqa-validation-1198"], "retrieved_ids": ["mrqa_naturalquestions-train-47291", "mrqa_naturalquestions-train-1304", "mrqa_naturalquestions-train-36531", "mrqa_naturalquestions-train-40732", "mrqa_naturalquestions-train-72397", "mrqa_naturalquestions-train-58458", "mrqa_naturalquestions-train-48296", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-train-68985", "mrqa_naturalquestions-train-42179", "mrqa_naturalquestions-train-13825", "mrqa_naturalquestions-train-44378", "mrqa_naturalquestions-train-48292", "mrqa_squad-validation-8542", "mrqa_naturalquestions-train-5418", "mrqa_naturalquestions-train-74285", "mrqa_naturalquestions-train-38516", "mrqa_naturalquestions-train-87680", "mrqa_naturalquestions-train-70763", "mrqa_naturalquestions-train-36720", "mrqa_naturalquestions-train-57441", "mrqa_naturalquestions-train-37465", "mrqa_naturalquestions-train-2426", "mrqa_naturalquestions-train-37815", "mrqa_naturalquestions-train-69584", "mrqa_naturalquestions-train-56943", "mrqa_naturalquestions-train-28976", "mrqa_naturalquestions-train-62147", "mrqa_naturalquestions-train-62234", "mrqa_naturalquestions-train-34949", "mrqa_naturalquestions-train-33811", "mrqa_naturalquestions-train-11987"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 35, "before_eval": {"predictions": ["up to 2% higher than during outbreaks of 13- and 17-year cicadas", "the interplay of supply and demand, which determines the prices of goods and services", "Emma Watson, Dan Stevens, Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Nathan Mack, Ian McKellen, and Emma Thompson", "brain, muscles, and liver", "ear-shaped pasta", "Washington Redskins", "in the courtyard adjoining the Assembly Hall, which is part of the School of Divinity of the University of Edinburgh", "William Howard Ashton", "teapot Dome scandal", "high and persistent unemployment, in which inequality increases, has a negative effect on subsequent long-run economic growth", "north of Broward County", "Song Kang-ho, Lee Byung-hun, and Jung Woo-sung", "changing display or audio settings quickly", "Battle of Edgehill", "derived from the spectroscopic notation for the associated atomic orbitals : sharp, principal, diffuse and fundamental, and then g which follows f in the alphabet", "declines over the medium term", "Beauty and the Beast", "South Africa", "Scotty Grainger", "Alamo", "seal illegally is broken", "United Methodist", "Brian Liesegang", "Roger Allers", "Port Moresby", "Alvin and the Chipmunks (2007)", "National Association for the Advancement of Colored People", "1963\u20131989", "the largest one ever made back in 1912", "John Prescott", "Darrin Stephens", "6500 - 1500 BC"], "metric_results": {"EM": 0.125, "QA-F1": 0.24691803804855275}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.5, 0.125, 0.14285714285714288, 0.4, 0.0, 0.0, 0.4, 1.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.4, 0.0, 0.4444444444444445, 0.4, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-9123", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-7704", "mrqa_triviaqa-validation-4829", "mrqa_naturalquestions-validation-4544", "mrqa_squad-validation-9400", "mrqa_triviaqa-validation-6423", "mrqa_squad-validation-7352", "mrqa_hotpotqa-validation-4447", "mrqa_hotpotqa-validation-2947", "mrqa_naturalquestions-validation-1587", "mrqa_triviaqa-validation-3767", "mrqa_naturalquestions-validation-585", "mrqa_squad-validation-7495", "mrqa_hotpotqa-validation-1475", "mrqa_triviaqa-validation-2999", "mrqa_squad-validation-3408", "mrqa_squad-validation-10036", "mrqa_hotpotqa-validation-3853", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-430", "mrqa_triviaqa-validation-11", "mrqa_hotpotqa-validation-1720", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-1812"], "retrieved_ids": ["mrqa_naturalquestions-train-16724", "mrqa_naturalquestions-train-43261", "mrqa_triviaqa-validation-3603", "mrqa_naturalquestions-train-5859", "mrqa_naturalquestions-train-56239", "mrqa_naturalquestions-train-82525", "mrqa_naturalquestions-train-70948", "mrqa_naturalquestions-train-66157", "mrqa_naturalquestions-train-42843", "mrqa_naturalquestions-train-4648", "mrqa_naturalquestions-train-20647", "mrqa_naturalquestions-train-17467", "mrqa_naturalquestions-train-39439", "mrqa_naturalquestions-train-23227", "mrqa_naturalquestions-train-11987", "mrqa_naturalquestions-train-6539", "mrqa_naturalquestions-train-79688", "mrqa_naturalquestions-train-48486", "mrqa_naturalquestions-train-83394", "mrqa_naturalquestions-train-20582", "mrqa_naturalquestions-train-78640", "mrqa_naturalquestions-train-32959", "mrqa_naturalquestions-train-7130", "mrqa_naturalquestions-train-87143", "mrqa_naturalquestions-train-52882", "mrqa_naturalquestions-train-46405", "mrqa_naturalquestions-train-43248", "mrqa_naturalquestions-train-44782", "mrqa_naturalquestions-train-73322", "mrqa_naturalquestions-train-13788", "mrqa_naturalquestions-train-3360", "mrqa_hotpotqa-validation-241"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 36, "before_eval": {"predictions": ["National Broadcasting Company", "Kevin Costner", "planet Uranus", "president harding", "Cobham\u2013Edmonds thesis", "human", "Best Male Pop Vocal Performance", "March 2012", "jazz", "Muhammad Ali", "Coldplay", "Gibraltar", "to civil disobedients", "Julius Caesar", "2%", "1979", "virtual reality", "decision problem", "elizabeth lecouvreur", "heart", "Miasma theory", "American pint of 16 US fluid ounces ( 473 ml )", "mountain ranges", "president harding", "Georgia", "nettle", "US$3 per barrel", "20 %", "bestnight girl", "network", "roughly west", "Sudan"], "metric_results": {"EM": 0.28125, "QA-F1": 0.3511656746031746}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.14285714285714288, 0.0, 1.0, 1.0, 0.25, 0.4444444444444445, 0.0, 0.4, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-652", "mrqa_triviaqa-validation-4824", "mrqa_triviaqa-validation-37", "mrqa_squad-validation-7720", "mrqa_hotpotqa-validation-4427", "mrqa_hotpotqa-validation-3911", "mrqa_triviaqa-validation-3803", "mrqa_hotpotqa-validation-2936", "mrqa_naturalquestions-validation-6011", "mrqa_squad-validation-6759", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-3993", "mrqa_squad-validation-1634", "mrqa_triviaqa-validation-5936", "mrqa_naturalquestions-validation-5552", "mrqa_squad-validation-4877", "mrqa_naturalquestions-validation-8525", "mrqa_triviaqa-validation-199", "mrqa_squad-validation-3635", "mrqa_triviaqa-validation-6290", "mrqa_squad-validation-4626", "mrqa_squad-validation-9146", "mrqa_hotpotqa-validation-452"], "retrieved_ids": ["mrqa_naturalquestions-train-61869", "mrqa_naturalquestions-train-34559", "mrqa_naturalquestions-train-64161", "mrqa_naturalquestions-train-30032", "mrqa_naturalquestions-train-54599", "mrqa_naturalquestions-train-53261", "mrqa_naturalquestions-train-73216", "mrqa_naturalquestions-train-44160", "mrqa_naturalquestions-train-60971", "mrqa_naturalquestions-train-30683", "mrqa_naturalquestions-train-24281", "mrqa_naturalquestions-train-56517", "mrqa_naturalquestions-train-27343", "mrqa_naturalquestions-train-29589", "mrqa_triviaqa-validation-1722", "mrqa_naturalquestions-train-45024", "mrqa_naturalquestions-train-11996", "mrqa_naturalquestions-train-7154", "mrqa_naturalquestions-train-67805", "mrqa_triviaqa-validation-5239", "mrqa_naturalquestions-train-13496", "mrqa_naturalquestions-train-18938", "mrqa_naturalquestions-train-46670", "mrqa_naturalquestions-train-49634", "mrqa_naturalquestions-train-85207", "mrqa_naturalquestions-train-79788", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-train-1481", "mrqa_naturalquestions-train-69664", "mrqa_naturalquestions-train-15691", "mrqa_naturalquestions-train-57639", "mrqa_naturalquestions-train-348"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 37, "before_eval": {"predictions": ["San Joaquin Valley Railroad", "broken", "7 December 2000", "Post Alley under Pike Place Market", "mother-of-pearl made between 500 AD and 2000", "February 20, 1978", "stomach", "Walter Mondale", "96", "the basic curriculum -- the enkuklios paideia or `` education in a circle '' -- of late Classical and Hellenistic Greece", "president harding", "a black background representing the circle with glossy gold letters", "the alluvial plain", "around 11 miles (18 km) south of San Jose", "india", "Rumplestiltskin", "Harry Kane", "small marsupials including the endangered sandhill dunnart ( Sminthopsis psammophila )", "numerous musical venues, including the Teatr Wielki, the Polish National Opera, the Chamber Opera, the National Philharmonic Hall and the National Theatre", "riper grapes", "1991", "india", "7 January 1936", "ten years", "twenty- three", "Edwin Hubble", "education, sanitation, and traffic control within the city limits", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Nlend Wom\u00e9", "mistreatment", "cream", "Oxford, UK"], "metric_results": {"EM": 0.1875, "QA-F1": 0.25260416666666663}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.1, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.13333333333333333, 0.8, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-4108", "mrqa_hotpotqa-validation-104", "mrqa_squad-validation-5451", "mrqa_triviaqa-validation-1015", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-3479", "mrqa_squad-validation-5702", "mrqa_naturalquestions-validation-9058", "mrqa_squad-validation-2660", "mrqa_triviaqa-validation-5258", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-1698", "mrqa_squad-validation-677", "mrqa_triviaqa-validation-7134", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-3876", "mrqa_naturalquestions-validation-969", "mrqa_hotpotqa-validation-2377", "mrqa_squad-validation-8069", "mrqa_squad-validation-7246", "mrqa_naturalquestions-validation-1375", "mrqa_hotpotqa-validation-633", "mrqa_triviaqa-validation-2524", "mrqa_hotpotqa-validation-5371"], "retrieved_ids": ["mrqa_naturalquestions-train-22530", "mrqa_naturalquestions-train-1611", "mrqa_naturalquestions-train-4319", "mrqa_naturalquestions-train-64974", "mrqa_naturalquestions-train-74532", "mrqa_naturalquestions-train-30026", "mrqa_naturalquestions-train-49336", "mrqa_naturalquestions-train-40245", "mrqa_naturalquestions-train-82397", "mrqa_naturalquestions-train-66951", "mrqa_naturalquestions-train-7327", "mrqa_naturalquestions-train-1701", "mrqa_naturalquestions-train-86755", "mrqa_naturalquestions-train-11548", "mrqa_naturalquestions-train-23803", "mrqa_naturalquestions-train-71470", "mrqa_naturalquestions-train-21057", "mrqa_naturalquestions-validation-5522", "mrqa_triviaqa-validation-639", "mrqa_naturalquestions-train-27782", "mrqa_naturalquestions-train-42553", "mrqa_naturalquestions-train-86246", "mrqa_naturalquestions-train-61692", "mrqa_naturalquestions-train-84997", "mrqa_squad-validation-5618", "mrqa_naturalquestions-train-18358", "mrqa_naturalquestions-train-66157", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-train-29378", "mrqa_naturalquestions-train-8562", "mrqa_naturalquestions-train-41174", "mrqa_naturalquestions-train-73591"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 38, "before_eval": {"predictions": ["the NP-complete Boolean satisfiability problem", "Dan Stevens", "New England", "Etienne de Mestre", "koreans", "slavery, especially Southern political leaders'resistance to attempts by Northern antislavery political forces to block the expansion of slavery into the western territories", "military units from their parent countries of Great Britain and France, as well as by American Indian allies", "The History of Little Goody Two - Shoes is a children's story published by John Newbery in London in 1765", "It has the longest rotation period ( 243 days ) of any planet in the Solar System and rotates in the opposite direction to most other planets ( meaning the Sun would rise in the west and set in the east )", "gathering money from the public", "Eden", "officers are given a one - time stipend when commissioned to purchase their required uniform items", "Jeff Meldrum", "741 weeks", "Norman Painting", "Shoshone, his mother tongue, and other western American Indian languages", "The Paris Sisters", "emigrant Ship to Australia", "60", "journalist", "the fact that there is no revising chamber", "avatar", "the points of algebro-geometric objects, via the notion of the spectrum of a ring", "most of the items in the collection, unless those were newly accessioned into the collection", "fiat money is the root cause of the continuum of economic crises, since it leads to the dominance of fraud, corruption, and manipulation precisely because it does not satisfy the criteria for a medium of exchange cited above", "glycine", "Alta Wind Energy Center in California", "The early modern period began approximately in the early 16th century ; notable historical milestones included the European Renaissance, the Age of Discovery, and the Protestant Reformation", "Lord's", "eddy Shah", "Ron Favreau", "In healthy adults, there are two normal heart sounds, often described as a lub and a dub ( or dup ), that occur in sequence with each heartbeat"], "metric_results": {"EM": 0.0625, "QA-F1": 0.18606956269700037}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.09523809523809523, 0.0909090909090909, 0.5217391304347826, 0.05714285714285714, 0.47058823529411764, 0.0, 0.25, 0.25, 0.0, 0.0, 0.15384615384615383, 0.0, 0.0, 0.2857142857142857, 1.0, 0.6, 0.0, 0.0, 0.5, 0.1142857142857143, 0.0, 0.0, 0.3846153846153846, 0.0, 0.0, 0.0, 0.0625]}}, "error_ids": ["mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-2499", "mrqa_triviaqa-validation-4677", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-8418", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-10255", "mrqa_hotpotqa-validation-727", "mrqa_naturalquestions-validation-4067", "mrqa_hotpotqa-validation-1116", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-4762", "mrqa_hotpotqa-validation-2682", "mrqa_naturalquestions-validation-3707", "mrqa_triviaqa-validation-3118", "mrqa_naturalquestions-validation-6210", "mrqa_squad-validation-9478", "mrqa_triviaqa-validation-1473", "mrqa_squad-validation-9032", "mrqa_squad-validation-5505", "mrqa_naturalquestions-validation-10279", "mrqa_triviaqa-validation-3320", "mrqa_naturalquestions-validation-7630", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-2100", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2555"], "retrieved_ids": ["mrqa_naturalquestions-train-66796", "mrqa_naturalquestions-train-53566", "mrqa_naturalquestions-train-6538", "mrqa_naturalquestions-train-59986", "mrqa_naturalquestions-train-51569", "mrqa_naturalquestions-train-28727", "mrqa_naturalquestions-train-47548", "mrqa_naturalquestions-train-11431", "mrqa_naturalquestions-train-32961", "mrqa_naturalquestions-train-56295", "mrqa_hotpotqa-validation-3253", "mrqa_naturalquestions-train-48795", "mrqa_naturalquestions-train-81356", "mrqa_triviaqa-validation-338", "mrqa_naturalquestions-train-63943", "mrqa_naturalquestions-train-22865", "mrqa_naturalquestions-train-27257", "mrqa_naturalquestions-train-21421", "mrqa_naturalquestions-train-36180", "mrqa_naturalquestions-train-32716", "mrqa_naturalquestions-train-23422", "mrqa_naturalquestions-train-16983", "mrqa_naturalquestions-train-24540", "mrqa_naturalquestions-train-68709", "mrqa_naturalquestions-train-78709", "mrqa_naturalquestions-train-16284", "mrqa_naturalquestions-train-25295", "mrqa_naturalquestions-train-19864", "mrqa_naturalquestions-train-83100", "mrqa_naturalquestions-train-29686", "mrqa_naturalquestions-train-31400", "mrqa_naturalquestions-train-69502"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 39, "before_eval": {"predictions": ["capital city of a nation", "Dan Conner", "checkpoint Charlie", "Lee Harvey Oswald", "Katharine Hepburn", "violence", "Joaquin Phoenix as Cash, Reese Witherspoon as Carter, Ginnifer Goodwin as Vivian Liberto, and Robert Patrick as Cash's father", "keskes", "1977", "Carl Sagan", "New York", "a man who makes potions in a travelling show", "2000", "beeh", "Fabbrica Italiana Automobili Torino", "the second Sunday of March", "relative units", "woman", "two", "August 10, 1933", "spanning the Golden Gate, the one - mile - wide ( 1.6 km ) strait connecting San Francisco Bay and the Pacific Ocean", "Sochi, Russia", "those who already hold wealth", "bilingual German author B. Traven", "Finding Nemo", "unidentified flying objects (UFOs) the extraterrestrial hypothesis (ETH) as well as paranormal and Fortean subjects in general", "oil", "beavers", "264,152", "Princeton", "the German Empire", "high pressure"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3294446169155921}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false], "QA-F1": [0.5714285714285715, 1.0, 0.0, 0.0, 0.5, 1.0, 0.9230769230769231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3157894736842105, 0.0, 1.0, 0.7142857142857143, 1.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-1438", "mrqa_hotpotqa-validation-944", "mrqa_naturalquestions-validation-9227", "mrqa_triviaqa-validation-2522", "mrqa_naturalquestions-validation-6554", "mrqa_squad-validation-8070", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-7195", "mrqa_naturalquestions-validation-3698", "mrqa_triviaqa-validation-3017", "mrqa_squad-validation-10428", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-5595", "mrqa_naturalquestions-validation-3108", "mrqa_triviaqa-validation-4024", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-1607", "mrqa_triviaqa-validation-2808", "mrqa_triviaqa-validation-1276", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-2203", "mrqa_squad-validation-3650"], "retrieved_ids": ["mrqa_naturalquestions-train-12010", "mrqa_naturalquestions-train-28488", "mrqa_hotpotqa-validation-2340", "mrqa_naturalquestions-train-53326", "mrqa_naturalquestions-train-73546", "mrqa_naturalquestions-train-17177", "mrqa_naturalquestions-train-24879", "mrqa_naturalquestions-train-44505", "mrqa_naturalquestions-train-53077", "mrqa_naturalquestions-train-20596", "mrqa_naturalquestions-train-47", "mrqa_hotpotqa-validation-1289", "mrqa_naturalquestions-train-19051", "mrqa_naturalquestions-train-82776", "mrqa_naturalquestions-train-51562", "mrqa_naturalquestions-train-23992", "mrqa_squad-validation-6303", "mrqa_naturalquestions-train-54002", "mrqa_naturalquestions-train-39061", "mrqa_naturalquestions-train-7002", "mrqa_naturalquestions-train-76576", "mrqa_naturalquestions-train-37755", "mrqa_naturalquestions-train-34095", "mrqa_hotpotqa-validation-2484", "mrqa_naturalquestions-train-85774", "mrqa_naturalquestions-train-25258", "mrqa_naturalquestions-train-11240", "mrqa_naturalquestions-train-62518", "mrqa_naturalquestions-train-1409", "mrqa_naturalquestions-train-27782", "mrqa_naturalquestions-train-63731", "mrqa_naturalquestions-train-10353"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 40, "before_eval": {"predictions": ["50 fund", "Bartle Frere", "Samarkand", "ice dancing", "Isabella (Belle) Baumfree", "corgis", "throughout the 14th to 17th centuries", "five starting pitchers, seven relief pitchers, two catchers, six infielders, and five outfielders", "prophecy", "Bacon", "Charlton Heston", "anti-inflammatory molecules", "vuHMOaD/JVI aiAQBAJA jaMOJOD", "Kevin Kolb", "a tradeable entity used to avoid the inconveniences of a pure barter system", "United States Presidents", "ch\u00e2teau de chambord in France", "Korean King, who was ranked last", "Sochi, Russia", "detroit", "Canadian Rockies continental divide east to central Saskatchewan, where it joins with another major river to make up the Saskatchewan River", "How soon after the cabin fire incident", "vida Goldstein", "smartpen", "160", "the Secret Intelligence Service", "100 billion", "tai su, teknon", "photolysis", "4.7 / 5.5", "Queen City", "an American federal law that imposes liability on persons and companies ( typically federal contractors ) who defraud governmental programs"], "metric_results": {"EM": 0.25, "QA-F1": 0.2799005681818182}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.375, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6212", "mrqa_squad-validation-6248", "mrqa_triviaqa-validation-1571", "mrqa_triviaqa-validation-2475", "mrqa_naturalquestions-validation-8961", "mrqa_squad-validation-2249", "mrqa_naturalquestions-validation-7457", "mrqa_squad-validation-6588", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3213", "mrqa_naturalquestions-validation-1063", "mrqa_triviaqa-validation-6127", "mrqa_squad-validation-8247", "mrqa_triviaqa-validation-1859", "mrqa_naturalquestions-validation-8514", "mrqa_squad-validation-3932", "mrqa_triviaqa-validation-3280", "mrqa_triviaqa-validation-4123", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1791", "mrqa_triviaqa-validation-1877", "mrqa_triviaqa-validation-579", "mrqa_naturalquestions-validation-6848", "mrqa_naturalquestions-validation-993"], "retrieved_ids": ["mrqa_naturalquestions-train-16116", "mrqa_hotpotqa-validation-1099", "mrqa_naturalquestions-train-1073", "mrqa_naturalquestions-train-35711", "mrqa_naturalquestions-train-9576", "mrqa_naturalquestions-train-13294", "mrqa_naturalquestions-train-79285", "mrqa_hotpotqa-validation-1210", "mrqa_naturalquestions-train-19051", "mrqa_squad-validation-9532", "mrqa_naturalquestions-train-9003", "mrqa_naturalquestions-train-77303", "mrqa_naturalquestions-train-56197", "mrqa_naturalquestions-train-36855", "mrqa_naturalquestions-train-54737", "mrqa_triviaqa-validation-7638", "mrqa_naturalquestions-train-2983", "mrqa_naturalquestions-train-58694", "mrqa_naturalquestions-train-44403", "mrqa_naturalquestions-train-70702", "mrqa_naturalquestions-train-33677", "mrqa_naturalquestions-train-16881", "mrqa_naturalquestions-train-6618", "mrqa_naturalquestions-train-10851", "mrqa_naturalquestions-train-51349", "mrqa_triviaqa-validation-1935", "mrqa_naturalquestions-train-16940", "mrqa_naturalquestions-train-80330", "mrqa_naturalquestions-train-36510", "mrqa_naturalquestions-train-73342", "mrqa_naturalquestions-train-14215", "mrqa_naturalquestions-train-59430"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 41, "before_eval": {"predictions": ["charged particle beam weapons", "Hindi film industry", "Gaelic", "Three card brag", "d\u00edsabl\u00f3t", "lion", "Russian film industry", "sediment load", "Washington metropolitan area", "a GTPase responsible for endocytosis in the eukaryotic cell", "User State Migration Tool ( USMT )", "Ordos City", "pie tins", "PPG Paints Arena, Pittsburgh, Pennsylvania", "philry wall museum", "Section 30 of the Teaching Council Act 2001", "Henry Gibson as Wilbur, a pig who was almost killed due to being a runt", "mid-1988", "quasars", "Monsoon", "Romansh", "Tudor king", "newstalk radio station 5AA (FIVEaa)", "James Bond quartermaster Q", "Philippi in Greece during Paul's second missionary journey, which occurred between approximately 49 and 51 AD", "the division of labour, productivity, and free markets", "Gerard Marenghi", "Whitney Houston", "Nebula Award", "conservative", "David", "Elvis Presley"], "metric_results": {"EM": 0.125, "QA-F1": 0.23410669191919192}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.5, 1.0, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.05, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.4, 0.33333333333333337, 0.4, 0.0, 0.0, 0.5, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-1592", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-2375", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-9712", "mrqa_triviaqa-validation-4955", "mrqa_naturalquestions-validation-3058", "mrqa_triviaqa-validation-684", "mrqa_squad-validation-2142", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-5007", "mrqa_triviaqa-validation-2384", "mrqa_naturalquestions-validation-774", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-2181", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-4823", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8338", "mrqa_hotpotqa-validation-3872", "mrqa_triviaqa-validation-1585", "mrqa_hotpotqa-validation-434", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-4320", "mrqa_naturalquestions-validation-9763"], "retrieved_ids": ["mrqa_naturalquestions-train-47188", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-train-17472", "mrqa_triviaqa-validation-1551", "mrqa_naturalquestions-train-12079", "mrqa_naturalquestions-train-68209", "mrqa_naturalquestions-train-7638", "mrqa_naturalquestions-train-86491", "mrqa_naturalquestions-train-25916", "mrqa_naturalquestions-train-84384", "mrqa_naturalquestions-train-45785", "mrqa_naturalquestions-train-85256", "mrqa_naturalquestions-train-52760", "mrqa_naturalquestions-train-1800", "mrqa_naturalquestions-train-38023", "mrqa_naturalquestions-train-40268", "mrqa_naturalquestions-train-13291", "mrqa_naturalquestions-train-54518", "mrqa_naturalquestions-train-21582", "mrqa_naturalquestions-train-19051", "mrqa_naturalquestions-train-76628", "mrqa_naturalquestions-train-88239", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-train-68685", "mrqa_naturalquestions-train-9295", "mrqa_naturalquestions-train-65150", "mrqa_naturalquestions-train-39977", "mrqa_naturalquestions-train-28911", "mrqa_naturalquestions-train-59327", "mrqa_naturalquestions-train-9504", "mrqa_naturalquestions-train-16445", "mrqa_triviaqa-validation-6250"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 42, "before_eval": {"predictions": ["Daryl Hannah", "cavatelli, acini di pepe, pastina, orzo, etc.", "king lilliput", "independence from the Duke of Savoy", "various causes", "questions about the name of the war, the tariff, states'rights and the nature of Abraham Lincoln's war goals", "chartered", "a lack of remorse, an attempt to avoid responsibility for her actions, and even a likelihood of repeating her illegal actions", "a large Danish shipping company that operates passenger and freight services across northern Europe", "a defender", "rommel", "in the duodenum where it performs proteolysis, the breakdown of proteins and polypeptides", "F fructose, or fruit sugar", "their unusual behavior, such as the number of men killed and the manner of the attacks", "Gainsborough Trinity Football Club", "david Thomas", "by functions ; Introverted Sensing ( Si ), Extroverted Thinking ( Te ), Introverted Feeling ( Fi ) and Extrovert Intuition ( Ne )", "Thursday", "yellow", "the appropriateness of the drug therapy (e.g. drug choice, dose, route, frequency, and duration of therapy) and its efficacy", "jonathan", "feats of exploration", "a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "Michael Bezjian", "The Education Service Contracting scheme of the government", "pre-19th century", "to rebuild St. Peter's Basilica in Rome", "to its colonies, with the only notable exception of Algeria, where French settlers nevertheless always remained a small minority", "two forces, one pointing north, and one pointing east", "new laws or amendments to existing laws as a bill", "Jack Murphy Stadium", "hierarchy theorems"], "metric_results": {"EM": 0.0625, "QA-F1": 0.22636037879604057}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.9333333333333333, 0.0, 0.0, 0.0, 0.11764705882352941, 1.0, 0.2857142857142857, 0.14285714285714288, 0.0, 0.0, 0.1764705882352941, 0.33333333333333337, 0.375, 0.5, 0.0, 0.33333333333333337, 0.0, 0.0, 0.6923076923076924, 0.0, 1.0, 0.13333333333333333, 0.0, 0.1818181818181818, 0.0, 0.7272727272727273, 0.1111111111111111, 0.19999999999999998, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2952", "mrqa_naturalquestions-validation-8284", "mrqa_triviaqa-validation-3261", "mrqa_squad-validation-3044", "mrqa_squad-validation-1429", "mrqa_naturalquestions-validation-9093", "mrqa_squad-validation-6735", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-5731", "mrqa_triviaqa-validation-4137", "mrqa_naturalquestions-validation-7233", "mrqa_naturalquestions-validation-4185", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-680", "mrqa_naturalquestions-validation-6706", "mrqa_squad-validation-9569", "mrqa_triviaqa-validation-7426", "mrqa_squad-validation-6369", "mrqa_triviaqa-validation-7133", "mrqa_squad-validation-3385", "mrqa_triviaqa-validation-1586", "mrqa_squad-validation-7034", "mrqa_triviaqa-validation-289", "mrqa_squad-validation-2150", "mrqa_squad-validation-9792", "mrqa_squad-validation-10395", "mrqa_squad-validation-9452", "mrqa_hotpotqa-validation-5522", "mrqa_squad-validation-1808"], "retrieved_ids": ["mrqa_naturalquestions-train-72399", "mrqa_naturalquestions-train-24610", "mrqa_naturalquestions-train-27203", "mrqa_naturalquestions-train-36986", "mrqa_triviaqa-validation-3515", "mrqa_naturalquestions-train-26439", "mrqa_naturalquestions-train-39182", "mrqa_naturalquestions-train-83375", "mrqa_naturalquestions-train-18259", "mrqa_naturalquestions-train-45549", "mrqa_naturalquestions-train-26513", "mrqa_naturalquestions-train-43276", "mrqa_naturalquestions-train-63822", "mrqa_triviaqa-validation-1585", "mrqa_squad-validation-10036", "mrqa_naturalquestions-train-7256", "mrqa_naturalquestions-train-46980", "mrqa_naturalquestions-train-12832", "mrqa_naturalquestions-train-83344", "mrqa_triviaqa-validation-5304", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-train-84465", "mrqa_naturalquestions-train-74979", "mrqa_naturalquestions-train-33011", "mrqa_naturalquestions-train-14866", "mrqa_naturalquestions-train-54823", "mrqa_naturalquestions-train-76196", "mrqa_naturalquestions-train-23512", "mrqa_naturalquestions-train-46441", "mrqa_naturalquestions-train-76271", "mrqa_naturalquestions-train-55882", "mrqa_naturalquestions-train-54611"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 43, "before_eval": {"predictions": ["American Falls", "letters", "Indiana", "joseph smith", "French", "a \"homeward bounder\"", "dot", "autoimmune diseases, inflammatory diseases and cancer", "Py", "personal care products", "there Was an old lady who swallowed a fly", "Rigoletto", "presidential aircraft", "most abundant", "furniture", "216 countries and territories around the world", "egypt", "Nicholas Stone, Caius Gabriel Cibber, Grinling Gibbons, John Michael Rysbrack, Louis-Fran\u00e7ois Roubiliac, Peter Scheemakers, Sir Henry Cheere,", "Algernod Lanier Washington", "the Outfield", "Croatia", "Michael Edwards", "marine engines", "eddie fisher", "third quarter ( also known as last quarter )", "bresslaw", "Yuan T. Lee", "Tennessee", "technology incidental to rocketry and manned spaceflight", "eve", "237 square miles", "magi"], "metric_results": {"EM": 0.09375, "QA-F1": 0.2351190476190476}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.4, 0.5, 0.6666666666666666, 0.5, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7744", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-6916", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-981", "mrqa_naturalquestions-validation-2663", "mrqa_squad-validation-6442", "mrqa_triviaqa-validation-5580", "mrqa_hotpotqa-validation-518", "mrqa_triviaqa-validation-5087", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-850", "mrqa_squad-validation-3671", "mrqa_triviaqa-validation-4090", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-5160", "mrqa_squad-validation-5586", "mrqa_hotpotqa-validation-5370", "mrqa_naturalquestions-validation-5822", "mrqa_triviaqa-validation-7482", "mrqa_squad-validation-3330", "mrqa_triviaqa-validation-2803", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-7215", "mrqa_hotpotqa-validation-4624", "mrqa_squad-validation-3812", "mrqa_triviaqa-validation-5899", "mrqa_hotpotqa-validation-5541", "mrqa_triviaqa-validation-305"], "retrieved_ids": ["mrqa_naturalquestions-train-82662", "mrqa_naturalquestions-train-25908", "mrqa_naturalquestions-train-79317", "mrqa_naturalquestions-train-71429", "mrqa_naturalquestions-train-64865", "mrqa_naturalquestions-train-22233", "mrqa_naturalquestions-train-2863", "mrqa_naturalquestions-train-71885", "mrqa_naturalquestions-train-87141", "mrqa_naturalquestions-train-88035", "mrqa_naturalquestions-train-47223", "mrqa_naturalquestions-train-32291", "mrqa_naturalquestions-train-4543", "mrqa_naturalquestions-train-6834", "mrqa_naturalquestions-train-60990", "mrqa_naturalquestions-train-21595", "mrqa_naturalquestions-train-17660", "mrqa_naturalquestions-train-16323", "mrqa_naturalquestions-train-54258", "mrqa_naturalquestions-train-27214", "mrqa_naturalquestions-train-50804", "mrqa_naturalquestions-train-85623", "mrqa_naturalquestions-train-3912", "mrqa_naturalquestions-train-32842", "mrqa_naturalquestions-train-48211", "mrqa_naturalquestions-validation-9227", "mrqa_naturalquestions-train-26327", "mrqa_naturalquestions-train-38415", "mrqa_naturalquestions-train-26214", "mrqa_naturalquestions-train-24727", "mrqa_naturalquestions-train-42395", "mrqa_naturalquestions-train-31881"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 44, "before_eval": {"predictions": ["King T'Chaka of the African nation Wakanda", "high test scores", "England and Wales Cricket Board ( ECB )", "London", "football match Helsingborgs IF GIF Sundsvall", "campaign setting", "2003", "867 feet", "possibly brought from the Byzantine Empire ( as \u039c\u03b1\u03bd\u03bf\u03c5\u03ae\u03bb ) to Spain and Portugal, where it has been used since at least the 13th century", "\u00f7", "Christopher Lee as Count Dooku / Darth Tyranus", "second", "Tikki tikki tembo-no sa rembo- chari bari ruchi-pip peri pembo", "medicine and other healthcare professionals to improve pharmaceutical care", "increased patient health outcomes and decreased costs to the health care system", "Good Boy Deserves Favour", "Gabriel Alberto Azucena", "12951 / 52 Mumbai Rajdhani Express considering average speed including halts 12049 / 50 : Agra Cantonment - H. Nizamuddin Gatimaan Express - maximum speed 160 km / h", "Italian : Scalinata di Trinit\u00e0 dei Monti", "May 18, 2010", "an activist in the cause of anti-fascism", "European Union (EU)", "philosopher, statesman, scientist, jurist, orator, and author", "ltd", "Ministry of Corporate Affairs", "British", "ancient cult activity as far back as 7th century BCE", "casket letters", "energy-storage molecules ATP and NADPH", "Hubble Space Telescope is brought into space and placed in Earth's orbit by the Space Shuttle Discovery and her crew in 1990.", "our neighbors as ourselves", "Christ lag"], "metric_results": {"EM": 0.125, "QA-F1": 0.23464920496170494}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.4, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.15384615384615385, 0.7272727272727273, 1.0, 0.0, 0.15384615384615383, 0.39999999999999997, 0.0, 0.0, 0.3571428571428571, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-2236", "mrqa_naturalquestions-validation-2119", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-6579", "mrqa_hotpotqa-validation-5305", "mrqa_naturalquestions-validation-10619", "mrqa_hotpotqa-validation-910", "mrqa_hotpotqa-validation-4649", "mrqa_naturalquestions-validation-5550", "mrqa_triviaqa-validation-3664", "mrqa_squad-validation-6319", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-2406", "mrqa_hotpotqa-validation-4278", "mrqa_naturalquestions-validation-3459", "mrqa_naturalquestions-validation-8491", "mrqa_hotpotqa-validation-4794", "mrqa_triviaqa-validation-5022", "mrqa_hotpotqa-validation-389", "mrqa_triviaqa-validation-3617", "mrqa_naturalquestions-validation-6057", "mrqa_hotpotqa-validation-5696", "mrqa_naturalquestions-validation-1725", "mrqa_triviaqa-validation-4027", "mrqa_squad-validation-8625", "mrqa_triviaqa-validation-1504", "mrqa_squad-validation-9951", "mrqa_squad-validation-2419"], "retrieved_ids": ["mrqa_naturalquestions-train-54304", "mrqa_naturalquestions-train-13583", "mrqa_naturalquestions-train-54888", "mrqa_naturalquestions-train-2753", "mrqa_naturalquestions-train-64319", "mrqa_naturalquestions-train-37272", "mrqa_naturalquestions-train-27433", "mrqa_naturalquestions-train-56172", "mrqa_naturalquestions-train-40896", "mrqa_squad-validation-1903", "mrqa_naturalquestions-train-16437", "mrqa_naturalquestions-train-67693", "mrqa_naturalquestions-train-63104", "mrqa_naturalquestions-train-25524", "mrqa_naturalquestions-train-10879", "mrqa_naturalquestions-train-21732", "mrqa_naturalquestions-train-74115", "mrqa_naturalquestions-train-7397", "mrqa_naturalquestions-train-8660", "mrqa_naturalquestions-train-40647", "mrqa_naturalquestions-train-28187", "mrqa_naturalquestions-train-32377", "mrqa_squad-validation-2660", "mrqa_naturalquestions-train-23421", "mrqa_naturalquestions-train-18796", "mrqa_naturalquestions-train-38514", "mrqa_naturalquestions-train-81906", "mrqa_naturalquestions-train-35039", "mrqa_naturalquestions-train-77111", "mrqa_squad-validation-7278", "mrqa_naturalquestions-train-27671", "mrqa_naturalquestions-train-64039"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 45, "before_eval": {"predictions": ["Mickey Spillane", "2008 Detroit Lions", "perique", "under `` the immortal Hawke ''", "death penalty", "a stout man with a \" double chin, strong mouth, piercing deep-set eyes, fleshy face, and squat neck", "agulhas", "Chr\u00e9tien de Troyes", "Mangal Pandey", "Colonia Agrippina", "Cartwright", "four", "paris", "the eighth series", "Pebble Beach", "Los Angeles", "French", "Henry Daniel Mills", "\"LOVE Radio\"", "Boston Red Sox", "Illinois", "travolta", "Donald Henkel", "brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil to put an end to Saddam Hussein's occupation of Kuwait", "The Zombies", "Fox News Specialists", "National Football League", "Lunar Excursion Module (LEM, later shortened to Lunar Module, LM)", "Santa Clara, California", "a fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Operation Neptune", "Mediterranean Sea"], "metric_results": {"EM": 0.125, "QA-F1": 0.19717261904761907}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.07142857142857142, 0.0, 0.0, 0.0, 0.16666666666666669, 0.8, 0.07142857142857142, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7473", "mrqa_hotpotqa-validation-1907", "mrqa_triviaqa-validation-2201", "mrqa_squad-validation-2598", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-5587", "mrqa_naturalquestions-validation-4097", "mrqa_squad-validation-9296", "mrqa_triviaqa-validation-6956", "mrqa_naturalquestions-validation-8728", "mrqa_triviaqa-validation-3335", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3363", "mrqa_hotpotqa-validation-5068", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-1433", "mrqa_hotpotqa-validation-5149", "mrqa_naturalquestions-validation-9931", "mrqa_triviaqa-validation-455", "mrqa_hotpotqa-validation-866", "mrqa_squad-validation-9513", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-3509", "mrqa_naturalquestions-validation-3217", "mrqa_squad-validation-3845", "mrqa_squad-validation-13", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-2067"], "retrieved_ids": ["mrqa_naturalquestions-validation-441", "mrqa_naturalquestions-train-30589", "mrqa_naturalquestions-train-9279", "mrqa_naturalquestions-train-7544", "mrqa_naturalquestions-train-55281", "mrqa_naturalquestions-train-85006", "mrqa_naturalquestions-train-15416", "mrqa_naturalquestions-train-80776", "mrqa_naturalquestions-train-56687", "mrqa_naturalquestions-train-38876", "mrqa_naturalquestions-train-77375", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-train-17268", "mrqa_naturalquestions-train-74250", "mrqa_naturalquestions-train-85923", "mrqa_naturalquestions-train-7004", "mrqa_naturalquestions-train-73367", "mrqa_naturalquestions-train-86671", "mrqa_naturalquestions-train-79514", "mrqa_naturalquestions-train-85136", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-train-36856", "mrqa_naturalquestions-train-46045", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-train-74661", "mrqa_naturalquestions-train-4886", "mrqa_naturalquestions-train-7054", "mrqa_naturalquestions-train-28169", "mrqa_naturalquestions-train-19259", "mrqa_naturalquestions-train-53387", "mrqa_naturalquestions-train-76472", "mrqa_naturalquestions-train-55396"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 46, "before_eval": {"predictions": ["baseball", "convection currents in the asthenosphere, which is ductile, or plastic, and the brittle lithosphere ( crust and upper mantle )", "Take That", "transgender", "electric lighting", "used obscure languages as a means of secret communication during wartime", "Galileo Galilei and Sir Isaac Newton", "electromagnetic theory", "Premier League", "Isabella", "Elizabeth Weber", "a first-person psychological horror adventure game", "hundreds", "Waiting for Guffman", "1999", "a new facility on The Watermark business park next to the MetroCentre in Gateshead", "pearmain", "partial funding for the 20th anniversary special The Five Doctors", "5% abv draught beer", "inefficient", "Chu'Tsai", "Liz", "Proportionality", "lago di Como", "Grissom, White, and Chaffee", "multinational retail corporation", "passion fruit", "The Natya Shastra", "the sand grains cause a scrubbing noise as they rub against each other when walked on", "1998", "the classical element fire", "the Emperor of Austria"], "metric_results": {"EM": 0.34375, "QA-F1": 0.3899219773680981}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, true, true, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.25, 1.0, 0.14814814814814817, 0.0, 1.0, 0.0, 0.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0, 0.42857142857142855, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0689655172413793, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_naturalquestions-validation-8204", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-5352", "mrqa_squad-validation-10341", "mrqa_hotpotqa-validation-1831", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-3284", "mrqa_hotpotqa-validation-1074", "mrqa_hotpotqa-validation-73", "mrqa_squad-validation-5464", "mrqa_triviaqa-validation-7350", "mrqa_squad-validation-7792", "mrqa_hotpotqa-validation-5239", "mrqa_naturalquestions-validation-2890", "mrqa_squad-validation-4064", "mrqa_triviaqa-validation-2914", "mrqa_naturalquestions-validation-2808", "mrqa_triviaqa-validation-2873", "mrqa_squad-validation-3523", "mrqa_triviaqa-validation-4430"], "retrieved_ids": ["mrqa_naturalquestions-train-7910", "mrqa_naturalquestions-train-37019", "mrqa_naturalquestions-train-70701", "mrqa_naturalquestions-train-39188", "mrqa_naturalquestions-train-68305", "mrqa_naturalquestions-train-4654", "mrqa_naturalquestions-train-6017", "mrqa_naturalquestions-train-54264", "mrqa_naturalquestions-train-58422", "mrqa_naturalquestions-train-14083", "mrqa_naturalquestions-train-48021", "mrqa_naturalquestions-train-3912", "mrqa_naturalquestions-train-83811", "mrqa_naturalquestions-train-22844", "mrqa_naturalquestions-train-6631", "mrqa_naturalquestions-train-63281", "mrqa_naturalquestions-train-67169", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-train-50935", "mrqa_naturalquestions-train-30455", "mrqa_naturalquestions-train-6824", "mrqa_naturalquestions-train-13583", "mrqa_naturalquestions-train-17322", "mrqa_naturalquestions-train-86254", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-train-64254", "mrqa_naturalquestions-train-61743", "mrqa_naturalquestions-train-1761", "mrqa_naturalquestions-train-10859", "mrqa_naturalquestions-train-69581", "mrqa_naturalquestions-train-81959", "mrqa_naturalquestions-train-42636"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 47, "before_eval": {"predictions": ["egypt", "horse racing", "New Zealand national team", "Yan'an", "Styal Mill", "big - name lawyers", "Milk Barn Animation", "when they enter the army during initial entry training", "one of The Canterbury Tales by Geoffrey Chaucer", "they announced a hiatus and re-united two years later for the release of their fourth and final studio album, Destiny Fulfilled ( 2004 )", "ltd", "star", "around 74 per cent", "Heathrow", "often social communities with considerable face-to-face interaction among members", "William Strauss and Neil Howe", "monophyletic", "insecticide toxicology", "a liturgical setting of the Lord's Prayer and as a means of examining candidates on specific catechism questions", "a pH indicator", "about 50%", "italy", "John and Charles Wesley", "Science and Discovery", "Euclid's fundamental theorem of arithmetic", "Mississippi State", "loner Cal Trask", "Jude", "cuba", "work in a bridal shop with Anita, the girlfriend of her brother, Bernardo", "XXXTentacion", "a downward pressure on wages"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2710375816993464}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false], "QA-F1": [0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5555555555555556, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2770", "mrqa_triviaqa-validation-945", "mrqa_squad-validation-6287", "mrqa_naturalquestions-validation-930", "mrqa_hotpotqa-validation-5877", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2864", "mrqa_triviaqa-validation-1256", "mrqa_naturalquestions-validation-5305", "mrqa_squad-validation-4212", "mrqa_triviaqa-validation-1516", "mrqa_hotpotqa-validation-5086", "mrqa_naturalquestions-validation-8582", "mrqa_squad-validation-4479", "mrqa_triviaqa-validation-3868", "mrqa_squad-validation-2346", "mrqa_squad-validation-3685", "mrqa_triviaqa-validation-4298", "mrqa_hotpotqa-validation-2801", "mrqa_squad-validation-9061", "mrqa_hotpotqa-validation-662", "mrqa_triviaqa-validation-1799", "mrqa_triviaqa-validation-7477", "mrqa_naturalquestions-validation-5241", "mrqa_squad-validation-7182"], "retrieved_ids": ["mrqa_naturalquestions-train-25899", "mrqa_naturalquestions-train-87809", "mrqa_naturalquestions-train-36143", "mrqa_naturalquestions-train-5112", "mrqa_naturalquestions-train-52874", "mrqa_naturalquestions-train-83148", "mrqa_naturalquestions-train-26514", "mrqa_naturalquestions-train-70576", "mrqa_naturalquestions-train-30376", "mrqa_naturalquestions-train-61331", "mrqa_naturalquestions-train-84973", "mrqa_naturalquestions-train-6049", "mrqa_naturalquestions-train-67301", "mrqa_naturalquestions-train-59662", "mrqa_squad-validation-2236", "mrqa_naturalquestions-train-61337", "mrqa_naturalquestions-train-16490", "mrqa_naturalquestions-train-60402", "mrqa_naturalquestions-train-68512", "mrqa_naturalquestions-train-4531", "mrqa_naturalquestions-train-84907", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-train-57150", "mrqa_naturalquestions-train-35232", "mrqa_naturalquestions-train-84400", "mrqa_naturalquestions-train-40976", "mrqa_naturalquestions-train-64271", "mrqa_naturalquestions-train-73178", "mrqa_naturalquestions-train-28444", "mrqa_naturalquestions-train-13452", "mrqa_naturalquestions-train-69260", "mrqa_naturalquestions-train-26509"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 48, "before_eval": {"predictions": ["a heroine who experienced many tragedies, mostly at the hands of her controlling ex-husband, the villainous James Stenbeck ( Anthony Herrera)", "It is a song by American hip hop recording artist Kendrick Lamar", "Yosemite", "Interventive treatment", "3", "Bishop Lloyd Christ Wicke", "Ray Charles", "a thousand years", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Social Democratic", "2001", "Thon Maker", "a loop ( also called a self - loop or a `` buckle '' )", "painting, mathematics, calligraphy, poetry, and theater", "San Francisco, California", "sin", "annuity", "twin sister", "Frank Theodore `` Ted '' Levine", "a place where justice resides", "French Union", "largely determined by President Woodrow Wilson", "picppuccino", "halal", "Arthur Russell (born Charles Arthur Russell, Jr", "a method of imparting the basics of Christianity to the congregations", "Wylie Draper", "a political role for Islam", "the university's off- campuses rental policies", "hockey greats Bobby Hull and Dennis Hull, as well as painter Manley MacDonald", "Pittsburgh Steelers", "a protracted siege, during which the Mongol army under Jani Beg was suffering from the disease, the army catapulted the infected corpses over the city walls of Kaffa to infect the inhabitants."], "metric_results": {"EM": 0.21875, "QA-F1": 0.35799263784461155}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.21052631578947367, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.22222222222222218, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.16666666666666669, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.2666666666666667, 0.4, 0.6666666666666665, 0.761904761904762, 1.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-5165", "mrqa_squad-validation-5665", "mrqa_triviaqa-validation-3906", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-2607", "mrqa_triviaqa-validation-2849", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-3789", "mrqa_hotpotqa-validation-4075", "mrqa_naturalquestions-validation-4043", "mrqa_squad-validation-2263", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-1649", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-4384", "mrqa_hotpotqa-validation-962", "mrqa_squad-validation-2337", "mrqa_naturalquestions-validation-1531", "mrqa_squad-validation-9518", "mrqa_squad-validation-7947", "mrqa_hotpotqa-validation-599", "mrqa_squad-validation-4774"], "retrieved_ids": ["mrqa_naturalquestions-train-8320", "mrqa_naturalquestions-train-31705", "mrqa_naturalquestions-train-41358", "mrqa_naturalquestions-train-68341", "mrqa_naturalquestions-train-54518", "mrqa_naturalquestions-train-49569", "mrqa_naturalquestions-train-40497", "mrqa_naturalquestions-train-25432", "mrqa_naturalquestions-train-61125", "mrqa_naturalquestions-train-36373", "mrqa_naturalquestions-train-52237", "mrqa_naturalquestions-train-34208", "mrqa_naturalquestions-train-75012", "mrqa_naturalquestions-train-7270", "mrqa_naturalquestions-train-79285", "mrqa_naturalquestions-train-27579", "mrqa_naturalquestions-train-84858", "mrqa_naturalquestions-train-35900", "mrqa_naturalquestions-train-54474", "mrqa_naturalquestions-train-56208", "mrqa_naturalquestions-train-87973", "mrqa_naturalquestions-train-20687", "mrqa_naturalquestions-train-86408", "mrqa_naturalquestions-train-83071", "mrqa_naturalquestions-train-755", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-train-34565", "mrqa_naturalquestions-validation-1489", "mrqa_naturalquestions-train-16184", "mrqa_squad-validation-7296", "mrqa_naturalquestions-train-75058", "mrqa_naturalquestions-train-19407"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 49, "before_eval": {"predictions": ["vehicles inspired by the Jeep that are suitable for use on rough terrain", "Aol", "Timur", "\"Losing My Religion\" is a song by the American alternative rock band R.E.M.", "between the Eastern Ghats and the Bay of Bengal", "Ravenna", "12", "fear of people or society.", "September 1895", "improved", "a biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight until it was flying like a conventional plane", "He even lost control of the patents he had generated since he had assigned them to the company in lieu of stock. He had to work at various electrical repair jobs and even as a ditch digger for $2 per day.", "Marxist and a Leninist", "Gregor Mendel", "civil service, common markets for UK goods and services, constitution, electricity, coal, oil, gas, nuclear energy, defence and national security, drug policy, employment, foreign policy and relations with Europe,", "Philadelphia Eagles, Denver Broncos, Berlin Thunder, Las Vegas Outlaws and Ottawa Renegades.", "3,600", "State Street", "al - Mamlakah al - \u02bbArab\u012byah as - Su\u02bb\u016bd\u012byah", "44 hectares", "georgia", "sun", "Lawton Mainor Chiles Jr.", "In the episode `` Kobol's Last Gleaming ''", "Wisconsin", "uneven trade agreements", "meat, fish or any other stew", "adenosine triphosphate", "unesco", "Ruth Elizabeth \"Bette\" Davis", "uranium", "29 September 2014"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3461505832189975}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, false, true], "QA-F1": [0.13333333333333333, 0.0, 1.0, 0.42857142857142855, 0.18181818181818182, 0.15384615384615385, 0.2222222222222222, 0.0, 0.6666666666666666, 0.6666666666666666, 0.3636363636363636, 0.34782608695652173, 0.0, 1.0, 0.0, 0.09523809523809525, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.4444444444444445, 0.0, 0.0, 0.7058823529411764, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2272", "mrqa_hotpotqa-validation-5452", "mrqa_hotpotqa-validation-2032", "mrqa_naturalquestions-validation-2275", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2941", "mrqa_triviaqa-validation-6073", "mrqa_hotpotqa-validation-987", "mrqa_squad-validation-716", "mrqa_squad-validation-1459", "mrqa_squad-validation-1306", "mrqa_hotpotqa-validation-2810", "mrqa_squad-validation-9489", "mrqa_hotpotqa-validation-3964", "mrqa_squad-validation-10291", "mrqa_naturalquestions-validation-9013", "mrqa_hotpotqa-validation-4530", "mrqa_triviaqa-validation-3250", "mrqa_triviaqa-validation-6475", "mrqa_hotpotqa-validation-1315", "mrqa_naturalquestions-validation-5283", "mrqa_squad-validation-7049", "mrqa_squad-validation-9807", "mrqa_squad-validation-8518", "mrqa_triviaqa-validation-5876", "mrqa_triviaqa-validation-6380"], "retrieved_ids": ["mrqa_naturalquestions-train-18487", "mrqa_naturalquestions-train-8163", "mrqa_naturalquestions-train-29038", "mrqa_naturalquestions-train-68187", "mrqa_naturalquestions-train-18805", "mrqa_naturalquestions-train-34860", "mrqa_naturalquestions-train-36951", "mrqa_naturalquestions-train-53040", "mrqa_naturalquestions-train-47890", "mrqa_naturalquestions-train-21734", "mrqa_naturalquestions-validation-5510", "mrqa_naturalquestions-train-65437", "mrqa_naturalquestions-train-39216", "mrqa_naturalquestions-train-50920", "mrqa_naturalquestions-train-70166", "mrqa_naturalquestions-train-81611", "mrqa_naturalquestions-train-48104", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-train-42324", "mrqa_naturalquestions-train-29462", "mrqa_naturalquestions-train-58860", "mrqa_naturalquestions-train-32001", "mrqa_naturalquestions-train-6438", "mrqa_naturalquestions-train-64511", "mrqa_naturalquestions-train-12912", "mrqa_naturalquestions-train-38333", "mrqa_naturalquestions-train-15531", "mrqa_naturalquestions-train-81426", "mrqa_naturalquestions-train-9599", "mrqa_naturalquestions-train-78898", "mrqa_squad-validation-2509", "mrqa_triviaqa-validation-6935"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}], "final_eval_results": {"overall_oncoming_test": {"EM": 0.141875, "QA-F1": 0.2497049939981414}, "overall_error_number": 1373, "overall_instant_fixing_rate": 0.0, "final_instream_test": {"EM": 0.735625, "QA-F1": 0.7956649805429546}, "final_upstream_test": {"EM": 0.736, "QA-F1": 0.8421102757429829}}}