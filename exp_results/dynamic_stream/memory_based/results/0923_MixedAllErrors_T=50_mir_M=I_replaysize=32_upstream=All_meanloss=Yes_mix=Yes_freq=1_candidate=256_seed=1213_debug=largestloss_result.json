{"model_update_steps": 1645, "method_class": "mir", "base_model_args": "Namespace(base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=1213_debug=largestloss_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=True, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=1213_debug=largestloss_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=4, pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', predict_batch_size=16, replay_stream_json_path='', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', task_name='mrqa_naturalquestions', train_batch_size=8, use_sampled_upstream=False)", "online_eval_results": [{"timecode": 0, "before_eval": {"predictions": ["Raymond Briggs' 1978 children's book 'The Snowman", "the acceleration due to gravity decreased as an inverse square law", "a marquetry commode by the \u00e9b\u00e9niste Jean Henri Riesener dated c1780", "a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work", "lymphocyte", "Chinghiz", "Doctor Who and the Daleks in the Seven Keys to Doomsday", "Super Bowl 50 Host Committee", "a satirical television comedy programme on BBC Television in 1962 and 1963", "dynasty", "Br'er Rabbit", "Bodhi Natural Health Products", "Amphitrite Goddess of the Sea", "the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ), the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho", "March 14 ( 3 / 14 in the month / day format ) since 3, 1, and 4 are the first three significant digits of \u03c0", "the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing", "Horace Rumpole", "combustion", "A Sunday Afternoon on the Island of La Grande Jatte", "nobody knows for sure how a do-over in golf came to be called a mulligan", "Captain Meriwether Lewis's 30th birthday", "An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses", "2011", "electric eels", "before the first year begins", "New Jerusalem", "Samantha Spiro", "2013", "Ronnie Biggs", "Steve Carell as Felonious Gru, the former villain turned Anti-Villain League agent, Margo, Edith, and Agnes'adoptive father, and Lucy's husband", "an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "the assassination of US President John F. Kennedy"], "metric_results": {"EM": 0.0, "QA-F1": 0.0812722081839729}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.4799999999999999, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.11764705882352942, 0.15384615384615383]}}, "error_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-10322", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-2376", "mrqa_squad-validation-6677", "mrqa_squad-validation-6303", "mrqa_squad-validation-7821", "mrqa_squad-validation-392", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9688", "mrqa_triviaqa-validation-1551", "mrqa_squad-validation-3478", "mrqa_triviaqa-validation-5937", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-2730", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-4185", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3241", "mrqa_triviaqa-validation-7369", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-validation-7017", "mrqa_squad-validation-7746"], "after_eval": {"predictions": ["The Snowman", "at larger distances", "art and furnishings", "turbine", "adaptive", "Chinghiz, Chinghis, and Chingiz", "Doctor Who \u2013 The Ultimate Adventure", "50 fund", "ned sherrin", "the Mandate of Heaven", "Tar Baby", "buddha", "Poseidon", "the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho ( including the communities of Parma, Wilder, Greenleaf, and Notus )", "2009", "the direction from which the wind is blowing", "john Mortimer", "oxygen", "seurat", "golf", "the Missouri River", "An elevator with a counterbalance", "137th", "piranha", "the student's transition from the study of preclinical to clinical health sciences", "prague", "Geoffrey Hutchings", "1998", "great train robbery", "Trey Parker", "omen of good or bad luck", "power blackouts"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9674107142857142}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428572, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": [], "fixed_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-10322", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-2376", "mrqa_squad-validation-6677", "mrqa_squad-validation-6303", "mrqa_squad-validation-7821", "mrqa_squad-validation-392", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9688", "mrqa_triviaqa-validation-1551", "mrqa_squad-validation-3478", "mrqa_triviaqa-validation-5937", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-2730", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-4185", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3241", "mrqa_triviaqa-validation-7369", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-validation-7017", "mrqa_squad-validation-7746"], "unfixed_ids": ["mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-5465"], "instant_fixing_rate": 0.9375, "instant_retention_rate": 0.0}, {"timecode": 1, "before_eval": {"predictions": ["the town of Acolman, just north of Mexico City", "Iroquois", "philanthropy", "neded on Mar 31, 2007", "Queen Elizabeth II", "Gary Morris", "to the anterolateral corner of the spinal cord", "1966", "for scientific observation", "pronseal", "The Stock Market crash in New York", "New York Stadium", "john Bercow", "continental units", "john Forster", "Comptroller General of the Receipt and Issue of Her Majesty's Exchequer", "Sergio P\u00e9rez", "River Welland", "The Concubine", "1543", "naba", "Vigor", "a policeman who investigates a series of mysterious killings and illnesses", "glowed even when turned off", "Florence Nightingale", "Budweiser", "numb3rs show Summary", "great heroism or of the most conspicuous courage in circumstances of extreme danger", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Galileo", "the check written by DC Comics to Jerry Siegel and Joe Shuster for the exclusive rights to their then-new character, Superman", "May and June 2010"], "metric_results": {"EM": 0.15625, "QA-F1": 0.23364713309566248}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.23529411764705885, 0.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0, 0.5, 0.0, 0.4444444444444445, 1.0, 0.1904761904761905, 0.4]}}, "error_ids": ["mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-5144", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-5406", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-1924", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-validation-1364", "mrqa_hotpotqa-validation-14", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-3971", "mrqa_squad-validation-2629", "mrqa_triviaqa-validation-2210", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-5802", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-2096", "mrqa_naturalquestions-validation-1864", "mrqa_triviaqa-validation-2367", "mrqa_hotpotqa-validation-3774"], "after_eval": {"predictions": ["Europe", "originally designated HU - 1", "philanthropy", "mariette", "norman Wade", "Gary Morris", "usually occurs 1 - 2 spinal nerve segments above the point of entry", "current denomination of U.S. currency", "either small fission systems or radioactive decay for electricity or heat", "ronseal", "1929 - 32", "Rotherham United", "norman tebbit", "Pangaea or Pangea", "red", "The comptroller ( who is also auditor general and head of the National Audit Office )", "Pedro Rodr\u00edguez", "65 mi", "an obsessed and tormented king", "three years before his death", "islam", "Honda Ballade", "illnesses", "within 100 feet of the lab glowed even when turned off", "Sister Anthony, S.C.", "Stroh's", "alimi Ballard", "for gallantry", "Kathleen Turner", "Isaac Newton", "DC comics", "2010"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8177083333333333}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-4684", "before_prediction": "The Stock Market crash in New York", "after_prediction": "1929 - 32"}, {"id": "mrqa_squad-validation-1516", "before_prediction": "glowed even when turned off", "after_prediction": "within 100 feet of the lab glowed even when turned off"}, {"id": "mrqa_squad-validation-10410", "before_prediction": "Galileo", "after_prediction": "Isaac Newton"}], "retained_ids": ["mrqa_squad-validation-10015", "mrqa_hotpotqa-validation-5899"], "fixed_ids": ["mrqa_naturalquestions-validation-5144", "mrqa_triviaqa-validation-7018", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-1924", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-validation-1364", "mrqa_hotpotqa-validation-14", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-3971", "mrqa_squad-validation-2629", "mrqa_triviaqa-validation-2210", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-5802", "mrqa_triviaqa-validation-2096", "mrqa_naturalquestions-validation-1864", "mrqa_hotpotqa-validation-3774"], "unfixed_ids": ["mrqa_naturalquestions-validation-10680", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-2367"], "instant_fixing_rate": 0.8518518518518519, "instant_retention_rate": 0.3999999992}, {"timecode": 2, "before_eval": {"predictions": ["MS Kronprins Harald", "boxing", "Puritanism", "+, -, *, and / keys", "2009", "in different parts of the globe", "Wales", "acetate", "John II Casimir Vasa", "james darnley", "A55", "phylum with relatively few species", "`` Everywhere '' is a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "the majority of the population lives in the suburbs", "second year", "Bothtec", "Terry Reid", "not carry out research nor does it monitor climate related data", "\ufffdEnigma\u2019 Variations", "North America", "Andr\u00e9 3000", "Commander", "Akhenaten", "Theodore Roosevelt", "the fourth season", "four", "the Western Bloc ( the United States, its NATO allies and others )", "1970s", "Op\u00e9ra-Comique", "Matt Winer", "1689", "Pacific"], "metric_results": {"EM": 0.125, "QA-F1": 0.25847513956350165}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true], "QA-F1": [0.0, 0.25, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.1, 0.20689655172413793, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 0.4444444444444445, 0.4, 0.0, 0.0, 0.5, 0.0, 1.0, 0.3636363636363636, 0.0, 0.0, 0.4, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-8542", "mrqa_squad-validation-7149", "mrqa_naturalquestions-validation-10364", "mrqa_squad-validation-4253", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-validation-1202", "mrqa_hotpotqa-validation-3632", "mrqa_triviaqa-validation-1935", "mrqa_squad-validation-4456", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-8448", "mrqa_triviaqa-validation-2136", "mrqa_naturalquestions-validation-539", "mrqa_squad-validation-8513", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-validation-5502", "mrqa_hotpotqa-validation-2679", "mrqa_squad-validation-4019", "mrqa_naturalquestions-validation-6896", "mrqa_naturalquestions-validation-2501", "mrqa_hotpotqa-validation-1376", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-683", "mrqa_triviaqa-validation-2722", "mrqa_hotpotqa-validation-4367", "mrqa_squad-validation-3113"], "after_eval": {"predictions": ["motor ships", "cricket, rallying, football, rugby union and boxing", "never", "*", "2003", "eleven separate regions of the Old and New World", "idris", "polyatomic anion", "Jan Kazimierz", "casket letters", "A55 North Wales Expressway", "oceanic species", "Everywhere", "Many residents of metropolitan regions work within the central urban area, and choose to live in satellite communities called suburbs and commute to work", "second year", "Bothtec", "Spencer Davis Group", "non-peer-reviewed sources", "Enigma Variations", "physiographically a part of the continent of North America", "OutKast", "two rookies", "the Aten, a representation of the Egyptian god, Ra", "Monroe Doctrine", "2010 to 2012", "four", "the United States", "in the very late 1980s", "bizet", "Matthew Ward Winer", "1700", "Pacific"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9083333333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-1888", "before_prediction": "A55", "after_prediction": "A55 North Wales Expressway"}], "retained_ids": ["mrqa_hotpotqa-validation-3242", "mrqa_squad-validation-194", "mrqa_squad-validation-4283"], "fixed_ids": ["mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-8542", "mrqa_squad-validation-7149", "mrqa_naturalquestions-validation-10364", "mrqa_squad-validation-4253", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-validation-1202", "mrqa_hotpotqa-validation-3632", "mrqa_triviaqa-validation-1935", "mrqa_squad-validation-4456", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-8448", "mrqa_naturalquestions-validation-539", "mrqa_squad-validation-8513", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-validation-5502", "mrqa_squad-validation-4019", "mrqa_naturalquestions-validation-6896", "mrqa_naturalquestions-validation-2501", "mrqa_hotpotqa-validation-1376", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-683", "mrqa_triviaqa-validation-2722", "mrqa_hotpotqa-validation-4367", "mrqa_squad-validation-3113"], "unfixed_ids": ["mrqa_triviaqa-validation-2136", "mrqa_hotpotqa-validation-2679"], "instant_fixing_rate": 0.9285714285714286, "instant_retention_rate": 0.7499999981250001}, {"timecode": 3, "before_eval": {"predictions": ["idia", "baijan", "180 degrees", "between 27 July and 7 August 2022", "New York", "tony hanks", "2006", "Least of the Great Powers", "usually restricted to the lower motor neurons, the efferent nerves that directly innervate muscles", "surtsey", "Fox's Glacier Mints", "Fred Archer", "death mask", "bollywood", "Overtime", "Sir Henry Cole", "trouble distinguishing between carbon dioxide and oxygen", "rik Mayall", "1910", "Democratic Unionist Party", "23 July 1989", "many educational institutions especially within the US", "often exercising a great deal of control over the lives of their disciples", "for control purposes", "stan", "callable bonds", "2.26 GHz quad - core Snapdragon 800 processor", "over 10,000", "al - khimar", "proteins were more complex than DNA", "gallbladder", "berenice Abbott"], "metric_results": {"EM": 0.15625, "QA-F1": 0.19458411654135338}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.10526315789473685, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-5362", "mrqa_naturalquestions-validation-5647", "mrqa_squad-validation-3058", "mrqa_triviaqa-validation-7382", "mrqa_squad-validation-7816", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-2571", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2327", "mrqa_triviaqa-validation-365", "mrqa_squad-validation-1539", "mrqa_triviaqa-validation-3901", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-3189", "mrqa_squad-validation-8832", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-2150", "mrqa_squad-validation-2191", "mrqa_squad-validation-2069", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-6402", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-5818", "mrqa_triviaqa-validation-2530"], "after_eval": {"predictions": ["period", "qpr", "360", "2022", "Staten Island", "splash", "2005\u20132010", "G20", "efferent nerves", "max bygraves", "polar bear", "lester piggott", "don't disturb\" sign", "nigeria", "the shootout", "commissioned by Sir Henry Cole and illustrated by John Callcott Horsley in London on 1st May 1843", "at high oxygen concentrations, rubisco starts accidentally adding oxygen to sugar precursors", "adrian edmondson", "1934", "Northern Ireland Assembly for Fermanagh and South Tyrone", "1973", "US", "the West", "data", "ringo starr", "callable bonds", "2.26 GHz quad - core Snapdragon 800 processor", "over 10,000 British and 2,000 old master works", "hijab", "The results of the Avery -- MacLeod -- McCarty experiment", "gallbladder", "museums"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9366071428571429}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-5662", "before_prediction": "23 July 1989", "after_prediction": "1973"}, {"id": "mrqa_squad-validation-5517", "before_prediction": "over 10,000", "after_prediction": "over 10,000 British and 2,000 old master works"}], "retained_ids": ["mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-6341", "mrqa_triviaqa-validation-6800"], "fixed_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-5362", "mrqa_naturalquestions-validation-5647", "mrqa_squad-validation-3058", "mrqa_triviaqa-validation-7382", "mrqa_squad-validation-7816", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-2571", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2327", "mrqa_triviaqa-validation-365", "mrqa_triviaqa-validation-3901", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-3189", "mrqa_squad-validation-8832", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-2150", "mrqa_squad-validation-2191", "mrqa_squad-validation-2069", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-6402", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-5818", "mrqa_triviaqa-validation-2530"], "unfixed_ids": ["mrqa_squad-validation-1539"], "instant_fixing_rate": 0.9629629629629629, "instant_retention_rate": 0.5999999988}, {"timecode": 4, "before_eval": {"predictions": ["Austria", "at Nijmegen, over the Waal distributary of the Rhine", "December 9, 2016", "NASA discontinued the manned Block I program, using the Block I spacecraft only for unmanned Saturn V flights", "British progressive folk-rock band Gryphon", "1898", "Januarius", "a shepherd", "at elevation 2 meters above sea level", "paris", "bounding the time or space used by the algorithm", "facets", "Steve McGarrett", "Eddie Leonski", "Lynwood", "a mixture of phencyclidine and cocaine", "bunker", "Fleet Street", "Professor Eobard Thawne", "All Souls'Day", "kansas city", "Baku", "new converts", "Mona Vanderwaal", "cricket", "Pyotr Ilyich Tchaikovsky", "2001 or 2010", "the English colonies of North America, and Quebec", "speech or language", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "turbine rotor", "Splodgenessabounds"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4855654761904762}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false, true, false, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.4, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 1.0, 1.0, 0.4, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.28571428571428575, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-9358", "mrqa_naturalquestions-validation-5146", "mrqa_squad-validation-3971", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-3647", "mrqa_triviaqa-validation-7253", "mrqa_squad-validation-1688", "mrqa_hotpotqa-validation-1289", "mrqa_naturalquestions-validation-5437", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-5654", "mrqa_naturalquestions-validation-2900", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-3978", "mrqa_squad-validation-3021", "mrqa_naturalquestions-validation-3840", "mrqa_squad-validation-3467"], "after_eval": {"predictions": ["Austria", "Arnhem", "30 days after the original air date", "unmanned Saturn V flights", "Gryphon", "1898", "june", "aaron", "new york city", "tetanus", "bounding", "facets", "Lieutenant Commander Steve McGarrett", "Edward Joseph Leonski", "Lynwood", "cocaine", "a chain or screw stoking mechanism", "Fleet Street", "Reverse - Flash", "All Saints ( or All Hallows )", "kansas city Royals", "azerbaijan", "new converts", "CeCe Drake", "cricket", "Tania Miller", "8 April 1912", "Quebec", "comprehend and formulate language", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "rotating discs", "Splodgenessabounds"], "metric_results": {"EM": 0.875, "QA-F1": 0.9208333333333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-1168", "before_prediction": "Steve McGarrett", "after_prediction": "Lieutenant Commander Steve McGarrett"}, {"id": "mrqa_squad-validation-3389", "before_prediction": "bunker", "after_prediction": "a chain or screw stoking mechanism"}, {"id": "mrqa_naturalquestions-validation-3033", "before_prediction": "Professor Eobard Thawne", "after_prediction": "Reverse - Flash"}, {"id": "mrqa_triviaqa-validation-93", "before_prediction": "kansas city", "after_prediction": "kansas city Royals"}], "retained_ids": ["mrqa_squad-validation-6399", "mrqa_naturalquestions-validation-1277", "mrqa_triviaqa-validation-3808", "mrqa_hotpotqa-validation-2944", "mrqa_triviaqa-validation-1575", "mrqa_squad-validation-3126", "mrqa_triviaqa-validation-5168", "mrqa_squad-validation-8700", "mrqa_naturalquestions-validation-220"], "fixed_ids": ["mrqa_squad-validation-9358", "mrqa_naturalquestions-validation-5146", "mrqa_squad-validation-3971", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-3647", "mrqa_triviaqa-validation-7253", "mrqa_squad-validation-1688", "mrqa_hotpotqa-validation-1289", "mrqa_naturalquestions-validation-5437", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-5654", "mrqa_naturalquestions-validation-2900", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-3978", "mrqa_squad-validation-3021", "mrqa_naturalquestions-validation-3840", "mrqa_squad-validation-3467"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.6923076917751478}, {"timecode": 5, "before_eval": {"predictions": ["Horace Walpole", "F\u00e9d\u00e9ration Sportive F\u00e9minine Internationale (FSFI )", "paisley", "Paspahegh Indians", "a delay or obstruction", "South Dakota", "7 : 25 a.m.", "swannee whistle", "rape oil", "used to start fires, hunt, and bury their dead", "Indian origin", "Parietal cells ( also known as oxyntic or delomorphous cells )", "placental", "Ready to Die", "president Garfield", "imperial rule", "1840", "make a defiant speech, or a speech explaining their actions", "George Sylvester Viereck", "rich folks", "a new tendency to take on debts", "entropy increases", "my mind is averse to wedlock because I daily expect the death of a heretic", "8.7 -- 9.2", "People's Republic of China", "2 November 1902", "southeastern Texas", "May 7, 2018", "8 December 1980", "Selden", "structural collapses", "king minos"], "metric_results": {"EM": 0.125, "QA-F1": 0.22199754901960783}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.47058823529411764, 0.0, 0.4, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2197", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-2020", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3625", "mrqa_naturalquestions-validation-4513", "mrqa_triviaqa-validation-1550", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-4621", "mrqa_squad-validation-6733", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-254", "mrqa_squad-validation-7554", "mrqa_squad-validation-10423", "mrqa_squad-validation-2757", "mrqa_naturalquestions-validation-2884", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-1489", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-5848", "mrqa_squad-validation-2531", "mrqa_triviaqa-validation-4560"], "after_eval": {"predictions": ["Horace Walpole", "International Association of Athletics Federations", "picisley", "uninhabited", "cardiac", "Idaho", "6 : 44 p.m. UTC", "clangers", "mustard", "hunt", "Indian", "gastric glands found in the lining of the fundus and in the body of the stomach", "monotremes", "September 13, 1994", "July 2, 1881", "Ming dynasty", "1787", "defiant speech", "Mark Twain", "sunny afternoon", "basis of the methodology used", "nonconservative forces", "death of a heretic", "The 1700 Cascadia earthquake", "China", "11 November 1869", "near Arenosa Creek and Matagorda Bay", "January 15, 2018", "19408", "Brookhaven", "property damage", "daedalus"], "metric_results": {"EM": 0.875, "QA-F1": 0.8958333333333333}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-2368", "before_prediction": "paisley", "after_prediction": "picisley"}, {"id": "mrqa_triviaqa-validation-5704", "before_prediction": "president Garfield", "after_prediction": "July 2, 1881"}, {"id": "mrqa_naturalquestions-validation-816", "before_prediction": "imperial rule", "after_prediction": "Ming dynasty"}], "retained_ids": ["mrqa_triviaqa-validation-4725"], "fixed_ids": ["mrqa_hotpotqa-validation-2197", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-validation-4506", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3625", "mrqa_naturalquestions-validation-4513", "mrqa_triviaqa-validation-1550", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-4621", "mrqa_squad-validation-6733", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-254", "mrqa_squad-validation-7554", "mrqa_squad-validation-10423", "mrqa_squad-validation-2757", "mrqa_naturalquestions-validation-2884", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-1489", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-5848", "mrqa_squad-validation-2531", "mrqa_triviaqa-validation-4560"], "unfixed_ids": ["mrqa_naturalquestions-validation-2020"], "instant_fixing_rate": 0.9642857142857143, "instant_retention_rate": 0.249999999375}, {"timecode": 6, "before_eval": {"predictions": ["Zinnemann", "The glass chimney needs a `` throat '', or slight constriction, to create the proper draft for complete combustion of the fuel", "Illinois", "2010", "North Carolina", "The Port of Mahon, located on the east coast of the island of Menorca", "90-60's", "unaided independent school", "dolph Camilli", "times sign", "The show has received recognition as one of Britain's finest television programmes, winning the 2006 British Academy Television Award for Best Drama Series and five consecutive ( 2005\u20132010) awards at the National Television Awards", "Emily Blunt", "Super Bowl LII", "HTTP Secure ( HTTPS )", "typically closes for two and half weeks in late summer so it can be converted into the Haunted Mansion Holiday", "The Chisholm trail", "monatomic", "Palm Springs is popular for its resort feel and nearby open spaces", "Shinto", "birds & worms", "butterflies and moths", "transfusion", "left coronary artery", "1.1 \u00d7 1011 metric tonnes", "dale", "leaf", "Indian club ATK", "land that a nation has conquered and expanded", "Indian Ocean", "\u20b9 \u200d5L '' ( for `` rupees 5 lakhs '' )", "Germanic", "burning of fossil fuels"], "metric_results": {"EM": 0.125, "QA-F1": 0.17455018939393938}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.5, 0.0, 0.0, 0.0606060606060606, 0.0, 1.0, 0.4, 0.08333333333333333, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-4165", "mrqa_hotpotqa-validation-5810", "mrqa_triviaqa-validation-5261", "mrqa_squad-validation-2659", "mrqa_squad-validation-6947", "mrqa_triviaqa-validation-105", "mrqa_naturalquestions-validation-10356", "mrqa_squad-validation-7819", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-226", "mrqa_triviaqa-validation-5795", "mrqa_squad-validation-3463", "mrqa_squad-validation-2584", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-227", "mrqa_squad-validation-4181", "mrqa_triviaqa-validation-4966", "mrqa_squad-validation-8821", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-10042", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6207", "mrqa_hotpotqa-validation-3919", "mrqa_naturalquestions-validation-6644"], "after_eval": {"predictions": ["film director", "capillary action", "2008", "2010", "n Carolina", "menorca", "70-50", "independent schools", "boston braves", "the symbol \u00d7", "Best Supporting Actress", "Juice Newton", "1960", "Hypertext Transfer Protocol ( HTTP ) for secure communication", "late - September through early January", "wichita", "simplest", "for its popular beaches, and the desert city of Palm Springs is popular for its resort feel and nearby open spaces", "japan", "true", "red admiral", "o", "left coronary artery", "10 percent of the carbon stores in ecosystems", "buttermere", "leaves", "Republic of Ireland national team", "distinction", "23 November 1996", "lakh", "Norwegian language", "Carbon dioxide"], "metric_results": {"EM": 0.84375, "QA-F1": 0.8925189393939394}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-6887", "before_prediction": "North Carolina", "after_prediction": "n Carolina"}, {"id": "mrqa_naturalquestions-validation-2190", "before_prediction": "Super Bowl LII", "after_prediction": "1960"}], "retained_ids": ["mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-5582"], "fixed_ids": ["mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-4165", "mrqa_hotpotqa-validation-5810", "mrqa_triviaqa-validation-5261", "mrqa_squad-validation-6947", "mrqa_triviaqa-validation-105", "mrqa_naturalquestions-validation-10356", "mrqa_squad-validation-7819", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-226", "mrqa_triviaqa-validation-5795", "mrqa_squad-validation-3463", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-227", "mrqa_triviaqa-validation-4966", "mrqa_squad-validation-8821", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-10042", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6207", "mrqa_hotpotqa-validation-3919", "mrqa_naturalquestions-validation-6644"], "unfixed_ids": ["mrqa_squad-validation-2659", "mrqa_squad-validation-2584", "mrqa_squad-validation-4181"], "instant_fixing_rate": 0.8928571428571429, "instant_retention_rate": 0.49999999875}, {"timecode": 7, "before_eval": {"predictions": ["the wisdom and prudence of certain decisions of procurement", "Maya group", "a few common complex biomolecules, such as squalene and the carotenes", "The U.S. Army Chaplain insignia is the only authorized army branch insignia to be worn on the ACU", "Kairi", "both inner city blacks, who wanted more involvement in government, and whites in the suburbs, who want more services and more control over the central city", "boston Becks", "Armenia and Azerbaijan", "the last book accepted into the Christian biblical canon", "Bruno Mars", "buttermens per meter", "George Cross", "16 million", "1950s", "cattle are slaughtered for meat before the age of three years, except where they are needed ( castrated ) as work oxen for haulage", "1998", "a priest", "second most common component of the Earth's atmosphere", "18 - season", "family member", "over-fishing and long-term environmental changes", "Bill Lear", "The unbalanced centripetal force felt by any object is always directed toward the center of the curving path", "Terrell Suggs", "decide on all the motions and amendments that have been moved that day", "a voyage of adventure", "James Hutton", "casket letters", "present-day Charleston", "\"quiescent\" stance", "Adam Karpel", "German"], "metric_results": {"EM": 0.15625, "QA-F1": 0.27329452614379085}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.19999999999999998, 0.0, 0.25, 0.0, 0.0, 0.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 0.25, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5522", "mrqa_squad-validation-3625", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-983", "mrqa_squad-validation-7296", "mrqa_triviaqa-validation-7415", "mrqa_squad-validation-6297", "mrqa_naturalquestions-validation-859", "mrqa_squad-validation-108", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-1293", "mrqa_squad-validation-7799", "mrqa_hotpotqa-validation-3898", "mrqa_naturalquestions-validation-98", "mrqa_squad-validation-358", "mrqa_squad-validation-3558", "mrqa_naturalquestions-validation-824", "mrqa_hotpotqa-validation-1142", "mrqa_squad-validation-10403", "mrqa_naturalquestions-validation-3474", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1301", "mrqa_triviaqa-validation-781", "mrqa_squad-validation-3181", "mrqa_squad-validation-9598", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-3146"], "after_eval": {"predictions": ["procurement of Armoured Personnel Carriers", "K'iche '", "a few", "the right", "Kairi in the video game series \" Kingdom Hearts\".", "public high schools lost their accreditation", "lost weekend", "near the Black Sea", "last book", "Beyonc\u00e9 and Bruno Mars", "conductivity", "george vi", "the most popular show at the time", "post\u2013World War II", "breeding", "2011", "a priest", "most abundant", "2001", "family member, or by anyone with knowledge or skills in the wider community setting", "over-fishing and long-term environmental changes", "8-track", "tangential force", "Mike Czerwien, Waynesburg University, 2002 -- 04", "vote", "a maritime signal, indicating that the vessel flying it is about to leave", "James Hutton", "george i", "the Charleston Orange district", "quiescent", "Andy Cohen", "Panzer"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9554487179487179}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-8374", "before_prediction": "the wisdom and prudence of certain decisions of procurement", "after_prediction": "procurement of Armoured Personnel Carriers"}, {"id": "mrqa_squad-validation-1863", "before_prediction": "family member", "after_prediction": "family member, or by anyone with knowledge or skills in the wider community setting"}], "retained_ids": ["mrqa_hotpotqa-validation-3846", "mrqa_squad-validation-4318", "mrqa_naturalquestions-validation-291"], "fixed_ids": ["mrqa_naturalquestions-validation-5522", "mrqa_squad-validation-3625", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-983", "mrqa_squad-validation-7296", "mrqa_triviaqa-validation-7415", "mrqa_squad-validation-6297", "mrqa_naturalquestions-validation-859", "mrqa_squad-validation-108", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-1293", "mrqa_squad-validation-7799", "mrqa_hotpotqa-validation-3898", "mrqa_naturalquestions-validation-98", "mrqa_squad-validation-358", "mrqa_squad-validation-3558", "mrqa_naturalquestions-validation-824", "mrqa_hotpotqa-validation-1142", "mrqa_squad-validation-10403", "mrqa_naturalquestions-validation-3474", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1301", "mrqa_triviaqa-validation-781", "mrqa_squad-validation-3181", "mrqa_squad-validation-9598", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-3146"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.5999999988}, {"timecode": 8, "before_eval": {"predictions": ["computational complexity theory", "Formula One", "gold in the half - pipe", "5 nanometers across, arranged in rows 6.4 nanometers apart", "tenth and eleventh", "Brad Keselowski", "over 400 games", "kidneys", "`` liberal arts '' or `` liberal pursuits '' ( Latin liberalia studia )", "\"Switzerland of England\"", "Edward IV of England and Elizabeth Woodville", "St. Louis County, Missouri, United States", "1918", "2018", "bramall lane", "law firm", "Pottawatomie County", "orangutan", "a correction", "The church tower", "bromley- by-Bow", "Toronto, Ontario, Canada", "slow", "110 miles (177 km)", "Kona coast", "Liberal conservatism", "gold rushes", "six", "not guilty", "psychotherapeutic", "discovers and attends a college of magic in New York", "acidic bogs"], "metric_results": {"EM": 0.28125, "QA-F1": 0.4398989898989899}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false, false, false, false], "QA-F1": [0.4, 1.0, 0.888888888888889, 0.3636363636363636, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.7272727272727273, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.6666666666666666]}}, "error_ids": ["mrqa_squad-validation-1705", "mrqa_naturalquestions-validation-6089", "mrqa_squad-validation-8869", "mrqa_naturalquestions-validation-856", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-3789", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5401", "mrqa_triviaqa-validation-7157", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-validation-3309", "mrqa_squad-validation-10369", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-1021", "mrqa_hotpotqa-validation-4812", "mrqa_squad-validation-6915", "mrqa_triviaqa-validation-7767", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-187"], "after_eval": {"predictions": ["computability theory", "giuseppe Antonio 'Nino'Farina", "won gold in the half - pipe", "6.4 nanometers", "departing the show to star in CBS's upcoming sci - fi drama Intelligence", "Kyle Busch", "400", "kidneys", "artes liberales", "Switzerland of England", "Edward IV of England", "Eureka", "1828", "2018", "blades", "to ensure wide visibility and understanding of cases in a region", "Shawnee", "tortoise", "theory of general relativity", "The church tower", "walford east", "Montreal", "slow", "110 miles", "Kona coast of the island of Hawai\u02bb i about 12 mi south of Kailua-Kona", "liberal conservative", "gold rushes", "six degrees of freedom", "creative plea", "freudian", "New York", "acidic"], "metric_results": {"EM": 0.875, "QA-F1": 0.9230072463768116}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.8695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-1521", "before_prediction": "Formula One", "after_prediction": "giuseppe Antonio 'Nino'Farina"}, {"id": "mrqa_hotpotqa-validation-5117", "before_prediction": "Kona coast", "after_prediction": "Kona coast of the island of Hawai\u02bb i about 12 mi south of Kailua-Kona"}, {"id": "mrqa_naturalquestions-validation-7906", "before_prediction": "six", "after_prediction": "six degrees of freedom"}], "retained_ids": ["mrqa_triviaqa-validation-7506", "mrqa_hotpotqa-validation-3161", "mrqa_naturalquestions-validation-6991", "mrqa_squad-validation-5313", "mrqa_triviaqa-validation-4268", "mrqa_squad-validation-2987"], "fixed_ids": ["mrqa_squad-validation-1705", "mrqa_naturalquestions-validation-6089", "mrqa_squad-validation-8869", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-3789", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5401", "mrqa_triviaqa-validation-7157", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-validation-3309", "mrqa_squad-validation-10369", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-1021", "mrqa_hotpotqa-validation-4812", "mrqa_squad-validation-6915", "mrqa_triviaqa-validation-7767", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-187"], "unfixed_ids": ["mrqa_naturalquestions-validation-856"], "instant_fixing_rate": 0.9565217391304348, "instant_retention_rate": 0.6666666659259258}, {"timecode": 9, "before_eval": {"predictions": ["I Seek You", "Argentinian", "a report, published in early February 2007 by the Ear Institute at the University College London, and Widex, a Danish hearing aid manufacturer", "siciliana", "photosynthesis", "to celebrate Queen Victoria's diamond jubilee", "sweden", "The Daily Stormer", "antibonding", "water", "president", "the citizens would elect almost all officeholders annually", "George, Margrave of Brandenburg-Ansbach", "Kamba version", "3D computer-animated comedy", "Bloomingdale Firehouse", "to pursue an acting career", "Sue Taylor Grafton", "illuminated display", "The North Bridge skirmish", "IPod Classic", "My Sassy Girl", "prevent damage to the body", "General Hospital", "Highly combustible materials that leave little residue, such as wood or coal, were thought to be made mostly of phlogiston", "pedagogy", "\u03b2\u03b1\u03c3\u03ba\u03b1\u03bd\u03af\u03b1", "ATP", "all land - living organisms, both alive and dead, as well as carbon stored in soils", "m Melbourne", "medium and heavy-duty diesel trucks", "testes"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3298324938949939}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false], "QA-F1": [0.0, 1.0, 0.1, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.13333333333333333, 0.4, 0.25, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.19999999999999998, 0.0, 0.0, 0.3333333333333333, 0.15384615384615383, 0.0, 0.09523809523809523, 0.0, 0.6666666666666666, 1.0, 0.2222222222222222, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6844", "mrqa_squad-validation-5210", "mrqa_triviaqa-validation-3387", "mrqa_squad-validation-8924", "mrqa_squad-validation-5618", "mrqa_triviaqa-validation-2703", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-4520", "mrqa_squad-validation-8259", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-5128", "mrqa_hotpotqa-validation-133", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-1327", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1436", "mrqa_naturalquestions-validation-1294", "mrqa_squad-validation-5940", "mrqa_squad-validation-3490", "mrqa_squad-validation-1879", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-2449", "mrqa_naturalquestions-validation-3677"], "after_eval": {"predictions": ["an instant messaging client", "an Argentinian professional tennis player", "a report", "cake", "consumes ATP and oxygen, releases CO2, and produces no sugar", "different animals and humans performimg various actions", "1600 Pennsylvania Avenue", "\"The Krypto Report\" a podcast produced by the white supremacist site \"The Daily Stormer\"", "antibonding", "shortcrust pastry ; however, more recently recipes have recommended a paste consisting of flour and water", "the President of the United States", "citizens", "George, Margrave of Brandenburg-Ansbach", "a very precise notation of a correct African pronunciation", "3D computer-animated comedy film", "Worcester Cold Storage and Warehouse Co. fire", "an acting career", "C. W. Grafton", "a liquid crystal on silicon ( LCoS ) ( based on an L coS chip from Himax ), field - sequential color system, LED illuminated display", "Americans acting under orders", "iPod+HP", "\"It Bizarre Girl\"", "removes excess, unnecessary materials from the body fluids of an organism", "The Edge of Night", "phlogiston", "field trips", "vaskania ( \u03b2\u03b1\u03c3\u03ba\u03b1\u03bd\u03af\u03b1 )", "ATP, generated by the root respiration", "plants", "1987", "medium and heavy-duty diesel trucks", "Reproductive system"], "metric_results": {"EM": 0.78125, "QA-F1": 0.9159185971685972}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.8, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.918918918918919, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-832", "before_prediction": "Argentinian", "after_prediction": "an Argentinian professional tennis player"}, {"id": "mrqa_hotpotqa-validation-3428", "before_prediction": "The Daily Stormer", "after_prediction": "\"The Krypto Report\" a podcast produced by the white supremacist site \"The Daily Stormer\""}, {"id": "mrqa_hotpotqa-validation-2673", "before_prediction": "3D computer-animated comedy", "after_prediction": "3D computer-animated comedy film"}, {"id": "mrqa_naturalquestions-validation-1682", "before_prediction": "ATP", "after_prediction": "ATP, generated by the root respiration"}], "retained_ids": ["mrqa_squad-validation-3442", "mrqa_squad-validation-2582", "mrqa_hotpotqa-validation-5823"], "fixed_ids": ["mrqa_naturalquestions-validation-6844", "mrqa_squad-validation-5210", "mrqa_triviaqa-validation-3387", "mrqa_squad-validation-8924", "mrqa_squad-validation-5618", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-4520", "mrqa_squad-validation-8259", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-5128", "mrqa_hotpotqa-validation-133", "mrqa_naturalquestions-validation-1327", "mrqa_hotpotqa-validation-3046", "mrqa_naturalquestions-validation-1294", "mrqa_squad-validation-5940", "mrqa_squad-validation-3490", "mrqa_squad-validation-1879", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-2449", "mrqa_naturalquestions-validation-3677"], "unfixed_ids": ["mrqa_triviaqa-validation-2703", "mrqa_naturalquestions-validation-754", "mrqa_hotpotqa-validation-1436"], "instant_fixing_rate": 0.88, "instant_retention_rate": 0.4285714279591837}, {"timecode": 10, "before_eval": {"predictions": ["philadelphia", "hump and Hampton's line", "three legal systems", "Summerlin", "HTTP / 1.1 200 OK", "globetrotters", "cruiserweight", "Manoir de la Fi\u00e8re and Chef - du - Pont", "1926", "1862", "caarnac", "Victoria, Duchess of Kent", "10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "Kon-Tiki", "Dream Within a Dream Tour", "Chicago History Museum", "Ronnie Hillman", "a single all-encompassing definition of the term is extremely difficult, if not impossible", "7 November 2016", "60", "Eagle Ridge Mall", "1860 Munich", "reduce pressure on the public food supply", "Monastir / Tunisia / Africa", "fire", "Gomer Pyle", "at least 18 or 21 years old ( or have a legal guardian present ), and must sign a waiver prior to shooting", "Ward", "poet", "Jamestown", "Claude Monet", "tree growth stages"], "metric_results": {"EM": 0.125, "QA-F1": 0.19965277777777776}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1860", "mrqa_hotpotqa-validation-3149", "mrqa_naturalquestions-validation-1085", "mrqa_hotpotqa-validation-132", "mrqa_naturalquestions-validation-8006", "mrqa_hotpotqa-validation-4162", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-6389", "mrqa_triviaqa-validation-4791", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-10205", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-5682", "mrqa_triviaqa-validation-681", "mrqa_squad-validation-273", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-6119", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-1553", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-1451", "mrqa_hotpotqa-validation-3976", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-4803", "mrqa_squad-validation-4506"], "after_eval": {"predictions": ["Alexander", "Hampton's hump and Hampton's line", "English law", "Las Vegas", "A status line", "the best known globetrotters", "British and Commonwealth light-heavyweight titles between 2010 and 2014", "over the Merderet in the fictional town of Ramelle", "mussolini", "victor hugo", "menhirs", "British Royal Family", "greater than 14", "thor heyerdahl", "Grand Garden Special Events Center", "Valentino Garavani", "C. J. Anderson", "impossible", "joseph smith", "more than 60 percent of the state's total land surface", "Winter Haven Mall", "Pel\u00e9", "aiding the war effort", "tunisia", "the classical element fire", "Barney Fife", "Typically, no", "Ann", "writer", "Virginia", "Rouen Cathedral", "carbon related emissions"], "metric_results": {"EM": 0.875, "QA-F1": 0.9020833333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-2016", "before_prediction": "cruiserweight", "after_prediction": "British and Commonwealth light-heavyweight titles between 2010 and 2014"}, {"id": "mrqa_squad-validation-3018", "before_prediction": "60", "after_prediction": "more than 60 percent of the state's total land surface"}, {"id": "mrqa_triviaqa-validation-6639", "before_prediction": "Claude Monet", "after_prediction": "Rouen Cathedral"}], "retained_ids": ["mrqa_squad-validation-3525"], "fixed_ids": ["mrqa_triviaqa-validation-1860", "mrqa_hotpotqa-validation-3149", "mrqa_naturalquestions-validation-1085", "mrqa_hotpotqa-validation-132", "mrqa_naturalquestions-validation-8006", "mrqa_hotpotqa-validation-4162", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-6389", "mrqa_triviaqa-validation-4791", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-10205", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-5682", "mrqa_squad-validation-273", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-6119", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-1553", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-1451", "mrqa_hotpotqa-validation-3976", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-4803", "mrqa_squad-validation-4506"], "unfixed_ids": ["mrqa_triviaqa-validation-681"], "instant_fixing_rate": 0.9642857142857143, "instant_retention_rate": 0.249999999375}, {"timecode": 11, "before_eval": {"predictions": ["1967", "Traumnovelle", "women not taking jobs due to marriage or pregnancy", "Treaty on the Functioning of the European Union", "1 atm pressure following a change introduced in 1743 by Jean - Pierre Christin to reverse the Celsius thermometer scale ( from water boiling at 0 degrees and ice melting at 100 degrees )", "the narrator driving a truck owned by his brother, who died in action in the United States Army", "joseph smith", "The world's longest suspension bridges are listed according to the length of their main span", "Dublin", "cricket bat", "cat food", "the Bulgars, and especially the Seljuk Turks", "died in battle", "car", "joseph smith", "Caribbean, the Central American and the South American countries", "British opera house", "infection, irritation, or allergies", "The tower is the most - visited paid monument in the world", "Town House Galleria", "catfish aquaculture", "atomic number 53", "John Michael Higgins as John Smith, an a cappella commentator making an insulting documentary about The Bellas", "war waged by coalition forces from 34 nations led by the United States against Iraq in response to Iraq's invasion and annexation of Kuwait", "a co-op of grape growers", "victor willsmeron 24 January 2009", "verdi", "1952", "the Charlotte Hornets", "advisory speed signs are classified as warning signs, not regulatory signs", "Jean F kernel ( 1497 -- 1558 ), a French physician", "the back of the head"], "metric_results": {"EM": 0.21875, "QA-F1": 0.36473605285645727}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.7272727272727272, 0.0625, 0.23529411764705882, 0.0, 0.14285714285714288, 1.0, 0.0, 0.0, 0.9090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.5454545454545454, 0.28571428571428575, 0.4, 0.0, 0.14285714285714288, 0.15384615384615383, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.22222222222222224, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-5078", "mrqa_squad-validation-4169", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-5248", "mrqa_squad-validation-1003", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-validation-441", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-5184", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-3208", "mrqa_triviaqa-validation-7703", "mrqa_hotpotqa-validation-3921", "mrqa_triviaqa-validation-792", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5769"], "after_eval": {"predictions": ["Green Bay Packers", "Traumnovelle", "Gender pay gap", "Treaty on European Union (TEU)", "100 \u00b0 C", "his brother", "eat porridge", "span", "ferdinand", "duke of edinburgh", "king crimson", "Seljuk Turks", "alamo", "ferdinand porsche", "arkansas", "Canada", "Britten", "infection", "visited paid monument", "Galleria Vittorio Emanuele II", "farm - raised catfish", "heaviest of the stable halogens", "Saddle Up", "kuwaitis", "An agricultural cooperative", "norway", "verdi", "1952", "Charlotte Hornets of the National Basketball Association ( NBA )", "advisory speed signs are classified as warning signs, not regulatory signs", "Jean Fernel ( 1497 -- 1558 ), a French physician", "back of the head"], "metric_results": {"EM": 0.875, "QA-F1": 0.890625}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-7447", "before_prediction": "women not taking jobs due to marriage or pregnancy", "after_prediction": "Gender pay gap"}, {"id": "mrqa_triviaqa-validation-6692", "before_prediction": "Dublin", "after_prediction": "ferdinand"}], "retained_ids": ["mrqa_hotpotqa-validation-2852", "mrqa_triviaqa-validation-5526", "mrqa_hotpotqa-validation-4274", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-6442"], "fixed_ids": ["mrqa_squad-validation-4169", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-5248", "mrqa_squad-validation-1003", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-validation-441", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-5184", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-3208", "mrqa_triviaqa-validation-7703", "mrqa_hotpotqa-validation-3921", "mrqa_triviaqa-validation-792", "mrqa_naturalquestions-validation-4653"], "unfixed_ids": ["mrqa_triviaqa-validation-5078", "mrqa_naturalquestions-validation-5769"], "instant_fixing_rate": 0.92, "instant_retention_rate": 0.7142857132653061}, {"timecode": 12, "before_eval": {"predictions": ["British rock group Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Joe Turano", "backsword", "Margaret Thatcher", "Kevin Harlan as play-by-play announcer, Boomer Esiason and Dan Fouts as color analysts, and James Lofton and Mark Malone as sideline reporters", "August", "The stability, security, and predictability of British law and government", "artisans of the later Minoan or Mykenaian age", "1860", "ring size of 50 millimeters and length of 141 millimeters", "Mitre", "Forbes", "Fort Bull", "Dandy", "eatanelle", "Orwell", "Kingdom of Bohemia", "Gregg Popovich", "that not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence", "for creative reasons", "adaptive", "uncle", "musician", "barleycorn", "December 1, 1969", "american", "ark ark", "California State Automobile Association", "faith", "Cinderella", "the cabin burst and the fire erupted onto the pad area", "due to a lack of understanding of the legal ramifications"], "metric_results": {"EM": 0.09375, "QA-F1": 0.2251564883917825}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.15384615384615385, 0.0, 0.0, 1.0, 0.37037037037037035, 0.0, 0.0, 0.22222222222222224, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.11764705882352941, 0.4, 0.0, 0.23529411764705882, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.2, 0.8571428571428571]}}, "error_ids": ["mrqa_squad-validation-93", "mrqa_naturalquestions-validation-6167", "mrqa_triviaqa-validation-1671", "mrqa_squad-validation-568", "mrqa_hotpotqa-validation-2680", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-1758", "mrqa_hotpotqa-validation-4263", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-6902", "mrqa_hotpotqa-validation-1099", "mrqa_squad-validation-10202", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-2737", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-2886", "mrqa_squad-validation-9793", "mrqa_naturalquestions-validation-5175", "mrqa_squad-validation-6678", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-7371", "mrqa_squad-validation-2812", "mrqa_squad-validation-2372", "mrqa_squad-validation-3935", "mrqa_squad-validation-6924"], "after_eval": {"predictions": ["Coldplay", "Frederick in a duet with Teresa James", "fencing", "Margaret Thatcher", "James Lofton and Mark Malone", "December 2013", "a centre for international trade", "Minoan civilization", "18 November", "cigars", "byker grove", "Forbes", "garrisons", "Bonnie Lipton ( portrayed by Skyler Samuels )", "ring", "possibly Orwell himself, called upon to shoot an aggressive elephant while working as a police officer in Burma", "Czech Kingdom", "Bob Hill", "secularism and secular nationalism", "creative reasons", "immunological memory", "the tutelage of his uncle", "Originally a musician", "thumbelina", "1973", "maryland", "john buchan", "AAA Auto Clubs", "alone", "Cinderella", "delayed the sealing of the hatch", "lack of understanding of the legal ramifications, or due to a fear of seeming rude"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9556818181818182}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-4241", "before_prediction": "uncle", "after_prediction": "the tutelage of his uncle"}], "retained_ids": ["mrqa_hotpotqa-validation-4826", "mrqa_hotpotqa-validation-4165"], "fixed_ids": ["mrqa_squad-validation-93", "mrqa_naturalquestions-validation-6167", "mrqa_triviaqa-validation-1671", "mrqa_squad-validation-568", "mrqa_hotpotqa-validation-2680", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-1758", "mrqa_hotpotqa-validation-4263", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-6902", "mrqa_squad-validation-10202", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-2737", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-2886", "mrqa_squad-validation-9793", "mrqa_naturalquestions-validation-5175", "mrqa_squad-validation-6678", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-7371", "mrqa_squad-validation-2812", "mrqa_squad-validation-2372", "mrqa_squad-validation-3935", "mrqa_squad-validation-6924"], "unfixed_ids": ["mrqa_hotpotqa-validation-1099"], "instant_fixing_rate": 0.9655172413793104, "instant_retention_rate": 0.6666666644444444}, {"timecode": 13, "before_eval": {"predictions": ["Sister, Sister ( 1982 film)", "president of Guggenheim Partners", "Buddy Pine / Incredi - Boy / Syndrome", "Napoleon's army", "ohor Frederick Rohwedder", "3.7 percent of the entire student population", "High and persistent unemployment, in which inequality increases, has a negative effect on subsequent long-run economic growth", "Matt Willis and Charlie Quirke", "Little Golden Lion award", "Jerry Ekandjo ( until February 2018 ), Erastus Utoni   Deputy : Agnes Tjongarero", "discipline problems with the Flight Director's orders during their flight", "jonathan", "paddington", "amyotrophic lateral sclerosis (ALS)", "Odorama", "Swiss made", "Evel Knievel craze of the mid 1970s", "Torah or Bible", "the western coast of Italy", "the first and only U.S. born world grand prix champion", "the quintessential New Orleans art form -- a jazz funeral without a body", "in mid November and lit in a public ceremony in late November or early December", "Facebook", "beigel", "Steel Dragon", "Seattle", "British and French colonies", "after an intense sketch with Jake, Miley finally ends it with him", "alternative rock", "Fort Snelling, Minnesota", "pinhole", "infrequent rain"], "metric_results": {"EM": 0.15625, "QA-F1": 0.28657617752044995}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true], "QA-F1": [0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.2857142857142857, 0.2222222222222222, 0.0, 0.3333333333333333, 0.33333333333333337, 0.3636363636363636, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5263157894736842, 1.0, 0.0, 0.0, 0.0, 0.0, 0.7058823529411764, 1.0, 0.3333333333333333, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-1509", "mrqa_naturalquestions-validation-1135", "mrqa_squad-validation-830", "mrqa_triviaqa-validation-7638", "mrqa_squad-validation-7042", "mrqa_squad-validation-7351", "mrqa_triviaqa-validation-6944", "mrqa_hotpotqa-validation-4058", "mrqa_naturalquestions-validation-6445", "mrqa_squad-validation-4060", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-4486", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-2037", "mrqa_squad-validation-1850", "mrqa_naturalquestions-validation-150", "mrqa_hotpotqa-validation-800", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-2896", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-1932", "mrqa_squad-validation-10168", "mrqa_naturalquestions-validation-7650", "mrqa_hotpotqa-validation-3669", "mrqa_triviaqa-validation-6913"], "after_eval": {"predictions": ["The Omega Man", "president", "Dashiell Robert Parr / Dash, the Parrs'second child", "Napoleon's", "pre sliced bread", "3.7", "negative", "Garth", "Ecumenical Award", "Erastus Utoni", "discipline problems", "nine", "michael hordern", "Lou Gehrig's Disease", "Odorama", "Swiss made", "October 17, 1938", "religious", "Sicily", "American-born", "Those who follow the band just to enjoy the music", "late November or early December", "Facebook", "bread", "Tim \"Ripper\" Owens", "Issaquah", "King George's War", "Miley finally ends it with him", "alternative rock", "Fort Saint Anthony", "fox talbot", "infrequent rain"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9927884615384616}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": ["mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1178", "mrqa_triviaqa-validation-2779", "mrqa_naturalquestions-validation-7310", "mrqa_squad-validation-2656"], "fixed_ids": ["mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-1509", "mrqa_squad-validation-830", "mrqa_triviaqa-validation-7638", "mrqa_squad-validation-7042", "mrqa_squad-validation-7351", "mrqa_triviaqa-validation-6944", "mrqa_hotpotqa-validation-4058", "mrqa_naturalquestions-validation-6445", "mrqa_squad-validation-4060", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-4486", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-2037", "mrqa_squad-validation-1850", "mrqa_naturalquestions-validation-150", "mrqa_hotpotqa-validation-800", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-2896", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-1932", "mrqa_squad-validation-10168", "mrqa_naturalquestions-validation-7650", "mrqa_hotpotqa-validation-3669", "mrqa_triviaqa-validation-6913"], "unfixed_ids": ["mrqa_naturalquestions-validation-1135"], "instant_fixing_rate": 0.9629629629629629, "instant_retention_rate": 0.9999999980000001}, {"timecode": 14, "before_eval": {"predictions": ["Beauty and the Breast", "Honolulu", "Daniel Hale Williams Preparatory School of Medicine", "dark blood", "FX option", "electromagnetic waves", "Wahhabi/ Salafi", "swastika", "Dimensions in Time", "Surveyor 3 unmanned lunar probe", "January 1981", "estrogen", "the structure and substance of his questions and answers concerning baptism", "may his body and soul be bound up in hell", "Brian Clough", "engine braking mechanism installed on some diesel engines", "Belle Fourche and Cheyenne", "organisms", "Hanna-barbera, The Jetsons", "Cortina d'Ampezzo", "efficient and effective management of money ( funds )", "Alba Longa", "duke and Duchess of Sto Helit", "Norfolk Island", "Timo Hildebrand", "state sector", "February", "poverty, the lack of access to education and weak government institutions", "a god of the Ammonites", "sclera", "Uncle Fester", "Charles Whitman"], "metric_results": {"EM": 0.28125, "QA-F1": 0.3692505411255411}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, false, false, false, true, false, false, true, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 0.18181818181818182, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-970", "mrqa_hotpotqa-validation-3253", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10046", "mrqa_squad-validation-9751", "mrqa_squad-validation-7836", "mrqa_squad-validation-3999", "mrqa_naturalquestions-validation-5944", "mrqa_squad-validation-2448", "mrqa_squad-validation-2509", "mrqa_naturalquestions-validation-727", "mrqa_squad-validation-6023", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-validation-3302", "mrqa_hotpotqa-validation-1244", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-5191", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-5256"], "after_eval": {"predictions": ["Hong Kong", "Honolulu", "the British military", "dark blood", "a foreign exchange option ( commonly shortened to just FX option or currency option )", "radio and microwave frequencies", "Wahhabi/ Salafi extremist extremist militant", "svastika", "Children in Need", "Apollo 12", "1981", "estrogen", "baptism", "Martin Luther", "brian clough", "slowing the vehicle", "the Belle Fourche and Cheyenne rivers", "organisms", "Hanna-Barbera", "the Veneto region of Northern Italy", "accomplish the objectives of the organization", "12 mi southeast of Rome", "binky", "tasmania", "Kur\u00e1nyi", "public sector ( also called the state sector )", "February", "poverty, the lack of access to education and weak government institutions", "king", "vitreous humor", "Judge Doom", "Texas Tower Sniper"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9831349206349207}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-116", "before_prediction": "Belle Fourche and Cheyenne", "after_prediction": "the Belle Fourche and Cheyenne rivers"}], "retained_ids": ["mrqa_hotpotqa-validation-3757", "mrqa_triviaqa-validation-46", "mrqa_triviaqa-validation-2442", "mrqa_naturalquestions-validation-8180", "mrqa_triviaqa-validation-7153", "mrqa_squad-validation-5178", "mrqa_hotpotqa-validation-241", "mrqa_squad-validation-8444"], "fixed_ids": ["mrqa_hotpotqa-validation-970", "mrqa_hotpotqa-validation-3253", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10046", "mrqa_squad-validation-7836", "mrqa_squad-validation-3999", "mrqa_naturalquestions-validation-5944", "mrqa_squad-validation-2448", "mrqa_squad-validation-2509", "mrqa_naturalquestions-validation-727", "mrqa_squad-validation-6023", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-validation-3302", "mrqa_hotpotqa-validation-1244", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-5191", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-5256"], "unfixed_ids": ["mrqa_squad-validation-9751"], "instant_fixing_rate": 0.9565217391304348, "instant_retention_rate": 0.8888888879012344}, {"timecode": 15, "before_eval": {"predictions": ["superhuman abilities", "Deathly Hallows", "the Flatbush section of Brooklyn, New York City", "San Antonio", "Royce da 5'9\" ( Bad) and Eminem ( Evil)", "viternicus", "a friend and publicist", "michael Ondaatje", "masons'marks", "Theodore Haynes (1989)", "the Old Town Hall, Gateshead", "that's the Way of the World", "The neck", "1898", "professional wrestler, mixed martial artist and a former amateur wrestler", "Payaya Indians", "to steal the plans for the Death Star", "Vito Corleone", "a Curtiss JN-4", "art", "chimpanpanzee", "March 15, 1945", "absolute temperature", "the private intelligence firm Stratfor and releasing the leaks through the whistle-blowing website", "J. Robert Oppenheimer", "the buccal cusp", "Aegisthus", "3 December", "tallahassee", "prefabricated housing projects", "fleet river", "WOTV"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2167207792207792}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false], "QA-F1": [0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2612", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1479", "mrqa_triviaqa-validation-7512", "mrqa_squad-validation-1374", "mrqa_triviaqa-validation-5979", "mrqa_naturalquestions-validation-10439", "mrqa_hotpotqa-validation-1048", "mrqa_squad-validation-5249", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-56", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-validation-368", "mrqa_hotpotqa-validation-573", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-5877", "mrqa_triviaqa-validation-7578", "mrqa_hotpotqa-validation-5188", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-365", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-2957", "mrqa_triviaqa-validation-1736", "mrqa_triviaqa-validation-1166", "mrqa_squad-validation-874", "mrqa_squad-validation-6091"], "after_eval": {"predictions": ["comic", "Harry Potter and the Deathly Hallows", "the Flatbush section of Brooklyn, New York City", "the Alamodome", "Eminem, Bad Meets Evil, Akon, Christina Aguilera and Taio Cruz", "galileo", "the editor of Electrical World magazine", "english patient", "the name of a work gang", "James Taylor", "a roof extension", "based on a Yogiism, or quotation from Yogi Berra", "midpiece", "after the Spanish -- American War in the 1898 Treaty of Paris", "martial artist", "Spanish", "stunt performances", "fred astaire", "postage stamp", "belgium", "Chimpanzees", "After World War II", "volume", "Jeremy Hammond", "Sam Waterston", "first premolar", "Aegisthus", "25 November 2015", "florida", "an Eastern Bloc city", "Fleet River", "KMBC-TV and KQTV"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9272058823529412}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-335", "before_prediction": "Royce da 5'9\" ( Bad) and Eminem ( Evil)", "after_prediction": "Eminem, Bad Meets Evil, Akon, Christina Aguilera and Taio Cruz"}, {"id": "mrqa_naturalquestions-validation-3808", "before_prediction": "1898", "after_prediction": "after the Spanish -- American War in the 1898 Treaty of Paris"}, {"id": "mrqa_hotpotqa-validation-413", "before_prediction": "3 December", "after_prediction": "25 November 2015"}], "retained_ids": ["mrqa_hotpotqa-validation-3456", "mrqa_naturalquestions-validation-9451", "mrqa_triviaqa-validation-1588"], "fixed_ids": ["mrqa_hotpotqa-validation-2612", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1479", "mrqa_triviaqa-validation-7512", "mrqa_squad-validation-1374", "mrqa_triviaqa-validation-5979", "mrqa_naturalquestions-validation-10439", "mrqa_hotpotqa-validation-1048", "mrqa_squad-validation-5249", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-56", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-validation-368", "mrqa_hotpotqa-validation-573", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-5877", "mrqa_triviaqa-validation-7578", "mrqa_hotpotqa-validation-5188", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-365", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-2957", "mrqa_triviaqa-validation-1736", "mrqa_triviaqa-validation-1166", "mrqa_squad-validation-874", "mrqa_squad-validation-6091"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.49999999916666665}, {"timecode": 16, "before_eval": {"predictions": ["galileo", "cow's milk cheese", "benedict", "the lateral side of the tibia", "cen\u00e9l n Gabr\u00e1in", "the North Sea, through the former Meuse estuary, near Rotterdam", "the Kalahari Desert", "Colin Montgomerie", "October 29, 1985", "Amway", "secondary school study", "Thomas Sowell", "Speaker of the Lok Sabha or in his absence, the Deputy - Chairman of the Rajya Sabha", "From Here to Eternity", "tanzibar", "Chad", "60-mile-wide river", "an open work crown", "a child to go through a torturous treatment to gain information", "Fulham", "French", "come dine with me", "U.S. Marshals", "South Korean television series", "supply chain management", "galileo", "Stanislaw August Poniatowski", "a rectangular array of coefficients", "Stuart Little", "three wise monkeys", "sheepskin and Merino Wool products", "the \"second city\" of Oahu"], "metric_results": {"EM": 0.0625, "QA-F1": 0.10461309523809524}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21428571428571425, 0.33333333333333337, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2324", "mrqa_naturalquestions-validation-9218", "mrqa_triviaqa-validation-313", "mrqa_squad-validation-9319", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4681", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2771", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-3483", "mrqa_hotpotqa-validation-1566", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-4055", "mrqa_squad-validation-5407", "mrqa_naturalquestions-validation-7144", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-2445", "mrqa_triviaqa-validation-639", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-1001", "mrqa_triviaqa-validation-1770", "mrqa_triviaqa-validation-866", "mrqa_squad-validation-8223", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-validation-9087", "mrqa_hotpotqa-validation-1250"], "after_eval": {"predictions": ["nightclub", "belgian", "blessed", "leg", "dunkeld", "North Sea", "Botswana", "sandy lyle", "March 30, 1983", "American Way", "secondary school study", "Milton Friedman", "President", "BBC Radio's \"The Show Band Show\"", "tanzania", "niger", "everglades", "top row of windows", "Sam's soul is not with him", "London", "French, English and Spanish", "dave lamb", "Beyond the Clouds", "\"The Heirs\" (2013)", "blood, platelets, and plasma", "flowing water", "polish state", "matrices", "geena davis", "The three wise monkeys ( Japanese : \u4e09\u733f, Hepburn : san'en or sanzaru", "Sheepskin", "city-county of Honolulu"], "metric_results": {"EM": 0.875, "QA-F1": 0.9453444839474251}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5294117647058824, 1.0, 0.28571428571428575]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-2287", "before_prediction": "the \"second city\" of Oahu", "after_prediction": "city-county of Honolulu"}], "retained_ids": ["mrqa_hotpotqa-validation-4094"], "fixed_ids": ["mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2324", "mrqa_naturalquestions-validation-9218", "mrqa_squad-validation-9319", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4681", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2771", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-3483", "mrqa_hotpotqa-validation-1566", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-4055", "mrqa_squad-validation-5407", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-2445", "mrqa_triviaqa-validation-639", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-1001", "mrqa_triviaqa-validation-1770", "mrqa_triviaqa-validation-866", "mrqa_squad-validation-8223", "mrqa_triviaqa-validation-298", "mrqa_hotpotqa-validation-1250"], "unfixed_ids": ["mrqa_triviaqa-validation-313", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-9087"], "instant_fixing_rate": 0.9, "instant_retention_rate": 0.4999999975}, {"timecode": 17, "before_eval": {"predictions": ["dymock", "20th Century Fox, Lionsgate, Paramount Pictures, Universal Studios and Walt Disney Studios paid for movie trailers to be aired", "August 6, 1845", "decay energy of 687 keV", "James Zeebo", "state system", "first person in the presidential line of succession", "The Discovery Institute's \"Teach the Controversy\" campaign", "Bumblebee", "Australian", "six additional months for men and 24 months for women", "opportunities will vary by geographic area and subject taught", "lower", "private liberal arts college", "Roy Spencer", "\"antiforms\"", "in the second half of the third season", "V. Prakash Kumar", "Grace Nail Johnson", "Mick Jagger", "n < p < 2n \u2212 2", "Bangor International Airport", "teacher who specialize in one subject and who tend to be more knowledgeable in that one area than a teacher who teaches many subjects", "360 \u00b0 - system", "Cartoon Network's late night programming block, Adult Swim", "Presiding Officer on the advice of the parliamentary bureau", "Miami Heat", "33", "vitifolia", "Annual Conference Cabinet", "olympic bronze medals", "first prompted by original star William Hartnell's poor health"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3530631740335688}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.0, 0.0, 0.8421052631578948, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 0.0, 0.3333333333333333, 0.0, 0.28571428571428575, 0.25, 0.08333333333333333, 0.0, 0.2, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1298", "mrqa_squad-validation-608", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3559", "mrqa_triviaqa-validation-314", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-3440", "mrqa_naturalquestions-validation-1090", "mrqa_squad-validation-2053", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-4868", "mrqa_naturalquestions-validation-9171", "mrqa_squad-validation-8966", "mrqa_hotpotqa-validation-2782", "mrqa_squad-validation-1904", "mrqa_naturalquestions-validation-10347", "mrqa_hotpotqa-validation-3573", "mrqa_squad-validation-9405", "mrqa_hotpotqa-validation-613", "mrqa_hotpotqa-validation-4463", "mrqa_triviaqa-validation-3071", "mrqa_squad-validation-7664"], "after_eval": {"predictions": ["pear", "Independence Day: Resurgence", "August 6, 1845 - October 6, 1931", "rubidium - 85", "James Zeebo", "norm that sovereigns had no internal equals within a defined territory and no external superiors as the ultimate authority within the territory's sovereign borders", "Dick Cheney", "\"Teach the Controversy\" campaign", "Ravage and the Decepticon Rampage", "Velichko Todorov Tsochev) is a Bulgarian-Canadian", "36 months for men and 24 months for women", "vary", "lower rates", "a private liberal arts college located on 575 acres (2.08 km\u00b2) in Hamilton Village, Hamilton Township, Madison County, New York, United States", "Roy Spencer", "antiforms", "the second half of the third season", "Veyyil", "James Weldon Johnson", "Rocky Dzidzornu -- congas", "n > 3", "Bangor Air National Guard Base", "knowledgeable", "antimeridian", "Cartoon Network", "Presiding Officer", "the Phoenix Suns", "33-member", "vitis", "Annual Conference Cabinet", "alex partridge", "the Doctor's third on-screen regeneration"], "metric_results": {"EM": 0.875, "QA-F1": 0.9349031007751938}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.9302325581395349, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.32, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-5628", "before_prediction": "August 6, 1845", "after_prediction": "August 6, 1845 - October 6, 1931"}, {"id": "mrqa_triviaqa-validation-1130", "before_prediction": "olympic bronze medals", "after_prediction": "alex partridge"}], "retained_ids": ["mrqa_naturalquestions-validation-430", "mrqa_squad-validation-7572", "mrqa_hotpotqa-validation-1895", "mrqa_squad-validation-5110", "mrqa_squad-validation-10074"], "fixed_ids": ["mrqa_triviaqa-validation-1298", "mrqa_squad-validation-608", "mrqa_naturalquestions-validation-3724", "mrqa_triviaqa-validation-314", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-3440", "mrqa_naturalquestions-validation-1090", "mrqa_squad-validation-2053", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-4868", "mrqa_naturalquestions-validation-9171", "mrqa_squad-validation-8966", "mrqa_hotpotqa-validation-2782", "mrqa_squad-validation-1904", "mrqa_naturalquestions-validation-10347", "mrqa_hotpotqa-validation-3573", "mrqa_squad-validation-9405", "mrqa_hotpotqa-validation-613", "mrqa_hotpotqa-validation-4463", "mrqa_triviaqa-validation-3071", "mrqa_squad-validation-7664"], "unfixed_ids": ["mrqa_naturalquestions-validation-3559", "mrqa_hotpotqa-validation-501"], "instant_fixing_rate": 0.92, "instant_retention_rate": 0.7142857132653061}, {"timecode": 18, "before_eval": {"predictions": ["the genocide against the Tutsi", "Co-teachers work in sync with one another", "4 \u00d7 400 metres relay", "Vili Fualaau and Mary Kay Letourneau, a student and teacher who made news for their sexual relationship", "the entertainment division", "the straight - line distance from A to B", "12", "the Great Exhibition of 1851", "King Edward I to Henry VIII", "the Chagos Archipelago", "dundee", "the state ( in Roosevelt's view and in the general social humanitarian approach ) needed to help", "darnley", "\"Grindhouse\" fake trailer", "norman iisevic", "digital transmission", "Baden-W\u00fcrttemberg", "lithium-ion battery", "821", "HD channels and Video On Demand content", "norm", "Hyuna", "the races of highest'social efficiency'", "transposition", "the \" King of Cool\"", "President Woodrow Wilson", "Socrates", "the fifth season", "rebecca", "Hockey Club Davos", "Michael Crawford", "a lightning strike"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3241071428571428}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, false, false, false, false], "QA-F1": [0.4, 0.4444444444444445, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.16666666666666666, 0.0, 1.0, 0.1, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 0.6666666666666665, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.4, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-1441", "mrqa_squad-validation-1914", "mrqa_hotpotqa-validation-523", "mrqa_hotpotqa-validation-2145", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-3300", "mrqa_squad-validation-5257", "mrqa_hotpotqa-validation-2993", "mrqa_triviaqa-validation-5996", "mrqa_naturalquestions-validation-5215", "mrqa_triviaqa-validation-2683", "mrqa_hotpotqa-validation-2201", "mrqa_triviaqa-validation-2715", "mrqa_naturalquestions-validation-2222", "mrqa_squad-validation-2862", "mrqa_triviaqa-validation-4279", "mrqa_squad-validation-9841", "mrqa_naturalquestions-validation-4497", "mrqa_squad-validation-9827", "mrqa_hotpotqa-validation-3798", "mrqa_triviaqa-validation-7100", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-511", "mrqa_naturalquestions-validation-10490"], "after_eval": {"predictions": ["Rwandan genocide", "harmoniously", "500 metres", "Piper", "ABC News", "displacement", "five", "Museum of Manufactures", "Edward Longshanks and the Hammer of the Scots", "diego garcia", "Dundee", "the person compelled to pay for reformist programs", "fotheringhay", "Spy Kids", "venus williams", "MFSK", "Baden-W\u00fcrttemberg", "Tesla Gigafactory", "821", "the basic channels", "pressure", "Kim Hyun-ah", "races of highest'social efficiency\"", "transposed", "King of Cool", "American delegation from the Paris Peace Conference", "Xenophon", "thirteenth", "violet", "HC Davos", "Michael Patrick Smith", "Qutab Ud - Din - Aibak"], "metric_results": {"EM": 0.875, "QA-F1": 0.8958333333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666665, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-4415", "before_prediction": "lithium-ion battery", "after_prediction": "Tesla Gigafactory"}, {"id": "mrqa_hotpotqa-validation-1855", "before_prediction": "Hyuna", "after_prediction": "Kim Hyun-ah"}, {"id": "mrqa_triviaqa-validation-1764", "before_prediction": "Socrates", "after_prediction": "Xenophon"}], "retained_ids": ["mrqa_triviaqa-validation-5036", "mrqa_squad-validation-9074", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4068"], "fixed_ids": ["mrqa_hotpotqa-validation-1441", "mrqa_squad-validation-1914", "mrqa_hotpotqa-validation-523", "mrqa_hotpotqa-validation-2145", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-3300", "mrqa_squad-validation-5257", "mrqa_hotpotqa-validation-2993", "mrqa_triviaqa-validation-5996", "mrqa_naturalquestions-validation-5215", "mrqa_triviaqa-validation-2683", "mrqa_hotpotqa-validation-2201", "mrqa_triviaqa-validation-2715", "mrqa_naturalquestions-validation-2222", "mrqa_squad-validation-2862", "mrqa_triviaqa-validation-4279", "mrqa_naturalquestions-validation-4497", "mrqa_squad-validation-9827", "mrqa_hotpotqa-validation-3798", "mrqa_triviaqa-validation-7100", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-511", "mrqa_naturalquestions-validation-10490"], "unfixed_ids": ["mrqa_squad-validation-9841"], "instant_fixing_rate": 0.96, "instant_retention_rate": 0.571428570612245}, {"timecode": 19, "before_eval": {"predictions": ["a Ghanaian boxer", "Norman Macdonnell and writer John Meston", "zaragoza", "11.1", "the first trans-Pacific flight from the United States to Australia", "Sharman Joshi", "quietly", "Forster I, Forster II, and Forster III", "p is not a prime factor of q", "Ana", "in Cherry Hill", "Goodbye Toby", "call me ishmael", "george madeleine Sopie Blanchard", "Six Degrees of Separation", "david bowie", "Star Plus", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "1889", "The chorus of Nicki Minaj's 2014 single `` Anaconda ''", "slave of duty", "surnames", "portier", "karl marx", "Teen Titans Go!", "William the Conqueror", "Ben Gurion International Airport", "two degrees of freedom", "peninsula", "taking blood", "Guinness World Records", "Sunset Publishing Corporation"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6504734848484849}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 0.5, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.3333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-2389", "mrqa_squad-validation-6963", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-303", "mrqa_triviaqa-validation-6935", "mrqa_hotpotqa-validation-3049", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-1565", "mrqa_hotpotqa-validation-5061", "mrqa_naturalquestions-validation-7881", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-2627"], "after_eval": {"predictions": ["a Ghanaian boxer", "John Meston", "zaragoza", "7.8", "trans-Pacific flight", "Soha Ali Khan", "quietly", "Forster I, Forster II, and Forster III", "p is not a prime factor of q", "Ana", "in Cherry Hill", "Goodbye Toby", "call me ishmael", "amelia earhart", "Six Degrees of Separation", "david bowie", "Indian", "fortitude", "1889", "Sir Mix - a-Lot", "slave of duty", "surnames", "20mph", "marx", "Teen Titans Go!", "Norman invaders", "Tel Aviv", "two", "peninsula", "taking blood", "youngest TV director ever", "Southern Progress Corporation"], "metric_results": {"EM": 0.96875, "QA-F1": 0.96875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-2959", "before_prediction": "portier", "after_prediction": "20mph"}], "retained_ids": ["mrqa_hotpotqa-validation-2262", "mrqa_triviaqa-validation-6901", "mrqa_squad-validation-1942", "mrqa_squad-validation-5538", "mrqa_squad-validation-9214", "mrqa_hotpotqa-validation-5710", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-9284", "mrqa_triviaqa-validation-5429", "mrqa_naturalquestions-validation-4354", "mrqa_triviaqa-validation-1995", "mrqa_naturalquestions-validation-4951", "mrqa_triviaqa-validation-2015", "mrqa_squad-validation-3151", "mrqa_triviaqa-validation-4068", "mrqa_hotpotqa-validation-57", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-3552"], "fixed_ids": ["mrqa_hotpotqa-validation-2389", "mrqa_squad-validation-6963", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-303", "mrqa_triviaqa-validation-6935", "mrqa_hotpotqa-validation-3049", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-1565", "mrqa_hotpotqa-validation-5061", "mrqa_naturalquestions-validation-7881", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-2627"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9473684205540166}, {"timecode": 20, "before_eval": {"predictions": ["gas turbines", "Robert Smigel, Michael Koman and David Feldman", "the Sackler Centre for arts education", "Mos Def", "watcher", "Langley", "Apollo", "ribosomal RNA", "kingfisher", "six-time", "Vanessa Ferlito", "How Was I to Know", "Bulgarian and Romanian sweet leavened bread", "gerry adams", "Moon's surface", "Lucius Cornelius Sulla Felix", "following the 2017 season", "Golden Globe", "Kenya's various ethnic groups typically speak their mother tongues within their own communities", "trust God's word", "norhan Pamuk", "Hexachrome", "Firoz Shah Tughlaq", "Reunited Worlds", "San Jose", "slow ringed octopus", "lower chamber", "a \"teleforce\" weapon", "Native American", "the most giving Super Bowl ever", "29.7", "4077th mASH"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5513621794871795}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.07692307692307693, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666666, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-5273", "mrqa_triviaqa-validation-5091", "mrqa_naturalquestions-validation-4590", "mrqa_squad-validation-3964", "mrqa_hotpotqa-validation-2434", "mrqa_triviaqa-validation-3486", "mrqa_hotpotqa-validation-4627", "mrqa_naturalquestions-validation-5168", "mrqa_hotpotqa-validation-2407", "mrqa_naturalquestions-validation-2448", "mrqa_triviaqa-validation-6721", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-10509", "mrqa_squad-validation-315", "mrqa_triviaqa-validation-1453"], "after_eval": {"predictions": ["gas turbine", "Robert Smigel, Michael Koman and David Feldman", "prints, drawings, paintings and photographs", "Mos Def", "kaleidoscope", "British Columbia, in the Abbotsford, Vancouver and Langley areas in August 2017, with a mansion in the Aldergrove area of Langely serving as the property at the centre of the story", "Eisele", "Ribosomes", "dacelo", "six-time", "Rob Kerkovich as Sebastian Lund, forensic scientist / NCIS Forensics Agent", "\"Ain't Got Nothin' on Us\"", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak", "Gerry Adams", "Moon's surface", "Sulla", "Super Bowl LII", "Golden Globe", "Kenya's various ethnic groups typically speak their mother tongues within their own communities", "personal presence and living word", "turkey", "CMYKOG process", "Qutab - ud - din Aibak", "\"Cinderella and Four Knights\"", "Santa Clara", "jellyfish", "lower chamber", "\"teleforce\" weapon", "Native American", "The Super Bowl 50 Host Committee has vowed to be \"the most giving Super Bowl ever\" and will dedicate 25 percent of all money it raises for philanthropic causes in the Bay Area", "29.7%", "4077th mash"], "metric_results": {"EM": 0.875, "QA-F1": 0.9099702380952381}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-3368", "before_prediction": "gas turbines", "after_prediction": "gas turbine"}, {"id": "mrqa_naturalquestions-validation-1279", "before_prediction": "Vanessa Ferlito", "after_prediction": "Rob Kerkovich as Sebastian Lund, forensic scientist / NCIS Forensics Agent"}, {"id": "mrqa_hotpotqa-validation-4015", "before_prediction": "Reunited Worlds", "after_prediction": "\"Cinderella and Four Knights\""}, {"id": "mrqa_squad-validation-393", "before_prediction": "the most giving Super Bowl ever", "after_prediction": "The Super Bowl 50 Host Committee has vowed to be \"the most giving Super Bowl ever\" and will dedicate 25 percent of all money it raises for philanthropic causes in the Bay Area"}], "retained_ids": ["mrqa_hotpotqa-validation-779", "mrqa_hotpotqa-validation-3547", "mrqa_hotpotqa-validation-1210", "mrqa_triviaqa-validation-4852", "mrqa_hotpotqa-validation-3623", "mrqa_hotpotqa-validation-2795", "mrqa_squad-validation-8464", "mrqa_squad-validation-2280", "mrqa_hotpotqa-validation-1906", "mrqa_squad-validation-1521", "mrqa_hotpotqa-validation-2064", "mrqa_squad-validation-7272", "mrqa_triviaqa-validation-935"], "fixed_ids": ["mrqa_squad-validation-5273", "mrqa_triviaqa-validation-5091", "mrqa_naturalquestions-validation-4590", "mrqa_squad-validation-3964", "mrqa_hotpotqa-validation-2434", "mrqa_triviaqa-validation-3486", "mrqa_hotpotqa-validation-4627", "mrqa_naturalquestions-validation-5168", "mrqa_hotpotqa-validation-2407", "mrqa_naturalquestions-validation-2448", "mrqa_triviaqa-validation-6721", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-10509", "mrqa_squad-validation-315", "mrqa_triviaqa-validation-1453"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.7647058819031142}, {"timecode": 21, "before_eval": {"predictions": ["Czech word, robota, meaning `` forced labor '' ; the word'robot'was first used to denote a fictional humanoid in a 1920 play R.U.R. by the Czech writer, Karel \u010capek", "daphne du maurier", "various registries", "transfused", "alison moyet", "1926", "neutron star", "\"the soul does not sleep (anima non sic dormit) but wakes (sed vigilat) and experiences visions\"", "They viewed the economic value of the Caribbean islands' sugar cane to be greater and easier to defend than the furs from the continent", "Johnny Cash, Waylon Jennings, Willie Nelson, and Kris Kristofferson", "auctions", "private", "Scottish", "waiter turned emerging young actor Smith Jerrod", "Charles Dickens and Beatrix Potter", "non-profit, ECOSOC non-governmental organization", "carbohydrates", "2000", "that there are infinitely many primes", "alastair burnet", "padlocking the gates and using sickles to deflate one of the large domes covering two satellite dishes", "1960s", "huldra", "Indian portions of the Great Lakes region (an area not directly subject to the conflict between the French and British) including the Huron, Mississauga, Ojibwa, Winnebago, and Potawatomi", "Protestant", "casino Royale", "4 in ( 10 cm )", "oh so Sharp", "Western Atlantic ctenophore Mnemiopsis leidyi", "\"Menace II Society\"", "a member of the Green Bay Packers, serving as a backup quarterback to Brett Favre and holder on placekicks, and winning Super Bowl XXXI with the team over the New England Patriots", "trio with his younger brothers Steve and Rudy"], "metric_results": {"EM": 0.4375, "QA-F1": 0.577510990825298}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false, true], "QA-F1": [0.5333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 0.23529411764705882, 0.0, 0.0, 0.3448275862068966, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7692307692307693, 1.0, 0.7441860465116279, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-3609", "mrqa_triviaqa-validation-5239", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-192", "mrqa_squad-validation-2412", "mrqa_hotpotqa-validation-5480", "mrqa_squad-validation-5345", "mrqa_hotpotqa-validation-35", "mrqa_squad-validation-3627", "mrqa_naturalquestions-validation-5897", "mrqa_squad-validation-9020", "mrqa_squad-validation-6844", "mrqa_naturalquestions-validation-8689", "mrqa_hotpotqa-validation-2399", "mrqa_squad-validation-10177", "mrqa_naturalquestions-validation-6832", "mrqa_squad-validation-4648", "mrqa_naturalquestions-validation-2758"], "after_eval": {"predictions": ["a 1920 play R.U.R. by the Czech writer, Karel \u010capek", "rebecca", "domestic cat", "transfused", "alison moyet", "1926", "black hole", "dreams", "They viewed the economic value of the Caribbean islands' sugar cane to be greater and easier to defend than the furs from the continent", "Johnny Cash, Waylon Jennings", "auctions", "private", "Scottish", "waiter turned emerging young actor Smith Jerrod", "Beatrix Potter", "Organizations could come together to address global issues", "proteins", "2001", "exceeds any given number", "alastair burnet", "padlocking the gates", "1969", "R\u00e5", "tribes in western portions of the Great Lakes region", "Protestant", "casino royale", "4 in", "oh so sharp", "ctenophore Mnemiopsis leidyi", "Menace II Society", "starting quarterback for the Eagles and Cleveland Browns", "trio with his younger brothers Steve and Rudy"], "metric_results": {"EM": 1.0, "QA-F1": 1.0}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": ["mrqa_triviaqa-validation-751", "mrqa_triviaqa-validation-4383", "mrqa_hotpotqa-validation-4430", "mrqa_squad-validation-10502", "mrqa_triviaqa-validation-2961", "mrqa_squad-validation-2709", "mrqa_hotpotqa-validation-4950", "mrqa_naturalquestions-validation-5439", "mrqa_triviaqa-validation-1954", "mrqa_squad-validation-8456", "mrqa_triviaqa-validation-6950", "mrqa_triviaqa-validation-1555", "mrqa_hotpotqa-validation-2642", "mrqa_hotpotqa-validation-4676"], "fixed_ids": ["mrqa_naturalquestions-validation-3609", "mrqa_triviaqa-validation-5239", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-192", "mrqa_squad-validation-2412", "mrqa_hotpotqa-validation-5480", "mrqa_squad-validation-5345", "mrqa_hotpotqa-validation-35", "mrqa_squad-validation-3627", "mrqa_naturalquestions-validation-5897", "mrqa_squad-validation-9020", "mrqa_squad-validation-6844", "mrqa_naturalquestions-validation-8689", "mrqa_hotpotqa-validation-2399", "mrqa_squad-validation-10177", "mrqa_naturalquestions-validation-6832", "mrqa_squad-validation-4648", "mrqa_naturalquestions-validation-2758"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9999999992857143}, {"timecode": 22, "before_eval": {"predictions": ["Doug Pruzan", "England", "May", "Mediterranean Shipping Company", "benjamin franklin", "Kitty Softpaws", "National Party", "Parliamentarians (\"Roundheads\") and Royalists (\"Cavaliers\")", "the presence of correctly oriented P waves", "are ceremonially placed on the heads of Christians on Ash Wednesday, either by being sprinkled over their heads or, in English - speaking countries, more often by being marked on their foreheads as a visible cross", "war II", "is a classic science fiction novel", "Augustus Waters", "1619", "tony blair", "\u2018 Often damaging\u2019, along with alcohol, tobacco and gambling", "June 11, 1973", "national parks", "chronological collection of critical quotations", "king edward ii", "long - standing policy of neutrality", "Cargill", "Cineplex Entertainment", "It's Always Sunny in Philadelphia", "summer of 1990 and continued until 1992", "March 1, 2018", "heavy W and Z bosons", "Blandings", "Dexter", "Development of Substitute Materials", "Chronicles of Barsetshire", "vast areas"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6157954545454545}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.1818181818181818, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-1883", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-1578", "mrqa_triviaqa-validation-2797", "mrqa_naturalquestions-validation-3859", "mrqa_squad-validation-2769", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-803", "mrqa_naturalquestions-validation-190", "mrqa_squad-validation-10459", "mrqa_triviaqa-validation-2179"], "after_eval": {"predictions": ["Doug Pruzan", "English language patronymic surname", "Thursday", "Mediterranean Shipping Company", "benjamin coBBETT", "Kitty Softpaws", "Nationals", "Parliamentarians (\"Roundheads\") and Royalists (\"Cavaliers\")", "the presence of correctly oriented P waves", "reminds worshippers of their sinfulness and mortality and thus, implicitly, of their need to repent in time", "smersh", "jules verne", "17 - year - old Augustus Waters, an ex-basketball player and amputee", "1547", "tony blair", "often damaging", "July 26, 1959", "national parks", "chronological collection of critical quotations", "edward ii", "long - standing policy of neutrality", "United Healthcare", "AMC Theatres", "It's Always Sunny in Philadelphia", "summer of 1990 and continued until 1992", "September 21, 2017", "weak force", "pig", "Yolanda", "Development of Substitute Materials", "Chronicles of Barsetshire", "vast areas"], "metric_results": {"EM": 0.90625, "QA-F1": 0.921875}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-5698", "before_prediction": "benjamin franklin", "after_prediction": "benjamin coBBETT"}, {"id": "mrqa_hotpotqa-validation-3944", "before_prediction": "1619", "after_prediction": "1547"}, {"id": "mrqa_hotpotqa-validation-1929", "before_prediction": "Dexter", "after_prediction": "Yolanda"}], "retained_ids": ["mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-7346", "mrqa_squad-validation-2884", "mrqa_hotpotqa-validation-2959", "mrqa_naturalquestions-validation-7731", "mrqa_triviaqa-validation-4731", "mrqa_squad-validation-8280", "mrqa_hotpotqa-validation-5655", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-1328", "mrqa_hotpotqa-validation-2448", "mrqa_squad-validation-2828"], "fixed_ids": ["mrqa_naturalquestions-validation-1883", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-1578", "mrqa_triviaqa-validation-2797", "mrqa_naturalquestions-validation-3859", "mrqa_squad-validation-2769", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-803", "mrqa_naturalquestions-validation-190", "mrqa_squad-validation-10459", "mrqa_triviaqa-validation-2179"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.8421052627146814}, {"timecode": 23, "before_eval": {"predictions": ["an additional $105 billion in growth to the country's economy over five years", "mono", "25 nm", "austin seven", "Dirty Dancing", "red", "Luna Park", "Best Animated Feature", "member states", "381.6 days", "nine", "NASA's", "yellow", "Khrushchev", "Jack Cassidy", "malaria", "Tata Consultancy Services in Kochi", "Leinster", "1939", "the 2017 / 18 Divisional Round game against the New Orleans Saints", "possibly 1707", "Sunnyside", "begins in central Nigeria in July bringing with it high humidity, heavy cloud cover and heavy rainfall which can be daily occurrence lasting till September when the monsoons gradually begin retreating southward to the southern part of Nigeria", "live and let die", "synovial joint", "moffit", "Democritus", "Santa Clara Marriott", "daniel barenboim", "political power generated by wealth by certain groups to shape government policies", "types of reductions", "Ted Ginn Jr."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6718018501106736}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, false, true, false, false, true, true], "QA-F1": [0.3076923076923077, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.6666666666666666, 1.0, 0.2380952380952381, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.0, 0.5882352941176471, 1.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-7389", "mrqa_triviaqa-validation-3716", "mrqa_hotpotqa-validation-4348", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-220", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-3052", "mrqa_squad-validation-2420", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-1731", "mrqa_hotpotqa-validation-2340", "mrqa_triviaqa-validation-6781", "mrqa_squad-validation-7481"], "after_eval": {"predictions": ["$105 billion", "kissing disease", "25 nm", "austin seven", "dirty dancing", "paintwork where it becomes seriously corroded", "Steeplechase Park", "Academy Award for Best Animated Feature", "member states", "381.6 days", "nine", "NASA's", "yellow", "Khrushchev", "Jack Cassidy", "malaria", "Mumbai", "the east of Ireland", "1940", "2017 / 18", "1707", "Sunnyside", "arrives in central Nigeria in July", "geoff keegan", "Incudomalleolar joint ( more correctly called incudomallear joint )", "moffit", "Leucippus", "Santa Clara Marriott", "beethoven", "political power", "types of reductions", "Corey Brown"], "metric_results": {"EM": 0.875, "QA-F1": 0.9007352941176471}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8235294117647058, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-1617", "before_prediction": "red", "after_prediction": "paintwork where it becomes seriously corroded"}, {"id": "mrqa_triviaqa-validation-571", "before_prediction": "live and let die", "after_prediction": "geoff keegan"}, {"id": "mrqa_squad-validation-769", "before_prediction": "Ted Ginn Jr.", "after_prediction": "Corey Brown"}], "retained_ids": ["mrqa_squad-validation-8850", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-3933", "mrqa_squad-validation-4118", "mrqa_hotpotqa-validation-2741", "mrqa_squad-validation-542", "mrqa_squad-validation-4228", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-validation-3714", "mrqa_hotpotqa-validation-1782", "mrqa_triviaqa-validation-3595", "mrqa_squad-validation-4662", "mrqa_triviaqa-validation-3420", "mrqa_squad-validation-327", "mrqa_squad-validation-1750"], "fixed_ids": ["mrqa_squad-validation-7389", "mrqa_triviaqa-validation-3716", "mrqa_hotpotqa-validation-4348", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-220", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-3052", "mrqa_squad-validation-2420", "mrqa_naturalquestions-validation-2212", "mrqa_hotpotqa-validation-2340", "mrqa_triviaqa-validation-6781", "mrqa_squad-validation-7481"], "unfixed_ids": ["mrqa_naturalquestions-validation-1731"], "instant_fixing_rate": 0.9285714285714286, "instant_retention_rate": 0.8333333328703704}, {"timecode": 24, "before_eval": {"predictions": ["2017 -- 15", "NADP+ though sometimes they can flow back down more H+-pumping electron transport chains to transport more hydrogen ions into the thylakoid space to generate more ATP", "Pitt", "alchemy", "WBC and lineal titles", "spice islands", "Saturday", "Albany", "They arranged for Israel to pull back from the Sinai Peninsula and the Golan Heights", "1991", "J.R. R. Tolkien", "John Elway", "Instagram's own account", "study for four years", "31", "He is from Pago Pago, American Samoa and played college football at Oregon.", "is an unofficial title sometimes given to new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "factorial primes", "coupe", "Fa Ze YouTubers", "dante", "Muslim", "along the eastern coast of the continent, from Nova Scotia and Newfoundland in the north, to Georgia in the south", "order of order", "Club Deportivo Castell\u00f3n", "between 1770 and 1848", "12\u20134", "more complex than sponges, about as complex as cnidarians (jellyfish, sea anemones, etc) and less complex than bilaterians", "Anne Fletcher starring Channing Tatum and Jenna Dewan", "Mission Specialist for mission STS-51-L.", "it will retreat to its den and winter will persist for six more weeks", "france"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5504851531447277}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, false, true, true, false, false, true, false, true, false, false, false, false, true], "QA-F1": [0.0, 0.07692307692307693, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.4, 0.33333333333333337, 0.5957446808510638, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8444", "mrqa_squad-validation-8876", "mrqa_triviaqa-validation-3678", "mrqa_hotpotqa-validation-4444", "mrqa_squad-validation-5399", "mrqa_hotpotqa-validation-3247", "mrqa_naturalquestions-validation-10381", "mrqa_squad-validation-8473", "mrqa_hotpotqa-validation-309", "mrqa_squad-validation-8979", "mrqa_naturalquestions-validation-3297", "mrqa_squad-validation-10148", "mrqa_triviaqa-validation-5304", "mrqa_naturalquestions-validation-3504", "mrqa_squad-validation-4417", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-1814", "mrqa_naturalquestions-validation-6508"], "after_eval": {"predictions": ["2003", "NADP+", "Pitt", "alchemists", "WBO lightweight title", "spice islands", "Saturday", "Albany ( in the Folio version )", "They arranged for Israel to pull back from the Sinai Peninsula and the Golan Heights.", "1971", "J.R. R. Tolkien", "John Elway", "Instagram", "join a vocational youth/village polytechnic", "31", "Pago Pago, American Samoa", "comparable to the seven Wonders of the World", "2p + 1 with p prime", "coupe", "FaZe Rug", "dante", "Muslim", "along the coast, the settlements were growing into the interior", "order of poor ladies", "CD Castell\u00f3n", "1780 -- 1830", "finished the regular season with a 12\u20134 record, and denied the New England Patriots a chance to defend their title from Super Bowl XLIX by defeating them 20\u201318 in the AFC Championship Game", "having colloblasts", "Anne Fletcher", "STS-51-L", "if a groundhog ( Deitsch : Grundsau, Grunddax, Dax ) emerging from its burrow on this day sees a shadow due to clear weather, it will retreat to its den and winter will", "france"], "metric_results": {"EM": 0.8125, "QA-F1": 0.9009067222140411}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0689655172413793, 1.0, 1.0, 1.0, 0.9180327868852458, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-9011", "before_prediction": "Albany", "after_prediction": "Albany ( in the Folio version )"}, {"id": "mrqa_hotpotqa-validation-3080", "before_prediction": "He is from Pago Pago, American Samoa and played college football at Oregon.", "after_prediction": "Pago Pago, American Samoa"}, {"id": "mrqa_hotpotqa-validation-5588", "before_prediction": "Club Deportivo Castell\u00f3n", "after_prediction": "CD Castell\u00f3n"}, {"id": "mrqa_squad-validation-67", "before_prediction": "12\u20134", "after_prediction": "finished the regular season with a 12\u20134 record, and denied the New England Patriots a chance to defend their title from Super Bowl XLIX by defeating them 20\u201318 in the AFC Championship Game"}], "retained_ids": ["mrqa_squad-validation-10261", "mrqa_triviaqa-validation-3340", "mrqa_hotpotqa-validation-3290", "mrqa_squad-validation-3741", "mrqa_squad-validation-384", "mrqa_triviaqa-validation-667", "mrqa_naturalquestions-validation-2164", "mrqa_triviaqa-validation-3847", "mrqa_squad-validation-6204", "mrqa_triviaqa-validation-1306"], "fixed_ids": ["mrqa_naturalquestions-validation-8444", "mrqa_squad-validation-8876", "mrqa_triviaqa-validation-3678", "mrqa_hotpotqa-validation-4444", "mrqa_squad-validation-5399", "mrqa_naturalquestions-validation-10381", "mrqa_squad-validation-8473", "mrqa_hotpotqa-validation-309", "mrqa_squad-validation-8979", "mrqa_naturalquestions-validation-3297", "mrqa_squad-validation-10148", "mrqa_triviaqa-validation-5304", "mrqa_naturalquestions-validation-3504", "mrqa_squad-validation-4417", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-1814"], "unfixed_ids": ["mrqa_hotpotqa-validation-3247", "mrqa_naturalquestions-validation-6508"], "instant_fixing_rate": 0.8888888888888888, "instant_retention_rate": 0.7142857137755102}, {"timecode": 25, "before_eval": {"predictions": ["synchronized swimming", "afghanistan", "over 50 million singles", "states'rights to expand slavery", "between 1923 and 1925", "Orlando\u2013Kissimmee\u2013 Sanford, Florida Metropolitan Statistical Area", "January 19, 1962", "Frigate class", "Buck Barrow in Bonnie and Clyde", "until all available list seats are allocated", "pomeranian", "the move from the manufacturing sector to the service sector", "moreton Bay", "Peter Davison, Colin Baker and Sylvester McCoy", "August 14, 1848", "higher rates of health and social problems, and lower rates of social goods, a lower level of economic utility in society from resources devoted on high-end consumption,", "In at least some species, juveniles are capable of reproduction before reaching the adult size and shape", "toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "the way they used `` rule '' and `` method '' to go about their religious affairs", "music by Marvin Hamlisch, lyrics by Edward Kleban", "2,664", "iPhone 6", "chute", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "cleaning, catering and security porterage business", "Symphony No. 7 in F major, Opus 24", "Garonne and Dordogne", "1603", "ranked above the two personal physicians of the Emperor", "flute", "make direct amends to such people wherever possible", "Chicago Cubs"], "metric_results": {"EM": 0.15625, "QA-F1": 0.3414286523661524}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.3333333333333333, 0.25, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.07407407407407407, 0.9333333333333333, 0.4615384615384615, 0.923076923076923, 0.0, 1.0, 1.0, 0.2857142857142857, 0.3636363636363636, 0.8, 0.5454545454545454, 0.0, 1.0, 0.22222222222222224, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-5788", "mrqa_hotpotqa-validation-1818", "mrqa_naturalquestions-validation-10169", "mrqa_naturalquestions-validation-4072", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-3977", "mrqa_triviaqa-validation-6091", "mrqa_squad-validation-9532", "mrqa_triviaqa-validation-6941", "mrqa_squad-validation-7382", "mrqa_triviaqa-validation-7336", "mrqa_hotpotqa-validation-4720", "mrqa_squad-validation-7301", "mrqa_squad-validation-4637", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-859", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-7309", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-499", "mrqa_triviaqa-validation-2098", "mrqa_squad-validation-6328", "mrqa_triviaqa-validation-6221", "mrqa_naturalquestions-validation-2481", "mrqa_triviaqa-validation-1259"], "after_eval": {"predictions": ["gymnastic", "abram", "40 million", "loyalty to the U.S. Constitution", "1923", "Orlando\u2013Kissimmee\u2013 Sanford", "January 19, 1962", "PPA, Pattugliatore Polivalente d'Altura", "crimson tide", "iteratively", "geese", "effect", "pacific region", "Peter Davison, Colin Baker and Sylvester McCoy", "February 14, 1859", "lower", "In at least some species, juveniles are capable of reproduction before reaching the adult size and shape", "breaded chicken patty", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "musical", "2,664", "opinion: why the iPhone 6 bends and why it won\u2019t be an issue if Apple addressed it properly", "a chute beneath his or her feet", "Claims adjuster", "cleaning, catering and security", "Symphony No. 7", "gironde", "1603", "Ranked positions", "ballet", "We admitted we were powerless over alcohol -- that our lives had become unmanageable.", "baseball"], "metric_results": {"EM": 0.875, "QA-F1": 0.9111111111111111}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-2454", "before_prediction": "iPhone 6", "after_prediction": "opinion: why the iPhone 6 bends and why it won\u2019t be an issue if Apple addressed it properly"}], "retained_ids": ["mrqa_hotpotqa-validation-3369", "mrqa_squad-validation-7677", "mrqa_hotpotqa-validation-4282", "mrqa_naturalquestions-validation-6545"], "fixed_ids": ["mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-5788", "mrqa_hotpotqa-validation-1818", "mrqa_naturalquestions-validation-10169", "mrqa_naturalquestions-validation-4072", "mrqa_hotpotqa-validation-3977", "mrqa_triviaqa-validation-6091", "mrqa_squad-validation-9532", "mrqa_triviaqa-validation-6941", "mrqa_squad-validation-7382", "mrqa_triviaqa-validation-7336", "mrqa_hotpotqa-validation-4720", "mrqa_squad-validation-7301", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-859", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-7309", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-499", "mrqa_triviaqa-validation-2098", "mrqa_squad-validation-6328", "mrqa_naturalquestions-validation-2481", "mrqa_triviaqa-validation-1259"], "unfixed_ids": ["mrqa_hotpotqa-validation-5762", "mrqa_squad-validation-4637", "mrqa_triviaqa-validation-6221"], "instant_fixing_rate": 0.8888888888888888, "instant_retention_rate": 0.7999999984}, {"timecode": 26, "before_eval": {"predictions": ["to Jewish audiences", "south of the Kancamagus Highway", "an investment technique outlined by Joel Greenblatt that uses the principles of value investing", "kelly gang", "City and County of Honolulu", "1910", "those who refuse vetting", "Catch Me Who Can", "jazz", "tennis", "4,000", "Khagan", "casket letters", "birmingham", "sri lanka", "The Simpsons Spin-Off Showcase", "The planner Raymond Unwin and the architect Barry Parker", "Riverside", "a portion of Grainger Town was demolished in the 1960s to make way for the Eldon Square Shopping Centre", "Albany High School for Educating People of Color", "Lalbagh Fort at Dhaka", "First Class", "Anakin Skywalker ( formerly Darth Vader )", "jury nullification", "the closing scene of the final episode of the first season", "Anglican", "hattie mcdaniel", "scharnhorst", "hypnotist", "located within nine coastal southern Nigerian states", "Necessity-based entrepreneurship", "January 11, 1755 or 1757July"], "metric_results": {"EM": 0.5, "QA-F1": 0.6369859307359307}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.4, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5454545454545454, 0.0, 0.8, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-2184", "mrqa_triviaqa-validation-6772", "mrqa_squad-validation-7435", "mrqa_triviaqa-validation-6689", "mrqa_hotpotqa-validation-5", "mrqa_triviaqa-validation-3472", "mrqa_hotpotqa-validation-2436", "mrqa_squad-validation-5184", "mrqa_hotpotqa-validation-1446", "mrqa_naturalquestions-validation-800", "mrqa_hotpotqa-validation-4585", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7801", "mrqa_squad-validation-7319"], "after_eval": {"predictions": ["to Jewish audiences", "south", "outlined by Joel Greenblatt", "kelly", "City and County of Honolulu", "1910\u20131940", "those who refuse vetting", "Salamanca", "jazz", "margaret smith court", "3,677 seated", "the founder of the Yuan dynasty", "catherine and heathcliff", "birmingham", "sri lanka", "the Simpson family", "The planner Raymond Unwin and the architect Barry Parker", "Riverside", "Shopping Centre", "Albany High School", "charbagh", "Sergeant First Class", "Anakin Skywalker", "inform the jury and the public of the political circumstances", "the closing scene of the final episode of the first season", "The Church of England", "hattie mcdaniel", "scharnhorst", "hypnotist", "sitting directly on the Gulf of Guinea on the Atlantic Ocean in Nigeria", "opportunity-based entrepreneurship", "January 11, 1755 or 1757July"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9131944444444444}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-3176", "before_prediction": "Catch Me Who Can", "after_prediction": "Salamanca"}, {"id": "mrqa_squad-validation-6148", "before_prediction": "Khagan", "after_prediction": "the founder of the Yuan dynasty"}, {"id": "mrqa_squad-validation-6837", "before_prediction": "jury nullification", "after_prediction": "inform the jury and the public of the political circumstances"}], "retained_ids": ["mrqa_naturalquestions-validation-3027", "mrqa_hotpotqa-validation-4553", "mrqa_squad-validation-1994", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6224", "mrqa_naturalquestions-validation-2214", "mrqa_squad-validation-2428", "mrqa_naturalquestions-validation-5476", "mrqa_triviaqa-validation-5839", "mrqa_triviaqa-validation-5050", "mrqa_triviaqa-validation-3393", "mrqa_hotpotqa-validation-945"], "fixed_ids": ["mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-2184", "mrqa_triviaqa-validation-6772", "mrqa_squad-validation-7435", "mrqa_triviaqa-validation-6689", "mrqa_hotpotqa-validation-5", "mrqa_triviaqa-validation-3472", "mrqa_hotpotqa-validation-2436", "mrqa_squad-validation-5184", "mrqa_hotpotqa-validation-1446", "mrqa_naturalquestions-validation-800", "mrqa_hotpotqa-validation-4585", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7801", "mrqa_squad-validation-7319"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.8124999994921874}, {"timecode": 27, "before_eval": {"predictions": ["alsivar", "2000", "blackberry", "horsehead", "Big Mamie", "orinoco river", "Teamsters leader", "clear substances with a light sky-blue color", "wat tyler", "1963", "Zaza Pachulia", "the inner chloroplast membrane", "renoir", "sports tourism", "Third-party channels", "the third season", "more fundamental electroweak interaction", "Cost of construction", "talc, 2-gypsum, 3-calcit, 4-corundum", "A simple iron boar crest adorns the top of this helmet associating it with the Benty Grange helmet and the Guilden Morden boar", "the University of Northumbria at Newcastle in 1992 as part of the UK-wide process in which polytechnics became new universities", "australia", "David", "award a touchback on kickoffs at the 25 - yard line", "named after the Swedish astronomer Anders Celsius", "7,000", "lion, leopard, buffalo, rhinoceros, and elephant", "faith", "cliff thorburn", "possible combinations of two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "peter paul rubens", "badgers"], "metric_results": {"EM": 0.75, "QA-F1": 0.8328392621870883}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.34782608695652173, 0.3636363636363636, 1.0, 1.0, 0.6666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6060606060606061, 1.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-2635", "mrqa_squad-validation-7711", "mrqa_hotpotqa-validation-158", "mrqa_triviaqa-validation-5962", "mrqa_hotpotqa-validation-1226", "mrqa_squad-validation-5337", "mrqa_naturalquestions-validation-9105", "mrqa_naturalquestions-validation-2893"], "after_eval": {"predictions": ["hair", "2000", "blackberry", "horsehead", "Big Mamie", "orinoco river", "Teamsters leader", "clear substances with a light sky-blue color caused by absorption in the red (in contrast with the blue color of the sky, which is due to Rayleigh scattering of blue light)", "wat tyler", "2009", "Zaza Pachulia", "the inner chloroplast membrane", "renoir", "sports tourism", "Third-party channels which opt for encryption receive discounts ranging from reduced price to free EPG entries, free carriage on a B SkyB leased transponder, or actual payment for being carried", "third", "electroweak interaction", "Cost of construction", "gypsum", "A simple iron boar crest", "polytechnics became new universities", "australian", "David", "a touchback on kickoffs at the 25 - yard line instead of the previous 20 - yard lines", "the Swedish astronomer Anders Celsius ( 1701 -- 1744 ), who developed a similar temperature scale", "7,000", "lion, leopard, buffalo, rhinoceros, and elephant", "faith", "cliff thorburn", "produced with constant technology and resources per unit of time", "peter paul rubens", "badgers"], "metric_results": {"EM": 0.875, "QA-F1": 0.9339479306272476}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12903225806451613, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9230769230769231, 0.4705882352941177, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-3539", "before_prediction": "clear substances with a light sky-blue color", "after_prediction": "clear substances with a light sky-blue color caused by absorption in the red (in contrast with the blue color of the sky, which is due to Rayleigh scattering of blue light)"}, {"id": "mrqa_squad-validation-2733", "before_prediction": "Third-party channels", "after_prediction": "Third-party channels which opt for encryption receive discounts ranging from reduced price to free EPG entries, free carriage on a B SkyB leased transponder, or actual payment for being carried"}, {"id": "mrqa_naturalquestions-validation-3771", "before_prediction": "named after the Swedish astronomer Anders Celsius", "after_prediction": "the Swedish astronomer Anders Celsius ( 1701 -- 1744 ), who developed a similar temperature scale"}], "retained_ids": ["mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-348", "mrqa_triviaqa-validation-2899", "mrqa_hotpotqa-validation-994", "mrqa_triviaqa-validation-6331", "mrqa_hotpotqa-validation-2428", "mrqa_triviaqa-validation-2980", "mrqa_hotpotqa-validation-4022", "mrqa_squad-validation-8739", "mrqa_triviaqa-validation-1423", "mrqa_squad-validation-2966", "mrqa_squad-validation-10312", "mrqa_squad-validation-6855", "mrqa_triviaqa-validation-1034", "mrqa_hotpotqa-validation-3949", "mrqa_squad-validation-5125", "mrqa_squad-validation-8279", "mrqa_squad-validation-2313", "mrqa_triviaqa-validation-388", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-1507"], "fixed_ids": ["mrqa_triviaqa-validation-2635", "mrqa_squad-validation-7711", "mrqa_hotpotqa-validation-158", "mrqa_triviaqa-validation-5962", "mrqa_hotpotqa-validation-1226", "mrqa_squad-validation-5337", "mrqa_naturalquestions-validation-2893"], "unfixed_ids": ["mrqa_naturalquestions-validation-9105"], "instant_fixing_rate": 0.875, "instant_retention_rate": 0.8749999996354166}, {"timecode": 28, "before_eval": {"predictions": ["pamphlets", "Anderson Silva", "inform the jury and the public of the political circumstances", "Kusha", "poet, and writer", "Eobard Thawne", "plum brandy", "US$10 a week raise over Tesla's US$18 per week salary", "1825", "member states", "it is least affected by humidity or other weather conditions", "McKinsey's offices", "fear of public speaking", "lionel bart", "short bowel syndrome", "Ondemar Dias", "Bear McCreary", "Cincinnati and No. 3 seed Tennessee", "berlin trilogy", "John D. Rockefeller", "Ketuvim ( or `` Writings '' )", "UPS", "local talent", "Preston North End Football Club ( often shortened to PNE) is a professional association football club located in the Deepdale area of Preston, Lancashire.", "peter davison", "belgian", "contemporary accounts were exaggerations", "Lincoln assassination", "1332", "the dodo bird", "people and their thoughts are both made from `` pure energy '', and that through the process of `` like energy attracting like energy '' a person can improve their own health, wealth and personal relationships", "Stan butler"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7063081065759637}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.5, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.32, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2040816326530612, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-4919", "mrqa_triviaqa-validation-4827", "mrqa_squad-validation-1276", "mrqa_naturalquestions-validation-10495", "mrqa_triviaqa-validation-5237", "mrqa_naturalquestions-validation-7110", "mrqa_triviaqa-validation-1347", "mrqa_squad-validation-8031", "mrqa_naturalquestions-validation-10687", "mrqa_hotpotqa-validation-1251", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-5810", "mrqa_naturalquestions-validation-7821"], "after_eval": {"predictions": ["critical pamphlets", "Anderson Silva", "inform the jury and the public of the political circumstances", "Kusha", "poet", "Professor Eobard Thawne", "plum", "a US$10 a week raise", "1825", "contributed by member states on a voluntary basis", "oboe", "McKinsey's offices", "fear of public speaking", "lionel bart", "prolonged diarrhea", "Ondemar Dias", "Bear McCreary", "UMBC", "riddick bowe", "Charles L. Hutchinson", "Song of Songs", "UPS", "local talent", "North End Football Club", "peter davison", "canada", "contemporary accounts were exaggerations", "abraham lincoln", "1332", "extinct", "the belief that by focusing on negative or negative thoughts people can bring positive or negative experiences into their life", "larky bus driver Stan Butler"], "metric_results": {"EM": 0.84375, "QA-F1": 0.927670739348371}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9473684210526315, 0.5714285714285715]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-2291", "before_prediction": "pamphlets", "after_prediction": "critical pamphlets"}, {"id": "mrqa_hotpotqa-validation-5637", "before_prediction": "poet, and writer", "after_prediction": "poet"}, {"id": "mrqa_triviaqa-validation-2953", "before_prediction": "the dodo bird", "after_prediction": "extinct"}, {"id": "mrqa_triviaqa-validation-4308", "before_prediction": "Stan butler", "after_prediction": "larky bus driver Stan Butler"}], "retained_ids": ["mrqa_hotpotqa-validation-1390", "mrqa_squad-validation-6835", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-4352", "mrqa_hotpotqa-validation-5110", "mrqa_triviaqa-validation-3760", "mrqa_triviaqa-validation-6250", "mrqa_naturalquestions-validation-4127", "mrqa_squad-validation-4309", "mrqa_naturalquestions-validation-9185", "mrqa_triviaqa-validation-7669", "mrqa_squad-validation-5086", "mrqa_triviaqa-validation-4584", "mrqa_squad-validation-4921", "mrqa_squad-validation-8190"], "fixed_ids": ["mrqa_naturalquestions-validation-4919", "mrqa_triviaqa-validation-4827", "mrqa_squad-validation-1276", "mrqa_naturalquestions-validation-10495", "mrqa_triviaqa-validation-5237", "mrqa_naturalquestions-validation-7110", "mrqa_triviaqa-validation-1347", "mrqa_squad-validation-8031", "mrqa_naturalquestions-validation-10687", "mrqa_hotpotqa-validation-1251", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-5810"], "unfixed_ids": ["mrqa_naturalquestions-validation-7821"], "instant_fixing_rate": 0.9230769230769231, "instant_retention_rate": 0.7894736837950138}, {"timecode": 29, "before_eval": {"predictions": ["poisonous", "886 AD", "his own projects", "Can-Am Championship", "a virtual sex game", "Tokyo", "linebacker Danny Trevathan recovered on the Broncos 40-yard line", "parallelogram rule of vector addition", "abram state", "40 gold rings", "Neutron sources ensure a constant minimal population of neutrons in the reactor core", "canada", "lower-pressure cylinders", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "the local administrative structure of past Chinese dynasties", "Doctorin' the Tardis", "National Basketball Development League (NBDL)", "gillingham", "St. Mary's County", "T. J. Ward", "2,615", "Pyeongchang", "Kaep (disambiguation)", "Cryptanalysis attacks", "Sordid Lives", "historical contributions to the development of modern architecture and furniture", "south korea", "abram williams", "the smallest subfield", "ginger", "53%", "normal grana and thylakoids"], "metric_results": {"EM": 0.1875, "QA-F1": 0.292781279178338}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true], "QA-F1": [0.0, 0.0, 0.4, 0.0, 0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 0.0, 0.0, 0.14285714285714288, 0.0, 0.0, 0.47058823529411764, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.13333333333333333, 0.0, 0.16666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-4708", "mrqa_naturalquestions-validation-866", "mrqa_squad-validation-1249", "mrqa_hotpotqa-validation-3497", "mrqa_triviaqa-validation-6632", "mrqa_naturalquestions-validation-6610", "mrqa_squad-validation-814", "mrqa_squad-validation-10408", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-3103", "mrqa_naturalquestions-validation-8653", "mrqa_triviaqa-validation-7032", "mrqa_squad-validation-3344", "mrqa_naturalquestions-validation-5826", "mrqa_hotpotqa-validation-2928", "mrqa_triviaqa-validation-1280", "mrqa_naturalquestions-validation-5961", "mrqa_squad-validation-772", "mrqa_hotpotqa-validation-3031", "mrqa_naturalquestions-validation-9765", "mrqa_hotpotqa-validation-5383", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-7567", "mrqa_hotpotqa-validation-5709", "mrqa_triviaqa-validation-2709", "mrqa_triviaqa-validation-814"], "after_eval": {"predictions": ["holly", "the 1960s", "His patents", "Formula One", "360", "Tokyo for the 2020 Summer Olympics", "DeMarcus Ware", "parallelogram", "evolution", "364", "startup neutron source", "van gough", "cylinder", "storage of minerals", "the local administrative structure of past Chinese dynasties", "Doctorin' the Tardis", "National Basketball Development League", "kent", "Washington metropolitan area", "Sanders", "The population was 2,615", "Beijing", "an American football quarterback", "recover many kinds of passwords using methods such as network packet sniffing, cracking various password hashes by using methods like dictionary attacks, brute force and cryptanalysis attacks", "The Man", "husband and wife", "2002", "arthur", "the smallest subfield of a field", "heartburn", "53%", "normal grana and thylakoids"], "metric_results": {"EM": 0.875, "QA-F1": 0.9253787878787878}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9454545454545454, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-1353", "before_prediction": "south korea", "after_prediction": "2002"}, {"id": "mrqa_squad-validation-9036", "before_prediction": "the smallest subfield", "after_prediction": "the smallest subfield of a field"}], "retained_ids": ["mrqa_squad-validation-8075", "mrqa_squad-validation-7914", "mrqa_squad-validation-7445", "mrqa_squad-validation-8873"], "fixed_ids": ["mrqa_triviaqa-validation-4708", "mrqa_naturalquestions-validation-866", "mrqa_squad-validation-1249", "mrqa_hotpotqa-validation-3497", "mrqa_naturalquestions-validation-6610", "mrqa_squad-validation-814", "mrqa_squad-validation-10408", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-3103", "mrqa_naturalquestions-validation-8653", "mrqa_triviaqa-validation-7032", "mrqa_squad-validation-3344", "mrqa_naturalquestions-validation-5826", "mrqa_hotpotqa-validation-2928", "mrqa_triviaqa-validation-1280", "mrqa_naturalquestions-validation-5961", "mrqa_squad-validation-772", "mrqa_hotpotqa-validation-3031", "mrqa_naturalquestions-validation-9765", "mrqa_hotpotqa-validation-5383", "mrqa_naturalquestions-validation-7567", "mrqa_hotpotqa-validation-5709", "mrqa_triviaqa-validation-2709", "mrqa_triviaqa-validation-814"], "unfixed_ids": ["mrqa_triviaqa-validation-6632", "mrqa_naturalquestions-validation-4572"], "instant_fixing_rate": 0.9230769230769231, "instant_retention_rate": 0.6666666655555555}, {"timecode": 30, "before_eval": {"predictions": ["the judiciary", "Dutch House of Orange-Nassau", "the Mayor's son", "abram lincoln", "Basil Fawlty", "Marty Ingels", "BBC UKTV", "stieg Larsson", "demographics and economic ties", "3", "the most recent Super Bowl champion", "narcolepsy", "arctic monkeys", "formula 1", "Jurassic Park", "usernames, passwords, commands and data can be read by anyone able to perform packet capture ( sniffing ) on the network", "A computer program is a collection of instructions that performs a specific task when executed by a computer. A computer requires programs to function.", "National Party", "abylon", "arthur", "largest source of foreign direct investment", "national network", "The Coral Sea is a marginal sea of the South Pacific off the northeast coast of Australia, and classified as an interim Australian bioregion.", "7", "New Jersey", "John-a-kite", "CCH Pounder", "indira priyadarshini gandhi", "state-franchised", "skylab", "spain", "to comply with the Do - Not - Call Implementation Act of 2003"], "metric_results": {"EM": 0.46875, "QA-F1": 0.48759920634920634}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.22222222222222224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.1904761904761905, 0.0, 0.0, 0.0, 1.0, 1.0, 0.1904761904761905, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-9608", "mrqa_triviaqa-validation-3920", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-4079", "mrqa_squad-validation-7793", "mrqa_triviaqa-validation-1921", "mrqa_squad-validation-2577", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-6888", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-9852", "mrqa_squad-validation-2885", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7090", "mrqa_naturalquestions-validation-4710", "mrqa_hotpotqa-validation-3333"], "after_eval": {"predictions": ["judges", "Dutch House of Orange-Nassau", "Jesse McCartney as JoJo, the Mayor's son", "john brown", "polly", "Martin Ingerman", "SyFy", "daniel craig", "historical political divisions", "3 years 280 days", "Super Bowl champion", "narcolepsy", "alex turner", "imola", "gennaro", "all transmissions", "A computer program", "The Greens", "babylon", "surtsey", "a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda", "national network", "South Pacific", "7", "New Jersey", "Netflix", "Lucas Black", "indira Priyadarshini gandhi", "state-franchised", "skylab", "spain", "to comply with the Do - Not - Call Implementation Act of 2003"], "metric_results": {"EM": 0.84375, "QA-F1": 0.907015435222672}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10526315789473684, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-4578", "before_prediction": "3", "after_prediction": "3 years 280 days"}, {"id": "mrqa_naturalquestions-validation-3209", "before_prediction": "the most recent Super Bowl champion", "after_prediction": "Super Bowl champion"}, {"id": "mrqa_triviaqa-validation-1722", "before_prediction": "Jurassic Park", "after_prediction": "gennaro"}, {"id": "mrqa_squad-validation-8451", "before_prediction": "largest source of foreign direct investment", "after_prediction": "a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda"}], "retained_ids": ["mrqa_hotpotqa-validation-2559", "mrqa_triviaqa-validation-2750", "mrqa_triviaqa-validation-7684", "mrqa_squad-validation-6961", "mrqa_hotpotqa-validation-4604", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-3134", "mrqa_hotpotqa-validation-5604", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-2091", "mrqa_naturalquestions-validation-10328"], "fixed_ids": ["mrqa_naturalquestions-validation-4645", "mrqa_triviaqa-validation-3920", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-4079", "mrqa_squad-validation-7793", "mrqa_triviaqa-validation-1921", "mrqa_squad-validation-2577", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-6888", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-9852", "mrqa_squad-validation-2885", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7090", "mrqa_naturalquestions-validation-4710", "mrqa_hotpotqa-validation-3333"], "unfixed_ids": ["mrqa_naturalquestions-validation-9608"], "instant_fixing_rate": 0.9411764705882353, "instant_retention_rate": 0.7333333328444444}, {"timecode": 31, "before_eval": {"predictions": ["Yolanda Sald\u00edvar", "Andreas", "sarajevo", "football", "Newell Highway between Melbourne and Brisbane", "tenth planet", "4", "shopping", "explaining their actions", "Andrew Adamson", "liszt Strauss Wagner Dvorak", "Naimans", "emissions resulting from human activities are substantially increasing the atmospheric concentrations of the greenhouse gases, resulting on average in an additional warming of the Earth's surface", "parthenon", "Britain", "encourage growth", "Ibbi-Sipish", "strictly come dancing", "Polish", "the Falange", "cole albert", "1967", "16,000 species", "Washington Street between Boylston Street and Kneeland Street", "1978", "two", "\"Fudge\"", "His frustration with the atmosphere in the group at that time -- namely, Paul McCartney's over-assertiveness and criticism of his guitar playing", "1977", "Paul the Apostle, later cited by John Smith in Jamestown, Virginia, and by Lenin during the Russian Revolution", "lusitania", "the variety of occupations necessary to sustain the community as distinct from the indigenous population"], "metric_results": {"EM": 0.625, "QA-F1": 0.7115244708994709}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.37037037037037035, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.6428571428571429, 0.0, 0.2222222222222222, 1.0, 0.13333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-1385", "mrqa_hotpotqa-validation-2762", "mrqa_triviaqa-validation-3300", "mrqa_squad-validation-8589", "mrqa_hotpotqa-validation-1444", "mrqa_hotpotqa-validation-3233", "mrqa_hotpotqa-validation-3728", "mrqa_squad-validation-932", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-743", "mrqa_naturalquestions-validation-4500", "mrqa_squad-validation-3106"], "after_eval": {"predictions": ["Guadalupe \"Lupe\" Ontiveros", "Andreas", "Bolero", "football", "Newell Highway", "Mondas", "4", "shopping", "explaining their actions", "Andrew Adamson", "waltz king", "Naimans", "warming of the Earth's surface", "parthenon", "Britain", "encourage growth", "Ibrium", "strictly come dancing", "Polish", "the Falange, the CEDA, and two rival monarchist claimants : the Alfonsists and the Carlists", "cole albert", "1967", "16,000 species", "Washington Street", "8 November 1978", "five", "\"Fudge\"", "his frustration with the atmosphere in the group at that time", "barbarella", "John Smith", "unrestricted submarine warfare", "economic separation"], "metric_results": {"EM": 0.875, "QA-F1": 0.8806818181818181}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-4209", "before_prediction": "sarajevo", "after_prediction": "Bolero"}, {"id": "mrqa_triviaqa-validation-457", "before_prediction": "tenth planet", "after_prediction": "Mondas"}, {"id": "mrqa_naturalquestions-validation-9825", "before_prediction": "the Falange", "after_prediction": "the Falange, the CEDA, and two rival monarchist claimants : the Alfonsists and the Carlists"}, {"id": "mrqa_triviaqa-validation-4524", "before_prediction": "lusitania", "after_prediction": "unrestricted submarine warfare"}], "retained_ids": ["mrqa_hotpotqa-validation-5585", "mrqa_triviaqa-validation-1050", "mrqa_triviaqa-validation-5071", "mrqa_naturalquestions-validation-5604", "mrqa_squad-validation-6734", "mrqa_hotpotqa-validation-2564", "mrqa_squad-validation-6128", "mrqa_triviaqa-validation-5513", "mrqa_naturalquestions-validation-954", "mrqa_squad-validation-7469", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-2493", "mrqa_triviaqa-validation-3286", "mrqa_triviaqa-validation-2609", "mrqa_squad-validation-4415", "mrqa_hotpotqa-validation-5307"], "fixed_ids": ["mrqa_hotpotqa-validation-1385", "mrqa_hotpotqa-validation-2762", "mrqa_triviaqa-validation-3300", "mrqa_squad-validation-8589", "mrqa_hotpotqa-validation-1444", "mrqa_hotpotqa-validation-3233", "mrqa_hotpotqa-validation-3728", "mrqa_squad-validation-932", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-743", "mrqa_naturalquestions-validation-4500", "mrqa_squad-validation-3106"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.7999999996}, {"timecode": 32, "before_eval": {"predictions": ["orbital scientific instrument package", "Orthodox Christians", "Fomento Econ\u00f3mico Mexicano, S.A.B. de C.V. doing business as FEMSA", "september", "South africa", "Thomas Middleditch", "potassium hydroxide", "In extreme circumstances, a driver may attempt to jackknife the vehicle deliberately in order to halt it following brake failure", "CD4+ and CD8+ (\u03b1\u03b2) T cells", "generally paid on graduated scales, with income depending on experience. Teachers with more experience and higher education earn more than those with a standard bachelor's degree and certificate", "usa", "Point of Entry", "bridge", "u", "Science Magazine", "England national team", "poverty", "Tough Enough", "France's Legislative Assembly", "5,922", "June 4, 1931", "is a 2016 science fiction psychological horror", "leg injury", "Saint Nicholas ( a fourth - century Greek bishop and gift - giver of Myra ), the British figure of Father Christmas and the Dutch figure of Sinterklaas ( himself also based on Saint Nicholas )", "Stern-Plaza", "Jimmy Ellis", "1991", "may", "Dallas", "Nairobi", "During the last Ice Age", "Neon City ( also known as Anno 2053 in Italy, Neonski Grad in Serbia) is a 1991 Canadian post-apocalyptic science fiction film"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7663439407606386}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.6666666666666666, 0.08695652173913045, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.13793103448275862, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.18181818181818182]}}, "error_ids": ["mrqa_hotpotqa-validation-572", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-5510", "mrqa_squad-validation-6602", "mrqa_squad-validation-2234", "mrqa_hotpotqa-validation-305", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-7049", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-5557"], "after_eval": {"predictions": ["orbital scientific instrument package", "Orthodox Christians", "Fomento Econ\u00f3mico Mexicano", "september", "South africa", "Thomas Middleditch", "k", "If a vehicle towing a trailer skids", "alternative T cell receptor (TCR)", "relatively low salaries", "usa", "Point of Entry", "bridge", "u", "Science Magazine", "Premier League club Manchester United and the England national team", "poverty", "Space is the Place", "France's Legislative Assembly", "5,922", "December 5, 1991", "is a 2016 science fiction psychological horror", "leg injury", "Saint Nicholas ( a fourth - century Greek bishop and gift - giver of Myra ), the British figure of Father Christmas and the Dutch figure of Sinterklaas ( himself also based on Saint Nicholas )", "Stern-Plaza", "Jimmy Ellis", "1991", "may", "Dallas", "Nairobi, Kenya", "During the last Ice Age", "Anno 2053"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9284174876847291}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.13793103448275862, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-2910", "before_prediction": "June 4, 1931", "after_prediction": "December 5, 1991"}], "retained_ids": ["mrqa_squad-validation-3887", "mrqa_squad-validation-6744", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3066", "mrqa_naturalquestions-validation-4761", "mrqa_triviaqa-validation-7156", "mrqa_hotpotqa-validation-1119", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2245", "mrqa_squad-validation-8617", "mrqa_squad-validation-8095", "mrqa_hotpotqa-validation-3508", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-2985", "mrqa_triviaqa-validation-1079", "mrqa_hotpotqa-validation-4221", "mrqa_naturalquestions-validation-5960"], "fixed_ids": ["mrqa_hotpotqa-validation-572", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-5510", "mrqa_squad-validation-6602", "mrqa_squad-validation-2234", "mrqa_hotpotqa-validation-305", "mrqa_naturalquestions-validation-5631", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-5557"], "unfixed_ids": ["mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-7049"], "instant_fixing_rate": 0.8181818181818182, "instant_retention_rate": 0.9523809519274377}, {"timecode": 33, "before_eval": {"predictions": ["the defeat of Napoleon", "Boston Herald", "1967", "amount charged by a bookmaker, or \"bookie\" for a shark's loan", "largest Filipino American community", "Roger Maris -- collectively known as the M&M Boys -- are the only teammates to reach the 50 home run club in the same season", "tintin", "a hyper - active kinase, that confers an aberrant, ligand - independent, non-regulated growth stimulus to the cancer cells", "higher rates", "J\u0101nis Strazdi\u0146\u0161", "Chava with Fyedka", "St. Lawrence River valley", "emperor", "oakum", "Spring city", "London", "Broken Hill and Sydney", "2005", "all punishments", "Smith and Jones", "wagons", "illich ramirez sanchez", "things that are a matter of custom or expectation", "Paris", "daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia", "staying with the same group of peers for all classes", "backflow prevention", "emotional contagion", "port Arthur", "air conditioning", "William Jefferson Clinton", "Oslo county"], "metric_results": {"EM": 0.8125, "QA-F1": 0.8413596207713854}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.1904761904761905, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2092", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-3523", "mrqa_hotpotqa-validation-2588", "mrqa_triviaqa-validation-2812", "mrqa_hotpotqa-validation-1211"], "after_eval": {"predictions": ["the defeat of Napoleon", "Boston Herald", "1967", "amount charged by a bookmaker", "largest Filipino American community", "Mickey Mantle", "tintin", "a hyper - active kinase, that confers an aberrant, ligand - independent, non-regulated growth stimulus to the cancer cells", "higher", "J\u0101nis Strazdi\u0146\u0161", "Hodel", "St. Lawrence River valley", "emperor", "oakum", "Spring city", "London", "Broken Hill and Sydney", "2005", "all punishments", "Smith and Jones", "wagons", "illich ramirez sanchez", "things that are a matter of custom or expectation", "Paris", "niece", "staying with the same group of peers for all classes", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "emotional contagion", "japan", "air conditioning", "William Jefferson Clinton", "Buskerud"], "metric_results": {"EM": 0.96875, "QA-F1": 0.96875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-5297", "before_prediction": "backflow prevention", "after_prediction": "prevent any contaminants in the sink from flowing into the potable water system by siphonage"}], "retained_ids": ["mrqa_squad-validation-9984", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1573", "mrqa_squad-validation-7278", "mrqa_triviaqa-validation-1410", "mrqa_naturalquestions-validation-9271", "mrqa_squad-validation-7571", "mrqa_hotpotqa-validation-650", "mrqa_squad-validation-10180", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-6499", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-5727", "mrqa_naturalquestions-validation-6358", "mrqa_hotpotqa-validation-2161", "mrqa_squad-validation-2010", "mrqa_squad-validation-7741", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-6186", "mrqa_squad-validation-6878", "mrqa_naturalquestions-validation-7387", "mrqa_squad-validation-1903", "mrqa_squad-validation-2147", "mrqa_squad-validation-3733", "mrqa_hotpotqa-validation-61"], "fixed_ids": ["mrqa_hotpotqa-validation-2092", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-3523", "mrqa_hotpotqa-validation-2588", "mrqa_triviaqa-validation-2812", "mrqa_hotpotqa-validation-1211"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9615384611686391}, {"timecode": 34, "before_eval": {"predictions": ["Speaker and a Deputy Speaker", "norman hartnell", "WBAI's broadcasting the track \" Filthy Words\" from a George Carlin comedy album", "Veronica", "Victorian College of the Arts", "Britain", "onions", "0.52", "phileas Fogg", "France", "Ian Richard Kyle Paisley", "bataan", "litas", "madness", "Taft -- Katsura Agreement", "1973", "1886", "75th", "Manhattan", "Lucius Verus", "revolution or orbital revolution", "Kenny Rogers and The First Edition", "stroke", "motorcycles or mopeds pulling trailers", "Euler's totient function", "eardrum", "how graphs are encoded as binary strings", "third", "afghanistan", "large", "Lauren Oliver", "tangled"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6465357559107558}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.18181818181818182, 0.14285714285714288, 0.6153846153846153, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2962", "mrqa_squad-validation-6673", "mrqa_naturalquestions-validation-6787", "mrqa_hotpotqa-validation-3982", "mrqa_triviaqa-validation-5496", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-1139", "mrqa_hotpotqa-validation-3072", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-5740", "mrqa_naturalquestions-validation-4366", "mrqa_triviaqa-validation-7592", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-3408"], "after_eval": {"predictions": ["Chairman", "norman hartnell", "email", "neither issue made it clear whether Archie was married to Betty or Veronica", "Victorian College of the Arts and Melbourne Conservatorium of Music", "Britain", "onions", "0.52", "separate tables", "It was held in France from 10 June to 12 July 1998.", "Ian Paisley", "World War II", "litas", "madness", "Taft", "late 1970s", "first published in 1890", "second", "Manhattan", "Lucius Verus", "revolution or orbital revolution", "Kenny Rogers and The First Edition", "head and neck", "motorcycles or mopeds pulling trailers", "the sum of divisors function", "earwax blockage", "how graphs are encoded as binary strings", "third", "afghanistan", "large", "Lauren Oliver", "tangled"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9583333333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-3002", "before_prediction": "75th", "after_prediction": "second"}], "retained_ids": ["mrqa_triviaqa-validation-7248", "mrqa_naturalquestions-validation-951", "mrqa_triviaqa-validation-5516", "mrqa_squad-validation-4369", "mrqa_triviaqa-validation-1220", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4288", "mrqa_squad-validation-4255", "mrqa_squad-validation-1635", "mrqa_squad-validation-2751", "mrqa_triviaqa-validation-7184", "mrqa_squad-validation-3708", "mrqa_squad-validation-8034", "mrqa_triviaqa-validation-1198"], "fixed_ids": ["mrqa_naturalquestions-validation-2962", "mrqa_squad-validation-6673", "mrqa_naturalquestions-validation-6787", "mrqa_hotpotqa-validation-3982", "mrqa_triviaqa-validation-5496", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-1139", "mrqa_hotpotqa-validation-3072", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-5740", "mrqa_naturalquestions-validation-4366", "mrqa_triviaqa-validation-7592", "mrqa_squad-validation-9063"], "unfixed_ids": ["mrqa_triviaqa-validation-3408"], "instant_fixing_rate": 0.9285714285714286, "instant_retention_rate": 0.944444443919753}, {"timecode": 35, "before_eval": {"predictions": ["up to 2% higher than during outbreaks of 13- and 17-year cicadas", "the interplay of supply and demand, which determines the prices of goods and services", "Ewan McGregor", "Skeletal muscle is an especially active site of arachidonic acid retention, accounting for roughly 10 - 20 % of the phospholipid fatty acid content on average", "ring", "Washington Redskins", "the former Strathclyde Regional Council debating chamber in Glasgow, and to the University of Aberdeen in May 2002", "William Howard Ashton", "wyo state", "low and persistent unemployment, in which inequality increases, has a negative effect on subsequent long-run economic growth", "Palm Beach County is a county located in the state of Florida, directly north of Broward County", "Song Kang-ho, Lee Byung-hun, and Jung Woo-sung", "changing display or audio settings quickly", "cropredy bridge", "the spectroscopic notation for the associated atomic orbitals : sharp, principal, diffuse and fundamental, and then g which follows f in the alphabet", "if the income share of the top 20 percent (the rich) increases, then GDP growth actually declines over the medium term, suggesting that the benefits don't trickle down", "Beauty and the Beast", "South Africa", "Scott \" Scotty\" Grainger Jr. is a fictional character from the CBS soap opera \"The Young and the Restless\"", "alamo", "seal illegally", "the UMC is generally considered one of the more moderate and tolerant denominations with respect to race, gender, and ideology", "Brian Liesegang", "Don Hahn", "Papua New Guinea", "Alvin and the Chipmunks", "National Association for the Advancement of Colored People", "1963\u20131989", "Titanic", "John Smith", "darrin Stephens", "india"], "metric_results": {"EM": 0.21875, "QA-F1": 0.31962219427244576}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true], "QA-F1": [0.5, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.4, 0.0, 0.4799999999999999, 0.08333333333333333, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.10526315789473684, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-9123", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-7704", "mrqa_triviaqa-validation-4829", "mrqa_naturalquestions-validation-4544", "mrqa_squad-validation-9400", "mrqa_triviaqa-validation-6423", "mrqa_squad-validation-7352", "mrqa_hotpotqa-validation-4447", "mrqa_hotpotqa-validation-2947", "mrqa_naturalquestions-validation-1587", "mrqa_triviaqa-validation-3767", "mrqa_naturalquestions-validation-585", "mrqa_squad-validation-7495", "mrqa_hotpotqa-validation-1475", "mrqa_triviaqa-validation-2999", "mrqa_squad-validation-3408", "mrqa_squad-validation-10036", "mrqa_hotpotqa-validation-3853", "mrqa_hotpotqa-validation-430", "mrqa_triviaqa-validation-11", "mrqa_hotpotqa-validation-1720", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-6969"], "after_eval": {"predictions": ["up to 2% higher", "capital and financial markets", "Dan Stevens", "brain, muscles, and liver", "butterfly", "New York Yankees, the Atlanta Braves, the Cincinnati Reds and the San Francisco Giants", "courtyard", "William Howard Ashton", "president harding", "Unemployment", "Miami", "Best Actor prize", "changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "king charles i", "the spectroscopic notation for the associated atomic orbitals", "declines", "Beauty and the Beast", "South Africa", "Tyler \"Ty\" Mendoza", "texas", "a seal", "the UMC", "Geno Lenardo", "Roger Allers and Rob Minkoff, produced by Don Hahn, and has a screenplay credited to Irene Mecchi, Jonathan Roberts, and Linda Woolverton", "Port Moresby, Papua New Guinea", "david seville", "NAACP", "1963\u20131989", "Titanic", "margaret beckett", "elizabeth montgomery", "india"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9554347826086957}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.17391304347826084, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-1136", "before_prediction": "Don Hahn", "after_prediction": "Roger Allers and Rob Minkoff, produced by Don Hahn, and has a screenplay credited to Irene Mecchi, Jonathan Roberts, and Linda Woolverton"}], "retained_ids": ["mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-5690", "mrqa_triviaqa-validation-6450", "mrqa_squad-validation-7610", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-1812"], "fixed_ids": ["mrqa_squad-validation-9123", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-10161", "mrqa_triviaqa-validation-4829", "mrqa_naturalquestions-validation-4544", "mrqa_squad-validation-9400", "mrqa_triviaqa-validation-6423", "mrqa_squad-validation-7352", "mrqa_hotpotqa-validation-4447", "mrqa_hotpotqa-validation-2947", "mrqa_naturalquestions-validation-1587", "mrqa_triviaqa-validation-3767", "mrqa_naturalquestions-validation-585", "mrqa_squad-validation-7495", "mrqa_hotpotqa-validation-1475", "mrqa_triviaqa-validation-2999", "mrqa_squad-validation-3408", "mrqa_squad-validation-10036", "mrqa_hotpotqa-validation-3853", "mrqa_hotpotqa-validation-430", "mrqa_triviaqa-validation-11", "mrqa_hotpotqa-validation-1720", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-6969"], "unfixed_ids": ["mrqa_naturalquestions-validation-7704"], "instant_fixing_rate": 0.96, "instant_retention_rate": 0.8571428559183674}, {"timecode": 36, "before_eval": {"predictions": ["NBC", "Amber Laura Heard", "Uranus", "Rudolph", "the Cobham\u2013Edmonds thesis", "Human, or humanoid aliens", "Best Male Pop Vocal Performance", "March 2012", "jazz musicians and other residents of the city", "Muhammad Ali", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Gibraltar", "submit cheerfully to the highest penalty that can be inflicted upon me for what in law is a deliberate crime and what appears to me to be the highest duty of a citizen", "Julius Caesar", "2", "March 28, 1979", "James Halliday who, when he died, had announced in his will to the public that he had left an Easter egg inside OASIS, and the first person to find it would inherit his entire fortune and the corporation", "formal language", "lecouvreur", "the lungs", "the Miasma theory", "imperial fluid ounces ( 568 ml )", "mountain ranges", "red", "other states", "perennial", "$12", "flat rate", "bestnight girl", "the ARPANET", "roughly west", "Sudan"], "metric_results": {"EM": 0.25, "QA-F1": 0.331936553030303}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, true, false, true, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.36363636363636365, 0.0, 0.19999999999999998, 0.0, 0.1, 0.0, 1.0, 0.5, 0.041666666666666664, 1.0, 0.0, 0.0, 0.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-152", "mrqa_hotpotqa-validation-652", "mrqa_triviaqa-validation-4824", "mrqa_squad-validation-7720", "mrqa_hotpotqa-validation-4427", "mrqa_hotpotqa-validation-3911", "mrqa_triviaqa-validation-3803", "mrqa_hotpotqa-validation-2936", "mrqa_squad-validation-110", "mrqa_naturalquestions-validation-6011", "mrqa_squad-validation-6759", "mrqa_naturalquestions-validation-4115", "mrqa_hotpotqa-validation-1884", "mrqa_naturalquestions-validation-3993", "mrqa_triviaqa-validation-5936", "mrqa_naturalquestions-validation-5552", "mrqa_squad-validation-4877", "mrqa_naturalquestions-validation-8525", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-4069", "mrqa_naturalquestions-validation-2323", "mrqa_triviaqa-validation-6290", "mrqa_squad-validation-9146", "mrqa_hotpotqa-validation-452"], "after_eval": {"predictions": ["National Broadcasting Company", "McG", "jupiter", "Rudolph", "the Cobham\u2013Edmonds thesis", "to remind the Doctor of his \"moral duty\"", "II", "April", "new orleans", "Raymond Patterson", "Coldplay", "Menorca", "submit cheerfully to the highest penalty that can be inflicted upon me for what in law is a deliberate crime and what appears to me to be the highest duty of a citizen", "emperors", "2%", "1979", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "binary strings", "conducting", "the right side of the heart", "bad air", "imperial fluid ounces", "mountain ranges", "white", "significant production of peaches as early as 1571, with exports to other states occurring around 1858", "nettle", "$12", "20 %", "love is all around", "to build a nationwide network in the UK", "west", "Republic of Chad"], "metric_results": {"EM": 0.84375, "QA-F1": 0.8607638888888889}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-1634", "before_prediction": "formal language", "after_prediction": "binary strings"}, {"id": "mrqa_naturalquestions-validation-3344", "before_prediction": "other states", "after_prediction": "significant production of peaches as early as 1571, with exports to other states occurring around 1858"}, {"id": "mrqa_squad-validation-4626", "before_prediction": "the ARPANET", "after_prediction": "to build a nationwide network in the UK"}], "retained_ids": ["mrqa_triviaqa-validation-37", "mrqa_squad-validation-1758", "mrqa_squad-validation-3060", "mrqa_hotpotqa-validation-1118", "mrqa_squad-validation-3635"], "fixed_ids": ["mrqa_hotpotqa-validation-152", "mrqa_hotpotqa-validation-652", "mrqa_squad-validation-7720", "mrqa_hotpotqa-validation-4427", "mrqa_hotpotqa-validation-3911", "mrqa_triviaqa-validation-3803", "mrqa_hotpotqa-validation-2936", "mrqa_squad-validation-110", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-4115", "mrqa_hotpotqa-validation-1884", "mrqa_naturalquestions-validation-3993", "mrqa_triviaqa-validation-5936", "mrqa_naturalquestions-validation-5552", "mrqa_squad-validation-4877", "mrqa_naturalquestions-validation-8525", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-4069", "mrqa_naturalquestions-validation-2323", "mrqa_triviaqa-validation-6290", "mrqa_squad-validation-9146", "mrqa_hotpotqa-validation-452"], "unfixed_ids": ["mrqa_triviaqa-validation-4824", "mrqa_squad-validation-6759"], "instant_fixing_rate": 0.9166666666666666, "instant_retention_rate": 0.6249999992187499}, {"timecode": 37, "before_eval": {"predictions": ["Southern Pacific", "the full extent of his injuries will never be known", "2007", "Post Alley under Pike Place Market", "mother-of-pearl", "February 20, 1978", "haggis", "George H.W. Bush", "96", "De Inventione by Marcus Tullius Cicero", "japan", "white, blue, pink, rainbow neon and glittering dotted lines", "c. 6500 to 3800 BC", "at exactly 37 \u00b0 9' 58.23\" latitude, around 11 miles (18 km) south of San Jose", "buttercup", "Rumplestiltskin", "Carlos Tevez", "sandhill dunnart ( Sminthopsis psammophila ) and the crest - tailed mulgara ( Dasycercus cristicauda )", "numerous musical venues, including the Teatr Wielki, the Polish National Opera, the Chamber Opera", "riper grapes", "1991", "morgan", "7 January 1936", "lifetime protection", "twenty- three", "Edwin Hubble, known for \"Hubble's Law\" NASA astronaut John M. Grunsfeld, geneticist James Watson, best known as one of the co-discoverers of the structure of DNA", "Many of the city's tax base dissipated, leading to problems with funding education, sanitation, and traffic control within the city limits. In addition, residents in unincorporated suburbs had difficulty obtaining municipal services", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Nlend Wom\u00e9 (born 26 March 1979) is a Cameroonian retired footballer who played as a defender", "allocution", "clangers", "Boston, Massachusetts"], "metric_results": {"EM": 0.25, "QA-F1": 0.3185459376901908}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8750000000000001, 0.0, 0.21052631578947367, 0.0, 0.0, 0.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2777777777777778, 0.13333333333333333, 0.23529411764705882, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-4768", "mrqa_squad-validation-1625", "mrqa_hotpotqa-validation-104", "mrqa_triviaqa-validation-1015", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-3479", "mrqa_squad-validation-5702", "mrqa_naturalquestions-validation-9058", "mrqa_squad-validation-2660", "mrqa_triviaqa-validation-5258", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-1698", "mrqa_squad-validation-677", "mrqa_triviaqa-validation-7134", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-3876", "mrqa_hotpotqa-validation-2377", "mrqa_squad-validation-8069", "mrqa_squad-validation-7246", "mrqa_naturalquestions-validation-1375", "mrqa_hotpotqa-validation-633", "mrqa_squad-validation-6737", "mrqa_triviaqa-validation-2524"], "after_eval": {"predictions": ["San Joaquin Valley Railroad", "broken", "2007", "San Luis Obispo, California", "mother-of-pearl", "Regina \"Gina\" Linetti", "sheep", "George H.W. Bush", "96", "The first recorded use of the term `` liberal arts '' ( artes liberales ) occurs in De Inventione by Marcus Tullius Cicero, but it is unclear if he created the term", "two", "white, blue, pink, rainbow neon and glittering", "Mesopotamia", "37\u00b0 9' 58.23\"", "woodentop", "Henry", "shared", "sandhill dunnart", "events and festivals", "kabinett", "2010", "avatar", "7 January 1936", "lifetime protection", "It contains twenty- three episodes", "Carl Sagan", "Much of the city's tax base dissipated", "Nationalists", "Pierre Nlend Wom\u00e9", "mistreatment from government officials", "whey", "Boston, Massachusetts"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9270833333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-513", "before_prediction": "February 20, 1978", "after_prediction": "Regina \"Gina\" Linetti"}], "retained_ids": ["mrqa_squad-validation-4108", "mrqa_squad-validation-5451", "mrqa_naturalquestions-validation-4664", "mrqa_hotpotqa-validation-4154", "mrqa_hotpotqa-validation-85", "mrqa_naturalquestions-validation-969", "mrqa_hotpotqa-validation-5371"], "fixed_ids": ["mrqa_squad-validation-4768", "mrqa_squad-validation-1625", "mrqa_hotpotqa-validation-104", "mrqa_triviaqa-validation-1015", "mrqa_triviaqa-validation-3479", "mrqa_squad-validation-5702", "mrqa_naturalquestions-validation-9058", "mrqa_squad-validation-2660", "mrqa_triviaqa-validation-5258", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-1698", "mrqa_squad-validation-677", "mrqa_triviaqa-validation-7134", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-3876", "mrqa_squad-validation-8069", "mrqa_squad-validation-7246", "mrqa_naturalquestions-validation-1375", "mrqa_hotpotqa-validation-633", "mrqa_squad-validation-6737", "mrqa_triviaqa-validation-2524"], "unfixed_ids": ["mrqa_naturalquestions-validation-4212", "mrqa_hotpotqa-validation-2377"], "instant_fixing_rate": 0.9166666666666666, "instant_retention_rate": 0.8749999989062499}, {"timecode": 38, "before_eval": {"predictions": ["the NP-complete Boolean satisfiability problem", "Dan Stevens", "1958", "Bart Cummings", "fox", "The primary catalyst for secession was slavery, especially Southern political leaders'resistance to attempts by Northern antislavery political forces to block the expansion of slavery into the western territories", "the colonies of British America", "The History of Little Goody Two - Shoes", "224.7 Earth days", "crowdfunding platforms for gathering money from the public, which circumvents traditional avenues of investment", "Thorgan Ganael Francis Hazard", "when commissioned", "Don Jeffrey \"Jeff\" Meldrum", "a week", "phil archer", "It is on the opposite side the Willamette River from the main area of the city", "The Chipettes", "Suez Canal", "60", "journalist", "take evidence from witnesses, conduct inquiries and scrutinise legislation", "beehive", "ramification", "newly accessioned", "fiat money", "strychnine", "Iowa", "approximately in the early 16th century", "13 June 2003", "Eddy Shah", "Hathi Jr.", "the closing of the atrioventricular valves and semilunar valves, respectively"], "metric_results": {"EM": 0.75, "QA-F1": 0.7788977770595418}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 0.07692307692307693, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-10406", "mrqa_triviaqa-validation-4677", "mrqa_naturalquestions-validation-685", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-2682", "mrqa_naturalquestions-validation-6210", "mrqa_naturalquestions-validation-2100", "mrqa_triviaqa-validation-2039"], "after_eval": {"predictions": ["NP-complete Boolean satisfiability problem", "Emma Watson and Dan Stevens as the eponymous characters with Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Ian McKellen,", "1958", "Bart Cummings", "dragon", "slavery", "colonies of British America", "The History of Little Goody Two - Shoes", "224.7 Earth days", "crowdfunding platforms for gathering money from the public, which circumvents traditional avenues of investment", "Thorgan Ganael Francis Hazard", "when commissioned", "Don Jeffrey \"Jeff\" Meldrum (born May 24, 1958) is a Professor of Anatomy and Anthropology", "a week", "phil archer", "French and English", "The Chipettes", "suez canal", "60 by West All - Stars ( 2017 )", "journalist", "no revising chamber", "beehive", "ramification", "newly accessioned into the collection", "fiat money", "strychnine", "Iowa", "approximately in the early 16th century", "Lord's", "today", "Hathi Jr.", "the closing of the atrioventricular valves and semilunar valves, respectively"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9655172413793103}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.896551724137931, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": ["mrqa_squad-validation-1862", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-8418", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-10255", "mrqa_hotpotqa-validation-727", "mrqa_naturalquestions-validation-4067", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-4762", "mrqa_naturalquestions-validation-3707", "mrqa_triviaqa-validation-3118", "mrqa_squad-validation-1660", "mrqa_squad-validation-9478", "mrqa_triviaqa-validation-1473", "mrqa_squad-validation-9032", "mrqa_squad-validation-5505", "mrqa_naturalquestions-validation-10279", "mrqa_triviaqa-validation-3320", "mrqa_naturalquestions-validation-7630", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2555"], "fixed_ids": ["mrqa_triviaqa-validation-4677", "mrqa_naturalquestions-validation-685", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-2682", "mrqa_naturalquestions-validation-6210", "mrqa_triviaqa-validation-2039"], "unfixed_ids": ["mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-2100"], "instant_fixing_rate": 0.75, "instant_retention_rate": 0.9999999995833333}, {"timecode": 39, "before_eval": {"predictions": ["capital city of Taiwan", "Dan Conner", "east and west berlin", "warren commission", "Katharine Hepburn, Joan Bennett, Frances Dee, and Jean Parker", "violence", "Reese Witherspoon as Carter, Ginnifer Goodwin as Vivian Liberto, and Robert Patrick as Cash's father", "couscous", "1980s", "Carl Sagan", "detroit", "your song", "2003", "every year", "Fabbrica Italiana Automobili Torino", "the first Sunday in November", "unclear", "agatha christie", "Porto", "August 10, 1933", "Golden Gate Bridge", "city of vancouver", "those who already hold wealth", "bilingual German author B. Traven", "Finding Nemo", "Fortean", "inflation", "squirrel", "247,597", "The Institute for Advanced Study (IAS) in Princeton, New Jersey", "German service", "DC electricity"], "metric_results": {"EM": 0.75, "QA-F1": 0.9011504120879121}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true], "QA-F1": [0.8571428571428571, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.8, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-6133", "mrqa_hotpotqa-validation-944", "mrqa_naturalquestions-validation-9227", "mrqa_squad-validation-8070", "mrqa_naturalquestions-validation-6839", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-2203"], "after_eval": {"predictions": ["taipei", "Dan Conner", "east and west berlin", "warren commission", "Katharine Hepburn, Joan Bennett, Frances Dee", "violence", "Joaquin Phoenix as Cash, Reese Witherspoon as Carter, Ginnifer Goodwin as Vivian Liberto, and Robert Patrick as Cash's father", "couscous", "1980s", "John M. Grunsfeld", "detroit", "your song", "2000", "every year", "Fabbrica Italiana Automobili Torino", "the second Sunday of March", "unclear", "agatha christie", "porto", "August 10, 1933", "Golden Gate Bridge", "city of vancouver", "those who already hold wealth", "bilingual German author B. Traven, whose identity remains unknown", "Finding Nemo", "Fortean", "inflation", "squirrel", "264,152", "The Institute for Advanced Study", "German service cartridge", "DC electricity"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9350961538461539}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9230769230769231, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-3698", "before_prediction": "2003", "after_prediction": "2000"}, {"id": "mrqa_hotpotqa-validation-4097", "before_prediction": "247,597", "after_prediction": "264,152"}], "retained_ids": ["mrqa_hotpotqa-validation-2243", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-1438", "mrqa_squad-validation-2493", "mrqa_triviaqa-validation-2522", "mrqa_naturalquestions-validation-6554", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-7195", "mrqa_triviaqa-validation-3017", "mrqa_hotpotqa-validation-5233", "mrqa_squad-validation-10428", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-5595", "mrqa_hotpotqa-validation-2332", "mrqa_naturalquestions-validation-3108", "mrqa_triviaqa-validation-4024", "mrqa_squad-validation-7547", "mrqa_hotpotqa-validation-489", "mrqa_hotpotqa-validation-1607", "mrqa_triviaqa-validation-2808", "mrqa_triviaqa-validation-1276", "mrqa_squad-validation-3650"], "fixed_ids": ["mrqa_triviaqa-validation-6133", "mrqa_hotpotqa-validation-944", "mrqa_squad-validation-8070", "mrqa_naturalquestions-validation-6839", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-2203"], "unfixed_ids": ["mrqa_naturalquestions-validation-9227"], "instant_fixing_rate": 0.875, "instant_retention_rate": 0.9166666662847222}, {"timecode": 40, "before_eval": {"predictions": ["50 fund", "Cetshwayo", "on the road back to Samarkand", "sarajevo", "Isabella (Belle) Baumfree", "corgi", "1518\u201331", "five starting pitchers, seven relief pitchers, two catchers, six infielders, and five outfielders", "placing them on prophetic faith", "Bacon", "Yul Brynner", "cytotoxic natural killer cells and CTLs (cytotoxic T lymphocytes)", "tartan", "philadelphia eagles", "barter system", "Constitution", "sound and light", "surrendered peacefully without violently resisting", "Sochi, Russia", "sweden", "central Saskatchewan", "immediately", "new zealand", "newlyWEDS", "30 Major League Baseball teams and their 160 minor league baseball affiliates", "MI6", "nerve cells", "caesar", "photolysis of ozone by light of short wavelength", "4 - inch screen size", "Queen City", "qui tam"], "metric_results": {"EM": 0.84375, "QA-F1": 0.8842677696078431}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5882352941176471, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-4953", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-10265", "mrqa_triviaqa-validation-4123", "mrqa_hotpotqa-validation-3151"], "after_eval": {"predictions": ["50 fund", "Cetshwayo", "on the road back to Samarkand", "sarajevo", "Isabella (Belle) Baumfree", "corgi", "throughout the 14th to 17th centuries", "five starting pitchers", "placing them on prophetic faith", "Bacon", "Yul Brynner", "anti-inflammatory molecules", "tartan", "philadelphia eagles", "a tradeable entity used to avoid the inconveniences of a pure barter system", "Constitution", "sound and light", "surrendered peacefully without violently resisting", "Sochi, Russia", "sweden", "central Saskatchewan", "immediately", "new zealand", "shorthand typist", "30 Major League Baseball teams", "MI6", "nerve cells", "caesar", "photolysis", "4 - inch screen size", "Queen City", "qui tam"], "metric_results": {"EM": 0.96875, "QA-F1": 0.96875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-6588", "before_prediction": "cytotoxic natural killer cells and CTLs (cytotoxic T lymphocytes)", "after_prediction": "anti-inflammatory molecules"}], "retained_ids": ["mrqa_squad-validation-395", "mrqa_naturalquestions-validation-6212", "mrqa_squad-validation-6248", "mrqa_triviaqa-validation-1571", "mrqa_hotpotqa-validation-1453", "mrqa_triviaqa-validation-2475", "mrqa_squad-validation-2249", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-validation-7457", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3213", "mrqa_naturalquestions-validation-1063", "mrqa_triviaqa-validation-6127", "mrqa_squad-validation-8247", "mrqa_hotpotqa-validation-4076", "mrqa_triviaqa-validation-1859", "mrqa_naturalquestions-validation-8514", "mrqa_squad-validation-3932", "mrqa_triviaqa-validation-3280", "mrqa_hotpotqa-validation-1791", "mrqa_triviaqa-validation-1877", "mrqa_triviaqa-validation-579", "mrqa_squad-validation-3617", "mrqa_naturalquestions-validation-6848", "mrqa_hotpotqa-validation-178", "mrqa_naturalquestions-validation-993"], "fixed_ids": ["mrqa_squad-validation-4953", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-10265", "mrqa_triviaqa-validation-4123", "mrqa_hotpotqa-validation-3151"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.96296296260631}, {"timecode": 41, "before_eval": {"predictions": ["all war", "Hindi", "Gaelic", "In Crash, there is no betting, as in Brag", "Idisi", "lion", "The cinema of Russia", "sediment load", "FedExField in Landover, Maryland", "GTPase", "Windows Easy Transfer", "Ordos City", "flying disc", "Boise State University", "le Leicester", "Section 30", "Paul Lynde", "October 1986", "4 billion", "Monsoon", "switzerland", "george iii", "Perth", "Q", "Paul", "the division of labour, productivity, and free markets", "Gerard Marenghi (nee Williams)", "bobby brown", "Nebula Award", "margaret thatcher", "jonathan", "Luigi Creatore"], "metric_results": {"EM": 0.75, "QA-F1": 0.8051136363636364}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_triviaqa-validation-684", "mrqa_naturalquestions-validation-774", "mrqa_hotpotqa-validation-3176", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8338", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-434"], "after_eval": {"predictions": ["all war", "Hindi", "Gaelic", "In Crash, there is no betting, as in Brag, but rather players aim to reach a total of 11 points, gained over successive deals", "Idisi", "European or Eurasian cave lion", "The cinema of Russia", "the sediment load of the Rhine has strongly increased", "FedExField in Landover, Maryland", "the scission of newly formed vesicles from the membrane of one cellular compartment", "Windows Easy Transfer", "Ordos City", "flying disc", "Boise State University", "leicester", "Section 30", "Paul Lynde as Templeton, a care - free, egotistical rat who lives on a web in a corner of Homer's barn above Wilbur's pig pen", "October 1986", "4 billion", "Northeast Monsoon or Retreating Monsoon", "switzerland", "george i", "5AA", "Q", "Philippians", "what builds nations'wealth", "Gerard Marenghi (born January 24, 1920)", "bobby brown", "Nebula Award, the Philip K. Dick Award, and the Hugo Award", "margaret thatcher", "jonathan", "Luigi Creatore"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8776756060213942}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5625, 1.0, 0.33333333333333337, 1.0, 0.4444444444444445, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.17391304347826084, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-1391", "before_prediction": "In Crash, there is no betting, as in Brag", "after_prediction": "In Crash, there is no betting, as in Brag, but rather players aim to reach a total of 11 points, gained over successive deals"}, {"id": "mrqa_hotpotqa-validation-2053", "before_prediction": "lion", "after_prediction": "European or Eurasian cave lion"}, {"id": "mrqa_squad-validation-9355", "before_prediction": "sediment load", "after_prediction": "the sediment load of the Rhine has strongly increased"}, {"id": "mrqa_naturalquestions-validation-6151", "before_prediction": "Paul Lynde", "after_prediction": "Paul Lynde as Templeton, a care - free, egotistical rat who lives on a web in a corner of Homer's barn above Wilbur's pig pen"}, {"id": "mrqa_triviaqa-validation-2181", "before_prediction": "george iii", "after_prediction": "george i"}], "retained_ids": ["mrqa_squad-validation-1592", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-2375", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-9712", "mrqa_hotpotqa-validation-201", "mrqa_triviaqa-validation-4955", "mrqa_naturalquestions-validation-3058", "mrqa_squad-validation-2142", "mrqa_naturalquestions-validation-5007", "mrqa_triviaqa-validation-2384", "mrqa_triviaqa-validation-6751", "mrqa_hotpotqa-validation-4823", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-4320", "mrqa_naturalquestions-validation-9763"], "fixed_ids": ["mrqa_triviaqa-validation-684", "mrqa_naturalquestions-validation-774", "mrqa_hotpotqa-validation-3176", "mrqa_naturalquestions-validation-7728", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-434"], "unfixed_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-8338"], "instant_fixing_rate": 0.75, "instant_retention_rate": 0.7916666663368055}, {"timecode": 42, "before_eval": {"predictions": ["kill bill", "usually cavatelli, acini di pepe, pastina, orzo, etc. ), lentils, or grated parmesan cheese", "ballets", "derision", "the ozone generated in contact with the skin", "the tariff", "Chartered", "her statement suggested a lack of remorse, an attempt to avoid responsibility for her actions, and even a likelihood of repeating her illegal actions", "Danish", "a defender", "egypt", "the side - chain of the amino acid N - terminal to the scissile amide bond ( the P position )", "Fructose", "their unusual behavior", "Gainsborough Trinity Football Club", "portrait", "Introverted Sensing ( Si )", "Thursday", "white", "patient-specific problems, identifying goals of therapy, and reviewing all prescribed medications prior to dispensing and administration to the patient", "mars", "feats of exploration", "a piston", "rob lowe", "The Private Education Student Financial Assistance", "bow", "rebuild St. Peter's Basilica", "Algeria", "two", "Bills", "Qualcomm", "DTIME(n2)"], "metric_results": {"EM": 0.75, "QA-F1": 0.8309676434676434}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.761904761904762, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3846153846153846, 1.0, 0.0, 1.0, 0.7222222222222222, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8284", "mrqa_naturalquestions-validation-9093", "mrqa_squad-validation-6735", "mrqa_hotpotqa-validation-5731", "mrqa_naturalquestions-validation-7233", "mrqa_hotpotqa-validation-4842", "mrqa_squad-validation-9569", "mrqa_squad-validation-6369"], "after_eval": {"predictions": ["kill bill", "usually cavatelli, acini di pepe, pastina, orzo, etc.", "ballets", "the connotations of a somewhat related word in German Eidgenosse (Confederates as in \"a citizen of one of the states of the Swiss Confederacy\")", "the ozone generated in contact with the skin, and to a lesser extent, by nitrous acid", "the American Civil War", "Chartered", "her statement suggested a lack of remorse, an attempt to avoid responsibility for her actions, and even a likelihood of repeating her illegal actions", "Danish", "centre-back", "egypt", "where the side - chain of the amino acid N - terminal to the scissile amide bond ( the P position ) is a large hydrophobic amino acid", "Fructose", "their unusual behavior", "Gainsborough Trinity Football Club is a football club based in Gainsboro, Lincolnshire, England.", "portrait", "Introverted Sensing ( Si )", "May", "white", "drug choice, dose, route, frequency, and duration of therapy", "mars", "feats of exploration", "a piston", "rob lowe", "The Private Education Student Financial Assistance", "bow", "to rebuild St. Peter's Basilica", "Algeria", "two", "Bills can be introduced to Parliament in a number of ways", "Qualcomm, a San Diego-based telecommunications equipment company and the stadium was known as Qualcomm Stadium", "DTIME(n2)"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8881127553002552}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3846153846153846, 1.0, 1.0, 1.0, 0.9047619047619047, 1.0, 1.0, 0.9166666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.18181818181818182, 0.14285714285714288, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-3044", "before_prediction": "derision", "after_prediction": "the connotations of a somewhat related word in German Eidgenosse (Confederates as in \"a citizen of one of the states of the Swiss Confederacy\")"}, {"id": "mrqa_squad-validation-2150", "before_prediction": "rebuild St. Peter's Basilica", "after_prediction": "to rebuild St. Peter's Basilica"}, {"id": "mrqa_squad-validation-9452", "before_prediction": "Bills", "after_prediction": "Bills can be introduced to Parliament in a number of ways"}, {"id": "mrqa_hotpotqa-validation-5522", "before_prediction": "Qualcomm", "after_prediction": "Qualcomm, a San Diego-based telecommunications equipment company and the stadium was known as Qualcomm Stadium"}], "retained_ids": ["mrqa_triviaqa-validation-2952", "mrqa_triviaqa-validation-3261", "mrqa_squad-validation-1429", "mrqa_squad-validation-1841", "mrqa_hotpotqa-validation-3899", "mrqa_triviaqa-validation-4137", "mrqa_naturalquestions-validation-4185", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-680", "mrqa_naturalquestions-validation-6706", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7133", "mrqa_hotpotqa-validation-4130", "mrqa_squad-validation-3385", "mrqa_triviaqa-validation-1586", "mrqa_squad-validation-7034", "mrqa_triviaqa-validation-289", "mrqa_squad-validation-9792", "mrqa_squad-validation-10395", "mrqa_squad-validation-1808"], "fixed_ids": ["mrqa_naturalquestions-validation-8284", "mrqa_naturalquestions-validation-9093", "mrqa_hotpotqa-validation-5731", "mrqa_squad-validation-9569", "mrqa_squad-validation-6369"], "unfixed_ids": ["mrqa_squad-validation-6735", "mrqa_naturalquestions-validation-7233", "mrqa_hotpotqa-validation-4842"], "instant_fixing_rate": 0.625, "instant_retention_rate": 0.8333333329861111}, {"timecode": 43, "before_eval": {"predictions": ["niagara falls", "15", "Indiana State Quarters", "salt lake city", "Italian", "sailor", "subdomain", "genetic disease", "meyerbeer", "natural-ingredients-only personal care products", "horse", "Sparafucile", "largest country", "third", "iKEA", "169", "mexico", "George Frampton", "Plies", "the Outfield", "lawn tennis", "Michael Edwards ( briefly as the older Connor ) and then by teenage actor Edward Furlong throughout the remainder of the film", "road engines", "richard burton", "when the Moon's ecliptic longitude differ by 0 \u00b0, 90 \u00b0, 180 \u00b0, and 270 \u00b0, respectively", "Bill Fraser", "Yuan T. Lee", "Kentucky, Virginia, and Tennessee", "rocketry and manned spaceflight, including avionics, telecommunications, and computers", "mitochondrial eve", "237", "matthew"], "metric_results": {"EM": 0.84375, "QA-F1": 0.903125}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.21052631578947367, 1.0, 1.0, 0.7894736842105263, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-7538", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-7215"], "after_eval": {"predictions": ["niagara falls", "15", "indiana", "salt lake city", "Italian", "sailor", "subdomain", "genetic disease", "meyerbeer", "natural-ingredients-only personal care products", "horse", "Sparafucile", "largest country", "third", "iKEA", "169", "mexico", "George Frampton", "Plies", "English rock band the Outfield", "lawn tennis", "Edward Furlong", "road engines", "richard burton", "when the Moon's ecliptic longitude and the Sun's Ecliptics longitude differ by 0 \u00b0, 90 \u00b0, 180 \u00b0, and 270 \u00b0, respectively", "snudge", "Glenn T. Seaborg", "Kentucky, Virginia, and Tennessee", "rocketry and manned spaceflight, including avionics, telecommunications, and computers", "mitochondrial eve", "237", "matthew"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9940476190476191}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8095238095238095, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": ["mrqa_triviaqa-validation-7744", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-6916", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-981", "mrqa_naturalquestions-validation-2663", "mrqa_squad-validation-6442", "mrqa_triviaqa-validation-5580", "mrqa_hotpotqa-validation-518", "mrqa_triviaqa-validation-5087", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-850", "mrqa_squad-validation-3671", "mrqa_triviaqa-validation-4090", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-5160", "mrqa_squad-validation-5586", "mrqa_hotpotqa-validation-5370", "mrqa_triviaqa-validation-7482", "mrqa_squad-validation-3330", "mrqa_triviaqa-validation-2803", "mrqa_squad-validation-8054", "mrqa_hotpotqa-validation-4624", "mrqa_squad-validation-3812", "mrqa_triviaqa-validation-5899", "mrqa_hotpotqa-validation-5541", "mrqa_triviaqa-validation-305"], "fixed_ids": ["mrqa_triviaqa-validation-7538", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-7641", "mrqa_triviaqa-validation-7215"], "unfixed_ids": ["mrqa_naturalquestions-validation-5968"], "instant_fixing_rate": 0.8, "instant_retention_rate": 0.9999999996296296}, {"timecode": 44, "before_eval": {"predictions": ["in Wakanda and the Savage Land", "more experience and higher education", "2003", "paris", "football", "published campaign settings", "`` Fourth Revised Edition '' ISBN 0 - 06 - 015547 - 7", "867", "the Hebrew name Immanu'el ( \u05e2\u05b4\u05de\u05b8\u05bc\u05e0\u05d5\u05bc\u05d0\u05b5\u05dc \u202c, which means `` God with us. '' It was possibly brought from the Byzantine Empire", "\u00f7 (pronounced \"divide\") is the third studio album by English singer-songwriter Ed Sheeran.", "R2 - D2", "second", "well", "in all health care settings", "pharmacists are expected to become more integral within the health care system", "music", "Gabriel Alberto Azucena (born September 23, 1988)", "Mumbai Rajdhani Express", "Rome", "May 18, 2010", "Sylvia Pankhurst", "schengen", "Lord Chancellor of England", "belfast", "Indian government", "Irish", "Vesta", "branwell", "energy", "hubble space telescope", "Christian Perfection", "chorale cantatas"], "metric_results": {"EM": 0.8125, "QA-F1": 0.8620780170122275}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [0.1818181818181818, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 1.0, 0.31578947368421056, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-2119", "mrqa_triviaqa-validation-6579", "mrqa_naturalquestions-validation-10612", "mrqa_squad-validation-6319", "mrqa_hotpotqa-validation-4278"], "after_eval": {"predictions": ["King T'Chaka of the African nation Wakanda", "more experience and higher education", "in 2003 for the inter-county competition in England and Wales", "paris", "sweden", "published campaign settings", "`` Fourth Revised Edition '' ISBN 0 - 06 - 015547 - 7", "867", "possibly brought from the Byzantine Empire ( as \u039c\u03b1\u03bd\u03bf\u03c5\u03ae\u03bb ) to Spain and Portugal, where it has been used since at least the 13th century", "\u00f7 (pronounced \"divide\") is the third studio album by English singer-songwriter Ed Sheeran.", "C - 3PO", "second", "little brother Chang", "drug product selection", "pharmacists are expected to become more integral within the health care system", "music", "Lecrae Devaughn Moore", "Mumbai Rajdhani Express", "Rome", "May 18, 2010", "Sylvia Pankhurst", "schengen", "Lord Chancellor of England", "belfast", "Indian government", "Irish", "Vesta", "branwell", "energy", "hubble space telescope", "Christian Perfection", "chorale cantatas"], "metric_results": {"EM": 0.96875, "QA-F1": 0.96875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-3664", "before_prediction": "well", "after_prediction": "little brother Chang"}], "retained_ids": ["mrqa_squad-validation-2236", "mrqa_triviaqa-validation-4197", "mrqa_hotpotqa-validation-5305", "mrqa_naturalquestions-validation-10619", "mrqa_hotpotqa-validation-910", "mrqa_hotpotqa-validation-4649", "mrqa_naturalquestions-validation-5550", "mrqa_squad-validation-7067", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-2406", "mrqa_naturalquestions-validation-3459", "mrqa_naturalquestions-validation-8491", "mrqa_naturalquestions-validation-2169", "mrqa_hotpotqa-validation-4794", "mrqa_triviaqa-validation-5022", "mrqa_hotpotqa-validation-389", "mrqa_triviaqa-validation-3617", "mrqa_naturalquestions-validation-6057", "mrqa_hotpotqa-validation-5696", "mrqa_naturalquestions-validation-1725", "mrqa_triviaqa-validation-4027", "mrqa_squad-validation-8625", "mrqa_triviaqa-validation-1504", "mrqa_squad-validation-9951", "mrqa_squad-validation-2419"], "fixed_ids": ["mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-2119", "mrqa_triviaqa-validation-6579", "mrqa_naturalquestions-validation-10612", "mrqa_squad-validation-6319", "mrqa_hotpotqa-validation-4278"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9615384611686391}, {"timecode": 45, "before_eval": {"predictions": ["mike hammer", "Detroit Lions", "smoke it", "under `` the immortal Hawke ''", "fredman v. Georgia", "frail Catholic saints", "gulf stream", "holy grail", "James Hewson", "Ubiorum", "lorne greene", "Pennsylvania", "1998", "They were the first group to win the competition", "the main highway entrance at California State Route 1", "Canadian Football League (CFL) team", "Canadian", "Gareth", "\"LOVE Radio\"", "Colorado Rockies", "the court from its members", "john travolta", "Dan Fogelman and Jessie Nelson", "worked to radicalize the Islamist movement", "People! and The Carnabeats", "\" Cashin' In\"", "the most recent Super Bowl champions", "Command/Service Module", "Santa Clara", "the tsar's Moscow residence", "Landing Barge, Kitchen", "peninsular"], "metric_results": {"EM": 0.75, "QA-F1": 0.7803571428571429}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-3989", "mrqa_naturalquestions-validation-4097", "mrqa_hotpotqa-validation-5068", "mrqa_naturalquestions-validation-9931", "mrqa_triviaqa-validation-455", "mrqa_hotpotqa-validation-866", "mrqa_naturalquestions-validation-4905", "mrqa_hotpotqa-validation-712"], "after_eval": {"predictions": ["mike hammer", "Detroit Lions", "smoke it", "under `` the immortal Hawke ''", "death penalty", "frail Catholic saints", "gulf stream", "holy grail", "29 - year - old Mangal Pandey of the 34th BNI", "Ubiorum", "lorne greene", "Pennsylvania", "1998", "They were the first group to win the competition", "the main highway entrance at California State Route 1", "St. Louis", "Canadian", "Gareth", "\"LOVE Radio\"", "Colorado Rockies", "the court", "tony manero", "David Dobkin", "worked to radicalize the Islamist movement", "People! and The Carnabeats", "\" Cashin' In\"", "the most recent Super Bowl champions", "Command/Service Module", "Santa Clara", "the tsar's Moscow residence", "Operation Neptune", "peninsular"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9866071428571428}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": ["mrqa_triviaqa-validation-7473", "mrqa_hotpotqa-validation-1907", "mrqa_triviaqa-validation-2201", "mrqa_naturalquestions-validation-4123", "mrqa_squad-validation-2598", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-5587", "mrqa_squad-validation-9296", "mrqa_triviaqa-validation-6956", "mrqa_naturalquestions-validation-8728", "mrqa_triviaqa-validation-3335", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3363", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-1433", "mrqa_squad-validation-5852", "mrqa_hotpotqa-validation-5149", "mrqa_squad-validation-9513", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-3509", "mrqa_naturalquestions-validation-3217", "mrqa_squad-validation-3845", "mrqa_squad-validation-13", "mrqa_naturalquestions-validation-2067"], "fixed_ids": ["mrqa_triviaqa-validation-3989", "mrqa_naturalquestions-validation-4097", "mrqa_hotpotqa-validation-5068", "mrqa_naturalquestions-validation-9931", "mrqa_triviaqa-validation-455", "mrqa_hotpotqa-validation-866", "mrqa_hotpotqa-validation-712"], "unfixed_ids": ["mrqa_naturalquestions-validation-4905"], "instant_fixing_rate": 0.875, "instant_retention_rate": 0.9999999995833333}, {"timecode": 46, "before_eval": {"predictions": ["cuba", "that continents `` ploughed '' through the sea.", "take that", "youngest person to become a national transgender figure", "electric lighting", "improved the speed of encryption of communications at both ends in front line operations during World War II", "Einstein", "electromagnetic theory", "Swansea City", "millais", "Joel", "massively multiplayer online role-playing video game", "hundreds", "\"House Alone\"", "1999", "a new facility on The Watermark business park next to the MetroCentre in Gateshead", "apple", "partial funding", "pale lager", "unattainable", "Chu'Tsai", "Liz", "least onerous", "italy", "Grissom, White, and Chaffee", "multinational retail corporation", "maracuya", "The Natya Shastra", "the sand grains cause a scrubbing noise as they rub against each other when walked on", "ryder cup matches", "incorrectly", "vienna"], "metric_results": {"EM": 0.875, "QA-F1": 0.8905480295566502}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0689655172413793, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-73", "mrqa_squad-validation-5464", "mrqa_naturalquestions-validation-2808"], "after_eval": {"predictions": ["cuba", "that continents `` ploughed '' through the sea.", "take that", "youngest person to become a national transgender figure", "electric lighting", "improved the speed of encryption of communications at both ends in front line operations during World War II", "Einstein", "electromagnetic theory", "Swansea City", "millais", "Joel", "massively multiplayer online role-playing video game", "hundreds", "Waiting for Guffman", "2003", "The Watermark business park", "apple", "partial funding", "pale lager", "unattainable", "Chu'Tsai", "Liz", "least onerous", "italy", "White", "multinational retail corporation", "maracuya", "The Natya Shastra", "sedimentary rock and other material with a high iron concentration which oxidizes upon exposure to the air", "ryder cup matches", "incorrectly", "vienna"], "metric_results": {"EM": 0.96875, "QA-F1": 0.98125}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-3913", "before_prediction": "Grissom, White, and Chaffee", "after_prediction": "White"}], "retained_ids": ["mrqa_triviaqa-validation-3929", "mrqa_naturalquestions-validation-8204", "mrqa_triviaqa-validation-6905", "mrqa_hotpotqa-validation-5251", "mrqa_squad-validation-5157", "mrqa_naturalquestions-validation-5352", "mrqa_squad-validation-10341", "mrqa_squad-validation-10489", "mrqa_hotpotqa-validation-1831", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-3284", "mrqa_hotpotqa-validation-1074", "mrqa_squad-validation-2673", "mrqa_triviaqa-validation-7350", "mrqa_squad-validation-7792", "mrqa_hotpotqa-validation-5239", "mrqa_naturalquestions-validation-2890", "mrqa_squad-validation-6271", "mrqa_hotpotqa-validation-5226", "mrqa_squad-validation-4064", "mrqa_triviaqa-validation-2914", "mrqa_naturalquestions-validation-3616", "mrqa_triviaqa-validation-1128", "mrqa_naturalquestions-validation-988", "mrqa_triviaqa-validation-2873", "mrqa_squad-validation-3523", "mrqa_triviaqa-validation-4430"], "fixed_ids": ["mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-73", "mrqa_squad-validation-5464", "mrqa_naturalquestions-validation-2808"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9642857139413265}, {"timecode": 47, "before_eval": {"predictions": ["florida", "racehorses", "New Zealand national team", "Kumbum Monastery or Ta'er Shi near Xining", "Styal Mill", "big - name lawyers", "Goldie & Bear", "during initial entry training", "moral tale", "Franklin quit after five months, leaving the group as a trio", "leeds", "a star ( representing either the Star of Bethlehem or the star of David ), finials, angels ( `` Christmas angel '' ), or fairies", "260", "piccadilly", "Neighbourhood", "The Washington Post", "tentacles", "insect pests", "candidates on specific catechism questions", "pH indicator", "about 50% oxygen composition at standard pressure or 2.5 times the normal sea-level O2 partial pressure of about 21 kPa", "2001", "John and Charles Wesley", "Science and Discovery", "if 1 were considered a prime", "Campbellsville University", "little bastard", "Jude", "morgan spurlock", "arranged marriage to Chino, a friend of Bernardo's", "XXXTentacion", "raises the productivity of each worker,"], "metric_results": {"EM": 0.8125, "QA-F1": 0.8735977564102564}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.7499999999999999, 1.0, 0.5384615384615384, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-930", "mrqa_triviaqa-validation-3868", "mrqa_squad-validation-2346", "mrqa_squad-validation-3685", "mrqa_hotpotqa-validation-2801", "mrqa_naturalquestions-validation-5241"], "after_eval": {"predictions": ["florida", "racehorse", "English Premier League club Burnley and the New Zealand national team", "Kumbum Monastery or Ta'er Shi near Xining", "Styal Mill", "William Jennings Bryan", "Goldie & Bear", "during initial entry training", "one of The Canterbury Tales by Geoffrey Chaucer", "Franklin quit after five months", "leeds", "a star ( representing either the Star of Bethlehem or the star of David ), finials, angels ( `` Christmas angel '' ), or fairies", "260", "piccadilly Line", "Neighbourhood", "The Washington Post", "tentacles", "insects", "specific catechism questions", "a pH indicator", "50% oxygen", "2001", "George Whitefield", "Science and Discovery", "if 1 were considered a prime", "Campbellsville University", "Rebel Without a Cause", "Jude", "morgan spurlock", "Maria works in a bridal shop with Anita", "XXXTentacion", "raises the productivity of each worker,"], "metric_results": {"EM": 0.84375, "QA-F1": 0.9047619047619047}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7142857142857143, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-1924", "before_prediction": "New Zealand national team", "after_prediction": "English Premier League club Burnley and the New Zealand national team"}, {"id": "mrqa_naturalquestions-validation-2143", "before_prediction": "moral tale", "after_prediction": "one of The Canterbury Tales by Geoffrey Chaucer"}, {"id": "mrqa_naturalquestions-validation-2864", "before_prediction": "Franklin quit after five months, leaving the group as a trio", "after_prediction": "Franklin quit after five months"}, {"id": "mrqa_triviaqa-validation-1516", "before_prediction": "piccadilly", "after_prediction": "piccadilly Line"}, {"id": "mrqa_triviaqa-validation-1799", "before_prediction": "little bastard", "after_prediction": "Rebel Without a Cause"}], "retained_ids": ["mrqa_triviaqa-validation-2770", "mrqa_triviaqa-validation-945", "mrqa_squad-validation-6287", "mrqa_hotpotqa-validation-5788", "mrqa_hotpotqa-validation-5877", "mrqa_naturalquestions-validation-4092", "mrqa_triviaqa-validation-1256", "mrqa_naturalquestions-validation-5305", "mrqa_squad-validation-4212", "mrqa_hotpotqa-validation-5086", "mrqa_naturalquestions-validation-8582", "mrqa_squad-validation-4479", "mrqa_naturalquestions-validation-7849", "mrqa_triviaqa-validation-4298", "mrqa_squad-validation-1609", "mrqa_squad-validation-9061", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-4173", "mrqa_triviaqa-validation-7477", "mrqa_naturalquestions-validation-2092", "mrqa_squad-validation-7182"], "fixed_ids": ["mrqa_naturalquestions-validation-930", "mrqa_triviaqa-validation-3868", "mrqa_squad-validation-2346", "mrqa_squad-validation-3685", "mrqa_hotpotqa-validation-2801", "mrqa_naturalquestions-validation-5241"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.8076923073816568}, {"timecode": 48, "before_eval": {"predictions": ["James Stenbeck", "Section.80", "el Capitan", "interventive", "3", "Bishop Lloyd Christ Wicke", "georgia", "a thousand years", "the director's own approved edit", "shirley williams", "unesco", "Emmanuel Mudiay", "loop", "painting, mathematics, calligraphy, poetry, and theater", "part of a pre-recorded television program, Rendezvous with Destiny", "sin", "whether they wish to collect a jackpot prize in cash or annuity", "Vader's daughter", "Buffalo Bill", "justice", "France", "neutrality", "coffee", "permissible", "Arthur Russell", "people", "Alex Burrall, Jason Weaver and Wylie Draper played Michael Jackson in different eras", "political", "the university's off-campus rental policies", "Dennis Hull", "Pittsburgh Steelers", "famine"], "metric_results": {"EM": 0.8125, "QA-F1": 0.8583333333333334}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666665, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4]}}, "error_ids": ["mrqa_triviaqa-validation-3967", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-3342", "mrqa_naturalquestions-validation-309", "mrqa_hotpotqa-validation-599", "mrqa_squad-validation-4774"], "after_eval": {"predictions": ["James Stenbeck", "Section.80", "yosemite national park", "interventive", "3", "Bishop Lloyd Christ Wicke", "georgia", "During his epic battle with Frieza", "the director's own approved edit", "margaret thatcher", "unesco", "Thon Maker", "loop", "painting, mathematics, calligraphy, poetry, and theater", "part of a pre-recorded television program, Rendezvous with Destiny", "sin", "whether they wish to collect a jackpot prize in cash or annuity", "Vader's daughter", "Buffalo Bill", "justice", "France", "neutrality", "coffee", "permissible", "Arthur Russell", "people", "Alex Burrall, Jason Weaver and Wylie Draper played Michael Jackson in different eras", "political", "the university's off-campus rental policies", "Dennis Hull, as well as painter Manley MacDonald.", "Pittsburgh Steelers", "war, famine, and weather"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9583333333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666665, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-2607", "before_prediction": "shirley williams", "after_prediction": "margaret thatcher"}], "retained_ids": ["mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-5165", "mrqa_squad-validation-5665", "mrqa_naturalquestions-validation-49", "mrqa_squad-validation-9860", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-2849", "mrqa_naturalquestions-validation-2445", "mrqa_squad-validation-8248", "mrqa_naturalquestions-validation-2582", "mrqa_squad-validation-2259", "mrqa_naturalquestions-validation-3789", "mrqa_hotpotqa-validation-4075", "mrqa_naturalquestions-validation-4043", "mrqa_squad-validation-2263", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-1649", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-4384", "mrqa_hotpotqa-validation-962", "mrqa_squad-validation-2337", "mrqa_naturalquestions-validation-1531", "mrqa_squad-validation-9518", "mrqa_squad-validation-7947", "mrqa_squad-validation-264"], "fixed_ids": ["mrqa_triviaqa-validation-3967", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-309", "mrqa_hotpotqa-validation-599", "mrqa_squad-validation-4774"], "unfixed_ids": ["mrqa_naturalquestions-validation-3342"], "instant_fixing_rate": 0.8333333333333334, "instant_retention_rate": 0.9615384611686391}, {"timecode": 49, "before_eval": {"predictions": ["light utility vehicles", "Sesame Street", "Genghis Khan and particularly Timur", "\"Losing My Religion\"", "Tamil Nadu in the south to West Bengal in the north through Andhra Pradesh and Odisha", "Ravenna (], also locally ] ; Romagnol: \"Rav\u00e8na\" ) is the capital city of the Province of Ravenna, in the Emilia- Romagna region of Northern Italy.", "Bocelli became completely blind at the age of 12", "meat", "September 1895", "improved", "VTOL aircraft", "assigned them to the company", "communist", "Gregor Mendel", "Reserved", "Los Angeles Xtreme, San Francisco Demons and Memphis Maniax", "3,600 Frenchmen", "State Street", "the Saudi Arab kingdom", "110", "my fair lady", "falcon", "Lawton Mainor Chiles Jr.", "`` Kobol's Last Gleaming ''", "McCrary", "informal\" imperialism", "bread, chapati, mahamri, boiled sweet potatoes or yams. Ugali with vegetables, sour milk, meat, fish or any other stew is generally eaten by much of the population for lunch or supper", "adenosine triphosphate", "stonehenge", "Ruth Elizabeth \"Bette\" Davis", "cobalt", "29 September 2014"], "metric_results": {"EM": 0.71875, "QA-F1": 0.8806580643869366}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.33333333333333337, 1.0, 0.9655172413793104, 0.7741935483870968, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.5365853658536585, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-6257", "mrqa_naturalquestions-validation-2275", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-987", "mrqa_squad-validation-716", "mrqa_squad-validation-10291", "mrqa_hotpotqa-validation-1315", "mrqa_naturalquestions-validation-5283", "mrqa_squad-validation-8518"], "after_eval": {"predictions": ["light utility vehicles", "Sesame Street", "Timur", "\"Losing My Religion\"", "from Tamil Nadu in the south to West Bengal in the north through Andhra Pradesh and Odisha", "Ravenna (], also locally ] ; Romagnol: \"Rav\u00e8na\" ) is the capital city of the Province of Ravenna", "Bocelli became completely blind at the age of 12", "meat", "1895", "improved markedly", "VTOL aircraft", "assigned them to the company", "communist", "Gregor Mendel", "Reserved", "Los Angeles Xtreme, San Francisco Demons and Memphis Maniax", "18,000 regulars, militia and Native American allies", "State Street", "Kingdom of Saudi Arabia", "110", "my fair lady", "falcon", "Lawton Chiles", "`` Kobol's Last Gleaming ''", "McCrary", "informal\" imperialism", "Ugali with vegetables, sour milk, meat, fish or any other stew", "adenosine triphosphate", "stonehenge", "Ruth Elizabeth \"Bette\" Davis", "cobalt", "29 September 2014"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9732142857142857}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-9013", "before_prediction": "the Saudi Arab kingdom", "after_prediction": "Kingdom of Saudi Arabia"}], "retained_ids": ["mrqa_naturalquestions-validation-2272", "mrqa_hotpotqa-validation-5452", "mrqa_hotpotqa-validation-2032", "mrqa_hotpotqa-validation-2941", "mrqa_triviaqa-validation-6073", "mrqa_squad-validation-1459", "mrqa_squad-validation-1306", "mrqa_hotpotqa-validation-2810", "mrqa_naturalquestions-validation-3663", "mrqa_squad-validation-9489", "mrqa_hotpotqa-validation-3964", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-4530", "mrqa_triviaqa-validation-3250", "mrqa_triviaqa-validation-6475", "mrqa_squad-validation-7049", "mrqa_squad-validation-9807", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-5876", "mrqa_hotpotqa-validation-1772", "mrqa_triviaqa-validation-6380", "mrqa_hotpotqa-validation-2750"], "fixed_ids": ["mrqa_squad-validation-6257", "mrqa_naturalquestions-validation-2275", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-987", "mrqa_squad-validation-716", "mrqa_squad-validation-10291", "mrqa_hotpotqa-validation-1315", "mrqa_squad-validation-8518"], "unfixed_ids": ["mrqa_naturalquestions-validation-5283"], "instant_fixing_rate": 0.8888888888888888, "instant_retention_rate": 0.9565217387145557}], "final_eval_results": {"overall_oncoming_test": {"EM": 0.439375, "QA-F1": 0.5247465731356139}, "overall_error_number": 897, "overall_instant_fixing_rate": 0.924596536456361, "final_instream_test": {"EM": 0.89, "QA-F1": 0.9271638631739344}, "final_upstream_test": {"EM": 0.675, "QA-F1": 0.7420444152078377}}}