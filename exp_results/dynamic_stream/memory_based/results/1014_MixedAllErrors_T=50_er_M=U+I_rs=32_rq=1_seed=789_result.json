{"model_update_steps": 1950, "method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/1014_MixedAllErrors_T=50_er_M=U+I_rs=32_rq=1_seed=789_ckpts/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/1014_MixedAllErrors_T=50_er_M=U+I_rs=32_rq=1_seed=789_ckpts/', replay_candidate_size=8, replay_frequency=1, replay_size=32, save_all_ckpts=0, skip_instant_eval=True, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=4, pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.upstream_eval.jsonl', predict_batch_size=16, replay_stream_json_path='', sampled_upstream_json_path='exp_results/data_streams/mrqa.nq_train.memory.jsonl', task_name='mrqa_naturalquestions', train_batch_size=8, use_sampled_upstream=True)", "online_eval_results": [{"timecode": 0, "before_eval": {"predictions": ["Raymond Briggs' 1978 children's book 'The Snowman", "the acceleration due to gravity decreased as an inverse square law", "a marquetry commode by the \u00e9b\u00e9niste Jean Henri Riesener dated c1780", "a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work", "lymphocyte", "Chinghiz", "Doctor Who and the Daleks in the Seven Keys to Doomsday", "Super Bowl 50 Host Committee", "a satirical television comedy programme on BBC Television in 1962 and 1963", "dynasty", "Br'er Rabbit", "Bodhi Natural Health Products", "Amphitrite Goddess of the Sea", "the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ), the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho", "March 14 ( 3 / 14 in the month / day format ) since 3, 1, and 4 are the first three significant digits of \u03c0", "the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing", "Horace Rumpole", "combustion", "A Sunday Afternoon on the Island of La Grande Jatte", "nobody knows for sure how a do-over in golf came to be called a mulligan", "Captain Meriwether Lewis's 30th birthday", "An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses", "2011", "electric eels", "before the first year begins", "New Jerusalem", "Samantha Spiro", "2013", "Ronnie Biggs", "Steve Carell as Felonious Gru, the former villain turned Anti-Villain League agent, Margo, Edith, and Agnes'adoptive father, and Lucy's husband", "an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "the assassination of US President John F. Kennedy"], "metric_results": {"EM": 0.0, "QA-F1": 0.0812722081839729}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.4799999999999999, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.11764705882352942, 0.15384615384615383]}}, "error_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-10322", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-2376", "mrqa_squad-validation-6677", "mrqa_squad-validation-6303", "mrqa_squad-validation-7821", "mrqa_squad-validation-392", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9688", "mrqa_triviaqa-validation-1551", "mrqa_squad-validation-3478", "mrqa_triviaqa-validation-5937", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-2730", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-4185", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3241", "mrqa_triviaqa-validation-7369", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-validation-7017", "mrqa_squad-validation-7746"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 1, "before_eval": {"predictions": ["the town of Acolman, just north of Mexico City", "The Iroquois", "philanthropy", "Catherine Zeta Jones", "Virginia Wade", "Gary Morris", "the anterolateral system", "1966", "radioisotope thermoelectric generator", "product or policy that is open and honest", "The Stock Market crash in New York", "New York Stadium", "john Bercow", "continental units", "john Forster", "Comptroller General of the Receipt and Issue of Her Majesty's Exchequer", "Sergio P\u00e9rez", "River Welland", "The Concubine", "1543", "the final revelation of God the Final Testament", "Vigor", "a policeman who investigates a series of mysterious killings and illnesses", "glowed even when turned off", "Florence Nightingale", "Budweiser", "mathematical-genius brother, Charlie", "George Cross", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object", "the exclusive rights to Superman", "May and June 2010"], "metric_results": {"EM": 0.125, "QA-F1": 0.18381734006734007}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.07407407407407407, 0.4, 0.4]}}, "error_ids": ["mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-5144", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-5406", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-1924", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-validation-1364", "mrqa_hotpotqa-validation-14", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-3971", "mrqa_squad-validation-2629", "mrqa_triviaqa-validation-2210", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-5802", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-2096", "mrqa_naturalquestions-validation-1864", "mrqa_squad-validation-10410", "mrqa_triviaqa-validation-2367", "mrqa_hotpotqa-validation-3774"], "retrieved_ids": ["mrqa_naturalquestions-train-64877", "mrqa_naturalquestions-train-34606", "mrqa_naturalquestions-train-16195", "mrqa_triviaqa-validation-5937", "mrqa_naturalquestions-train-55422", "mrqa_naturalquestions-train-60999", "mrqa_naturalquestions-train-80708", "mrqa_naturalquestions-train-9516", "mrqa_naturalquestions-train-55149", "mrqa_naturalquestions-train-42305", "mrqa_naturalquestions-train-87885", "mrqa_naturalquestions-train-71385", "mrqa_naturalquestions-train-36648", "mrqa_naturalquestions-train-43493", "mrqa_naturalquestions-train-17790", "mrqa_naturalquestions-train-30083", "mrqa_naturalquestions-train-76832", "mrqa_naturalquestions-train-12277", "mrqa_naturalquestions-train-56376", "mrqa_naturalquestions-train-64300", "mrqa_naturalquestions-train-44386", "mrqa_naturalquestions-train-12335", "mrqa_naturalquestions-train-81468", "mrqa_naturalquestions-train-45978", "mrqa_naturalquestions-train-73605", "mrqa_naturalquestions-train-43261", "mrqa_naturalquestions-train-69935", "mrqa_naturalquestions-train-21162", "mrqa_naturalquestions-train-20596", "mrqa_naturalquestions-train-57121", "mrqa_naturalquestions-train-7748", "mrqa_naturalquestions-train-72006"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 2, "before_eval": {"predictions": ["MS Kronprins Harald", "boxing", "Puritanism", "+, -, *, and / keys", "2009", "in different parts of the globe", "Wales", "acetate", "John II Casimir Vasa", "marries Lord Darnley", "A55", "phylum with relatively few species", "`` Everywhere '' is a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "many residents of metropolitan regions work within the central urban area, and choose to live in satellite communities called suburbs and commute to work via automobile or mass transit", "is no longer an exempt", "Bothtec", "Terry Reid", "is of sufficient quality", "sept Princesses", "North America", "Andr\u00e9 3000", "rookies", "Akhenaten", "Theodore Roosevelt", "the fourth season", "four", "the Western Bloc ( the United States, its NATO allies and others )", "the 1970s", "is an opera in four acts by French composer Georges Bizet", "Matt Winer", "1688 and 1689", "the Pacific across the Amazonas Basin"], "metric_results": {"EM": 0.125, "QA-F1": 0.26191938168041107}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.25, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.1, 0.9019607843137255, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.4, 1.0, 0.0, 0.5, 0.0, 1.0, 0.3636363636363636, 0.0, 0.18181818181818182, 0.4, 0.0, 0.4]}}, "error_ids": ["mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-8542", "mrqa_squad-validation-7149", "mrqa_naturalquestions-validation-10364", "mrqa_squad-validation-4253", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-validation-1202", "mrqa_hotpotqa-validation-3632", "mrqa_triviaqa-validation-1935", "mrqa_squad-validation-4456", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-8448", "mrqa_triviaqa-validation-2136", "mrqa_naturalquestions-validation-539", "mrqa_squad-validation-8513", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-validation-5502", "mrqa_hotpotqa-validation-2679", "mrqa_naturalquestions-validation-6896", "mrqa_naturalquestions-validation-2501", "mrqa_hotpotqa-validation-1376", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-683", "mrqa_triviaqa-validation-2722", "mrqa_hotpotqa-validation-4367", "mrqa_squad-validation-3113", "mrqa_squad-validation-4283"], "retrieved_ids": ["mrqa_naturalquestions-train-39148", "mrqa_naturalquestions-train-8018", "mrqa_naturalquestions-train-18064", "mrqa_naturalquestions-train-83231", "mrqa_naturalquestions-train-51653", "mrqa_naturalquestions-train-31848", "mrqa_naturalquestions-train-38341", "mrqa_naturalquestions-train-4389", "mrqa_naturalquestions-train-25133", "mrqa_naturalquestions-train-71101", "mrqa_naturalquestions-train-61859", "mrqa_naturalquestions-train-63703", "mrqa_naturalquestions-train-40743", "mrqa_naturalquestions-train-23005", "mrqa_naturalquestions-train-24474", "mrqa_naturalquestions-train-4298", "mrqa_naturalquestions-train-83913", "mrqa_naturalquestions-train-60289", "mrqa_naturalquestions-train-44530", "mrqa_naturalquestions-train-65378", "mrqa_naturalquestions-train-53652", "mrqa_naturalquestions-train-73434", "mrqa_naturalquestions-train-34229", "mrqa_naturalquestions-train-35039", "mrqa_naturalquestions-train-16130", "mrqa_naturalquestions-train-75572", "mrqa_naturalquestions-train-18690", "mrqa_naturalquestions-train-42307", "mrqa_naturalquestions-train-43281", "mrqa_naturalquestions-train-62068", "mrqa_naturalquestions-train-56208", "mrqa_naturalquestions-train-43133"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 3, "before_eval": {"predictions": ["id", "four-year-plan", "id", "between 27 July and 7 August 2022", "New York", "brazer", "2006 British Academy Television Award for Best Drama Series", "Least of the Great Powers", "the lower motor neurons, the efferent nerves that directly innervate muscles", "babbage", "Impulse - Packaging", "Dettori", "death mask", "bollywood", "Overtime", "Sir Henry Cole", "it has trouble distinguishing between carbon dioxide and oxygen", "is a British sitcom, broadcast in the United Kingdom from 1982 to 1984", "Clyde Barrow", "Arlene Foster of the Democratic Unionist Party (DUP )", "23 July 1989", "many educational institutions especially within the US", "in many traditions of Hinduism - especially those common in the West", "timeslot 16 on an E1, while it is timeslot 24 for a T1", "Postcards from Paradise", "Callability", "a 2.26 GHz quad - core Snapdragon 800 processor with 2 GB of RAM, either 16 or 32 GB of internal storage, and a 2300 mAh battery", "over 10,000 British and 2,000 old master works", "al - khimar", "proteins", "laparoscopic cholecystectomy", "berenice Abbott"], "metric_results": {"EM": 0.03125, "QA-F1": 0.10400061967602289}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.09523809523809525, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.18181818181818182, 0.0, 0.0, 0.0, 0.45161290322580644, 0.4, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-5362", "mrqa_naturalquestions-validation-5647", "mrqa_squad-validation-3058", "mrqa_triviaqa-validation-7382", "mrqa_squad-validation-7816", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-2571", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2327", "mrqa_triviaqa-validation-365", "mrqa_squad-validation-1539", "mrqa_triviaqa-validation-3901", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-3189", "mrqa_squad-validation-8832", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-2150", "mrqa_squad-validation-2191", "mrqa_squad-validation-2069", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-6402", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-6341", "mrqa_squad-validation-5517", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-5818", "mrqa_triviaqa-validation-6800", "mrqa_triviaqa-validation-2530"], "retrieved_ids": ["mrqa_naturalquestions-train-6311", "mrqa_naturalquestions-train-41411", "mrqa_naturalquestions-train-16782", "mrqa_naturalquestions-train-58475", "mrqa_naturalquestions-train-40156", "mrqa_naturalquestions-train-10041", "mrqa_naturalquestions-train-49644", "mrqa_naturalquestions-train-86826", "mrqa_naturalquestions-train-80951", "mrqa_naturalquestions-train-42821", "mrqa_naturalquestions-train-77661", "mrqa_naturalquestions-train-44360", "mrqa_naturalquestions-train-28414", "mrqa_naturalquestions-train-39529", "mrqa_naturalquestions-train-35951", "mrqa_naturalquestions-train-51326", "mrqa_naturalquestions-train-65057", "mrqa_naturalquestions-train-78201", "mrqa_naturalquestions-train-396", "mrqa_naturalquestions-train-36484", "mrqa_naturalquestions-train-8562", "mrqa_naturalquestions-train-76755", "mrqa_naturalquestions-train-32633", "mrqa_naturalquestions-train-38817", "mrqa_naturalquestions-train-16027", "mrqa_naturalquestions-train-47816", "mrqa_naturalquestions-train-72405", "mrqa_naturalquestions-train-17330", "mrqa_naturalquestions-train-15170", "mrqa_naturalquestions-train-28169", "mrqa_naturalquestions-train-63309", "mrqa_naturalquestions-train-60297"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 4, "before_eval": {"predictions": ["United Kingdom", "at Remagen", "December 9, 2016", "NASA discontinued the manned Block I program, using the Block I spacecraft only for unmanned Saturn V flights", "British progressive folk-rock band Gryphon", "1898", "January", "a shepherd", "at elevation 2 meters above sea level", "tetanus disease", "bounding the time or space used by the algorithm", "maxerboard", "Alex O'Loughlin", "Eddie Leonski", "Jack Ridley", "a mixture of phencyclidine and cocaine", "bunker", "1934", "the Reverse - Flash", "All Hallows'Day", "1968", "EOC", "new converts", "Mona Vanderwaal", "cricket", "Pyotr Ilyich Tchaikovsky", "2001 or 2010", "Quebec", "comprehend and formulate language", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "steam turbine", "Splodgenessabounds"], "metric_results": {"EM": 0.25, "QA-F1": 0.33340773809523805}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.4, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.25, 0.0, 0.0, 0.4, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-6399", "mrqa_squad-validation-9358", "mrqa_naturalquestions-validation-5146", "mrqa_squad-validation-3971", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-3647", "mrqa_triviaqa-validation-7253", "mrqa_squad-validation-1688", "mrqa_triviaqa-validation-3808", "mrqa_hotpotqa-validation-1168", "mrqa_hotpotqa-validation-1289", "mrqa_hotpotqa-validation-2944", "mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-5654", "mrqa_naturalquestions-validation-2900", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-3978", "mrqa_squad-validation-3467"], "retrieved_ids": ["mrqa_naturalquestions-train-61948", "mrqa_naturalquestions-train-61937", "mrqa_naturalquestions-train-68547", "mrqa_naturalquestions-train-62903", "mrqa_naturalquestions-train-68041", "mrqa_naturalquestions-train-75844", "mrqa_naturalquestions-train-49827", "mrqa_naturalquestions-train-56544", "mrqa_naturalquestions-train-59311", "mrqa_naturalquestions-train-73534", "mrqa_naturalquestions-train-80023", "mrqa_naturalquestions-train-83114", "mrqa_naturalquestions-train-61292", "mrqa_naturalquestions-train-24119", "mrqa_naturalquestions-train-10554", "mrqa_naturalquestions-train-24082", "mrqa_naturalquestions-train-28105", "mrqa_naturalquestions-train-20613", "mrqa_naturalquestions-train-77088", "mrqa_naturalquestions-train-50935", "mrqa_naturalquestions-train-34182", "mrqa_naturalquestions-train-29677", "mrqa_naturalquestions-train-37502", "mrqa_naturalquestions-train-44690", "mrqa_naturalquestions-train-63807", "mrqa_naturalquestions-train-2399", "mrqa_naturalquestions-train-29697", "mrqa_naturalquestions-train-62068", "mrqa_naturalquestions-train-51573", "mrqa_naturalquestions-train-55983", "mrqa_naturalquestions-train-51644", "mrqa_naturalquestions-train-6133"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 5, "before_eval": {"predictions": ["otranto", "F\u00e9d\u00e9ration Sportive F\u00e9minine Internationale (FSFI )", "a soft wool fabric with a colorful swirled pattern of curved shapes", "Paspahegh Indians", "a branch of the anterior internodal tract", "South Dakota", "7 : 25 a.m. PDT", "a swanee or swannee whistle", "a genetically engineered plant", "used stone tools", "Cochin University of science and Technology", "parietal cells", "placental", "September 13, 1994", "june", "imperial rule", "1840", "a defiant speech, or a speech explaining their actions", "George Sylvester Viereck", "kinks", "a new study by the World Institute for Development Economics Research at United Nations University", "nonconservative forces", "my mind is averse to wedlock because I daily expect the death of a heretic", "8.7 -- 9.2", "China", "2 November 1902", "present - day southeastern Texas", "May 7, 2018", "9 October 19408 December 1980", "Selden", "structural collapses", "a man named Daedalus"], "metric_results": {"EM": 0.125, "QA-F1": 0.20046977124183007}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.47058823529411764, 0.0, 1.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.33333333333333337, 0.0, 0.0, 0.5]}}, "error_ids": ["mrqa_triviaqa-validation-4725", "mrqa_hotpotqa-validation-2197", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-2020", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3625", "mrqa_naturalquestions-validation-4513", "mrqa_triviaqa-validation-1550", "mrqa_triviaqa-validation-5704", "mrqa_hotpotqa-validation-4621", "mrqa_squad-validation-6733", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-254", "mrqa_squad-validation-7554", "mrqa_squad-validation-2757", "mrqa_naturalquestions-validation-2884", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-1489", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-5848", "mrqa_squad-validation-2531", "mrqa_triviaqa-validation-4560"], "retrieved_ids": ["mrqa_naturalquestions-train-55800", "mrqa_naturalquestions-train-87192", "mrqa_naturalquestions-train-20736", "mrqa_naturalquestions-train-31007", "mrqa_naturalquestions-train-38551", "mrqa_naturalquestions-train-21783", "mrqa_naturalquestions-train-77563", "mrqa_naturalquestions-train-33854", "mrqa_naturalquestions-train-67255", "mrqa_naturalquestions-train-36550", "mrqa_naturalquestions-train-16995", "mrqa_naturalquestions-train-8339", "mrqa_naturalquestions-train-26798", "mrqa_naturalquestions-train-67177", "mrqa_naturalquestions-train-12954", "mrqa_naturalquestions-train-8861", "mrqa_naturalquestions-train-10111", "mrqa_naturalquestions-train-69717", "mrqa_naturalquestions-train-54288", "mrqa_naturalquestions-train-76738", "mrqa_naturalquestions-train-38626", "mrqa_naturalquestions-train-46240", "mrqa_naturalquestions-train-50727", "mrqa_naturalquestions-train-72269", "mrqa_naturalquestions-train-37229", "mrqa_naturalquestions-train-36146", "mrqa_naturalquestions-train-85700", "mrqa_naturalquestions-train-17749", "mrqa_naturalquestions-train-708", "mrqa_naturalquestions-train-15887", "mrqa_naturalquestions-train-87234", "mrqa_naturalquestions-train-39049"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 6, "before_eval": {"predictions": ["zinnemann", "to prevent the flame from being blown out", "Barack Hussein Obama II", "1996", "n Carolina", "Bermuda", "90-60's", "an aided or an unaided school", "dolph Camilli", "the symbol \u00d7", "Best Supporting Actress", "Juice Newton", "1960", "HTTP Secure ( HTTPS )", "late - September through early January", "The Chisholm Trail", "monatomic", "for its popular beaches", "june", "clangers", "sunny fruit", "blood", "The anterior interventricular branch of left coronary artery, ( also left anterior descending artery ( LAD ), or anterior descending branch )", "1.1 \u00d7 1011 metric tonnes", "clangers", "leaf", "Indian club ATK", "land that a nation has conquered and expanded", "near Grande Comore, Comoros Islands", "`` 5 lakhs of rupees ''", "the Norwegian language", "burning of fossil fuels"], "metric_results": {"EM": 0.1875, "QA-F1": 0.22499999999999998}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-4165", "mrqa_hotpotqa-validation-5810", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-6887", "mrqa_triviaqa-validation-5261", "mrqa_squad-validation-2659", "mrqa_squad-validation-6947", "mrqa_triviaqa-validation-105", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-484", "mrqa_triviaqa-validation-5795", "mrqa_squad-validation-3463", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-227", "mrqa_naturalquestions-validation-5582", "mrqa_squad-validation-4181", "mrqa_triviaqa-validation-4966", "mrqa_squad-validation-8821", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-10042", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-6644"], "retrieved_ids": ["mrqa_naturalquestions-train-59239", "mrqa_naturalquestions-train-65277", "mrqa_naturalquestions-train-83796", "mrqa_naturalquestions-train-79779", "mrqa_naturalquestions-train-16437", "mrqa_naturalquestions-train-31868", "mrqa_naturalquestions-train-9090", "mrqa_naturalquestions-train-13206", "mrqa_naturalquestions-train-1404", "mrqa_naturalquestions-train-54838", "mrqa_naturalquestions-train-69565", "mrqa_naturalquestions-train-48900", "mrqa_naturalquestions-train-11491", "mrqa_naturalquestions-train-10574", "mrqa_naturalquestions-train-31251", "mrqa_naturalquestions-train-40665", "mrqa_naturalquestions-train-3076", "mrqa_squad-validation-2191", "mrqa_naturalquestions-train-39985", "mrqa_naturalquestions-train-6930", "mrqa_naturalquestions-train-83218", "mrqa_naturalquestions-train-17368", "mrqa_naturalquestions-train-16871", "mrqa_naturalquestions-train-23275", "mrqa_naturalquestions-train-20042", "mrqa_naturalquestions-train-79972", "mrqa_naturalquestions-train-81410", "mrqa_naturalquestions-train-66511", "mrqa_naturalquestions-train-87477", "mrqa_naturalquestions-validation-539", "mrqa_naturalquestions-train-29076", "mrqa_naturalquestions-train-80019"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 7, "before_eval": {"predictions": ["the wisdom and prudence of certain decisions of procurement", "The Q'eqchi '", "Only a few", "The U.S. Army Chaplain insignia", "Kairi", "both inner city blacks, who wanted more involvement in government, and whites in the suburbs, who want more services and more control over the central city", "director", "near the Black Sea", "the last book accepted into the Christian biblical canon", "Bruno Mars", "172.41", "gallantry", "16 million", "1950s", "work oxen for haulage", "1998", "Jeff Brannigan", "23.1", "18", "family member", "long-term environmental changes", "William Powell Lear", "the radial (centripetal ) force", "Terrell Suggs", "decide on all the motions and amendments that have been moved that day", "to represent'a voyage of adventure'on which the programme would set out", "Abraham Gottlob Werner", "june", "present-day Charleston", "calling for the destruction of Israel and the establishment of an Islamic state in Palestine", "Scott Dunlop", "Panzer"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2758680555555556}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.09523809523809525, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5522", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-983", "mrqa_squad-validation-7296", "mrqa_triviaqa-validation-7415", "mrqa_naturalquestions-validation-859", "mrqa_squad-validation-108", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-1293", "mrqa_squad-validation-7799", "mrqa_hotpotqa-validation-3898", "mrqa_naturalquestions-validation-98", "mrqa_squad-validation-358", "mrqa_hotpotqa-validation-3846", "mrqa_squad-validation-3558", "mrqa_naturalquestions-validation-824", "mrqa_hotpotqa-validation-1142", "mrqa_squad-validation-10403", "mrqa_naturalquestions-validation-3474", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-291", "mrqa_triviaqa-validation-781", "mrqa_squad-validation-3181", "mrqa_squad-validation-9598", "mrqa_hotpotqa-validation-2902"], "retrieved_ids": ["mrqa_naturalquestions-train-35164", "mrqa_naturalquestions-train-46200", "mrqa_naturalquestions-train-23573", "mrqa_naturalquestions-train-70855", "mrqa_naturalquestions-train-18543", "mrqa_naturalquestions-train-68743", "mrqa_naturalquestions-train-62596", "mrqa_naturalquestions-train-63141", "mrqa_naturalquestions-train-44892", "mrqa_triviaqa-validation-1792", "mrqa_naturalquestions-train-36398", "mrqa_naturalquestions-train-3310", "mrqa_naturalquestions-train-44510", "mrqa_naturalquestions-train-19694", "mrqa_naturalquestions-train-74005", "mrqa_naturalquestions-train-79720", "mrqa_naturalquestions-train-84933", "mrqa_naturalquestions-train-83394", "mrqa_naturalquestions-train-26719", "mrqa_naturalquestions-train-41055", "mrqa_naturalquestions-train-25890", "mrqa_naturalquestions-train-41801", "mrqa_naturalquestions-train-74393", "mrqa_naturalquestions-train-35493", "mrqa_naturalquestions-train-30204", "mrqa_naturalquestions-train-11916", "mrqa_naturalquestions-train-49897", "mrqa_naturalquestions-train-12824", "mrqa_naturalquestions-train-85256", "mrqa_naturalquestions-train-52033", "mrqa_naturalquestions-train-8357", "mrqa_hotpotqa-validation-2944"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 8, "before_eval": {"predictions": ["theoretical computer science", "georgeppe Antonio 'Nino' Farina", "37", "about 5 nanometers across, arranged in rows 6.4 nanometers apart", "the eighth and eleventh episodes of the season", "Kyle Busch", "400", "adrenal glands", "liberal arts", "the Bowland Fells", "Richard, Duke of Gloucester", "St. Louis County", "1918", "2018", "george warnock", "law firm", "Pottawatomie County", "orangutan", "Albert Einstein", "The church tower", "george dyer", "Toronto", "foreign", "110 miles", "Kona coast of the island of Hawai\u02bb i about 12 mi south of Kailua-Kona", "Liberal conservatism", "largest gold rushes the world has ever seen", "six", "not guilty", "psychoanalysis", "Quentin Coldwater, a young man who discovers and attends a college of magic in New York", "acidic bogs"], "metric_results": {"EM": 0.1875, "QA-F1": 0.2722222222222222}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2666666666666667, 0.5, 0.4444444444444445, 1.0, 0.0, 0.0, 0.25, 0.6666666666666666]}}, "error_ids": ["mrqa_squad-validation-1705", "mrqa_triviaqa-validation-1521", "mrqa_naturalquestions-validation-6089", "mrqa_squad-validation-8869", "mrqa_naturalquestions-validation-856", "mrqa_triviaqa-validation-7506", "mrqa_naturalquestions-validation-1360", "mrqa_hotpotqa-validation-3161", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5401", "mrqa_triviaqa-validation-7157", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-validation-3309", "mrqa_squad-validation-10369", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-582", "mrqa_triviaqa-validation-4268", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-4812", "mrqa_squad-validation-2987", "mrqa_squad-validation-6915", "mrqa_triviaqa-validation-7767", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-187"], "retrieved_ids": ["mrqa_naturalquestions-train-68048", "mrqa_naturalquestions-train-14520", "mrqa_naturalquestions-train-85554", "mrqa_naturalquestions-train-16100", "mrqa_naturalquestions-train-31911", "mrqa_naturalquestions-train-21529", "mrqa_naturalquestions-train-82952", "mrqa_naturalquestions-train-63175", "mrqa_naturalquestions-train-3478", "mrqa_naturalquestions-train-21300", "mrqa_naturalquestions-train-29886", "mrqa_naturalquestions-train-71624", "mrqa_naturalquestions-train-54715", "mrqa_naturalquestions-train-50481", "mrqa_naturalquestions-train-41535", "mrqa_naturalquestions-train-13788", "mrqa_naturalquestions-train-19157", "mrqa_naturalquestions-train-87685", "mrqa_naturalquestions-train-33229", "mrqa_naturalquestions-train-21146", "mrqa_naturalquestions-train-18339", "mrqa_naturalquestions-train-70203", "mrqa_naturalquestions-train-88029", "mrqa_naturalquestions-train-41503", "mrqa_naturalquestions-train-58357", "mrqa_naturalquestions-train-80951", "mrqa_naturalquestions-train-49953", "mrqa_naturalquestions-train-47917", "mrqa_naturalquestions-train-81122", "mrqa_naturalquestions-train-51246", "mrqa_naturalquestions-train-74354", "mrqa_naturalquestions-train-1022"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 9, "before_eval": {"predictions": ["the English phrase `` I Seek You ''", "Argentinian", "Ear Institute at the University College London", "almond paste", "photosynthesis", "images of different animals and humans perform various actions", "1600 Pennsylvania Avenue", "The Daily Stormer", "spin triplet", "water", "president", "citizens", "George, Margrave of Brandenburg-Ansbach", "Kamba version", "3D computer-animated comedy film", "w Worcester Cold Storage and Warehouse Co.", "acting", "C. W. Grafton", "liquid crystal on silicon ( L CoS ) ( based on an LCoS chip from Himax ), field - sequential color system, LED illuminated display", "Americans", "IPod Classic", "My Sassy Girl", "prevent damage to the body", "General Hospital", "Highly combustible materials that leave little residue, such as wood or coal", "pedagogy", "vaskania from the Megan Hieron Synek Demon ( \u039c\u03ad\u03b3\u03b1\u03bd \u0399\u03b5\u03c1\u03cc\u03bd \u03a3\u03c5\u03bd\u03ad\u03ba\u03b4\u03b7\u03bc\u03bf\u03bd ) book of prayers", "the root respiration", "soils", "drug dealer", "medium and heavy-duty diesel trucks", "testes"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3922540672540672}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.8888888888888888, 0.8, 1.0, 0.0, 0.13333333333333333, 0.4, 1.0, 1.0, 0.0, 0.8571428571428571, 0.8571428571428571, 0.6666666666666666, 1.0, 0.918918918918919, 0.4, 0.0, 0.3333333333333333, 0.15384615384615383, 0.0, 0.0, 0.0, 0.14285714285714285, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6844", "mrqa_squad-validation-5210", "mrqa_triviaqa-validation-3387", "mrqa_squad-validation-8924", "mrqa_squad-validation-5618", "mrqa_triviaqa-validation-2703", "mrqa_squad-validation-3442", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-321", "mrqa_squad-validation-8259", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-5128", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-1327", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1436", "mrqa_naturalquestions-validation-1294", "mrqa_squad-validation-5940", "mrqa_squad-validation-3490", "mrqa_squad-validation-1879", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-2449", "mrqa_naturalquestions-validation-3677"], "retrieved_ids": ["mrqa_naturalquestions-train-66074", "mrqa_naturalquestions-train-31638", "mrqa_naturalquestions-train-18866", "mrqa_naturalquestions-train-85064", "mrqa_naturalquestions-train-47039", "mrqa_naturalquestions-train-79723", "mrqa_naturalquestions-train-73886", "mrqa_naturalquestions-train-76742", "mrqa_naturalquestions-train-464", "mrqa_naturalquestions-train-78633", "mrqa_naturalquestions-train-6563", "mrqa_naturalquestions-train-72017", "mrqa_naturalquestions-train-33786", "mrqa_naturalquestions-train-77417", "mrqa_naturalquestions-train-8810", "mrqa_naturalquestions-train-43283", "mrqa_naturalquestions-train-84161", "mrqa_naturalquestions-train-71706", "mrqa_naturalquestions-train-71270", "mrqa_naturalquestions-train-16354", "mrqa_naturalquestions-train-288", "mrqa_naturalquestions-train-47693", "mrqa_naturalquestions-train-61361", "mrqa_naturalquestions-train-7104", "mrqa_naturalquestions-train-39849", "mrqa_naturalquestions-train-19647", "mrqa_naturalquestions-train-8638", "mrqa_naturalquestions-train-9383", "mrqa_naturalquestions-train-17119", "mrqa_naturalquestions-train-11392", "mrqa_naturalquestions-train-85516", "mrqa_naturalquestions-train-48136"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 10, "before_eval": {"predictions": ["Pope, Alexander Pope", "yellow fever", "three legal systems", "Las Vegas", "optional message body", "globetrotters", "cruiserweight", "the fictional town of Ramelle", "a Dubliner tried to kill Benito Mussolini", "no plan", "cromlech", "Victoria resided with her mother prior to acceding the throne", "the base 10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "theory of east-to-west colonization of the islands", "the MGM Grand Garden Special Events Center", "digital fashion gallery", "Ronnie Hillman", "formulation of a single all-encompassing definition of the term is extremely difficult, if not impossible", "george w. Boggs", "more than 60 percent of the state's total land surface", "Eagle Ridge Mall", "Pel\u00e9", "reduce pressure on the public food supply", "Monastir / Tunisia / Africa", "fire", "James Thurston Nabors", "must be at least 18 or 21 years old ( or have a legal guardian present ), and must sign a waiver prior to shooting", "Ward", "novelist and poet", "The Jamestown settlement in the Colony of Virginia", "Rouen Cathedral", "tree growth stages"], "metric_results": {"EM": 0.125, "QA-F1": 0.26869588744588746}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.5, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.7272727272727273, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9090909090909091, 0.0, 0.0, 0.14285714285714288, 0.0, 0.19999999999999998, 0.3333333333333333, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1860", "mrqa_hotpotqa-validation-3149", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-8006", "mrqa_hotpotqa-validation-4162", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-6389", "mrqa_triviaqa-validation-4791", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-10205", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-5682", "mrqa_triviaqa-validation-681", "mrqa_squad-validation-273", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-6119", "mrqa_squad-validation-3018", "mrqa_hotpotqa-validation-897", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-1451", "mrqa_hotpotqa-validation-3976", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-4803", "mrqa_triviaqa-validation-6639", "mrqa_squad-validation-4506"], "retrieved_ids": ["mrqa_naturalquestions-train-10375", "mrqa_naturalquestions-train-12010", "mrqa_naturalquestions-train-215", "mrqa_naturalquestions-train-35448", "mrqa_naturalquestions-train-59662", "mrqa_naturalquestions-train-45009", "mrqa_squad-validation-4456", "mrqa_naturalquestions-train-56859", "mrqa_naturalquestions-train-59519", "mrqa_naturalquestions-train-10001", "mrqa_naturalquestions-train-85505", "mrqa_naturalquestions-train-31501", "mrqa_naturalquestions-train-39393", "mrqa_naturalquestions-train-12559", "mrqa_naturalquestions-train-26357", "mrqa_naturalquestions-train-39195", "mrqa_naturalquestions-train-56723", "mrqa_naturalquestions-train-74065", "mrqa_naturalquestions-train-77161", "mrqa_naturalquestions-train-48955", "mrqa_naturalquestions-train-80559", "mrqa_naturalquestions-train-4173", "mrqa_naturalquestions-train-34987", "mrqa_naturalquestions-train-83861", "mrqa_naturalquestions-train-42950", "mrqa_naturalquestions-train-10993", "mrqa_naturalquestions-train-64161", "mrqa_naturalquestions-train-48741", "mrqa_naturalquestions-train-37787", "mrqa_naturalquestions-train-22570", "mrqa_naturalquestions-train-72462", "mrqa_naturalquestions-train-13148"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 11, "before_eval": {"predictions": ["Vince Lombardi", "Arthur Schnitzler's 1926 novella \" Traumnovelle\"", "a Gender pay gap in favor of males in the labor market", "The TEU", "ice melting", "a father, Paul Monti, whose son, Medal of Honor recipient Jared, was killed in Afghanistan while trying to save a fellow soldier", "tunisia", "The world's longest suspension bridges are listed according to the length of their main span", "tunisia", "tunisia", "tunisia", "the Bulgars, and especially the Seljuk Turks", "died in battle", "tunisia", "tunisia", "North American Technate", "Queen Elizabeth I", "infection, irritation, or allergies", "The tower is the most - visited paid monument in the world. An average of 25,000 people ascend the tower every day which can result in long queues", "the Vittorio Emanuele II Gallery and Piazza della Scala", "catfish aquaculture", "atomic number 53", "Evermoist", "Iraq", "a co-op of grape growers", "tunisia", "tunisia", "1952", "the Charlotte Hornets", "`` speed limit '' omitted and an additional panel stating the type of hazard ahead", "Jean F kernel ( 1497 -- 1558 ), a French physician", "the back of the head"], "metric_results": {"EM": 0.09375, "QA-F1": 0.21925595238095238}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true], "QA-F1": [0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.0, 0.0, 0.9090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.24000000000000002, 0.5, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.09090909090909091, 0.22222222222222224, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-5078", "mrqa_hotpotqa-validation-2852", "mrqa_squad-validation-7447", "mrqa_squad-validation-4169", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-5248", "mrqa_squad-validation-1003", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-validation-441", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-5184", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-9054", "mrqa_triviaqa-validation-7703", "mrqa_hotpotqa-validation-3921", "mrqa_triviaqa-validation-792", "mrqa_triviaqa-validation-5526", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5769"], "retrieved_ids": ["mrqa_naturalquestions-train-61658", "mrqa_naturalquestions-train-42769", "mrqa_naturalquestions-train-56943", "mrqa_naturalquestions-train-9617", "mrqa_naturalquestions-train-58120", "mrqa_naturalquestions-train-73368", "mrqa_naturalquestions-train-26623", "mrqa_naturalquestions-train-59165", "mrqa_naturalquestions-train-36480", "mrqa_naturalquestions-train-59147", "mrqa_naturalquestions-train-38786", "mrqa_naturalquestions-train-30040", "mrqa_naturalquestions-train-61286", "mrqa_naturalquestions-train-35112", "mrqa_squad-validation-5622", "mrqa_naturalquestions-train-60848", "mrqa_naturalquestions-train-6467", "mrqa_naturalquestions-train-53835", "mrqa_naturalquestions-train-15170", "mrqa_naturalquestions-train-84346", "mrqa_naturalquestions-train-64015", "mrqa_naturalquestions-train-33914", "mrqa_naturalquestions-train-72123", "mrqa_naturalquestions-train-51844", "mrqa_naturalquestions-train-17322", "mrqa_naturalquestions-train-43592", "mrqa_naturalquestions-train-62544", "mrqa_naturalquestions-train-28271", "mrqa_naturalquestions-train-59067", "mrqa_naturalquestions-train-77865", "mrqa_naturalquestions-train-14805", "mrqa_naturalquestions-train-10043"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 12, "before_eval": {"predictions": ["Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Joe Turano", "counter-parry", "Margaret Thatcher", "Kevin Harlan as play-by-play announcer, Boomer Esiason and Dan Fouts as color analysts, and James Lofton and Mark Malone as sideline reporters", "2014", "The stability, security, and predictability of British law and government enabled Hong Kong to flourish as a centre for international trade", "Minos and Kokalos", "18 November [O.S. 6 November ) 1860", "cienfuegos in the Las Villas province of Cuba", "dublin", "Forbes", "the French", "gi LaBelle", "dublin", "Orwell", "the Czech Kingdom", "Bob Hill", "not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence", "for creative reasons and `` not a reflection '' of the actress'performance", "adaptive immune system", "under the tutelage of his uncle", "a musician", "dublin", "December 1, 1969", "american", "dubodenal", "California State Automobile Association", "\"alone\"", "Cinderella", "the crew noticed a strange odor in their spacesuits, which delayed the sealing of the hatch", "due to a lack of understanding of the legal ramifications"], "metric_results": {"EM": 0.15625, "QA-F1": 0.30366339992299074}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false], "QA-F1": [0.19999999999999998, 0.0, 0.0, 1.0, 0.37037037037037035, 0.0, 0.34782608695652173, 0.0, 0.5, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.11764705882352941, 1.0, 1.0, 0.2424242424242424, 0.4, 0.0, 0.33333333333333337, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.8571428571428571]}}, "error_ids": ["mrqa_squad-validation-93", "mrqa_naturalquestions-validation-6167", "mrqa_triviaqa-validation-1671", "mrqa_squad-validation-568", "mrqa_hotpotqa-validation-2680", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-1758", "mrqa_hotpotqa-validation-4263", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-6902", "mrqa_hotpotqa-validation-1099", "mrqa_squad-validation-10202", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-2737", "mrqa_squad-validation-9793", "mrqa_naturalquestions-validation-5175", "mrqa_squad-validation-6678", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-7371", "mrqa_squad-validation-2812", "mrqa_squad-validation-3935", "mrqa_squad-validation-6924"], "retrieved_ids": ["mrqa_naturalquestions-train-44589", "mrqa_naturalquestions-train-53209", "mrqa_naturalquestions-train-8296", "mrqa_naturalquestions-train-46661", "mrqa_naturalquestions-train-63785", "mrqa_naturalquestions-train-21131", "mrqa_naturalquestions-train-51059", "mrqa_naturalquestions-train-17091", "mrqa_naturalquestions-train-68728", "mrqa_naturalquestions-train-50291", "mrqa_naturalquestions-train-36004", "mrqa_naturalquestions-train-23538", "mrqa_naturalquestions-train-3222", "mrqa_naturalquestions-train-50459", "mrqa_naturalquestions-train-2798", "mrqa_naturalquestions-train-35291", "mrqa_naturalquestions-train-88003", "mrqa_naturalquestions-train-83537", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-train-39472", "mrqa_naturalquestions-train-14604", "mrqa_naturalquestions-train-5004", "mrqa_naturalquestions-train-5903", "mrqa_naturalquestions-train-39009", "mrqa_naturalquestions-train-464", "mrqa_naturalquestions-train-31108", "mrqa_naturalquestions-train-26817", "mrqa_naturalquestions-train-68341", "mrqa_naturalquestions-train-80742", "mrqa_naturalquestions-train-46184", "mrqa_naturalquestions-train-20418", "mrqa_naturalquestions-train-72801"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 13, "before_eval": {"predictions": ["Sister, Sister", "former president of Guggenheim Partners", "Jason Lee", "Napoleon's army", "baking", "3.7%", "a negative effect on subsequent long-run economic growth", "Tracey Stubbs", "Tenacious D", "Jerry Ekandjo ( until February 2018 ), Erastus Utoni", "discipline problems with the Flight Director's orders during their flight", "thumbydemas", "paddington", "amyotrophic lateral sclerosis", "\"Odorama\" whereby viewers could smell what they saw on screen through scratch and sniff cards", "Swiss made", "October 17, 1938", "Torah or Bible", "on the western coast of Italy", "Phil Hill, who went on to become the first and only U.S. born world grand prix champion", "brass band parades", "mid November", "Facebook", "krak\u00f3w", "Tim \"Ripper\" Owens, singer in a Judas Priest tribute band", "Issaquah, Washington (a suburb of Seattle)", "King George's War", "cheated on Miley", "punk rock", "Fort Snelling, Minnesota", "daguerreotypes", "infrequent rain"], "metric_results": {"EM": 0.1875, "QA-F1": 0.33543583152958156}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true], "QA-F1": [0.0, 0.33333333333333337, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4444444444444445, 0.0, 0.0, 0.4444444444444445, 0.3636363636363636, 0.0, 0.0, 0.0, 0.125, 1.0, 1.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.28571428571428575, 1.0, 0.0, 0.5, 0.33333333333333337, 1.0, 0.2222222222222222, 0.5, 0.3333333333333333, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-1509", "mrqa_naturalquestions-validation-1135", "mrqa_squad-validation-830", "mrqa_triviaqa-validation-7638", "mrqa_squad-validation-7351", "mrqa_triviaqa-validation-6944", "mrqa_hotpotqa-validation-4058", "mrqa_naturalquestions-validation-6445", "mrqa_squad-validation-4060", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-4486", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-1591", "mrqa_squad-validation-1850", "mrqa_naturalquestions-validation-150", "mrqa_hotpotqa-validation-800", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-2896", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-1932", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7310", "mrqa_hotpotqa-validation-3669", "mrqa_triviaqa-validation-6913"], "retrieved_ids": ["mrqa_naturalquestions-train-45602", "mrqa_naturalquestions-train-12852", "mrqa_naturalquestions-train-9779", "mrqa_naturalquestions-train-23969", "mrqa_naturalquestions-train-82397", "mrqa_naturalquestions-train-74865", "mrqa_hotpotqa-validation-3870", "mrqa_naturalquestions-train-59801", "mrqa_naturalquestions-train-63614", "mrqa_naturalquestions-train-60834", "mrqa_naturalquestions-train-54810", "mrqa_naturalquestions-train-24717", "mrqa_naturalquestions-train-12193", "mrqa_naturalquestions-train-44843", "mrqa_naturalquestions-train-39049", "mrqa_naturalquestions-train-15974", "mrqa_naturalquestions-train-63141", "mrqa_naturalquestions-train-14875", "mrqa_naturalquestions-train-42135", "mrqa_naturalquestions-train-15384", "mrqa_naturalquestions-train-192", "mrqa_naturalquestions-train-63283", "mrqa_naturalquestions-train-21529", "mrqa_naturalquestions-train-47764", "mrqa_naturalquestions-train-42076", "mrqa_naturalquestions-train-9747", "mrqa_squad-validation-6677", "mrqa_naturalquestions-train-75408", "mrqa_naturalquestions-train-8436", "mrqa_naturalquestions-train-70320", "mrqa_naturalquestions-train-50793", "mrqa_naturalquestions-train-22729"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 14, "before_eval": {"predictions": ["Hong Kong", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "an American sympathizer in the American Revolutionary War", "foxes and My Own Private Idaho", "FX option or currency option", "electromagnetic waves", "a Wahhabi/ Salafi Jihad extremist militant group", "marykaba", "Dimensions in Time", "Surveyor 3 unmanned lunar probe", "January 1981", "gonadotropin - releasing hormone ( GnRH )", "baptism", "a Lutheran pastor in Hochfelden used a sermon to urge his parishioners to murder Jews", "\u00a31,150,000", "slowing the vehicle", "Belle Fourche and Cheyenne rivers", "fossils in sedimentary rocks", "Hanna-barbera", "Cortina d'Ampezzo", "efficient and effective management of money ( funds ) in such a manner as to accomplish the objectives of the organization", "Alba Longa", "Grim Reaper", "fox Vez de Torres", "goalkeeper Timo Hildebrand", "the state sector", "2 February 1940", "poverty", "refers to a god of the Ammonites, as well as Tyrian Melqart and others", "retina", "Uncle Fester", "Charles Whitman"], "metric_results": {"EM": 0.125, "QA-F1": 0.2740957367795603}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.16666666666666669, 0.0, 0.0, 0.5882352941176471, 0.0, 0.5454545454545454, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3253", "mrqa_triviaqa-validation-46", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10046", "mrqa_squad-validation-9751", "mrqa_triviaqa-validation-2442", "mrqa_squad-validation-7836", "mrqa_squad-validation-3999", "mrqa_naturalquestions-validation-5944", "mrqa_naturalquestions-validation-8180", "mrqa_squad-validation-2509", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-116", "mrqa_squad-validation-5178", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-validation-3302", "mrqa_hotpotqa-validation-1244", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-5191", "mrqa_naturalquestions-validation-6019", "mrqa_hotpotqa-validation-241", "mrqa_squad-validation-8444", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-5256"], "retrieved_ids": ["mrqa_naturalquestions-train-87613", "mrqa_naturalquestions-train-71885", "mrqa_naturalquestions-train-62777", "mrqa_naturalquestions-train-76321", "mrqa_naturalquestions-train-35017", "mrqa_naturalquestions-train-53250", "mrqa_naturalquestions-train-40188", "mrqa_naturalquestions-train-18537", "mrqa_naturalquestions-train-26716", "mrqa_naturalquestions-train-70465", "mrqa_naturalquestions-train-80542", "mrqa_naturalquestions-train-34565", "mrqa_naturalquestions-train-84792", "mrqa_naturalquestions-train-6919", "mrqa_naturalquestions-train-56841", "mrqa_naturalquestions-train-51272", "mrqa_naturalquestions-train-70545", "mrqa_naturalquestions-train-42", "mrqa_naturalquestions-train-24237", "mrqa_naturalquestions-train-85607", "mrqa_naturalquestions-train-45622", "mrqa_naturalquestions-train-1808", "mrqa_naturalquestions-train-82025", "mrqa_naturalquestions-train-21577", "mrqa_naturalquestions-train-3316", "mrqa_naturalquestions-train-130", "mrqa_naturalquestions-train-45104", "mrqa_naturalquestions-train-23742", "mrqa_naturalquestions-train-29648", "mrqa_naturalquestions-train-16429", "mrqa_naturalquestions-train-44452", "mrqa_naturalquestions-train-27576"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 15, "before_eval": {"predictions": ["superhuman abilities", "Part 2", "the Flatbush section of Brooklyn, New York City", "the Alamodome and city of San Antonio", "Taio Cruz", "viternicus", "a friend and publicist", "tatharine clifton", "Relieving Chambers", "1983", "the Old Town Hall, Gateshead", "That's the Way of the World", "The neck", "1898", "professional wrestler, mixed martial artist and a former amateur wrestler", "Payaya Indians", "to steal the plans for the Death Star, the Galactic Empire's superweapon", "vito corleone", "jenny jN-4HM", "tibility for impressions, and an inclination to be touched by emotions,  as may be termed sorrowful, suffering and tender.", "chimpanpanzee", "March 15, 1945", "absolute temperature, assuming in a closed system", "the private intelligence firm Stratfor", "Sam Waterston", "the distal cusp ridge", "Aegisthus", "25 November 2015", "tallahassee", "prefabricated housing projects", "London", "WWSB and WOTV"], "metric_results": {"EM": 0.125, "QA-F1": 0.15570887445887444}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-2612", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1479", "mrqa_hotpotqa-validation-335", "mrqa_triviaqa-validation-7512", "mrqa_squad-validation-1374", "mrqa_triviaqa-validation-5979", "mrqa_naturalquestions-validation-10439", "mrqa_hotpotqa-validation-1048", "mrqa_squad-validation-5249", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-56", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-validation-368", "mrqa_hotpotqa-validation-573", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-5877", "mrqa_triviaqa-validation-7578", "mrqa_hotpotqa-validation-5188", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-365", "mrqa_hotpotqa-validation-1714", "mrqa_triviaqa-validation-1736", "mrqa_hotpotqa-validation-413", "mrqa_triviaqa-validation-1166", "mrqa_squad-validation-874", "mrqa_triviaqa-validation-1588", "mrqa_squad-validation-6091"], "retrieved_ids": ["mrqa_naturalquestions-train-85313", "mrqa_naturalquestions-train-20952", "mrqa_naturalquestions-train-70043", "mrqa_naturalquestions-train-79525", "mrqa_naturalquestions-train-43736", "mrqa_naturalquestions-train-68784", "mrqa_naturalquestions-train-8343", "mrqa_naturalquestions-train-82843", "mrqa_naturalquestions-train-56694", "mrqa_naturalquestions-train-49029", "mrqa_naturalquestions-train-78797", "mrqa_naturalquestions-train-32598", "mrqa_naturalquestions-train-2792", "mrqa_naturalquestions-train-44051", "mrqa_naturalquestions-train-16856", "mrqa_naturalquestions-train-17277", "mrqa_naturalquestions-train-10681", "mrqa_naturalquestions-train-70633", "mrqa_naturalquestions-train-71122", "mrqa_naturalquestions-train-19209", "mrqa_naturalquestions-train-46930", "mrqa_naturalquestions-train-53131", "mrqa_naturalquestions-train-14956", "mrqa_naturalquestions-train-57469", "mrqa_naturalquestions-train-36004", "mrqa_naturalquestions-train-30309", "mrqa_naturalquestions-train-49827", "mrqa_naturalquestions-train-32216", "mrqa_naturalquestions-train-9346", "mrqa_naturalquestions-train-35505", "mrqa_naturalquestions-train-16003", "mrqa_naturalquestions-train-14520"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 16, "before_eval": {"predictions": ["florida", "cheeses made with milk collected from a group of farms that are located within close proximity to where the cheese is produced", "the Benedictines", "the lateral side of the tibia", "ferguside royal clan", "the North Sea, through the former Meuse estuary, near Rotterdam", "the Kalahari Desert", "Colin Montgomerie", "Ximena Sari\u00f1ana Rivera", "Amway", "secondary school study", "Thomas Sowell", "the Speaker or, in his absence, by the Deputy Speaker of the Lok Sabha or in the absence, the Deputy - Chairman of the Rajya Sabha", "The Show Band Show", "tANU", "Chad", "florida", "an open work crown surmounted by a statue of fame", "allowing a child to go through a torturous treatment to gain information", "Fulham, Greater London, England", "French, English and Spanish", "michael Buerk", "U.S. Marshals", "What's Up (TV series)", "supply chain management", "galater", "Poland's last king and English culture", "polynomial algebra", "snowbell", "The three wise monkeys", "sheepskin and Merino Wool products", "the \"second city\" of Oahu"], "metric_results": {"EM": 0.09375, "QA-F1": 0.18013392857142857}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.0, 0.0, 0.5, 0.16666666666666666, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21428571428571425, 0.33333333333333337, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2324", "mrqa_naturalquestions-validation-9218", "mrqa_triviaqa-validation-313", "mrqa_squad-validation-9319", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4681", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2771", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-3483", "mrqa_hotpotqa-validation-1566", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-4055", "mrqa_squad-validation-5407", "mrqa_naturalquestions-validation-7144", "mrqa_hotpotqa-validation-4164", "mrqa_triviaqa-validation-639", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-1001", "mrqa_triviaqa-validation-1770", "mrqa_triviaqa-validation-866", "mrqa_squad-validation-8223", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-validation-9087", "mrqa_hotpotqa-validation-1250"], "retrieved_ids": ["mrqa_naturalquestions-train-39500", "mrqa_naturalquestions-train-19223", "mrqa_naturalquestions-train-22788", "mrqa_naturalquestions-train-75354", "mrqa_naturalquestions-train-25133", "mrqa_naturalquestions-train-61880", "mrqa_naturalquestions-train-72454", "mrqa_naturalquestions-train-88072", "mrqa_naturalquestions-train-83741", "mrqa_naturalquestions-train-86341", "mrqa_naturalquestions-train-59339", "mrqa_naturalquestions-train-79317", "mrqa_naturalquestions-train-60391", "mrqa_naturalquestions-train-16940", "mrqa_naturalquestions-train-60001", "mrqa_naturalquestions-train-41775", "mrqa_naturalquestions-train-71867", "mrqa_naturalquestions-train-25578", "mrqa_naturalquestions-train-40394", "mrqa_naturalquestions-train-12740", "mrqa_naturalquestions-train-57478", "mrqa_naturalquestions-train-81178", "mrqa_naturalquestions-train-66819", "mrqa_naturalquestions-train-5887", "mrqa_naturalquestions-train-21691", "mrqa_naturalquestions-train-43966", "mrqa_naturalquestions-train-287", "mrqa_naturalquestions-train-54518", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-train-34860", "mrqa_naturalquestions-train-46240", "mrqa_naturalquestions-train-35862"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 17, "before_eval": {"predictions": ["belastic", "Fantastic Life of Pets", "August 6, 1845", "stable, non-radioactive rubidium - 85", "James Zeebo", "sovereign states", "president of the united States Senate", "The Discovery Institute's \"Teach the Controversy\" campaign", "Bumblebee", "Australian", "men 18 months ( although in accordance with a temporary order from January 10, 1968, six additional months for men and 24 months for women respectively", "opportunities will vary by geographic area and subject taught", "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued)", "a private liberal arts college", "Frank Wentz", "\"antiforms\"", "second half of the third season", "Veyyil", "Grace Nail Johnson", "Mick Jagger", "prime number p with n < p < 2n \u2212 2, for any natural number n > 3", "Bangor International Airport", "teachers who specialize in one subject and who tend to be more knowledgeable in that one area than a teacher who teaches many subjects", "the 180th meridian in a 360 \u00b0 - system", "Cartoon Network", "the Presiding Officer on the advice of the parliamentary bureau", "Miami Heat", "33-member", "vitifoliae", "Annual Conference Cabinet", "bronze medal", "first prompted by original star William Hartnell's poor health"], "metric_results": {"EM": 0.25, "QA-F1": 0.3861237373737374}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, true, false, false, true, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.08333333333333334, 0.0, 0.7499999999999999, 0.0, 0.0, 0.48484848484848486, 0.19999999999999998, 0.16, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.5, 0.25, 0.08333333333333333, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1298", "mrqa_squad-validation-608", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-3559", "mrqa_triviaqa-validation-314", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-3440", "mrqa_naturalquestions-validation-1090", "mrqa_squad-validation-2053", "mrqa_squad-validation-7572", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-4868", "mrqa_naturalquestions-validation-9171", "mrqa_squad-validation-8966", "mrqa_hotpotqa-validation-2782", "mrqa_squad-validation-1904", "mrqa_naturalquestions-validation-10347", "mrqa_squad-validation-9405", "mrqa_hotpotqa-validation-613", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-1130", "mrqa_squad-validation-7664"], "retrieved_ids": ["mrqa_naturalquestions-train-82809", "mrqa_naturalquestions-train-66909", "mrqa_naturalquestions-train-5988", "mrqa_naturalquestions-train-9056", "mrqa_naturalquestions-train-47031", "mrqa_naturalquestions-train-53174", "mrqa_naturalquestions-train-74439", "mrqa_naturalquestions-train-4543", "mrqa_naturalquestions-train-61850", "mrqa_triviaqa-validation-5248", "mrqa_naturalquestions-train-23101", "mrqa_naturalquestions-train-77661", "mrqa_naturalquestions-train-17424", "mrqa_naturalquestions-train-37755", "mrqa_naturalquestions-train-68087", "mrqa_naturalquestions-train-33929", "mrqa_naturalquestions-train-6018", "mrqa_naturalquestions-train-66640", "mrqa_naturalquestions-train-31935", "mrqa_naturalquestions-train-43627", "mrqa_naturalquestions-train-85643", "mrqa_naturalquestions-train-42399", "mrqa_naturalquestions-train-4041", "mrqa_naturalquestions-train-42037", "mrqa_naturalquestions-train-29648", "mrqa_naturalquestions-train-71231", "mrqa_naturalquestions-train-51423", "mrqa_naturalquestions-train-12326", "mrqa_naturalquestions-train-56267", "mrqa_naturalquestions-train-71918", "mrqa_naturalquestions-train-38219", "mrqa_naturalquestions-train-17027"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 18, "before_eval": {"predictions": ["The Rwandaandan genocide, also known as the genocide against the Tutsi", "working harmoniously to fulfill the needs of every student in the classroom", "500 metres", "Vili Fualaau and Mary Kay Letourneau, a student and teacher who made news for their sexual relationship", "ABC News", "distance covered by a vehicle ( for example as recorded by an odometer ), person, animal, or object along a curved path from a point A to a point B", "12", "the Museum of Manufactures", "King Edward I to Henry VIII", "the Chagos Archipelago", "dundee", "those at the bottom of the economic government whom the state ( in Roosevelt's view and in the general social humanitarian approach ) needed to help", "king James V of Scotland", "\"Grindhouse\" fake trailer", "nave davenport", "digital transmission modes such as MFSK and Olivia", "the Swiss- Austrian border", "Tesla Gigafactory 1", "821", "Sky channels", "liquid", "Kim Hyun-ah", "the races of highest'social efficiency'", "transposition", "the \" King of Cool\"", "President Wilson and the American delegation from the Paris Peace Conference", "nabasis", "the fifth season", "Criminal twins", "Hockey Club Davos", "Michael Crawford", "a lightning strike"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3003577441077441}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.2, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666666, 0.0, 1.0, 0.07407407407407408, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666665, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.4, 0.4, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-1441", "mrqa_squad-validation-1914", "mrqa_hotpotqa-validation-2145", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-3300", "mrqa_hotpotqa-validation-2993", "mrqa_triviaqa-validation-5996", "mrqa_naturalquestions-validation-5215", "mrqa_triviaqa-validation-2683", "mrqa_hotpotqa-validation-2201", "mrqa_triviaqa-validation-2715", "mrqa_naturalquestions-validation-2222", "mrqa_squad-validation-9074", "mrqa_hotpotqa-validation-4415", "mrqa_squad-validation-2862", "mrqa_triviaqa-validation-4279", "mrqa_hotpotqa-validation-1855", "mrqa_squad-validation-9841", "mrqa_naturalquestions-validation-4497", "mrqa_squad-validation-9827", "mrqa_triviaqa-validation-1764", "mrqa_hotpotqa-validation-3798", "mrqa_triviaqa-validation-7100", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-511", "mrqa_naturalquestions-validation-10490"], "retrieved_ids": ["mrqa_naturalquestions-train-60350", "mrqa_naturalquestions-train-10208", "mrqa_naturalquestions-train-62103", "mrqa_naturalquestions-train-2782", "mrqa_naturalquestions-train-10206", "mrqa_naturalquestions-train-28976", "mrqa_naturalquestions-train-56184", "mrqa_naturalquestions-train-80629", "mrqa_naturalquestions-train-991", "mrqa_naturalquestions-train-28955", "mrqa_naturalquestions-train-62651", "mrqa_naturalquestions-train-80058", "mrqa_naturalquestions-train-73737", "mrqa_naturalquestions-train-48565", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-train-57", "mrqa_naturalquestions-train-21582", "mrqa_naturalquestions-train-82122", "mrqa_naturalquestions-train-32970", "mrqa_naturalquestions-train-23762", "mrqa_naturalquestions-train-3177", "mrqa_naturalquestions-train-66530", "mrqa_naturalquestions-train-80268", "mrqa_naturalquestions-train-24747", "mrqa_naturalquestions-train-5988", "mrqa_naturalquestions-train-6578", "mrqa_naturalquestions-train-50719", "mrqa_naturalquestions-train-51637", "mrqa_naturalquestions-train-9240", "mrqa_naturalquestions-train-23486", "mrqa_naturalquestions-train-16317", "mrqa_naturalquestions-train-63822"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 19, "before_eval": {"predictions": ["Mauritian", "Norman Macdonnell", "flamenco", "11.1%", "trans-Pacific", "Sharman Joshi", "in a classroom", "Forster I, Forster II, and Forster III", "a sufficient condition for p to be prime", "Tropical Storm Ann", "Cherry Hill", "In `` Happy Hour ''", "White Whale", "elisabeth Thible", "a 1993 American comedy - drama film directed by Fred Schepisi, adapted from the Pulitzer Prize - nominated John Guare play of the same name", "blackstar", "India", "fear of the Lord", "1889", "Nicki Minaj", "comic opera", "surnames indicating their French Huguenot ancestry", "fagioli", "friedrich", "Drawn Together", "William the Conqueror", "Ben Gurion International Airport", "two degrees of freedom", "Mainland Greece", "taking blood samples from patients and correctly cataloging them for lab analysis", "the youngest TV director ever by the Guinness World Records for his work on the episode \" Fight at the Museum\" in the fourth season of the \" Kickin' It\" TV series at age 16", "Sunset Publishing Corporation"], "metric_results": {"EM": 0.0625, "QA-F1": 0.18432939708141322}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.8, 0.0, 0.0, 0.0, 0.08333333333333334, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.2857142857142857, 0.25806451612903225, 0.3333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2389", "mrqa_triviaqa-validation-6901", "mrqa_squad-validation-6963", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-303", "mrqa_squad-validation-1942", "mrqa_squad-validation-9214", "mrqa_hotpotqa-validation-5710", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-9284", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-6935", "mrqa_naturalquestions-validation-4354", "mrqa_triviaqa-validation-1995", "mrqa_hotpotqa-validation-3049", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-8025", "mrqa_triviaqa-validation-2015", "mrqa_squad-validation-3151", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4068", "mrqa_hotpotqa-validation-57", "mrqa_naturalquestions-validation-1565", "mrqa_hotpotqa-validation-5061", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-3552", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-2627"], "retrieved_ids": ["mrqa_naturalquestions-train-33039", "mrqa_naturalquestions-train-83939", "mrqa_naturalquestions-train-53898", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-train-18273", "mrqa_naturalquestions-train-84973", "mrqa_naturalquestions-train-78716", "mrqa_naturalquestions-train-31935", "mrqa_naturalquestions-train-22739", "mrqa_naturalquestions-train-62389", "mrqa_squad-validation-3463", "mrqa_naturalquestions-train-86749", "mrqa_naturalquestions-train-76539", "mrqa_naturalquestions-train-36373", "mrqa_naturalquestions-train-74926", "mrqa_naturalquestions-train-557", "mrqa_naturalquestions-train-14699", "mrqa_naturalquestions-train-81715", "mrqa_naturalquestions-train-21077", "mrqa_naturalquestions-train-62864", "mrqa_naturalquestions-train-70166", "mrqa_naturalquestions-train-22952", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-train-12276", "mrqa_naturalquestions-train-60297", "mrqa_naturalquestions-train-16915", "mrqa_naturalquestions-train-67469", "mrqa_naturalquestions-train-88103", "mrqa_naturalquestions-train-21351", "mrqa_naturalquestions-train-58277", "mrqa_naturalquestions-train-54650", "mrqa_naturalquestions-train-57121"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 20, "before_eval": {"predictions": ["reciprocating Diesel engines", "Robert Smigel", "the Sackler Centre for arts education", "marx terrell Smith", "kalos", "a mansion in the Aldergrove area of Langely serving as the property at the centre of the story", "Apollo", "ribosomal", "kingfisher", "six", "Zoe McLellan", "I Love the Way You Love Me", "Cozonac", "brian marx", "a heliocentric orbit", "Lucius Cornelius Sulla Felix", "Super Bowl LII, following the 2017 season", "a Golden Globe", "Swahili", "the primacy of core Christian values such as love, patience, charity, and freedom", "snow", "Pantone Matching System", "Firoz Shah Tughlaq", "\" My Love from the Star\"", "San Jose", "sea wasp", "Hawai\u02bbi State Senate", "a \"teleforce\" weapon", "Thunderbird of Native American tradition", "giving Super Bowl ever", "7.9%", "marx marx"], "metric_results": {"EM": 0.0625, "QA-F1": 0.189343763821318}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.6842105263157895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.888888888888889, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-3368", "mrqa_hotpotqa-validation-779", "mrqa_squad-validation-5273", "mrqa_hotpotqa-validation-3547", "mrqa_triviaqa-validation-5091", "mrqa_naturalquestions-validation-4590", "mrqa_squad-validation-3964", "mrqa_hotpotqa-validation-2434", "mrqa_triviaqa-validation-3486", "mrqa_hotpotqa-validation-1210", "mrqa_naturalquestions-validation-1279", "mrqa_hotpotqa-validation-4627", "mrqa_naturalquestions-validation-5168", "mrqa_triviaqa-validation-4852", "mrqa_hotpotqa-validation-3623", "mrqa_hotpotqa-validation-2407", "mrqa_naturalquestions-validation-2448", "mrqa_squad-validation-8464", "mrqa_squad-validation-2280", "mrqa_triviaqa-validation-6721", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-4015", "mrqa_squad-validation-315", "mrqa_triviaqa-validation-1453", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2064", "mrqa_squad-validation-393", "mrqa_squad-validation-7272", "mrqa_triviaqa-validation-935"], "retrieved_ids": ["mrqa_naturalquestions-train-88050", "mrqa_naturalquestions-train-48555", "mrqa_naturalquestions-train-5243", "mrqa_naturalquestions-train-75754", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-train-52799", "mrqa_naturalquestions-train-26242", "mrqa_naturalquestions-train-27545", "mrqa_naturalquestions-train-14250", "mrqa_naturalquestions-train-74693", "mrqa_naturalquestions-train-9667", "mrqa_naturalquestions-train-52771", "mrqa_naturalquestions-train-29571", "mrqa_naturalquestions-train-56650", "mrqa_naturalquestions-train-24389", "mrqa_naturalquestions-train-74246", "mrqa_naturalquestions-train-56596", "mrqa_naturalquestions-train-32015", "mrqa_naturalquestions-train-49601", "mrqa_naturalquestions-train-38395", "mrqa_naturalquestions-train-45994", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-train-16982", "mrqa_naturalquestions-train-18815", "mrqa_naturalquestions-train-24628", "mrqa_naturalquestions-train-38605", "mrqa_naturalquestions-train-25607", "mrqa_naturalquestions-train-25190", "mrqa_naturalquestions-train-83831", "mrqa_naturalquestions-train-1611", "mrqa_naturalquestions-validation-10046", "mrqa_naturalquestions-train-81727"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 21, "before_eval": {"predictions": ["a Czech word, robota, meaning `` forced labor ''", "daphne du maurier", "various registries", "Matthew Rowlinson", "Yazoo", "22 April 1894", "stars exceeding about eight times the mass of the sun", "(anima non sic dormit)", "defence of their North American colonies would no longer be an issue", "Willie Nelson and Kris Kristofferson", "ill. (some col.)", "private institutions such as the California Institute of Technology, Chapman University, the Claremont Colleges (Claremont McKenna College, Harvey Mudd College, Pitzer College, Pomona College, and Scripps College)", "a French pirate", "Lewis", "Charles Dickens", "a forum in which the Nobel Peace Laureates and the Peace Laureate Organizations could come together to address global issues with a view to encourage and support peace and human well being in the world", "carbohydrates", "2000", "(i.e. exceeds any given number)", "news bulletin", "padlocking the gates and using sickles to deflate one of the large domes covering two satellite dishes", "1969", "Tallemaja \"pine tree Mary\"", "in western portions of the Great Lakes region (an area not directly subject to the conflict between the French and British) including the Huron, Mississauga, Ojibwa, Winnebago, and Potawatomi", "Orthodox Christians", "First Editions and Rare Books", "a 4 in ( 10 cm ) LCD multi-touch Retina display and a screen resolution of 640 \u00d7 1136 at 326 ppi", "geena davis", "eating both fish larvae and small crustaceans that would otherwise feed the adult fish", "\"Menace II Society\"", "quarterback", "Larry Wayne Gatlin & the Gatlin Brothers Band"], "metric_results": {"EM": 0.0625, "QA-F1": 0.1552901888822037}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.14285714285714288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17391304347826086, 0.0, 0.0, 0.07692307692307693, 0.0, 0.0, 0.0, 0.4210526315789474, 0.0, 0.0, 0.888888888888889, 0.0, 0.23529411764705882, 1.0, 0.0, 0.43750000000000006, 0.0, 0.0, 0.19999999999999998, 0.0, 0.0, 1.0, 0.25, 0.14285714285714288]}}, "error_ids": ["mrqa_naturalquestions-validation-3609", "mrqa_triviaqa-validation-5239", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-751", "mrqa_triviaqa-validation-4383", "mrqa_hotpotqa-validation-4430", "mrqa_triviaqa-validation-192", "mrqa_squad-validation-2412", "mrqa_squad-validation-10502", "mrqa_hotpotqa-validation-5480", "mrqa_triviaqa-validation-2961", "mrqa_squad-validation-2709", "mrqa_hotpotqa-validation-4950", "mrqa_naturalquestions-validation-5439", "mrqa_squad-validation-5345", "mrqa_hotpotqa-validation-35", "mrqa_squad-validation-3627", "mrqa_naturalquestions-validation-5897", "mrqa_squad-validation-9020", "mrqa_triviaqa-validation-1954", "mrqa_squad-validation-6844", "mrqa_hotpotqa-validation-2399", "mrqa_squad-validation-10177", "mrqa_squad-validation-8456", "mrqa_triviaqa-validation-6950", "mrqa_naturalquestions-validation-6832", "mrqa_triviaqa-validation-1555", "mrqa_squad-validation-4648", "mrqa_naturalquestions-validation-2758", "mrqa_hotpotqa-validation-4676"], "retrieved_ids": ["mrqa_naturalquestions-train-44809", "mrqa_naturalquestions-train-74601", "mrqa_naturalquestions-train-12029", "mrqa_naturalquestions-train-68305", "mrqa_naturalquestions-train-69947", "mrqa_naturalquestions-train-19213", "mrqa_naturalquestions-train-58865", "mrqa_naturalquestions-train-17730", "mrqa_naturalquestions-train-29965", "mrqa_naturalquestions-train-12849", "mrqa_naturalquestions-train-40409", "mrqa_naturalquestions-train-22630", "mrqa_naturalquestions-train-71096", "mrqa_naturalquestions-train-79999", "mrqa_naturalquestions-train-13399", "mrqa_naturalquestions-train-78177", "mrqa_naturalquestions-train-59", "mrqa_naturalquestions-train-41703", "mrqa_naturalquestions-train-57332", "mrqa_naturalquestions-train-33678", "mrqa_naturalquestions-train-40171", "mrqa_naturalquestions-train-17964", "mrqa_naturalquestions-train-14631", "mrqa_naturalquestions-train-45499", "mrqa_naturalquestions-train-7375", "mrqa_naturalquestions-train-45441", "mrqa_naturalquestions-train-72503", "mrqa_naturalquestions-train-5706", "mrqa_naturalquestions-train-20483", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-train-44230", "mrqa_naturalquestions-train-73237"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 22, "before_eval": {"predictions": ["Alex Skuby", "England", "the first Thursday in May", "Mediterranean Shipping Company S.A.", "casino", "his friends, Humpty Dumpty and Kitty Softpaws", "Liberals", "Royalists", "any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "are ceremonially placed on the heads of Christians on Ash Wednesday, either by being sprinkled over their heads or, in English - speaking countries, more often by being marked on their foreheads as a visible cross", "al\u00e9a seydoux", "alibris", "Augustus Waters", "1619", "Tony Blair, The Journey", "casino, tobacco and gambling", "June 11, 1973", "in Kenya and in the Masai Mara in particular", "a chronological collection of critical quotations about William Shakespeare and his works", "alred", "an active supporter of the League of Nations", "Cargill", "Cinemark Theatres", "\"The Gang\"", "3 October 1990", "September 21, 2017", "The weak force is due to the exchange of the heavy W and Z bosons. Its most familiar effect is beta decay ( of neutrons in atomic nuclei) and the associated radioactivity.", "daedalus", "Martin Luther King III", "Development of Substitute Materials", "Chronicles of Barsetshire", "vast areas"], "metric_results": {"EM": 0.15625, "QA-F1": 0.3285755414365328}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, true], "QA-F1": [0.0, 0.0, 0.4, 0.8571428571428571, 0.0, 0.4444444444444445, 0.0, 0.33333333333333337, 0.1111111111111111, 0.16, 0.0, 0.0, 0.3636363636363636, 1.0, 0.8, 0.0, 0.0, 0.4, 0.625, 0.0, 0.1818181818181818, 0.0, 0.5, 0.0, 0.2, 1.0, 0.13793103448275862, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1883", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-9789", "mrqa_triviaqa-validation-5698", "mrqa_naturalquestions-validation-7346", "mrqa_squad-validation-2884", "mrqa_hotpotqa-validation-2959", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-1578", "mrqa_triviaqa-validation-2797", "mrqa_naturalquestions-validation-3859", "mrqa_triviaqa-validation-4731", "mrqa_squad-validation-2769", "mrqa_hotpotqa-validation-482", "mrqa_squad-validation-8280", "mrqa_hotpotqa-validation-5655", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-661", "mrqa_squad-validation-10459", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-1929"], "retrieved_ids": ["mrqa_naturalquestions-train-9279", "mrqa_naturalquestions-train-396", "mrqa_naturalquestions-train-37128", "mrqa_naturalquestions-train-9061", "mrqa_naturalquestions-train-56197", "mrqa_naturalquestions-train-68035", "mrqa_naturalquestions-train-66404", "mrqa_naturalquestions-train-63929", "mrqa_naturalquestions-train-32880", "mrqa_naturalquestions-train-75913", "mrqa_naturalquestions-train-68578", "mrqa_naturalquestions-train-71074", "mrqa_naturalquestions-train-62975", "mrqa_naturalquestions-train-27893", "mrqa_naturalquestions-train-77585", "mrqa_naturalquestions-train-80651", "mrqa_naturalquestions-train-14519", "mrqa_naturalquestions-train-83028", "mrqa_naturalquestions-train-74764", "mrqa_naturalquestions-train-39068", "mrqa_naturalquestions-train-17361", "mrqa_naturalquestions-train-43097", "mrqa_naturalquestions-train-63770", "mrqa_naturalquestions-train-16386", "mrqa_naturalquestions-train-21166", "mrqa_naturalquestions-train-51929", "mrqa_naturalquestions-train-59549", "mrqa_naturalquestions-train-79868", "mrqa_naturalquestions-train-43015", "mrqa_naturalquestions-train-66453", "mrqa_naturalquestions-train-11790", "mrqa_naturalquestions-train-24760"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 23, "before_eval": {"predictions": ["an additional $105 billion in growth to the country's economy over five years", "mono", "about two-thirds the size", "1937 Austin Seven Ruby Open Top Tourer", "javier (Luna)", "red", "Dreamland", "animated film", "European Union institutions", "26, 45, and 46", "Death Wish Coffee", "CAL IPSO satellite", "celandine flowers", "Ulbricht", "Ronald Ralph \"Ronnie\" Schell", "artemisinin-based therapy", "Tata Consultancy Services Limited (TCS)", "in the east of Ireland", "1939", "the 2017 / 18 Divisional Round game against the New Orleans Saints", "possibly 1707, in his second annual cycle (1724 to 1725)", "on Fresno's far southeast side, bounded by Chestnut Avenue to the West", "the south western escarpment of the Jos Plateau", "benjamin franklin", "Incudomalleolar joint", "bobby riggs", "Democritus", "Santa Clara Marriott", "benjamin barenboim", "political power generated by wealth", "polynomial-time reductions", "Corey Brown"], "metric_results": {"EM": 0.09375, "QA-F1": 0.199627109002109}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false], "QA-F1": [0.3076923076923077, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.3636363636363636, 0.18181818181818182, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0]}}, "error_ids": ["mrqa_squad-validation-7389", "mrqa_triviaqa-validation-3716", "mrqa_squad-validation-8850", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-3933", "mrqa_hotpotqa-validation-4348", "mrqa_hotpotqa-validation-789", "mrqa_squad-validation-4118", "mrqa_hotpotqa-validation-2741", "mrqa_squad-validation-542", "mrqa_squad-validation-4228", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-validation-3714", "mrqa_hotpotqa-validation-1782", "mrqa_triviaqa-validation-3595", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-220", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-3052", "mrqa_squad-validation-2420", "mrqa_squad-validation-4662", "mrqa_naturalquestions-validation-2212", "mrqa_triviaqa-validation-571", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-3420", "mrqa_hotpotqa-validation-2340", "mrqa_triviaqa-validation-6781", "mrqa_squad-validation-1750", "mrqa_squad-validation-769"], "retrieved_ids": ["mrqa_naturalquestions-train-82480", "mrqa_naturalquestions-train-40034", "mrqa_naturalquestions-train-81789", "mrqa_naturalquestions-train-5262", "mrqa_naturalquestions-train-58522", "mrqa_naturalquestions-train-65386", "mrqa_naturalquestions-train-5477", "mrqa_naturalquestions-train-23212", "mrqa_naturalquestions-train-81862", "mrqa_naturalquestions-train-30187", "mrqa_naturalquestions-train-63739", "mrqa_naturalquestions-train-19802", "mrqa_naturalquestions-train-15753", "mrqa_naturalquestions-train-83474", "mrqa_naturalquestions-train-1201", "mrqa_naturalquestions-train-12880", "mrqa_naturalquestions-train-31943", "mrqa_naturalquestions-train-69688", "mrqa_naturalquestions-train-32422", "mrqa_naturalquestions-train-49426", "mrqa_naturalquestions-train-37532", "mrqa_naturalquestions-train-44544", "mrqa_naturalquestions-train-47188", "mrqa_naturalquestions-train-25028", "mrqa_naturalquestions-train-7835", "mrqa_naturalquestions-train-39546", "mrqa_naturalquestions-train-12807", "mrqa_naturalquestions-train-37787", "mrqa_naturalquestions-train-82177", "mrqa_naturalquestions-train-45657", "mrqa_naturalquestions-train-75863", "mrqa_naturalquestions-train-30265"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 24, "before_eval": {"predictions": ["the 1965 -- 66 season", "NADP+ though sometimes they can flow back down more H+-pumping electron transport chains to transport more hydrogen ions into the thylakoid space to generate more ATP", "the Duke of Cumberland", "gold", "WBO lightweight title", "moluccas", "the first Saturday in May", "Cordelia", "United States Secretary of State Henry Kissinger had negotiated an Israeli troop withdrawal from parts of the Sinai Peninsula", "1970", "J.R. R. Tolkien", "Peyton Manning", "Selena Gomez", "join a vocational youth/village polytechnic or make their own arrangements for an apprenticeship program and learn a trade such as tailoring, carpentry, motor vehicle repair, brick-laying and masonry", "live numbers", "Eugene", "new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "primes of the form 2p + 1 with p prime", "letter", "Fa Ze Rug", "nine", "three", "along the eastern coast of the continent, from Nova Scotia and Newfoundland in the north, to Georgia in the south", "the Friars Minor Conventual (O.F.M. Conv)", "CD Castell\u00f3n", "between 1770 and 1848", "20\u201318", "having colloblasts", "Patrick Wachsberger and Erik Feig of Summit Entertainment produced with Adam Shankman and Jennifer Gibgot of Offspring Entertainment", "STS-51-C.", "it will retreat to its den and winter will persist for six more weeks", "minister of agriculture"], "metric_results": {"EM": 0.09375, "QA-F1": 0.24325618654076103}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 0.07692307692307693, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.20000000000000004, 0.0, 0.5714285714285715, 0.0, 0.0, 0.6486486486486487, 0.0, 0.0, 0.5, 0.7692307692307693, 0.0, 0.4, 0.0, 0.0, 0.2222222222222222, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5957446808510638, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8444", "mrqa_squad-validation-8876", "mrqa_triviaqa-validation-3678", "mrqa_triviaqa-validation-3340", "mrqa_hotpotqa-validation-3290", "mrqa_naturalquestions-validation-9011", "mrqa_squad-validation-3741", "mrqa_squad-validation-5399", "mrqa_hotpotqa-validation-3247", "mrqa_squad-validation-384", "mrqa_naturalquestions-validation-10381", "mrqa_squad-validation-8473", "mrqa_triviaqa-validation-667", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-309", "mrqa_squad-validation-8979", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-3297", "mrqa_triviaqa-validation-3847", "mrqa_squad-validation-6204", "mrqa_squad-validation-10148", "mrqa_triviaqa-validation-5304", "mrqa_hotpotqa-validation-5588", "mrqa_naturalquestions-validation-3504", "mrqa_squad-validation-67", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-1814", "mrqa_naturalquestions-validation-6508", "mrqa_triviaqa-validation-1306"], "retrieved_ids": ["mrqa_naturalquestions-train-55785", "mrqa_naturalquestions-train-27053", "mrqa_naturalquestions-train-58088", "mrqa_naturalquestions-train-79347", "mrqa_naturalquestions-train-87902", "mrqa_naturalquestions-train-42186", "mrqa_naturalquestions-train-26156", "mrqa_naturalquestions-train-37169", "mrqa_naturalquestions-train-25838", "mrqa_naturalquestions-train-9779", "mrqa_naturalquestions-train-53097", "mrqa_naturalquestions-train-86251", "mrqa_naturalquestions-train-25158", "mrqa_naturalquestions-train-9732", "mrqa_squad-validation-1750", "mrqa_naturalquestions-train-47917", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-train-68182", "mrqa_naturalquestions-train-59640", "mrqa_naturalquestions-train-85696", "mrqa_naturalquestions-train-55356", "mrqa_squad-validation-10403", "mrqa_naturalquestions-train-32682", "mrqa_naturalquestions-train-73233", "mrqa_naturalquestions-train-86379", "mrqa_naturalquestions-train-55907", "mrqa_naturalquestions-train-48839", "mrqa_naturalquestions-train-65437", "mrqa_squad-validation-3467", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-train-87196", "mrqa_naturalquestions-train-9994"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 25, "before_eval": {"predictions": ["alpine skiing", "france", "over 50 million singles", "secessionists of the Confederate States, who advocated for states'rights to expand slavery", "between 1923 and 1925", "Metropolitan Statistical Area", "January 19, 1962", "Frigate class", "france", "iteratively", "yellow", "the move from the manufacturing sector to the service sector", "jagera peoples", "Sylvester McCoy", "August 14, 1848", "lower rates of health and social problems", "juveniles", "toasted wheat bun", "the way they used `` rule '' and `` method '' to go about their religious affairs", "co-written the book of the musical \"A Chorus Line\"", "2,664", "on the inside of the chassis right beneath the volume buttons", "through a chute beneath his or her feet", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "services include: cleaning services, support services, property services, catering services, security services and facility management services", "Symphony No. 7 in A major, Opus 24", "gironde", "1603", "ranked above the two personal physicians of the Emperor", "alison", "He admitted we were powerless over alcohol -- that our lives had become unmanageable.   Came to believe that a Power greater than ourselves could restore us to sanity.", "wrigley field"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3925277847152847}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.3333333333333333, 0.13333333333333333, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.3333333333333333, 0.25, 0.33333333333333337, 1.0, 0.923076923076923, 0.2857142857142857, 1.0, 0.0, 0.923076923076923, 0.3636363636363636, 0.4, 0.6, 1.0, 1.0, 0.22222222222222224, 0.0, 0.6153846153846155, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-5788", "mrqa_hotpotqa-validation-1818", "mrqa_naturalquestions-validation-10169", "mrqa_naturalquestions-validation-4072", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-3977", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6941", "mrqa_squad-validation-7382", "mrqa_triviaqa-validation-7336", "mrqa_squad-validation-7677", "mrqa_hotpotqa-validation-4720", "mrqa_squad-validation-7301", "mrqa_squad-validation-4637", "mrqa_naturalquestions-validation-5939", "mrqa_hotpotqa-validation-859", "mrqa_triviaqa-validation-2454", "mrqa_naturalquestions-validation-4996", "mrqa_naturalquestions-validation-7309", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-499", "mrqa_squad-validation-6328", "mrqa_triviaqa-validation-6221", "mrqa_naturalquestions-validation-2481", "mrqa_triviaqa-validation-1259"], "retrieved_ids": ["mrqa_naturalquestions-train-40141", "mrqa_naturalquestions-train-35596", "mrqa_naturalquestions-train-63352", "mrqa_naturalquestions-train-57144", "mrqa_naturalquestions-train-56602", "mrqa_naturalquestions-train-14196", "mrqa_squad-validation-8542", "mrqa_naturalquestions-train-34756", "mrqa_naturalquestions-train-73353", "mrqa_naturalquestions-train-75298", "mrqa_naturalquestions-train-79850", "mrqa_naturalquestions-train-56208", "mrqa_naturalquestions-train-85648", "mrqa_naturalquestions-validation-6341", "mrqa_naturalquestions-train-19578", "mrqa_naturalquestions-train-69914", "mrqa_naturalquestions-train-64472", "mrqa_naturalquestions-train-45833", "mrqa_naturalquestions-train-31279", "mrqa_naturalquestions-train-21786", "mrqa_naturalquestions-train-1404", "mrqa_hotpotqa-validation-2340", "mrqa_naturalquestions-train-6110", "mrqa_naturalquestions-train-73391", "mrqa_naturalquestions-train-38269", "mrqa_naturalquestions-train-21442", "mrqa_naturalquestions-train-11462", "mrqa_naturalquestions-train-24980", "mrqa_naturalquestions-train-41892", "mrqa_naturalquestions-train-33243", "mrqa_naturalquestions-train-43261", "mrqa_naturalquestions-train-50012"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 26, "before_eval": {"predictions": ["Jewish audiences, with the Romans serving as external arbiters on disputes concerning Jewish customs and law", "south of the Kancamagus Highway", "an investment technique outlined by Joel Greenblatt that uses the principles of value investing", "true history of the Kelly Gang", "Honolulu", "1910\u20131940", "non-teaching posts", "Catch Me Who Can", "jazz saxophonist", "tennis", "4,000", "Khagan", "wuthering Heights", "canal", "spice", "The Simpsons Spin-Off Showcase", "Barry Parker", "San Bernardino", "A portion of Grainger Town was demolished in the 1960s to make way for the Eldon Square Shopping Centre", "Albany High School for Educating People of Color", "Rashtrapati Bhavan", "a non-commissioned officer in the United States Army's premier special operations unit, the 1st Special Forces Operational Detachment- Delta (1SFOD-D) or \" Delta Force\"", "Anakin Skywalker", "may enhance the chances for acquittal but make for more boring proceedings and reduced press coverage", "Cee - Lo", "Anglican", "pacific region", "battleship", "magnetism", "sitting directly on the Gulf of Guinea on the Atlantic Ocean in Nigeria", "opportunity-based entrepreneurship", "January 11, 1755 or 1757"], "metric_results": {"EM": 0.1875, "QA-F1": 0.3326704545454545}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false], "QA-F1": [0.2222222222222222, 0.4, 0.5, 0.33333333333333337, 0.33333333333333337, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.4, 0.5454545454545454, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8000000000000002]}}, "error_ids": ["mrqa_naturalquestions-validation-3027", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-2184", "mrqa_triviaqa-validation-6772", "mrqa_hotpotqa-validation-4553", "mrqa_squad-validation-1994", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-6689", "mrqa_hotpotqa-validation-5", "mrqa_triviaqa-validation-3472", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6224", "mrqa_hotpotqa-validation-2436", "mrqa_naturalquestions-validation-2214", "mrqa_squad-validation-2428", "mrqa_squad-validation-5184", "mrqa_hotpotqa-validation-1446", "mrqa_naturalquestions-validation-800", "mrqa_hotpotqa-validation-4585", "mrqa_squad-validation-6837", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-7581", "mrqa_triviaqa-validation-5839", "mrqa_triviaqa-validation-5050", "mrqa_triviaqa-validation-3393", "mrqa_hotpotqa-validation-945"], "retrieved_ids": ["mrqa_naturalquestions-train-18537", "mrqa_naturalquestions-train-64391", "mrqa_naturalquestions-train-46417", "mrqa_naturalquestions-train-36550", "mrqa_naturalquestions-train-18811", "mrqa_naturalquestions-train-25676", "mrqa_naturalquestions-train-28391", "mrqa_naturalquestions-train-64565", "mrqa_naturalquestions-train-18868", "mrqa_naturalquestions-train-87103", "mrqa_naturalquestions-train-18614", "mrqa_naturalquestions-train-22543", "mrqa_naturalquestions-train-72300", "mrqa_naturalquestions-train-759", "mrqa_naturalquestions-train-16145", "mrqa_naturalquestions-train-17031", "mrqa_naturalquestions-train-87911", "mrqa_naturalquestions-train-82315", "mrqa_naturalquestions-train-6733", "mrqa_naturalquestions-train-49045", "mrqa_naturalquestions-train-44166", "mrqa_naturalquestions-train-66187", "mrqa_naturalquestions-train-80128", "mrqa_triviaqa-validation-4681", "mrqa_naturalquestions-train-30455", "mrqa_naturalquestions-train-27465", "mrqa_naturalquestions-train-70278", "mrqa_naturalquestions-train-31555", "mrqa_naturalquestions-train-25662", "mrqa_naturalquestions-train-51582", "mrqa_naturalquestions-train-5321", "mrqa_naturalquestions-train-51240"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 27, "before_eval": {"predictions": ["l.A. producer Bones Howe", "jools Holland", "blackberry and dewberry", "dark dust and gases", "\" Big Mamie\"", "birmingham", "the \"eternal outsider, the sardonic drifter\" someone who rebels against the social structure", "a light sky-blue color caused by absorption in the red", "they captured the Tower of London", "1963", "2005\u201306 NBA season", "the internal thylakoid system", "sociable scene", "Melbourne", "no veto", "the fourth season", "the weak and electromagnetic forces are expressions of a more fundamental electrostrong interaction", "the availability of skilled tradespeople", "hardness", "A simple iron boar crest adorns the top of this helmet associating it with the Benty Grange helmet and the Guilden Morden boar from the same period", "the University of Northumbria at Newcastle in 1992", "curtin", "Lofton", "25 - yard line", "Swedish astronomer Anders Celsius", "about 7,000 out of 20,000 inhabitants of Newcastle", "lion, leopard, buffalo, rhinoceros, and elephant", "in Romans 1:17) lives by faith", "margaret", "possible combinations of two goods ( such as butter and guns )", "antwerp", "group"], "metric_results": {"EM": 0.0625, "QA-F1": 0.2141416742979243}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.42857142857142855, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.39999999999999997, 0.28571428571428575, 0.0, 0.3076923076923077, 0.0, 0.0, 0.0, 0.375, 0.8, 0.2222222222222222, 1.0, 0.6666666666666666, 0.0, 0.20000000000000004, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-348", "mrqa_triviaqa-validation-2899", "mrqa_triviaqa-validation-6331", "mrqa_hotpotqa-validation-2428", "mrqa_squad-validation-3539", "mrqa_triviaqa-validation-2980", "mrqa_squad-validation-7711", "mrqa_hotpotqa-validation-4022", "mrqa_squad-validation-8739", "mrqa_triviaqa-validation-1423", "mrqa_squad-validation-2966", "mrqa_squad-validation-2733", "mrqa_hotpotqa-validation-158", "mrqa_squad-validation-10312", "mrqa_squad-validation-6855", "mrqa_triviaqa-validation-5962", "mrqa_hotpotqa-validation-1226", "mrqa_squad-validation-5337", "mrqa_triviaqa-validation-1034", "mrqa_hotpotqa-validation-3949", "mrqa_naturalquestions-validation-9105", "mrqa_naturalquestions-validation-3771", "mrqa_squad-validation-5125", "mrqa_squad-validation-2313", "mrqa_triviaqa-validation-388", "mrqa_naturalquestions-validation-2893", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-1507"], "retrieved_ids": ["mrqa_naturalquestions-train-59497", "mrqa_naturalquestions-train-43541", "mrqa_naturalquestions-train-54264", "mrqa_naturalquestions-train-75135", "mrqa_hotpotqa-validation-1142", "mrqa_naturalquestions-train-70738", "mrqa_naturalquestions-train-28589", "mrqa_naturalquestions-train-51868", "mrqa_hotpotqa-validation-5256", "mrqa_naturalquestions-train-19", "mrqa_naturalquestions-train-50525", "mrqa_naturalquestions-train-6436", "mrqa_naturalquestions-train-40572", "mrqa_naturalquestions-train-64005", "mrqa_naturalquestions-train-57654", "mrqa_naturalquestions-train-21351", "mrqa_naturalquestions-train-20368", "mrqa_naturalquestions-train-48627", "mrqa_naturalquestions-train-47477", "mrqa_naturalquestions-train-62033", "mrqa_naturalquestions-train-4912", "mrqa_naturalquestions-train-66021", "mrqa_naturalquestions-train-73300", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-train-50123", "mrqa_naturalquestions-train-20821", "mrqa_naturalquestions-train-82365", "mrqa_naturalquestions-train-39378", "mrqa_naturalquestions-train-15907", "mrqa_naturalquestions-train-36986", "mrqa_naturalquestions-train-80542", "mrqa_naturalquestions-train-14699"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 28, "before_eval": {"predictions": ["the Turk", "Chris Weidman", "nullification", "Harishchandra", "writing", "Professor Eobard Thawne", "slivovitz", "a US>10 a week raise over Tesla's US>18 per week salary", "1875", "member states", "gypsichord", "McKinsey's offices in Silicon Valley and India", "gypsophobia", "2000", "Crohn's disease or ulcerative colitis", "Alceu Ranzi", "Raya Yarbrough", "Cincinnati", "cruiserweight", "John D. Rockefeller", "the Old Testament", "UPS", "touring productions", "Football League", "re-encountered tegan", "renoir", "the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague", "Mary Eugenia Surratt", "1340", "the dodo bird", "people and their thoughts are both made from `` pure energy '', and that through the process of `` like energy attracting like energy '' a person can improve their own health, wealth and personal relationships", "on the Buses"], "metric_results": {"EM": 0.09375, "QA-F1": 0.17492913832199547}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.4444444444444445, 0.0, 0.4444444444444445, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.1, 0.0, 0.0, 1.0, 0.2040816326530612, 0.0]}}, "error_ids": ["mrqa_squad-validation-2291", "mrqa_hotpotqa-validation-1390", "mrqa_squad-validation-6835", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-5637", "mrqa_triviaqa-validation-4827", "mrqa_squad-validation-1276", "mrqa_hotpotqa-validation-4352", "mrqa_naturalquestions-validation-10495", "mrqa_triviaqa-validation-5237", "mrqa_hotpotqa-validation-5110", "mrqa_triviaqa-validation-3760", "mrqa_triviaqa-validation-6250", "mrqa_naturalquestions-validation-4127", "mrqa_squad-validation-4309", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-7110", "mrqa_triviaqa-validation-1347", "mrqa_squad-validation-8031", "mrqa_naturalquestions-validation-10687", "mrqa_squad-validation-5086", "mrqa_hotpotqa-validation-1251", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-6881", "mrqa_squad-validation-4921", "mrqa_triviaqa-validation-5810", "mrqa_squad-validation-8190", "mrqa_naturalquestions-validation-7821", "mrqa_triviaqa-validation-4308"], "retrieved_ids": ["mrqa_naturalquestions-train-63669", "mrqa_naturalquestions-train-87522", "mrqa_naturalquestions-train-14089", "mrqa_naturalquestions-train-80729", "mrqa_naturalquestions-train-86241", "mrqa_naturalquestions-train-67224", "mrqa_naturalquestions-train-21595", "mrqa_naturalquestions-train-78716", "mrqa_naturalquestions-train-15460", "mrqa_naturalquestions-train-70099", "mrqa_naturalquestions-train-7448", "mrqa_naturalquestions-train-23244", "mrqa_naturalquestions-train-20542", "mrqa_naturalquestions-train-52203", "mrqa_naturalquestions-train-24102", "mrqa_naturalquestions-train-82635", "mrqa_naturalquestions-train-52298", "mrqa_naturalquestions-train-78936", "mrqa_naturalquestions-train-17622", "mrqa_naturalquestions-train-36985", "mrqa_naturalquestions-train-36720", "mrqa_naturalquestions-train-23656", "mrqa_naturalquestions-train-61509", "mrqa_naturalquestions-train-83419", "mrqa_naturalquestions-train-49822", "mrqa_naturalquestions-train-2012", "mrqa_naturalquestions-train-24743", "mrqa_naturalquestions-train-70892", "mrqa_naturalquestions-train-83458", "mrqa_naturalquestions-train-49591", "mrqa_naturalquestions-train-48547", "mrqa_naturalquestions-train-73906"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 29, "before_eval": {"predictions": ["poisonous", "886 AD", "to finance his own projects with varying degrees of success", "Le Mans", "a virtual sex video game", "Tokyo", "defensive end Kony Ealy", "the parallelogram rule of vector addition", "abraham lincoln", "pheasants", "startup neutron source", "starry starry night", "larger", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "the local administrative structure of past Chinese dynasties", "Doctorin' the Tardis", "National Basketball Development League", "gillingham", "St. Mary's County", "T. J. Ward", "2,615", "Pyeongchang", "athlete Colin Kaepernick ( although he prefers \"Kap\")", "It can recover many kinds of passwords using methods such as network packet sniffing, cracking various password hashes by using method such as dictionary attacks, brute force and cryptanalysis attacks", "Homeless Man", "Charles and Ray Eames", "Brazil", "pawel Kopczynski", "the smallest subfield", "heartburn", "53%", "the light reactions"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3339792813677195}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false], "QA-F1": [0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.0, 0.47058823529411764, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.9310344827586207, 0.6666666666666666, 0.28571428571428575, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4708", "mrqa_naturalquestions-validation-866", "mrqa_squad-validation-1249", "mrqa_hotpotqa-validation-3497", "mrqa_triviaqa-validation-6632", "mrqa_naturalquestions-validation-6610", "mrqa_squad-validation-814", "mrqa_squad-validation-10408", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-3103", "mrqa_triviaqa-validation-7032", "mrqa_squad-validation-3344", "mrqa_naturalquestions-validation-5826", "mrqa_triviaqa-validation-1280", "mrqa_naturalquestions-validation-5961", "mrqa_squad-validation-772", "mrqa_hotpotqa-validation-3031", "mrqa_naturalquestions-validation-9765", "mrqa_hotpotqa-validation-5383", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-7567", "mrqa_hotpotqa-validation-5709", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-2709", "mrqa_squad-validation-8873"], "retrieved_ids": ["mrqa_naturalquestions-train-6798", "mrqa_naturalquestions-train-36203", "mrqa_naturalquestions-train-69992", "mrqa_hotpotqa-validation-4734", "mrqa_naturalquestions-train-33320", "mrqa_naturalquestions-train-54031", "mrqa_naturalquestions-train-31194", "mrqa_naturalquestions-train-37009", "mrqa_naturalquestions-train-56197", "mrqa_naturalquestions-train-6172", "mrqa_naturalquestions-train-59067", "mrqa_naturalquestions-train-62941", "mrqa_naturalquestions-train-34977", "mrqa_naturalquestions-train-19182", "mrqa_naturalquestions-train-64865", "mrqa_naturalquestions-train-38801", "mrqa_naturalquestions-train-6834", "mrqa_naturalquestions-train-34737", "mrqa_naturalquestions-train-50602", "mrqa_squad-validation-67", "mrqa_naturalquestions-train-32041", "mrqa_naturalquestions-train-56745", "mrqa_naturalquestions-train-86770", "mrqa_naturalquestions-train-51906", "mrqa_naturalquestions-train-55308", "mrqa_naturalquestions-train-2860", "mrqa_naturalquestions-train-34933", "mrqa_naturalquestions-train-45634", "mrqa_naturalquestions-train-60350", "mrqa_triviaqa-validation-6331", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-train-18920"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 30, "before_eval": {"predictions": ["the judiciary", "Frederick Louis, Prince of Wales, son of King George II", "the Mayor's son", "abraham lincoln", "Basil Fawlty", "Erick Avari, Michael McKean, Amy D. Jacobson, Marty Ingels, Earl Boen, Jordana Capra, Dirk Benedict, and Rip Taylor", "BBC UKTV", "arthur karsg\u00e5rd", "demographics and economic ties", "three or more", "The Kickoff Game", "narcolepsy", "arctic monkeys", "monza", "arthur arthur", "usernames, passwords, commands and data can be read by anyone able to perform packet capture ( sniffing ) on the network", "computer program", "The centre-right Australian Labor Party (ALP) the rural-right Liberal Party of Australia, and the environmentalist Australian Greens", "marduk", "arkla", "a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda, as well as a nearly $1.8 billion dam", "arthur d. Daniels", "extends 2,000 kilometres ( 1,200 mi ) down the Australian northeast coast", "Article 7, Paragraph 4", "New Jersey to the east", "Easy", "Sebastian Lund ( Rob Kerkovich ), a criminalist turned forensics agent and the team's newest member", "the first female to hold this position", "National Lottery", "arctic", "aragon", "to facilitate compliance with the Telephone Consumer Protection Act of 1991"], "metric_results": {"EM": 0.0625, "QA-F1": 0.13888698107448105}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.14285714285714288, 0.22222222222222224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.07692307692307691, 0.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.4000000000000001]}}, "error_ids": ["mrqa_naturalquestions-validation-4645", "mrqa_hotpotqa-validation-2559", "mrqa_naturalquestions-validation-9608", "mrqa_triviaqa-validation-3920", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-4079", "mrqa_squad-validation-7793", "mrqa_triviaqa-validation-1921", "mrqa_squad-validation-2577", "mrqa_hotpotqa-validation-4578", "mrqa_naturalquestions-validation-3209", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-1722", "mrqa_naturalquestions-validation-3533", "mrqa_squad-validation-2885", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7090", "mrqa_squad-validation-8451", "mrqa_triviaqa-validation-7684", "mrqa_naturalquestions-validation-4710", "mrqa_squad-validation-6961", "mrqa_hotpotqa-validation-4604", "mrqa_hotpotqa-validation-3333", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-3134", "mrqa_hotpotqa-validation-5604", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-2091", "mrqa_naturalquestions-validation-10328"], "retrieved_ids": ["mrqa_naturalquestions-train-77229", "mrqa_naturalquestions-train-16856", "mrqa_naturalquestions-train-49179", "mrqa_naturalquestions-train-2690", "mrqa_naturalquestions-train-29950", "mrqa_naturalquestions-train-49900", "mrqa_naturalquestions-train-29087", "mrqa_naturalquestions-train-40261", "mrqa_naturalquestions-train-40604", "mrqa_naturalquestions-train-5988", "mrqa_naturalquestions-train-71243", "mrqa_hotpotqa-validation-2944", "mrqa_naturalquestions-train-4728", "mrqa_naturalquestions-train-3469", "mrqa_naturalquestions-train-69875", "mrqa_naturalquestions-train-16027", "mrqa_naturalquestions-train-3737", "mrqa_naturalquestions-train-60248", "mrqa_naturalquestions-train-77013", "mrqa_naturalquestions-train-2559", "mrqa_naturalquestions-train-28644", "mrqa_naturalquestions-train-13334", "mrqa_naturalquestions-train-29021", "mrqa_naturalquestions-train-2851", "mrqa_naturalquestions-train-13049", "mrqa_naturalquestions-train-52436", "mrqa_naturalquestions-train-10810", "mrqa_naturalquestions-train-25659", "mrqa_naturalquestions-train-34977", "mrqa_naturalquestions-train-68832", "mrqa_triviaqa-validation-7253", "mrqa_naturalquestions-train-33519"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 31, "before_eval": {"predictions": ["Yolanda Sald\u00edvar", "Andreas", "sarajevo", "alexander", "Newell Highway", "cybermen", "foot", "shopping mall located in Bloomington, Minnesota, United States ( a suburb of the Twin Cities )", "to avoid responsibility for her actions", "Andrew Adamson, Kelly Asbury and Conrad Vernon", "zigeunerbaron", "Subutai", "emissions resulting from human activities are substantially increasing the atmospheric concentrations of the greenhouse gases, resulting on average in an additional warming of the Earth's surface", "alexhaestus", "the RAF", "reduce growth in relatively poor countries but encourage growth", "Ibrium", "strictly", "Polish-Jewish", "General Francisco Franco", "Anything Goes", "ottoexeter", "390 billion", "Washington Street", "May 10, 1976", "five", "London Tipton", "his frustration with the atmosphere in the group at that time -- namely, Paul McCartney's over-assertiveness and criticism of his guitar playing", "dan davan", "John Smith", "lusitania", "the variety of occupations necessary to sustain the community as distinct from the indigenous population"], "metric_results": {"EM": 0.25, "QA-F1": 0.3384596884596885}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.15384615384615385, 0.2222222222222222, 0.4444444444444445, 0.0, 0.0, 0.37037037037037035, 0.0, 0.0, 0.3636363636363636, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6428571428571429, 0.0, 1.0, 1.0, 0.13333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-1385", "mrqa_triviaqa-validation-1050", "mrqa_triviaqa-validation-457", "mrqa_triviaqa-validation-5071", "mrqa_naturalquestions-validation-5604", "mrqa_squad-validation-6734", "mrqa_hotpotqa-validation-2564", "mrqa_triviaqa-validation-3300", "mrqa_squad-validation-6128", "mrqa_squad-validation-8589", "mrqa_triviaqa-validation-5513", "mrqa_naturalquestions-validation-954", "mrqa_squad-validation-7469", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-2493", "mrqa_naturalquestions-validation-9825", "mrqa_triviaqa-validation-3286", "mrqa_triviaqa-validation-2609", "mrqa_squad-validation-4415", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-5307", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-743", "mrqa_squad-validation-3106"], "retrieved_ids": ["mrqa_naturalquestions-train-54291", "mrqa_naturalquestions-train-28771", "mrqa_squad-validation-1374", "mrqa_naturalquestions-train-49638", "mrqa_naturalquestions-train-84240", "mrqa_naturalquestions-train-29950", "mrqa_naturalquestions-train-20432", "mrqa_naturalquestions-train-45823", "mrqa_naturalquestions-train-31864", "mrqa_naturalquestions-train-31170", "mrqa_naturalquestions-train-45708", "mrqa_naturalquestions-train-56239", "mrqa_naturalquestions-train-16854", "mrqa_naturalquestions-train-83966", "mrqa_naturalquestions-train-57333", "mrqa_naturalquestions-train-33865", "mrqa_naturalquestions-train-53906", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-train-61078", "mrqa_naturalquestions-train-69865", "mrqa_naturalquestions-train-49822", "mrqa_naturalquestions-train-49688", "mrqa_naturalquestions-train-59713", "mrqa_naturalquestions-train-53592", "mrqa_naturalquestions-train-47560", "mrqa_naturalquestions-train-73802", "mrqa_naturalquestions-train-1148", "mrqa_naturalquestions-train-62106", "mrqa_naturalquestions-train-16730", "mrqa_naturalquestions-train-54288", "mrqa_naturalquestions-train-47646", "mrqa_squad-validation-608"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 32, "before_eval": {"predictions": ["A high-gain S-band antenna", "Orthodox Christians", "Heineken International", "beer", "gender testing", "Tom Bower", "Health and Environmental effects", "following brake failure", "CD4+ and CD8+ (\u03b1\u03b2) T cells", "relatively low salaries", "fruit", "Heading Out to the Highway", "Moonraker", "$12.99", "Michael Oppenheimer", "England national team", "poor and well socially standing Chinese", "Tough Enough", "Convention", "5,922", "December 5, 1991", "\"Chronicle\"", "Philadelphia 76ers", "Dutch figure of Sinterklaas ( himself also based on Saint Nicholas )", "Stern-Plaza", "WBC/WBA heavyweight champion Joe Frazier", "23 March 1991", "Sunday", "Dealey Plaza", "Nairobi, Kenya", "During the last Ice Age", "Anno 2053"], "metric_results": {"EM": 0.21875, "QA-F1": 0.27391098484848486}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.1818181818181818, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-3887", "mrqa_hotpotqa-validation-572", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3066", "mrqa_naturalquestions-validation-4761", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-5510", "mrqa_squad-validation-6602", "mrqa_triviaqa-validation-7156", "mrqa_hotpotqa-validation-1119", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2245", "mrqa_squad-validation-8617", "mrqa_hotpotqa-validation-305", "mrqa_squad-validation-8095", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-10311", "mrqa_hotpotqa-validation-2910", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1660", "mrqa_naturalquestions-validation-7049", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-2985", "mrqa_triviaqa-validation-1079", "mrqa_hotpotqa-validation-4221"], "retrieved_ids": ["mrqa_naturalquestions-train-5867", "mrqa_naturalquestions-train-6525", "mrqa_naturalquestions-train-43791", "mrqa_naturalquestions-train-55313", "mrqa_naturalquestions-train-39378", "mrqa_naturalquestions-train-49731", "mrqa_naturalquestions-train-79627", "mrqa_naturalquestions-train-59640", "mrqa_naturalquestions-train-26161", "mrqa_naturalquestions-train-3758", "mrqa_naturalquestions-train-26306", "mrqa_naturalquestions-train-5942", "mrqa_naturalquestions-train-37323", "mrqa_naturalquestions-train-42726", "mrqa_naturalquestions-train-16910", "mrqa_naturalquestions-train-18324", "mrqa_naturalquestions-train-80699", "mrqa_naturalquestions-train-63703", "mrqa_naturalquestions-train-74228", "mrqa_naturalquestions-train-30865", "mrqa_naturalquestions-train-86013", "mrqa_naturalquestions-train-56528", "mrqa_naturalquestions-train-39061", "mrqa_naturalquestions-train-10733", "mrqa_naturalquestions-train-84490", "mrqa_naturalquestions-train-55090", "mrqa_naturalquestions-train-74828", "mrqa_naturalquestions-train-41886", "mrqa_naturalquestions-train-76537", "mrqa_naturalquestions-train-56194", "mrqa_naturalquestions-train-6726", "mrqa_naturalquestions-train-58280"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 33, "before_eval": {"predictions": ["Habsburg control of the First Empire, the Spanish throne, and other royal houses", "is a controversial Canadian-American Roman Catholic priest based in the United States near Detroit at Royal Oak, Michigan's National Shrine of the Little Flower church", "1967", "is the interest on a shark's loan", "the twelfth most populous city in the United States", "115", "bridge", "it triggers a cascade of events through phosphorylation of intracellular proteins that ultimately transmit ( `` transduce '' ) the extracellular signal to the nucleus, causing changes in gene expression", "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued)", "Bass", "Chava", "Acadia (present-day New Brunswick and parts of Nova Scotia, including \u00cele Royale ( present-day Cape Breton Island )", "japan", "bridge", "Yunnan- Fu", "Mumbai", "Broken Hill and Sydney", "2005", "forgiveness was God's alone to grant", "\"The Doctor's Daughter\"", "september", "september", "the desire to prevent things that are indisputably bad", "1879", "was the niece of Empress Taitu Bitul, consort of Emperor Menelik II of Ethiopia", "staying with the same group of peers for all classes", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "enthusiasm and energy", "september", "Datsun 810", "Arkansas", "Oslo county"], "metric_results": {"EM": 0.125, "QA-F1": 0.15375784266409265}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.14285714285714288, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.1081081081081081, 0.08, 0.0, 0.0, 0.07142857142857144, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.14285714285714288, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-9984", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-2092", "mrqa_squad-validation-7278", "mrqa_naturalquestions-validation-8203", "mrqa_triviaqa-validation-1410", "mrqa_naturalquestions-validation-9271", "mrqa_squad-validation-7571", "mrqa_hotpotqa-validation-650", "mrqa_naturalquestions-validation-3523", "mrqa_squad-validation-10180", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-6499", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-5727", "mrqa_squad-validation-2010", "mrqa_squad-validation-7741", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-6186", "mrqa_squad-validation-6878", "mrqa_naturalquestions-validation-7387", "mrqa_hotpotqa-validation-2588", "mrqa_naturalquestions-validation-5297", "mrqa_squad-validation-2147", "mrqa_triviaqa-validation-2812", "mrqa_squad-validation-3733", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-1211"], "retrieved_ids": ["mrqa_naturalquestions-train-52004", "mrqa_naturalquestions-train-16856", "mrqa_naturalquestions-train-6148", "mrqa_naturalquestions-train-32728", "mrqa_naturalquestions-train-68456", "mrqa_naturalquestions-train-60977", "mrqa_naturalquestions-train-24899", "mrqa_naturalquestions-train-55723", "mrqa_naturalquestions-train-14505", "mrqa_naturalquestions-train-33438", "mrqa_naturalquestions-train-75600", "mrqa_naturalquestions-train-29950", "mrqa_naturalquestions-train-7459", "mrqa_naturalquestions-train-48534", "mrqa_naturalquestions-train-43875", "mrqa_naturalquestions-train-54176", "mrqa_naturalquestions-train-36021", "mrqa_naturalquestions-train-18600", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-train-75211", "mrqa_naturalquestions-train-25701", "mrqa_naturalquestions-train-51073", "mrqa_naturalquestions-train-12832", "mrqa_naturalquestions-train-52700", "mrqa_naturalquestions-train-30360", "mrqa_naturalquestions-train-21131", "mrqa_naturalquestions-train-85867", "mrqa_naturalquestions-train-83575", "mrqa_naturalquestions-train-58080", "mrqa_naturalquestions-train-9279", "mrqa_naturalquestions-train-61185", "mrqa_naturalquestions-train-65265"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 34, "before_eval": {"predictions": ["a Deputy Chairman", "Norman Hartnell", "engaging in the forbidden speech", "Veronica", "Victorian College of the Arts", "Britain", "boulangere", "0.2 inhabitants per square kilometre", "James David Graham Niven", "France", "Ian Richard Kyle Paisley", "Bataan Death March", "euro", "Graham McPherson", "the United States", "by Sherry Rowland and Mario Molina", "1886", "2008", "Stanwyck's bedroom window overlooks the night skyline of Manhattan", "Joaquin Phoenix", "pole", "Johnny Darrell", "carotid artery disease", "a Belgian law requiring all margarine to be in cube shaped packages", "Euler's totient function", "earwax", "the set of all connected graphs", "third", "red", "Toyota Corona", "Kurt Vonnegut", "princess Rapunzel"], "metric_results": {"EM": 0.125, "QA-F1": 0.22071504884004883}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.14285714285714288, 0.6153846153846153, 1.0, 0.0, 0.0, 0.0, 0.15384615384615385, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42857142857142855, 1.0, 0.16666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2962", "mrqa_squad-validation-6673", "mrqa_naturalquestions-validation-6787", "mrqa_hotpotqa-validation-3982", "mrqa_triviaqa-validation-5516", "mrqa_squad-validation-4369", "mrqa_triviaqa-validation-5496", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-1139", "mrqa_hotpotqa-validation-3072", "mrqa_triviaqa-validation-1220", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-5740", "mrqa_naturalquestions-validation-4366", "mrqa_hotpotqa-validation-3002", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4288", "mrqa_triviaqa-validation-7592", "mrqa_squad-validation-4255", "mrqa_squad-validation-9063", "mrqa_squad-validation-1635", "mrqa_triviaqa-validation-7184", "mrqa_squad-validation-3708", "mrqa_squad-validation-8034", "mrqa_triviaqa-validation-1198"], "retrieved_ids": ["mrqa_naturalquestions-train-49277", "mrqa_naturalquestions-train-75165", "mrqa_naturalquestions-train-29378", "mrqa_naturalquestions-train-54427", "mrqa_naturalquestions-train-1304", "mrqa_naturalquestions-train-48829", "mrqa_naturalquestions-train-82840", "mrqa_naturalquestions-train-28623", "mrqa_naturalquestions-train-37455", "mrqa_naturalquestions-train-85212", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-train-29585", "mrqa_naturalquestions-train-16629", "mrqa_naturalquestions-train-35683", "mrqa_naturalquestions-train-3076", "mrqa_naturalquestions-train-1555", "mrqa_squad-validation-3964", "mrqa_naturalquestions-train-27643", "mrqa_naturalquestions-train-69336", "mrqa_naturalquestions-train-37282", "mrqa_naturalquestions-train-55245", "mrqa_naturalquestions-train-39073", "mrqa_naturalquestions-train-33257", "mrqa_naturalquestions-train-66650", "mrqa_naturalquestions-train-32277", "mrqa_naturalquestions-train-31524", "mrqa_naturalquestions-train-85053", "mrqa_squad-validation-7554", "mrqa_naturalquestions-train-70639", "mrqa_naturalquestions-train-84781", "mrqa_naturalquestions-train-82710", "mrqa_naturalquestions-train-13496"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 35, "before_eval": {"predictions": ["up to 2% higher than during outbreaks of 13- and 17-year cicadas", "supply and demand", "Emma Watson, Dan Stevens, Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Nathan Mack, Ian McKellen, and Emma Thompson", "brain, muscles, and liver", "afghanistan", "Washington Redskins", "in the courtyard adjoining the Assembly Hall, which is part of the School of Divinity of the University of Edinburgh", "William Howard Ashton", "national security, big oil companies and bribery and corruption at the highest levels of the government of the United States", "high and persistent unemployment", "Broward County", "Lee Byung-hun", "changing display or audio settings quickly", "British Civil War", "from the spectroscopic notation for the associated atomic orbitals : sharp, principal, diffuse and fundamental, and then g which follows f in the alphabet", "if the income share of the top 20 percent (the rich) increases", "Beauty and the Beast", "South Africa", "Tyler \" Ty\" Mendoza", "William B. Travis", "a seal illegally", "the UMC", "Brian Liesegang", "Don Hahn", "Port Moresby", "David Seville", "National Association for the Advancement of Colored People", "1963\u20131989", "The Trenton Evening Times", "John Prescott", "Darrin Stephens", "6500 - 1500 BC"], "metric_results": {"EM": 0.25, "QA-F1": 0.3821314102564103}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false], "QA-F1": [0.5, 0.28571428571428575, 0.14285714285714288, 0.4, 0.0, 0.0, 0.4, 1.0, 0.0, 0.4, 0.0, 0.0, 0.4, 0.0, 0.4615384615384615, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-9123", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-7704", "mrqa_triviaqa-validation-4829", "mrqa_naturalquestions-validation-4544", "mrqa_squad-validation-9400", "mrqa_triviaqa-validation-6423", "mrqa_squad-validation-7352", "mrqa_hotpotqa-validation-4447", "mrqa_hotpotqa-validation-2947", "mrqa_naturalquestions-validation-1587", "mrqa_triviaqa-validation-3767", "mrqa_naturalquestions-validation-585", "mrqa_squad-validation-7495", "mrqa_triviaqa-validation-2999", "mrqa_squad-validation-3408", "mrqa_hotpotqa-validation-3853", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-1720", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-1812"], "retrieved_ids": ["mrqa_naturalquestions-train-10687", "mrqa_naturalquestions-train-76238", "mrqa_naturalquestions-train-73640", "mrqa_naturalquestions-train-77959", "mrqa_naturalquestions-train-13614", "mrqa_triviaqa-validation-5071", "mrqa_naturalquestions-train-24193", "mrqa_naturalquestions-train-45208", "mrqa_naturalquestions-train-38175", "mrqa_naturalquestions-train-69688", "mrqa_naturalquestions-train-39448", "mrqa_naturalquestions-train-40755", "mrqa_naturalquestions-train-77723", "mrqa_squad-validation-7351", "mrqa_naturalquestions-train-76031", "mrqa_naturalquestions-train-43326", "mrqa_naturalquestions-train-43906", "mrqa_naturalquestions-train-55300", "mrqa_naturalquestions-train-41827", "mrqa_naturalquestions-train-73988", "mrqa_naturalquestions-train-23586", "mrqa_naturalquestions-train-59297", "mrqa_naturalquestions-train-71074", "mrqa_naturalquestions-train-38925", "mrqa_triviaqa-validation-4852", "mrqa_naturalquestions-train-31201", "mrqa_naturalquestions-train-46967", "mrqa_naturalquestions-train-16854", "mrqa_naturalquestions-train-12822", "mrqa_naturalquestions-train-101", "mrqa_naturalquestions-train-75087", "mrqa_naturalquestions-train-82802"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 36, "before_eval": {"predictions": ["National Broadcasting Company", "Luc Besson and Adi Hasak", "Jupiter", "president", "the Cobham\u2013Edmonds thesis", "human, or humanoid aliens", "II", "March 2012", "jazz", "Muhammad Ali", "British rock group Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Menorca", "to civil disobedients", "Julius Caesar", "2%", "March 28, 1979", "James Halliday", "The formal language", "elizabeth lecouvreur", "the right side of the heart to the lungs", "Miasma theory", "to serve beer, and also often for cider", "two mountain ranges (sub-ranges of the Rocky Mountains)", "red", "The U.S. state of Georgia", "nettle", "$12", "flat rate", "Bestnight Girl", "to build a nationwide network in the UK", "roughly west", "Sudan"], "metric_results": {"EM": 0.25, "QA-F1": 0.3540636446886447}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.15384615384615385, 1.0, 0.14285714285714288, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.4444444444444445, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.6666666666666666, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-652", "mrqa_triviaqa-validation-4824", "mrqa_triviaqa-validation-37", "mrqa_squad-validation-7720", "mrqa_hotpotqa-validation-3911", "mrqa_triviaqa-validation-3803", "mrqa_hotpotqa-validation-2936", "mrqa_squad-validation-110", "mrqa_squad-validation-6759", "mrqa_naturalquestions-validation-4115", "mrqa_hotpotqa-validation-1884", "mrqa_naturalquestions-validation-3993", "mrqa_triviaqa-validation-5936", "mrqa_naturalquestions-validation-5552", "mrqa_squad-validation-4877", "mrqa_naturalquestions-validation-8525", "mrqa_hotpotqa-validation-1118", "mrqa_triviaqa-validation-199", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-2323", "mrqa_triviaqa-validation-6290", "mrqa_squad-validation-4626", "mrqa_squad-validation-9146", "mrqa_hotpotqa-validation-452"], "retrieved_ids": ["mrqa_naturalquestions-train-62064", "mrqa_naturalquestions-train-41128", "mrqa_naturalquestions-train-68301", "mrqa_naturalquestions-train-68305", "mrqa_naturalquestions-train-68054", "mrqa_naturalquestions-train-77618", "mrqa_naturalquestions-train-19706", "mrqa_squad-validation-6915", "mrqa_naturalquestions-train-71306", "mrqa_naturalquestions-train-39154", "mrqa_naturalquestions-train-19169", "mrqa_naturalquestions-train-41714", "mrqa_naturalquestions-train-62721", "mrqa_naturalquestions-train-49177", "mrqa_naturalquestions-train-59430", "mrqa_naturalquestions-train-101", "mrqa_naturalquestions-train-29555", "mrqa_naturalquestions-train-45880", "mrqa_naturalquestions-train-45811", "mrqa_naturalquestions-train-25513", "mrqa_naturalquestions-train-24487", "mrqa_naturalquestions-train-9477", "mrqa_naturalquestions-train-63141", "mrqa_naturalquestions-train-40535", "mrqa_naturalquestions-train-5475", "mrqa_naturalquestions-train-85550", "mrqa_naturalquestions-train-79569", "mrqa_naturalquestions-train-21686", "mrqa_naturalquestions-train-71112", "mrqa_hotpotqa-validation-1251", "mrqa_naturalquestions-train-58035", "mrqa_naturalquestions-train-17510"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 37, "before_eval": {"predictions": ["San Joaquin Valley Railroad", "his ribs were broken", "2007", "San Luis Obispo", "mother- of-pearl", "February 20, 1978", "haggis", "Walter Mondale", "at least 96", "De Inventione by Marcus Tullius Cicero", "india", "a bubble on a black background representing the circle with glossy gold letters", "The period known as the Ubaid period", "around 11 miles (18 km) south of San Jose", "Spotty Dog", "Rumplestiltskin", "Carlos Tevez", "lizard", "numerous musical venues, including the Teatr Wielki, the Polish National Opera, the Chamber opera, the National Philharmonic Hall and the National Theatre", "riper", "1991", "hydrogen sulfide", "7 January 1936", "lifetime protection", "twenty- three", "Edwin Hubble", "Many of the city's tax base dissipated", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Christian Poulsen", "make a defiant speech, or a speech explaining their actions", "sour cream", "Boston, Massachusetts"], "metric_results": {"EM": 0.1875, "QA-F1": 0.28616071428571427}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8333333333333334, 0.13333333333333333, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-1625", "mrqa_hotpotqa-validation-104", "mrqa_squad-validation-5451", "mrqa_triviaqa-validation-1015", "mrqa_naturalquestions-validation-4664", "mrqa_hotpotqa-validation-4154", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-3479", "mrqa_squad-validation-5702", "mrqa_naturalquestions-validation-9058", "mrqa_squad-validation-2660", "mrqa_triviaqa-validation-5258", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-1698", "mrqa_squad-validation-677", "mrqa_triviaqa-validation-7134", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-3876", "mrqa_hotpotqa-validation-2377", "mrqa_squad-validation-8069", "mrqa_squad-validation-7246", "mrqa_naturalquestions-validation-1375", "mrqa_hotpotqa-validation-633", "mrqa_squad-validation-6737", "mrqa_triviaqa-validation-2524"], "retrieved_ids": ["mrqa_naturalquestions-train-56113", "mrqa_naturalquestions-train-25579", "mrqa_naturalquestions-train-6559", "mrqa_naturalquestions-train-27782", "mrqa_naturalquestions-train-45881", "mrqa_naturalquestions-train-53589", "mrqa_naturalquestions-train-23178", "mrqa_naturalquestions-train-70837", "mrqa_naturalquestions-train-85118", "mrqa_naturalquestions-train-5561", "mrqa_naturalquestions-train-13385", "mrqa_naturalquestions-train-51167", "mrqa_naturalquestions-train-10768", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-train-27234", "mrqa_naturalquestions-train-81285", "mrqa_naturalquestions-train-79514", "mrqa_naturalquestions-train-68857", "mrqa_naturalquestions-train-17936", "mrqa_naturalquestions-train-56514", "mrqa_naturalquestions-train-86039", "mrqa_naturalquestions-train-71762", "mrqa_naturalquestions-train-26018", "mrqa_naturalquestions-train-53047", "mrqa_naturalquestions-train-80852", "mrqa_naturalquestions-train-73343", "mrqa_naturalquestions-train-21092", "mrqa_naturalquestions-train-76832", "mrqa_naturalquestions-train-52234", "mrqa_naturalquestions-train-87680", "mrqa_naturalquestions-train-68730", "mrqa_naturalquestions-train-10813"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 38, "before_eval": {"predictions": ["NP-complete knapsack", "Dan Stevens", "New England", "Etienne de Mestre", "dragon", "The primary catalyst for secession was slavery, especially Southern political leaders'resistance to attempts by Northern antislavery political forces to block the expansion of slavery into the western territories", "American Indian allies", "The History of Little Goody Two - Shoes", "It has the longest rotation period ( 243 days ) of any planet in the Solar System and rotates in the opposite direction to most other planets ( meaning the Sun would rise in the west and set in the east ).", "gathering money from the public", "Ganael Francis Hazard", "commissioned to purchase their required uniform items", "Jeff Meldrum", "It topped the Billboard Top LPs & Tapes chart for a week and remained in the chart for 741 weeks from 1973 to 1988. After the rules were changed in 2009", "Phil Archer", "It is on the opposite side the Willamette River from the main area of the city.", "The Paris Sisters", "gisela Pfab", "57", "journalist", "the fact that there is no revising chamber", "florida", "the points of algebro-geometric objects", "most of the items in the collection, unless those were newly accessioned into the collection", "it does not satisfy the criteria for a medium of exchange", "strychnine", "The Alta Wind Energy Center in California", "It took all of human history up to 1804 for the world's population to reach 1 billion ; the next billion came just over a century later, in 1927.", "13 June 2003", "Eddy Shah", "Darleen Carr", "in sequence with each heartbeat"], "metric_results": {"EM": 0.15625, "QA-F1": 0.27372105359237714}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.4, 0.11764705882352941, 0.0, 0.0, 1.0, 0.07692307692307693, 0.0, 1.0, 0.05714285714285714, 0.47058823529411764, 0.8571428571428571, 0.22222222222222224, 0.25, 0.07407407407407407, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.13333333333333333, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-1862", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-10255", "mrqa_hotpotqa-validation-727", "mrqa_naturalquestions-validation-4067", "mrqa_hotpotqa-validation-1116", "mrqa_naturalquestions-validation-4428", "mrqa_hotpotqa-validation-2682", "mrqa_naturalquestions-validation-3707", "mrqa_triviaqa-validation-3118", "mrqa_naturalquestions-validation-6210", "mrqa_squad-validation-9478", "mrqa_triviaqa-validation-1473", "mrqa_squad-validation-9032", "mrqa_squad-validation-5505", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-7630", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-2100", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2555"], "retrieved_ids": ["mrqa_naturalquestions-train-73128", "mrqa_naturalquestions-train-38547", "mrqa_hotpotqa-validation-1168", "mrqa_naturalquestions-train-48307", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-train-57794", "mrqa_naturalquestions-train-87438", "mrqa_naturalquestions-train-69961", "mrqa_naturalquestions-train-40188", "mrqa_naturalquestions-train-20364", "mrqa_naturalquestions-train-83558", "mrqa_naturalquestions-train-7211", "mrqa_naturalquestions-train-43371", "mrqa_naturalquestions-train-84875", "mrqa_naturalquestions-train-75454", "mrqa_naturalquestions-train-9524", "mrqa_squad-validation-5125", "mrqa_triviaqa-validation-7592", "mrqa_naturalquestions-train-3330", "mrqa_naturalquestions-train-25890", "mrqa_triviaqa-validation-7304", "mrqa_naturalquestions-train-6134", "mrqa_naturalquestions-train-65495", "mrqa_naturalquestions-train-73866", "mrqa_naturalquestions-train-66511", "mrqa_triviaqa-validation-1280", "mrqa_naturalquestions-train-34615", "mrqa_naturalquestions-train-38850", "mrqa_naturalquestions-train-51179", "mrqa_naturalquestions-train-20421", "mrqa_naturalquestions-train-105", "mrqa_naturalquestions-train-72723"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 39, "before_eval": {"predictions": ["capital city of Taiwan", "Dan Conner", "checkpoint Charlie", "President John F. Kennedy", "Katharine Hepburn, Joan Bennett, Frances Dee, and Jean Parker", "violence", "Joaquin Phoenix as Cash, Reese Witherspoon as Carter, Ginnifer Goodwin as Vivian Liberto, and Robert Patrick as Cash's father", "kuskusu", "1977", "John M. Grunsfeld", "New York", "may be quite simple but now that it's done.", "2000", "birmingham", "Fabbrica Italiana Automobili Torino", "the second Sunday of March", "relative units of force and mass", "woman", "two", "August 10, 1933", "The Golden Gate Bridge", "Sochi, Russia", "those who already hold wealth", "B. Traven", "Finding Nemo", "unidentified flying objects (UFOs) the extraterrestrial hypothesis (ETH) as well as paranormal and Fortean subjects in general", "oil prices", "birmingham", "264,152", "Princeton", "the German Empire", "high pressure or an electric current"], "metric_results": {"EM": 0.28125, "QA-F1": 0.38942197508373977}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false], "QA-F1": [0.8571428571428571, 1.0, 0.0, 0.0, 0.8, 1.0, 0.9230769230769231, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.11764705882352941, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-1438", "mrqa_hotpotqa-validation-944", "mrqa_naturalquestions-validation-9227", "mrqa_triviaqa-validation-2522", "mrqa_naturalquestions-validation-6554", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-7195", "mrqa_naturalquestions-validation-3698", "mrqa_triviaqa-validation-3017", "mrqa_squad-validation-10428", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-4024", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-1607", "mrqa_triviaqa-validation-2808", "mrqa_triviaqa-validation-1276", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-2203", "mrqa_squad-validation-3650"], "retrieved_ids": ["mrqa_naturalquestions-train-65486", "mrqa_naturalquestions-train-64697", "mrqa_naturalquestions-train-1653", "mrqa_naturalquestions-train-22793", "mrqa_naturalquestions-train-55379", "mrqa_naturalquestions-train-15460", "mrqa_naturalquestions-train-12716", "mrqa_naturalquestions-train-71683", "mrqa_naturalquestions-train-60098", "mrqa_naturalquestions-train-17510", "mrqa_naturalquestions-train-39360", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-train-72741", "mrqa_naturalquestions-train-13446", "mrqa_triviaqa-validation-3933", "mrqa_naturalquestions-train-15097", "mrqa_naturalquestions-train-2939", "mrqa_naturalquestions-train-63013", "mrqa_naturalquestions-train-55431", "mrqa_naturalquestions-train-56674", "mrqa_naturalquestions-train-70439", "mrqa_naturalquestions-train-57497", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-train-51906", "mrqa_naturalquestions-train-28780", "mrqa_naturalquestions-train-18295", "mrqa_naturalquestions-train-45809", "mrqa_naturalquestions-train-21560", "mrqa_naturalquestions-train-42014", "mrqa_naturalquestions-train-75202", "mrqa_naturalquestions-train-25513"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 40, "before_eval": {"predictions": ["50 fund", "Bartle Frere", "Samarkand", "ice dancing", "Isabella", "corgis", "14th to 17th centuries", "five starting pitchers, seven relief pitchers, two catchers, six infielders, and five outfielders", "he broadened the foundations of the Reformation placing them on prophetic faith", "Aristotle", "Cecil B. DeMille", "anti-inflammatory molecules, such as cortisol and catecholamines", "vUHMOaD/JVI aiAQBAJA jaMOJOD", "Jeff Garcia", "money", "The ability to make such orders is also based on express or implied Acts of Congress that delegate to the President some degree of discretionary power ( delegated legislation )", "son et lumi\u00e8re", "the Karluk Kara-Khanid ruler", "Sochi, Russia", "right", "from the Canadian Rockies continental divide east to central Saskatchewan, where it joins with another major river to make up the Saskatchewan River", "How soon after the cabin fire incident", "new Zealand", "detroit", "30", "Secret Intelligence Service", "100 billion", "et tu, brute", "by the photolysis of ozone by light of short wavelength", "4.7 / 5.5 - inch", "Queen City", "a qui tam provision that allows people who are not affiliated with the government, called `` relators '' under the law, to file actions on behalf of the government"], "metric_results": {"EM": 0.15625, "QA-F1": 0.3056358633764231}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.4, 0.0, 0.5, 0.0, 1.0, 0.375, 0.6666666666666666, 0.0, 0.0, 0.15384615384615383, 0.0, 0.0, 0.0, 0.7906976744186047, 0.0, 0.0, 1.0, 0.0, 0.17391304347826084, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.9411764705882353, 0.28571428571428575, 1.0, 0.16]}}, "error_ids": ["mrqa_naturalquestions-validation-6212", "mrqa_squad-validation-6248", "mrqa_triviaqa-validation-1571", "mrqa_hotpotqa-validation-1453", "mrqa_triviaqa-validation-2475", "mrqa_naturalquestions-validation-8961", "mrqa_squad-validation-2249", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-validation-7457", "mrqa_squad-validation-6588", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3213", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-1063", "mrqa_triviaqa-validation-6127", "mrqa_squad-validation-8247", "mrqa_triviaqa-validation-1859", "mrqa_naturalquestions-validation-8514", "mrqa_squad-validation-3932", "mrqa_triviaqa-validation-4123", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1791", "mrqa_triviaqa-validation-1877", "mrqa_triviaqa-validation-579", "mrqa_squad-validation-3617", "mrqa_naturalquestions-validation-6848", "mrqa_naturalquestions-validation-993"], "retrieved_ids": ["mrqa_naturalquestions-train-28312", "mrqa_naturalquestions-train-65889", "mrqa_naturalquestions-train-87577", "mrqa_naturalquestions-train-80171", "mrqa_naturalquestions-train-13555", "mrqa_naturalquestions-train-22400", "mrqa_naturalquestions-train-56738", "mrqa_naturalquestions-train-41112", "mrqa_triviaqa-validation-1792", "mrqa_naturalquestions-train-43775", "mrqa_naturalquestions-train-7599", "mrqa_naturalquestions-train-1219", "mrqa_naturalquestions-train-19", "mrqa_naturalquestions-train-5763", "mrqa_naturalquestions-train-84116", "mrqa_naturalquestions-train-33202", "mrqa_naturalquestions-train-85972", "mrqa_naturalquestions-train-24847", "mrqa_naturalquestions-train-25500", "mrqa_naturalquestions-train-15972", "mrqa_naturalquestions-train-13788", "mrqa_naturalquestions-train-14311", "mrqa_naturalquestions-train-76667", "mrqa_naturalquestions-train-13530", "mrqa_naturalquestions-train-46925", "mrqa_naturalquestions-train-19346", "mrqa_naturalquestions-train-67597", "mrqa_naturalquestions-train-44530", "mrqa_naturalquestions-train-6205", "mrqa_naturalquestions-train-19491", "mrqa_naturalquestions-train-65679", "mrqa_naturalquestions-train-18001"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 41, "before_eval": {"predictions": ["an end to all war", "Indian", "Gaels", "Three-card brag", "d\u00edsir", "cave lion", "Russian drama film", "the sediment load of the Rhine has strongly increased", "Washington metropolitan area", "scission", "User State Migration Tool ( USMT )", "Ordos City China Science Flying Universe Science and Technology Co.", "tartans", "PPG Paints Arena, Pittsburgh, Pennsylvania", "archaeology", "Section 30 of the Teaching Council Act 2001", "agnes Moorehead as the Goose", "1984", "quasars", "Northeast Monsoon", "Romansh", "lisabeth II", "breakfast news anchor and News Director of Perth's number one rating radio station", "Ian Fleming's James Bond quartermaster Q", "Epistle of Paul to the Philippians, often referred to simply as Philippians", "the division of labour, productivity, and free markets", "Gerard Marenghi", "Whitney Houston", "Nebula Award", "tabled a motion of no confidence", "war", "American singer Elvis Presley"], "metric_results": {"EM": 0.0, "QA-F1": 0.14636384856973095}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 0.0, 0.0, 0.1818181818181818, 0.0, 0.6666666666666666, 0.0, 0.4444444444444445, 0.0, 0.05882352941176471, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.2857142857142857, 0.16666666666666669, 0.0, 0.5, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-1592", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-2375", "mrqa_squad-validation-9355", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-9712", "mrqa_hotpotqa-validation-201", "mrqa_triviaqa-validation-4955", "mrqa_naturalquestions-validation-3058", "mrqa_triviaqa-validation-684", "mrqa_squad-validation-2142", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-5007", "mrqa_triviaqa-validation-2384", "mrqa_naturalquestions-validation-774", "mrqa_triviaqa-validation-6751", "mrqa_triviaqa-validation-2181", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-4823", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8338", "mrqa_hotpotqa-validation-3872", "mrqa_triviaqa-validation-1585", "mrqa_hotpotqa-validation-434", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-4320", "mrqa_naturalquestions-validation-9763"], "retrieved_ids": ["mrqa_naturalquestions-train-52189", "mrqa_naturalquestions-train-26334", "mrqa_naturalquestions-train-78598", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-train-39034", "mrqa_triviaqa-validation-4560", "mrqa_naturalquestions-train-71646", "mrqa_naturalquestions-train-21180", "mrqa_naturalquestions-train-77585", "mrqa_naturalquestions-train-37629", "mrqa_naturalquestions-train-46282", "mrqa_naturalquestions-train-42550", "mrqa_squad-validation-8739", "mrqa_naturalquestions-train-34229", "mrqa_naturalquestions-train-87031", "mrqa_naturalquestions-train-80872", "mrqa_naturalquestions-train-30360", "mrqa_naturalquestions-train-81588", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-train-75627", "mrqa_naturalquestions-train-17450", "mrqa_naturalquestions-train-83161", "mrqa_naturalquestions-train-67468", "mrqa_naturalquestions-train-43276", "mrqa_naturalquestions-train-41952", "mrqa_naturalquestions-train-9003", "mrqa_naturalquestions-train-6572", "mrqa_naturalquestions-train-50008", "mrqa_triviaqa-validation-1736", "mrqa_naturalquestions-train-19053", "mrqa_triviaqa-validation-6781", "mrqa_naturalquestions-train-45827"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 42, "before_eval": {"predictions": ["daryl Hannah", "cavatelli, acini di pepe, pastina, orzo, etc. ), lentils, or grated parmesan cheese", "lilliput", "an alliance between the city-state of Geneva and the Swiss Confederation", "the ozone generated in contact with the skin", "the nature of Abraham Lincoln's war goals", "physicians, lawyers, engineers, and accountants", "the ones who are violating the greater law are the members of the Navy", "Finland", "second vice-captain", "jonathan", "where the side - chain of the amino acid N - terminal to the scissile amide bond ( the P position )", "it is often bonded to glucose to form the disaccharide sucrose", "the number of men killed and the manner of the attacks", "Gainsborough Trinity Football Club", "public and private collections in the United States and abroad", "Extroverted Feeling ( Fi )", "Thursday", "yellow", "the appropriateness of the drug therapy (e.g. drug choice, dose, route, frequency, and duration of therapy) and its efficacy", "jonathan", "feats of exploration", "the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "bobby kirby keger", "The Private Education Student Financial Assistance", "violin", "The benefits of good works could be obtained by donating money to the church", "It became a moral justification to lift the world up to French standards by bringing Christianity and French culture. In 1884 the leading exponent of colonialism, Jules Ferry declared France had a civilising mission", "two forces, one pointing north, and one pointing east", "Bills", "Jack Murphy Stadium", "hierarchy theorems"], "metric_results": {"EM": 0.125, "QA-F1": 0.24661873411873414}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.7000000000000001, 0.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.15384615384615383, 0.0, 0.0, 0.0, 0.7567567567567568, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6923076923076924, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2952", "mrqa_naturalquestions-validation-8284", "mrqa_triviaqa-validation-3261", "mrqa_squad-validation-3044", "mrqa_naturalquestions-validation-9093", "mrqa_squad-validation-1841", "mrqa_squad-validation-6735", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-5731", "mrqa_triviaqa-validation-4137", "mrqa_naturalquestions-validation-7233", "mrqa_naturalquestions-validation-4185", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-680", "mrqa_naturalquestions-validation-6706", "mrqa_squad-validation-9569", "mrqa_triviaqa-validation-7426", "mrqa_squad-validation-6369", "mrqa_triviaqa-validation-7133", "mrqa_squad-validation-3385", "mrqa_triviaqa-validation-1586", "mrqa_triviaqa-validation-289", "mrqa_squad-validation-2150", "mrqa_squad-validation-9792", "mrqa_squad-validation-10395", "mrqa_hotpotqa-validation-5522", "mrqa_squad-validation-1808"], "retrieved_ids": ["mrqa_naturalquestions-train-86442", "mrqa_naturalquestions-train-25749", "mrqa_naturalquestions-train-73866", "mrqa_naturalquestions-train-32620", "mrqa_naturalquestions-train-26439", "mrqa_naturalquestions-train-46375", "mrqa_naturalquestions-train-73028", "mrqa_naturalquestions-train-58738", "mrqa_naturalquestions-train-72503", "mrqa_naturalquestions-train-8116", "mrqa_naturalquestions-train-77715", "mrqa_naturalquestions-train-15444", "mrqa_hotpotqa-validation-1385", "mrqa_naturalquestions-train-53177", "mrqa_naturalquestions-train-6073", "mrqa_naturalquestions-train-19157", "mrqa_naturalquestions-train-61037", "mrqa_triviaqa-validation-4955", "mrqa_naturalquestions-train-55518", "mrqa_naturalquestions-train-83419", "mrqa_naturalquestions-train-47744", "mrqa_naturalquestions-train-61694", "mrqa_naturalquestions-train-41714", "mrqa_naturalquestions-train-40105", "mrqa_naturalquestions-train-68723", "mrqa_naturalquestions-train-87229", "mrqa_naturalquestions-train-41917", "mrqa_naturalquestions-train-33112", "mrqa_naturalquestions-train-37773", "mrqa_naturalquestions-train-19082", "mrqa_squad-validation-10428", "mrqa_naturalquestions-train-33348"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 43, "before_eval": {"predictions": ["American Falls", "Scrabble", "egypt", "temple", "French", "a sailor coming home from a round trip", "A hostname is a domain name that has at least one associated IP address. For example, the domain names www.example.com and example.com are also hostnames", "the immune system is less active than normal", "opera", "Tom's of Maine", "a children's rhyme and song of a kind known as cumulative", "Rigoletto", "largest country on Earth", "the third-most abundant element in the universe", "furniture", "169", "football", "Nicholas Stone, Caius Gabriel Cibber, Grinling Gibbons, John Michael Rysbrack, Louis-Fran\u00e7ois Roubiliac, Peter Scheemakers", "Algernod Lanier Washington", "the Outfield", "singles", "Michael Edwards", "railway locomotives", "Mike Todd", "third quarter ( also known as last quarter )", "bresslaw", "Yuan T. Lee", "Kentucky, Virginia, and Tennessee", "avionics, telecommunications, and computers", "Mitochondrial Eve", "237 square miles", "egypt"], "metric_results": {"EM": 0.1875, "QA-F1": 0.27414596273291925}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, true, true, true, true, false, false], "QA-F1": [0.5, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.08695652173913045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7744", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-7538", "mrqa_triviaqa-validation-6916", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-981", "mrqa_naturalquestions-validation-2663", "mrqa_squad-validation-6442", "mrqa_triviaqa-validation-5580", "mrqa_hotpotqa-validation-518", "mrqa_triviaqa-validation-5087", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-850", "mrqa_squad-validation-3671", "mrqa_triviaqa-validation-4090", "mrqa_triviaqa-validation-5160", "mrqa_squad-validation-5586", "mrqa_hotpotqa-validation-5370", "mrqa_naturalquestions-validation-5822", "mrqa_triviaqa-validation-7482", "mrqa_squad-validation-3330", "mrqa_triviaqa-validation-2803", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-7215", "mrqa_hotpotqa-validation-5541", "mrqa_triviaqa-validation-305"], "retrieved_ids": ["mrqa_naturalquestions-train-79957", "mrqa_naturalquestions-train-21453", "mrqa_naturalquestions-train-39500", "mrqa_naturalquestions-train-80104", "mrqa_naturalquestions-train-85660", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-train-67102", "mrqa_naturalquestions-train-34683", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-train-30142", "mrqa_naturalquestions-train-59672", "mrqa_squad-validation-830", "mrqa_naturalquestions-train-785", "mrqa_naturalquestions-train-9346", "mrqa_triviaqa-validation-6751", "mrqa_naturalquestions-train-35095", "mrqa_naturalquestions-train-32228", "mrqa_naturalquestions-train-7389", "mrqa_naturalquestions-train-50694", "mrqa_naturalquestions-train-72916", "mrqa_naturalquestions-train-78723", "mrqa_naturalquestions-train-61933", "mrqa_naturalquestions-train-46650", "mrqa_naturalquestions-train-47271", "mrqa_naturalquestions-train-32924", "mrqa_naturalquestions-train-23168", "mrqa_naturalquestions-train-47840", "mrqa_naturalquestions-train-33125", "mrqa_naturalquestions-train-86043", "mrqa_naturalquestions-train-3363", "mrqa_naturalquestions-train-43963", "mrqa_naturalquestions-train-64366"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 44, "before_eval": {"predictions": ["Wakanda", "high test scores", "2003", "cricket", "mexico", "campaign setting", "2003", "867 feet (265 m)", "possibly brought from the Byzantine Empire ( as \u039c\u03b1\u03bd\u03bf\u03c5\u03ae\u03bb ) to Spain and Portugal, where it has been used since at least the 13th century", "\"\u00f7\" (2017)", "Christopher Lee as Count Dooku / Darth Tyranus", "8th", "Tikki Tikki Tembo", "in all health care settings", "increased patient health outcomes", "treble clef", "Gabriel Alberto Azucena (born September 23, 1988) who goes by the stage name Gawvi, formerly G-Styles", "12951 / 52 Mumbai Rajdhani Express", "between the Piazza di Spagna at the base and Piazzo Trinit\u00e0 dei Monti", "December 1, 2009", "Estelle Sylvia Pankhurst", "egypt", "philosophical advocate and practitioner of the scientific method", "city of burghs", "The Ministry of Corporate Affairs", "British", "ancient cult activity", "Penguin Classics", "energy-storage molecules", "NASA", "a genuine love of God with heart, soul, mind, and strength", "Todes Banden"], "metric_results": {"EM": 0.03125, "QA-F1": 0.18944579725829724}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.2857142857142857, 0.0, 0.19999999999999998, 0.0, 0.0, 0.4, 0.0, 0.4, 1.0, 0.14285714285714285, 0.7272727272727273, 0.0, 0.0, 0.888888888888889, 0.28571428571428575, 0.0, 0.0, 0.7499999999999999, 0.0, 0.0, 0.8, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6015", "mrqa_squad-validation-2236", "mrqa_naturalquestions-validation-2119", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-6579", "mrqa_hotpotqa-validation-5305", "mrqa_naturalquestions-validation-10619", "mrqa_hotpotqa-validation-910", "mrqa_hotpotqa-validation-4649", "mrqa_naturalquestions-validation-5550", "mrqa_squad-validation-7067", "mrqa_triviaqa-validation-3664", "mrqa_squad-validation-6319", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-2406", "mrqa_hotpotqa-validation-4278", "mrqa_naturalquestions-validation-3459", "mrqa_naturalquestions-validation-8491", "mrqa_naturalquestions-validation-2169", "mrqa_hotpotqa-validation-4794", "mrqa_triviaqa-validation-5022", "mrqa_hotpotqa-validation-389", "mrqa_triviaqa-validation-3617", "mrqa_naturalquestions-validation-6057", "mrqa_hotpotqa-validation-5696", "mrqa_naturalquestions-validation-1725", "mrqa_triviaqa-validation-4027", "mrqa_squad-validation-8625", "mrqa_triviaqa-validation-1504", "mrqa_squad-validation-9951", "mrqa_squad-validation-2419"], "retrieved_ids": ["mrqa_naturalquestions-train-62848", "mrqa_naturalquestions-train-66350", "mrqa_naturalquestions-train-34540", "mrqa_naturalquestions-train-11406", "mrqa_naturalquestions-train-13148", "mrqa_naturalquestions-train-17631", "mrqa_naturalquestions-train-6798", "mrqa_naturalquestions-train-22626", "mrqa_naturalquestions-train-81207", "mrqa_naturalquestions-train-21355", "mrqa_naturalquestions-train-64528", "mrqa_triviaqa-validation-4584", "mrqa_naturalquestions-train-19008", "mrqa_naturalquestions-train-52978", "mrqa_naturalquestions-train-11431", "mrqa_hotpotqa-validation-4164", "mrqa_naturalquestions-train-84647", "mrqa_naturalquestions-train-46587", "mrqa_naturalquestions-train-60014", "mrqa_naturalquestions-train-79454", "mrqa_naturalquestions-train-8116", "mrqa_naturalquestions-train-51965", "mrqa_naturalquestions-train-32089", "mrqa_naturalquestions-train-64504", "mrqa_naturalquestions-train-20263", "mrqa_hotpotqa-validation-4734", "mrqa_naturalquestions-train-23252", "mrqa_naturalquestions-train-83939", "mrqa_naturalquestions-train-80507", "mrqa_naturalquestions-train-36077", "mrqa_naturalquestions-train-76468", "mrqa_naturalquestions-train-16184"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 45, "before_eval": {"predictions": ["Mickey Spillane", "Detroit Lions", "tobacco", "under `` the immortal Hawke ''", "death penalty", "frail Catholic saints", "south and east coast of  southern Africa", "rich Fisher king", "Mangal Pandey of the 34th BNI", "Pannonian troops", "pinto horse", "four", "curling", "2011", "Pebble Beach", "Los Angeles", "French", "Henry Daniel Mills", "\"LOVE Radio\"", "Miami Marlins", "the court", "gypsy", "Donald Henkel", "put an end to Saddam Hussein's occupation of Kuwait", "The Zombies", "Fox News Specialists", "usually scheduled for the Thursday following Labor Day and since 2004, it was hosted by the most recent Super Bowl champions", "Lunar Excursion Module (LEM, later shortened to Lunar Module, LM)", "San Francisco Bay Area at Santa Clara, California", "the official residence of the President of the Russian Federation", "Operation Neptune", "Isthmus of Corinth"], "metric_results": {"EM": 0.21875, "QA-F1": 0.28400713869463867}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7692307692307693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.4166666666666667, 0.16666666666666669, 0.4, 0.18181818181818182, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-2201", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-5587", "mrqa_naturalquestions-validation-4097", "mrqa_squad-validation-9296", "mrqa_triviaqa-validation-6956", "mrqa_naturalquestions-validation-8728", "mrqa_triviaqa-validation-3335", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3363", "mrqa_hotpotqa-validation-5068", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-1433", "mrqa_hotpotqa-validation-5149", "mrqa_triviaqa-validation-455", "mrqa_hotpotqa-validation-866", "mrqa_squad-validation-9513", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-3509", "mrqa_naturalquestions-validation-3217", "mrqa_squad-validation-3845", "mrqa_squad-validation-13", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-2067"], "retrieved_ids": ["mrqa_naturalquestions-train-20273", "mrqa_naturalquestions-train-75621", "mrqa_naturalquestions-train-25843", "mrqa_naturalquestions-train-39154", "mrqa_naturalquestions-validation-368", "mrqa_hotpotqa-validation-1426", "mrqa_naturalquestions-train-41577", "mrqa_naturalquestions-train-73920", "mrqa_naturalquestions-train-51423", "mrqa_naturalquestions-train-40464", "mrqa_squad-validation-7571", "mrqa_naturalquestions-train-1073", "mrqa_naturalquestions-train-85591", "mrqa_naturalquestions-train-55492", "mrqa_naturalquestions-train-51507", "mrqa_triviaqa-validation-3515", "mrqa_naturalquestions-train-24157", "mrqa_naturalquestions-train-16200", "mrqa_naturalquestions-train-29215", "mrqa_naturalquestions-train-2652", "mrqa_naturalquestions-train-49877", "mrqa_naturalquestions-train-35711", "mrqa_naturalquestions-train-37503", "mrqa_naturalquestions-train-81789", "mrqa_naturalquestions-train-14856", "mrqa_naturalquestions-train-45617", "mrqa_naturalquestions-train-33080", "mrqa_triviaqa-validation-1550", "mrqa_naturalquestions-train-43937", "mrqa_naturalquestions-train-53890", "mrqa_naturalquestions-train-24246", "mrqa_triviaqa-validation-6699"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 46, "before_eval": {"predictions": ["baseball", "convection currents in the asthenosphere, which is ductile, or plastic, and the brittle lithosphere ( crust and upper mantle )", "Take That, East 17 and Boyzone", "the youngest publicly documented people to be identified as transgender, and for being the youngest person to become a national transgender figure", "electric lighting", "used their knowledge of Native American languages as a basis to transmit coded messages", "Galileo Galilei and Sir Isaac Newton", "a new theory of electromagnetism", "Premier League club Swansea City", "lily", "Elizabeth Weber", "a first-person psychological horror adventure game", "hundreds of television and radio channels", "\"Waiting for Guffman\"", "2003", "a new facility on The Watermark business park next to the MetroCentre in Gateshead", "pearmain", "The Five Doctors", "a 5% abv pale lager produced by Boon Rawd Brewery", "produced with currently available resources", "Chu'Tsai", "Liz", "least onerous", "lago di Como", "Grissom, White", "multinational retail corporation", "purple", "the ancient scholar Bharata Muni", "The geological properties of a white silica sand found at Basin Head", "pGA", "parts of the air in the vessel were converted into the classical element fire", "the Emperor of Austria"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3625926157176157}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.5, 0.5384615384615384, 1.0, 0.06666666666666667, 0.0, 0.3333333333333333, 0.5714285714285715, 0.0, 0.0, 0.1818181818181818, 0.2857142857142857, 1.0, 1.0, 0.42857142857142855, 0.0, 0.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_naturalquestions-validation-8204", "mrqa_triviaqa-validation-6905", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-5352", "mrqa_squad-validation-10341", "mrqa_squad-validation-10489", "mrqa_hotpotqa-validation-1831", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-3284", "mrqa_hotpotqa-validation-1074", "mrqa_squad-validation-2673", "mrqa_squad-validation-5464", "mrqa_triviaqa-validation-7350", "mrqa_squad-validation-7792", "mrqa_hotpotqa-validation-5239", "mrqa_naturalquestions-validation-2890", "mrqa_triviaqa-validation-2914", "mrqa_squad-validation-3913", "mrqa_triviaqa-validation-1128", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-2808", "mrqa_triviaqa-validation-2873", "mrqa_squad-validation-3523", "mrqa_triviaqa-validation-4430"], "retrieved_ids": ["mrqa_naturalquestions-train-26147", "mrqa_naturalquestions-train-3222", "mrqa_naturalquestions-train-83106", "mrqa_naturalquestions-train-45753", "mrqa_naturalquestions-train-85505", "mrqa_hotpotqa-validation-4015", "mrqa_naturalquestions-train-67521", "mrqa_naturalquestions-train-23760", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-train-87093", "mrqa_naturalquestions-train-15621", "mrqa_naturalquestions-train-86481", "mrqa_naturalquestions-train-6488", "mrqa_naturalquestions-train-18889", "mrqa_naturalquestions-train-49121", "mrqa_naturalquestions-train-29731", "mrqa_naturalquestions-train-72741", "mrqa_naturalquestions-train-19491", "mrqa_naturalquestions-train-16017", "mrqa_naturalquestions-train-23459", "mrqa_naturalquestions-train-72921", "mrqa_naturalquestions-train-54258", "mrqa_naturalquestions-train-22793", "mrqa_naturalquestions-train-46020", "mrqa_naturalquestions-train-32711", "mrqa_naturalquestions-train-74290", "mrqa_squad-validation-10148", "mrqa_naturalquestions-train-8972", "mrqa_naturalquestions-train-80489", "mrqa_naturalquestions-train-75021", "mrqa_naturalquestions-train-51681", "mrqa_naturalquestions-train-49589"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 47, "before_eval": {"predictions": ["millais", "taghrooda", "New Zealand national team", "at the'Lord's Enclosure' (Mongolian: Edsen Khoroo) in Mongolia", "Styal Mill", "William Jennings Bryan", "Milk Barn Animation", "when they enter the army during initial entry training", "moral tale", "they announced a hiatus and re-united two years later for the release of their fourth and final studio album, Destiny Fulfilled ( 2004 )", "marley town council", "A tree - topper or treetopper", "around 74 per cent, or 260 of the 352 votes", "cuba", "often social communities with considerable face-to-face interaction among members", "William Strauss and Neil Howe", "monophyletic", "insecticide toxicology", "catechism questions", "a pH indicator, a color marker, and a dye", "about 50% oxygen composition at standard pressure or 2.5 times the normal sea-level O2 partial pressure of about 21 kPa.", "63,182,000", "John and Charles Wesley", "Science and Discovery", "Euclid's fundamental theorem of arithmetic", "Campbellsville University", "cuba", "appearing as Jude in the musical romance drama film \" Across the Universe\" (2007)", "cuba", "Maria works in a bridal shop with Anita, the girlfriend of her brother, Bernardo", "Jocelyn Flores", "stagnant wages for the working class amidst rising levels of property income for the capitalist class"], "metric_results": {"EM": 0.1875, "QA-F1": 0.30668437921727393}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.1, 0.19999999999999998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.5, 0.5384615384615384, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 0.0, 0.7368421052631579, 0.0, 0.10526315789473682]}}, "error_ids": ["mrqa_triviaqa-validation-2770", "mrqa_triviaqa-validation-945", "mrqa_squad-validation-6287", "mrqa_hotpotqa-validation-5877", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2864", "mrqa_triviaqa-validation-1256", "mrqa_naturalquestions-validation-5305", "mrqa_squad-validation-4212", "mrqa_triviaqa-validation-1516", "mrqa_hotpotqa-validation-5086", "mrqa_naturalquestions-validation-8582", "mrqa_squad-validation-4479", "mrqa_triviaqa-validation-3868", "mrqa_squad-validation-2346", "mrqa_naturalquestions-validation-7849", "mrqa_squad-validation-3685", "mrqa_triviaqa-validation-4298", "mrqa_hotpotqa-validation-2801", "mrqa_squad-validation-9061", "mrqa_triviaqa-validation-1799", "mrqa_hotpotqa-validation-4173", "mrqa_triviaqa-validation-7477", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-2092", "mrqa_squad-validation-7182"], "retrieved_ids": ["mrqa_naturalquestions-train-10991", "mrqa_naturalquestions-train-8492", "mrqa_triviaqa-validation-4090", "mrqa_squad-validation-2577", "mrqa_naturalquestions-train-43372", "mrqa_naturalquestions-train-24328", "mrqa_naturalquestions-train-53456", "mrqa_naturalquestions-train-32608", "mrqa_naturalquestions-train-22037", "mrqa_naturalquestions-train-11270", "mrqa_naturalquestions-train-5260", "mrqa_naturalquestions-train-60893", "mrqa_naturalquestions-train-58421", "mrqa_naturalquestions-train-78480", "mrqa_naturalquestions-train-60555", "mrqa_naturalquestions-train-10828", "mrqa_naturalquestions-train-8970", "mrqa_naturalquestions-train-34565", "mrqa_naturalquestions-train-15497", "mrqa_naturalquestions-train-86209", "mrqa_naturalquestions-train-81419", "mrqa_naturalquestions-train-39092", "mrqa_naturalquestions-train-59094", "mrqa_naturalquestions-train-13046", "mrqa_naturalquestions-train-79514", "mrqa_naturalquestions-train-67807", "mrqa_naturalquestions-train-2596", "mrqa_naturalquestions-train-70528", "mrqa_naturalquestions-train-83761", "mrqa_naturalquestions-train-86968", "mrqa_naturalquestions-train-12004", "mrqa_naturalquestions-train-17277"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 48, "before_eval": {"predictions": ["a heroine who experienced many tragedies, mostly at the hands of her controlling ex-husband, the villainous James Stenbeck ( Anthony Herrera)", "Good Kid, M.A.a.D City", "el Capitan", "Interventive treatment", "3", "Bishop Reuben H. Mueller", "Ray Charles", "During his epic battle with Frieza", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "florida", "morgan", "Thon Maker", "a loop ( also called a self - loop or a `` buckle '' )", "painting", "at the 1964 Republican National Convention in San Francisco, California as a nomination speech for presidential candidate Senator Barry Goldwater", "sin", "annuity", "Padm\u00e9 Amidala", "Frank Theodore `` Ted '' Levine", "justice", "the Viet Minh", "the United States declared neutrality and worked to broker a peace", "half steamed milk and half foam", "morgan", "Hecuba", "a method of imparting the basics of Christianity to the congregations", "Wylie Draper", "political", "George Beadle's office", "Manley MacDonald", "New England Patriots", "famine"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3146394632414369}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [0.21052631578947367, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.22222222222222218, 0.0, 0.0, 1.0, 0.25, 0.2857142857142857, 0.0, 1.0, 0.16666666666666669, 0.0, 0.0, 1.0, 0.0, 0.19999999999999998, 0.0, 0.0, 0.0, 0.0, 0.2666666666666667, 1.0, 0.0, 0.4, 0.0, 0.4]}}, "error_ids": ["mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-5165", "mrqa_triviaqa-validation-3967", "mrqa_squad-validation-5665", "mrqa_triviaqa-validation-3906", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-2607", "mrqa_triviaqa-validation-2849", "mrqa_naturalquestions-validation-2445", "mrqa_squad-validation-8248", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-3789", "mrqa_hotpotqa-validation-4075", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-1649", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-4384", "mrqa_hotpotqa-validation-962", "mrqa_squad-validation-2337", "mrqa_naturalquestions-validation-1531", "mrqa_squad-validation-7947", "mrqa_hotpotqa-validation-599", "mrqa_squad-validation-264", "mrqa_squad-validation-4774"], "retrieved_ids": ["mrqa_naturalquestions-train-74971", "mrqa_naturalquestions-train-55611", "mrqa_naturalquestions-train-1912", "mrqa_naturalquestions-train-49613", "mrqa_naturalquestions-train-54895", "mrqa_naturalquestions-train-11548", "mrqa_naturalquestions-train-42060", "mrqa_squad-validation-9061", "mrqa_naturalquestions-train-76701", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-train-12315", "mrqa_naturalquestions-train-25607", "mrqa_naturalquestions-train-57180", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-train-26156", "mrqa_naturalquestions-train-84762", "mrqa_naturalquestions-train-23905", "mrqa_naturalquestions-train-12929", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-train-31799", "mrqa_naturalquestions-train-4264", "mrqa_naturalquestions-train-45440", "mrqa_naturalquestions-train-75544", "mrqa_naturalquestions-train-27882", "mrqa_naturalquestions-train-16925", "mrqa_naturalquestions-train-829", "mrqa_naturalquestions-validation-2020", "mrqa_naturalquestions-train-326", "mrqa_naturalquestions-train-32965", "mrqa_naturalquestions-train-64300", "mrqa_squad-validation-2862", "mrqa_naturalquestions-train-14629"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}, {"timecode": 49, "before_eval": {"predictions": ["vehicles inspired by the Jeep that are suitable for use on rough terrain", "AOL", "Genghis Khan", "R.E.M.", "between the Eastern Ghats and the Bay of Bengal", "Ravenna", "12", "georgia", "September 1895", "improved", "a biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight until it was flying like a conventional plane", "he had assigned them to the company in lieu of stock", "Georgian-born Soviet revolutionary and political leader", "the Palack\u00fd University, Olomouc ( Friedrich Franz and Johann Karl Nestler ), and his colleagues at the monastery ( such as Franz Diebl ) to study variation in plants", "civil service, common markets for UK goods and services, constitution, electricity, coal, oil, gas, nuclear energy, defence and national security, drug policy, employment, foreign policy and relations with Europe", "Philadelphia Eagles, Denver Broncos, Berlin Thunder, Las Vegas Outlaws and Ottawa Renegades", "3,600", "Albany Schenectady Road", "al - Mamlakah al - \u02bbArab\u012byah as - Su\u02bb\u016bd\u012byah", "44 hectares", "georgia cukor", "king Den", "Kenneth Hood \"Buddy\" MacKay Jr.", "In the episode `` Kobol's Last Gleaming ''", "Wisconsin", "domination or control by a group of people over another", "tea or porridge with bread, chapati, mahamri, boiled sweet potatoes or yams. Ugali with vegetables, sour milk, meat, fish or any other stew", "ATP", "georgia", "Ruth Elizabeth \"Bette\" Davis", "uranium", "7 December 2004"], "metric_results": {"EM": 0.03125, "QA-F1": 0.17074437000907589}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.13333333333333333, 0.0, 0.0, 0.0, 0.18181818181818182, 0.15384615384615385, 0.2222222222222222, 0.0, 0.6666666666666666, 0.6666666666666666, 0.3636363636363636, 0.888888888888889, 0.0, 0.0, 0.0, 0.09523809523809525, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.6470588235294118, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2272", "mrqa_hotpotqa-validation-5452", "mrqa_squad-validation-6257", "mrqa_hotpotqa-validation-2032", "mrqa_naturalquestions-validation-2275", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2941", "mrqa_triviaqa-validation-6073", "mrqa_hotpotqa-validation-987", "mrqa_squad-validation-716", "mrqa_squad-validation-1459", "mrqa_squad-validation-1306", "mrqa_hotpotqa-validation-2810", "mrqa_naturalquestions-validation-3663", "mrqa_squad-validation-9489", "mrqa_hotpotqa-validation-3964", "mrqa_squad-validation-10291", "mrqa_hotpotqa-validation-1023", "mrqa_naturalquestions-validation-9013", "mrqa_hotpotqa-validation-4530", "mrqa_triviaqa-validation-3250", "mrqa_triviaqa-validation-6475", "mrqa_hotpotqa-validation-1315", "mrqa_naturalquestions-validation-5283", "mrqa_squad-validation-7049", "mrqa_squad-validation-9807", "mrqa_squad-validation-8518", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-5876", "mrqa_triviaqa-validation-6380", "mrqa_hotpotqa-validation-2750"], "retrieved_ids": ["mrqa_naturalquestions-train-84177", "mrqa_hotpotqa-validation-3982", "mrqa_naturalquestions-train-14048", "mrqa_naturalquestions-train-38724", "mrqa_naturalquestions-train-2191", "mrqa_naturalquestions-train-60934", "mrqa_naturalquestions-train-17260", "mrqa_hotpotqa-validation-5526", "mrqa_naturalquestions-train-58277", "mrqa_naturalquestions-train-8807", "mrqa_naturalquestions-train-23244", "mrqa_triviaqa-validation-2201", "mrqa_naturalquestions-train-21076", "mrqa_triviaqa-validation-1921", "mrqa_naturalquestions-train-39641", "mrqa_naturalquestions-train-29677", "mrqa_naturalquestions-train-14595", "mrqa_hotpotqa-validation-1897", "mrqa_naturalquestions-train-70162", "mrqa_naturalquestions-train-19025", "mrqa_naturalquestions-train-61641", "mrqa_naturalquestions-train-5321", "mrqa_naturalquestions-train-2165", "mrqa_triviaqa-validation-5091", "mrqa_naturalquestions-train-39614", "mrqa_naturalquestions-train-62680", "mrqa_naturalquestions-train-56048", "mrqa_naturalquestions-train-19536", "mrqa_triviaqa-validation-7415", "mrqa_naturalquestions-train-79169", "mrqa_naturalquestions-train-79113", "mrqa_naturalquestions-train-11571"], "instant_fixing_rate": 0.0, "instant_retention_rate": 0.0}], "final_eval_results": {"overall_oncoming_test": {"EM": 0.14625, "QA-F1": 0.25828961066579925}, "overall_error_number": 1366, "overall_instant_fixing_rate": 0.0, "final_instream_test": {"EM": 0.719375, "QA-F1": 0.7822746298546792}, "final_upstream_test": {"EM": 0.749, "QA-F1": 0.847811427753052}}}