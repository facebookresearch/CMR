{"model_update_steps": 1635, "method_class": "mir", "base_model_args": "Namespace(base_model_path='out/mrqa_naturalquestions_bart-base_0617v4/best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, gradient_accumulation_steps=1, inference_query_size=1, learning_rate=3e-05, local_adapt_lr=3e-05, max_grad_norm=0.1, memory_key_cache_path='na', memory_key_encoder='facebook/bart-base', memory_path='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=largestloss_ckpts/memory_dict.pkl', memory_store_rate=1.0, mir_debug_largestloss=True, mir_debug_reverse=False, num_adapt_epochs=3, num_epochs=5.0, overtime_ckpt_dir='exp_results/dynamic_stream/memory_based/ckpt_dir/0923_MixedAllErrors_T=50_mir_M=I_replaysize=32_upstream=All_meanloss=Yes_mix=Yes_freq=1_candidate=256_seed=42_debug=largestloss_ckpts/', replay_candidate_size=256, replay_frequency=1, replay_size=32, save_all_ckpts=1, total_steps=10000, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, bug_stream_json_path='bug_data/mrqa_naturalquestions_dev.static_bug_stream.json', data_stream_json_path='exp_results/data_streams/mrqa.mixed.data_stream.test.json', do_lowercase=False, max_input_length=888, max_output_length=50, max_timecode=50, num_beams=4, pass_pool_jsonl_path='exp_results/data_streams/mrqa.mixed.hidden_passes.jsonl', predict_batch_size=16, replay_stream_json_path='', sampled_upstream_json_path='data/mrqa_naturalquestions/mrqa_naturalquestions_train.jsonl', task_name='mrqa_naturalquestions', train_batch_size=8, use_sampled_upstream=False)", "online_eval_results": [{"timecode": 0, "before_eval": {"predictions": ["Raymond Briggs' 1978 children's book 'The Snowman", "the acceleration due to gravity decreased as an inverse square law", "a marquetry commode by the \u00e9b\u00e9niste Jean Henri Riesener dated c1780", "a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work", "lymphocyte", "Chinghiz", "Doctor Who and the Daleks in the Seven Keys to Doomsday", "Super Bowl 50 Host Committee", "a satirical television comedy programme on BBC Television in 1962 and 1963", "dynasty", "Br'er Rabbit", "Bodhi Natural Health Products", "Amphitrite Goddess of the Sea", "the Hallertau in Germany ( more hop - growing area than any other country as of 2006 ), the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho", "March 14 ( 3 / 14 in the month / day format ) since 3, 1, and 4 are the first three significant digits of \u03c0", "the head of the arrow or cockerel ( or equivalent depending on the chosen design ) will indicate the direction from which the wind is blowing", "Horace Rumpole", "combustion", "A Sunday Afternoon on the Island of La Grande Jatte", "nobody knows for sure how a do-over in golf came to be called a mulligan", "Captain Meriwether Lewis's 30th birthday", "An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses", "2011", "electric eels", "before the first year begins", "New Jerusalem", "Samantha Spiro", "2013", "Ronnie Biggs", "Steve Carell as Felonious Gru, the former villain turned Anti-Villain League agent, Margo, Edith, and Agnes'adoptive father, and Lucy's husband", "an allusion to Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "the assassination of US President John F. Kennedy"], "metric_results": {"EM": 0.0, "QA-F1": 0.0812722081839729}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.4799999999999999, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.11764705882352942, 0.15384615384615383]}}, "error_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-10322", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-2376", "mrqa_squad-validation-6677", "mrqa_squad-validation-6303", "mrqa_squad-validation-7821", "mrqa_squad-validation-392", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9688", "mrqa_triviaqa-validation-1551", "mrqa_squad-validation-3478", "mrqa_triviaqa-validation-5937", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-2730", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-4185", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3241", "mrqa_triviaqa-validation-7369", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-validation-7017", "mrqa_squad-validation-7746"], "after_eval": {"predictions": ["The Snowman", "at larger distances.", "art and furnishings", "impulse turbine", "the adaptive immune system", "Chinghiz, Chinghis, and Chingiz", "Doctor Who \u2013 The Ultimate Adventure", "50 fund", "ned sherrin", "the Mandate of Heaven", "Tar- Baby", "enlightenment", "Poseidon", "the Yakima ( Washington ) and Willamette ( Oregon ) valleys, and western Canyon County, Idaho ( including the communities of Parma, Wilder, Greenleaf, and Notus )", "2009", "the direction from which the wind is blowing", "john Mortimer", "oxygen", "gees seurat", "golf", "the Missouri River", "An elevator with a counterbalance", "137th", "piranha", "the student's transition from the study of preclinical to clinical health sciences", "prague", "Geoffrey Hutchings", "1998", "john Stratford", "Miranda Cosgrove", "a metaphor for a burden to be carried as penance", "a series of power blackouts across the country"], "metric_results": {"EM": 0.84375, "QA-F1": 0.8944940476190476}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428572, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": [], "fixed_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-10322", "mrqa_squad-validation-5622", "mrqa_triviaqa-validation-2376", "mrqa_squad-validation-6677", "mrqa_squad-validation-6303", "mrqa_squad-validation-7821", "mrqa_squad-validation-392", "mrqa_triviaqa-validation-4054", "mrqa_naturalquestions-validation-6157", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-671", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-9688", "mrqa_triviaqa-validation-1551", "mrqa_squad-validation-3478", "mrqa_triviaqa-validation-5972", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-2730", "mrqa_hotpotqa-validation-409", "mrqa_squad-validation-4185", "mrqa_triviaqa-validation-6351", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3241", "mrqa_naturalquestions-validation-3490", "mrqa_naturalquestions-validation-7017", "mrqa_squad-validation-7746"], "unfixed_ids": ["mrqa_triviaqa-validation-3915", "mrqa_naturalquestions-validation-2248", "mrqa_triviaqa-validation-5937", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-7369"], "instant_fixing_rate": 0.84375, "instant_retention_rate": 0.0}, {"timecode": 1, "before_eval": {"predictions": ["the town of Acolman, just north of Mexico City", "The Iroquois", "a natural extension of capitalism that arose from need for capitalist economies to constantly expand investment, material resources and manpower in such a way that necessitated colonial expansion", "ned", "Virginia Wade", "Gary Morris", "the anterolateral system", "1966", "for scientific observation", "john Cameron", "The Stock Market crash in New York", "New York Stadium", "john Bercow", "continental units", "john Forster", "Comptroller General of the Receipt and Issue of Her Majesty's Exchequer", "Sergio P\u00e9rez", "River Welland", "The Concubine", "1543", "naba", "Vigor", "a policeman who investigates a series of mysterious killings and illnesses", "light bulbs within 100 feet of the lab glowed even when turned off", "Florence Nightingale", "Budweiser", "numb3rs", "acmthompson", "Michael Douglas", "Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object", "Superman", "2010"], "metric_results": {"EM": 0.1875, "QA-F1": 0.23243026224770647}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true], "QA-F1": [0.0, 0.0, 0.06896551724137931, 0.0, 0.0, 1.0, 0.0, 0.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19999999999999998, 0.5882352941176471, 0.0, 0.0, 1.0, 0.0, 1.0, 0.07407407407407407, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-5144", "mrqa_squad-validation-10015", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-5406", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-1924", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-validation-1364", "mrqa_hotpotqa-validation-14", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-3971", "mrqa_squad-validation-2629", "mrqa_triviaqa-validation-2210", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-5325", "mrqa_squad-validation-1516", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-5802", "mrqa_triviaqa-validation-2096", "mrqa_squad-validation-10410"], "after_eval": {"predictions": ["from the Italian pignatta", "originally designated HU - 1", "philanthropy", "mariette", "sue Barker and Joe Durie", "Gary Morris", "usually occurs 1 - 2 spinal nerve segments above the point of entry", "current denomination of U.S. currency", "either small fission systems or radioactive decay for electricity or heat", "ronseal", "1929 - 32", "Rotherham United", "tye tebbit", "Pangaea or Pangea", "red", "The comptroller ( who is also auditor general and head of the National Audit Office )", "Pedro Rodr\u00edguez", "65 mi", "an obsessed and tormented king", "three years before his death", "islam", "Honda Ballade", "illnesses", "glowed", "Sister Anthony, S.C.", "Stroh's", "rob Morrow", "gallantry", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Galileo", "Superman", "May and June 2010"], "metric_results": {"EM": 0.75, "QA-F1": 0.8307043650793651}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4444444444444445, 1.0, 1.0, 0.4]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-4684", "before_prediction": "The Stock Market crash in New York", "after_prediction": "1929 - 32"}, {"id": "mrqa_triviaqa-validation-338", "before_prediction": "numb3rs", "after_prediction": "rob Morrow"}, {"id": "mrqa_naturalquestions-validation-1864", "before_prediction": "Michael Douglas", "after_prediction": "Michael Douglas, Kathleen Turner, and Danny DeVito"}, {"id": "mrqa_hotpotqa-validation-3774", "before_prediction": "2010", "after_prediction": "May and June 2010"}], "retained_ids": ["mrqa_hotpotqa-validation-5899", "mrqa_triviaqa-validation-2367"], "fixed_ids": ["mrqa_naturalquestions-validation-5144", "mrqa_squad-validation-10015", "mrqa_triviaqa-validation-7018", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3828", "mrqa_triviaqa-validation-5026", "mrqa_triviaqa-validation-4856", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1437", "mrqa_naturalquestions-validation-1364", "mrqa_hotpotqa-validation-14", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-3971", "mrqa_squad-validation-2629", "mrqa_triviaqa-validation-2210", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-5325", "mrqa_squad-validation-1516", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-5802", "mrqa_squad-validation-10410"], "unfixed_ids": ["mrqa_naturalquestions-validation-10680", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-2096"], "instant_fixing_rate": 0.8461538461538461, "instant_retention_rate": 0.33333333277777777}, {"timecode": 2, "before_eval": {"predictions": ["Kronprins Harald", "rugby union", "Puritanism", "+, -, *, and / keys", "2009", "in different parts of the globe", "Wales", "acetic acid", "Jan Kazimierz", "darnley", "A55 North Wales Expressway", "a phylum with relatively few species", "`` Everywhere '' is a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "the United States, which is believed to be the first country in which the majority of the population lives in the suburbs", "student in the second year at a high school or college", "Bothtec", "Terry Reid", "non- peer- reviewed sources", "Elgar", "North America", "Andr\u00e9 3000", "Lovell, Jack Swigert, and Fred Haise", "Akhenaten", "President Theodore Roosevelt", "the fourth season", "Denver Broncos", "the Western Bloc ( the United States, its NATO allies and others )", "the 1970s", "Georges Bizet", "Matt Winer", "1689", "Pacific across the Amazonas Basin"], "metric_results": {"EM": 0.0625, "QA-F1": 0.25316176878676877}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 1.0, 0.0, 0.4, 0.28571428571428575, 0.1, 0.19999999999999998, 0.19999999999999998, 1.0, 0.0, 0.3333333333333333, 0.0, 0.4444444444444445, 0.4, 0.9090909090909091, 0.0, 0.4, 0.0, 0.0, 0.3636363636363636, 0.0, 0.6666666666666666, 0.4, 0.0, 0.4]}}, "error_ids": ["mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-8542", "mrqa_squad-validation-7149", "mrqa_naturalquestions-validation-10364", "mrqa_squad-validation-4253", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-893", "mrqa_naturalquestions-validation-1202", "mrqa_triviaqa-validation-1935", "mrqa_hotpotqa-validation-1888", "mrqa_squad-validation-4456", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-8448", "mrqa_triviaqa-validation-2136", "mrqa_naturalquestions-validation-539", "mrqa_squad-validation-8513", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-validation-5502", "mrqa_hotpotqa-validation-2679", "mrqa_squad-validation-4019", "mrqa_naturalquestions-validation-6896", "mrqa_naturalquestions-validation-2501", "mrqa_hotpotqa-validation-1376", "mrqa_squad-validation-194", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-683", "mrqa_triviaqa-validation-2722", "mrqa_hotpotqa-validation-4367", "mrqa_squad-validation-3113", "mrqa_squad-validation-4283"], "after_eval": {"predictions": ["motor ships", "cricket, rallying, football, rugby union and boxing", "never", "*", "2003", "eleven separate regions of the Old and New World", "idris the Dragon", "polyatomic anion", "Jan Kazimierz", "casketrothals", "A55", "oceanic species", "Everywhere", "Many residents of metropolitan regions work within the central urban area, and choose to live in satellite communities called suburbs and commute to work", "second", "bakusou Buggy Ippatsu Yarou", "Spencer Davis Group", "non-peer-reviewed sources", "Enigma\u2019 Variations", "physiographically a part of the continent of North America", "OutKast", "Jack Swigert, and Fred Haise", "the Aten, a representation of the Egyptian god, Ra", "Monroe Doctrine", "2010 to 2012", "four", "the United States", "in the very late 1980s", "bizet", "Matthew Ward Winer", "1700", "the Pacific"], "metric_results": {"EM": 0.875, "QA-F1": 0.8645833333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-3242", "before_prediction": "Bothtec", "after_prediction": "bakusou Buggy Ippatsu Yarou"}], "retained_ids": ["mrqa_hotpotqa-validation-3632"], "fixed_ids": ["mrqa_hotpotqa-validation-5699", "mrqa_squad-validation-8542", "mrqa_squad-validation-7149", "mrqa_naturalquestions-validation-10364", "mrqa_squad-validation-4253", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-1202", "mrqa_hotpotqa-validation-1888", "mrqa_squad-validation-4456", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-8448", "mrqa_triviaqa-validation-2136", "mrqa_naturalquestions-validation-539", "mrqa_squad-validation-8513", "mrqa_triviaqa-validation-4729", "mrqa_naturalquestions-validation-5502", "mrqa_squad-validation-4019", "mrqa_naturalquestions-validation-6896", "mrqa_naturalquestions-validation-2501", "mrqa_hotpotqa-validation-1376", "mrqa_squad-validation-194", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-683", "mrqa_triviaqa-validation-2722", "mrqa_hotpotqa-validation-4367", "mrqa_squad-validation-3113", "mrqa_squad-validation-4283"], "unfixed_ids": ["mrqa_triviaqa-validation-893", "mrqa_triviaqa-validation-1935", "mrqa_hotpotqa-validation-2679"], "instant_fixing_rate": 0.9, "instant_retention_rate": 0.4999999975}, {"timecode": 3, "before_eval": {"predictions": ["us", "four-year plan", "parallelogram", "between 27 July and 7 August 2022", "New York", "branet maslin", "2005\u20132010", "various International Contact Groups", "usually restricted to the lower motor neurons, the efferent nerves that directly innervate muscles", "babbage", "sugar confectionery", "epson Derby", "a death mask", "video film", "Overtime", "Sir Henry Cole", "trouble distinguishing between carbon dioxide and oxygen", "idney", "cement City, Texas", "the Democratic Unionist Party (DUP )", "23 July 1989", "many educational institutions especially within the US", "many traditions of Hinduism - especially those common in the West", "for control purposes", "bran Ballard", "callable bonds", "2.26 GHz quad - core Snapdragon 800 processor", "over 10,000 British and 2,000 old master works", "al - khimar", "proteins", "gallbladder", "berenice Abbott"], "metric_results": {"EM": 0.15625, "QA-F1": 0.21333411654135337}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.10526315789473685, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.19999999999999998, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-5362", "mrqa_naturalquestions-validation-5647", "mrqa_squad-validation-3058", "mrqa_triviaqa-validation-7382", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-2571", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2327", "mrqa_triviaqa-validation-365", "mrqa_squad-validation-1539", "mrqa_triviaqa-validation-3901", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-3189", "mrqa_squad-validation-8832", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-6683", "mrqa_hotpotqa-validation-2150", "mrqa_squad-validation-2191", "mrqa_squad-validation-2069", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-6402", "mrqa_squad-validation-5517", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-5818", "mrqa_triviaqa-validation-2530"], "after_eval": {"predictions": ["period", "qpr", "360", "2022", "Staten Island", "splash", "2005", "G20", "efferent nerves", "max bygraves", "polar bear", "lester piggott", "the \"do not disturb\" sign", "nigeria", "the shootout", "commissioned by Sir Henry Cole and illustrated by John Callcott Horsley in London on 1st May 1843", "at high oxygen concentrations, rubisco starts accidentally adding oxygen to sugar precursors", "adrian edmondson", "birmingham", "Northern Ireland Assembly for Fermanagh and South Tyrone", "23 July 1989", "US", "the West", "data", "ringo starr", "Callability", "Snapdragon 800", "over 10,000", "hijab", "The results of the Avery -- MacLeod -- McCarty experiment", "gallbladder", "museum"], "metric_results": {"EM": 0.875, "QA-F1": 0.8888888888888888}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-7816", "before_prediction": "2005\u20132010", "after_prediction": "2005"}, {"id": "mrqa_naturalquestions-validation-2385", "before_prediction": "callable bonds", "after_prediction": "Callability"}, {"id": "mrqa_naturalquestions-validation-6341", "before_prediction": "2.26 GHz quad - core Snapdragon 800 processor", "after_prediction": "Snapdragon 800"}], "retained_ids": ["mrqa_hotpotqa-validation-5662", "mrqa_triviaqa-validation-6800"], "fixed_ids": ["mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-5362", "mrqa_naturalquestions-validation-5647", "mrqa_squad-validation-3058", "mrqa_triviaqa-validation-7382", "mrqa_hotpotqa-validation-4825", "mrqa_naturalquestions-validation-2571", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2327", "mrqa_triviaqa-validation-365", "mrqa_squad-validation-1539", "mrqa_triviaqa-validation-3901", "mrqa_hotpotqa-validation-2263", "mrqa_naturalquestions-validation-3189", "mrqa_squad-validation-8832", "mrqa_triviaqa-validation-3339", "mrqa_hotpotqa-validation-2150", "mrqa_squad-validation-2191", "mrqa_squad-validation-2069", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-6402", "mrqa_squad-validation-5517", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-5818", "mrqa_triviaqa-validation-2530"], "unfixed_ids": ["mrqa_triviaqa-validation-6683"], "instant_fixing_rate": 0.9629629629629629, "instant_retention_rate": 0.3999999992}, {"timecode": 4, "before_eval": {"predictions": ["Austria", "at Nijmegen, over the Waal distributary of the Rhine", "December 9, 2016", "NASA discontinued the manned Block I program", "British progressive folk-rock band Gryphon", "1898", "maius", "biblical Moses", "museum", "tetanus", "bounding the time or space used by the algorithm", "diamond", "Lieutenant Commander Steve McGarrett", "Eddie Leonski", "Ridley", "a mixture of phencyclidine and cocaine", "a chain or screw stoking mechanism and its drive engine or motor", "Demon Barber of Fleet Street", "Reverse - Flash", "Celtic festival of the dead", "baseball", "baku", "new converts", "Mona Vanderwaal", "cricket", "Pyotr Ilyich Tchaikovsky", "2001 or 2010", "the English colonies of North America, and Quebec", "speech or language", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "propeller", "Splodgenessabounds"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3242559523809524}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.2857142857142857, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.6666666666666666, 0.4, 0.0, 0.33333333333333337, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.28571428571428575, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-9358", "mrqa_naturalquestions-validation-5146", "mrqa_squad-validation-3971", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-3647", "mrqa_squad-validation-1688", "mrqa_triviaqa-validation-3808", "mrqa_hotpotqa-validation-1168", "mrqa_hotpotqa-validation-1289", "mrqa_hotpotqa-validation-2944", "mrqa_naturalquestions-validation-5437", "mrqa_squad-validation-3389", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-5654", "mrqa_naturalquestions-validation-2900", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-3978", "mrqa_squad-validation-3021", "mrqa_naturalquestions-validation-3840", "mrqa_squad-validation-3467"], "after_eval": {"predictions": ["Austria", "Arnhem", "30 days after the original air date", "discontinued", "Gryphon", "1898", "june", "aaron", "new york city", "a rare but often fatal disease that affects the central nervous system by causing painful muscular contractions", "bounding", "facets", "Steve McGarrett", "Edward Joseph Leonski", "Lynwood", "phencyclidine and cocaine", "bunker", "Fleet Street", "Professor Eobard Thawne", "All Saints ( or All Hallows )", "kansas city", "detroit", "new converts", "CeCe Drake", "cricket", "Tania Miller", "8 April 1912", "Quebec", "comprehend and formulate language", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes (MBH99 )", "rotating discs", "Splodgenessabounds"], "metric_results": {"EM": 0.875, "QA-F1": 0.9203869047619048}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9523809523809523, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-7253", "before_prediction": "tetanus", "after_prediction": "a rare but often fatal disease that affects the central nervous system by causing painful muscular contractions"}, {"id": "mrqa_squad-validation-8700", "before_prediction": "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "after_prediction": "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes (MBH99 )"}], "retained_ids": ["mrqa_squad-validation-6399", "mrqa_naturalquestions-validation-1277", "mrqa_squad-validation-3126", "mrqa_triviaqa-validation-5168", "mrqa_naturalquestions-validation-220"], "fixed_ids": ["mrqa_squad-validation-9358", "mrqa_naturalquestions-validation-5146", "mrqa_squad-validation-3971", "mrqa_squad-validation-5360", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-3647", "mrqa_squad-validation-1688", "mrqa_triviaqa-validation-3808", "mrqa_hotpotqa-validation-1168", "mrqa_hotpotqa-validation-1289", "mrqa_hotpotqa-validation-2944", "mrqa_squad-validation-3389", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-8545", "mrqa_triviaqa-validation-93", "mrqa_naturalquestions-validation-2900", "mrqa_hotpotqa-validation-4734", "mrqa_hotpotqa-validation-3978", "mrqa_squad-validation-3021", "mrqa_naturalquestions-validation-3840", "mrqa_squad-validation-3467"], "unfixed_ids": ["mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-5654"], "instant_fixing_rate": 0.92, "instant_retention_rate": 0.7142857132653061}, {"timecode": 5, "before_eval": {"predictions": ["london", "F\u00e9d\u00e9ration Sportive F\u00e9minine Internationale (FSFI)", "wool", "Paspahegh Indians", "heart's conduction system", "South Dakota", "2 : 44 p.m. EDT", "swanee or swannee whistle", "detroit", "used stone tools", "India", "gastric glands found in the lining of the fundus and in the body of the stomach", "placental", "September 13, 1994", "detroit", "Ming dynasty ( 1368 -- 1644 )", "1840", "make a defiant speech, or a speech explaining their actions", "Francis Marion Crawford, Robert Underwood Johnson, Stanford White, Fritz Lowenstein, George Scherff, and Kenneth Swezey", "kinks", "on the basis of the methodology used: by using net wealth (adding up assets and subtracting debts )", "entropy increases", "my mind is averse to wedlock because I daily expect the death of a heretic", "8.7 -- 9.2", "China", "2 November 1902", "southeastern Texas", "May 7, 2018", "9 October 1940", "Brookhaven", "structural collapses", "as a way of housing a fierce half-man, half-bull creature known as the Minotaur"], "metric_results": {"EM": 0.125, "QA-F1": 0.20703044375644994}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.4210526315789474, 0.0, 0.47058823529411764, 0.0, 1.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-4725", "mrqa_hotpotqa-validation-2197", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-2020", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3625", "mrqa_triviaqa-validation-1550", "mrqa_triviaqa-validation-5704", "mrqa_naturalquestions-validation-816", "mrqa_hotpotqa-validation-4621", "mrqa_squad-validation-6733", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-254", "mrqa_squad-validation-7554", "mrqa_squad-validation-10423", "mrqa_squad-validation-2757", "mrqa_naturalquestions-validation-2884", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-1489", "mrqa_hotpotqa-validation-5651", "mrqa_squad-validation-2531", "mrqa_triviaqa-validation-4560"], "after_eval": {"predictions": ["horace walpole", "International Association of Athletics Federations", "paisley", "uninhabited", "heart", "Idaho", "6 : 44 p.m. UTC", "flute", "mustard", "bury their dead", "Indian", "stomach", "monotreme", "September 13, 1994", "President Garfield", "imperial rule", "1787", "defiant speech", "Mark Twain", "sunny afternoon", "the methodology used", "nonconservative forces", "the death of a heretic", "The 1700 Cascadia earthquake", "People's Republic of China", "11 November 1869", "near Arenosa Creek and Matagorda Bay", "January 15, 2018", "19408", "Selden and Farmingville", "property damage", "daedalus"], "metric_results": {"EM": 0.84375, "QA-F1": 0.8818910256410256}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-4513", "before_prediction": "gastric glands found in the lining of the fundus and in the body of the stomach", "after_prediction": "stomach"}, {"id": "mrqa_hotpotqa-validation-1718", "before_prediction": "China", "after_prediction": "People's Republic of China"}, {"id": "mrqa_hotpotqa-validation-5848", "before_prediction": "Brookhaven", "after_prediction": "Selden and Farmingville"}], "retained_ids": ["mrqa_hotpotqa-validation-484"], "fixed_ids": ["mrqa_triviaqa-validation-4725", "mrqa_hotpotqa-validation-2197", "mrqa_triviaqa-validation-2368", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-3074", "mrqa_naturalquestions-validation-4506", "mrqa_triviaqa-validation-1540", "mrqa_naturalquestions-validation-6736", "mrqa_hotpotqa-validation-3625", "mrqa_triviaqa-validation-1550", "mrqa_triviaqa-validation-5704", "mrqa_naturalquestions-validation-816", "mrqa_hotpotqa-validation-4621", "mrqa_squad-validation-6733", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-254", "mrqa_squad-validation-7554", "mrqa_squad-validation-10423", "mrqa_squad-validation-2757", "mrqa_naturalquestions-validation-2884", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-1489", "mrqa_hotpotqa-validation-5651", "mrqa_squad-validation-2531", "mrqa_triviaqa-validation-4560"], "unfixed_ids": ["mrqa_naturalquestions-validation-2020", "mrqa_triviaqa-validation-1494"], "instant_fixing_rate": 0.9285714285714286, "instant_retention_rate": 0.249999999375}, {"timecode": 6, "before_eval": {"predictions": ["zinnemann", "to prevent the flame from being blown out", "2005 to 2008", "1998", "n Carolina", "belle belle", "90-60's", "independent schools", "The Babe's Last Game", "times sign", "Best Supporting Actress", "Juice Newton's", "1960", "HTTP Secure ( HTTPS )", "late - September through early January", "kansas", "monatomic", "popular for its resort feel and nearby open spaces", "japan", "toad", "belfast", "afghanistan", "left coronary artery", "1.1 \u00d7 1011 metric tonnes", "belle belle", "leaf tissue", "Indian club ATK", "land that a nation has conquered and expanded", "Indian Ocean", "lakh", "Norwegian language", "burning of fossil fuels"], "metric_results": {"EM": 0.21875, "QA-F1": 0.29254807692307694}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, true, false], "QA-F1": [0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.4, 1.0, 0.0, 0.0, 0.46153846153846156, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-4165", "mrqa_hotpotqa-validation-5810", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-6887", "mrqa_triviaqa-validation-5261", "mrqa_squad-validation-2659", "mrqa_triviaqa-validation-105", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-484", "mrqa_triviaqa-validation-5795", "mrqa_squad-validation-3463", "mrqa_squad-validation-2584", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-227", "mrqa_squad-validation-4181", "mrqa_triviaqa-validation-4966", "mrqa_squad-validation-8821", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-10042", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6644"], "after_eval": {"predictions": ["film directors", "capillary action", "2008", "2010", "north carolina", "menorca", "70", "independent schools", "boston braves", "the symbol \u00d7", "Best Supporting Actress nomination for her work as Missy", "Juice Newton", "Super Bowl LII,", "Hypertext Transfer Protocol ( HTTP ) for secure communication", "late summer", "wichita", "simplest", "beaches", "japan", "true", "red admiral", "o", "The anterior interventricular branch of left coronary artery, ( also left anterior descending artery ( LAD ), or anterior descending branch )", "10 %", "borrowdale", "concentrated in the leaves", "Republic of Ireland national team", "distinction", "23 November 1996", "lakh", "the Norwegian language", "Carbon dioxide"], "metric_results": {"EM": 0.875, "QA-F1": 0.9089285714285714}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-7819", "before_prediction": "Best Supporting Actress", "after_prediction": "Best Supporting Actress nomination for her work as Missy"}, {"id": "mrqa_naturalquestions-validation-226", "before_prediction": "late - September through early January", "after_prediction": "late summer"}, {"id": "mrqa_naturalquestions-validation-5582", "before_prediction": "left coronary artery", "after_prediction": "The anterior interventricular branch of left coronary artery, ( also left anterior descending artery ( LAD ), or anterior descending branch )"}], "retained_ids": ["mrqa_squad-validation-6947", "mrqa_triviaqa-validation-3714", "mrqa_naturalquestions-validation-6207", "mrqa_hotpotqa-validation-3919"], "fixed_ids": ["mrqa_triviaqa-validation-2465", "mrqa_naturalquestions-validation-4165", "mrqa_hotpotqa-validation-5810", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-6887", "mrqa_triviaqa-validation-5261", "mrqa_squad-validation-2659", "mrqa_triviaqa-validation-105", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-484", "mrqa_triviaqa-validation-5795", "mrqa_squad-validation-3463", "mrqa_squad-validation-2584", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-227", "mrqa_squad-validation-4181", "mrqa_squad-validation-8821", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-10042", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6644"], "unfixed_ids": ["mrqa_triviaqa-validation-4966"], "instant_fixing_rate": 0.96, "instant_retention_rate": 0.571428570612245}, {"timecode": 7, "before_eval": {"predictions": ["the wisdom and prudence of certain decisions of procurement", "The Q'eqchi '", "a few common complex biomolecules, such as squalene and the carotenes", "The U.S. Army Chaplain insignia is the only authorized army branch insignia", "Kairi", "high schools lost their accreditation", "lost weekend", "the Caucasus and into Russia", "the last book accepted into the Christian biblical canon", "Beyonc\u00e9", "% IACS conductivity values", "gallantry", "high viewership levels for the evening on which the episode is broadcast", "the late 1950s", "work oxen for haulage", "2011", "a priest", "most abundant", "2001", "family member", "over-fishing and long-term environmental changes", "Bill Lear", "the unbalanced centripetal force felt by any object", "Terrell Suggs", "decide on all the motions and amendments that have been moved that day", "a voyage of adventure", "Abraham Gottlob Werner ( 1749 -- 1817 )", "geena", "the Charleston Orange district", "the Brotherhood's base of devout middle class found common cause with the impoverished youth of the intifada in their cultural conservatism and antipathy for activities of the secular middle class such as drinking alcohol and going about without hijab", "Adam Karpel, Alex Baskin, Douglas Ross, Gregory Stewart, Scott Dunlop, Stephanie Boyriven and Andy Cohen", "Panzer"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4444809173669468}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.19999999999999998, 0.0, 0.25, 1.0, 1.0, 0.4, 0.4444444444444445, 0.4, 0.5, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.23529411764705882, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-5522", "mrqa_squad-validation-3625", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-983", "mrqa_squad-validation-6297", "mrqa_naturalquestions-validation-859", "mrqa_squad-validation-108", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-1293", "mrqa_squad-validation-7799", "mrqa_hotpotqa-validation-3898", "mrqa_naturalquestions-validation-98", "mrqa_hotpotqa-validation-1142", "mrqa_squad-validation-10403", "mrqa_naturalquestions-validation-3474", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-291", "mrqa_triviaqa-validation-781", "mrqa_squad-validation-9598", "mrqa_hotpotqa-validation-2902"], "after_eval": {"predictions": ["the wisdom and prudence of certain decisions of procurement", "K'iche '", "Only a few", "the right", "Kairi in the video game series \" Kingdom Hearts\".", "high schools lost their accreditation", "lost weekend", "Russia", "last book", "Beyonc\u00e9 and Bruno Mars", "conductivity", "george vi", "the most popular show at the time", "post\u2013World War II", "breeding", "2011", "a priest", "most abundant", "2001", "family member", "long-term environmental changes", "8-track", "tangential force", "Mike Czerwien, Waynesburg University, 2002 -- 04", "vote", "a maritime signal, indicating that the vessel flying it is about to leave", "James Hutton", "george i", "Charleston Orange district", "quiescent", "Andy Cohen", "German general (colonel-general from 1940 ) during World War II"], "metric_results": {"EM": 0.96875, "QA-F1": 0.96875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-3146", "before_prediction": "Panzer", "after_prediction": "German general (colonel-general from 1940 ) during World War II"}], "retained_ids": ["mrqa_squad-validation-8374", "mrqa_squad-validation-7296", "mrqa_triviaqa-validation-7415", "mrqa_squad-validation-358", "mrqa_hotpotqa-validation-3846", "mrqa_squad-validation-3558", "mrqa_naturalquestions-validation-824", "mrqa_squad-validation-1863", "mrqa_squad-validation-4318", "mrqa_squad-validation-3181"], "fixed_ids": ["mrqa_naturalquestions-validation-5522", "mrqa_squad-validation-3625", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-983", "mrqa_squad-validation-6297", "mrqa_naturalquestions-validation-859", "mrqa_squad-validation-108", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-1293", "mrqa_squad-validation-7799", "mrqa_hotpotqa-validation-3898", "mrqa_naturalquestions-validation-98", "mrqa_hotpotqa-validation-1142", "mrqa_squad-validation-10403", "mrqa_naturalquestions-validation-3474", "mrqa_squad-validation-9281", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-291", "mrqa_triviaqa-validation-781", "mrqa_squad-validation-9598", "mrqa_hotpotqa-validation-2902"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9090909082644627}, {"timecode": 8, "before_eval": {"predictions": ["theoretical computer science", "tipo", "37.7", "6.4 nanometers apart", "the eighth and eleventh episodes of the season", "Carl Michael Edwards II", "over 400 games", "cortisol", "Latin liberalia studia", "Forest of Bowland in Lancashire", "Edward V, King of England and Richard of Shrewsbury, Duke of York", "St. Louis County", "1918", "2018", "cricket club", "law firm", "Pottawatomie County", "orangutan", "Einstein's theory of general relativity (GR )", "The church tower", "nightclub", "Toronto", "wales", "110 miles (177 km ) from the East River in New York City, along the North Shore of Long Island, to the south", "Kona coast of the island of Hawai\u02bb i about 12 mi south of Kailua-Kona", "Liberal conservatism", "largest gold rushes the world has ever seen", "six degrees of freedom", "not guilty", "psychotherapeutic", "Quentin Coldwater", "acidic"], "metric_results": {"EM": 0.09375, "QA-F1": 0.2386149440836941}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.5, 0.0, 0.0, 0.25, 0.375, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.9090909090909091, 1.0, 0.0, 0.0, 0.0, 0.1904761904761905, 0.2666666666666667, 0.5, 0.4444444444444445, 0.4, 0.0, 0.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-1705", "mrqa_triviaqa-validation-1521", "mrqa_naturalquestions-validation-6089", "mrqa_squad-validation-8869", "mrqa_naturalquestions-validation-856", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-3789", "mrqa_triviaqa-validation-7506", "mrqa_naturalquestions-validation-1360", "mrqa_hotpotqa-validation-3161", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5401", "mrqa_triviaqa-validation-7157", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-validation-3309", "mrqa_squad-validation-10369", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-582", "mrqa_triviaqa-validation-4268", "mrqa_hotpotqa-validation-1021", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-4812", "mrqa_squad-validation-2987", "mrqa_naturalquestions-validation-7906", "mrqa_squad-validation-6915", "mrqa_triviaqa-validation-7767", "mrqa_hotpotqa-validation-1032"], "after_eval": {"predictions": ["computability theory", "formula one", "won gold in the half - pipe", "6.4 nanometers", "departing the show to star in CBS's upcoming sci - fi drama Intelligence", "Kyle Busch", "400", "kidneys", "artes liberales", "\"Switzerland of England\"", "Edward IV of England", "Eureka", "1828", "2018", "blades", "to ensure wide visibility and understanding of cases in a region", "Shawnee", "tortoise", "theory of general relativity (GR)", "The church tower", "walford east", "Montreal", "slow", "110 miles", "Kona coast", "liberal conservative", "gold rushes", "six", "creative plea", "sigmund freudians", "New York", "acidic"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9802989130434783}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": ["mrqa_naturalquestions-validation-6991", "mrqa_squad-validation-5313", "mrqa_hotpotqa-validation-187"], "fixed_ids": ["mrqa_squad-validation-1705", "mrqa_triviaqa-validation-1521", "mrqa_naturalquestions-validation-6089", "mrqa_squad-validation-8869", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-3789", "mrqa_triviaqa-validation-7506", "mrqa_naturalquestions-validation-1360", "mrqa_hotpotqa-validation-3161", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5401", "mrqa_triviaqa-validation-7157", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1934", "mrqa_naturalquestions-validation-3309", "mrqa_squad-validation-10369", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-582", "mrqa_triviaqa-validation-4268", "mrqa_hotpotqa-validation-1021", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-4812", "mrqa_squad-validation-2987", "mrqa_naturalquestions-validation-7906", "mrqa_squad-validation-6915", "mrqa_hotpotqa-validation-1032"], "unfixed_ids": ["mrqa_naturalquestions-validation-856", "mrqa_triviaqa-validation-7767"], "instant_fixing_rate": 0.9310344827586207, "instant_retention_rate": 0.9999999966666667}, {"timecode": 9, "before_eval": {"predictions": ["the English phrase `` I Seek You ''", "Argentinian", "a report, published in early February 2007 by the Ear Institute at the University College London, and Widex, a Danish hearing aid manufacturer", "almond paste", "This process reduces the efficiency of photosynthesis", "to celebrate Queen Victoria's diamond jubilee", "may not have ever set foot in the White House", "The Daily Stormer", "antibonding", "water", "president", "the citizens", "George, Margrave of Brandenburg-Ansbach", "a very precise notation of a correct African pronunciation", "3D computer-animated comedy film", "It was listed on the National Register of Historic Places in 1980", "acting", "C. W. Grafton", "LED illuminated display", "Americans", "iPod+HP", "My Sassy Girl", "prevent damage to the body", "The Edge of Night", "Non-combustible substances that corrode, such as iron, contained very little", "field trips", "\u039c\u03ad\u03b3\u03b1\u03bd \u0399\u03b5\u03c1\u03cc\u03bd \u03a3\u03c5\u03bd\u03ad\u03ba\u03b4\u03b7\u03bc\u03bf\u03bd ) book of prayers", "the root respiration", "land - living organisms, both alive and dead, as well as carbon stored in soils", "sherryn Tolhurst on guitar", "medium and heavy- Duty diesel trucks", "seminal vesicles"], "metric_results": {"EM": 0.3125, "QA-F1": 0.45800749495602433}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.1, 0.0, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.13333333333333333, 0.4, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 0.2857142857142857, 0.4, 1.0, 0.3333333333333333, 0.15384615384615383, 1.0, 0.0, 1.0, 0.0, 0.0, 0.23529411764705882, 0.0, 0.7272727272727272, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6844", "mrqa_squad-validation-5210", "mrqa_triviaqa-validation-3387", "mrqa_squad-validation-8924", "mrqa_squad-validation-5618", "mrqa_triviaqa-validation-2703", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-321", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-5128", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-1327", "mrqa_hotpotqa-validation-1436", "mrqa_naturalquestions-validation-1294", "mrqa_squad-validation-3490", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-5823", "mrqa_naturalquestions-validation-3677"], "after_eval": {"predictions": ["an instant messaging client", "Argentinian", "a report", "cake", "consumes ATP and oxygen, releases CO2, and produces no sugar", "animals and humans performimg various actions", "1600 Pennsylvania Avenue", "\"The Krypto Report\" a podcast produced by the white supremacist site \"The Daily Stormer\"", "triplet electronic ground state", "shortcrust pastry ; however, more recently recipes have recommended a paste consisting of flour and water", "the President of the United States", "citizens", "George, Margrave of Brandenburg-Ansbach", "a very precise notation of a correct African pronunciation", "3D computer-animated comedy", "Worcester Cold Storage and Warehouse Co. fire", "an acting career", "C. W. Grafton", "a liquid crystal on silicon ( LCoS ) ( based on an L CoS chip from Himax ), field - sequential color system, LED illuminated display", "Americans acting under orders", "iPod+HP", "\" That Bizarre Girl\"", "removes excess, unnecessary materials from the body fluids of an organism", "The Edge of Night", "phlogiston", "field trips", "vaskania ( \u03b2\u03b1\u03c3\u03ba\u03b1\u03bd\u03af\u03b1 )", "ATP", "soil", "1987", "medium and heavy-duty diesel trucks", "Reproductive system"], "metric_results": {"EM": 0.875, "QA-F1": 0.9391328828828829}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.918918918918919, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-3428", "before_prediction": "The Daily Stormer", "after_prediction": "\"The Krypto Report\" a podcast produced by the white supremacist site \"The Daily Stormer\""}, {"id": "mrqa_squad-validation-3442", "before_prediction": "antibonding", "after_prediction": "triplet electronic ground state"}], "retained_ids": ["mrqa_hotpotqa-validation-832", "mrqa_naturalquestions-validation-4520", "mrqa_squad-validation-2582", "mrqa_squad-validation-8259", "mrqa_hotpotqa-validation-133", "mrqa_hotpotqa-validation-3046", "mrqa_squad-validation-5940", "mrqa_squad-validation-1879"], "fixed_ids": ["mrqa_naturalquestions-validation-6844", "mrqa_squad-validation-5210", "mrqa_triviaqa-validation-3387", "mrqa_squad-validation-8924", "mrqa_squad-validation-5618", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-321", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-5128", "mrqa_naturalquestions-validation-1327", "mrqa_hotpotqa-validation-1436", "mrqa_naturalquestions-validation-1294", "mrqa_squad-validation-3490", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-5823", "mrqa_naturalquestions-validation-3677"], "unfixed_ids": ["mrqa_triviaqa-validation-2703", "mrqa_naturalquestions-validation-754"], "instant_fixing_rate": 0.9090909090909091, "instant_retention_rate": 0.7999999992}, {"timecode": 10, "before_eval": {"predictions": ["alexander", "Hampton's hump and Hampton's line", "substantive fields of law", "Las Vegas, Nevada", "optional message body", "globetrotters", "Anthony Bellew", "a bridge over the Merderet in the fictional town of Ramelle", "fascist leader", "victor Hugo Quotes", "cromlech", "Victoria, Duchess of Kent", "the base 10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "kon-Tiki", "a special Las Vegas concert", "Valentino Garavani", "Ronnie Hillman", "a single all-encompassing definition of the term", "joseph smith, Jr.", "more than 60% of the state's total land surface", "Winter Haven Mall", "Geoff Hurst and Martin Peters just one season before all three went on to star in England's World Cup winning side of 1966", "food gardens for defense", "Monastir / Tunisia / Africa", "that parts of the air in the vessel were converted into the classical element fire and thus were able to escape through pores in the glass", "Andy Griffith", "usually required for using a shooting range in the United States ; the only common requirement is that the shooter must be at least 18 or 21 years old ( or have a legal guardian present ), and must sign a waiver prior to shooting", "Ann", "an American novelist and poet", "Virginia", "Rouen Cathedral", "tree growth stages"], "metric_results": {"EM": 0.15625, "QA-F1": 0.3495138888888889}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 0.8, 0.0, 0.5, 0.0, 0.9333333333333333, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.6666666666666666, 0.0, 0.16666666666666666, 0.8, 0.2222222222222222, 1.0, 0.0, 0.0, 0.5, 0.24000000000000002, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-1085", "mrqa_hotpotqa-validation-132", "mrqa_naturalquestions-validation-8006", "mrqa_hotpotqa-validation-4162", "mrqa_hotpotqa-validation-2016", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-6389", "mrqa_triviaqa-validation-4791", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-10205", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-5682", "mrqa_triviaqa-validation-681", "mrqa_squad-validation-273", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-6119", "mrqa_squad-validation-3018", "mrqa_hotpotqa-validation-1553", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-1451", "mrqa_squad-validation-3525", "mrqa_hotpotqa-validation-3976", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-1864", "mrqa_triviaqa-validation-6639", "mrqa_squad-validation-4506"], "after_eval": {"predictions": ["Alexander Pope", "yellow fever", "English law", "Las Vegas", "A status line", "the best known globetrotters", "cruiserweight", "over the Merderet in the fictional town of Ramelle", "Mussolini", "victor hugo", "menhirs", "British Royal Family", "greater than 14", "thor heyerdahl", "Grand Garden Special Events Center", "valentino", "Anderson", "impossible", "joseph smith", "60%", "Winter Haven Mall", "Pel\u00e9", "aiding the war effort", "tunisia", "fire", "Barney Fife", "Typically, no", "Ann", "writer", "Virginia", "monet", "carbon related emissions"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9583333333333333}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-1860", "before_prediction": "alexander", "after_prediction": "Alexander Pope"}, {"id": "mrqa_hotpotqa-validation-3149", "before_prediction": "Hampton's hump and Hampton's line", "after_prediction": "yellow fever"}], "retained_ids": ["mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-62", "mrqa_naturalquestions-validation-4803"], "fixed_ids": ["mrqa_naturalquestions-validation-1085", "mrqa_hotpotqa-validation-132", "mrqa_naturalquestions-validation-8006", "mrqa_hotpotqa-validation-4162", "mrqa_hotpotqa-validation-2016", "mrqa_naturalquestions-validation-5651", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-6389", "mrqa_triviaqa-validation-4791", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-10205", "mrqa_triviaqa-validation-3515", "mrqa_hotpotqa-validation-5682", "mrqa_triviaqa-validation-681", "mrqa_squad-validation-273", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-6119", "mrqa_squad-validation-3018", "mrqa_hotpotqa-validation-1553", "mrqa_naturalquestions-validation-4837", "mrqa_triviaqa-validation-1451", "mrqa_squad-validation-3525", "mrqa_hotpotqa-validation-3976", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-1864", "mrqa_triviaqa-validation-6639", "mrqa_squad-validation-4506"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.5999999988}, {"timecode": 11, "before_eval": {"predictions": ["american football", "\"Traumnovelle\"", "women not taking jobs due to marriage or pregnancy", "Treaty on the Functioning of the European Union (TFEU)", "absolute zero, and the triple point of Vienna Standard Mean Ocean Water ( VSMOW ), a specially purified water", "the narrator driving a truck owned by his brother, who died in action in the United States Army", "dennis the menace", "span ( i.e. the length of suspended roadway between the bridge's towers )", "Dublin", "cricket", "catfood", "the Bulgars, and especially the Seljuk Turks", "die in Battle at the Alamo", "ferdinand porsche", "kansas", "Maastricht Treaty", "queen Elizabeth I", "infection, irritation, or allergies", "visited paid monument", "Town House Galleria", "catfish", "atomic number 53", "Andy Allo, Venzella Joy Williams, and Hannah Fairlight as Calamity, Serenity, Charity, and Veracity, respectively, members of the band Evermoist", "kuwait", "An agricultural cooperative", "mann on a mission", "nabucodonosor", "July 25, 1951", "the Charlotte Hornets", "the type of hazard ahead", "Jean F kernel ( 1497 -- 1558 ), a French physician", "the head"], "metric_results": {"EM": 0.25, "QA-F1": 0.39989893684746625}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.23529411764705882, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0, 0.9090909090909091, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.28571428571428575, 0.5, 0.0, 0.1, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.22222222222222224, 0.5]}}, "error_ids": ["mrqa_squad-validation-4169", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-5248", "mrqa_squad-validation-1003", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-validation-441", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-2666", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-3208", "mrqa_triviaqa-validation-792", "mrqa_triviaqa-validation-5526", "mrqa_hotpotqa-validation-4274", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6442"], "after_eval": {"predictions": ["american football", "Arthur Schnitzler's 1926 novella", "women are more likely than men to consider factors other than pay when looking for work, and may be less willing to travel or relocate", "Treaty on European Union (TEU)", "100 \u00b0 C", "his brother", "eat porridge", "span", "Dublin", "duke of edinburgh", "king crimson", "the Pechenegs, the Bulgars, and especially the Seljuk Turks", "alamo", "ferdinand porsche", "arkansas", "Canada", "britten", "allergies", "visited paid monument", "Galleria Vittorio Emanuele II", "farm - raised catfish", "heaviest of the stable halogens", "Evermoist", "kuwait", "An agricultural cooperative", "norway", "verdi", "1952", "Charlotte Hornets of the National Basketball Association ( NBA )", "advisory speed signs are classified as warning signs, not regulatory signs", "Jean Fernel ( 1497 -- 1558 ), a French physician", "chest"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9339717741935484}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.3870967741935484, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-2852", "before_prediction": "\"Traumnovelle\"", "after_prediction": "Arthur Schnitzler's 1926 novella"}, {"id": "mrqa_squad-validation-7447", "before_prediction": "women not taking jobs due to marriage or pregnancy", "after_prediction": "women are more likely than men to consider factors other than pay when looking for work, and may be less willing to travel or relocate"}], "retained_ids": ["mrqa_triviaqa-validation-5078", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-5487", "mrqa_naturalquestions-validation-5184", "mrqa_triviaqa-validation-7703", "mrqa_hotpotqa-validation-3921"], "fixed_ids": ["mrqa_squad-validation-4169", "mrqa_naturalquestions-validation-9194", "mrqa_naturalquestions-validation-9499", "mrqa_triviaqa-validation-7300", "mrqa_naturalquestions-validation-8877", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-5248", "mrqa_squad-validation-1003", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-7179", "mrqa_naturalquestions-validation-441", "mrqa_triviaqa-validation-6699", "mrqa_naturalquestions-validation-2666", "mrqa_hotpotqa-validation-3191", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-3208", "mrqa_triviaqa-validation-792", "mrqa_triviaqa-validation-5526", "mrqa_hotpotqa-validation-4274", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-6442"], "unfixed_ids": ["mrqa_naturalquestions-validation-5769"], "instant_fixing_rate": 0.9583333333333334, "instant_retention_rate": 0.7499999990624999}, {"timecode": 12, "before_eval": {"predictions": ["Coldplay", "Joe Turano", "fencers", "Margaret Thatcher", "Dan Fouts", "2014", "The stability, security, and predictability of British law and government", "Minoan civilization", "1860", "quintero", "byker grove", "Forbes", "Johnson", "dandy", "venetian", "Orwell", "Kingdom of Bohemia", "Bob Hill", "that not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence", "for creative reasons", "innate immune system", "drug trafficking", "Originally a musician", "nodel", "December 1, 1969", "maryland", "author John Buchan", "California State Automobile Association", "the word \"alone\"", "Cinderella", "delayed the sealing of the hatch", "lack of understanding of the legal ramifications, or due to a fear of seeming rude"], "metric_results": {"EM": 0.3125, "QA-F1": 0.4125445632798574}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.11764705882352941, 0.4, 1.0, 0.23529411764705882, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6167", "mrqa_triviaqa-validation-1671", "mrqa_squad-validation-568", "mrqa_hotpotqa-validation-2680", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-4263", "mrqa_triviaqa-validation-5852", "mrqa_hotpotqa-validation-1099", "mrqa_squad-validation-10202", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-2737", "mrqa_hotpotqa-validation-4904", "mrqa_squad-validation-9793", "mrqa_naturalquestions-validation-5175", "mrqa_squad-validation-6678", "mrqa_hotpotqa-validation-4241", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-7371", "mrqa_squad-validation-2812", "mrqa_squad-validation-2372"], "after_eval": {"predictions": ["Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Frederick in a duet with Teresa James", "fencing", "Margaret Thatcher", "James Lofton and Mark Malone", "December 2013", "a centre for international trade", "Minoan civilization", "18 November", "cuban cigars", "a youth club in the Byker district of Newcastle upon Tyne, England", "Forbes in the Central West region of New South Wales, Australia", "garrisons", "Bonnie Lipton ( portrayed by Skyler Samuels )", "ring", "possibly Orwell himself, called upon to shoot an aggressive elephant while working as a police officer in Burma", "Czech Kingdom", "Bob Hill", "secularism and secular nationalism", "creative reasons", "immunological memory", "uncle", "Originally a musician", "thumbelina", "1973", "maryland", "john buchan", "AAA Auto Clubs", "alone", "Cinderella", "delayed the sealing of the hatch", "a suspect's talking to criminal investigators can serve no useful purpose"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9385416666666666}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false], "QA-F1": [0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-93", "before_prediction": "Coldplay", "after_prediction": "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars"}, {"id": "mrqa_triviaqa-validation-6902", "before_prediction": "byker grove", "after_prediction": "a youth club in the Byker district of Newcastle upon Tyne, England"}, {"id": "mrqa_squad-validation-6924", "before_prediction": "lack of understanding of the legal ramifications, or due to a fear of seeming rude", "after_prediction": "a suspect's talking to criminal investigators can serve no useful purpose"}], "retained_ids": ["mrqa_hotpotqa-validation-4826", "mrqa_hotpotqa-validation-1758", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-6556", "mrqa_hotpotqa-validation-4165", "mrqa_squad-validation-3935"], "fixed_ids": ["mrqa_naturalquestions-validation-6167", "mrqa_triviaqa-validation-1671", "mrqa_squad-validation-568", "mrqa_hotpotqa-validation-2680", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-4263", "mrqa_triviaqa-validation-5852", "mrqa_hotpotqa-validation-1099", "mrqa_squad-validation-10202", "mrqa_naturalquestions-validation-4309", "mrqa_triviaqa-validation-444", "mrqa_naturalquestions-validation-2737", "mrqa_hotpotqa-validation-4904", "mrqa_squad-validation-9793", "mrqa_naturalquestions-validation-5175", "mrqa_squad-validation-6678", "mrqa_hotpotqa-validation-4241", "mrqa_triviaqa-validation-4402", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-7371", "mrqa_squad-validation-2812", "mrqa_squad-validation-2372"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.6999999992999999}, {"timecode": 13, "before_eval": {"predictions": ["Diahann Carroll, Rosalind Cash, and Irene Cara", "former president of Guggenheim Partners", "Buddy Pine / Incredi - Boy / Syndrome", "Napoleon", "john voorhees", "3.7%", "negative", "Pauline Quirke and Lesley Joseph", "the Ecumenical Award", "Jerry Ekandjo ( until February 2018 ), Erastus Utoni   Deputy : Agnes Tjongarero", "discipline problems with the Flight Director's orders during their flight", "alcaeus", "Seaside", "amyotrophic lateral sclerosis", "\"Odorama\" whereby viewers could smell what they saw on screen through scratch and sniff cards", "a pioneer in watch design, manufacturing and distribution", "mid 1970s", "Torah or Bible", "the western coast of Italy", "Phil Hill, who went on to become the first and only U.S. born world grand prix champion", "a jazz funeral without a body", "mid November", "Facebook", "ring", "Rock Star", "Issaquah, Washington (a suburb of Seattle)", "King George's War", "cheated on Miley", "alternative rock", "Fort Snelling, Minnesota", "daguerreotypes", "infrequent rain"], "metric_results": {"EM": 0.21875, "QA-F1": 0.2915595689033189}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, true], "QA-F1": [0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.3636363636363636, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.2222222222222222, 1.0, 0.3333333333333333, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-1509", "mrqa_naturalquestions-validation-1135", "mrqa_squad-validation-830", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-6944", "mrqa_naturalquestions-validation-6445", "mrqa_squad-validation-4060", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-4486", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-2037", "mrqa_squad-validation-1850", "mrqa_naturalquestions-validation-150", "mrqa_hotpotqa-validation-800", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-2896", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-1932", "mrqa_naturalquestions-validation-7650", "mrqa_hotpotqa-validation-3669", "mrqa_triviaqa-validation-6913"], "after_eval": {"predictions": ["The Omega Man", "president", "Dashiell Robert Parr / Dash, the Parrs'second child", "Napoleon's", "sliced bread", "3.7%", "negative", "garth", "the Ecumenical Award", "Erastus Utoni", "discipline problems", "nine", "michael hordern", "Lou Gehrig's Disease", "\"Odorama\"", "Swiss made", "October 17, 1938", "religious", "Sicily", "American-born", "Those who follow the band just to enjoy the music", "late November or early December", "Facebook", "breads", "Tim \"Ripper\" Owens", "Issaquah", "British and French colonies", "Miley finally ends it with him", "heavy metal", "Fort Saint Anthony", "henry fox talbot", "infrequent rain and many sunny days"], "metric_results": {"EM": 0.875, "QA-F1": 0.9146634615384616}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-10168", "before_prediction": "King George's War", "after_prediction": "British and French colonies"}, {"id": "mrqa_naturalquestions-validation-7310", "before_prediction": "alternative rock", "after_prediction": "heavy metal"}, {"id": "mrqa_squad-validation-2656", "before_prediction": "infrequent rain", "after_prediction": "infrequent rain and many sunny days"}], "retained_ids": ["mrqa_squad-validation-7042", "mrqa_squad-validation-7351", "mrqa_hotpotqa-validation-4058", "mrqa_triviaqa-validation-2779"], "fixed_ids": ["mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-1509", "mrqa_squad-validation-830", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-6944", "mrqa_naturalquestions-validation-6445", "mrqa_squad-validation-4060", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-4486", "mrqa_hotpotqa-validation-1355", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-2037", "mrqa_squad-validation-1850", "mrqa_naturalquestions-validation-150", "mrqa_hotpotqa-validation-800", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8884", "mrqa_triviaqa-validation-2896", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-1932", "mrqa_naturalquestions-validation-7650", "mrqa_hotpotqa-validation-3669", "mrqa_triviaqa-validation-6913"], "unfixed_ids": ["mrqa_naturalquestions-validation-1135"], "instant_fixing_rate": 0.96, "instant_retention_rate": 0.571428570612245}, {"timecode": 14, "before_eval": {"predictions": ["Hong Kong", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "the British military", "Dark Blood", "FX option or currency option", "electromagnetic waves", "Wahhabi/ Salafi", "\"su\" meaning \"good\"", "Dimensions in Time", "Apollo 12", "January 1981", "luteinizing hormone ( LH ) and follicle - stimulating hormone ( FSH )", "the structure and substance of his questions and answers concerning baptism in the Small Catechism", "the Jewish spokesman who tried to help the Jews of Saxony in 1537, later blamed their plight on", "brian clough", "slowing the vehicle", "Belle Fourche and Cheyenne", "organisms", "Hanna-barbera, The Jetsons", "Cortina d'Ampezzo", "efficient and effective management of money ( funds )", "Latium in central Italy, 12 mi southeast of Rome, in the Alban Hills", "duke and Duchess of Sto Helit", "Norfolk Island", "Timo Hildebrand", "the state sector", "February", "poverty", "a god of the Ammonites, as well as Tyrian Melqart and others", "vitreous humor", "Uncle Fester", "Texas Tower Sniper"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4692592946269417}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 0.16666666666666669, 1.0, 1.0, 0.5882352941176471, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 0.14285714285714288, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.18181818181818182, 0.5882352941176471, 0.0, 0.0, 0.0, 0.5, 1.0, 0.18181818181818182, 0.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-3757", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10046", "mrqa_squad-validation-9751", "mrqa_triviaqa-validation-2442", "mrqa_squad-validation-7836", "mrqa_naturalquestions-validation-5944", "mrqa_naturalquestions-validation-8180", "mrqa_squad-validation-2448", "mrqa_squad-validation-2509", "mrqa_squad-validation-6023", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-validation-3302", "mrqa_hotpotqa-validation-1244", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-5191", "mrqa_naturalquestions-validation-6019", "mrqa_squad-validation-8444", "mrqa_naturalquestions-validation-2085", "mrqa_hotpotqa-validation-3877"], "after_eval": {"predictions": ["2002 Hong Kong comedy film", "Honolulu", "the British military", "my Own Private Idaho", "a foreign exchange option ( commonly shortened to just FX option or currency option )", "radio and microwave frequencies", "Wahhabi/Salafi extremist militant group", "swastika", "Children in Need", "Apollo 12", "1981", "estrogen", "baptism", "Martin Luther", "brian clough", "slowing the vehicle", "Belle Fourche and Cheyenne", "organisms", "Hanna-Barbera", "the Veneto region of Northern Italy", "accomplish the objectives of the organization", "12 mi southeast of Rome, in the Alban Hills", "binky", "tasmania", "Kur\u00e1nyi", "public sector ( also called the state sector )", "February", "poverty, the lack of access to education and weak government institutions", "king", "vitreous humor", "Judge Doom", "Texas Tower Sniper"], "metric_results": {"EM": 0.875, "QA-F1": 0.9446733821733821}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-970", "before_prediction": "Hong Kong", "after_prediction": "2002 Hong Kong comedy film"}, {"id": "mrqa_triviaqa-validation-46", "before_prediction": "Dark Blood", "after_prediction": "my Own Private Idaho"}], "retained_ids": ["mrqa_hotpotqa-validation-3253", "mrqa_squad-validation-3999", "mrqa_triviaqa-validation-7153", "mrqa_naturalquestions-validation-727", "mrqa_triviaqa-validation-116", "mrqa_squad-validation-5178", "mrqa_hotpotqa-validation-241", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-5256"], "fixed_ids": ["mrqa_hotpotqa-validation-3757", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10046", "mrqa_triviaqa-validation-2442", "mrqa_squad-validation-7836", "mrqa_naturalquestions-validation-5944", "mrqa_naturalquestions-validation-8180", "mrqa_squad-validation-2448", "mrqa_squad-validation-2509", "mrqa_squad-validation-6023", "mrqa_hotpotqa-validation-1920", "mrqa_naturalquestions-validation-3302", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-5191", "mrqa_naturalquestions-validation-6019", "mrqa_squad-validation-8444", "mrqa_naturalquestions-validation-2085", "mrqa_hotpotqa-validation-3877"], "unfixed_ids": ["mrqa_squad-validation-9751", "mrqa_hotpotqa-validation-1244"], "instant_fixing_rate": 0.9047619047619048, "instant_retention_rate": 0.8181818174380164}, {"timecode": 15, "before_eval": {"predictions": ["comic", "Part 2", "the Flatbush section of Brooklyn, New York City", "San Antonio", "Akon, Christina Aguilera and Taio Cruz", "jupiter", "a friend and publicist", "hana", "the name of a work gang", "James Taylor", "a roof extension", "It Ain't Over'til It's Over '' is a song recorded, written, and produced by American musician Lenny Kravitz", "The neck", "1898", "Heavyweight", "Payaya Indians", "stunt performances", "fred astaire", "postage stamp", "antwerp", "chimpanzees", "March 15, 1945", "volume", "Jeremy Hammond", "Sam Waterston", "buccal cusp", "his brother, Menelaus", "25 November 2015", "tallahassee", "prefabricated housing projects", "fleet river", "WWSB and WOTV serve areas that don't receive an adequate signal from their market's primary ABC affiliate"], "metric_results": {"EM": 0.4375, "QA-F1": 0.4455971659919028}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.10526315789473684]}}, "error_ids": ["mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1479", "mrqa_hotpotqa-validation-335", "mrqa_triviaqa-validation-7512", "mrqa_squad-validation-1374", "mrqa_triviaqa-validation-5979", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-56", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-validation-368", "mrqa_triviaqa-validation-7578", "mrqa_naturalquestions-validation-6266", "mrqa_triviaqa-validation-1736", "mrqa_naturalquestions-validation-9451", "mrqa_hotpotqa-validation-413", "mrqa_triviaqa-validation-1166", "mrqa_squad-validation-874", "mrqa_squad-validation-6091"], "after_eval": {"predictions": ["comic", "Harry Potter and the Deathly Hallows", "the Flatbush section of Brooklyn, New York City", "the Alamodome", "Royce da 5'9\" (Bad) and Eminem (Evil)", "galileo", "editor of Electrical World magazine", "english patient", "the name of a work gang", "James Taylor", "a roof extension", "based on a Yogiism, or quotation from Yogi Berra", "midpiece", "1898", "martial artist", "Spanish", "stunt performances", "belgium", "postage stamp", "belgium", "chimpanzee/human hybrid", "After World War II", "volume", "Jeremy Hammond", "J. Robert Oppenheimer", "second premolar", "Aegisthus", "3 December", "florida", "an Eastern Bloc city", "sir william herschel", "KMBC-TV and KQTV"], "metric_results": {"EM": 0.875, "QA-F1": 0.875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-83", "before_prediction": "fred astaire", "after_prediction": "belgium"}, {"id": "mrqa_hotpotqa-validation-5188", "before_prediction": "chimpanzees", "after_prediction": "chimpanzee/human hybrid"}, {"id": "mrqa_hotpotqa-validation-2957", "before_prediction": "Sam Waterston", "after_prediction": "J. Robert Oppenheimer"}, {"id": "mrqa_triviaqa-validation-1588", "before_prediction": "fleet river", "after_prediction": "sir william herschel"}], "retained_ids": ["mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-3456", "mrqa_naturalquestions-validation-10439", "mrqa_hotpotqa-validation-1048", "mrqa_squad-validation-5249", "mrqa_naturalquestions-validation-3808", "mrqa_hotpotqa-validation-573", "mrqa_triviaqa-validation-5877", "mrqa_naturalquestions-validation-365", "mrqa_hotpotqa-validation-1714"], "fixed_ids": ["mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1479", "mrqa_hotpotqa-validation-335", "mrqa_triviaqa-validation-7512", "mrqa_squad-validation-1374", "mrqa_triviaqa-validation-5979", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-56", "mrqa_hotpotqa-validation-4537", "mrqa_naturalquestions-validation-368", "mrqa_triviaqa-validation-7578", "mrqa_naturalquestions-validation-6266", "mrqa_triviaqa-validation-1736", "mrqa_naturalquestions-validation-9451", "mrqa_hotpotqa-validation-413", "mrqa_triviaqa-validation-1166", "mrqa_squad-validation-874", "mrqa_squad-validation-6091"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.7142857137755102}, {"timecode": 16, "before_eval": {"predictions": ["Broadway", "belgium", "blessed", "leg", "dunkeld dynasty", "the North Sea", "sir william herschel", "oBE", "October 29, 1985", "Amway", "Mauritius", "Thomas Sowell", "President", "1962", "tanzibar", "niger", "tip of florida", "top row of windows", "Sam's soul is not with him", "Greater London, England", "French, English and Spanish", "morgan - TV News", "Beyond the Clouds", "What's Up (TV series)", "blood, platelets, and plasma", "flowing water", "poland", "modern matrices", "geena davis", "three mystic apes", "sheepskin and Merino Wool products", "Honolulu"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5463598901098901}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7692307692307692, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.21428571428571425, 0.33333333333333337, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4681", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-4094", "mrqa_squad-validation-8037", "mrqa_hotpotqa-validation-1566", "mrqa_triviaqa-validation-3238", "mrqa_naturalquestions-validation-7144", "mrqa_hotpotqa-validation-4164", "mrqa_triviaqa-validation-639", "mrqa_hotpotqa-validation-2382", "mrqa_squad-validation-8223", "mrqa_naturalquestions-validation-9087", "mrqa_hotpotqa-validation-1250", "mrqa_hotpotqa-validation-2287"], "after_eval": {"predictions": ["nightclub", "belgium", "benedict", "leg", "dunkeld dynasty", "the North Sea, through the former Meuse estuary, near Rotterdam", "Botswana", "sandy lyle", "March 30, 1983", "American Way", "secondary school study", "Milton Friedman", "President", "BBC Radio's \"The Show Band Show\"", "kPMG (T) Limited", "niger", "tip of florida", "top row of windows", "Sam's soul is not with him", "London", "French", "dave lamb", "Beyond the Clouds", "\"The Heirs\" (2013)", "human blood, platelets, and plasma", "flowing water", "poland's last king and English culture", "matrices", "Michael J. Fox", "The three wise monkeys ( Japanese : \u4e09\u733f, Hepburn : san'en or sanzaru, alternatively \u4e09 \u5339 \u306e \u733f sanbiki no saru, literally `` three monkeys", "Sheepskin", "the \"second city\" of Oahu"], "metric_results": {"EM": 0.71875, "QA-F1": 0.8233440170940172}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7692307692307692, 1.0, 0.4, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-2324", "before_prediction": "blessed", "after_prediction": "benedict"}, {"id": "mrqa_squad-validation-9319", "before_prediction": "the North Sea", "after_prediction": "the North Sea, through the former Meuse estuary, near Rotterdam"}, {"id": "mrqa_hotpotqa-validation-2445", "before_prediction": "French, English and Spanish", "after_prediction": "French"}, {"id": "mrqa_hotpotqa-validation-1001", "before_prediction": "blood, platelets, and plasma", "after_prediction": "human blood, platelets, and plasma"}, {"id": "mrqa_triviaqa-validation-866", "before_prediction": "poland", "after_prediction": "poland's last king and English culture"}, {"id": "mrqa_triviaqa-validation-298", "before_prediction": "geena davis", "after_prediction": "Michael J. Fox"}], "retained_ids": ["mrqa_triviaqa-validation-2551", "mrqa_naturalquestions-validation-9218", "mrqa_triviaqa-validation-313", "mrqa_naturalquestions-validation-3483", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-4055", "mrqa_squad-validation-5407", "mrqa_hotpotqa-validation-2937", "mrqa_triviaqa-validation-1770"], "fixed_ids": ["mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-5997", "mrqa_triviaqa-validation-4681", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-4094", "mrqa_squad-validation-8037", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4164", "mrqa_triviaqa-validation-639", "mrqa_hotpotqa-validation-2382", "mrqa_squad-validation-8223", "mrqa_hotpotqa-validation-1250", "mrqa_hotpotqa-validation-2287"], "unfixed_ids": ["mrqa_triviaqa-validation-3238", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-9087"], "instant_fixing_rate": 0.8235294117647058, "instant_retention_rate": 0.5999999996}, {"timecode": 17, "before_eval": {"predictions": ["pear", "Deadpool, X-Men: Apocalypse, Independence Day: Resurgence and Eddie the Eagle", "August 6, 1845", "rubidium - 85", "James Zeebo", "sovereign states", "Dick Cheney", "the Discovery Institute's \"Teach the Controversy\" campaign", "Bumblebee", "British", "30 months", "vary", "lower", "private liberal arts college", "Roy Spencer", "\"antiforms\"", "June 9, 2015", "Veyyil\" (2006)", "James Weldon Johnson", "Mick Jagger", "n < p < 2n \u2212 2", "Godfrey Army Airfield", "students learn from teachers who specialize in one subject and who tend to be more knowledgeable in that one area than a teacher who teaches many subjects", "antimeridian ( the 180th meridian in a 360 \u00b0 - system )", "Cartoon Network's late night programming block, Adult Swim", "the Presiding Officer on the advice of the parliamentary bureau", "Miami Heat", "33", "grapevines", "the Annual Conference Cabinet", "olympic bronze medals", "William Hartnell's poor health"], "metric_results": {"EM": 0.375, "QA-F1": 0.4823765933140933}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, false, false, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 0.08333333333333334, 1.0, 0.7499999999999999, 0.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.28571428571428575, 0.0, 0.07407407407407407, 0.25, 0.2, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_squad-validation-608", "mrqa_naturalquestions-validation-3559", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-3440", "mrqa_naturalquestions-validation-1090", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-3637", "mrqa_naturalquestions-validation-9171", "mrqa_squad-validation-8966", "mrqa_hotpotqa-validation-2782", "mrqa_squad-validation-1904", "mrqa_naturalquestions-validation-10347", "mrqa_hotpotqa-validation-3573", "mrqa_squad-validation-9405", "mrqa_hotpotqa-validation-613", "mrqa_hotpotqa-validation-4463", "mrqa_triviaqa-validation-3071", "mrqa_squad-validation-7664"], "after_eval": {"predictions": ["pear", "Independence Day: Resurgence", "August 6, 1845", "rubidium - 85", "James Zeebo", "norm that sovereigns had no internal equals within a defined territory and no external superiors as the ultimate authority within the territory's sovereign borders", "president of the United States", "\"Teach the Controversy\" campaign", "Ravage and the Decepticon Rampage", "Velichko Todorov Tsochev) is a Bulgarian-Canadian", "36 months for men and 24 months for women", "vary", "lower", "575 acres (2.08 km\u00b2)", "Roy Spencer", "\"antiforms\"", "the second half of the third season", "Veyyil", "James Weldon Johnson", "Rocky Dzidzornu -- congas", "> 3", "Bangor Air National Guard Base", "knowledgeable", "antimeridian", "Cartoon Network", "the Presiding Officer", "the Phoenix Suns", "33-member", "vitis", "the Bishop has read the appointments at the session of the Annual Conference, no appointments are officially fixed", "arthur Matthews", "the Doctor's third on-screen regeneration"], "metric_results": {"EM": 0.875, "QA-F1": 0.9114227086183311}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.9302325581395349, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-314", "before_prediction": "Dick Cheney", "after_prediction": "president of the United States"}, {"id": "mrqa_squad-validation-10074", "before_prediction": "the Annual Conference Cabinet", "after_prediction": "the Bishop has read the appointments at the session of the Annual Conference, no appointments are officially fixed"}, {"id": "mrqa_triviaqa-validation-1130", "before_prediction": "olympic bronze medals", "after_prediction": "arthur Matthews"}], "retained_ids": ["mrqa_triviaqa-validation-1298", "mrqa_hotpotqa-validation-5628", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-430", "mrqa_squad-validation-2053", "mrqa_squad-validation-7572", "mrqa_hotpotqa-validation-1895", "mrqa_squad-validation-5110", "mrqa_hotpotqa-validation-4868"], "fixed_ids": ["mrqa_squad-validation-608", "mrqa_hotpotqa-validation-113", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-3440", "mrqa_naturalquestions-validation-1090", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-3637", "mrqa_naturalquestions-validation-9171", "mrqa_squad-validation-8966", "mrqa_hotpotqa-validation-2782", "mrqa_squad-validation-1904", "mrqa_naturalquestions-validation-10347", "mrqa_hotpotqa-validation-3573", "mrqa_squad-validation-9405", "mrqa_hotpotqa-validation-613", "mrqa_hotpotqa-validation-4463", "mrqa_triviaqa-validation-3071", "mrqa_squad-validation-7664"], "unfixed_ids": ["mrqa_naturalquestions-validation-3559"], "instant_fixing_rate": 0.95, "instant_retention_rate": 0.749999999375}, {"timecode": 18, "before_eval": {"predictions": ["The Rwandaandan genocide, also known as the genocide against the Tutsi", "Co-teachers work in sync with one another to create a climate of learning", "4 \u00d7 400 metres relay", "Piper", "the entertainment division", "distance covered by a vehicle ( for example as recorded by an odometer ), person, animal, or object along a curved path from a point A to a point B", "the most times", "the Great Exhibition of 1851", "Edward Longshanks and the Hammer of the Scots", "the Chagos Archipelago", "marialaura\u00a0Di Domenico", "the person compelled to pay for reformist programs", "king James V of Scotland", "Spy Kids", "venus william", "digital transmission", "the Swiss- Austrian border", "Gigafactory 1", "821", "the basic channels", "pressure", "Kim Hyun-ah", "the races of highest'social efficiency'", "transposed", "the \" King of Cool\"", "President Wilson and the American delegation from the Paris Peace Conference", "Socrates", "thirteenth", "violet", "Hockey Club Davos", "Michael Crawford and Robin Hawdon", "Aibak's successor and son - in - law Iltutmish"], "metric_results": {"EM": 0.375, "QA-F1": 0.4976190476190476}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, true, false, false, false], "QA-F1": [0.2, 0.2857142857142857, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666665, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.4, 0.25, 0.25]}}, "error_ids": ["mrqa_hotpotqa-validation-1441", "mrqa_squad-validation-1914", "mrqa_hotpotqa-validation-523", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-3300", "mrqa_squad-validation-5257", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-5036", "mrqa_triviaqa-validation-2683", "mrqa_triviaqa-validation-2715", "mrqa_naturalquestions-validation-2222", "mrqa_squad-validation-9074", "mrqa_hotpotqa-validation-4415", "mrqa_hotpotqa-validation-1855", "mrqa_squad-validation-9841", "mrqa_squad-validation-9827", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-511", "mrqa_naturalquestions-validation-10490"], "after_eval": {"predictions": ["Rwandan genocide", "in sync", "500 metres", "Piper", "ABC News", "displacement", "five", "Museum of Manufactures", "Edward I", "diego garcia", "dundee", "the person compelled to pay for reformist programs", "fotheringhay", "Spy Kids", "venus williams", "MFSK", "Baden-W\u00fcrttemberg", "lithium-ion battery", "821", "basic channels", "pressure", "Hyuna", "the races of highest'social efficiency'", "transposed", "the \" King of Cool\"", "American delegation from the Paris Peace Conference", "socrates", "thirteenth", "violet", "HC Davos", "Michael Patrick Smith", "Firoz Shah Tughlaq"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9661458333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-2993", "before_prediction": "Edward Longshanks and the Hammer of the Scots", "after_prediction": "Edward I"}], "retained_ids": ["mrqa_hotpotqa-validation-2145", "mrqa_naturalquestions-validation-5215", "mrqa_hotpotqa-validation-2201", "mrqa_hotpotqa-validation-44", "mrqa_squad-validation-2862", "mrqa_triviaqa-validation-4279", "mrqa_naturalquestions-validation-4497", "mrqa_hotpotqa-validation-4068", "mrqa_triviaqa-validation-1764", "mrqa_hotpotqa-validation-3798", "mrqa_triviaqa-validation-7100"], "fixed_ids": ["mrqa_hotpotqa-validation-1441", "mrqa_squad-validation-1914", "mrqa_hotpotqa-validation-523", "mrqa_squad-validation-6029", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-3300", "mrqa_squad-validation-5257", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-5036", "mrqa_triviaqa-validation-2683", "mrqa_triviaqa-validation-2715", "mrqa_naturalquestions-validation-2222", "mrqa_squad-validation-9074", "mrqa_hotpotqa-validation-4415", "mrqa_hotpotqa-validation-1855", "mrqa_squad-validation-9827", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-511", "mrqa_naturalquestions-validation-10490"], "unfixed_ids": ["mrqa_squad-validation-9841"], "instant_fixing_rate": 0.95, "instant_retention_rate": 0.9166666659027777}, {"timecode": 19, "before_eval": {"predictions": ["Mauritian", "Norman Macdonnell", "zaragoza", "11.1%", "the first trans-Pacific flight from the United States to Australia", "Sharman Joshi", "do work, write lines or a punishment essay, or sit quietly", "Forster I, Forster II, and Forster III", "a divisor of p \u2212 1", "Ana", "Cherry Hill", "the end of `` Goodbye Toby '' Phyllis", "ishmael", "aida de Acosta", "a 1993 American comedy - drama film directed by Fred Schepisi", "blackstar", "India", "piety", "1889", "Nicki Minaj", "comic opera", "surnames indicating their French Huguenot ancestry", "ayrton Senna", "friedrich Engels", "\"Drawn Together\"", "William the Conqueror", "Sde Dov Airport", "two", "Corinthian and Saronic Gulfs", "taking blood samples from patients and correctly cataloging them for lab analysis", "Leo Howard, Dylan Riley Snyder,, Olivia Holt, and", "Southern Progress Corporation"], "metric_results": {"EM": 0.21875, "QA-F1": 0.3236877705627706}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.18181818181818182, 1.0, 0.3333333333333333, 1.0, 0.8, 0.5714285714285715, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2389", "mrqa_squad-validation-6963", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-303", "mrqa_squad-validation-1942", "mrqa_squad-validation-9214", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-9284", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-6935", "mrqa_naturalquestions-validation-4354", "mrqa_triviaqa-validation-1995", "mrqa_hotpotqa-validation-3049", "mrqa_naturalquestions-validation-8025", "mrqa_triviaqa-validation-2015", "mrqa_squad-validation-3151", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4068", "mrqa_hotpotqa-validation-57", "mrqa_naturalquestions-validation-1565", "mrqa_hotpotqa-validation-5061", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-3552", "mrqa_hotpotqa-validation-4020"], "after_eval": {"predictions": ["a Ghanaian boxer", "John Meston", "zaragoza", "7.8", "trans-Pacific flight", "Soha Ali Khan", "quietly", "Forster I, Forster II, and Forster III", "p is not a prime factor of q.", "Ana", "Cherry Hill", "`` Goodbye Toby ''", "call me ishmael", "amelia mary earhart", "Six Degrees of Separation", "blackstar", "Indian", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "1974", "Sir Mix - a-Lot", "pirates of penzance", "surnames", "portier", "karl marx", "Teen Titans Go!", "Norman invaders", "Tel Aviv", "two", "peninsula", "taking blood", "youngest TV director ever", "Southern Progress Corporation"], "metric_results": {"EM": 0.875, "QA-F1": 0.9056818181818181}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-5017", "before_prediction": "piety", "after_prediction": "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord"}, {"id": "mrqa_naturalquestions-validation-4951", "before_prediction": "1889", "after_prediction": "1974"}], "retained_ids": ["mrqa_triviaqa-validation-6901", "mrqa_squad-validation-5538", "mrqa_hotpotqa-validation-5710", "mrqa_naturalquestions-validation-7881", "mrqa_hotpotqa-validation-2627"], "fixed_ids": ["mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2389", "mrqa_squad-validation-6963", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-303", "mrqa_squad-validation-1942", "mrqa_squad-validation-9214", "mrqa_naturalquestions-validation-9284", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-6935", "mrqa_naturalquestions-validation-4354", "mrqa_hotpotqa-validation-3049", "mrqa_naturalquestions-validation-8025", "mrqa_triviaqa-validation-2015", "mrqa_squad-validation-3151", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4068", "mrqa_hotpotqa-validation-57", "mrqa_naturalquestions-validation-1565", "mrqa_hotpotqa-validation-5061", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-3552", "mrqa_hotpotqa-validation-4020"], "unfixed_ids": ["mrqa_naturalquestions-validation-3951", "mrqa_triviaqa-validation-1995"], "instant_fixing_rate": 0.92, "instant_retention_rate": 0.7142857132653061}, {"timecode": 20, "before_eval": {"predictions": ["gas turbines", "David Feldman", "the Sackler Centre for arts education", "Mos Def", "kaleidoscope", "British Columbia, in the Abbotsford, Vancouver and Langley areas", "Mercury", "ribosomal RNA (rRNA) molecules", "kookaburra", "six", "CCH Pounder", "\"I Miss You a Little\"", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak", "Gerry Adams", "Moon's surface", "Sulla", "Super Bowl LII, following the 2017 season", "Golden Globe", "Swahili", "my personal presence and living word", "detroit", "Pantone Matching System (PMS)", "Firoz Shah Tughlaq", "Reunited Worlds", "Santa Clara", "jellyfish", "the Hawai\u02bbi House of Representatives", "a \"teleforce\" weapon", "Native American", "\"the most giving Super Bowl ever\"", "family size was 3.21", "4077th MASH"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6407828282828283}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, true, true, false, true], "QA-F1": [1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.48484848484848486, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.9090909090909091, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-779", "mrqa_squad-validation-5273", "mrqa_naturalquestions-validation-4590", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-1210", "mrqa_hotpotqa-validation-4627", "mrqa_naturalquestions-validation-2448", "mrqa_squad-validation-8464", "mrqa_squad-validation-2280", "mrqa_triviaqa-validation-6721", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-1906", "mrqa_squad-validation-7272"], "after_eval": {"predictions": ["reciprocating", "Robert Smigel, Michael Koman and David Feldman", "(prints, drawings, paintings and photographs)", "Mos Def", "kaleidoscope", "British Columbia, in the Abbotsford, Vancouver and Langley areas in August 2017, with a mansion in the Aldergrove area of Langely serving as the property at the centre of the story", "Mercury", "Ribosomes", "kookaburra", "six-time", "CCH Pounder", "\"Ain't Got Nothin' on Us\"", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) )", "south the estates of Twinbrook and Poleglass", "Moon's surface", "Lucius Cornelius Sulla Felix", "Super Bowl LII", "Golden Globe", "Kenya's various ethnic groups typically speak their mother tongues within their own communities", "personal presence and living word", "turkey", "CMYKOG process", "Qutab - ud - din Aibak", "Reunited Worlds", "Santa Clara", "jellyfish", "lower chamber", "\"teleforce\" weapon", "Native American", "the most giving Super Bowl ever", "29.7%", "4077th MASH"], "metric_results": {"EM": 0.875, "QA-F1": 0.9125}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-3368", "before_prediction": "gas turbines", "after_prediction": "reciprocating"}, {"id": "mrqa_naturalquestions-validation-5168", "before_prediction": "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak", "after_prediction": "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) )"}, {"id": "mrqa_triviaqa-validation-4852", "before_prediction": "Gerry Adams", "after_prediction": "south the estates of Twinbrook and Poleglass"}, {"id": "mrqa_hotpotqa-validation-2407", "before_prediction": "Sulla", "after_prediction": "Lucius Cornelius Sulla Felix"}], "retained_ids": ["mrqa_hotpotqa-validation-3547", "mrqa_triviaqa-validation-5091", "mrqa_squad-validation-3964", "mrqa_triviaqa-validation-3486", "mrqa_naturalquestions-validation-1279", "mrqa_hotpotqa-validation-3623", "mrqa_hotpotqa-validation-2795", "mrqa_hotpotqa-validation-4015", "mrqa_squad-validation-315", "mrqa_triviaqa-validation-1453", "mrqa_squad-validation-1521", "mrqa_hotpotqa-validation-2064", "mrqa_squad-validation-393", "mrqa_triviaqa-validation-935"], "fixed_ids": ["mrqa_hotpotqa-validation-779", "mrqa_squad-validation-5273", "mrqa_naturalquestions-validation-4590", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-1210", "mrqa_hotpotqa-validation-4627", "mrqa_naturalquestions-validation-2448", "mrqa_squad-validation-8464", "mrqa_squad-validation-2280", "mrqa_triviaqa-validation-6721", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-1906", "mrqa_squad-validation-7272"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.777777777345679}, {"timecode": 21, "before_eval": {"predictions": ["Czech word, robota, meaning `` forced labor ''", "rebecca", "breeds with \"Longhair\" names", "William Harvey", "alison maryet", "1926", "star", "The soul does not sleep (anima non sic dormit) but wakes (sed vigilat) and experiences visions", "value of the Caribbean islands' sugar cane to be greater and easier to defend than the furs from the continent", "Johnny Cash, Waylon Jennings, Willie Nelson, and Kris Kristofferson", "auction news", "private", "Scottish", "Smith Jerrod", "Charles Dickens and Beatrix Potter", "a forum in which the Nobel Peace Laureates and the Peace Laureate Organizations could come together to address global issues with a view to encourage and support peace and human well being in the world", "carbohydrates", "2001", "(i.e. exceeds any given number) so there must be infinitely many primes", "alastair burnet", "by padlocking the gates and using sickles to deflate one of the large domes covering two satellite dishes", "1969", "R\u00e5", "western portions of the Great Lakes region (an area not directly subject to the conflict between the French and British)", "Orthodox Christians", "casino royale", "640 \u00d7 1136", "Oh So Sharp", "eating both fish larvae and small crustaceans that would otherwise feed the adult fish", "\"Menace II Society\"", "a member of the Green Bay Packers, serving as a backup quarterback to Brett Favre and holder on placekicks", "Larry Gatlin & the Gatlin Brothers Band"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5586538826670405}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, true, true, false, false, true, false, true, false, true, true, false], "QA-F1": [0.14285714285714288, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.6153846153846153, 0.6666666666666666, 1.0, 1.0, 0.4444444444444445, 0.5714285714285715, 0.4210526315789474, 0.0, 1.0, 0.5, 1.0, 0.3157894736842105, 1.0, 1.0, 0.5454545454545454, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385]}}, "error_ids": ["mrqa_naturalquestions-validation-3609", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-751", "mrqa_triviaqa-validation-4383", "mrqa_triviaqa-validation-192", "mrqa_squad-validation-2412", "mrqa_hotpotqa-validation-5480", "mrqa_triviaqa-validation-2961", "mrqa_naturalquestions-validation-5439", "mrqa_squad-validation-5345", "mrqa_hotpotqa-validation-35", "mrqa_squad-validation-3627", "mrqa_squad-validation-9020", "mrqa_squad-validation-6844", "mrqa_squad-validation-10177", "mrqa_squad-validation-8456", "mrqa_naturalquestions-validation-6832", "mrqa_squad-validation-4648", "mrqa_hotpotqa-validation-4676"], "after_eval": {"predictions": ["a 1920 play R.U.R. by the Czech writer, Karel \u010capek", "dust jacket", "domestic cat", "blood transfusion", "alison moyet", "1926", "black hole", "dreams", "The British offered France the choice of surrendering either its continental North American possessions east of the Mississippi or the Caribbean islands of Guadeloupe and Martinique", "Johnny Cash, Waylon Jennings", "auction", "private", "Scottish", "waiter turned emerging young actor Smith Jerrod", "Beatrix Potter", "Organizations could come together to address global issues", "proteins", "2001", "exceeds any given number", "alastair burnet", "by padlocking the gates", "1969", "R\u00e5", "Great Lakes", "Protestant", "casino royale", "4 in", "oh so sharp", "Mnemiopsis leidyi", "\"Menace II Society\"", "a member of the Green Bay Packers, serving as a backup quarterback to Brett Favre and holder on placekicks", "trio with his younger brothers Steve and Rudy"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9444444444444444}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222218, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-5239", "before_prediction": "rebecca", "after_prediction": "dust jacket"}, {"id": "mrqa_squad-validation-10502", "before_prediction": "value of the Caribbean islands' sugar cane to be greater and easier to defend than the furs from the continent", "after_prediction": "The British offered France the choice of surrendering either its continental North American possessions east of the Mississippi or the Caribbean islands of Guadeloupe and Martinique"}], "retained_ids": ["mrqa_hotpotqa-validation-4430", "mrqa_squad-validation-2709", "mrqa_hotpotqa-validation-4950", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-1954", "mrqa_naturalquestions-validation-8689", "mrqa_hotpotqa-validation-2399", "mrqa_triviaqa-validation-6950", "mrqa_triviaqa-validation-1555", "mrqa_hotpotqa-validation-2642", "mrqa_naturalquestions-validation-2758"], "fixed_ids": ["mrqa_naturalquestions-validation-3609", "mrqa_hotpotqa-validation-3780", "mrqa_triviaqa-validation-751", "mrqa_triviaqa-validation-4383", "mrqa_triviaqa-validation-192", "mrqa_squad-validation-2412", "mrqa_hotpotqa-validation-5480", "mrqa_triviaqa-validation-2961", "mrqa_naturalquestions-validation-5439", "mrqa_squad-validation-5345", "mrqa_hotpotqa-validation-35", "mrqa_squad-validation-3627", "mrqa_squad-validation-9020", "mrqa_squad-validation-6844", "mrqa_squad-validation-10177", "mrqa_squad-validation-8456", "mrqa_naturalquestions-validation-6832", "mrqa_squad-validation-4648", "mrqa_hotpotqa-validation-4676"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.8461538455029586}, {"timecode": 22, "before_eval": {"predictions": ["Alex Skuby", "England", "the first Thursday in May", "MSC Crociere S. p.A.", "alexes", "his friends, Humpty Dumpty and Kitty Softpaws", "The Greens", "Royalists", "cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "are ceremonially placed on the heads of Christians on Ash Wednesday, either by being sprinkled over their heads or, in English - speaking countries, more often by being marked on their foreheads as a visible cross", "Death to Spies", "jules verne", "Augustus Waters", "1619", "Tony Blair", "\u2018expensive damaging\u2019, along with alcohol, tobacco and gambling", "June 11, 1973", "in Kenya and in the Masai Mara in particular", "critical quotations", "aethelbert", "neutrality", "United Healthcare", "AMC Entertainment Inc.", "\"The Gang\"", "3 October 1990", "March 1, 2018", "heavy W and Z bosons", "p.G. Wodehouse", "Bernice", "Manhattan Project", "Barsetshire", "vast areas"], "metric_results": {"EM": 0.15625, "QA-F1": 0.28824447203123676}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 0.0, 0.4, 0.0, 0.0, 0.4444444444444445, 0.0, 0.33333333333333337, 0.11764705882352942, 0.16, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.0, 0.4, 0.5714285714285715, 0.0, 0.33333333333333337, 1.0, 0.4, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1883", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-9789", "mrqa_triviaqa-validation-5698", "mrqa_naturalquestions-validation-7346", "mrqa_squad-validation-2884", "mrqa_hotpotqa-validation-2959", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-1578", "mrqa_naturalquestions-validation-3859", "mrqa_squad-validation-2769", "mrqa_hotpotqa-validation-482", "mrqa_squad-validation-8280", "mrqa_hotpotqa-validation-5655", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-190", "mrqa_squad-validation-10459", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-1929", "mrqa_naturalquestions-validation-1328", "mrqa_hotpotqa-validation-2448"], "after_eval": {"predictions": ["Doug Pruzan", "England", "Thursday", "Mediterranean Shipping Company", "benjamin franklin", "Humpty Dumpty", "Nationals", "Parliamentarians (\"Roundheads\") and Royalists (\"Cavaliers\")", "the presence of correctly oriented P waves", "reminds worshippers of their sinfulness and mortality and thus, implicitly, of their need to repent in time", "smersh", "jules verne", "17 - year - old Augustus Waters, an ex-basketball player and amputee", "1619", "Tony Blair", "often damaging", "July 26, 1959", "Masai Mara", "chronological collection of critical quotations", "edward i", "long - standing policy of neutrality", "United Healthcare", "AMC Theatres", "It's Always Sunny in Philadelphia", "summer of 1990 and continued until 1992", "September 21, 2017", "weak force", "pig", "Dexter", "Development of Substitute Materials", "Chronicles of Barsetshire", "vast areas"], "metric_results": {"EM": 0.9375, "QA-F1": 0.953125}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": ["mrqa_triviaqa-validation-2797", "mrqa_hotpotqa-validation-3944", "mrqa_triviaqa-validation-4731", "mrqa_hotpotqa-validation-2511", "mrqa_squad-validation-2828"], "fixed_ids": ["mrqa_naturalquestions-validation-1719", "mrqa_squad-validation-9568", "mrqa_naturalquestions-validation-9789", "mrqa_triviaqa-validation-5698", "mrqa_naturalquestions-validation-7346", "mrqa_squad-validation-2884", "mrqa_hotpotqa-validation-2959", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-1578", "mrqa_naturalquestions-validation-3859", "mrqa_squad-validation-2769", "mrqa_hotpotqa-validation-482", "mrqa_squad-validation-8280", "mrqa_hotpotqa-validation-5655", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1435", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-190", "mrqa_squad-validation-10459", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-1929", "mrqa_naturalquestions-validation-1328", "mrqa_hotpotqa-validation-2448"], "unfixed_ids": ["mrqa_naturalquestions-validation-1883", "mrqa_triviaqa-validation-6872"], "instant_fixing_rate": 0.9259259259259259, "instant_retention_rate": 0.9999999980000001}, {"timecode": 23, "before_eval": {"predictions": ["$105 billion", "mono", "about two-thirds the size", "1934 Austin seven box saloon", "15/10", "red", "Steeplechase Park", "Best Animated Feature", "European Union institutions", "(381.6 days)", "nine", "NASA's CAL IPSO satellite", "celandine", "Ulbricht", "Ronald Ralph \"Ronnie\" Schell", "artemisinin-based therapy", "Mumbai, Maharashtra", "east", "1940", "the 2017 / 18 Divisional Round game against the New Orleans Saints", "possibly 1707, in his second annual cycle (1724 to 1725)", "on Fresno's far southeast side, bounded by Chestnut Avenue to the West", "south western escarpment of the Jos Plateau", "samedi", "Incudomalleolar joint", "tennis", "Leucippus", "Santa Clara Marriott", "benjamin barenboim", "political power generated by wealth", "log-space reductions", "Corey Brown"], "metric_results": {"EM": 0.28125, "QA-F1": 0.4166734307359307}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.6666666666666666, 0.25, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.3636363636363636, 0.18181818181818182, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0]}}, "error_ids": ["mrqa_triviaqa-validation-3716", "mrqa_squad-validation-8850", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-3933", "mrqa_hotpotqa-validation-789", "mrqa_squad-validation-4118", "mrqa_squad-validation-4228", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-validation-3714", "mrqa_hotpotqa-validation-1782", "mrqa_triviaqa-validation-3595", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-220", "mrqa_naturalquestions-validation-3052", "mrqa_squad-validation-2420", "mrqa_squad-validation-4662", "mrqa_naturalquestions-validation-2212", "mrqa_triviaqa-validation-571", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-6781", "mrqa_squad-validation-1750", "mrqa_squad-validation-769"], "after_eval": {"predictions": ["$105 billion", "mononucleosis", "17 nm vs 25 nm", "Austin Seven Ruby Open Top Tourer", "dirty dancing", "zinc silicate primer and vinyl topcoats", "Dreamland", "Academy Award for Best Animated Feature", "the European Convention on Human Rights", "(381.6 days)", "nine", "NASA", "yellow", "Khrushchev", "Jack Cassidy", "falciparum malaria", "Mumbai", "the east of Ireland", "1940", "2017 / 18", "1707", "Sunnyside", "begins in central Nigeria", "Live and let die", "Incudomalleolar joint ( more correctly called incudomallear joint )", "moffit", "Democritus", "Santa Clara Marriott", "van beethoven", "political power generated by wealth", "polynomial-time reductions or log-space reductions", "Ted Ginn Jr."], "metric_results": {"EM": 0.75, "QA-F1": 0.8357799369747898}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.5714285714285715, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.8235294117647058, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-1617", "before_prediction": "red", "after_prediction": "zinc silicate primer and vinyl topcoats"}, {"id": "mrqa_hotpotqa-validation-4348", "before_prediction": "Steeplechase Park", "after_prediction": "Dreamland"}, {"id": "mrqa_hotpotqa-validation-2340", "before_prediction": "Leucippus", "after_prediction": "Democritus"}], "retained_ids": ["mrqa_squad-validation-7389", "mrqa_hotpotqa-validation-2741", "mrqa_squad-validation-542", "mrqa_naturalquestions-validation-4048", "mrqa_squad-validation-327", "mrqa_squad-validation-7481"], "fixed_ids": ["mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-3933", "mrqa_hotpotqa-validation-789", "mrqa_squad-validation-4118", "mrqa_squad-validation-4228", "mrqa_triviaqa-validation-6107", "mrqa_naturalquestions-validation-3714", "mrqa_hotpotqa-validation-1782", "mrqa_triviaqa-validation-3595", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-220", "mrqa_naturalquestions-validation-3052", "mrqa_squad-validation-2420", "mrqa_squad-validation-4662", "mrqa_triviaqa-validation-571", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-6781", "mrqa_squad-validation-769"], "unfixed_ids": ["mrqa_squad-validation-8850", "mrqa_triviaqa-validation-6438", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-1731", "mrqa_squad-validation-1750"], "instant_fixing_rate": 0.782608695652174, "instant_retention_rate": 0.6666666659259258}, {"timecode": 24, "before_eval": {"predictions": ["the 1965 -- 66 season", "NADP+ though sometimes they can flow back down more H+-pumping electron transport chains to transport more hydrogen ions into the thylakoid space to generate more ATP", "Pitt", "elixir of perpetual youth", "WBC and lineal titles", "moluccas", "the Kentucky Derby and Belmont Stakes", "Cordelia", "to pull back from the Sinai Peninsula and the Golan Heights", "1990", "The Lord of the Rings: The Return of the King", "John Elway", "Selena Gomez", "join a vocational youth/village polytechnic or make their own arrangements for an apprenticeship program and learn a trade such as tailoring, carpentry, motor vehicle repair, brick-laying and masonry", "bingo", "Eugene", "is an unofficial title sometimes given to new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "the form 2p + 1 with p prime", "letter series", "FaZe Rug", "nine", "Mongols", "the interior", "friars Minor Conventual", "CD Castell\u00f3n", "between 1770 and 1848", "12\u20134", "having colloblasts", "Jon M. Chu", "STS-51-L.", "it will retreat to its den and winter will persist for six more weeks", "minister of industrial restructuring and external trade"], "metric_results": {"EM": 0.1875, "QA-F1": 0.31558091042798486}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, true, false, false], "QA-F1": [0.0, 0.07692307692307693, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8181818181818181, 0.0, 0.0, 1.0, 0.0, 0.6486486486486487, 0.0, 0.0, 0.4, 0.9090909090909091, 0.0, 1.0, 0.0, 0.0, 0.25, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5957446808510638, 0.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8444", "mrqa_squad-validation-8876", "mrqa_triviaqa-validation-3678", "mrqa_hotpotqa-validation-4444", "mrqa_triviaqa-validation-3340", "mrqa_hotpotqa-validation-3290", "mrqa_naturalquestions-validation-9011", "mrqa_squad-validation-3741", "mrqa_squad-validation-5399", "mrqa_hotpotqa-validation-3247", "mrqa_naturalquestions-validation-10381", "mrqa_squad-validation-8473", "mrqa_triviaqa-validation-667", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-309", "mrqa_squad-validation-8979", "mrqa_naturalquestions-validation-2164", "mrqa_triviaqa-validation-3847", "mrqa_squad-validation-6204", "mrqa_squad-validation-10148", "mrqa_triviaqa-validation-5304", "mrqa_hotpotqa-validation-5588", "mrqa_naturalquestions-validation-3504", "mrqa_hotpotqa-validation-4542", "mrqa_naturalquestions-validation-6508", "mrqa_triviaqa-validation-1306"], "after_eval": {"predictions": ["2003", "NADP+", "The British failures in North America, combined with other failures in the European theater, led to the fall from power of Newcastle and his principal military advisor, the Duke of Cumberland.", "alchemists", "WBO lightweight title", "spice islands", "Saturday", "Kent", "They arranged for Israel to pull back from the Sinai Peninsula and the Golan Heights.", "1971", "The Lord of the Rings: The Return of the King", "John Elway", "Instagram", "join a vocational youth/village polytechnic or make their own arrangements for an apprenticeship program", "31", "Eugene", "comparable to the seven Wonders of the World", "2p + 1 with p prime", "coupe", "Fa Ze YouTubers", "dante alighieri", "the Muslim", "along the coast", "poor clares", "Club Deportivo Castell\u00f3n", "1780 -- 1830", "12\u20134", "having colloblasts", "Anne Fletcher", "STS-51-C.", "if a groundhog ( Deitsch : Grundsau, Grunddax, Dax ) emerging from its burrow on this day sees a shadow due to clear weather, it will retreat to its den and winter will", "france"], "metric_results": {"EM": 0.8125, "QA-F1": 0.8474385245901639}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9180327868852458, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-10261", "before_prediction": "Pitt", "after_prediction": "The British failures in North America, combined with other failures in the European theater, led to the fall from power of Newcastle and his principal military advisor, the Duke of Cumberland."}, {"id": "mrqa_naturalquestions-validation-3297", "before_prediction": "FaZe Rug", "after_prediction": "Fa Ze YouTubers"}, {"id": "mrqa_hotpotqa-validation-1814", "before_prediction": "STS-51-L.", "after_prediction": "STS-51-C."}], "retained_ids": ["mrqa_squad-validation-384", "mrqa_squad-validation-67", "mrqa_squad-validation-4417"], "fixed_ids": ["mrqa_naturalquestions-validation-8444", "mrqa_squad-validation-8876", "mrqa_triviaqa-validation-3678", "mrqa_hotpotqa-validation-4444", "mrqa_triviaqa-validation-3340", "mrqa_hotpotqa-validation-3290", "mrqa_naturalquestions-validation-9011", "mrqa_squad-validation-3741", "mrqa_squad-validation-5399", "mrqa_naturalquestions-validation-10381", "mrqa_squad-validation-8473", "mrqa_triviaqa-validation-667", "mrqa_hotpotqa-validation-309", "mrqa_squad-validation-8979", "mrqa_naturalquestions-validation-2164", "mrqa_triviaqa-validation-3847", "mrqa_squad-validation-6204", "mrqa_squad-validation-10148", "mrqa_triviaqa-validation-5304", "mrqa_hotpotqa-validation-5588", "mrqa_naturalquestions-validation-3504", "mrqa_hotpotqa-validation-4542", "mrqa_triviaqa-validation-1306"], "unfixed_ids": ["mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3080", "mrqa_naturalquestions-validation-6508"], "instant_fixing_rate": 0.8846153846153846, "instant_retention_rate": 0.49999999916666665}, {"timecode": 25, "before_eval": {"predictions": ["gymnastics", "abram", "over 50 million singles", "states'rights to expand slavery", "1923 and 1925", "Orlando\u2013Kissimmee\u2013 Sanford", "January 19, 1962", "PPA, Pattugliatore Polivalente d'Altura", "crimson tide", "until all available list seats have been determined", "geese", "in effect", "pacific", "Colin Baker and Sylvester McCoy", "August 14, 1848", "life expectancy is lower", "While at least some species, juveniles are capable of reproduction before reaching the adult size and shape.", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "musical", "2,664 rooms", "iphone 6 Plus", "a chute beneath his or her feet", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "cleaning, catering and security", "Violin Sonata No. 5 in F major, Opus 24", "gironde", "1603", "ranked above the two personal physicians of the Emperor", "nutcracker", "we admitted we were powerless over alcohol -- that our lives had become unmanageable.   Came to believe that a Power greater than ourselves could restore us to sanity.", "baseball team"], "metric_results": {"EM": 0.5625, "QA-F1": 0.739501991064491}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, true, false, true, true, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.3333333333333333, 0.25, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.3333333333333333, 1.0, 0.8666666666666666, 0.4615384615384615, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3636363636363636, 1.0, 0.16666666666666666, 1.0, 1.0, 0.22222222222222224, 1.0, 0.6666666666666666, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-1818", "mrqa_naturalquestions-validation-10169", "mrqa_naturalquestions-validation-4072", "mrqa_hotpotqa-validation-5762", "mrqa_squad-validation-9532", "mrqa_squad-validation-7677", "mrqa_hotpotqa-validation-4720", "mrqa_squad-validation-4637", "mrqa_naturalquestions-validation-390", "mrqa_hotpotqa-validation-4282", "mrqa_naturalquestions-validation-7309", "mrqa_hotpotqa-validation-499", "mrqa_squad-validation-6328", "mrqa_naturalquestions-validation-2481"], "after_eval": {"predictions": ["gymnastics", "Genesis 16:1-16", "40 million", "loyalty to the U.S. Constitution", "1923", "Orlando\u2013Kissimmee\u2013Sanford", "January 19, 1962", "PPA, Pattugliatore Polivalente d'Altura", "crimson tide", "iteratively", "geese", "in effect", "pacific", "Peter Davison, Colin Baker and Sylvester McCoy", "February 14, 1859", "life expectancy is lower", "In at least some species, juveniles are capable of reproduction before reaching the adult size", "breaded chicken patty", "the way they used `` rule '' and `` method '' to go about their religious affairs", "musical", "2,664", "iphone 6 Plus", "a chute beneath his or her feet", "Claims adjuster", "cleaning, catering and security", "Symphony No. 7", "gironde", "1603", "status superior to all others in health-related fields such as physicians and acupuncturists", "nutcracker", "We admitted we were powerless over alcohol -- that our lives had become unmanageable.", "baseball"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9663461538461539}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-5788", "before_prediction": "abram", "after_prediction": "Genesis 16:1-16"}, {"id": "mrqa_naturalquestions-validation-5939", "before_prediction": "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "after_prediction": "the way they used `` rule '' and `` method '' to go about their religious affairs"}], "retained_ids": ["mrqa_triviaqa-validation-6059", "mrqa_hotpotqa-validation-3369", "mrqa_hotpotqa-validation-3977", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6941", "mrqa_squad-validation-7382", "mrqa_triviaqa-validation-7336", "mrqa_squad-validation-7301", "mrqa_hotpotqa-validation-859", "mrqa_triviaqa-validation-2454", "mrqa_naturalquestions-validation-4996", "mrqa_hotpotqa-validation-1414", "mrqa_triviaqa-validation-2098", "mrqa_naturalquestions-validation-6545", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-1259"], "fixed_ids": ["mrqa_hotpotqa-validation-1818", "mrqa_naturalquestions-validation-10169", "mrqa_naturalquestions-validation-4072", "mrqa_hotpotqa-validation-5762", "mrqa_squad-validation-9532", "mrqa_squad-validation-7677", "mrqa_hotpotqa-validation-4720", "mrqa_squad-validation-4637", "mrqa_naturalquestions-validation-390", "mrqa_hotpotqa-validation-4282", "mrqa_naturalquestions-validation-7309", "mrqa_hotpotqa-validation-499", "mrqa_squad-validation-6328", "mrqa_naturalquestions-validation-2481"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.8888888883950616}, {"timecode": 26, "before_eval": {"predictions": ["to Jewish audiences", "north of the Lakes Region", "outlined by Joel Greenblatt", "true history of the Kelly Gang", "City and County of Honolulu", "1910", "those who refuse vetting", "Catch Me Who Can", "jazz", "margaret court", "4,000", "the founder of the Yuan dynasty", "catherine and heathcliff", "birmingham", "sri lankan", "The Simpsons Spin-Off Showcase", "The planner Raymond Unwin and the architect Barry Parker", "San Bernardino", "Shopping Centre", "Albany High School", "charbagh", "Sergeant First Class", "Anakin Skywalker", "jury nullification", "the closing scene of the final episode of the first season", "The Church of England was legally established in the colony in 1619, and authorities in England sent in 22 Anglican clergyman by 1624", "Saturday Evening Puss", "scharnhorst", "hypnotist", "sitting directly on the Gulf of Guinea on the Atlantic Ocean in Nigeria", "opportunity-based entrepreneurship", "January 11, 1755 or 1757July 12, 1804"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7317708333333333}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333]}}, "error_ids": ["mrqa_hotpotqa-validation-1426", "mrqa_triviaqa-validation-6772", "mrqa_squad-validation-7435", "mrqa_hotpotqa-validation-5", "mrqa_squad-validation-6148", "mrqa_hotpotqa-validation-2436", "mrqa_squad-validation-2428", "mrqa_naturalquestions-validation-7581", "mrqa_triviaqa-validation-5839", "mrqa_hotpotqa-validation-945"], "after_eval": {"predictions": ["to Jewish audiences", "south", "outlined by Joel Greenblatt", "kelly", "City and County of Honolulu", "1910\u20131940", "those who refuse vetting", "Catch Me Who Can", "jazz", "september", "3,677 seated", "Khagan", "catherine and heathcliff", "birmingham", "sri lankan", "the Simpson family", "The planner Raymond Unwin and the architect Barry Parker", "Riverside", "Shopping Centre", "Albany High School", "charbagh", "Sergeant First Class", "Anakin Skywalker", "jury nullification", "the closing scene of the final episode of the first season", "The Church of England", "hattie mcdaniel", "scharnhorst", "hypnotist", "sitting directly on the Gulf of Guinea on the Atlantic Ocean in Nigeria", "opportunity-based", "January 11, 1755 or 1757July"], "metric_results": {"EM": 0.96875, "QA-F1": 0.96875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-6689", "before_prediction": "margaret court", "after_prediction": "september"}], "retained_ids": ["mrqa_naturalquestions-validation-3027", "mrqa_hotpotqa-validation-2184", "mrqa_hotpotqa-validation-4553", "mrqa_squad-validation-1994", "mrqa_squad-validation-3176", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-3472", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6224", "mrqa_naturalquestions-validation-2214", "mrqa_squad-validation-5184", "mrqa_hotpotqa-validation-1446", "mrqa_naturalquestions-validation-800", "mrqa_hotpotqa-validation-4585", "mrqa_naturalquestions-validation-3658", "mrqa_squad-validation-6837", "mrqa_naturalquestions-validation-5476", "mrqa_triviaqa-validation-5050", "mrqa_triviaqa-validation-3393", "mrqa_naturalquestions-validation-7801", "mrqa_squad-validation-7319"], "fixed_ids": ["mrqa_hotpotqa-validation-1426", "mrqa_triviaqa-validation-6772", "mrqa_squad-validation-7435", "mrqa_hotpotqa-validation-5", "mrqa_squad-validation-6148", "mrqa_hotpotqa-validation-2436", "mrqa_squad-validation-2428", "mrqa_naturalquestions-validation-7581", "mrqa_triviaqa-validation-5839", "mrqa_hotpotqa-validation-945"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9545454541115702}, {"timecode": 27, "before_eval": {"predictions": ["hair", "september", "blackberry", "horsehead", "Big Mamie", "orinoco", "Teamsters leader", "a light sky-blue color caused by absorption in the red", "wat tyler", "2009", "2017", "the inner chloroplast membrane", "september", "sports", "Third-party channels", "third", "electroweak interaction", "Cost of construction", "gypsum", "A simple iron boar crest", "polytechnics became new universities", "World War II", "David", "25 - yard line", "the Latin centum", "7,000", "lion, leopard, buffalo, rhinoceros, and elephant", "the righteousness of Christ", "september", "possible combinations of two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "rubenesque", "badgers"], "metric_results": {"EM": 0.6875, "QA-F1": 0.731551001082251}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.375, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6060606060606061, 1.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-2007", "mrqa_squad-validation-3539", "mrqa_hotpotqa-validation-4022", "mrqa_triviaqa-validation-1423", "mrqa_triviaqa-validation-1034", "mrqa_naturalquestions-validation-9105", "mrqa_naturalquestions-validation-3771", "mrqa_squad-validation-2313", "mrqa_triviaqa-validation-388", "mrqa_naturalquestions-validation-2893"], "after_eval": {"predictions": ["hair", "2000", "blackberry", "horsehead Nebula", "Big Mamie", "orinoco", "Teamsters leader", "clear", "wat tyler", "2009", "Zaza Pachulia", "the inner chloroplast membrane", "renoir", "sports", "Third-party channels", "third", "the more fundamental electroweak interaction", "Cost of construction", "gypsum", "A simple iron boar crest", "polytechnics became new universities", "australian", "Lofton", "a touchback on kickoffs at the 25 - yard line instead of the previous 20 - yard lines", "named after the Swedish astronomer Anders Celsius", "7,000 out of 20,000 inhabitants", "lion, leopard, buffalo, rhinoceros, and elephant", "lives by faith", "cliff thorburn", "produced with constant technology and resources per unit of time", "peter paul rubens", "badgers"], "metric_results": {"EM": 0.875, "QA-F1": 0.9350961538461539}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9230769230769231, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-2899", "before_prediction": "horsehead", "after_prediction": "horsehead Nebula"}, {"id": "mrqa_hotpotqa-validation-3949", "before_prediction": "David", "after_prediction": "Lofton"}, {"id": "mrqa_squad-validation-5125", "before_prediction": "7,000", "after_prediction": "7,000 out of 20,000 inhabitants"}], "retained_ids": ["mrqa_triviaqa-validation-2635", "mrqa_triviaqa-validation-348", "mrqa_hotpotqa-validation-994", "mrqa_triviaqa-validation-6331", "mrqa_hotpotqa-validation-2428", "mrqa_triviaqa-validation-2980", "mrqa_squad-validation-7711", "mrqa_squad-validation-8739", "mrqa_squad-validation-2966", "mrqa_squad-validation-2733", "mrqa_hotpotqa-validation-158", "mrqa_squad-validation-10312", "mrqa_squad-validation-6855", "mrqa_triviaqa-validation-5962", "mrqa_hotpotqa-validation-1226", "mrqa_squad-validation-5337", "mrqa_squad-validation-8279", "mrqa_triviaqa-validation-622", "mrqa_triviaqa-validation-1507"], "fixed_ids": ["mrqa_triviaqa-validation-2007", "mrqa_squad-validation-3539", "mrqa_hotpotqa-validation-4022", "mrqa_triviaqa-validation-1423", "mrqa_triviaqa-validation-1034", "mrqa_naturalquestions-validation-3771", "mrqa_squad-validation-2313", "mrqa_triviaqa-validation-388", "mrqa_naturalquestions-validation-2893"], "unfixed_ids": ["mrqa_naturalquestions-validation-9105"], "instant_fixing_rate": 0.9, "instant_retention_rate": 0.8636363632438017}, {"timecode": 28, "before_eval": {"predictions": ["the Turk", "Michael Bisping", "jury nullification", "Harishchandra", "studied Arabic grammar", "Professor Eobard Thawne", "plum brandy", "a US$10 a week raise", "1875", "member states", "clarinets", "McKinsey's offices in Silicon Valley and India", "spiders", "Living Doll by Cliff Richard", "Crohn's disease or ulcerative colitis", "Ondemar Dias", "Raya Yarbrough", "Cincinnati", "Michael Dokes", "Charles L. Hutchinson", "the Old Testament", "UPS", "local talent", "Preston North End Football Club", "american", "meager", "that contemporary accounts were exaggerations", "John Surratt", "1349", "dodo bird", "people and their thoughts are both made from `` pure energy '', and that through the process of `` like energy attracting like energy '' a person can improve their own health, wealth and personal relationships", "Stan Butler"], "metric_results": {"EM": 0.25, "QA-F1": 0.3888180272108843}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, true], "QA-F1": [0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4444444444444445, 0.0, 0.4444444444444445, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.888888888888889, 0.0, 0.0, 1.0, 0.2040816326530612, 1.0]}}, "error_ids": ["mrqa_squad-validation-2291", "mrqa_hotpotqa-validation-1390", "mrqa_squad-validation-6835", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-5637", "mrqa_triviaqa-validation-4827", "mrqa_hotpotqa-validation-4352", "mrqa_naturalquestions-validation-10495", "mrqa_triviaqa-validation-5237", "mrqa_hotpotqa-validation-5110", "mrqa_triviaqa-validation-3760", "mrqa_triviaqa-validation-6250", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-7110", "mrqa_triviaqa-validation-1347", "mrqa_naturalquestions-validation-10687", "mrqa_hotpotqa-validation-1251", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-6881", "mrqa_squad-validation-4921", "mrqa_triviaqa-validation-5810", "mrqa_squad-validation-8190", "mrqa_naturalquestions-validation-7821"], "after_eval": {"predictions": ["Islam", "Anderson Silva", "inform the jury and the public of the political circumstances", "Kusha", "poet, and writer", "Professor Eobard Thawne", "plum", "a US$10 a week raise", "1825", "contributed by member states on a voluntary basis", "oboe", "McKinsey's offices", "public speaking", "lionel bart", "high - output fistula", "Ondemar Dias", "Bear McCreary", "UMBC", "riddick bowe", "John D. Rockefeller", "Song of Songs", "UPS", "local talent", "North End Football Club", "peter davison", "canada", "contemporary accounts were exaggerations", "president Lincoln", "1332", "bird", "the belief that by focusing on positive or negative thoughts people can bring positive or positive experiences into their life", "larky bus driver Stan Butler"], "metric_results": {"EM": 0.84375, "QA-F1": 0.9328790726817042}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.9473684210526315, 0.5714285714285715]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-8031", "before_prediction": "Charles L. Hutchinson", "after_prediction": "John D. Rockefeller"}, {"id": "mrqa_triviaqa-validation-2953", "before_prediction": "dodo bird", "after_prediction": "bird"}, {"id": "mrqa_triviaqa-validation-4308", "before_prediction": "Stan Butler", "after_prediction": "larky bus driver Stan Butler"}], "retained_ids": ["mrqa_naturalquestions-validation-4919", "mrqa_squad-validation-1276", "mrqa_squad-validation-4309", "mrqa_triviaqa-validation-7669", "mrqa_squad-validation-5086"], "fixed_ids": ["mrqa_squad-validation-2291", "mrqa_hotpotqa-validation-1390", "mrqa_squad-validation-6835", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-5637", "mrqa_triviaqa-validation-4827", "mrqa_hotpotqa-validation-4352", "mrqa_naturalquestions-validation-10495", "mrqa_triviaqa-validation-5237", "mrqa_hotpotqa-validation-5110", "mrqa_triviaqa-validation-6250", "mrqa_naturalquestions-validation-4127", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-7110", "mrqa_triviaqa-validation-1347", "mrqa_naturalquestions-validation-10687", "mrqa_hotpotqa-validation-1251", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-6881", "mrqa_squad-validation-4921", "mrqa_triviaqa-validation-5810", "mrqa_squad-validation-8190"], "unfixed_ids": ["mrqa_triviaqa-validation-3760", "mrqa_naturalquestions-validation-7821"], "instant_fixing_rate": 0.9166666666666666, "instant_retention_rate": 0.6249999992187499}, {"timecode": 29, "before_eval": {"predictions": ["hollies", "the 1960s", "patents", "Formula One", "microsoft", "Beijing", "Kony Ealy", "the parallelogram rule of vector addition", "theory that denies the story of the Divine Creation of man as taught in the Bible", "ring-necked pheasants", "startup neutron source", "Don McLean", "cylinder volume", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "the local administrative structure of past Chinese dynasties", "Doctorin' the Tardis", "National Basketball Development League", "kent", "St. Mary's County", "Emmanuel Sanders", "The population was 2,615", "Beijing", "an American football quarterback", "recover many kinds of passwords using methods such as network packet sniffing, cracking various password hashes by using methods", "Treme", "husband and wife", "south korea", "arthur", "the smallest subfield", "heartburn", "women not taking jobs due to marriage or pregnancy", "normal grana and thylakoids"], "metric_results": {"EM": 0.625, "QA-F1": 0.6832010064664163}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.25, 0.0, 1.0, 0.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8085106382978724, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-4708", "mrqa_naturalquestions-validation-6610", "mrqa_squad-validation-814", "mrqa_squad-validation-10408", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-3103", "mrqa_triviaqa-validation-7032", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-7567", "mrqa_squad-validation-7445"], "after_eval": {"predictions": ["holly", "the 1960s", "patents", "Formula One", "microsoft", "Tokyo for the 2020 Summer Olympics", "DeMarcus Ware", "parallelogram", "theory that denies the story of the Divine Creation of man as taught in the Bible", "364", "startup neutron source", "van gogh", "cylinder volume", "storage of minerals", "the local administrative structure of past Chinese dynasties", "Doctorin' the Tardis", "The league was known as the National Basketball Development League (NBDL) from 2001 to 2005", "kent", "Washington metropolitan area", "Emmanuel Sanders", "The population was 2,615", "Beijing", "an American football quarterback", "recover many kinds of passwords using methods such as network packet sniffing, cracking various password hashes by using method such as dictionary attacks, brute force and cryptanalysis attacks", "The Man", "husband and wife", "south korea", "arthur", "the smallest subfield", "heartburn", "53%", "grana and thylakoids"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9589023109243697}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9642857142857143, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-2928", "before_prediction": "National Basketball Development League", "after_prediction": "The league was known as the National Basketball Development League (NBDL) from 2001 to 2005"}], "retained_ids": ["mrqa_naturalquestions-validation-866", "mrqa_squad-validation-1249", "mrqa_hotpotqa-validation-3497", "mrqa_triviaqa-validation-6632", "mrqa_naturalquestions-validation-8653", "mrqa_squad-validation-3344", "mrqa_squad-validation-8075", "mrqa_squad-validation-7914", "mrqa_triviaqa-validation-1280", "mrqa_squad-validation-772", "mrqa_hotpotqa-validation-3031", "mrqa_naturalquestions-validation-9765", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5709", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-2709", "mrqa_squad-validation-9036", "mrqa_triviaqa-validation-814", "mrqa_squad-validation-8873"], "fixed_ids": ["mrqa_triviaqa-validation-4708", "mrqa_naturalquestions-validation-6610", "mrqa_squad-validation-814", "mrqa_squad-validation-10408", "mrqa_triviaqa-validation-3103", "mrqa_triviaqa-validation-7032", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-7567", "mrqa_squad-validation-7445"], "unfixed_ids": ["mrqa_triviaqa-validation-3603", "mrqa_naturalquestions-validation-4572"], "instant_fixing_rate": 0.8333333333333334, "instant_retention_rate": 0.9499999995249999}, {"timecode": 30, "before_eval": {"predictions": ["the judiciary", "named for Frederick Louis, Prince of Wales, son of King George II", "a wife named Sally", "john brown", "john leese and Connie Booth", "Erick Avari, Michael McKean, Amy D. Jacobson, Marty Ingels, Earl Boen, Jordana Capra, Dirk Benedict, and Rip Taylor", "SyFy", "lester lbeth", "demographics and economic ties", "3 years 280 days", "the most recent Super Bowl champion", "narcolepsy", "Arctic Monkeys", "imola", "jurassic park", "usernames, passwords, commands and data", "A computer program is a collection of instructions that performs a specific task when executed by a computer", "Nationals are strongest in Melbourne's North Western and Eastern rural regional areas", "king Nebuchadnezzar", "vatnajokull glacier", "$474 million", "national network", "2,000 kilometres ( 1,200 mi ) down the Australian northeast coast", "7", "Delaware to the southeast, Maryland to the south, West Virginia to the southwest, Ohio to the west, Lake Erie and the Canadian province of Ontario to the northwest, New York to the north, and New Jersey to the east", "Netflix", "CCH Pounder", "tamer youssef", "state-franchised national lottery", "skylab", "spain", "to comply with the Do - Not - Call Implementation Act of 2003"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4498816287878788}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, true], "QA-F1": [0.0, 0.125, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.12121212121212122, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-4645", "mrqa_hotpotqa-validation-2559", "mrqa_naturalquestions-validation-9608", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-4079", "mrqa_triviaqa-validation-1921", "mrqa_squad-validation-2577", "mrqa_hotpotqa-validation-4578", "mrqa_triviaqa-validation-5266", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-9852", "mrqa_squad-validation-2885", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7090", "mrqa_squad-validation-8451", "mrqa_naturalquestions-validation-4710", "mrqa_hotpotqa-validation-4604", "mrqa_triviaqa-validation-3134", "mrqa_hotpotqa-validation-5604"], "after_eval": {"predictions": ["judges", "Dutch House of Orange-Nassau", "Jesse McCartney as JoJo, the Mayor's son", "john brown", "polly", "Martin Ingerman", "SyFy", "daniel craig", "historical political divisions", "3", "the most recent Super Bowl champion", "narcolepsy", "alex turner", "imola", "jurassic park", "all transmissions", "A computer program", "The Greens", "babylon", "surtsey", "Kenya's largest source of foreign direct investment", "national network", "South Pacific", "Article 7", "New Jersey", "Netflix", "CCH Pounder", "indira priyadarshini gandhi", "state-franchised", "skylab", "spain", "to comply with the Do - Not - Call Implementation Act of 2003"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9823717948717949}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-6961", "before_prediction": "7", "after_prediction": "Article 7"}], "retained_ids": ["mrqa_triviaqa-validation-3920", "mrqa_squad-validation-7793", "mrqa_naturalquestions-validation-3209", "mrqa_triviaqa-validation-2750", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-1722", "mrqa_triviaqa-validation-7684", "mrqa_hotpotqa-validation-3333", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-2091", "mrqa_naturalquestions-validation-10328"], "fixed_ids": ["mrqa_naturalquestions-validation-4645", "mrqa_hotpotqa-validation-2559", "mrqa_triviaqa-validation-2802", "mrqa_hotpotqa-validation-4079", "mrqa_triviaqa-validation-1921", "mrqa_squad-validation-2577", "mrqa_hotpotqa-validation-4578", "mrqa_triviaqa-validation-5266", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-9852", "mrqa_squad-validation-2885", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7090", "mrqa_squad-validation-8451", "mrqa_naturalquestions-validation-4710", "mrqa_hotpotqa-validation-4604", "mrqa_triviaqa-validation-3134", "mrqa_hotpotqa-validation-5604"], "unfixed_ids": ["mrqa_naturalquestions-validation-9608"], "instant_fixing_rate": 0.9473684210526315, "instant_retention_rate": 0.9230769223668639}, {"timecode": 31, "before_eval": {"predictions": ["Lupe Ontiveros", "The Unwinding: An Inner History of the New America is a 2013 non-fiction book by the American journalist George Packer.", "sarajevo", "football", "Newell Highway", "tenth planet", "4", "shopping", "make a defiant speech, or a speech explaining their actions,", "Andrew Adamson, Kelly Asbury and Conrad Vernon", "johann Strauss", "Naimans (Naiman Mongols)", "emissions resulting from human activities are substantially increasing the atmospheric concentrations of the greenhouse gases, resulting on average in an additional warming of the Earth's surface", "poseidon", "Britain", "encourage", "Ibrium", "strictly come dancing", "Polish", "a variety of political groups that supported the Spanish coup of July 1936 against the Second Spanish Republic, including the Falange, the CEDA, and two rival monarchist claimants : the Alfonsists and the Carlists", "cole porter", "1967", "16,000 species", "Washington Street", "May 10, 1976", "6", "\"Fudge\"", "frustration with the atmosphere in the group at that time -- namely, Paul McCartney's over-assertiveness and criticism of his guitar playing, John Lennon's lack of engagement with the project and dismissal of Harrison as a songwriter", "Barbarella", "John Smith", "lusitania", "They also settled elsewhere in Kent, particularly Sandwich, Faversham and Maidstone"], "metric_results": {"EM": 0.625, "QA-F1": 0.7074652777777778}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false], "QA-F1": [0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.5, 0.37037037037037035, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.45000000000000007, 1.0, 1.0, 1.0, 0.0]}}, "error_ids": ["mrqa_hotpotqa-validation-1385", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-2564", "mrqa_triviaqa-validation-3300", "mrqa_squad-validation-6128", "mrqa_squad-validation-8589", "mrqa_triviaqa-validation-5513", "mrqa_naturalquestions-validation-9825", "mrqa_hotpotqa-validation-3728", "mrqa_squad-validation-932", "mrqa_naturalquestions-validation-4148", "mrqa_squad-validation-3106"], "after_eval": {"predictions": ["Guadalupe \"Lupe\" Ontiveros", "Andreas", "sarajevo", "football", "Newell Highway", "tenth planet", "4", "shopping", "make a defiant speech, or a speech explaining their actions,", "Andrew Adamson", "waltz king", "the Naimans", "additional warming of the Earth's surface", "parthenon", "Britain", "encourage", "Ibrium", "strictly come dancing", "Polish", "the Falange", "cole albert", "1967", "16,000 species", "Kneeland Street", "8 November 1978", "2\u00bd", "100 Deeds for Eddie McDowd", "his frustration with the atmosphere in the group at that time", "Barbarella", "John Smith", "lusitania", "economic separation"], "metric_results": {"EM": 0.9375, "QA-F1": 0.953125}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-3233", "before_prediction": "Washington Street", "after_prediction": "Kneeland Street"}, {"id": "mrqa_hotpotqa-validation-5307", "before_prediction": "\"Fudge\"", "after_prediction": "100 Deeds for Eddie McDowd"}], "retained_ids": ["mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-1050", "mrqa_hotpotqa-validation-2762", "mrqa_triviaqa-validation-457", "mrqa_triviaqa-validation-5071", "mrqa_naturalquestions-validation-5604", "mrqa_squad-validation-6734", "mrqa_naturalquestions-validation-954", "mrqa_squad-validation-7469", "mrqa_hotpotqa-validation-1444", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-2493", "mrqa_triviaqa-validation-3286", "mrqa_triviaqa-validation-2609", "mrqa_squad-validation-4415", "mrqa_triviaqa-validation-743", "mrqa_naturalquestions-validation-4500", "mrqa_triviaqa-validation-4524"], "fixed_ids": ["mrqa_hotpotqa-validation-1385", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-2564", "mrqa_triviaqa-validation-3300", "mrqa_squad-validation-6128", "mrqa_squad-validation-8589", "mrqa_triviaqa-validation-5513", "mrqa_naturalquestions-validation-9825", "mrqa_hotpotqa-validation-3728", "mrqa_squad-validation-932", "mrqa_naturalquestions-validation-4148", "mrqa_squad-validation-3106"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.89999999955}, {"timecode": 32, "before_eval": {"predictions": ["orbital scientific instrument package", "the Orthodox Christians", "Fomento Econ\u00f3mico Mexicano", "september", "south africa", "Blake Garrett Rosenthal, Thomas Middleditch, Matt Jones, and Tom Bower as members of Dwight's family", "k", "when a driver may attempt to jackknife the vehicle deliberately in order to halt it following brake failure", "T cell receptor (TCR)", "relatively low salaries", "usa", "Point of Entry", "moonraker", "u", "Science Magazine", "England national team", "poverty", "No Night Today", "Convention", "5,922", "December 5, 1991", "James Dean in Anton Corbijn's \"Life\" (2015)", "76ers", "the Dutch figure of Sinterklaas ( himself also based on Saint Nicholas )", "Stern-Plaza", "Jimmy Ellis", "23 March 1991", "lily-of-the-valley", "Dealey Plaza", "Nairobi", "As sea levels rose", "Anno 2053"], "metric_results": {"EM": 0.53125, "QA-F1": 0.604079131652661}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 0.09523809523809523, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-5510", "mrqa_triviaqa-validation-946", "mrqa_hotpotqa-validation-305", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-10311", "mrqa_hotpotqa-validation-2910", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1660", "mrqa_naturalquestions-validation-7049", "mrqa_hotpotqa-validation-2985", "mrqa_triviaqa-validation-1079", "mrqa_hotpotqa-validation-4221", "mrqa_hotpotqa-validation-3557", "mrqa_naturalquestions-validation-5960"], "after_eval": {"predictions": ["orbital scientific instrument package", "the Orthodox Christians", "Fomento Econ\u00f3mico Mexicano", "september", "south africa", "Thomas Middleditch", "k", "If a vehicle towing a trailer skids", "T cell receptor (TCR)", "relatively low salaries", "genetically engineered corn or soybeans", "Point of Entry", "bridge", "u", "Science Magazine", "Premier League club Manchester United and the England national team", "poverty", "Space is the Place", "France's Legislative Assembly", "5,922", "June 4, 1931", "is a 2016 science fiction psychological horror", "leg injury", "Father Christmas", "Stern-Plaza", "Jimmy Ellis", "1991", "may", "Dallas", "Nairobi, Kenya", "During the last Ice Age", "Anno 2053"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9553571428571428}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-7156", "before_prediction": "usa", "after_prediction": "genetically engineered corn or soybeans"}], "retained_ids": ["mrqa_squad-validation-3887", "mrqa_squad-validation-6744", "mrqa_hotpotqa-validation-572", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3066", "mrqa_triviaqa-validation-7394", "mrqa_squad-validation-6602", "mrqa_squad-validation-2234", "mrqa_hotpotqa-validation-1119", "mrqa_triviaqa-validation-2245", "mrqa_squad-validation-8617", "mrqa_squad-validation-8095", "mrqa_hotpotqa-validation-3508", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5557"], "fixed_ids": ["mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-5510", "mrqa_triviaqa-validation-946", "mrqa_hotpotqa-validation-305", "mrqa_naturalquestions-validation-5631", "mrqa_hotpotqa-validation-2910", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1660", "mrqa_naturalquestions-validation-7049", "mrqa_hotpotqa-validation-2985", "mrqa_triviaqa-validation-1079", "mrqa_hotpotqa-validation-4221", "mrqa_hotpotqa-validation-3557", "mrqa_naturalquestions-validation-5960"], "unfixed_ids": ["mrqa_naturalquestions-validation-10311"], "instant_fixing_rate": 0.9333333333333333, "instant_retention_rate": 0.941176470034602}, {"timecode": 33, "before_eval": {"predictions": ["the defeat of Napoleon", "Boston Herald", "1967", "the amount charged by a bookmaker", "the twelfth most populous city in the United States", "Roger Maris", "tintin", "is able to bind a specific ligand, a transmembrane domain, and an intracellular catalytic domain", "higher", "bass", "Hodel", "Biloxi", "emperor", "oakum", "Spring city", "London", "Broken Hill and Sydney", "2005", "all punishments", "The Doctor's Daughter", "wagons", "ilich ramirez sanchez", "the desire to prevent things that are indisputably bad", "Paris", "niece", "staying with the same group of peers for all classes", "backflow prevention", "emotional contagion", "japan", "power windows", "Bill Clinton", "Buskerud"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8102764423076922}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0]}}, "error_ids": ["mrqa_squad-validation-7278", "mrqa_naturalquestions-validation-9271", "mrqa_hotpotqa-validation-650", "mrqa_squad-validation-10180", "mrqa_squad-validation-7741", "mrqa_squad-validation-6878", "mrqa_hotpotqa-validation-61"], "after_eval": {"predictions": ["the defeat of Napoleon", "Boston Herald", "1967", "the amount charged by a bookmaker", "largest Filipino American community", "Roger Maris", "tintin", "a hyper - active kinase, that confers an aberrant, ligand - independent, non-regulated growth stimulus to the cancer cells", "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued)", "J\u0101nis Strazdi\u0146\u0161", "Hodel", "St. Lawrence River valley", "emperor", "oakum", "Yunnan- Fu", "London", "Broken Hill and Sydney", "2005", "all punishments and granted them salvation were in error", "Smith and Jones", "wagons", "Ilich Ramirez Sanchez", "things that are a matter of custom or expectation", "Paris", "niece", "staying with the same group of peers for all classes", "backflow prevention", "emotional contagion", "japan", "passenger space and amenities such as air conditioning, power steering, AM-FM radios, and even power windows and central locking without increasing the price of the vehicle", "William Jefferson Clinton", "Buskerud"], "metric_results": {"EM": 0.875, "QA-F1": 0.8936713286713287}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_squad-validation-7571", "before_prediction": "higher", "after_prediction": "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued)"}, {"id": "mrqa_hotpotqa-validation-270", "before_prediction": "Spring city", "after_prediction": "Yunnan- Fu"}, {"id": "mrqa_squad-validation-2010", "before_prediction": "all punishments", "after_prediction": "all punishments and granted them salvation were in error"}, {"id": "mrqa_squad-validation-3733", "before_prediction": "power windows", "after_prediction": "passenger space and amenities such as air conditioning, power steering, AM-FM radios, and even power windows and central locking without increasing the price of the vehicle"}], "retained_ids": ["mrqa_squad-validation-9984", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-2092", "mrqa_naturalquestions-validation-8203", "mrqa_triviaqa-validation-1410", "mrqa_naturalquestions-validation-3523", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-6499", "mrqa_hotpotqa-validation-5727", "mrqa_naturalquestions-validation-6358", "mrqa_hotpotqa-validation-2161", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-6186", "mrqa_naturalquestions-validation-7387", "mrqa_hotpotqa-validation-2588", "mrqa_squad-validation-1903", "mrqa_naturalquestions-validation-5297", "mrqa_squad-validation-2147", "mrqa_triviaqa-validation-2812", "mrqa_hotpotqa-validation-1211"], "fixed_ids": ["mrqa_squad-validation-7278", "mrqa_naturalquestions-validation-9271", "mrqa_hotpotqa-validation-650", "mrqa_squad-validation-10180", "mrqa_squad-validation-7741", "mrqa_squad-validation-6878", "mrqa_hotpotqa-validation-61"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.839999999664}, {"timecode": 34, "before_eval": {"predictions": ["Chairman", "norman hartnell", "email", "neither issue made it clear whether Archie was married to Betty or Veronica", "Victorian College of the Arts and Melbourne Conservatorium of Music", "Britain", "onions", "0.52/ sq mi", "separate tables", "France", "Ian Paisley", "World War II", "litas", "suggs", "Taft", "late 1970s", "first published in 1890", "75th", "Manhattan", "Lucius Verus", "revolution or orbital revolution", "Johnny Darrell", "head and neck", "motorcycles or mopeds pulling trailers", "Euler's totient function", "ear canal", "how graphs are encoded as binary strings", "third", "afghanistan", "large cars", "Kurt Vonnegut", "healing incantation"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8307005494505495}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-4369", "mrqa_hotpotqa-validation-3107", "mrqa_triviaqa-validation-3324", "mrqa_naturalquestions-validation-4288", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-3408", "mrqa_squad-validation-8034"], "after_eval": {"predictions": ["Chairman", "norman hartnell", "email", "neither issue made it clear whether Archie was married to Betty or Veronica", "Victorian College of the Arts and Melbourne Conservatorium of Music", "Britain", "onions", "0.52/sq mi", "Separate Tables", "It was held in France from 10 June to 12 July 1998.", "Ian Richard Kyle Paisley, Baron Bannside, PC", "World War II", "litas", "London", "Taft", "late 1970s", "first published in 1890", "75th", "Manhattan", "Lucius Verus", "revolution or orbital revolution", "Kenny Rogers and The First Edition", "head and neck", "motorcycles or mopeds pulling trailers", "the sum of divisors function", "earwax", "how graphs are encoded as binary strings", "third", "afghanistan", "large", "Lauren Oliver", "healing incantation"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9513888888888888}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-1139", "before_prediction": "Ian Paisley", "after_prediction": "Ian Richard Kyle Paisley, Baron Bannside, PC"}], "retained_ids": ["mrqa_naturalquestions-validation-2962", "mrqa_triviaqa-validation-7248", "mrqa_squad-validation-6673", "mrqa_naturalquestions-validation-6787", "mrqa_hotpotqa-validation-3982", "mrqa_naturalquestions-validation-951", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-5496", "mrqa_hotpotqa-validation-3072", "mrqa_triviaqa-validation-1220", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-5740", "mrqa_naturalquestions-validation-4366", "mrqa_hotpotqa-validation-3002", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-10676", "mrqa_naturalquestions-validation-4192", "mrqa_triviaqa-validation-7592", "mrqa_squad-validation-4255", "mrqa_squad-validation-1635", "mrqa_squad-validation-2751", "mrqa_triviaqa-validation-7184", "mrqa_squad-validation-3708", "mrqa_triviaqa-validation-1198"], "fixed_ids": ["mrqa_squad-validation-4369", "mrqa_hotpotqa-validation-3107", "mrqa_naturalquestions-validation-4288", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-3408", "mrqa_squad-validation-8034"], "unfixed_ids": ["mrqa_triviaqa-validation-3324"], "instant_fixing_rate": 0.8571428571428571, "instant_retention_rate": 0.959999999616}, {"timecode": 35, "before_eval": {"predictions": ["2% higher", "capital and financial markets", "Dan Stevens", "the brain, muscles, and liver", "ear-shaped", "Atlanta Falcons, the San Francisco 49ers, the Dallas Cowboys, the Washington Redskins and the Baltimore Ravens", "courtyard", "Howard Ashton", "president harding", "high and persistent unemployment", "Broward County", "Best Actor prize", "changing display or audio settings quickly", "charles i", "the spectroscopic notation for the associated atomic orbitals : sharp, principal, diffuse and fundamental, and then g which follows f in the alphabet", "declines", "Beauty and the Beast", "south africa", "Scott \" Scotty\" Grainger Jr. is a fictional character from the CBS soap opera \"The Young and the Restless\"", "texas state", "a seal", "the UMC", "Geno Lenardo", "Don Hahn", "Port Moresby, Papua New Guinea", "david seville", "NAACP", "1963\u20131989", "rms titanic", "margaret beckett", "elizabeth montgomery", "india"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7967028985507246}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 0.34782608695652173, 1.0, 0.8, 1.0, 0.4, 0.0, 1.0, 0.4, 1.0, 0.4799999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_squad-validation-9123", "mrqa_naturalquestions-validation-7704", "mrqa_triviaqa-validation-4829", "mrqa_naturalquestions-validation-4544", "mrqa_hotpotqa-validation-2971", "mrqa_squad-validation-7352", "mrqa_hotpotqa-validation-4447", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-585", "mrqa_hotpotqa-validation-1475"], "after_eval": {"predictions": ["2%", "capital and financial markets", "Dan Stevens, Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Nathan Mack, Ian McKellen, and Emma Thompson", "brain, muscles, and liver", "butterfly", "New York Yankees, the Atlanta Braves, the Cincinnati Reds and the San Francisco Giants", "courtyard", "William Howard Ashton, (born 19 August 1943) better known by his stage name Billy J. Kramer is an English pop singer.", "president harding", "Unemployment", "Miami", "Best Actor prize", "changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "king charles i", "the spectroscopic notation for the associated atomic orbitals", "declines", "Beauty and the Beast", "south africa", "Tyler \"Ty\" Mendoza", "texas state", "a seal", "the UMC", "Geno Lenardo", "Don Hahn", "Port Moresby, Papua New Guinea", "david seville", "NAACP", "1963\u20131989", "rms titanic", "margaret beckett", "elizabeth montgomery", "india"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9317098662207357}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.15384615384615385, 0.4, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-10161", "before_prediction": "Dan Stevens", "after_prediction": "Dan Stevens, Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Nathan Mack, Ian McKellen, and Emma Thompson"}], "retained_ids": ["mrqa_naturalquestions-validation-25", "mrqa_squad-validation-9400", "mrqa_triviaqa-validation-6423", "mrqa_hotpotqa-validation-2947", "mrqa_triviaqa-validation-3767", "mrqa_squad-validation-7495", "mrqa_hotpotqa-validation-5690", "mrqa_triviaqa-validation-6450", "mrqa_triviaqa-validation-2999", "mrqa_squad-validation-3408", "mrqa_squad-validation-10036", "mrqa_hotpotqa-validation-3853", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-430", "mrqa_triviaqa-validation-11", "mrqa_hotpotqa-validation-1720", "mrqa_squad-validation-7610", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-1812"], "fixed_ids": ["mrqa_squad-validation-9123", "mrqa_triviaqa-validation-4829", "mrqa_naturalquestions-validation-4544", "mrqa_squad-validation-7352", "mrqa_hotpotqa-validation-4447", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-585", "mrqa_hotpotqa-validation-1475"], "unfixed_ids": ["mrqa_naturalquestions-validation-7704", "mrqa_hotpotqa-validation-2971"], "instant_fixing_rate": 0.8, "instant_retention_rate": 0.9545454541115702}, {"timecode": 36, "before_eval": {"predictions": ["NBC", "McG", "sir william herschel", "Rudolph", "Cobham\u2013Edmonds thesis", "to remind the Doctor of his \"moral duty\"", "II", "April", "new orleans", "Raymond Patterson", "Beyonc\u00e9 and Bruno Mars", "Menorca", "plead guilty to one misdemeanor count and receive no jail time", "emperors", "2%", "1979", "a virtual reality simulator", "The formal language", "conductor", "the right side of the heart", "That the plague was caused by bad air", "American pint of 16 US fluid ounces ( 473 ml )", "mountain ranges", "white", "The U.S. state of Georgia is known as the `` Peach State '' due to its significant production of peaches as early as 1571, with exports to other states occurring around 1858", "turf grasses", "nearly $12", "20 %", "love is all around", "to build a nationwide network in the UK", "west", "Libya to the north, Sudan to the east, the Central African Republic to the south, Cameroon and Nigeria to the southwest and Niger to the west"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7540245808468315}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.08695652173913045]}}, "error_ids": ["mrqa_hotpotqa-validation-152", "mrqa_squad-validation-110", "mrqa_naturalquestions-validation-3993", "mrqa_triviaqa-validation-5936", "mrqa_naturalquestions-validation-8525", "mrqa_naturalquestions-validation-3344", "mrqa_triviaqa-validation-4069", "mrqa_squad-validation-4626", "mrqa_hotpotqa-validation-452"], "after_eval": {"predictions": ["National Broadcasting Company", "McG", "sir william herschel", "Rudolph", "the Cobham\u2013Edmonds thesis", "to remind the Doctor of his \" moral duty\"", "II", "April", "new orleans", "Raymond Patterson", "Coldplay", "Menorca", "plead guilty to one misdemeanor count and receive no jail time", "emperors", "2%", "1979", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "The formal language", "conducting", "the right side of the heart", "That the plague was caused by bad air", "imperial fluid ounces", "mountain ranges", "white cross", "other states", "nettle", "nearly $12", "20 %", "love is all around", "use in the ARPANET", "west", "Republic of Chad"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9895833333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-199", "before_prediction": "white", "after_prediction": "white cross"}], "retained_ids": ["mrqa_hotpotqa-validation-652", "mrqa_triviaqa-validation-4824", "mrqa_triviaqa-validation-37", "mrqa_squad-validation-1758", "mrqa_squad-validation-7720", "mrqa_hotpotqa-validation-4427", "mrqa_hotpotqa-validation-3911", "mrqa_triviaqa-validation-3803", "mrqa_hotpotqa-validation-2936", "mrqa_naturalquestions-validation-6011", "mrqa_squad-validation-6759", "mrqa_naturalquestions-validation-4115", "mrqa_squad-validation-3060", "mrqa_hotpotqa-validation-1884", "mrqa_squad-validation-1634", "mrqa_naturalquestions-validation-5552", "mrqa_squad-validation-4877", "mrqa_hotpotqa-validation-1118", "mrqa_squad-validation-3635", "mrqa_naturalquestions-validation-2323", "mrqa_triviaqa-validation-6290", "mrqa_squad-validation-9146"], "fixed_ids": ["mrqa_hotpotqa-validation-152", "mrqa_squad-validation-110", "mrqa_naturalquestions-validation-3993", "mrqa_triviaqa-validation-5936", "mrqa_naturalquestions-validation-8525", "mrqa_naturalquestions-validation-3344", "mrqa_triviaqa-validation-4069", "mrqa_squad-validation-4626", "mrqa_hotpotqa-validation-452"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9565217387145557}, {"timecode": 37, "before_eval": {"predictions": ["San Joaquin Valley Railroad", "broken", "2007", "San Luis Obispo, California", "mother-of-pearl", "February 20, 1978", "sheep", "Walter Mondale", "96", "the Roman Empire", "two", "white, blue, pink, rainbow neon and glittering dotted lines", "the alluvial plain", "37\u00b0 9' 58.23\" latitude, around 11 miles (18 km) south of San Jose", "woodentop", "Henry", "shared", "large birds or mammals", "events and festivals", "kabinett", "1991", "avatar", "7 January 1936", "lifetime protection", "It contains twenty-three episodes, starting with \"Lard of the Dance\"", "Edwin Hubble, known for \"Hubble's Law\" NASA astronaut John M. Grunsfeld, geneticist James Watson, best known as one of the co-discoverers of the structure of DNA", "public services", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Nlend Wom\u00e9 (born 26 March 1979) is a Cameroonian retired footballer who played as a defender", "make a defiant speech, or a speech explaining their actions,", "whey", "Boston, Massachusetts"], "metric_results": {"EM": 0.625, "QA-F1": 0.6948128770739064}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8750000000000001, 0.0, 0.375, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.0, 0.0, 0.13333333333333333, 0.23529411764705882, 0.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-4664", "mrqa_squad-validation-5702", "mrqa_naturalquestions-validation-9058", "mrqa_squad-validation-2660", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-10554", "mrqa_hotpotqa-validation-2377", "mrqa_squad-validation-8069", "mrqa_squad-validation-7246", "mrqa_naturalquestions-validation-1375", "mrqa_hotpotqa-validation-633", "mrqa_squad-validation-6737"], "after_eval": {"predictions": ["San Joaquin Valley Railroad", "broken", "2007", "San Luis Obispo, California", "mother-of-pearl", "February 20, 1978", "sheep", "George H.W. Bush", "96", "De Inventione by Marcus Tullius Cicero", "two-thirdsths of its territory", "white, blue, pink, rainbow neon and glittering", "Mesopotamia", "37\u00b0 9' 58.23\"", "woodentop", "Henry", "shared", "great desert skink", "events and festivals", "tafelwein", "2010", "avatar", "7 January 1936", "lifetime protection", "It contains twenty-three episodes", "Carl Sagan", "Much of the city's tax base dissipated", "Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Pierre Nlend Wom\u00e9", "mistreatment from government officials", "whey", "Boston, Massachusetts"], "metric_results": {"EM": 0.875, "QA-F1": 0.8791666666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-4212", "before_prediction": "the Roman Empire", "after_prediction": "De Inventione by Marcus Tullius Cicero"}, {"id": "mrqa_triviaqa-validation-3479", "before_prediction": "two", "after_prediction": "two-thirdsths of its territory"}, {"id": "mrqa_triviaqa-validation-7134", "before_prediction": "kabinett", "after_prediction": "tafelwein"}], "retained_ids": ["mrqa_squad-validation-4768", "mrqa_squad-validation-1625", "mrqa_squad-validation-4108", "mrqa_hotpotqa-validation-104", "mrqa_squad-validation-5451", "mrqa_hotpotqa-validation-513", "mrqa_triviaqa-validation-1015", "mrqa_hotpotqa-validation-4154", "mrqa_triviaqa-validation-5258", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-4032", "mrqa_squad-validation-677", "mrqa_triviaqa-validation-3876", "mrqa_hotpotqa-validation-85", "mrqa_naturalquestions-validation-969", "mrqa_triviaqa-validation-2524", "mrqa_hotpotqa-validation-5371"], "fixed_ids": ["mrqa_naturalquestions-validation-4664", "mrqa_squad-validation-5702", "mrqa_naturalquestions-validation-9058", "mrqa_squad-validation-2660", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-10554", "mrqa_hotpotqa-validation-2377", "mrqa_squad-validation-8069", "mrqa_squad-validation-7246", "mrqa_hotpotqa-validation-633", "mrqa_squad-validation-6737"], "unfixed_ids": ["mrqa_naturalquestions-validation-1375"], "instant_fixing_rate": 0.9166666666666666, "instant_retention_rate": 0.8499999995749999}, {"timecode": 38, "before_eval": {"predictions": ["the NP-complete Boolean satisfiability problem", "Dan Stevens", "1958", "Etienne de Mestre", "dragon", "slavery", "the colonies of British America", "a children's story published by John Newbery in London in 1765", "224.7 Earth days", "crowdfunding platforms for gathering money from the public, which circumvents traditional avenues of investment", "Thorgan Ganael Francis Hazard", "when commissioned", "Don Jeffrey \"Jeff\" Meldrum", "a week", "phil archer", "Shoshone, his mother tongue, and other western American Indian languages", "The Chipettes", "suez canal", "60 by West All - Stars ( 2017 )", "journalist", "the fact that there is no revising chamber", "beehive", "ramification in geometry", "newly accessioned into the collection", "fiat money", "strychnine", "Iowa", "approximately in the early 16th century", "13 June 2003", "Today", "Hathi Jr.", "the closing of the atrioventricular valves and semilunar valves, respectively"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8223730517848165}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.11764705882352941, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-8418", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-2682", "mrqa_squad-validation-9478", "mrqa_naturalquestions-validation-2100"], "after_eval": {"predictions": ["the NP-complete Boolean satisfiability problem", "Emma Watson and Dan Stevens as the eponymous characters with Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Ian McKellen,", "1958", "Bart Cummings", "dragon", "slavery", "the colonies of British America", "The History of Little Goody Two - Shoes", "224.7 Earth days", "crowdfunding platforms for gathering money from the public, which circumvents traditional avenues of investment", "Thorgan Ganael Francis Hazard", "when commissioned", "Don Jeffrey \"Jeff\" Meldrum (born May 24, 1958) is a Professor of Anatomy and Anthropology", "a week", "phil archer", "French and English", "The Chipettes", "suez canal", "60 by West All - Stars ( 2017 )", "journalist", "take evidence from witnesses, conduct inquiries and scrutinise legislation", "beehive hairdo", "ramification in geometry", "newly accessioned into the collection", "fiat money", "strychnine", "Iowa", "approximately in the early 16th century", "Lord's", "Today", "Hathi Jr.", "the closing of the atrioventricular valves and semilunar valves, respectively"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9551005747126436}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.896551724137931, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-1473", "before_prediction": "beehive", "after_prediction": "beehive hairdo"}], "retained_ids": ["mrqa_squad-validation-1862", "mrqa_naturalquestions-validation-3893", "mrqa_triviaqa-validation-4677", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-10255", "mrqa_hotpotqa-validation-727", "mrqa_naturalquestions-validation-4067", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-4762", "mrqa_naturalquestions-validation-3707", "mrqa_triviaqa-validation-3118", "mrqa_naturalquestions-validation-6210", "mrqa_squad-validation-1660", "mrqa_squad-validation-9032", "mrqa_squad-validation-5505", "mrqa_naturalquestions-validation-10279", "mrqa_triviaqa-validation-3320", "mrqa_naturalquestions-validation-7630", "mrqa_naturalquestions-validation-3006", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2555"], "fixed_ids": ["mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-8418", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-2682", "mrqa_squad-validation-9478"], "unfixed_ids": ["mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-2100"], "instant_fixing_rate": 0.7142857142857143, "instant_retention_rate": 0.959999999616}, {"timecode": 39, "before_eval": {"predictions": ["capital of a nation", "Dan Conner", "east and west berlin", "warren commission", "Katharine Hepburn, Joan Bennett, Frances Dee, and Jean Parker", "violence", "Reese Witherspoon as Carter, Ginnifer Goodwin as Vivian Liberto, and Robert Patrick as Cash's father", "couscous", "1977", "Carl Sagan", "detroit", "your song", "2003", "every year", "Fabbrica Italiana Automobili Torino", "the second Sunday of March", "fixed", "agatha christie", "porto", "August 10, 1933", "one - mile - wide", "vancouver", "those who already hold wealth", "B. Traven", "Finding Nemo", "Forteans", "inflation", "squirrels", "264,152", "The Institute for Advanced Study", "German service cartridge", "DC electricity"], "metric_results": {"EM": 0.75, "QA-F1": 0.8321969696969698}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-6133", "mrqa_hotpotqa-validation-944", "mrqa_naturalquestions-validation-9227", "mrqa_naturalquestions-validation-6554", "mrqa_squad-validation-8070", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-4097"], "after_eval": {"predictions": ["capital of taiwan", "Dan Conner", "east and west berlin", "warren commission", "Katharine Hepburn, Joan Bennett, Frances Dee", "violence", "Joaquin Phoenix as Cash, Reese Witherspoon as Carter, Ginnifer Goodwin as Vivian Liberto, and Robert Patrick as Cash's father", "couscous", "1980s", "John M. Grunsfeld", "detroit", "your song", "2003", "every year", "Fabbrica Italiana Automobili Torino", "the second Sunday of March", "fixed", "agatha christie", "porto", "August 10, 1933", "one - mile - wide", "vancouver", "those who already hold wealth", "bilingual German author B. Traven, whose identity remains unknown", "Finding Nemo", "Fortean", "inflation", "black bear, white-tailed deer, raccoon, coyote, grey squirrel, chipmunk, and other small rodents", "247,597", "The Institute for Advanced Study", "German service cartridge", "DC"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9708104395604396}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9230769230769231, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-1276", "before_prediction": "squirrels", "after_prediction": "black bear, white-tailed deer, raccoon, coyote, grey squirrel, chipmunk, and other small rodents"}], "retained_ids": ["mrqa_hotpotqa-validation-2243", "mrqa_triviaqa-validation-2890", "mrqa_triviaqa-validation-1438", "mrqa_squad-validation-2493", "mrqa_triviaqa-validation-2522", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-7195", "mrqa_naturalquestions-validation-3698", "mrqa_triviaqa-validation-3017", "mrqa_hotpotqa-validation-5233", "mrqa_naturalquestions-validation-6839", "mrqa_squad-validation-10428", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-5595", "mrqa_hotpotqa-validation-2332", "mrqa_naturalquestions-validation-3108", "mrqa_triviaqa-validation-4024", "mrqa_squad-validation-7547", "mrqa_hotpotqa-validation-489", "mrqa_triviaqa-validation-2808", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-2203", "mrqa_squad-validation-3650"], "fixed_ids": ["mrqa_triviaqa-validation-6133", "mrqa_hotpotqa-validation-944", "mrqa_naturalquestions-validation-6554", "mrqa_squad-validation-8070", "mrqa_naturalquestions-validation-5272", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-4097"], "unfixed_ids": ["mrqa_naturalquestions-validation-9227"], "instant_fixing_rate": 0.875, "instant_retention_rate": 0.9583333329340278}, {"timecode": 40, "before_eval": {"predictions": ["50 fund", "Bartle Frere", "Samarkand", "sarajevo", "Isabella (Belle) Baumfree", "corgi", "14th to 17th centuries", "five starting pitchers, seven relief pitchers, two catchers, six infielders, and five outfielders", "placing them on prophetic faith", "Bacon", "Yul Brynner", "anti-inflammatory molecules, such as cortisol and catecholamines", "tartan", "Philadelphia Eagles", "tradeable entity used to avoid the inconveniences of a pure barter system", "United States Presidents and are generally directed towards officers and agencies of the U.S. federal government", "sound and light", "the Uighurs surrendered peacefully without violently resisting", "Sochi, Russia", "right", "Canadian Rockies continental divide east to central Saskatchewan, where it joins with another major river to make up the Saskatchewan River", "immediately", "australia", "stenographer", "30 Major League Baseball teams", "MI6", "nerve cells", "caesar", "photolysis", "4 - inch", "Queen City", "qui tam"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7762447188228438}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.375, 1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-6212", "mrqa_squad-validation-6248", "mrqa_naturalquestions-validation-8961", "mrqa_squad-validation-6588", "mrqa_naturalquestions-validation-1063", "mrqa_triviaqa-validation-1859", "mrqa_naturalquestions-validation-8514", "mrqa_triviaqa-validation-3280", "mrqa_naturalquestions-validation-6848"], "after_eval": {"predictions": ["50 fund", "Cetshwayo", "on the road back to Samarkand", "sarajevo", "Isabella (Belle) Baumfree", "corgi", "14th to 17th centuries", "seven relief pitchers", "placing them on prophetic faith", "Bacon", "Yul Brynner", "cytotoxic natural killer cells and CTLs", "tartan", "philadelphia eagles", "a tradeable entity used to avoid the inconveniences of a pure barter system", "express or implied Acts of Congress that delegate to the President some degree of discretionary power ( delegated legislation )", "clangers", "the Uighurs surrendered peacefully without violently resisting", "Sochi, Russia", "sweden", "central Saskatchewan", "immediately", "new zealand", "stenographer", "30 Major League Baseball teams", "MI6", "neuron", "caesar", "photolysis of ozone by light of short wavelength", "4 - inch screen size", "Queen City", "qui tam provision that allows people who are not affiliated with the government, called `` relators '' under the law, to file actions on behalf of the government"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9425}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-6127", "before_prediction": "sound and light", "after_prediction": "clangers"}, {"id": "mrqa_naturalquestions-validation-993", "before_prediction": "qui tam", "after_prediction": "qui tam provision that allows people who are not affiliated with the government, called `` relators '' under the law, to file actions on behalf of the government"}], "retained_ids": ["mrqa_squad-validation-395", "mrqa_triviaqa-validation-1571", "mrqa_hotpotqa-validation-1453", "mrqa_triviaqa-validation-2475", "mrqa_squad-validation-4953", "mrqa_squad-validation-2249", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-validation-7457", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-3213", "mrqa_naturalquestions-validation-10265", "mrqa_squad-validation-8247", "mrqa_hotpotqa-validation-4076", "mrqa_squad-validation-3932", "mrqa_triviaqa-validation-4123", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1791", "mrqa_triviaqa-validation-1877", "mrqa_triviaqa-validation-579", "mrqa_squad-validation-3617", "mrqa_hotpotqa-validation-178"], "fixed_ids": ["mrqa_naturalquestions-validation-6212", "mrqa_squad-validation-6248", "mrqa_naturalquestions-validation-8961", "mrqa_squad-validation-6588", "mrqa_naturalquestions-validation-1063", "mrqa_triviaqa-validation-1859", "mrqa_naturalquestions-validation-8514", "mrqa_triviaqa-validation-3280", "mrqa_naturalquestions-validation-6848"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9130434778638941}, {"timecode": 41, "before_eval": {"predictions": ["all war", "kishore Kumar, Mukri, Raj Kishore and Keshto Mukherjee played the supporting roles", "Gaels", "Three-card brag", "Idisi", "cave lion", "Russian film industry", "sediment load", "Washington metropolitan area", "GTPase responsible for endocytosis in the eukaryotic cell", "User State Migration Tool", "Ordos City China Science Flying Universe Science and Technology Co.", "berlin trilogy", "Duquesne University", "leicester", "Section 30", "Paul Lynde as Templeton, a care - free, egotistical rat who lives on a web in a corner of Homer's barn above Wilbur's pig pen", "October 1986", "Huge-LQG", "Retreating Monsoon", "Romansh", "george iii", "Perth", "Q Branch (or later Q Division) the fictional research and development division of the British Secret Service", "Philippians", "the division of labour, productivity, and free markets", "Gerard Marenghi", "bobby brown", "Nebula Award, the Philip K. Dick Award, and the Hugo Award", "margaret thatcher", "jonathan", "Luigi Creatore"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5188174936476566}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, false, true, false, false, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.1818181818181818, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.05, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.17391304347826084, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.125, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-2375", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-9712", "mrqa_hotpotqa-validation-201", "mrqa_triviaqa-validation-4955", "mrqa_naturalquestions-validation-6151", "mrqa_triviaqa-validation-2384", "mrqa_naturalquestions-validation-774", "mrqa_triviaqa-validation-6751", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-4823", "mrqa_naturalquestions-validation-8338", "mrqa_hotpotqa-validation-3872"], "after_eval": {"predictions": ["all war", "Hindi", "Gaelic", "In Crash, there is no betting, as in Brag", "Idisi", "lion", "The cinema of Russia", "sediment load", "FedExField in Landover, Maryland", "the scission of newly formed vesicles from the membrane of one cellular compartment and their targeting to, and fusion with, another compartment, both at the cell surface", "Windows Easy Transfer", "Ordos City", "flying disc", "Duquesne University", "leicester", "Section 30", "Paul Lynde", "October 1986", "4 billion", "Northeast Monsoon or Retreating Monsoon", "switzerland", "george iii", "5AA", "Q", "Philippians", "what builds nations'wealth", "Gerard Marenghi (born January 24, 1920)", "bobby brown", "Nebula Award, the Philip K. Dick Award, and the Hugo Award", "margaret smith", "jonathan", "Luigi Creatore"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9660479323308271}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8421052631578948, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-791", "before_prediction": "margaret thatcher", "after_prediction": "margaret smith"}], "retained_ids": ["mrqa_squad-validation-1592", "mrqa_hotpotqa-validation-5526", "mrqa_squad-validation-9355", "mrqa_naturalquestions-validation-3058", "mrqa_triviaqa-validation-684", "mrqa_squad-validation-2142", "mrqa_naturalquestions-validation-5007", "mrqa_triviaqa-validation-2181", "mrqa_naturalquestions-validation-7728", "mrqa_triviaqa-validation-1585", "mrqa_hotpotqa-validation-434", "mrqa_triviaqa-validation-4320", "mrqa_naturalquestions-validation-9763"], "fixed_ids": ["mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-2375", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-9712", "mrqa_hotpotqa-validation-201", "mrqa_triviaqa-validation-4955", "mrqa_naturalquestions-validation-6151", "mrqa_triviaqa-validation-2384", "mrqa_naturalquestions-validation-774", "mrqa_triviaqa-validation-6751", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-4823", "mrqa_hotpotqa-validation-3872"], "unfixed_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-8338"], "instant_fixing_rate": 0.8888888888888888, "instant_retention_rate": 0.9285714279081632}, {"timecode": 42, "before_eval": {"predictions": ["kill bill", "cavatelli, acini di pepe, pastina, orzo", "ballets", "derision", "not caused by the Roentgen rays, but by the ozone generated in contact with the skin, and to a lesser extent, by nitrous acid", "the American Civil War", "Chartered", "the judge increased her sentence from 40 to 60 days", "Danish", "second vice-captain", "egypt", "the side - chain of the amino acid N - terminal to the scissile amide bond ( the P position )", "Fructose", "the number of men killed and the manner of the attacks", "Gainsborough Trinity Football Club", "portrait", "Introverted Thinking ( Te )", "May", "white", "the appropriateness of the drug therapy (e.g. drug choice, dose, route, frequency, and duration of therapy) and its efficacy", "mars", "feats of exploration", "a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "rob lowe", "The Private Education Student Financial Assistance", "bow", "rebuild St. Peter's Basilica", "Algeria", "two forces", "Bills", "Qualcomm", "DTIME(n2)"], "metric_results": {"EM": 0.65625, "QA-F1": 0.8325500888000887}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 0.787878787878788, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 1.0, 0.7222222222222222, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6923076923076924, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-8284", "mrqa_squad-validation-1429", "mrqa_squad-validation-6735", "mrqa_hotpotqa-validation-5731", "mrqa_naturalquestions-validation-7233", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-4842", "mrqa_naturalquestions-validation-6706", "mrqa_squad-validation-6369", "mrqa_squad-validation-3385", "mrqa_squad-validation-10395"], "after_eval": {"predictions": ["kill bill", "usually cavatelli, acini di pepe, pastina, orzo, etc.", "ballets", "derision", "ozone generated in contact with the skin", "the American Civil War", "Chartered", "lack of remorse", "Danish", "centre-back", "egypt", "where the side - chain of the amino acid N - terminal to the scissile amide bond ( the P position ) is a large hydrophobic amino acid", "high fructose is a sweet, white, odorless, crystalline solid, and is the most water - soluble of all the sugars", "their unusual behavior", "Gainsborough Trinity Football Club is a football club based in Gainborough, Lincolnshire, England.", "portrait", "Extroverted Intuition ( Ne )", "May", "yellow", "drug choice, dose, route, frequency, and duration of therapy", "mars", "feats of exploration", "piston", "rob lowe", "The Private Education Student Financial Assistance", "bow", "rebuild St. Peter's Basilica in Rome", "Algeria", "two", "Bills", "Qualcomm", "DTIME(n2)"], "metric_results": {"EM": 0.84375, "QA-F1": 0.9293461134453781}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9047619047619047, 0.11764705882352941, 1.0, 0.9166666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-4185", "before_prediction": "Fructose", "after_prediction": "high fructose is a sweet, white, odorless, crystalline solid, and is the most water - soluble of all the sugars"}, {"id": "mrqa_triviaqa-validation-7426", "before_prediction": "white", "after_prediction": "yellow"}, {"id": "mrqa_squad-validation-2150", "before_prediction": "rebuild St. Peter's Basilica", "after_prediction": "rebuild St. Peter's Basilica in Rome"}], "retained_ids": ["mrqa_triviaqa-validation-2952", "mrqa_triviaqa-validation-3261", "mrqa_squad-validation-3044", "mrqa_naturalquestions-validation-9093", "mrqa_squad-validation-1841", "mrqa_hotpotqa-validation-3899", "mrqa_triviaqa-validation-4137", "mrqa_hotpotqa-validation-680", "mrqa_squad-validation-9569", "mrqa_triviaqa-validation-7133", "mrqa_hotpotqa-validation-4130", "mrqa_triviaqa-validation-1586", "mrqa_squad-validation-7034", "mrqa_triviaqa-validation-289", "mrqa_squad-validation-9792", "mrqa_squad-validation-9452", "mrqa_hotpotqa-validation-5522", "mrqa_squad-validation-1808"], "fixed_ids": ["mrqa_naturalquestions-validation-8284", "mrqa_squad-validation-1429", "mrqa_squad-validation-6735", "mrqa_hotpotqa-validation-5731", "mrqa_hotpotqa-validation-3308", "mrqa_naturalquestions-validation-6706", "mrqa_squad-validation-6369", "mrqa_squad-validation-3385", "mrqa_squad-validation-10395"], "unfixed_ids": ["mrqa_naturalquestions-validation-7233", "mrqa_hotpotqa-validation-4842"], "instant_fixing_rate": 0.8181818181818182, "instant_retention_rate": 0.8571428567346938}, {"timecode": 43, "before_eval": {"predictions": ["niagara falls", "15", "indiana", "salt lake city", "Italian", "sailor coming home from a round trip", "hostname", "recurring and life-threatening infections", "meyerbeer", "natural-ingredients-only personal care products", "horse", "Rigoletto", "largest country", "third", "ikea", "169", "mexico", "George Frampton", "Plies", "English rock band the Outfield", "tennis", "Michael Edwards ( briefly as the older Connor ) and then by teenage actor Edward Furlong", "road engines", "richard burton", "when the Moon's ecliptic longitude and the Sun's Ecliptica longitude differ by 0 \u00b0, 90 \u00b0, 180 \u00b0, and 270 \u00b0, respectively", "snudge", "Yuan T. Seaborg", "Kentucky, Virginia, and Tennessee", "avionics, telecommunications, and computers.", "Mitochondrial Eve", "237 square miles", "matthew"], "metric_results": {"EM": 0.8125, "QA-F1": 0.891517857142857}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 0.8095238095238095, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-981", "mrqa_hotpotqa-validation-4101", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-5968", "mrqa_squad-validation-8054", "mrqa_hotpotqa-validation-5541"], "after_eval": {"predictions": ["niagara falls", "15", "19th state", "salt lake city", "Italian", "sailor", "hostname", "recurring and life-threatening infections", "meyerbeer", "natural-ingredients-only personal care products", "horse", "Sparafucile", "largest country", "third-most", "ikea", "169", "mexico", "George Frampton", "Plies", "English rock band the Outfield", "tennis", "Edward Furlong", "road engines", "richard burton", "when the Moon's ecliptic longitude differ by 0 \u00b0, 90 \u00b0, 180 \u00b0, and 270 \u00b0, respectively", "snudge", "Yuan T. Lee", "Kentucky, Virginia, and Tennessee", "avionics, telecommunications, and computers.", "africa", "237", "matthew"], "metric_results": {"EM": 0.9375, "QA-F1": 0.962171052631579}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7894736842105263, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-5899", "before_prediction": "Mitochondrial Eve", "after_prediction": "africa"}], "retained_ids": ["mrqa_triviaqa-validation-7744", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-7538", "mrqa_triviaqa-validation-6916", "mrqa_hotpotqa-validation-2403", "mrqa_naturalquestions-validation-2663", "mrqa_squad-validation-6442", "mrqa_triviaqa-validation-5580", "mrqa_hotpotqa-validation-518", "mrqa_triviaqa-validation-5087", "mrqa_hotpotqa-validation-850", "mrqa_squad-validation-3671", "mrqa_triviaqa-validation-4090", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-5160", "mrqa_squad-validation-5586", "mrqa_hotpotqa-validation-5370", "mrqa_naturalquestions-validation-5822", "mrqa_triviaqa-validation-7482", "mrqa_squad-validation-3330", "mrqa_triviaqa-validation-2803", "mrqa_triviaqa-validation-7215", "mrqa_hotpotqa-validation-4624", "mrqa_squad-validation-3812", "mrqa_triviaqa-validation-305"], "fixed_ids": ["mrqa_hotpotqa-validation-981", "mrqa_hotpotqa-validation-4101", "mrqa_naturalquestions-validation-7641", "mrqa_squad-validation-8054", "mrqa_hotpotqa-validation-5541"], "unfixed_ids": ["mrqa_naturalquestions-validation-5968"], "instant_fixing_rate": 0.8333333333333334, "instant_retention_rate": 0.9615384611686391}, {"timecode": 44, "before_eval": {"predictions": ["King T'Chaka of the African nation Wakanda", "more experience and higher education", "the England and Wales Cricket Board ( ECB )", "paris", "football", "published campaign settings", "`` Fourth Revised Edition '' ISBN 0 - 06 - 015547 - 7", "867 feet", "the Byzantine Empire ( as \u039c\u03b1\u03bd\u03bf\u03c5\u03ae\u03bb ) to Spain and Portugal", "\u00f7 (pronounced \"divide\") is the third studio album by English singer-songwriter Ed Sheeran.", "C - 3PO", "8th", "well", "other healthcare professionals", "pharmacists are expected to become more integral within the health care system", "Favour", "Lecrae Devaughn Moore", "Mumbai Rajdhani Express", "Piazza Trinit\u00e0 dei Monti", "May 18, 2010", "Sylvia Pankhurst", "schengen", "Francis Bacon, 1st Viscount St Alban", "belgium", "The Ministry of Corporate Affairs", "Irish", "Vesta", "bront\u00eb", "energy", "hubble space telescope", "Christian Perfection", "chorale cantatas"], "metric_results": {"EM": 0.625, "QA-F1": 0.6755747126436782}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, true, true, false, true, false, true, false, true, true, false, true, true, true, false, false, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5517241379310345, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-2119", "mrqa_triviaqa-validation-6579", "mrqa_hotpotqa-validation-910", "mrqa_naturalquestions-validation-10612", "mrqa_squad-validation-7067", "mrqa_squad-validation-6319", "mrqa_triviaqa-validation-2406", "mrqa_naturalquestions-validation-8491", "mrqa_hotpotqa-validation-389", "mrqa_triviaqa-validation-3617", "mrqa_naturalquestions-validation-6057", "mrqa_triviaqa-validation-4027"], "after_eval": {"predictions": ["King T'Chaka of the African nation Wakanda", "more experience and higher education", "2003 for the inter-county competition in England and Wales", "paris", "sweden", "published campaign settings", "`` Fourth Revised Edition '' ISBN 0 - 06 - 015547 - 7", "867", "possibly brought from the Byzantine Empire ( as \u039c\u03b1\u03bd\u03bf\u03c5\u03ae\u03bb ) to Spain and Portugal, where it has been used since at least the 13th century", "\u00f7 (pronounced \"divide\") is the third studio album by English singer-songwriter Ed Sheeran.", "C - 3PO", "second most commonly", "well", "all health care settings", "patient care skills", "music", "Lecrae Devaughn Moore", "Mumbai Rajdhani Express", "Rome", "May 18, 2010", "Sylvia Pankhurst", "schengen", "Lord Chancellor of England", "belfast", "Indian government", "Irish", "Vesta", "branwell", "energy", "hubble space telescope", "Christian Perfection", "chorale cantatas"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9981617647058824}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": ["mrqa_naturalquestions-validation-6015", "mrqa_squad-validation-2236", "mrqa_triviaqa-validation-4197", "mrqa_hotpotqa-validation-5305", "mrqa_naturalquestions-validation-10619", "mrqa_hotpotqa-validation-4649", "mrqa_naturalquestions-validation-5550", "mrqa_triviaqa-validation-3664", "mrqa_squad-validation-6403", "mrqa_hotpotqa-validation-4278", "mrqa_naturalquestions-validation-3459", "mrqa_naturalquestions-validation-2169", "mrqa_hotpotqa-validation-4794", "mrqa_triviaqa-validation-5022", "mrqa_hotpotqa-validation-5696", "mrqa_naturalquestions-validation-1725", "mrqa_squad-validation-8625", "mrqa_triviaqa-validation-1504", "mrqa_squad-validation-9951", "mrqa_squad-validation-2419"], "fixed_ids": ["mrqa_triviaqa-validation-6579", "mrqa_hotpotqa-validation-910", "mrqa_naturalquestions-validation-10612", "mrqa_squad-validation-7067", "mrqa_squad-validation-6319", "mrqa_triviaqa-validation-2406", "mrqa_naturalquestions-validation-8491", "mrqa_hotpotqa-validation-389", "mrqa_triviaqa-validation-3617", "mrqa_naturalquestions-validation-6057", "mrqa_triviaqa-validation-4027"], "unfixed_ids": ["mrqa_naturalquestions-validation-2119"], "instant_fixing_rate": 0.9166666666666666, "instant_retention_rate": 0.9999999995}, {"timecode": 45, "before_eval": {"predictions": ["mike hammer", "Detroit Lions", "smoke it", "under `` the immortal Hawke ''", "florida", "frail", "gulf stream", "holy grail", "29 - year - old Mangal Pandey of the 34th BNI", "oppidum Ubiorum", "lorne greene", "Massachusetts", "1998", "they signed with Simon Cowell's record label Syco Music and released a cover of Damien Rice's `` Cannonball '' as their winner's single", "the main highway entrance at California State Route 1, and entrances in Carmel and Pacific Grove", "St. Louis", "Canadian", "Gareth", "LOVE Radio", "Colorado Rockies", "the court", "tony manero", "Sven Davison and David Dobkin", "worked to radicalize the Islamist movement", "People! and The Carnabeats", "Cashin' In", "the most recent Super Bowl champions", "Command/Service Module", "Santa Clara", "the tsar's Moscow residence", "Operation Neptune", "peninsular"], "metric_results": {"EM": 0.84375, "QA-F1": 0.9035182309638832}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407408, 0.6956521739130436, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-3989", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3363", "mrqa_hotpotqa-validation-866", "mrqa_naturalquestions-validation-4905"], "after_eval": {"predictions": ["mike hammer", "Detroit Lions", "smoke it", "his leg under `` the immortal Hawke ''", "death penalty", "frail", "tip of florida", "holy grail", "29 - year - old Mangal Pandey of the 34th BNI", "oppidum Ubiorum", "lorne greene", "Massachusetts", "1998", "They were the first group to win the competition", "the main highway entrance at California State Route 1", "St. Louis", "Canadian", "Gareth", "LOVE Radio", "Colorado Rockies", "the court", "john travolta", "David Dobkin", "worked to radicalize the Islamist movement", "People! and The Carnabeats", "\" Cashin' In\"", "the most recent Super Bowl champions", "Command/Service Module", "Santa Clara", "the tsar's Moscow residence", "Operation Neptune", "peninsular"], "metric_results": {"EM": 0.875, "QA-F1": 0.9162946428571428}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-4123", "before_prediction": "under `` the immortal Hawke ''", "after_prediction": "his leg under `` the immortal Hawke ''"}, {"id": "mrqa_triviaqa-validation-2823", "before_prediction": "gulf stream", "after_prediction": "tip of florida"}, {"id": "mrqa_triviaqa-validation-455", "before_prediction": "tony manero", "after_prediction": "john travolta"}], "retained_ids": ["mrqa_triviaqa-validation-7473", "mrqa_hotpotqa-validation-1907", "mrqa_triviaqa-validation-2201", "mrqa_squad-validation-2598", "mrqa_triviaqa-validation-5587", "mrqa_naturalquestions-validation-4097", "mrqa_squad-validation-9296", "mrqa_triviaqa-validation-6956", "mrqa_naturalquestions-validation-8728", "mrqa_triviaqa-validation-3335", "mrqa_hotpotqa-validation-5068", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-1433", "mrqa_squad-validation-5852", "mrqa_hotpotqa-validation-5149", "mrqa_naturalquestions-validation-9931", "mrqa_squad-validation-9513", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-3509", "mrqa_naturalquestions-validation-3217", "mrqa_squad-validation-3845", "mrqa_squad-validation-13", "mrqa_hotpotqa-validation-712", "mrqa_naturalquestions-validation-2067"], "fixed_ids": ["mrqa_triviaqa-validation-3989", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3363", "mrqa_hotpotqa-validation-866"], "unfixed_ids": ["mrqa_naturalquestions-validation-4905"], "instant_fixing_rate": 0.8, "instant_retention_rate": 0.8888888885596707}, {"timecode": 46, "before_eval": {"predictions": ["cuba", "that continents `` ploughed '' through the sea.", "m.E.N. Arena", "youngest person to become a national transgender figure", "the development of safety lamps, Stephenson's Rocket, Lord Armstrong's artillery, Be-Ro flour, Joseph Swan's electric light bulbs, and Charles Parsons' invention of the steam turbine", "improved the speed of encryption of communications at both ends in front line operations during World War II", "Galileo Galilei", "the absenceistence of the ultraviolet catastrophe", "Swansea City", "millais", "Joel", "massively multiplayer online role-playing video game", "hundreds", "Waiting for Guffman", "2003", "The Watermark business park", "pearmain", "partial funding", "pale lager", "unattainable", "Chu' Tsai", "Liz", "the least onerous", "italy", "Grissom, White, and Chaffee", "multinational retail corporation", "passion fruit", "The Natya Shastra is the foundational treatise for classical dances of India, and this text is attributed to the ancient scholar Bharata Muni.", "sedimentary rock and other material with a high iron concentration which oxidizes upon exposure to the air", "ryder cup", "incorrectly", "vienna"], "metric_results": {"EM": 0.78125, "QA-F1": 0.7894318181818182}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.08, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_triviaqa-validation-6905", "mrqa_squad-validation-5157", "mrqa_squad-validation-10341", "mrqa_squad-validation-10489", "mrqa_triviaqa-validation-7350", "mrqa_squad-validation-6271", "mrqa_naturalquestions-validation-988"], "after_eval": {"predictions": ["cuba", "that continents `` ploughed '' through the sea.", "take that", "youngest person to become a national transgender figure", "the incandescent lightbulb", "improved the speed of encryption of communications at both ends in front line operations during World War II.", "Einstein", "electromagnetic theory", "Swansea City", "millais", "Joel", "massively multiplayer online role-playing video game", "hundreds", "Waiting for Guffman", "2003", "The Watermark business park", "apple", "partial funding", "pale lager", "unattainable", "Chu'Tsai", "Liz", "the least onerous", "italy", "Grissom, White, and Chaffee", "multinational retail corporation", "purple passion fruit", "The Natya Shastra", "sedimentary rock and other material with a high iron concentration which oxidizes upon exposure to the air", "ryder cup", "incorrectly", "vienna"], "metric_results": {"EM": 0.96875, "QA-F1": 0.99375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_triviaqa-validation-1128", "before_prediction": "passion fruit", "after_prediction": "purple passion fruit"}], "retained_ids": ["mrqa_triviaqa-validation-3929", "mrqa_naturalquestions-validation-8204", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-5352", "mrqa_hotpotqa-validation-1831", "mrqa_triviaqa-validation-4081", "mrqa_naturalquestions-validation-3284", "mrqa_hotpotqa-validation-1074", "mrqa_squad-validation-2673", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-73", "mrqa_squad-validation-5464", "mrqa_squad-validation-7792", "mrqa_hotpotqa-validation-5239", "mrqa_naturalquestions-validation-2890", "mrqa_hotpotqa-validation-5226", "mrqa_squad-validation-4064", "mrqa_triviaqa-validation-2914", "mrqa_squad-validation-3913", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-2808", "mrqa_triviaqa-validation-2873", "mrqa_squad-validation-3523", "mrqa_triviaqa-validation-4430"], "fixed_ids": ["mrqa_triviaqa-validation-6905", "mrqa_squad-validation-5157", "mrqa_squad-validation-10341", "mrqa_squad-validation-10489", "mrqa_triviaqa-validation-7350", "mrqa_squad-validation-6271", "mrqa_naturalquestions-validation-988"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.959999999616}, {"timecode": 47, "before_eval": {"predictions": ["florida", "racehorses", "Burnley and the New Zealand national team", "Kumbum Monastery or Ta'er Shi near Xining", "Styal Mill", "big - name lawyers", "Milk Barn Animation", "during initial entry training", "moral tale", "They announced a hiatus and re-united two years later for the release of their fourth and final studio album, Destiny Fulfilled ( 2004 )", "l Leeds", "a star ( representing either the Star of Bethlehem or the star of David ), finials, angels ( `` Christmas angel '' ), or fairies", "260", "piccadilly", "Neighbourhood", "The Washington Post", "tentacles", "insect", "specific catechism questions", "a pH indicator, a color marker, and a dye", "about 50% oxygen composition at standard pressure", "2001", "George Whitefield", "Science and Discovery", "if 1 were considered a prime", "Western Kentucky University", "james Byron dean", "appearing as Jude in the musical romance drama film \"Across the Universe\" (2007)", "morgan spurlock", "her arranged marriage to Chino, a friend of Bernardo's", "XXXTentacion", "raises the productivity of each worker,"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7979166666666666}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0]}}, "error_ids": ["mrqa_hotpotqa-validation-1924", "mrqa_naturalquestions-validation-930", "mrqa_hotpotqa-validation-5877", "mrqa_naturalquestions-validation-2864", "mrqa_triviaqa-validation-1256", "mrqa_naturalquestions-validation-7849", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-4173", "mrqa_naturalquestions-validation-5241"], "after_eval": {"predictions": ["florida", "horseracing", "New Zealand national team", "Kumbum Monastery or Ta'er Shi near Xining", "Styal Mill", "William Jennings Bryan", "Goldie & Bear", "during initial entry training", "one of The Canterbury Tales by Geoffrey Chaucer", "Franklin quit after five months, leaving the group as a trio", "leeds", "a star ( representing either the Star of Bethlehem or the star of David ), finials, angels ( `` Christmas angel '' ), or fairies", "260", "piccadilly", "Neighbourhood", "The Washington Post", "tentacles", "insect", "specific catechism questions", "a pH indicator", "about 50% oxygen composition at standard pressure", "2001", "George Whitefield", "Science and Discovery", "1 were considered a prime", "Campbellsville University", "james byron dean", "Jude", "morgan spurlock", "Maria works in a bridal shop with Anita", "Jocelyn Flores", "raises the productivity of each worker,"], "metric_results": {"EM": 0.9375, "QA-F1": 0.9375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_naturalquestions-validation-2143", "before_prediction": "moral tale", "after_prediction": "one of The Canterbury Tales by Geoffrey Chaucer"}, {"id": "mrqa_naturalquestions-validation-2092", "before_prediction": "XXXTentacion", "after_prediction": "Jocelyn Flores"}], "retained_ids": ["mrqa_triviaqa-validation-2770", "mrqa_triviaqa-validation-945", "mrqa_squad-validation-6287", "mrqa_hotpotqa-validation-5788", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-5305", "mrqa_squad-validation-4212", "mrqa_triviaqa-validation-1516", "mrqa_hotpotqa-validation-5086", "mrqa_naturalquestions-validation-8582", "mrqa_squad-validation-4479", "mrqa_triviaqa-validation-3868", "mrqa_squad-validation-2346", "mrqa_squad-validation-3685", "mrqa_triviaqa-validation-4298", "mrqa_hotpotqa-validation-2801", "mrqa_squad-validation-1609", "mrqa_squad-validation-9061", "mrqa_triviaqa-validation-1799", "mrqa_triviaqa-validation-7477", "mrqa_squad-validation-7182"], "fixed_ids": ["mrqa_hotpotqa-validation-1924", "mrqa_naturalquestions-validation-930", "mrqa_hotpotqa-validation-5877", "mrqa_naturalquestions-validation-2864", "mrqa_triviaqa-validation-1256", "mrqa_naturalquestions-validation-7849", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-4173", "mrqa_naturalquestions-validation-5241"], "unfixed_ids": [], "instant_fixing_rate": 1.0, "instant_retention_rate": 0.9130434778638941}, {"timecode": 48, "before_eval": {"predictions": ["James Stenbeck", "Section.80", "el Capitan", "interventive", "3", "Lloyd Christ Wicke", "george charles", "During his epic battle with Frieza", "the director's own approved edit", "Shirley Williams", "UNESCO", "Satnam Singh Bhamara", "loop", "painting, mathematics, calligraphy, poetry, and theater", "part of a pre-recorded television program, Rendezvous with Destiny", "a sin", "whether they wish to collect a jackpot prize in cash or annuity", "Vader's daughter", "Buffalo Bill", "a place where justice resides", "France", "neutrality", "coffee", "permissible", "Arthur Russell", "the people", "Alex Burrall, Jason Weaver and Wylie Draper played Michael Jackson in different eras", "political", "the university's off-campus rental policies", "Dennis Hull, as well as painter Manley MacDonald.", "Pittsburgh Steelers", "famine"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8538690476190476}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.6666666666666665, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4]}}, "error_ids": ["mrqa_triviaqa-validation-3967", "mrqa_squad-validation-9860", "mrqa_triviaqa-validation-3906", "mrqa_naturalquestions-validation-3342", "mrqa_naturalquestions-validation-309", "mrqa_squad-validation-2263", "mrqa_squad-validation-4774"], "after_eval": {"predictions": ["James Stenbeck", "Section.80", "yosemite national park", "interventive", "3", "Bishop Lloyd Christ Wicke", "georgia", "During his epic battle with Frieza", "the director's own approved edit", "Shirley Williams", "UNESCO", "Thon Maker", "loop", "painting, mathematics, calligraphy, poetry, and theater", "part of a pre-recorded television program, Rendezvous with Destiny", "sin", "whether they wish to collect a jackpot prize in cash or annuity", "Vader's daughter", "Buffalo Bill", "justice", "France", "neutrality", "coffee", "permissible", "Arthur Russell", "the people", "Alex Burrall, Jason Weaver and Wylie Draper played Michael Jackson in different eras", "political", "the university's off-campus rental policies", "Dennis Hull, as well as painter Manley MacDonald.", "Pittsburgh Steelers", "war, famine, and weather"], "metric_results": {"EM": 0.96875, "QA-F1": 0.9895833333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [], "retained_ids": ["mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-5165", "mrqa_squad-validation-5665", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-1834", "mrqa_triviaqa-validation-2607", "mrqa_triviaqa-validation-2849", "mrqa_naturalquestions-validation-2445", "mrqa_squad-validation-8248", "mrqa_naturalquestions-validation-2582", "mrqa_squad-validation-2259", "mrqa_naturalquestions-validation-3789", "mrqa_hotpotqa-validation-4075", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-1649", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-4384", "mrqa_hotpotqa-validation-962", "mrqa_squad-validation-2337", "mrqa_naturalquestions-validation-1531", "mrqa_squad-validation-9518", "mrqa_squad-validation-7947", "mrqa_hotpotqa-validation-599", "mrqa_squad-validation-264"], "fixed_ids": ["mrqa_triviaqa-validation-3967", "mrqa_squad-validation-9860", "mrqa_triviaqa-validation-3906", "mrqa_naturalquestions-validation-309", "mrqa_squad-validation-2263", "mrqa_squad-validation-4774"], "unfixed_ids": ["mrqa_naturalquestions-validation-3342"], "instant_fixing_rate": 0.8571428571428571, "instant_retention_rate": 0.9999999996}, {"timecode": 49, "before_eval": {"predictions": ["light utility vehicles", "Sesame Street", "Timur", "\"Losing My Religion\"", "from Tamil Nadu in the south to West Bengal in the north through Andhra Pradesh and Odisha", "Ravenna (], also locally ] ; Romagnol: \"Rav\u00e8na\" ) is the capital city of the Province of Ravenna", "Bocelli became completely blind at the age of 12", "meat", "1895", "improved markedly", "a biplane capable of taking off vertically", "assigned them to the company in lieu of stock", "communist", "inspired by both his professors at the Palack\u00fd University, Olomouc ( Friedrich Franz and Johann Karl Nestler ), and his colleagues at the monastery ( such as Franz Diebl ) to study variation in plants", "Reserved matters", "Los Angeles Xtreme, San Francisco Demons and Memphis Maniax", "18,000 regulars, militia and Native American allies", "State Street", "the Saudi Arab kingdom", "110", "my fair lady", "horakhty", "Lawton Chiles", "In the episode `` Kobol's Last Gleaming ''", "McCrary", "informal\" imperialism", "Ugali with vegetables, sour milk, meat, fish or any other stew", "adenosine triphosphate", "stonehenge", "Ruth Elizabeth \"Bette\" Davis", "cobalt", "29 September 2014"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9201388888888888}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "error_ids": ["mrqa_naturalquestions-validation-3663", "mrqa_triviaqa-validation-6475", "mrqa_naturalquestions-validation-5283"], "after_eval": {"predictions": ["light utility vehicles", "Sesame Street", "Timur", "\"Losing My Religion\"", "from Tamil Nadu in the south to West Bengal in the north through Andhra Pradesh and Odisha", "Ravenna", "Bocelli became completely blind at the age of 12", "meat", "1895", "improved markedly", "a biplane capable of taking off vertically", "assigned them to the company", "communist", "the common edible pea", "Reserved", "Los Angeles Xtreme, San Francisco Demons and Memphis Maniax", "18,000 regulars, militia and Native American allies", "State Street", "the Saudi Arab kingdom", "110", "my fair lady", "falcon", "Lawton Chiles", "`` Kobol's Last Gleaming ''", "McCrary", "informal \"informal\" imperialism", "Ugali with vegetables, sour milk, meat, fish or any other stew", "adenosine triphosphate", "stonehenge", "Ruth Elizabeth \"Bette\" Davis", "cobalt", "29 September 2014"], "metric_results": {"EM": 0.90625, "QA-F1": 0.9539148351648352}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "forgotten_examples": [{"id": "mrqa_hotpotqa-validation-1364", "before_prediction": "Ravenna (], also locally ] ; Romagnol: \"Rav\u00e8na\" ) is the capital city of the Province of Ravenna", "after_prediction": "Ravenna"}, {"id": "mrqa_squad-validation-9807", "before_prediction": "informal\" imperialism", "after_prediction": "informal \"informal\" imperialism"}], "retained_ids": ["mrqa_naturalquestions-validation-2272", "mrqa_hotpotqa-validation-5452", "mrqa_squad-validation-6257", "mrqa_hotpotqa-validation-2032", "mrqa_naturalquestions-validation-2275", "mrqa_hotpotqa-validation-2941", "mrqa_triviaqa-validation-6073", "mrqa_hotpotqa-validation-987", "mrqa_squad-validation-716", "mrqa_squad-validation-1459", "mrqa_squad-validation-1306", "mrqa_hotpotqa-validation-2810", "mrqa_squad-validation-9489", "mrqa_hotpotqa-validation-3964", "mrqa_squad-validation-10291", "mrqa_hotpotqa-validation-1023", "mrqa_naturalquestions-validation-9013", "mrqa_hotpotqa-validation-4530", "mrqa_triviaqa-validation-3250", "mrqa_hotpotqa-validation-1315", "mrqa_squad-validation-7049", "mrqa_squad-validation-8518", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-5876", "mrqa_hotpotqa-validation-1772", "mrqa_triviaqa-validation-6380", "mrqa_hotpotqa-validation-2750"], "fixed_ids": ["mrqa_naturalquestions-validation-3663", "mrqa_triviaqa-validation-6475"], "unfixed_ids": ["mrqa_naturalquestions-validation-5283"], "instant_fixing_rate": 0.6666666666666666, "instant_retention_rate": 0.9310344824375743}], "final_eval_results": {"overall_oncoming_test": {"EM": 0.465, "QA-F1": 0.5544084843996151}, "overall_error_number": 856, "overall_instant_fixing_rate": 0.9147203101790524, "final_instream_test": {"EM": 0.8975, "QA-F1": 0.9332977974339158}, "final_upstream_test": {"EM": 0.683, "QA-F1": 0.7424515640791955}}}